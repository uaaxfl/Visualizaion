2014.lilt-9.2,P10-2045,1,0.900842,"Missing"
2014.lilt-9.2,H05-1079,0,0.0468659,"om another research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al. 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Until now rather few and quite limited RTE systems were made publicly available. These systems are quite restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS 3 (Kouylekov and Negri 2010) is a distance-based RTE system, which can exploit only lexical knowledge resources. NutCracker 4 (Bos and Markert 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum 1998) as a lexical knowledge resource. To address the above needs, we provide our open-source textualentailment system, BiuTee.5 Our system provides state-of-the-art linguistic analysis tools and exploits various types of manually built and automatically acquired knowledge resources, including lexical, lexicalsyntactic and syntactic rewrite rules. Furthermore, the system components, including pre-processing utilities, knowledge resources, and even the steps of the inference algorithm"
2014.lilt-9.2,W04-3205,0,0.121231,"Missing"
2014.lilt-9.2,P05-1045,0,0.0115967,"tagger Parser Coreference resolver Text normalization is done through a collection of text modifications that are performed over the raw text, prior to any linguistic analysis. Mainly, we perform number normalization,9 and heuristically add missing punctuations when necessary. Then, BiuTee proceeds to linguistic processing, using state-of-theart utilities: Tokenization and part-of-speech-tagging are performed by Stanford utilities (Toutanova et al. 2003), parsing is performed by EasyFirst parser (Goldberg and Elhadad 2010), named-entity recognition is done by Stanford named-entity-recognizer (Finkel et al. 2005) and coreference resolution is performed by ArkRef coreference resolver.10 9 Available to download as a stand-alone utility at http://www.cs.biu.ac.il/ nlp/downloads/normalizer.html ~ 10 This tool is a reimplementation of (Haghighi and Klein 2009), and is downloadable from http://www.ark.cs.cmu.edu/ARKref/. The BIUTEE Research Platform / 15 As a flexible system, BiuTee provides simple interfaces for each type of linguistic analysis utility. This way, replacing the above-mentioned utilities can be done by merely implementing the relevant interfaces. 3.2 Proof construction Entailment recognition"
2014.lilt-9.2,W07-1401,1,0.858389,"ave already been constructed for a complete (T, H) pairs dataset. However, to construct these proofs, a weight vector is needed for the search algorithm. We solve this problem by an iterative learning scheme. In this scheme we initialize w with a reasonable guess vector, and iteratively process the whole dataset, such that in each iteration w is improved. More information about the learning scheme can be found in (Stern and Dagan 2011). 3.4 Configuration, adaptation and extension BiuTee supports all of the RTE datasets that have been published so far (Dagan et al. 2006b; Bar-Haim et al. 2006; Giampiccolo et al. 2007, 2008; Bentivogli et al. 2009, 2010, 2011). Controlling the system behaviour, adapting it to specific needs as well as extending it, can be done at various levels. First, many of the system parameters are controlled by a configuration file, as follows: 1. Parser: the user can choose to use either easy-first parser (Goldberg and Elhadad 2010) or Minipar parser (Lin 1998b). 2. Coreference resolver: The user can choose the coreference resolver to be either ArkRef13 or Bart (Versley et al. 2008). A third option is to configure the system to skip coreference resolution. 3. Knowledge Resources: The"
2014.lilt-9.2,N10-1115,0,0.323159,"transformations, such that each of them preserves the meaning of the text, as follows. When a transformation is applied on a text t, it transforms it into a new text, t’. The goal is that the meaning of t’ will be entailed from t. We apply many types of transformations, that are derived from many knowledge and linguistic resources. These transformations are relatively reliable, in most cases. In addition, we allow less reliable transformations, to be utilized when no prior knowledge of reliable transformations 6 / Asher Stern and Ido Dagan FIGURE 1: A parse tree, parsed by Easy-First parser (Goldberg and Elhadad 2010). is available. Then, we estimate the validity of every transformation, and consequently of every sequence of transformations, and decide whether the text entails the hypothesis based on this estimation. In the rest of this subsection we describe the transformations allowed by BiuTee, and in the next subsection we describe the model by which transformation reliability is estimated. Entailment rules The main type of transformations, and the most reliable one, is the application of entailment-rules (Bar-Haim et al. 2007), which are available from various knowledge resources. An entailment rule i"
2014.lilt-9.2,N03-1013,0,0.0195887,"e of English words, which are interconnected by means of conceptual-semantic and lexical relations. We used the following WordNet relations: synonym, derivationally-related, hypernym, instance-hypernym, partholonym, member-holonym, substance-meronym and verb-entailment. . Wikipedia – Wikipedia-based entailment-rules (Shnarch et al. 2009) which contain background knowledge about various entities and concepts, e.g., Einstein is a Scientist. . GEO – a geographical knowledge resource (Mirkin et al. 2009) which contains information like New-York is in United States. . CatVar – English derivations (Habash and Dorr 2003). For example motivation (noun) is derived from motivate (verb). . DIRECT – directional distributional similarity (Kotlerman et al. 2010). . Distributional-similarity-based entailment rules by Lin (1998a), as well as a reimplementation of the Lin-Similarity algorithm on the Reuters-corpus.7 7 http://trec.nist.gov/data/reuters/reuters.html The BIUTEE Research Platform / 9 . VerbOcean (Chklovski and Pantel 2004). – A broad-coverage semantic network of verbs. The used VerbOcean relations are configurable in BiuTee’s configuration file. We achieved the best results when using only the “stronger th"
2014.lilt-9.2,D09-1120,0,0.0309946,"Missing"
2014.lilt-9.2,H05-1087,0,0.0208403,"ng algorithm, but, similar to other components, alternative learning-algorithms can be integrated easily by implementing an appropriate interface. Note that for the last two RTE datasets of 2010 and 2011 (RTE-6 and RTE-7), the goal is to optimize the F1 measure12 of the positive entailments, while in the older datasets, the goal is to optimize the accuracy measure. We implemented the logistic regression classifier to optimize each of these measures. The accuracy-optimized classifier is the standard logistic-regression classifier, while the F1-optimized classifier was implemented according to (Jansche 2005). The appropriate classifier is automatically chosen by the system, based on the given dataset. We note that the weight vector, w, has a dual role. One is to assign a cost to the constructed proof. The other role is to be used by the search algorithm. As described above, the search algorithm prunes some of the intermediate trees that are generated during the proof construction, and it favours trees that are more likely to be part of the best (cheapest) proof. One of the parameters by which the search algorithm estimates this likelihood is the current cost of the generated tree. If that cost is"
2014.lilt-9.2,P07-2045,0,0.00290253,"ch a complete RTE system for their research, but could integrate their novel research components into an existing opensource system. Such research e↵orts might include developing knowledge resources, developing inference components for specific phenomena such as temporal inference (see, for example, (Wang and Zhang 2008)), or extending RTE to di↵erent languages. A flexible and extensible RTE system is expected to encourage researchers to create and share their textual-inference components. A good example from another research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al. 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Until now rather few and quite limited RTE systems were made publicly available. These systems are quite restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS 3 (Kouylekov and Negri 2010) is a distance-based RTE system, which can exploit only lexical knowledge resources. NutCracker 4 (Bos and Markert 2005) is a system based on logical representation and automatic theorem proving, but utili"
2014.lilt-9.2,P10-4008,0,0.022261,"le RTE system is expected to encourage researchers to create and share their textual-inference components. A good example from another research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al. 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Until now rather few and quite limited RTE systems were made publicly available. These systems are quite restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS 3 (Kouylekov and Negri 2010) is a distance-based RTE system, which can exploit only lexical knowledge resources. NutCracker 4 (Bos and Markert 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum 1998) as a lexical knowledge resource. To address the above needs, we provide our open-source textualentailment system, BiuTee.5 Our system provides state-of-the-art linguistic analysis tools and exploits various types of manually built and automatically acquired knowledge resources, including lexical, lexicalsyntactic and syntactic rewrite rules. Furthermore, the s"
2014.lilt-9.2,P98-2127,0,0.0328071,", member-holonym, substance-meronym and verb-entailment. . Wikipedia – Wikipedia-based entailment-rules (Shnarch et al. 2009) which contain background knowledge about various entities and concepts, e.g., Einstein is a Scientist. . GEO – a geographical knowledge resource (Mirkin et al. 2009) which contains information like New-York is in United States. . CatVar – English derivations (Habash and Dorr 2003). For example motivation (noun) is derived from motivate (verb). . DIRECT – directional distributional similarity (Kotlerman et al. 2010). . Distributional-similarity-based entailment rules by Lin (1998a), as well as a reimplementation of the Lin-Similarity algorithm on the Reuters-corpus.7 7 http://trec.nist.gov/data/reuters/reuters.html The BIUTEE Research Platform / 9 . VerbOcean (Chklovski and Pantel 2004). – A broad-coverage semantic network of verbs. The used VerbOcean relations are configurable in BiuTee’s configuration file. We achieved the best results when using only the “stronger than” relation. Lexical-Syntactic knowledge resources, which capture entailment between predicate-argument structures within BiuTee include: . DIRT (Lin and Pantel 2001) . REVERB (Berant 2012) . FrameNet-"
2014.lilt-9.2,N13-1091,1,0.838562,"ly due to negations, but also due to other natural language constructions that change the truth-value of a predicate. For example, given a text “The computer failed to calculate the equation.”, the truth-value of “calculate” is negative, since it can be inferred that the computer did not calculate the equation (See (Karttunen 1971)). Thus, we should not treat the predicate “calculate” in a hypothesis “The computer calculated the equation.” as identical to “calculate” in the text. The BIUTEE Research Platform / 11 Handling such cases is performed by the integration of Truth-Teller (Lotan 2012; Lotan et al. 2013), which annotates truth-values of all the predicates in a given sentence, by utilizing several mechanisms. Given a sentence, Truth-Teller begins by annotating some of its clauses with a polarity of positive, negative, or unknown. This is done by identifying pre-suppositions, which are marked as positive-polarity clauses, as well as the main clause of the sentence, which is always marked as positive. In addition, it annotates the predicates of the annotated clauses with a positive, negative or unknown polarity. This annotation is based on the clause annotation along with identification of negat"
2014.lilt-9.2,P10-1123,1,0.856729,"between predicate-argument structures within BiuTee include: . DIRT (Lin and Pantel 2001) . REVERB (Berant 2012) . FrameNet-based entailment-rules (Ben-Aharon et al. 2010) As for entailment-rules that capture syntactic phenomena, we use the freely available collection of rules by Lotan (2012). Discourse phenomena Other phenomena that must be handled by textual entailment systems are discourse phenomena, by which the desired information, expressed within a single sentence hypothesis, can be spread over several sentences of the text. The importance of this type of phenomena was investigated in (Mirkin et al. 2010), along with several proposals for ways to handle it by utilizing coreference information. In BiuTee, we utilize coference resolution information by defining two types of transformations: coreference substitution and Is-A coreference. We obtain the coreference information from an o↵-the-shelf coreference resolution system, ArkRef (see Subsection 3.1). Coreference substitution is implemented as follows: one mention of an entity is replaced by another mention of the same entity, based on a coreference relation between them. Consider, for example, the following text-hypothesis pair: Text: ... Oba"
2014.lilt-9.2,P09-1051,1,0.815845,"res (X Verb[active] Y $ Y is Verb[passive] by X). See illustration in Figure 3. BiuTee incorporates a comprehensive set of knowledge resources, which are represented as entailment-rules. Lexical knowledge resources within BiuTee include: . WordNet (Fellbaum 1998) – a large database of English words, which are interconnected by means of conceptual-semantic and lexical relations. We used the following WordNet relations: synonym, derivationally-related, hypernym, instance-hypernym, partholonym, member-holonym, substance-meronym and verb-entailment. . Wikipedia – Wikipedia-based entailment-rules (Shnarch et al. 2009) which contain background knowledge about various entities and concepts, e.g., Einstein is a Scientist. . GEO – a geographical knowledge resource (Mirkin et al. 2009) which contains information like New-York is in United States. . CatVar – English derivations (Habash and Dorr 2003). For example motivation (noun) is derived from motivate (verb). . DIRECT – directional distributional similarity (Kotlerman et al. 2010). . Distributional-similarity-based entailment rules by Lin (1998a), as well as a reimplementation of the Lin-Similarity algorithm on the Reuters-corpus.7 7 http://trec.nist.gov/dat"
2014.lilt-9.2,P11-2098,1,0.859159,"n the following subsections we discuss each of these aspects. Finally, we deal with another crucial issue, namely: 6 In practice, this goal is heuristically relaxed in the current version of BiuTee to having the hypothesis embedded in the obtained transformed text. The BIUTEE Research Platform / 5 4. How to find automatically an “optimal” sequence of transformations that transforms T into H? 2.1 Representation level There are several levels on which the text and the hypothesis can be represented. The simplest level of representation is the lexical level (e.g. bag of words) (see, for example, (Shnarch et al. 2011; Clark and Harrison 2010)). While this level has some advantages, for example, it can be easily implemented for languages that lack linguistic processing tools, it cannot handle structural di↵erences of T and H. Consider, for example, the text “The first chapter of the book was written yesterday” and the hypothesis “The book was written yesterday”. Though the hypothesis words are embedded in the text in the same order, the text does not entail the hypothesis. A common representation level of sentence structure is the syntactic representation, given as parse trees (See Figure 1). This level of"
2014.lilt-9.2,R11-1063,1,0.823363,"Missing"
2014.lilt-9.2,P12-1030,1,0.858506,"in the size of the proof. BiuTee provides implementations of several search algorithms, which di↵er from one another in their speed and proof quality (measured by the cost of the proofs they find). A novel improved search algorithm that directly utilizes characteristics of the textual-inference domain was recently developed and integrated into BiuTee. This algorithm iteratively generates limited-length subsequences of transformations. In each iteration it measures the quality-cost ratio of each generated subsequence, and chooses the one with the best ratio. The full details are described in (Stern et al. 2012). 3 System Architecture The input of BiuTee is a collection of (T,H) pairs, and the output is an entailment / non-entailment classification for each pair. To determine these classifications, each (T,H) pair is pipe-lined in several processing phases which (1) generate the appropriate representations, (2) find a sequence of transformations (a proof) which transforms the text into the hypothesis, (3) classify these proofs. During training, the last step, classification of proofs, is replaced by learning a classification model. We note that in phase (2) we start with inference-related calculation"
2014.lilt-9.2,P06-2105,0,0.0259252,"sterday” and the hypothesis “The book was written yesterday”. Though the hypothesis words are embedded in the text in the same order, the text does not entail the hypothesis. A common representation level of sentence structure is the syntactic representation, given as parse trees (See Figure 1). This level of representation was adopted by the vast majority of RTE systems (e.g., Iftene 2008; Cabrio et al. 2008; Wang and Neumann 2008). A deeper representation is the logical form level, which represents T and H as logical clauses, extracted from the syntactic representation. Typical examples are Tatu and Moldovan (2006), Raina et al. (2005) and Clark and Harrison (2010) While deep logical representations may capture additional aspects of the meaning of the text, their much higher complexity makes them more vulnerable to inaccuracies and errors involved in their generation. Moreover, most of the structural information required for inference can be found in the syntactic representation, which was therefore chosen for BiuTee. However, since syntax does not capture some key semantic properties (e.g., the truth-value of predicates), we enrich the syntactic parse trees with additional annotations, as described lat"
2014.lilt-9.2,N03-1033,0,0.00674351,"ext fragment (which may be either T or H) is processed through the following steps. 1. 2. 3. 4. 5. 6. Text normalization Sentence splitter Tokenizer Part-of-speech tagger Parser Coreference resolver Text normalization is done through a collection of text modifications that are performed over the raw text, prior to any linguistic analysis. Mainly, we perform number normalization,9 and heuristically add missing punctuations when necessary. Then, BiuTee proceeds to linguistic processing, using state-of-theart utilities: Tokenization and part-of-speech-tagging are performed by Stanford utilities (Toutanova et al. 2003), parsing is performed by EasyFirst parser (Goldberg and Elhadad 2010), named-entity recognition is done by Stanford named-entity-recognizer (Finkel et al. 2005) and coreference resolution is performed by ArkRef coreference resolver.10 9 Available to download as a stand-alone utility at http://www.cs.biu.ac.il/ nlp/downloads/normalizer.html ~ 10 This tool is a reimplementation of (Haghighi and Klein 2009), and is downloadable from http://www.ark.cs.cmu.edu/ARKref/. The BIUTEE Research Platform / 15 As a flexible system, BiuTee provides simple interfaces for each type of linguistic analysis uti"
2014.lilt-9.2,P08-4003,0,0.0165607,"all of the RTE datasets that have been published so far (Dagan et al. 2006b; Bar-Haim et al. 2006; Giampiccolo et al. 2007, 2008; Bentivogli et al. 2009, 2010, 2011). Controlling the system behaviour, adapting it to specific needs as well as extending it, can be done at various levels. First, many of the system parameters are controlled by a configuration file, as follows: 1. Parser: the user can choose to use either easy-first parser (Goldberg and Elhadad 2010) or Minipar parser (Lin 1998b). 2. Coreference resolver: The user can choose the coreference resolver to be either ArkRef13 or Bart (Versley et al. 2008). A third option is to configure the system to skip coreference resolution. 3. Knowledge Resources: The user can provide a list of knowledge resources to be used for the proof construction. Using many knowledge resources often increases the system performance, but also increases its runtime. 4. Multi-threading: The number of concurrent threads used by the system is a configurable parameter. 5. Plug-ins: If the user writes a plug-in, it can be integrated into the system through configuration file parameters. Beyond configuration, the system can be extended by the plug-in mechanism, described in"
2014.lilt-9.2,2003.mtsummit-systems.9,0,\N,Missing
2014.lilt-9.2,C98-2122,0,\N,Missing
2020.acl-main.626,P13-1023,0,0.017291,"annotations, exhibited in the Dense dataset (§2) as well as the output of the Fitzgerald et al. (2018) parser (§5). To that end, we ignore redundant true-positives, and collapse false-positive errors (see Appendix for details). 4 Dataset Quality Analysis Inter-Annotator Agreement (IAA) To estimate dataset consistency across different annotations, we measure F1 using our UA metric. 10 individual worker-vs-worker experiments yield 79.8 F1 agreement over 150 predicates, indicating high consistency across our annotators, in line with agreement rates in other structured semantic annotations, e.g. Abend and Rappoport (2013). Overall consistency of the dataset is assessed by measuring agreement between different consolidated annotations, obtained by disjoint triplets of workers, which achieves F1 of 84.1, averaged over 4 experiments, 35 predicates each. Notably, consolidation boosts agreement, indicating its necessity. For LA agreement, averaged F1 was 67.8; however, it is likely that the drop from UA is mainly due to falsely rejecting semantically equivalent questions under the S TRICT-M ATCH criterion, given that we found equal LA and UA scores in a manual evaluation of our dataset (see Table 4 below). Dataset"
2020.acl-main.626,P98-1013,0,0.0994746,"t an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering que"
2020.acl-main.626,P19-1409,1,0.751829,"t set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model em"
2020.acl-main.626,N18-1076,0,0.0125319,"e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achie"
2020.acl-main.626,I17-1010,0,0.0131356,"nding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implici"
2020.acl-main.626,P18-1191,1,0.943217,"20. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not resign, cut funding to a program she ran. (2) Why was something cut by someone? she did not resign (3) Who cut something? Perry Table 1: QA-SRL examples. The bar (|) separates multiple answers. Implicit arguments are highlighted. Previous attempts to annotate QA-SRL initially involved trained annotators (He et al., 2015) but later resorted to crowdsourcing (Fitzgerald et al., 2018) for scalability. Naturally, employing crowd workers is challenging when annotating fairly demanding structures like SRL. As Fitzgerald et al. (2018) acknowledge, the main shortage of their large-scale dataset is limited recall, which we estimate to be in the lower 70s (see §4). Unfortunately, such low recall in gold standard datasets hinders proper research and evaluation, undermining the current viability of the QA-SRL paradigm. Aiming to enable future QA-SRL research, we present a generic controlled crowdsourcing annotation protocol and apply it to QA-SRL. Our process addresses worker quali"
2020.acl-main.626,D15-1076,1,0.934655,"rd will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception of semantic roles as answering questions about the predicate (“Who did What to Whom, When, Where and How”, with, e.g., “Who” corresponding to the agent role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexic"
2020.acl-main.626,J05-1004,0,0.177821,"valuation. In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase. Applying this protocol to QA-SRL yielded highquality annotation with drastically higher coverage, producing a new gold evaluation dataset. We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations. 1 Introduction Semantic Role Labeling (SRL) provides explicit annotation of predicate-argument relations. Common SRL schemes, particularly PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998), rely on predefined role inventories and extensive predicate lexicons. Consequently, SRL annotation of new texts requires substantial efforts involving expert annotation, and possibly lexicon extension, limiting scalability. Aiming to address these limitations, QuestionAnswer driven Semantic Role Labeling (QA-SRL) (He et al., 2015) labels each predicate-argument relationship with a question-answer pair, where natural language questions represent semantic roles, and answers correspond to arguments (see Table 1). This approach follows the colloquial perception"
2020.acl-main.626,D16-1252,1,0.870589,"L schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could be arrested, including the councillor. (1) Who might be arrested? 47 people |the councillor Perry called for the DAs resignation, and when she did not r"
2020.acl-main.626,P15-2115,0,0.0277604,"ehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in t"
2020.acl-main.626,J12-4003,0,0.0287838,"t role). QA-SRL carries two attractive promises. First, using a question-answer format makes the annotation task intuitive and easily attainable by laymen, as it does not depend on linguistic resources (e.g. role lexicons), thus facilitating greater annotation scalability. Second, by relying on intuitive human comprehension, these annotations elicit a richer argument set, including valuable implicit semantic arguments not manifested in syntactic structure (highlighted in Table 1). The importance of implicit arguments has been recognized in the literature (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), yet they are mostly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as"
2020.acl-main.626,2020.acl-main.772,0,0.0935617,"stly overlooked by common SRL formalisms and tools. Overall, QA-SRL largely subsumes predicateargument information captured by traditional SRL schemes, which were shown beneficial for complex downstream tasks, such as dialog modeling (Chen et al., 2013), machine comprehension (Wang et al., 2015) and cross-document coreference (Barhom et al., 2019). At the same time, it contains richer information, and is easier to understand and collect. Similarly to SRL, one can utilize QA-SRL both as a source of semantic supervision, in order to achieve better implicit neural NLU models, as done recently by He et al. (2020), as well as an explicit semantic structure for downstream use, e.g. for producing Open Information Extraction propositions (Stanovsky and Dagan, 2016).1 1 Indeed, making direct use of QA-SRL role questions might seem more challenging than with categorical semantic roles, as in traditional SRL. In practice, however, when a model embeds QA-SRL questions in context, we would expect 7008 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7008–7013 c July 5 - 10, 2020. 2020 Association for Computational Linguistics WH Why Why Who Around 47 people could b"
2020.acl-main.626,C98-1013,0,\N,Missing
2020.coling-main.274,J12-4003,0,0.0254671,"ing the intuitive nature of QA-SRL, Fitzgerald et al. (2018) crowdsourced a large scale QA-SRL corpus via Amazon Mechanical Turk, and released the first QA-SRL parser. QA-SRL is appealing not just for its scalability, but also for its content. By relying on natural comprehension of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 202"
2020.coling-main.274,N03-1013,0,0.0817758,"ks — detect predicates in text, extract their arguments, and label each with a semantic role. While verb predicates are relatively easy to detect, requiring no more than a POS tagger, this phase is not trivial for nominal SRL. For QANom, the predicate detection task is coupled with finding a corresponding verb whose meaning is aligned to the nominal predicate. In this way, the QA-SRL questions centered by this verb could naturally capture arguments of the noun. Thus, our data collection setting involves leveraging lexical resources to find candidate nouns having a related verb. We use CatVar (Habash and Dorr, 2003), WordNet (Miller, 1995) and an in-house suffix-driven heuristic to identify those noun candidates, along with their corresponding verb. More details about our lexical candidate extraction procedure are in Appendix 8.2. During annotation, workers first determine whether a candidate noun mention carries verbal meaning in the context of the sentence (IS V ERBAL). We instructed the workers to consider the automatically extracted related verb, and to judge whether it will be natural to ask questions about the target noun instance using this verb. For example, given the noun phrase “the organizatio"
2020.coling-main.274,D15-1076,1,0.375208,"uestion-Answer driven SRL for Nominalizations Ayal Klein1 Jonathan Mamou2 Valentina Pyatkin1 Daniela Brook Weiss1 Hangfeng He3 Dan Roth3 Luke Zettlemoyer4 Ido Dagan1 1 Computer Science Department, Bar Ilan University 2 Intel Labs, Israel 3 University of Pennsylvania 4 University of Washington {ayal.s.klein,jonathan.mamou,valpyatkin, daniela.stepanov}@gmail.com {hangfeng,danroth}@seas.upenn.edu lsz@cs.washington.edu dagan@cs.biu.ac.il Abstract We propose a new semantic scheme for capturing predicate-argument relations for nominalizations, termed QANom. This scheme extends the QA-SRL formalism (He et al., 2015), modeling the relations between nominalizations and their arguments via natural language question-answer pairs. We construct the first QANom dataset using controlled crowdsourcing, analyze its quality and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction S"
2020.coling-main.274,P18-2058,1,0.840711,"the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-d"
2020.coling-main.274,2020.acl-main.772,1,0.904092,"it arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal predicates. We identify verbal nouns that are eventive in nature along with their correspo"
2020.coling-main.274,W06-1617,0,0.0756852,"rhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalization"
2020.coling-main.274,kingsbury-palmer-2002-treebank,0,0.789046,"nnotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument str"
2020.coling-main.274,P13-1008,0,0.0813397,"Missing"
2020.coling-main.274,meyers-etal-2004-annotating,0,0.129033,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,W04-2705,0,0.84605,"ally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file shared by PropBank’s verbal predicates, and accordingly a shared role-set. This design principle is meant for converging the semantic role representation of deverbal nominalizations with their corresponding verbal predicates, thus abstracting semantic content over surface rea"
2020.coling-main.274,N18-2089,1,0.772819,"ts not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). A related QA-driven semantic annotation dataset is QA-driven Meaning Represen"
2020.coling-main.274,W18-3601,0,0.0213093,"Missing"
2020.coling-main.274,S15-2153,0,0.0987273,"Missing"
2020.coling-main.274,C08-1084,0,0.111033,"Missing"
2020.coling-main.274,D14-1162,0,0.0861543,"Missing"
2020.coling-main.274,W13-3516,0,0.0484846,"Missing"
2020.coling-main.274,P18-2124,0,0.0583201,"in focus, to investigate the interaction between the semantic space of the pre-training task and of the target task. See Appendix 8.5 for further experimental details. Our findings are shown in Table 6. Generally, results re-establish the findings of He et al. (2020), that further pre-training on QA annotations improve downstream performance over the BERT encoder, especially in low resource settings. This effect is more profound for semantically-oriented QA annotation (QAMR, QA-SRL and QANom), tackling semantic relations within a sentence, than for simple questionanswering data, i.e., SQuAD (Rajpurkar et al., 2018). As a trend, results also indicate that the performance gain is larger the more relevant the semantic space of the pre-training task is for the target task. Specifically, QAMR provides the best “generalpurpose” semantic signal, outperforming all the other models on most heterogeneous tasks (i.e. tasks with mixed types of “targets”). This is inline with its “whole-sentence” semantic nature, capturing relations for all word types. On the other hand, QANom’s semantic signal is improving BERT’s performance particularly for noun-targeting tasks, performing comparably to QAMR, and the same is true"
2020.coling-main.274,2020.acl-main.626,1,0.402678,"”. Thomas ARG0 Who has proved something? NomBank ARG1 QANom What has someone proved? the existence of God How did someone prove something? the Ontological argument Table 1: An illustration of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as t"
2020.coling-main.274,D16-1252,1,0.933373,"of PropBank, NomBank, QA-SRL and QANom annotations for corresponding semantic information. Implicit arguments are highlighted, captured by the QA formalisms but not the others. The bar (|) separates multiple answers. easily interpretable semantic annotations, facilitating scalable crowdsourced annotation methodologies, both for large train sets (Fitzgerald et al., 2018) and for high-quality evaluation sets (Roit et al., 2020). QA-SRL was shown to cover most predicate-argument structures captured by PropBank (He et al., 2015; Roit et al., 2020), to subsume popular intermediate representations (Stanovsky and Dagan, 2016), and recently, to enhance strong transformer-based sentence encoders (He et al., 2020) (§2.2). In this work, we further pursue the overarching goal of developing a broad-coverage structured representation of sentence semantics through a natural, easily interpretable, and scalably attainable annotation scheme, following the QA-SRL paradigm. We introduce QA-SRL for Nominalizations, denoted QANom, as the most natural first extension of verbal QA-SRL (See Table 1). Analogical to the original NomBank motivation, we wish to construct a unified question-answer based scheme for verbal and nominal pre"
2020.coling-main.274,N18-1081,1,0.845623,"sion of the sentence, the QA format elicits a richer argument set than traditional linguistically-rooted formalisms, including many valuable implicit arguments not manifested in syntactic structure (Roit et al., 2020). Although the importance of implicit relations has been established (Cheng and Erk, 2018; Do et al., 2017; Gerber and Chai, 2012), most SRL resources leave them out of scope. QA-SRL was also proved beneficial for downstream processing. It was shown to subsume open information extraction (OIE) (Stanovsky and Dagan, 2016), which enabled constructing a large supervised OIE dataset (Stanovsky et al., 2018) to serve as an intermediate structure for end applications. Additionally, QA-SRL — as well as related QA-based semantic annotations (Michael et al., 2018) — were recently shown to improve downstream tasks by providing additional semantic signal through indirect supervision for modern pretrained-LM encoders (He et al., 2020). Overall, QA-SRL is shown to subsume traditional predicate-argument information (He et al., 2015; Roit et al., 2020) — which in turn has exhibited downstream utility for various tasks, such as machine comprehension (Wang et al., 2015), crossdocument coreference (Barhom et"
2020.coling-main.274,D18-1548,0,0.0201207,"notating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research attention (Hajic et al., 2009; Jiang and Ng, 2006; Zhao and Titov, 2020). As a significant milestone, the NomBank initiative (Meyers et al., 2004a) provided extensive predicateargument annotations for various types of nouns. In particular, since deverbal nominalizations share an underlying argument structure with their verbal counterparts, NomBank annotates these by applying the same annotation scheme as PropBank. A verb-derived noun will be mapped to a frame file"
2020.coling-main.274,W11-2822,0,0.0544657,"Missing"
2020.coling-main.274,P15-2115,0,0.140863,"and compare it to expertly annotated nominal-SRL annotations, as well as to other QA-driven annotations. In addition, we train a baseline QANom parser for identifying nominalizations and labeling their arguments with question-answer pairs. Finally, we demonstrate the extrinsic utility of our annotations for downstream tasks using both indirect supervision and zero-shot settings. 1 Introduction Semantic Role Labeling (SRL) is the prominent representation for annotating predicate-argument structures. SRL annotations were shown useful for various downstream tasks, such as machine comprehension (Wang et al., 2015), cross-document coreference (Barhom et al., 2019), dialog (Chen et al., 2013) and summarization (Trandab˘a¸t, 2011). Traditionally, SRL research is biased toward focusing on verbal predicates, as evident by their dominance in large scale semantic resources, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998) and OntoNotes (Pradhan et al., 2013), and consequentially among SRL models (He et al., 2018; Tan et al., 2018; Strubell et al., 2018). Nevertheless, other types of predicates, such as nominalizations, are frequent in natural language, which also draw some research"
2020.emnlp-demos.27,P19-1409,1,0.82313,"ilot Study To further assess C O R EFI’s effectiveness in a crowdsourcing environment, we performed a smallscale trial on Amazon Mechanical Turk, employing 5 annotators, focusing on the coreference annotation functionality (rather than mention validation). To allow objective assessment of annotation quality, we experimented with replicating coreference annotations from the ECB+ dataset (Cybulska and Vossen, 2014), the commonly used dataset for cross-document coreference over English news articles (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Accordingly, we considered the ECB+ gold mentions as input, requesting crowdworkers to assign them to coreference clusters. Focusing on the controlled crowdsourcing setting, we hired five annotators that were previously selected for annotation by Roit et al. (2020). adapted to the ECB+ guidelines and applied to a part of an ECB+ subtopic (cluster of documents). These two tasks took altogether 11 minutes on average to complete, at a rate of $1.5 a task. Next, workers were asked to annotate an entire ECB+ subtopic (through the actual annotation task). We provided them man"
2020.emnlp-demos.27,L16-1323,0,0.155303,"ter-based approach since we aim at exhaustive coreference annotation across documents, whose complexity would become too high under the pairwise approach. At the same time, we simplify the annotation process and functionality, making it crowdsourceable. Our second design choice regards detecting referring mentions in text. As elaborated in Section 4, coreference annotation tools, particularly cluster-based (e.g. (Reiter, 2018; Oberle, 2018)), often require annotators to first detect the target mentions before annotating them for coreference. Conversely, recent local pair-based decision tools (Chamberlain et al., 2016; Li et al., 2020) delegate mention extraction to a preprocessing phase, presenting coreference annotators with pre-determined mentions. This simplifies the task and allows annotators to focus their attention on the coreference decisions. As we target exhaustive crowdsourced coreference annotation, we chose to follow this recent facilitating approach. In addition to the input texts, C O R EFI takes as input an annotation of the targeted mentions, while optionally allowing annotators to fix this mention annotation. In our tool suite, we followed the approach of Prodigy,1 where corpus developers"
2020.emnlp-demos.27,D17-1226,0,0.356058,"Missing"
2020.emnlp-demos.27,cybulska-vossen-2014-using,0,0.262433,"orefi Video Tour: aka.ms/corefivideo Github Repo: https://github.com/aribornstein/ corefi 1 Introduction Coreference resolution is the task of clustering textual expressions (mentions) that refer to the same concept in a described scenario. This challenging task has been mostly investigated within a single document scope, seeing great research progress in recent years. The rather under-explored crossdocument coreference setting is even more challenging. For example, consider the following sentences originating in two different documents in the standard cross-document coreference dataset ECB+ (Cybulska and Vossen, 2014): 1. A man suspected of shooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient"
2020.emnlp-demos.27,N18-2007,0,0.0189413,"two different documents in the standard cross-document coreference dataset ECB+ (Cybulska and Vossen, 2014): 1. A man suspected of shooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, incurring high costs and limiting annotation scale. Aiming to address the"
2020.emnlp-demos.27,I17-1081,0,0.0155134,"allenging. For example, consider the following sentences originating in two different documents in the standard cross-document coreference dataset ECB+ (Cybulska and Vossen, 2014): 1. A man suspected of shooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, i"
2020.emnlp-demos.27,girardi-etal-2014-cromer,0,0.155389,"om the input. To simplify annotation, the tool allows only non-overlapping spans. The annotator then makes a coreference decision, by assigning the current mention to a new or existing cluster. An existing cluster can be rapidly selected either by selecting it in the cluster bank or by selecting one of its previously-assigned mentions in the text. Once a cluster is selected, it is highlighted in blue along with all its previous text mentions ((3) and (1) in the figure). Rather than assigning mentions to clusters through a slower drag and drop interface (Reiter, 2018; Oberle, 2018) or buttons (Girardi et al., 2014; Aralikatte and Søgaard, 2020), annotation is driven primarily by faster keyboard operations, such as S PACE (assign to an existing cluster) and C TRL +S PACE (new cluster), with quick navigation through arrow keys and mouse clicks. At any point, the annotator can re-assign a previously assigned mention to another cluster or view any cluster mentions. C O R EFI supports an unlimited number of documents to be annotated, presented sequentially in a configurable order. Finally, C O R EFI guarantees exhaustive annotation by allowing task submission only once all candidate mentions are processed."
2020.emnlp-demos.27,N13-1132,0,0.0342948,"y first decide to modify its span. Next, the reviewer has to decide on cluster assignment for the current mention. The only difference at this point is that the reviewer is presented with candidate cluster assignments which reflect the original annotator assignment (as explained below), displayed just above the cluster bank (Figure 3). Reviewing To promote annotation quality, annotation projects typically rely on multiple annotations per item. One approach for doing that involves collecting such annotations in parallel and then merging them in some way, such as simple or sophisticated voting (Hovy et al., 2013). Another approach is sequential, where one or more annotations are collected initially, and are then manually consolidated by 207 In fact, it is not trivial to reflect the cluster assignment by the original annotator to the reviewer, since that assignment has to be mapped to the current clustering configuration of the reviewer. Ambiguity may arise, resulting in multiple candidate clusters, since an early cluster modification by the reviewer can impact the interpretation of downstream cluster assignments in the original annotation. To illustrate this issue, consider reviewing a cluster assigne"
2020.emnlp-demos.27,S18-2001,0,0.0137869,"TF-8 encoded language. 3 Pilot Study To further assess C O R EFI’s effectiveness in a crowdsourcing environment, we performed a smallscale trial on Amazon Mechanical Turk, employing 5 annotators, focusing on the coreference annotation functionality (rather than mention validation). To allow objective assessment of annotation quality, we experimented with replicating coreference annotations from the ECB+ dataset (Cybulska and Vossen, 2014), the commonly used dataset for cross-document coreference over English news articles (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019; Cattan et al., 2020). Accordingly, we considered the ECB+ gold mentions as input, requesting crowdworkers to assign them to coreference clusters. Focusing on the controlled crowdsourcing setting, we hired five annotators that were previously selected for annotation by Roit et al. (2020). adapted to the ECB+ guidelines and applied to a part of an ECB+ subtopic (cluster of documents). These two tasks took altogether 11 minutes on average to complete, at a rate of $1.5 a task. Next, workers were asked to annotate an entire ECB+ subtopic (through the actual annotation task)."
2020.emnlp-demos.27,E14-2024,0,0.0601199,"Missing"
2020.emnlp-demos.27,landragin-etal-2012-analec,0,0.0331561,"coreference as a pairwise annotation decision, and cluster-based, in which mentions are assigned to clusters. While targeting simplicity, only two pair-based tools supported crowdsourcing annotation, yet they were not applied for producing exhaustively annotated daasets: Phrase Detective (Chamberlain et al., 2016), which was employed in a web-based game setting, and (Li et al., 2020), which was applied in an active learning environment. Pair-based tools differ in their annotation approaches. In certain tools, such as BRAT (Stenetorp et al., 2012), Glozz (Widl¨ocher and Mathet, 2012), Analec (Landragin et al., 2012), and MMAX2 (Kope´c, 2014), the annotator first determines mention span boundaries and then links a pair of mentions. Other Pair-based tools (Chamberlain et al., 2016; Li et al., 2020) either provide annotators a single (pre-determined) mention, asking to find a coreferring antecedent, or provide a pair of mentions, asking to judge whether the two corefer. Notably, pair-based tools are less effective for exhaustive coreference annotation, for two reasons. First, they require comparing each mention to all other mentions, rather than to already constructed clusters. Second, local pairwise decisi"
2020.emnlp-demos.27,2020.acl-main.738,0,0.10796,"e aim at exhaustive coreference annotation across documents, whose complexity would become too high under the pairwise approach. At the same time, we simplify the annotation process and functionality, making it crowdsourceable. Our second design choice regards detecting referring mentions in text. As elaborated in Section 4, coreference annotation tools, particularly cluster-based (e.g. (Reiter, 2018; Oberle, 2018)), often require annotators to first detect the target mentions before annotating them for coreference. Conversely, recent local pair-based decision tools (Chamberlain et al., 2016; Li et al., 2020) delegate mention extraction to a preprocessing phase, presenting coreference annotators with pre-determined mentions. This simplifies the task and allows annotators to focus their attention on the coreference decisions. As we target exhaustive crowdsourced coreference annotation, we chose to follow this recent facilitating approach. In addition to the input texts, C O R EFI takes as input an annotation of the targeted mentions, while optionally allowing annotators to fix this mention annotation. In our tool suite, we followed the approach of Prodigy,1 where corpus developers may implement the"
2020.emnlp-demos.27,C18-1101,0,0.0251046,"le, consider the following sentences originating in two different documents in the standard cross-document coreference dataset ECB+ (Cybulska and Vossen, 2014): 1. A man suspected of shooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, incurring high costs"
2020.emnlp-demos.27,H05-1004,0,0.111157,"paid workers $8 to annotate two additional subtopics in full (of about 150 and 200 mentions; in ECB+ only a few sentences are annotated per document, and these were presented for annotation). Each subtopic took 27 minutes on average to annotate, corresponding to an annotation rate of ~400 mentions per hour. Table 1 presents the performance (F1) of each of the annotators, compared to the ECB+ gold annotations, averaged over the two subtopics. The results are reported using the common evaluation metrics for coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL — the average of the three metrics. Considering the decision volume and complexity, as well as the limited training (not providing guideline slides and a single practice round), we find that these results support C O R EFI’s effectiveness for crowdsourcing.3 As previously mentioned, we expect that annotation quality may be further improved in an actual dataset creation On-boarding Annotators were given C O R EFI’s walk-through tutorial and guided annotation tasks, 2 https://vuejs.org/ 209 3 There are no comparable annotator performance evaluations in the literature. Ontonotes (Prad"
2020.emnlp-demos.27,W12-4501,0,0.549436,"hooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, incurring high costs and limiting annotation scale. Aiming to address the cost and scalability issues in coreference annotation, we present C O R EFI, an embeddable web-component tool suite that supports an e"
2020.emnlp-demos.27,2020.acl-main.626,1,0.833648,", 2020. 2020 Association for Computational Linguistics annotation into the progressive construction of the reviewer’s annotation. By open sourcing C O R EFI, we hope to facilitate the creation of large-scale coreference datasets, especially for the cross-document setting, at modest cost while maintaining quality. 2 The C O R EFI Annotation Tool C O R EFI provides a suite for annotating single and cross-document coreference, designed to embed into crowdsourcing environments. Since coreference annotation is an involved and complex task, we target a controlled crowdsourcing setup, as proposed by Roit et al. (2020). This setup consists of selecting designated promising crowd workers, identified in preliminary trap-tasks, and then quickly training them for the target task and testing their performance. This yields a pool of reliable lightly trained annotators, who perform the actual annotation of the dataset. C O R EFI supports both the annotator training (onboarding) and annotation production phases, as illustrated in Figure 1. The training phase (Section 2.4) consists of two crowdsourcing tasks, first teaching the tool’s functionality and then practicing guided annotation, interactively learning basics"
2020.emnlp-demos.27,E12-2021,0,0.0683271,"Missing"
2020.emnlp-demos.27,M95-1005,0,0.780366,"r time per trained annotator. Annotation After training, we paid workers $8 to annotate two additional subtopics in full (of about 150 and 200 mentions; in ECB+ only a few sentences are annotated per document, and these were presented for annotation). Each subtopic took 27 minutes on average to annotate, corresponding to an annotation rate of ~400 mentions per hour. Table 1 presents the performance (F1) of each of the annotators, compared to the ECB+ gold annotations, averaged over the two subtopics. The results are reported using the common evaluation metrics for coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and CoNLL — the average of the three metrics. Considering the decision volume and complexity, as well as the limited training (not providing guideline slides and a single practice round), we find that these results support C O R EFI’s effectiveness for crowdsourcing.3 As previously mentioned, we expect that annotation quality may be further improved in an actual dataset creation On-boarding Annotators were given C O R EFI’s walk-through tutorial and guided annotation tasks, 2 https://vuejs.org/ 209 3 There are no comparable annotator performan"
2020.emnlp-demos.27,L18-1480,0,0.0124493,"ting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, incurring high costs and limiting annotation scale. Aiming to address the cost and scalability issues in coreference annotation, we present C O R EFI, an embeddable web-component tool suite that supports an end-to-end crowdsourcing process (Figure 1), while providing several contributions over earlier annotation tools (Section 4). C O R EFI includes an automated onboarding training phase, familiarizing annotators wit"
2020.emnlp-demos.27,D19-5813,0,0.017609,"ts in the standard cross-document coreference dataset ECB+ (Cybulska and Vossen, 2014): 1. A man suspected of shooting three people at an accounting firm where he had worked ... 2. A gunman shot three people at a suburban Detroit office building Monday morning. Recognizing that both sentences refer to the same event (“shooting”,“shot”) at the same location (“accounting firm”, “Detroit office”) can be very useful for downstream tasks, particularly across documents, such as multi-document summarization (Falke et al., 2017; Liao et al., 2018) or multihop question answering (Dhingra et al., 2018; Wang et al., 2019). High-quality annotated datasets are valuable to develop efficient models. While Ontonotes (Pradhan et al., 2012) provides a useful dataset for generic single-document coreference resolution, large-scale datasets are lacking for cross-document coreference (Cybulska and Vossen, 2014; Minard et al., 2016; Vossen et al., 2018) or for targeted domains, such as medical (Nguyen et al., 2011). Due to the complexity of the coreference task, existing datasets have been annotated mostly by linguistic experts, incurring high costs and limiting annotation scale. Aiming to address the cost and scalability"
2020.emnlp-main.224,W15-4612,0,0.178323,"d that might or might not be answered in the following discourse. Previous Discourse Parsing Efforts Most of the recent work on models for (shallow) discourse parsing focuses on specific subtasks, for example on argument identification (Knaebel et al., 2019), or discourse sense classification (Dai and Huang, 2019; Shi and Demberg, 2019; Van Ngo et al., 2019). Full (shallow) discourse parsers tend to use a pipeline approach, for example by having separate classifiers for implicit and explicit relations (Lin et al., 2014), or by building different models for intra- vs. inter-sentence relations (Biran and McKeown, 2015). We also adopt the pipeline approach for our baseline model (Section 6), which performs both relation classification and argument identification, since our QA pairs jointly represent arguments and relations. Previous Discourse Crowdsourcing Efforts There has been research on how to crowd-source discourse relation annotations. Kawahara et al. (2014) crowd-source Japanese discourse relations and simplify the task by reducing the tagset and extracting the argument spans automatically. A follow-up paper found that the data quality of these Japanese annotations was lacking compared to expert annot"
2020.emnlp-main.224,W15-2707,0,0.0445097,"Missing"
2020.emnlp-main.224,D16-1264,0,0.14151,"Missing"
2020.emnlp-main.224,D19-1586,0,0.0278389,"rmation through QAs, solicited from laymen speakers. 2805 The main difference lies in the propositions captured: we collect questions that have an answer in the sentence, targeting specific relation types. In the QUD annotations (Westera et al., 2020) any type of question can be asked that might or might not be answered in the following discourse. Previous Discourse Parsing Efforts Most of the recent work on models for (shallow) discourse parsing focuses on specific subtasks, for example on argument identification (Knaebel et al., 2019), or discourse sense classification (Dai and Huang, 2019; Shi and Demberg, 2019; Van Ngo et al., 2019). Full (shallow) discourse parsers tend to use a pipeline approach, for example by having separate classifiers for implicit and explicit relations (Lin et al., 2014), or by building different models for intra- vs. inter-sentence relations (Biran and McKeown, 2015). We also adopt the pipeline approach for our baseline model (Section 6), which performs both relation classification and argument identification, since our QA pairs jointly represent arguments and relations. Previous Discourse Crowdsourcing Efforts There has been research on how to crowd-source discourse relati"
2020.emnlp-main.284,2020.tacl-1.21,0,0.063843,"Missing"
2020.emnlp-main.284,2020.acl-main.356,1,0.615611,"at are frequently labeled as hypernyms. For example, the vast majority of training examples that include the word fruit are labeled as hypernymy (fruit is the hypernym of apple, banana, etc.). Therefore, at inference time, the classifier is likely to predict the hypernymy relation even for unrelated word-pairs that contain fruit, e.g., (fruit,chair). Another relevant line of research, which inspired our work, pertains to the integration of external lexical information to improve static word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Glavaˇs and Vuli´c, 2018b; Arora et al., 2020; Barkan et al., 2020). Most of these methods aim to modify the distributional vector space, originally learned from corpus co-occurrence data, by using additional relational constraints. To that end, these techniques rely on lexical databases, e.g., Wordnet (Miller, 1995). Notably, Arora et al. (2020) present the LEXSUB model and suggest training static word 3521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3521–3527, c November 16–20, 2020. 2020 Association for Computational Linguistics embeddings by integrating lexical-relation and distributional data, through the"
2020.emnlp-main.284,E12-1004,0,0.0272214,"tain the random relation, we add it artificially for the negative sampling purpose. Due to space limitations, we do not provide the datasets’ statistics. The reader may refer to (Wang et al., 2019) for the full details of the datasets. For co-occurrence data, we extracted cooccurring word-pairs from the English Wikipedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these words. 3.2 Evaluated Models For baselines, we considered both traditional distributional models: Concat (Baroni et al., 2012) and Diff (Weeds et al., 2014), and path-based models NPB (Shwartz et al., 2016b), LexNET (Shwartz and Dagan, 2016) (which integrates both distributional model and pure path-based data), NPB+Aug and LexNET+Aug (the base models are trained on augmented dependency paths, used to improve coverege) (Washio and Kato, 2018), and the recent state-of-the-art model SphereRE (Wang et al., 2019). Note that SphereRE performs a pre-training phase for generating initial pseudo labels, and the (unlabeled) test data is used for both this phase and the training. Our method does not require the test data and do"
2020.emnlp-main.284,W11-2501,0,0.12345,"0.859 0.864 P 0.531 0.521 0.530 0.601 0.620 0.543 0.576 0.636 EVALution R F1 0.544 0.525 0.531 0.528 0.537 0.503 0.489 0.607 0.600 0.545 0.621 0.620 0.601 0.571 0.608 0.591 0.620 0.628 Table 1: Precision, Recall and F1 results over lexical relation classification benchmarks. Best results are bolded. evaluate our model and compare it with other methods. 3.1 Benchmarks and Co-Occurrence Data In order to evaluate our model, we adopted the same experimental setup from Wang et al. (2019). The lexical relation classification datasets that were considered are K&H+N (Necs¸ulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al.) and EVALution (Santus et al., 2015). Since the EVALution benchmark does not contain the random relation, we add it artificially for the negative sampling purpose. Due to space limitations, we do not provide the datasets’ statistics. The reader may refer to (Wang et al., 2019) for the full details of the datasets. For co-occurrence data, we extracted cooccurring word-pairs from the English Wikipedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these w"
2020.emnlp-main.284,Q17-1010,0,0.404543,"natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here that while contextualized embeddings (Devlin et al., 2019; Peters et al., 2018) have replaced the use of non-contextualized embeddings ∗ Equal contribution, order d"
2020.emnlp-main.284,K18-1054,0,0.0194535,"l Abstract We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks. 1 Introduction and Related Work Recognizing lexical-semantic relations between words is beneficial for a variety of NLP tasks such as machine translation (Thompson et al., 2019), relation extraction (Shen et al., 2018), natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2"
2020.emnlp-main.284,N19-1423,0,0.0263662,"2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here that while contextualized embeddings (Devlin et al., 2019; Peters et al., 2018) have replaced the use of non-contextualized embeddings ∗ Equal contribution, order determined randomly. in many settings, static word embeddings remain the standard choice for lexical relation classification, since in this task the input word-pair is given out-of-context. Taking the word embeddings as input, a classifier is trained while considering each word’s representation in the pair. The recent SphereRE method (Wang et al., 2019), a purely distributional method that learns hyperspherical relation representation, presented state-of-the-art lexical relation classifica"
2020.emnlp-main.284,N15-1184,0,0.0304896,"rd. Notably, lexical memorization is common for prototypical hypernyms — “category” words that are frequently labeled as hypernyms. For example, the vast majority of training examples that include the word fruit are labeled as hypernymy (fruit is the hypernym of apple, banana, etc.). Therefore, at inference time, the classifier is likely to predict the hypernymy relation even for unrelated word-pairs that contain fruit, e.g., (fruit,chair). Another relevant line of research, which inspired our work, pertains to the integration of external lexical information to improve static word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Glavaˇs and Vuli´c, 2018b; Arora et al., 2020; Barkan et al., 2020). Most of these methods aim to modify the distributional vector space, originally learned from corpus co-occurrence data, by using additional relational constraints. To that end, these techniques rely on lexical databases, e.g., Wordnet (Miller, 1995). Notably, Arora et al. (2020) present the LEXSUB model and suggest training static word 3521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3521–3527, c November 16–20, 2020. 2020 Association for Computational"
2020.emnlp-main.284,C92-2082,0,0.614929,"ions between words is beneficial for a variety of NLP tasks such as machine translation (Thompson et al., 2019), relation extraction (Shen et al., 2018), natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here"
2020.emnlp-main.284,N15-1098,1,0.743953,"ttings, static word embeddings remain the standard choice for lexical relation classification, since in this task the input word-pair is given out-of-context. Taking the word embeddings as input, a classifier is trained while considering each word’s representation in the pair. The recent SphereRE method (Wang et al., 2019), a purely distributional method that learns hyperspherical relation representation, presented state-of-the-art lexical relation classification results. While presenting state-of-the-art performance, prior distributional methods suffer from the “lexical memorization” problem Levy et al. (2015). This problem arises when a test word-pair includes a rather frequent word in the training set, which is labeled by a dominant category in training. In such cases, the supervised model often ignores the second word in the input pair and resorts to the dominant training label according to the frequent word. Notably, lexical memorization is common for prototypical hypernyms — “category” words that are frequently labeled as hypernyms. For example, the vast majority of training examples that include the word fruit are labeled as hypernymy (fruit is the hypernym of apple, banana, etc.). Therefore,"
2020.emnlp-main.284,N16-1018,0,0.0483269,"Missing"
2020.emnlp-main.284,D12-1104,0,0.031315,"for a variety of NLP tasks such as machine translation (Thompson et al., 2019), relation extraction (Shen et al., 2018), natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here that while contextualized embeddings (Devl"
2020.emnlp-main.284,S15-1021,0,0.0280952,"Missing"
2020.emnlp-main.284,D14-1162,0,0.104218,"Missing"
2020.emnlp-main.284,N18-1202,0,0.0506332,"2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here that while contextualized embeddings (Devlin et al., 2019; Peters et al., 2018) have replaced the use of non-contextualized embeddings ∗ Equal contribution, order determined randomly. in many settings, static word embeddings remain the standard choice for lexical relation classification, since in this task the input word-pair is given out-of-context. Taking the word embeddings as input, a classifier is trained while considering each word’s representation in the pair. The recent SphereRE method (Wang et al., 2019), a purely distributional method that learns hyperspherical relation representation, presented state-of-the-art lexical relation classification results. While pr"
2020.emnlp-main.284,D19-1142,0,0.0138822,"llen Institute for Artificial Intelligence {barkanoren1,avi.c33}@gmail.com, dagan@cs.biu.ac.il Abstract We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words. Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation. We show that the proposed model is competitive and outperforms other baselines, across various benchmarks. 1 Introduction and Related Work Recognizing lexical-semantic relations between words is beneficial for a variety of NLP tasks such as machine translation (Thompson et al., 2019), relation extraction (Shen et al., 2018), natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikol"
2020.emnlp-main.284,N13-1008,0,0.0401774,"ks such as machine translation (Thompson et al., 2019), relation extraction (Shen et al., 2018), natural language inference (Chen et al., 2018), and question answering (Yang et al., 2017). The lexical relation classification task assigns a word-pair (pair of words) to its corresponding relation out of a finite set of relations. This set contains lexical relations, including the random relation (indicating that the words are unrelated). Two main lexical relation classification techniques are studied in the literature: Path-based methods (Hearst, 1992; Snow et al., 2005; Nakashole et al., 2012; Riedel et al., 2013) and distributional methods (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017; Glavaˇs and Vuli´c, 2018a), with some effort for integrating the two (Shwartz et al., 2016a). In this work we follow the distributional approach, which was shown to improve upon pathbased methods. This approach considers static word embeddings such as word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014),and FastText (Bojanowski et al., 2017), which produce out-of-context vector representation for each word. Note here that while contextualized embeddings (Devlin et al., 2019; Peter"
2020.emnlp-main.284,L16-1722,0,0.0277398,"Missing"
2020.emnlp-main.284,W15-4208,0,0.0186103,"VALution R F1 0.544 0.525 0.531 0.528 0.537 0.503 0.489 0.607 0.600 0.545 0.621 0.620 0.601 0.571 0.608 0.591 0.620 0.628 Table 1: Precision, Recall and F1 results over lexical relation classification benchmarks. Best results are bolded. evaluate our model and compare it with other methods. 3.1 Benchmarks and Co-Occurrence Data In order to evaluate our model, we adopted the same experimental setup from Wang et al. (2019). The lexical relation classification datasets that were considered are K&H+N (Necs¸ulescu et al., 2015), BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al.) and EVALution (Santus et al., 2015). Since the EVALution benchmark does not contain the random relation, we add it artificially for the negative sampling purpose. Due to space limitations, we do not provide the datasets’ statistics. The reader may refer to (Wang et al., 2019) for the full details of the datasets. For co-occurrence data, we extracted cooccurring word-pairs from the English Wikipedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these words. 3.2 Evaluated Models For baselines, we considered both"
2020.emnlp-main.284,P19-1169,0,0.0605919,"Missing"
2020.emnlp-main.284,N18-1102,0,0.0238944,"pedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these words. 3.2 Evaluated Models For baselines, we considered both traditional distributional models: Concat (Baroni et al., 2012) and Diff (Weeds et al., 2014), and path-based models NPB (Shwartz et al., 2016b), LexNET (Shwartz and Dagan, 2016) (which integrates both distributional model and pure path-based data), NPB+Aug and LexNET+Aug (the base models are trained on augmented dependency paths, used to improve coverege) (Washio and Kato, 2018), and the recent state-of-the-art model SphereRE (Wang et al., 2019). Note that SphereRE performs a pre-training phase for generating initial pseudo labels, and the (unlabeled) test data is used for both this phase and the training. Our method does not require the test data and does not perform and initial classification before training. We refer readers to the previous works for detailed descriptions of these baselines. Note that (Washio and Kato, 2018) reported only the F1 scores over the models that were trained using augmented dependency paths. 3.2.1 Ablation Study In order to assess the c"
2020.emnlp-main.284,C14-1212,0,0.0279425,"d it artificially for the negative sampling purpose. Due to space limitations, we do not provide the datasets’ statistics. The reader may refer to (Wang et al., 2019) for the full details of the datasets. For co-occurrence data, we extracted cooccurring word-pairs from the English Wikipedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these words. 3.2 Evaluated Models For baselines, we considered both traditional distributional models: Concat (Baroni et al., 2012) and Diff (Weeds et al., 2014), and path-based models NPB (Shwartz et al., 2016b), LexNET (Shwartz and Dagan, 2016) (which integrates both distributional model and pure path-based data), NPB+Aug and LexNET+Aug (the base models are trained on augmented dependency paths, used to improve coverege) (Washio and Kato, 2018), and the recent state-of-the-art model SphereRE (Wang et al., 2019). Note that SphereRE performs a pre-training phase for generating initial pseudo labels, and the (unlabeled) test data is used for both this phase and the training. Our method does not require the test data and does not perform and initial cla"
2020.emnlp-main.284,W16-5304,1,0.834886,"e do not provide the datasets’ statistics. The reader may refer to (Wang et al., 2019) for the full details of the datasets. For co-occurrence data, we extracted cooccurring word-pairs from the English Wikipedia corpus. We sampled co-occurrence data that correspond to the vocabulary of the relation classification dataset, by picking the sentences form the corpus that contain these words. 3.2 Evaluated Models For baselines, we considered both traditional distributional models: Concat (Baroni et al., 2012) and Diff (Weeds et al., 2014), and path-based models NPB (Shwartz et al., 2016b), LexNET (Shwartz and Dagan, 2016) (which integrates both distributional model and pure path-based data), NPB+Aug and LexNET+Aug (the base models are trained on augmented dependency paths, used to improve coverege) (Washio and Kato, 2018), and the recent state-of-the-art model SphereRE (Wang et al., 2019). Note that SphereRE performs a pre-training phase for generating initial pseudo labels, and the (unlabeled) test data is used for both this phase and the training. Our method does not require the test data and does not perform and initial classification before training. We refer readers to the previous works for detailed desc"
2020.emnlp-main.284,P16-1226,1,0.910875,"Missing"
2020.findings-emnlp.440,P19-1409,1,0.177396,"to the benefit of the other. 1 Director Chris Weitz is expected to direct∨ New Moon. Chris Weitz will take on∨ the sequel to “Twilight”. Gary Ross is still in negotiations to direct× the sequel. Table 1: Examples from ECB+ (a cross-document coreference dataset) that illustrate the context-sensitive nature of event coreference. The illustrated predicates are co-referable, and hence may be used to refer to the same event in certain contexts, but obviously not all their mentions corefer. Introduction Recognizing that mentions of different lexical predicates discuss the same event is challenging (Barhom et al., 2019). Lexical resources such as WordNet (Miller, 1995) capture such synonyms (say, tell) and hypernyms (whisper, talk), as well as antonyms, which can be used to refer to the same event when the arguments are reversed ([a]0 beat [a]1 , [a]1 lose to [a]0 ). However, WordNet’s coverage is insufficient, in particular, missing contextspecific paraphrases (e.g. (hide, launder), in the context of money). Conversely, distributional methods enjoy broader coverage, but their precision for this purpose is limited because distributionally similar terms may often be mutually-exclusive (born, die) or may refer"
2020.findings-emnlp.440,N03-1003,0,0.343931,"Missing"
2020.findings-emnlp.440,P01-1008,0,0.494554,"s rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphras"
2020.findings-emnlp.440,P10-1143,0,0.890532,"s difference with examples of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on t"
2020.findings-emnlp.440,J14-2004,0,0.306166,"Missing"
2020.findings-emnlp.440,D17-1226,0,0.794831,"Missing"
2020.findings-emnlp.440,cybulska-vossen-2014-using,0,0.822158,"es and clusters event mentions, across multiple documents, that refer to the same event within their respective contexts. The latter task, on the other hand, collects pairs of event expressions that, at the generic lexical level, may refer to the same event in certain contexts. Table 1 illustrates this difference with examples of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation acr"
2020.findings-emnlp.440,P18-1128,0,0.0169198,"aluation and the Average Precision (AP) for ranking evaluation. Our scorer dramatically improves upon the baselines in all metrics. To show that the improved scoring generalizes beyond examples that appear in the ECB+ dataset, we selected a random subset of 500 predicate pairs with at least 6 support pairs from the entire Chirps resource and annotated them in the same method described in Section 3.2. The ranker evaluated on this subset gained 8 points in AP, relative to the original Chirps ranking. All results are statistically significant using bootstrap and permutation tests with p &lt; 0.001 (Dror et al., 2018). Table 6 exemplifies highly ranked predicate pairs by our Chirps* scorer, the original Chirps scorer and the GloVe scorer, which illustrates the improved ranking performance of Chirps* (as measured in table 5 by the AP score). Ablation Test To evaluate the importance of each type of feature, we perform an ablation test. Table 7 displays the performance of various ablated models, each of which with one set of features (Section 3.1) removed from the representation. In the classification task, removing the named entity coverage features somewhat improved the performance, mostly by increasing the"
2020.findings-emnlp.440,N13-1092,0,0.141604,"Missing"
2020.findings-emnlp.440,S18-2001,0,0.469453,"aining and evaluation are ECB+ (Cybulska and Vossen, 2014), and its predecessors, EECB (Lee et al., 2012) and ECB (Bejan and Harabagiu, 2010). ECB+ contains a set of topics, each containing a set of documents describing the same global event. Both event and entity coreferences are annotated in ECB+, within and across documents. Models for CD event coreference utilize a range of features, including lexical overlap among mention pairs and semantic knowledge from WordNet (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015), distributional (Choubey and Huang, 2017) and contextual representations (Kenyon-Dean et al., 2018; Barhom et al., 2019). The current state-of-the-art model from Barhom et al. (2019) iteratively and intermittently learns to cluster events and entities. A mention representation mi consists of several components, representing both the mention span and its surrounding context. The interdependence between clustering event vs. entity mentions is encoded into the mention representation, such that an event mention representation contains a component reflecting the current entity clustering, and vice versa. Using this representation, the model trains a pairwise mention scoring function that predic"
2020.findings-emnlp.440,D17-1126,0,0.0369788,"Missing"
2020.findings-emnlp.440,D12-1045,0,0.682228,"lution aims to identify and cluster event mentions, that, within their respective contexts, refer to the same event. The task has two variants, one in which coreferring mentions are within the same document (within document) and another in which corefering mentions may be in different documents (cross-document, CD), on which we focus in this paper. 1 Code available at github.com/yehudit96/coreferrability, github.com/yehudit96/event entity coref ecb plus The standard datasets used for CD event coreference training and evaluation are ECB+ (Cybulska and Vossen, 2014), and its predecessors, EECB (Lee et al., 2012) and ECB (Bejan and Harabagiu, 2010). ECB+ contains a set of topics, each containing a set of documents describing the same global event. Both event and entity coreferences are annotated in ECB+, within and across documents. Models for CD event coreference utilize a range of features, including lexical overlap among mention pairs and semantic knowledge from WordNet (Bejan and Harabagiu, 2010, 2014; Yang et al., 2015), distributional (Choubey and Huang, 2017) and contextual representations (Kenyon-Dean et al., 2018; Barhom et al., 2019). The current state-of-the-art model from Barhom et al. (20"
2020.findings-emnlp.440,H05-1004,0,0.430778,"sentation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through M LPch into the same embedding space. The two vectors are concatenated to form the mention pair representation, which is fed to the scoring function M LPscorer . CEAF-e (Luo, 2005) and CoNLL F1 (the average of MUC, B 3 and CEAF-e scores). We compare the integrated model to the original model and to the lemma baseline which clusters together mentions that share the same mentionhead lemma. The results in Table 8 show that the Chirps-enhanced model provides an improvement of 3.5 points over the lemma baseline and a small improvement upon Barhom et al. (2019) in all F1 score measures. The greatest improvement is in the link-based MUC measure, which counts the number of corresponding links between the mentions. The Chirps component helps link more coreferring mentions (impro"
2020.findings-emnlp.440,E17-1083,0,0.15302,"air of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We sho"
2020.findings-emnlp.440,D14-1162,0,0.0873361,"s are paraphrases or not; and (2) ranking the pairs based on the predicted positive class score. We consider the ranking evaluation as more informative, as we expect the ranking to reflect the number of contexts in which a pair of predicates may be coreferring. That is, predicate pairs that are coreferring in many contexts will be ranked higher than those that are coreferring in just a few contexts. We compare our model with two baselines: the original Chirps scores, and a baseline that assigns each pair of predicates the cosine similarity scores between the predicates using GloVe embeddings (Pennington et al., 2014).3 For the classification decisions made by the two baseline scores (Chirps score and cosine similarity for Glove vectors), we learn a threshold that yields the best accuracy score over the train set, above which a pair of predicates is classified as positive. 3 We were motivated to compare with Glove since this resource is utilized as a lexical representation in the state-ofthe-art system of (Barhom et al., 2019), which is utilized in the next section. Here, multi-word predicates were represented by the average of their Glove word vectors. Table 5 displays the accuracy, precision, recall and"
2020.findings-emnlp.440,N18-1202,0,0.0177949,"vent mentions coreference-resolution results on ECB+ test set. Pairwise Score about 370 lexically-divergent pairs of coreferring event mentions appearing in the ECB+ training set, and about 200 in the test set. MLPscorer 4.1 Integration Method The state-of-the-art CD coreference resolution model, by Barhom et al. (2019), trained a pairwise mention scoring function, M LPscorer (mi , mj ), which predicts the probability that two mentions mi , mj refer to the same event. The mention representation includes a lexical component (GloVe embeddings) as well as a contextual component (ELMo embeddings, Peters et al., 2018). The mention pair representation ~vi,j , which is fed to the pairwise scorer, combines the two separate mention representations. We extended the model by changing the input to the pairwise event mention scoring function to include information regarding the mention pair from Chirps*, as illustrated in Figure 1. We defined v~0 i,j = [~vi,j ; ~ci,j ], where ~ci,j denotes the Chirps* features, computed in the following way: ~ci,j ( M LPch (f~mi ,mj ) = M LPch (~0) if mi , mj ∈ Chirps otherwise f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an"
2020.findings-emnlp.440,P14-2006,0,0.110581,"~mi ,mj ) = M LPch (~0) if mi , mj ∈ Chirps otherwise f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an entry in Chirps, otherwise the input is a zero vector. M LPch is an MLP with a single hidden layer of size 50 and output layer of size 100, which is used to transform the discrete values in f~mi ,mj into the same embedding space of ~vi,j . The rest of the model remains the same, including the model architecture, training, and inference.5 4.2 Evaluation We evaluate the event coreference performance on ECB+ using the official CoNLL scorer (Pradhan et al., 2014). The reported metrics are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), 5 We also tried to incorporate only the final Chirps* score into the mention pair representation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through"
2020.findings-emnlp.440,N13-1110,0,0.0188116,"ents discussing the same event. The underlying assumption is that such redundant texts may refer to the same entities or events using lexically-divergent mentions. Coreferring mentions are identified heuristically and 4898 extracted as candidate paraphrases. When long documents are used, the first step in this approach is to align each pair of documents by sentences. This was done by finding sentences with shared named entities (Shinyama et al., 2002) or lexical overlap (Barzilay and Lee, 2003; Shinyama and Sekine, 2006), and by aligning pairs of predicates or arguments (Zhang and Weld, 2013; Recasens et al., 2013). In more recent work, Xu et al. (2014) and Lan et al. (2017) extracted sentential paraphrases from Twitter by heuristically matching pairs of tweets discussing the same topic. Predicate Paraphrases. In contrast to sentential paraphrases, it is also beneficial to identify differing textual templates of the same meaning. In this paper we focus on binary predicate paraphrases such as (“[a0 ] quit from [a1 ]”, “[a0 ] resign from [a1 ]”). Earlier approaches for acquiring predicate paraphrases considered a pair of predicate templates as paraphrases if the distributions of their argument instantiati"
2020.findings-emnlp.440,N06-1039,0,0.0735699,"icate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improv"
2020.findings-emnlp.440,S17-1019,1,0.868975,"een argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the unsupervised Chirps resource (Shwartz et al"
2020.findings-emnlp.440,W04-3206,1,0.646309,"aphrases such as (“[a0 ] quit from [a1 ]”, “[a0 ] resign from [a1 ]”). Earlier approaches for acquiring predicate paraphrases considered a pair of predicate templates as paraphrases if the distributions of their argument instantiations were similar. For instance, in “[a0 ] quit from [a1 ]”, [a0 ] would typically be instantiated by people names while [a1 ] by employer organizations or job titles. A paraphrastic template like “[a0 ] resign from [a1 ]” is hence expected to have similar argument distributions, and can thus be detected by a distributional similarity approach (Lin and Pantel, 2001; Szpektor et al., 2004; Berant, 2012). Yet, as mentioned earlier, predicates with similar argument distributions are not necessarily paraphrastic, which introduces a substantial level of noise when acquiring paraphrase pairs using this approach. In this paper, we follow the potentially more reliable paraphrase acquisition approach, which tries to heuristically identify concrete co-referring predicate mentions. Identifying such mention pairs, detected as actually being used to refer to the same event, can provide a strong signal for identifying these predicates as paraphrastic (vs. the quite noisy corpus-level signa"
2020.findings-emnlp.440,M95-1005,0,0.909677,"f~mi ,mj ∈ R17 is the feature vector representing a pair of predicates (mi , mj ) for which there is an entry in Chirps, otherwise the input is a zero vector. M LPch is an MLP with a single hidden layer of size 50 and output layer of size 100, which is used to transform the discrete values in f~mi ,mj into the same embedding space of ~vi,j . The rest of the model remains the same, including the model architecture, training, and inference.5 4.2 Evaluation We evaluate the event coreference performance on ECB+ using the official CoNLL scorer (Pradhan et al., 2014). The reported metrics are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), 5 We also tried to incorporate only the final Chirps* score into the mention pair representation, but the performance improvement was smaller. Mention Pair Representation Chirps* Features MLPch Barhom et al. Representation Generator Raw Chirps* Features Chirps* Feature Extractor mi, mj Mention Pair Figure 1: An illustration of the integrated mention pair scorer. The right vector is the original mention pair vector from Barhom et al. (2019), and the left one is our Chirps* extension, which is transformed through M LPch into the same embedding space. The two vecto"
2020.findings-emnlp.440,Q14-1034,0,0.181576,"d similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the unsupervised Chirps"
2020.findings-emnlp.440,Q15-1037,0,0.862586,"of co-referable predicate paraphrases, while their mentions obviously do not always co-refer. Cross-document event coreference resolution systems are typically supervised, usually trained on the ECB+ dataset, which contains clusters of news articles on different topics (Cybulska and Vossen, 2014). Recent systems rely on neural representations of the mentions and their contexts (KenyonDean et al., 2018; Barhom et al., 2019), while earlier approaches leveraged WordNet and other lexical resources to obtain a signal of whether a pair of mentions may be coreferring (e.g. Bejan and Harabagiu, 2010; Yang et al., 2015). Approaches for acquiring predicate paraphrase, in the form of a pair of paraphrastic predicates or predicate templates, were based mostly on unsupervised signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which"
2020.findings-emnlp.440,D13-1183,0,0.160022,"signals. These included similarity between argument distributions (Lin and Pantel, 2001; Berant, 2012), backtranslation across languages (Barzilay and McKeown, 2001; Ganitkevitch et al., 2013; 4897 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4897–4907 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Mallinson et al., 2017), or leveraging redundant news reports on the same event, which are hence likely to refer to the same events and entities using different words (Shinyama et al., 2002; Shinyama and Sekine, 2006; Barzilay and Lee, 2003; Zhang and Weld, 2013; Xu et al., 2014; Shwartz et al., 2017). In some cases, the paraphrase collection phase includes a step of validating a subset of the paraphrases and training a model on these gold paraphrases to re-rank the entire resource (Lan et al., 2017). In this paper, we study the potential synergy between predicate paraphrases and event coreference resolution. We show that the data and models for one task can benefit the other. In one direction (Section 3), we use event coreference annotations from the ECB+ dataset as distant supervision to learn an improved scoring of predicate paraphrases in the uns"
2021.conll-1.25,D19-5408,0,0.019312,"wo aligners becomes. As the average summary length of CNN/Daily Mail is 3.8 sentences, the advantage of the SuperPALsent in those lengths stresses its benefit over ROUGE. Moreover, the SuperPALsent data achieved the highest global result across all summary lengths. 7 Conclusion and Discussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our releas"
2021.conll-1.25,P18-1063,1,0.930179,"derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE out1 put, which satisfies these requirements, is best suitAll corresponding datasets and code are publicly available at https://github.com/oriern/SuperPAL. able for extracting such units (while EDU format 311 Summary Sentence The BBC reports 56-year-old Allan Matthews pleaded guilty Wednesday to removing another man’s left hand at an Australian motel despite not being qualified to practice medicine. The lawsuit, which also alleges a hostile work env"
2021.conll-1.25,P19-1098,0,0.0191218,"the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor evaluated explicitly, making matically from available summarization evaluthem difficult to compare a"
2021.conll-1.25,2020.emnlp-main.509,0,0.0240252,"As the average summary length of CNN/Daily Mail is 3.8 sentences, the advantage of the SuperPALsent in those lengths stresses its benefit over ROUGE. Moreover, the SuperPALsent data achieved the highest global result across all summary lengths. 7 Conclusion and Discussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our released resources mon se"
2021.conll-1.25,P19-1102,0,0.343819,"position spans (§3), extracted automatically using Open IE (§5). 3 Dev and Test Alignment Datasets This section presents the manually-annotated development and test datasets for reference-source alignments, including their structure (§3.1), source data (§3.2) and annotation process (§3.3). These datasets allow direct tuning and evaluation of alignment algorithms, lacking in prior work (§1). 3.1 Dataset Structure In the typical MDS setting, a summarization instance consists of a set of topically-related documents, often termed a topic, and corresponding gold reference summary(ies) (NIST, 2014; Fabbri et al., 2019). For such an instance, we collect all alignments between each proposition span in the reference summary and the corresponding propositions, conveying the same information, in the source documents. We choose the proposition-span level, termed information unit (IU), as the basis for alignment following the rationale of similar SCU-level alignments in the well-established Pyramid evaluation method (§2). To facilitate crowdsourcing, we adapt a somewhat looser definition for our IUs (Task 1 below). Figure 1 illustrates some IU spans and their alignments. 3.2 Source Data 3.3 Annotation Our annotati"
2021.conll-1.25,D18-1443,0,0.027366,"documents. with their counterparts in source documents Such alignments were generated automatically over was shown as a useful auxiliary summarizalarge summarization datasets, most typically at the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate"
2021.conll-1.25,D18-1450,0,0.0187465,"was favored over the more coarse sentence level, since a sys2 Background and Related Work tem summary sentence may include some propoAs mentioned above, several methods leveraged sitions that match the reference and some that automatically generated reference-source sentence don’t. Later works attempted to automate the Pyraalignments, to derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE out1 put, which satisfies these requirements, is best suitAll corresponding datasets and code are publicly ava"
2021.conll-1.25,2020.aacl-main.52,0,0.0181809,"scussion CNN/DailyMail (Fabbri et al., 2019)), albeit somewhat less coherent. An example of such potential summary, illustrated by an oracle-system summary derived from our supervised aligner predictions on CNN/DailyMail, is shown in Table 9. Alternatively, our data can contribute to the recent highlighting task (Arumae et al., 2019; Cho et al., 2020), where salient information fragments are marked inside a document, thus circumventing the need to generate coherent text. Further, propositions may be fused together to generate a coherent abstractive summary. Recently, such a cascaded approach (Lebanoff et al., 2020), consisting of text fragment selection followed by a generation step, exhibited comparable or improved results over end-to-end systems. We advocate the potential of summary-source proposition-level alignment to extract cleaner and more accurate alignments than through the comOverall, we suggest that our released resources mon sentence-level approach. To that end, we establish a new proposition-level alignment task by would encourage appealing future research on proposition-based summarization approaches, as releasing high-quality dev and test datasets, and an well as on developing improved al"
2021.conll-1.25,P19-1209,0,0.267409,"matically over was shown as a useful auxiliary summarizalarge summarization datasets, most typically at the tion task, notably for generating training data full sentence level. Then, (noisy) training data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor e"
2021.conll-1.25,W04-1013,0,0.0517351,"ners, which in turn may trigger appealing research on proposition-based summarization methods. alignments to create a sentence fusion dataset: the input for each fusion instance consists of a pair of source sentences that are aligned to the same summary sentence, while the aligned summary sentence is regarded as the output fused sentence. The underlying sentence alignments, from which the above training datasets were derived, were extracted automatically from large summarization datasets. Alignments were detected using unsupervised sentence similarity measures, typically based on ROUGE score (Lin, 2004) (see §6 for more details). Typically, models trained over the alignmentbased datasets were evaluated only on the final summary. Yet, alignment quality, which determines the quality of the utilized training datasets, was never optimized or assessed explicitly, as we do in this paper. Notably, alignment of matching pieces of information provided the basis for the prominent Pyramid method for summarization evaluation (Nenkova and Passonneau, 2004), capturing information overlap between system summaries and reference summaries. Alignments were performed at the level of individual propositions, te"
2021.conll-1.25,2021.ccl-1.108,0,0.059214,"Missing"
2021.conll-1.25,N04-1019,0,0.748837,"nce Norodom Ranariddh and Sam Rainsy are out of the country following threats of arrest from strongman Hun Sen. Figure 1: Aligning IUs between a summary sentence (top) and sentences from documents (bottom). For fuller example see Appendix B. proposition-level alignment, we first developed an elaborate crowdsourcing methodology and created high-quality development and test datasets (§3). Next, we automatically derive a larger-scale training dataset consisting of 23K alignment instances from available Multi-Document Summarization (MDS) evaluation data, available as reliable Pyrmaid annotations (Nenkova and Passonneau, 2004) (§4). This data is utilized to train a supervised alignment baseline model (§5), which outperforms traditional unsupervised alignment approaches.1 Moreover, thanks to this novel training dataset, we show (§5.4) that our baseline aligner is capable of producing “abstractive” alignments, where there is almost no lexical overlap, while traditional aligners fail to do so. We further show intrinsically that our proposition-level aligner extracts better salient sentences than common sentence-level aligners. Notably, while our datasets are derived from MDS sources, the data and model are applicable"
2021.conll-1.25,P17-1100,0,0.0236324,"Content Units (SCUs) (similar to the information units marked in Fig. 1). Matching information at the proposition level was favored over the more coarse sentence level, since a sys2 Background and Related Work tem summary sentence may include some propoAs mentioned above, several methods leveraged sitions that match the reference and some that automatically generated reference-source sentence don’t. Later works attempted to automate the Pyraalignments, to derive (noisy) training sets for sum- mid procedure, using Open IE (Yang et al., 2016; marization subtasks (Zhang et al., 2018; Cho et al., Peyrard and Eckle-Kohler, 2017) or Elementary 2019). For example, training datasets for sentence Discourse Units (EDU) (Hirao et al., 2018), to exsalience detection were derived from reference- tract proposition-level units. As propositional units source alignments, by marking as salient those (like SCUs) may share their arguments (such as source sentences that were aligned with a sum- in conjunctions and other constructions, e.g. “The mary sentence (e.g. Chen and Bansal (2018)). As boy went home and ate dinner”), and may be disanother example, Lebanoff et al. (2019) leveraged contiguous (“The boy...ate dinner”), Open IE ou"
2021.conll-1.25,2020.acl-main.626,1,0.883925,"Missing"
2021.conll-1.25,N18-1081,1,0.823045,"Missing"
2021.conll-1.25,Q14-1018,0,0.0289173,"tage apa reference summary sentence is aligned with one proach, where we first align a summary IU with or two source sentences which are most similar the three source sentences most similar to it. The to it. We adjust this approach to the IU level, similarity score at this stage was a tuned combidenoted ROUGEIU . Accordingly, each summary nation of ROUGE, RoBERTa-MNLI (Liu et al., IU is matched with the k document IUs of highest 2019) and BERTscore (Zhang et al., 2019). Then, ROUGE similarity, choosing k = 2, which worked to find aligned spans, we applied a word aligner best on the dev set.4 (Sultan et al., 2014) to align words between a document sentence and the summary IU, and aligned 4 Our ROUGE similarity is an average of recall R-1, R-2, the consecutive text spans between the first and last and R-L, where the summary-IU is considered the reference. aligned words on each side (filtering pairs with too We also experimented with setting a threshold over the similarity score, but the common top-k approach worked best. few word alignments). 315 5.1 ROUGE-based Lexical Model 5.3 Supervised Model This model is a binary classifier, deciding whether two given IUs align. We follow the standard usage of RoB"
2021.conll-1.25,D18-1088,0,0.120249,"data for for salience detection. Despite its assessed utilcertain summarization subtasks was automatically ity, the alignment step was mostly approached with heuristic unsupervised methods, typically derived from these alignments (§2), notably for ROUGE-based, and was never independently salience detection (Gehrmann et al., 2018; Chen optimized or evaluated. In this paper, we proand Bansal, 2018; Lebanoff et al., 2019), but also pose establishing summary-source alignment for redundancy recognition (Cho et al., 2019) and as an explicit task, while introducing two matext rephrasing and fusion (Zhang et al., 2018; jor novelties: (1) applying it at the more accuLebanoff et al., 2019). Even though the quality of rate proposition span level, and (2) approachthe subsequent trained models relies on alignment ing it as a supervised classification task. To that end, we created a novel training dataset quality, the intermediate alignment methods were for proposition-level alignment, derived autoneither optimized nor evaluated explicitly, making matically from available summarization evaluthem difficult to compare and improve. ation data. In addition, we crowdsourced dev In this paper, we establish summary-sou"
2021.eacl-main.21,P98-1012,0,0.862855,"annotation for CD2 CR including a novel sampling mechanism for calculating inter-annotator agreement (Section 3.4). 2 Co-reference Resolution Intra-document co-reference resolution is a well understood task with mature training data sets (Weischedel et al., 2013) and academic tasks (Recasens et al., 2010). The current state of the art model by Joshi et al. (2020) is based on Lee et al. (2017, 2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus (Cybulska and Vossen, 2014) which contains news articles annotated with both entity and event mentions. • A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between"
2021.eacl-main.21,P19-1409,1,0.946256,"on (CDCR) is the task of recognising when multiple documents mention and refer to the same real-world entity or concept. CDCR is a useful NLP process that has many downstream applications. For example, CDCR carried out on separate news articles that refer to the same politician can facilitate inter-document sentence alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabu"
2021.eacl-main.21,D19-1371,0,0.0199462,"nilla (CA-V) Baseline Here we aim to evaluate whether training the CA model on the CD2 CR dataset from the RoBERTa baseline without first training on the ECB+ corpus allows it to fit well to the new task setting. We re-initialise the CA encoder (Section 4.2) using weights from RoBERTa (Liu et al., 2019) and randomly initialise the remaining model parameters. We then train the model on the CD2 CR corpus for up to 20 epochs with early stopping with pseudorandom sub-sampling as above. 4.5 CA - SciBERT (CA-S) Baseline This model is the same as CA-V but we replace the RoBERTa encoder with SciBERT (Beltagy et al., 2019), a version of BERT pre-trained on scientific literature in order to test whether the scientific terms and context captured by SciBERT improve performance at the CD2 CR task compared to RoBERTa. Similarly to CA-V in section 4.4, we initialise the BERT model with weights from SciBERTscivocab-uncased (Beltagy et al., 2019) and randomly initialise the remaining model parameters, training on the CD2 CR corpus for up to 20 epochs with early stopping. 5 the “gold standard” in all experiments rather than using the end-to-end Named Entity Recognition capabilities provided by some of the models. We eva"
2021.eacl-main.21,cybulska-vossen-2014-using,0,0.13871,"2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus (Cybulska and Vossen, 2014) which contains news articles annotated with both entity and event mentions. • A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between different domains and document types (we call this CD2 CR). 1 Related Work 2.2 Entity Linking (EL) focuses on alignment of mentions in documents to resources in an external knowledge resource (Ji et al., 2010) such as SNOMED CT3 or DBPedia4 . EL is challenging due to the large number of pairwise comparisons between document mentions and knowledge resource entities that may need to be carried out."
2021.eacl-main.21,N19-1423,0,0.46707,"alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabulary and structure) is useful. One such example is the task of resolving references to concepts across scientific papers and related news articles. This can help scientists understand how their work is being presented to the public by mainstream media or facilitate fact checking of journalists’ work (Wadden et al., 2"
2021.eacl-main.21,Q15-1002,0,0.149971,"nt co-reference resolution (CDCR) is the task of recognising when multiple documents mention and refer to the same real-world entity or concept. CDCR is a useful NLP process that has many downstream applications. For example, CDCR carried out on separate news articles that refer to the same politician can facilitate inter-document sentence alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significa"
2021.eacl-main.21,D19-1620,0,0.0196852,"he scientific paper (abstract). For each document pair, we ask the annotators to identify co-referent mentions between the scientific paper abstract and a summary of the news article that is of similar length (e.g. 5-10 sentences). Scientific paper abstracts act as a natural summary of a scientific work and have been used as a strong baseline or even a gold-standard in scientific summarisation tasks (Liakata et al., 2013). Furthermore, abstracts are almost always available rather than behind paywalls like full text articles. For news summarisation, we used a state-of-the-art extractive model (Grenander et al., 2019) to extract sentences forming a summary of the original text. This model provides a summary de-biasing mechanism preventing it from focusing on specific parts of the full article, preserving the summary’s informational authenticity as much as possible. The difference in style between the two documents is preserved by both types of summary since abstracts are written in the same scientific style as full papers and the extractive summaries use verbatim excerpts of the original news articles. 3.3 Generation of pairs for annotation To populate our annotation tool, we generate pairs of candidate cr"
2021.eacl-main.21,2020.coling-main.118,0,0.0405676,"Missing"
2021.eacl-main.21,D17-1018,0,0.304597,"source English language CD2 CR dataset with 7602 co-reference pair annotations over 528 documents and detailed 11 page annotation guidelines (section 3.1). • A novel annotation tool to support ongoing data collection and annotation for CD2 CR including a novel sampling mechanism for calculating inter-annotator agreement (Section 3.4). 2 Co-reference Resolution Intra-document co-reference resolution is a well understood task with mature training data sets (Weischedel et al., 2013) and academic tasks (Recasens et al., 2010). The current state of the art model by Joshi et al. (2020) is based on Lee et al. (2017, 2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus"
2021.eacl-main.21,N18-2108,0,0.132247,"Missing"
2021.eacl-main.21,2020.acl-main.738,0,0.0172838,"ere shown in bold font whereas mentions already flagged as co-referent were shown in green. This enabled annotators to understand the implications for existing co-reference chains before responding (see Figure 3). Questions were generated and ranked via our task generation pipeline (see Section 3.3 above). We added two additional features to our annotation interface to improve annotators’ experience and to speed up the annotation process. Firstly, if the candidate pair is marked as co-referent, the user is allowed to add more mentions to the coreference cluster at once. Secondly, inspired by (Li et al., 2020), if the automatically shown mention pair is not co-referent, the user can select a different mention that is co-referent. The upstream automated mention detection mechanism can sometimes introduce incomplete or erroneous mentions, leading to comparisons that don’t make sense or that are particularly difficult. Therefore, annotators can also move or resize the mention spans they are annotating. We use string offsets of mention span pairs to tokens to check that they do not overlap with each other in order to prevent the creation of duplicates. Figure 1 shows an illustrated example of the gener"
2021.eacl-main.21,D13-1070,1,0.735366,"a very low chance for good inter-annotator agreement (IAA). We therefore decided to simplify the task by asking annotators to compare summaries of the newspaper article (5-10 sentences long) and the scientific paper (abstract). For each document pair, we ask the annotators to identify co-referent mentions between the scientific paper abstract and a summary of the news article that is of similar length (e.g. 5-10 sentences). Scientific paper abstracts act as a natural summary of a scientific work and have been used as a strong baseline or even a gold-standard in scientific summarisation tasks (Liakata et al., 2013). Furthermore, abstracts are almost always available rather than behind paywalls like full text articles. For news summarisation, we used a state-of-the-art extractive model (Grenander et al., 2019) to extract sentences forming a summary of the original text. This model provides a summary de-biasing mechanism preventing it from focusing on specific parts of the full article, preserving the summary’s informational authenticity as much as possible. The difference in style between the two documents is preserved by both types of summary since abstracts are written in the same scientific style as f"
2021.eacl-main.21,Q15-1023,0,0.0317577,"• A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between different domains and document types (we call this CD2 CR). 1 Related Work 2.2 Entity Linking (EL) focuses on alignment of mentions in documents to resources in an external knowledge resource (Ji et al., 2010) such as SNOMED CT3 or DBPedia4 . EL is challenging due to the large number of pairwise comparisons between document mentions and knowledge resource entities that may need to be carried out. Raiman and Raiman (2018) provide state of the art performance by building on Ling et al. (2015)’s work in which an entity type system is used to limit the number of required pairwise comparisons to related types. Yin et al. (2019) achieved comparable results using a graph-traversal method to similarly constrain the problem space to candidates within a similar graph neighbourhood. EL can be considered a narrow sub-task of CDCR since it cannot resolve novel and rare entities or pronouns (Shen et al., 2015). Moreover EL’s dependency on expensiveto-maintain external knowledge graphs is also problematic when limited human expertise is available. 3 DOI: 10.1101/2020.03.16.20036145 https://tin"
2021.eacl-main.21,N18-1202,0,0.296749,"detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabulary and structure) is useful. One such example is the task of resolving references to concepts across scientific papers and related news articles. This can help scientists understand how their work is being presented to the public by mainstream media or facilitate fact checking of journalists’ work (Wadden et al., 2020). A chatbot or recommender"
2021.eacl-main.21,D18-1026,0,0.0221577,"Missing"
2021.eacl-main.21,Q15-1025,0,0.0157882,"rpora to learn context-aware word embeddings that can be used for downstream NLP tasks. However, these models do not learn about formal lexical constraints, often conflating different types of semantic relatedness (Ponti et al., 2018; Lauscher et al., 2020). This is a weakness of all distributional language models that is particularly problematic in the context of CD2 CR for entity mentions that are related but not co-referent (e.g. “Mars” and “Jupiter”) as shown in section 5. A number of solutions have been proposed for adding lexical knowledge to static word embeddings (Yu and Dredze, 2014; Wieting et al., 2015; Ponti et al., 2018) but contextual language models have received comparatively less attention. Lauscher et al (2020) propose adding a lexical relation classification step to BERT’s language model pre-training phase to allow the model to integrate both lexical and distributional knowledge. Their model, LIBERT, has been shown to facilitate statistically-significant performance boosts on a variety of downstream NLP tasks. 3 Dataset creation Our dataset is composed of pairs of news articles and scientific papers gathered automatically (Section 3.1). Our annotation process begins by obtaining sum"
2021.eacl-main.21,P18-4004,1,0.830395,"or scoring (Section 3.4). Annotation quality is measured on an ongoing basis as new candidates are added to the system (Section 3.5). 3.1 Documents 300 142 86 Mentions 4,604 1,821 1,177 Clusters 426 199 101 Table 1: Total individual documents, mentions, coreference clusters of each subset excluding singletons. and is split into training, development and test sets (statistics for each subset are provided in Table 1). Each pair of documents consists of a scientific paper and a newspaper article that discusses the scientific work. In order to detect pairs of documents, we follow the approach of (Ravenscroft et al., 2018), using approximate matching of author name and affiliation metadata, date of publishing and exact DOI matching where available to connect news articles to scientific publications. We built a web scraper that scans for new articles from the ‘Science’ and ‘Technology’ sections of 3 well-known online news outlets (BBC, The Guardian, New York Times) and press releases from Eurekalert, a widely popular scientific press release aggregator. Once a newspaper article and related scientific paper are detected, the full text from the news article and the scientific paper abstract and metadata are stored"
2021.eacl-main.21,W09-2411,0,0.112286,"Missing"
2021.eacl-main.21,2020.acl-main.442,0,0.0243761,"erent’ respectively B3 F1 P R F1 0.84 0.63 0.68 0.65 0.81 0.56 0.53 0.55 suggests that disentangling these pairs is likely to be a challenging task for the downstream classification layer in the CA-V model. These challenges are less likely to occur in homogeneous corpora like ECB+ where descriptions and relationships remain consistent in detail and complexity. Table 5: MUC and B 3 results from running the CD2 CR baseline model (CA-V) on ECB+ dataset compared with original Cattan et al. (2020) (CA). Finally, the best model (CA-V) is analysed using a series of challenging test cases inspired by Ribeiro et al (2020). These test cases were created using 210 manually annotated mention-pairs found in the test subset of the CD2 CR corpus according to the type of relationship illustrated (Anaphora & Exophora, Subset relationships, paraphrases). We collected a balanced set of 30-40 examples of both co-referent and non-coreferentbut-challenging pairs for each type of relationship (exact numbers in Table 4). We then recorded whether the model correctly predicted co-reference for these pairs. The results along with illustrative examples of each relationship type are shown in Table 4. The results suggest that the"
2021.eacl-main.21,M95-1005,0,0.536732,"terature in order to test whether the scientific terms and context captured by SciBERT improve performance at the CD2 CR task compared to RoBERTa. Similarly to CA-V in section 4.4, we initialise the BERT model with weights from SciBERTscivocab-uncased (Beltagy et al., 2019) and randomly initialise the remaining model parameters, training on the CD2 CR corpus for up to 20 epochs with early stopping. 5 the “gold standard” in all experiments rather than using the end-to-end Named Entity Recognition capabilities provided by some of the models. We evaluate the models using the metrics described by Vilain et al. (1995) (henceforth MUC) and Bagga and Baldwin (1998) (henceforth B 3 ). MUC F1, precision and recall are defined in terms of pairwise co-reference relationships between each mention. B 3 , F1, precision and recall are defined in terms of presence or absence of specific entities in the cluster. When measuring B 3 , we remove entities with no co-references (singletons) from the evaluation to avoid inflation of results (Cattan et al., 2020). The threshold baseline (BCOS) gives the highest MUC recall but also poor MUC precision and poorest B 3 precision. The B 3 metric is highly specific with respect to"
2021.eacl-main.21,P14-2089,0,0.0378128,"rent in large text corpora to learn context-aware word embeddings that can be used for downstream NLP tasks. However, these models do not learn about formal lexical constraints, often conflating different types of semantic relatedness (Ponti et al., 2018; Lauscher et al., 2020). This is a weakness of all distributional language models that is particularly problematic in the context of CD2 CR for entity mentions that are related but not co-referent (e.g. “Mars” and “Jupiter”) as shown in section 5. A number of solutions have been proposed for adding lexical knowledge to static word embeddings (Yu and Dredze, 2014; Wieting et al., 2015; Ponti et al., 2018) but contextual language models have received comparatively less attention. Lauscher et al (2020) propose adding a lexical relation classification step to BERT’s language model pre-training phase to allow the model to integrate both lexical and distributional knowledge. Their model, LIBERT, has been shown to facilitate statistically-significant performance boosts on a variety of downstream NLP tasks. 3 Dataset creation Our dataset is composed of pairs of news articles and scientific papers gathered automatically (Section 3.1). Our annotation process b"
2021.emnlp-demo.33,P19-1409,1,0.902456,"Missing"
2021.emnlp-demo.33,P99-1071,0,0.178254,"grasping the topic, and render an intuitive medium for navigating through the information. The abstractive summaries generated at real-time expose concise details for any combination of sub-topics of choice. Furthermore, we innovatively employ coreference resolution and proposition alignment to generate fine-grained opendomain facets. Attaining information of interest from large document sets has been approached with different techniques. A vast amount of research has been conducted on multi-document summarization, as a method for presenting the central aspects of a target set of texts (e.g. Barzilay et al., 1999; Haghighi and Vanderwende, 2009; Bing et al., 2015; Yasunaga et al., 2017), where query-focused summarization (Dang, 2005) biases the output summary around a given query (e.g. Daumé III and Marcu, 2006; Baumel et al., 2018; Xu and Lapata, 2020). Recognizing the need for dynamically acquiring a broader or deeper scope of the source texts, exploratory search (Marchionini, 2006; White and Roth, 2009) was coined as an umbrella term for allowing more dynamic interactive exploration of in- 6 Conclusion and Future Work formation. Adapting the summarization paradigm to the exploratory setting, intera"
2021.emnlp-demo.33,P14-1086,0,0.0278025,"chionini, 2006; White and Roth, 2009) was coined as an umbrella term for allowing more dynamic interactive exploration of in- 6 Conclusion and Future Work formation. Adapting the summarization paradigm to the exploratory setting, interactive summariza- In this paper, we presented iFACET S UM, a novel tion enables a user to refine or expand on a sum- text exploration approach and tool over large docmary via different modes of interaction. For exam- ument sets, which incorporates faceted search ple, Shapira et al. (2021), Avinesh et al. (2018) and into interactive summarization. Its faceted naviBaumel et al. (2014) provide a limited (or no) initial gation design provides a user with an overview of summary on the document set, and support iterative the topic and the ability to gradually investigate interaction, via queries or preference highlights, to subtopics of interest, communicating concise inforupdate the summary. However, the succinct initial mation via multi-facet abstractive summarization. summary, possibly accompanied by few suggested Fine-grained facet-values are generated from the queries, do not display the full scope of the source source texts based on cross-document coreference texts, whic"
2021.emnlp-demo.33,2021.findings-emnlp.225,1,0.70029,"ted using crossdocument (CD) coreference resolution pipelines, while Statements via a proposition alignment pipeline, described next.2 Concepts. We found that identifying and grouping together significant co-occurring events within the source document collection helps to expose and emphasize the notable concepts in the topic. To that end, we employ CD event coreference resolution which detects these concepts. CD coreference resolution (Lee et al., 2012) clusters text mentions that refer to the same event or entity across multiple documents. Presently, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be marked within the input texts. We therefore leverage the mention detection ability of the model by Cattan et al. (2021). Once we have obtained the coreference clusters"
2021.emnlp-demo.33,2021.findings-acl.453,1,0.82721,"ly, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be marked within the input texts. We therefore leverage the mention detection ability of the model by Cattan et al. (2021). Once we have obtained the coreference clusters from CDLM, events whose mentions are predominantly verbs are filtered out,3 since those usually present specific actions that tend to be less informative compared to nominal types that refer to more generic events (e.g., “said”, “found” “increase” compared to “unemployment”, “poverty”, “crash”). CD event coreference resolution separates specific event instances, hence differentiating between clusters of similar event types with different arguments (e.g., “unemployment” in Navajo vs. “unemployment” in Cayuga). Since generic event types, like “une"
2021.emnlp-demo.33,2020.tacl-1.5,0,0.0130802,"architecture. CD = cross-document, WD = within-document. Entities. The Entities facet-values help the user focus on entities such as people (e.g., ""Clinton""), locations (e.g., ""New York""), organizations (e.g., ""FBI"") and others (e.g., ""the casino""). We created a separate pipeline for CD entity coreference resolution, since we observed subpar performance when applying the above CD coreference pipeline for entity coreference.5 Unlike event coreference, mostly studied in the CD setting, entity coreference has recently seen impressive progress in the within-document (WD) setting (Wu et al., 2020; Joshi et al., 2020). Hence, we leverage WD entity coreference in our entity recognition pipeline, which comprises three main steps. (1) We use SpanBERT6 (Joshi et al., 2020), a state-of-the-art transformer-based LM for WD entity coreference resolution, to detect and cluster coreferring entity mentions within each separate document. (2) The entity mentions detected in the first step are marked as input for a CD entity coreference reolution model. To overcome ECB+ entity scarcity referred earlier, we use an alternative model that is trained on the WEC-Eng dataset (Eirew et al., 2021).7 (3) Finally, we apply agglom"
2021.emnlp-demo.33,S18-2001,0,0.0394528,"Missing"
2021.emnlp-demo.33,2020.acl-srw.26,0,0.0190972,"ils in Appendix A.2). This text is then given as input to BART (Lewis et al., 2020), a denoising sequence-to-sequence model fine-tuned on the single-document abstractive summarization task.8 iFACET S UM presents abstractive rather than extractive summaries due to their enhanced readability, particularly when summarizing a set of related sentences. This choice follows prior work, which 8 We use the huggingface model from https://hugg ingface.co/facebook/bart-large-cnn. 286 showed that fusing sentences with shared points of coreference potentially facilitates coherence of abstractive summaries (Lebanoff et al., 2020). Indeed, in an internal manual assessment of 30 random individual summaries produced by iFACET S UM, with 5 readability measures (Dang, 2006), testers found overall that the summaries are highly readable. To verify that factuality is not compromised, an additional inspection found that these summaries were also factually consistent to the input text, with 28 out of 30 sampled sentences marked as consistent. See Appendix B.3 for scores and more details on these assessments. 4 System Experiments iFACET S UM aims to provide an effective means of information seeking in scenarios that require lear"
2021.emnlp-demo.33,D12-1045,0,0.0413212,"e 5 in Appendix). 3 Backend Algorithms 3.1 Coreference-based Facet Formation As described in §2.1, there are three main facets. Concepts and Entities are extracted using crossdocument (CD) coreference resolution pipelines, while Statements via a proposition alignment pipeline, described next.2 Concepts. We found that identifying and grouping together significant co-occurring events within the source document collection helps to expose and emphasize the notable concepts in the topic. To that end, we employ CD event coreference resolution which detects these concepts. CD coreference resolution (Lee et al., 2012) clusters text mentions that refer to the same event or entity across multiple documents. Presently, the Cross-Document Language Model (CDLM) (Caciularu et al., 2021) is the state-of-the-art for CD coreference resolution. This model is pretrained on multiple related documents via cross-document masking, encouraging the model to learn crossdocument and long-range relationships. Specifically, we employ the CDLM version fine-tuned for coreference on the ECB+ corpus (Cybulska and Vossen, 2014). This model does not include a mention detection component, but rather expects relevant mentions to be ma"
2021.emnlp-demo.33,2020.acl-main.703,0,0.0214699,"e graph (more details in Appendix A.2). 3.2 Abstractive Facet Summarization In the standard summarization setting, a system receives a single or multiple documents as input, as well as a query in the query-focused task. In our case, the input is a set of sentences that have one or more selected facet-values in common, effectively providing a multi-facet summary. Given the set of sentences that correspond to the facet-value selection(s), these sentences are concatenated, ordered by their position in their source document (more details in Appendix A.2). This text is then given as input to BART (Lewis et al., 2020), a denoising sequence-to-sequence model fine-tuned on the single-document abstractive summarization task.8 iFACET S UM presents abstractive rather than extractive summaries due to their enhanced readability, particularly when summarizing a set of related sentences. This choice follows prior work, which 8 We use the huggingface model from https://hugg ingface.co/facebook/bart-large-cnn. 286 showed that fusing sentences with shared points of coreference potentially facilitates coherence of abstractive summaries (Lebanoff et al., 2020). Indeed, in an internal manual assessment of 30 random indiv"
2021.emnlp-demo.33,2020.findings-emnlp.440,1,0.849023,"Missing"
2021.emnlp-demo.33,Q19-1016,0,0.0587726,"Missing"
2021.emnlp-demo.33,2021.naacl-main.54,1,0.804173,"Missing"
2021.emnlp-demo.33,N18-1081,1,0.765441,"7 Fine-tuning CDLM on WEC-Eng is computationally infeasible, and therefore we use the model by Eirew et al. (2021). no NER label is assigned to a cluster, it is tagged as “Miscellaneous” (more details in Appendix A.2). Statements. Key statements benefit a user by presenting information about specific facts. To generate these statements, we group together coreferring propositions (rather than words) that describe the same fact within the source documents, as seen in §2.1. Following Ernst et al. (2020), our pipeline consists of three steps. (1) Proposition candidates are extracted with OpenIE (Stanovsky et al., 2018). (2) Pairs of propositions expressing the same statement are matched using the SuperPAL model (Ernst et al., 2020), considering proposition pairs whose alignment score is above 0.5 as matched. (3) A propositions graph is created by connecting pairs of nodes that represent similar propositions, and proposition clusters are matched for the connected components in the graph (more details in Appendix A.2). 3.2 Abstractive Facet Summarization In the standard summarization setting, a system receives a single or multiple documents as input, as well as a query in the query-focused task. In our case,"
2021.emnlp-main.108,bonial-etal-2014-propbank,0,0.017027,"prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing i"
2021.emnlp-main.108,N19-1423,0,0.0254101,"does someone win at? is at once too specific (inappropriate for locations better described with in, e.g., in Texas) and too general (also potentially applying to win.01’s A2 role, the contest being won). To choose the right prototype, we run a consistency check using an off-the-shelf QA model (see Figure 2, Bottom). We sample a set of gold arguments5 for the role from OntoNotes (Weischedel et al., 2017) and instantiate each prototype for each sampled predicate using the question contextualizer described in the next section (§4.2). We then select the prototype for which a BERT-based QA model (Devlin et al., 2019) trained on SQuAD 1.0 (Rajpurkar et al., 2016) achieves the highest tokenwise F1 in recovering the gold argument from the contextualized question. 4.2 Generating Contextualized Questions For our second stage, we introduce a question contextualizer model which takes in a prototype question and passage, and outputs a contextualized ver5 For core roles we sample 50 argument instances, and for adjunct roles we take 100 but select samples from any predicate sense. 1432 Air molecules move a lot and bump into things. QA-SRL: What bumps into something? ,→ Air molecules What does something bump into? ,"
2021.emnlp-main.108,2020.acl-main.69,0,0.036114,"Missing"
2021.emnlp-main.108,doddington-etal-2004-automatic,0,0.2616,"Missing"
2021.emnlp-main.108,2020.emnlp-main.49,0,0.188877,"prehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was SBJ someone something VERB bring arrive s"
2021.emnlp-main.108,P17-1123,0,0.0198036,"tomatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologie"
2021.emnlp-main.108,2020.coling-main.274,1,0.906344,"ive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may o"
2021.emnlp-main.108,N18-1020,0,0.0184749,"does the question correspond to the correct semantic role? For all of these measures, we source our data from existing SRL datasets and use human evaluation by a curated set of trusted workers on Amazon Mechanical Turk.6 Automated metrics like B LEU or ROUGE are not appropriate for our case because our questions’ meanings can be highly dependent on minor lexical choices (such as with prepositions) and because we lack gold references (particularly for questions without answers present). We assess grammaticality and adequacy on a 5point Likert scale, as previous work uses for similar measures (Elsahar et al., 2018; Dhole and Manning, 2020). We measure role correspondence with two metrics: role accuracy, which asks annotators to assign the question a semantic role based on PropBank role glosses, and question answering accuracy, which compares annotators’ answers to the question against the gold SRL argument (or the absence of such an argument).7 5.2 Main Evaluation Data We evaluate our system on a random sample of 400 predicate instances (1210 questions) from Ontonotes 5.0 (Weischedel et al., 2017) and 120 predicate instances (268 questions) from two small implicit SRL datasets: Gerber and Chai (2010, G"
2021.emnlp-main.108,K17-1034,0,0.0188967,"t al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found ways to align existing relation ontologies with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast maj"
2021.emnlp-main.108,P18-1191,1,0.943025,"s allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions"
2021.emnlp-main.108,2020.acl-main.703,0,0.0647819,"Missing"
2021.emnlp-main.108,W18-0530,0,0.016194,"ntial step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification of the kind and scope of information to be sought. As a result, previous work has found way"
2021.emnlp-main.108,W18-2501,0,0.0210027,"Missing"
2021.emnlp-main.108,P10-1160,0,0.0876674,"roach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is def"
2021.emnlp-main.108,D15-1076,0,0.168872,"e questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 142"
2021.emnlp-main.108,N10-1086,0,0.0565531,"t is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferred, or even absent from the text. Doing so requires prior specification"
2021.emnlp-main.108,W04-2705,0,0.0418399,"fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can"
2021.emnlp-main.108,2021.findings-acl.389,1,0.848242,"Missing"
2021.emnlp-main.108,W03-0203,0,0.245581,"e ability to exhaustively enumerate a set of questions corresponding to a known, broadcoverage underlying ontology of relations allows for a comprehensive, interpretable, and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a"
2021.emnlp-main.108,W13-0211,0,0.0536231,"Missing"
2021.emnlp-main.108,N19-1236,1,0.88712,"Missing"
2021.emnlp-main.108,J05-1004,0,0.239174,"ogy, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions for leverage corpus-wide statistics to compile an ontolanswers already identified in the text (Heilman and ogy of simplified, context-independent prototype ∗ Equal contribution questions for each PropBank role. In the second 1429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1429–1441 c November 7–11, 2021. 2021 Association for Computational Linguistics WH Who Where What AUX might would was"
2021.emnlp-main.108,2020.emnlp-main.224,1,0.745168,"and flexible way of representing and manipulating the information that is contained in—or missing from—natural language text. In this way, our work takes an essential step towards combining the advantages of formal ontologies and QA pairs for broad-coverage natural language understanding. 2 Background Question Generation Automatic question generation has been employed for use cases such as constructing educational materials (Mitkov and Ha, 2003), clarifying user intents (Aliannejadi et al., 2019), and eliciting labels of semantic relations in text (FitzGerald et al., 2018; Klein et al., 2020; Pyatkin et al., 2020). Methods include transforming syntactic trees (Heilman and Smith, 2010; Dhole and Manning, 2020) and SRL parses (Mazidi and Nielsen, 2014; Flor and Riordan, 2018), as well as training seq2seq models conditioned on the question’s answer (FitzGerald et al., 2018) or a text passage containing the answer (Du et al., 2017). By and large, these approaches are built to generate questions whose answers are already identifiable in a passage of text. However, question generation has the further potential to seek new information, which requires asking questions whose answers may only be implicit, inferr"
2021.emnlp-main.108,2020.findings-emnlp.3,0,0.0188773,", and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting covera two-stage approach. In the first stage (§4.1), we age and question fluency, or generated questions f"
2021.emnlp-main.108,P18-2124,0,0.0554899,"Missing"
2021.emnlp-main.108,D16-1264,0,0.330999,"by asking questions is an es- generate a contextually-appropriate question whose sential communicative ability, and natural language answer—if present—corresponds to an argument question-answer (QA) pairs provide a flexible for- of the predicate bearing the desired role. Some mat for representing and querying the information examples are shown in Figure 1. Since the set of expressed in a text. This flexibility has led to ap- possible questions is scoped by the relations in the plications in a wide range of tasks from reading underlying ontology, this gives us the ability to ask comprehension (Rajpurkar et al., 2016) to informa- it all: pose information-seeking questions that extion seeking dialogues (Qi et al., 2020). haustively cover a broad set of semantic relations Automatically generating questions can poten- that may be of interest to downstream applications. tially serve as an essential building block for such Concretely, we generate questions derived from applications. Previous work in question generation QA-SRL (He et al., 2015) for the semantic role has either required human-curated templates (Levy ontology in PropBank (Palmer et al., 2005) using et al., 2017; Du and Cardie, 2020), limiting cove"
2021.emnlp-main.108,2020.acl-main.626,1,0.831683,"with questions, either through human curation (Levy et al., 2017) — which limits the approach to very small ontologies — or with a small set of fixed question templates (Du and Cardie, 2020) — which relies heavily on glosses provided in the ontology, sometimes producing stilted or ungrammatical questions. In this work, we generate natural-sounding information-seeking questions for a broad-coverage ontology of relations such as that in PropBank (Bonial et al., 2014). QA-SRL Integral to our approach is QA-SRL (He et al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic"
2021.emnlp-main.108,W09-2417,0,0.031509,"al., 2015), a representation based on question-answer pairs which was shown by Roit et al. (2020) and Klein et al. (2020) to capture the vast majority of arguments and modifiers in PropBank and NomBank (Palmer et al., 2005; Meyers et al., 2004). Instead of using a pre-defined role lexicon, QA-SRL labels semantic roles with questions drawn from a 7-slot template, whose answers denote the argument bearing the role (see Table 1 for examples). Unlike in PropBank, QA-SRL argument spans may appear outside of syntactic argument positions, capturing implicit semantic relations (Gerber and Chai, 2010; Ruppenhofer et al., 2009). QA-SRL is useful to us because of its close correspondence to semantic roles and its carefully restricted slot-based format: it allows us to easily transform questions into context-independent prototypes which we can align to the ontology, by removing tense, negation, and other information immaterial to the semantic role (§4.1). It also allows us to produce contextualized questions which sound natural in the context of a passage, by automatically aligning the syntactic structure of different questions for the same predicate (§4.2). 3 Task Definition Our task is defined with respect to an ont"
2021.emnlp-main.778,N19-1348,0,0.0254904,"ltiple text sources, such as for multi-document summarization (MDS), requires intermediate explicit representations for cross-text content consolidation (Liao et al., 2018; Wities et al., 2017). Further, modeling redundant or overlapping text has been tackled through the sentence fusion task (Barzilay 1 and McKeown, 2005; Marsi and Krahmer, 2005; Our Code and data can be found here: https:// github.com/DanielaBWeiss/QA-ALIGN McKeown et al., 2010; Thadani and McKeown, 9880 2013), and recently as “disparate” sentence fusion, targeting related sentences in a single document (Nayeem et al., 2018; Geva et al., 2019; Lebanoff et al., 2019a,b, 2020b). In a recent series of investigations, Lebanoff et al. (2019a) highlight sentence fusion as a necessary step for improving summarization. In particular, pairs of sentences to be fused were empirically shown as a better source for generating summary sentences than single sentences (Lebanoff et al., 2019b). In Section 7, we analyze the potential utility of our QA alignments as a redundancy signal in a sentence fusion model. QA-SRL QA-SRL has been shown to attain high quality annotation of predicate-argument structure for verbs, via crowdsourcing (FitzGerald et"
2021.emnlp-main.778,2020.acl-main.703,0,0.0141445,"ed from a single source (Lebanoff et al., 2019a), rather than merging information from multiple sources. In that vein, we aim to examine whether explicitly incorporating alignments into the fusion task would encourage a model to attend to corresponding information originated in multiple sentences. 7.1 The Sentence Fusion Experiment uments, which need to be summarized. We create a new modern baseline for the dataset of Thadani and McKeown (2013), which outperforms their pre-neural one, evaluated using bigramF1. As a baseline end-to-end fusion model, we employ the pre-trained auto-encoder BART (Lewis et al., 2020), which has achieved state-of-the-art results on summarization tasks. In comparison, we predict QA alignments for the fusion data using our best reported model, and then incorporate them into the fusion model (termed Fuse-Align) using an input-augmentation technique (resembling the one in §6.1). As shown in Table 6, the input encompasses the alignment information by attaching indexed markup tokens around aligned predicates and arguments.12 7.2 Results and Analysis We create 20 different variations of the dataset by randomly shuffling the order of the input sentences in each input, and take the"
2021.emnlp-main.778,N04-1002,0,0.0382324,", we compile and release our crowdsourced QA-Align dataset (accompanied by new QA-SRL annotations), over semantically similar paired texts (§4) that we collect. 1 We further analyze the quality of our data and compare it to an established crossdocument (CD) coreference benchmark, ECB+ (Cybulska and Vossen, 2014) (§5). Finally, we implement a baseline modeling approach for QAAlign (§6), and present an analysis of appealing potential downstream use, over a sentence fusion task (§7). 2 Related Work Proposition alignment is closely related to the Cross Document Coreference Resolution (CDCR) task (Gooi and Allan, 2004; Mayfield et al., 2009). This task concerns clustering together entity or event mentions across topically related documents that refer to the same “real world” element, (entity or event). It has drawn substantial recent attention (Barhom et al., 2019; Zeng et al., 2020; Cattan et al., 2020; Yu et al., 2020), as its considered a fundamental intermediate task for cross-text language understanding. While the concept of coreference is essential for defining our alignment criteria (§3), we consider matching predicate–argument relations as expressing alignments of propositional information, and acc"
2021.emnlp-main.778,C18-1101,0,0.136087,"tions, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks. Figure 1: An example of two alignments expressing the same propositions captured by QASRL questionanswers. challenges, a few earlier MDS works (Liu et al., 2015; Liao et al., 2018; Shapira et al., 2017) attempted at leveraging semantic structures for consolidation, such as Abstract Meaning Representation (Banarescu et al., 2013) or Open Knowledge Graph (Wities et al., 2017), however, these initial means were found either too fine-grained (AMR) or immature (OKR) for efficient downstream consolidation. 1 Introduction In this paper, we propose that a useful step toEnd-to-end neural methods have become the de- ward effective consolidation is to detect and align minimal propositions that refer to the same informafacto standard for natural language understanding tion. Theref"
2021.emnlp-main.778,D15-1076,0,0.200754,"as a conceptual extension over the established Coreference Resolution framework, while handling content overlap at the more complex level of semantic relations (conveying stated information) rather than entity and event mentions (denoting referents). This difference in alignment scope also pertains to the cross-document coreference dataset used in these prior works, as we analyze and compare to in §5.2. Unlike earlier work (Roth and Frank, 2012) that leveraged structured SRL to align predicateargument-structures, we leverage the QuestionAnswer driven Semantic Role Labeling paradigm (QA-SRL) (He et al., 2015). QA-SRL captures predicate-argument relations using a naturally phrased question-answer pair, where the question type (who, what, where, etc.) reflects the role being captured, while the answer denotes the argument. For example, in Figure 1 sentence A, the Agent role (A0) of the predicate “purchase” is captured through the question-answer Who purchased something? — Wade. Once such relations are captured by QAs, we can align these QAs to capture similar propositional information expressed in a pair of sentences (see Figure 1). Since QAs are naturally intelligible and do not require a pre-defin"
2021.emnlp-main.778,N15-1114,0,0.1504,"cate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks. Figure 1: An example of two alignments expressing the same propositions captured by QASRL questionanswers. challenges, a few earlier MDS works (Liu et al., 2015; Liao et al., 2018; Shapira et al., 2017) attempted at leveraging semantic structures for consolidation, such as Abstract Meaning Representation (Banarescu et al., 2013) or Open Knowledge Graph (Wities et al., 2017), however, these initial means were found either too fine-grained (AMR) or immature (OKR) for efficient downstream consolidation. 1 Introduction In this paper, we propose that a useful step toEnd-to-end neural methods have become the de- ward effective consolidation is to detect and align minimal propositions that refer to the same informafacto standard for natural language underst"
2021.emnlp-main.778,N10-1044,0,0.0338688,"aptures more exhaustively information overlap across related texts (§5.2). As potential uses for downstream tasks, various prior works presume that effectively handling multiple text sources, such as for multi-document summarization (MDS), requires intermediate explicit representations for cross-text content consolidation (Liao et al., 2018; Wities et al., 2017). Further, modeling redundant or overlapping text has been tackled through the sentence fusion task (Barzilay 1 and McKeown, 2005; Marsi and Krahmer, 2005; Our Code and data can be found here: https:// github.com/DanielaBWeiss/QA-ALIGN McKeown et al., 2010; Thadani and McKeown, 9880 2013), and recently as “disparate” sentence fusion, targeting related sentences in a single document (Nayeem et al., 2018; Geva et al., 2019; Lebanoff et al., 2019a,b, 2020b). In a recent series of investigations, Lebanoff et al. (2019a) highlight sentence fusion as a necessary step for improving summarization. In particular, pairs of sentences to be fused were empirically shown as a better source for generating summary sentences than single sentences (Lebanoff et al., 2019b). In Section 7, we analyze the potential utility of our QA alignments as a redundancy signal"
2021.emnlp-main.778,Q18-1021,0,0.0155158,"representing a single predication involving two sentence elements, along with the semanficulty concerns consolidating information from tic role that relates them. Identifying these types different, possibly redundant texts, which is crucial for applications such as multi-document summa- of alignments at the propositional level would farization (MDS), sentence fusion (McKeown et al., cilitate recognizing information redundancies and salience. Aligning propositions across documents 2010; Thadani and McKeown, 2013) or multi-hop may also prove useful to a variety of cross-text question-answering (Welbl et al., 2018; Feldman tasks, such as knowledge-base population, Multiand El-Yaniv, 2019). Previous works show that hop QA and cross document event extraction. MDS methods for example, often just concatenate rather than merge inputs (Lebanoff et al., 2019a), Consider the example in Figure 1; our alignor erroneously consolidate on non-coreferring ele- ments represent that both sentences express simiments (Lebanoff et al., 2020b). Recognizing such lar propositional information — sentence A talks 9879 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9879–9894 c Nov"
2021.emnlp-main.778,C18-1102,0,0.0470016,"Missing"
2021.emnlp-main.778,N04-1019,0,0.065972,"Missing"
2021.emnlp-main.778,2020.acl-main.626,1,0.83638,"that include aligned spans. Table 3 details the source distributions of our data. The average ROUGE-2 similarity score across paired sentences indicates that we indeed achieve a dataset of semantically similar sentences, yet exhibiting a limited degree of lexical overlap, providing challenging alignment cases. We maintained original train/dev/test splits for each corpora where suitably available and created our own otherwise. For more details regarding our dataset creation, see Appendix B. 4.2 Crowdsourcing Aiming at high quality annotation, we applied a controlled crowdsourcing methodology (Roit et al., 2020), over the Amazon Mechanical Turk platform. 2 https://www-nlpir.nist.gov/projects/duc/data.html, years Crowd workers were selected and trained, ensuring used 2005-2008 3 https://tac.nist.gov/, years used 2009-2011 a reliable, scalable, cheap and rapid process. 9882 4.2.1 QA-SRL Annotation We begin with crowdsourcing the prerequisite QASRL annotation over our data. We followed the guidelines and protocol of Roit et al. (2020), utilizing their released annotation tools. For our dev and test sets, we collected a single worker’s annotation per predicate. Evaluating their performance against an exp"
2021.emnlp-main.778,S12-1030,0,0.439243,"xample in Figure 1; our alignor erroneously consolidate on non-coreferring ele- ments represent that both sentences express simiments (Lebanoff et al., 2020b). Recognizing such lar propositional information — sentence A talks 9879 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9879–9894 c November 7–11, 2021. 2021 Association for Computational Linguistics about a “buying” event while sentence B is framing the same event through the “seller” perspective, essentially capturing the same reversed roles. In previous predicate-argument alignment works (Roth and Frank, 2012; Wolfe et al., 2013), predicates and arguments were aligned individually, rather than aligning the predicate argument relation as a whole, along with its semantic role. Our approach can be seen as a conceptual extension over the established Coreference Resolution framework, while handling content overlap at the more complex level of semantic relations (conveying stated information) rather than entity and event mentions (denoting referents). This difference in alignment scope also pertains to the cross-document coreference dataset used in these prior works, as we analyze and compare to in §5.2"
2021.emnlp-main.778,N15-1002,0,0.0544043,"Missing"
2021.emnlp-main.778,P13-2012,0,0.0491172,"Missing"
2021.emnlp-main.778,2020.emnlp-main.582,0,0.0275868,"alignments are required in order to properly capture information overlap, complementing the more elementary task of traditional event and entity coreference resolution. 6 6.1 Baseline QA-Align Model Model Description Taking a simplistic modeling approach as an initial baseline, we reduce the QA alignment prediction problem into a binary classification task. Let (S1 , S2 ) be a sentence pair, where each sentence is provided with a set R of QA-SRL relations (QAs): ris ∈ Rs , 1 ≥ i ≥ |Rs |, s ∈ {1, 2} We experimented with BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefRoBERTa (Ye et al., 2020)9 as the pretrained models. Following previous works on CD coreference (Cybulska and Vossen, 2014; Barhom et al., 2019; Zeng et al., 2020) and predicate-argument alignment (Wolfe et al., 2013; Roth and Frank, 2012), we compare our models to a lemma-based baseline method. Similarly to the alignment criterion applied for the ECB-based analysis of our crowdsourced data (§5.2), our lemma baseline model aligns QA pairs in which the two predicates, as well as the head words of the answer spans, share a lemma. 6.2 Model Performance Results are shown in Table 5. 10 Notably, the lemma baseline performa"
2021.emnlp-main.778,2020.coling-main.275,0,0.217137,"chmark, ECB+ (Cybulska and Vossen, 2014) (§5). Finally, we implement a baseline modeling approach for QAAlign (§6), and present an analysis of appealing potential downstream use, over a sentence fusion task (§7). 2 Related Work Proposition alignment is closely related to the Cross Document Coreference Resolution (CDCR) task (Gooi and Allan, 2004; Mayfield et al., 2009). This task concerns clustering together entity or event mentions across topically related documents that refer to the same “real world” element, (entity or event). It has drawn substantial recent attention (Barhom et al., 2019; Zeng et al., 2020; Cattan et al., 2020; Yu et al., 2020), as its considered a fundamental intermediate task for cross-text language understanding. While the concept of coreference is essential for defining our alignment criteria (§3), we consider matching predicate–argument relations as expressing alignments of propositional information, and accordingly as a useful component in capturing information correspondence between texts, unlike prior approaches which match individual event and entity mentions disjointly. Our work is largely inspired by the earlier predicate-argument alignment task (Roth and Frank, 2012"
2021.findings-acl.453,P19-1409,1,0.924762,"s and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spa"
2021.findings-acl.453,P10-1143,0,0.293584,"Missing"
2021.findings-acl.453,2021.starsem-1.13,1,0.761257,"Missing"
2021.findings-acl.453,D17-1226,0,0.224187,"Missing"
2021.findings-acl.453,cybulska-vossen-2014-using,0,0.252298,". Our model achieves competitive results for event and entity coreference resolution on gold mentions. More importantly, we set first baseline results, on the standard ECB+ dataset, for CD coreference resolution over predicted mentions. Further, our model is simpler and more efficient than recent CD coreference resolution systems, while not using any external resources.1 1 Introduction Cross-document (CD) coreference resolution consists of identifying textual mentions across multiple documents that refer to the same concept. For example, consider the following sentences from the ECB+ dataset (Cybulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020"
2021.findings-acl.453,W15-0801,0,0.0132712,"erative clustering that was shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A"
2021.findings-acl.453,2020.tacl-1.5,1,0.905254,"Missing"
2021.findings-acl.453,D19-1588,1,0.93481,"bulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD corefere"
2021.findings-acl.453,S18-2001,0,0.379007,"es competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolu"
2021.findings-acl.453,D12-1045,0,0.28704,"omerative clustering can effectively find cross-document coreference clusters. 4 4.1 Experiments Experimental setup Following most recent work, we conduct our experiments ECB+ (Cybulska and Vossen, 2014), which is the largest dataset that includes both WD and CD coreference annotation (see Appendix A.2). We use the document clustering of Barhom et al. (2019) for pre-processing and apply our coreference model separately on each predicted document cluster. Following Barhom et al. (2019), we present the model’s performance on both event and entity coreference resolution. In addition, inspired by Lee et al. (2012), we train our model to perform event and entity coreference jointly, which we term “ALL”. This represents a useful scenario when we are interested in finding all the coreference links in a set of documents, without having to distinguish event and entity mentions. Addressing CD coreference with ALL is challenging because (1) the search space is larger than when treating separately event and entity coreference and (2) models need to make subtle distinctions between event and entity mentions that are lexically similar but do not corefer. For example, the entity voters do not corefer with the eve"
2021.findings-acl.453,D17-1018,0,0.322866,"e ECB+ dataset (Cybulska and Vossen, 2014), where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combi"
2021.findings-acl.453,2021.ccl-1.108,0,0.0839573,"Missing"
2021.findings-acl.453,2020.findings-emnlp.440,1,0.861846,"tter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spans, (2) keep top spans according to sm (i), (3) score all pairs s(i, j), and (4) cluster spans using agglomerative clustering. ing a paraphrase resource (Chirps; Shwartz et al., 2017) as distant supervision. Parallel to our work, recent approaches propose to fine"
2021.findings-acl.453,Q15-1037,0,0.344054,"shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simpler and substantially more efficient than existing CD coreference systems. Taken together, our work seeks to bridge the gap between WD and CD coreference, driving further research of the latter in realistic settings. 2 Background Cross-document coreference Previous works on CD coreference resolution learn a pairwise scorer between mentions and use a clustering approach to form the coreference clusters (Cybulska and Vossen, 2015; Yang et al., 2015; Choubey and Huang, 2017; Kenyon-Dean et al., 2018; Bugert et al., 2020). Barhom et al. (2019) proposed to jointly learn entity and event coreference resolution, leveraging predicate-argument structures. Their model forms the coreference clusters incrementally, while alternating between event and entity coreference. Based on this work, Meged et al. (2020) improved results on event coreference by leverag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram"
2021.findings-acl.453,2020.coling-main.275,0,0.678153,"ag5100 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5100–5107 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: A high-level diagram of our model for cross-document coreference resolution. (1) Extract and score all possible spans, (2) keep top spans according to sm (i), (3) score all pairs s(i, j), and (4) cluster spans using agglomerative clustering. ing a paraphrase resource (Chirps; Shwartz et al., 2017) as distant supervision. Parallel to our work, recent approaches propose to fine-tune BERT on the pairwise coreference scorer (Zeng et al., 2020), where the state-of-the-art on ECB+ is achieved using a cross-document language model (CDLM) on pairs of full documents (Caciularu et al., 2021). Instead of applying BERT for all mentions pairs which is quadratically costly, our work separately encodes each (predicted) mention. All above models suffer from several drawbacks. First, they use only gold mentions and treat entities and events separately.2 Second, pairwise scores are recomputed after each merging step, which is resource and time consuming. Finally, they rely on additional resources, such as semantic role labeling, a within-documen"
2021.findings-acl.453,W12-4501,0,0.085386,"gi , gj , gi ◦ gj ]) s(i, j) = sm (i) + sm (j) + sa (i, j) 3 Model The overall structure of our model is shown in Figure 1. The major obstacle in applying the e2e-coref model directly in the CD setting is its reliance on textual ordering – it forms coreference chains by linking each mention to an antecedent span appearing before it in the document. This linear clustering method cannot be used in the multipledocument setting since there is no inherent ordering between the documents. Additionally, ECB+ (the main benchmark for CD coreference resolution) is relatively small compared to OntoNotes (Pradhan et al., 2012), making it hard to jointly optimize mention detection and coreference decision. These challenges have implications in all stages of model development, as elaborated below. Pre-training To address the small scale of the dataset, we pre-train the mention scorer sm (·) on the gold mention spans, as ECB+ includes singleton annotation. This enables generating good candidate spans from the first epoch, and as we show in Section 4.3, it substantially improves performance. Training Instead of comparing a mention only to its previous spans in the text, our pairwise scorer sa (i, j) compares a mention"
2021.findings-acl.453,S17-1019,1,0.924892,"car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD coreference model with agglomerative clustering that was shown useful in CD models. Our model achieves competitive results on ECB+ over gold mentions and sets baseline results over predicted mentions. Our model is also simp"
2021.findings-acl.453,2020.acl-main.622,0,0.0308084,"where colors represent coreference clusters (for brevity, we omit some clusters): 1. Thieves pulled off a two million euro jewellery heist in central Paris on Monday after smashing their car through the store’s front window. 2. Four men drove a 4x4 through the front window of the store on Rue de Castiglione, before making off with the jewellery and watches. Despite its importance for downstream tasks, CD coreference resolution has been lagging behind the 1 https://github.com/ariecattan/coref impressive strides made in the scope of a single document (Lee et al., 2017; Joshi et al., 2019, 2020; Wu et al., 2020). Further, state-of-the-art models exhibit several shortcomings, such as operating on gold mentions or relying on external resources such as SRL or a paraphrase dataset (Shwartz et al., 2017), preventing them from being applied on realistic settings. To address these limitations, we develop the first end-to-end CD coreference model building upon a prominent within-document (WD) coreference model (Lee et al., 2017) which we extend with recent advances in transformer-based encoders. We address the inherently non-linear nature of the CD setting by combining the WD coreference model with agglomera"
2021.findings-emnlp.225,2021.emnlp-main.382,0,0.42587,"imate. To accommodate our proposed CDLM model, we modify this modeling by including the entire documents containing the two candidate mentions, instead of just their containing sentences, and assigning the global attention mode to the mentions’ tokens and to the [CLS] token. The full method and hyperparameters are elaborated in Appendix C.1. – Cattan et al. (2020) is a model trained in an endto-end manner (jointly learning mention detection and coreference following Lee et al. (2017)), employing the RoBERTa-large model to encode each document separately and to train a pair-wise scorer atop. – Allaway et al. (2021) is a BERT-based model combining sequential prediction with incremental clustering. The following baselines were used for event coreference resolution. They all integrate external linguistic information as additional features. – Meged et al. (2020) is an extension of Barhom et al. (2019), leveraging external knowledge acquired from a paraphrase resource (Shwartz et al., 2017). – Zeng et al. (2020) is an end-to-end model, encoding the concatenated two sentences containing the two mentions by the BERT-large model. Similarly to our algorithm, they feed a MLP-based pairwise scorer with the concate"
2021.findings-emnlp.225,P19-1409,1,0.884613,"od and hyperparameters are elaborated in Appendix C.1. – Cattan et al. (2020) is a model trained in an endto-end manner (jointly learning mention detection and coreference following Lee et al. (2017)), employing the RoBERTa-large model to encode each document separately and to train a pair-wise scorer atop. – Allaway et al. (2021) is a BERT-based model combining sequential prediction with incremental clustering. The following baselines were used for event coreference resolution. They all integrate external linguistic information as additional features. – Meged et al. (2020) is an extension of Barhom et al. (2019), leveraging external knowledge acquired from a paraphrase resource (Shwartz et al., 2017). – Zeng et al. (2020) is an end-to-end model, encoding the concatenated two sentences containing the two mentions by the BERT-large model. Similarly to our algorithm, they feed a MLP-based pairwise scorer with the concatenation of the [CLS] representation and an attentive function of the candidate mentions representations. – Yu et al. (2020) is an end-to-end model similar to Zeng et al. (2020), but uses rather RoBERTa-large and does not consider the [CLS] contextualized token representation for the pairw"
2021.findings-emnlp.225,N18-1022,0,0.0373998,"Missing"
2021.findings-emnlp.225,W19-4828,0,0.0719158,"s setting, the et al., 2018). model is encouraged to learn to consider and repreExisting language models (LMs) (Devlin et al., sent such relationships, since they provide useful signals when optimizing for the language modeling 2019a; Liu et al., 2019; Raffel et al., 2020), which objective. For example, we may expect that it will are pretrained with variants of the masked language be easier for a model to unmask the word alleges modeling (MLM) self-supervised objective, are known to provide powerful representations for in- in Document 2 if it would manage to effectively ternal text structure (Clark et al., 2019; Rogers “peek” at Document 2, by matching the masked position and its context with the corresponding inet al., 2020a), which were shown to be beneficial formation in the other document. ∗ Work partly done as an intern at AI2. 1 Naturally, considering cross-document context Code and models are available at https://github. com/aviclu/CDLM in pretraining, as well as in finetuning, requires 2648 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648–2662 November 7–11, 2021. ©2021 Association for Computational Linguistics a model that can process a fairly large amount o"
2021.findings-emnlp.225,2020.emnlp-main.550,0,0.0243615,"y et al., 2020; Zaheer et al., 2020) introduced the idea of processing multi-document tasks using a single long-context sequence encoder. However, pretraining objectives in these models consider only single documents. Here, we showed that additional gains can be obtained by MLM pretraining using multiple related documents as well as a new dynamic global attention pattern. Processing and aggregating information from multiple documents has been also explored in the context of document retieval, aiming to extract information from a large set of documents (Guu et al., 2020; Lewis et al., 2020a,b; Karpukhin et al., 2020). These works focus on retrieving relevant information from often a large collection of documents, by utilizing short-context LMs, and then generate information of interest. CDLM instead provides an approach for improving the encoding and contextualizing information across multiple documents. As opposed to the mentioned works, our model utilizes long-context LM and can include broader Acknowledgments contexts of more than a single document. The use of cross-document attention has been We thank Doug Downey and Luke Zettlemoyer for recently explored by the Cross-Document Atten- fruitful discussi"
2021.findings-emnlp.225,S18-2001,0,0.365513,"Missing"
2021.findings-emnlp.225,D17-1018,0,0.068032,"gated into a single feature vector which is passed into an additional MLP-based scorer to produce the coreference probability estimate. To accommodate our proposed CDLM model, we modify this modeling by including the entire documents containing the two candidate mentions, instead of just their containing sentences, and assigning the global attention mode to the mentions’ tokens and to the [CLS] token. The full method and hyperparameters are elaborated in Appendix C.1. – Cattan et al. (2020) is a model trained in an endto-end manner (jointly learning mention detection and coreference following Lee et al. (2017)), employing the RoBERTa-large model to encode each document separately and to train a pair-wise scorer atop. – Allaway et al. (2021) is a BERT-based model combining sequential prediction with incremental clustering. The following baselines were used for event coreference resolution. They all integrate external linguistic information as additional features. – Meged et al. (2020) is an extension of Barhom et al. (2019), leveraging external knowledge acquired from a paraphrase resource (Shwartz et al., 2017). – Zeng et al. (2020) is an end-to-end model, encoding the concatenated two sentences co"
2021.findings-emnlp.225,2021.ccl-1.108,0,0.088317,"Missing"
2021.findings-emnlp.225,2020.acl-main.447,0,0.0279259,"tion and plagiarism detection. The goal of both tasks is categorizing whether a particular relationship holds between two input documents. Citation recommendation deals with detecting whether one reference document should cite the other one, while the plagiarism detection task infers whether one document plagiarizes the other one. To compare with recent state-of-the-art models, we utilized the setup and data selection from Zhou et al. (2020), which provides three datasets for citation recommendation and one for plagiarism detection. 2018), and the Semantic Scholar Open Research Corpus (S2ORC; Lo et al., 2020). For plagiarism detection, the dataset is the Plagiarism Detection Challenge (PAN; Potthast et al., 2013). AAN is composed of computational linguistics papers which were published on the ACL Anthology from 2001 to 2014, OC is composed of computer science and neuroscience papers, S2ORC is composed of open access papers across broad domains of science, and PAN is composed of web documents that contain several kinds of plagiarism phenomena. For further dataset prepossessing details and statistics, see Appendix A.3. Algorithm. For our models, we added the [CLS] token at the beginning of the input"
2021.findings-emnlp.225,H05-1004,0,0.0363098,"rases, or event mentions, typically verbs or nominalizations that appear in the text. Benchmark. We evaluated our CDLM by utilizing it over the ECB+ corpus (Cybulska and Vossen, 2014), the most commonly used dataset for CD coreference. ECB+ consists of within- and crossdocument coreference annotations for entities and events (statistics are given in Appendix A.2). Following previous work, for comparison, we conduct our experiments on gold event and entity mentions. We follow the standard coreference resolution evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), their average CoNLL F1, and the more recent LEA metric (Moosavi and Strube, 2016). Algorithm. Recent approaches for CD coreference resolution train a pairwise scorer to learn the probability that two mentions are co-referring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. We made several modifications to the pairwise scorer. The current state-of-the-art models (Zeng et al., 2020; Yu et al., 2020) train the pairwise scorer by including only the local contexts (containing sentences) of the candidate mentions. They conc"
2021.findings-emnlp.225,2020.findings-emnlp.440,1,0.462418,"s and to the [CLS] token. The full method and hyperparameters are elaborated in Appendix C.1. – Cattan et al. (2020) is a model trained in an endto-end manner (jointly learning mention detection and coreference following Lee et al. (2017)), employing the RoBERTa-large model to encode each document separately and to train a pair-wise scorer atop. – Allaway et al. (2021) is a BERT-based model combining sequential prediction with incremental clustering. The following baselines were used for event coreference resolution. They all integrate external linguistic information as additional features. – Meged et al. (2020) is an extension of Barhom et al. (2019), leveraging external knowledge acquired from a paraphrase resource (Shwartz et al., 2017). – Zeng et al. (2020) is an end-to-end model, encoding the concatenated two sentences containing the two mentions by the BERT-large model. Similarly to our algorithm, they feed a MLP-based pairwise scorer with the concatenation of the [CLS] representation and an attentive function of the candidate mentions representations. – Yu et al. (2020) is an end-to-end model similar to Zeng et al. (2020), but uses rather RoBERTa-large and does not consider the [CLS] contextua"
2021.findings-emnlp.225,P16-1060,0,0.088595,"pear in the text. Benchmark. We evaluated our CDLM by utilizing it over the ECB+ corpus (Cybulska and Vossen, 2014), the most commonly used dataset for CD coreference. ECB+ consists of within- and crossdocument coreference annotations for entities and events (statistics are given in Appendix A.2). Following previous work, for comparison, we conduct our experiments on gold event and entity mentions. We follow the standard coreference resolution evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), their average CoNLL F1, and the more recent LEA metric (Moosavi and Strube, 2016). Algorithm. Recent approaches for CD coreference resolution train a pairwise scorer to learn the probability that two mentions are co-referring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. We made several modifications to the pairwise scorer. The current state-of-the-art models (Zeng et al., 2020; Yu et al., 2020) train the pairwise scorer by including only the local contexts (containing sentences) of the candidate mentions. They concatenate the two input sentences and feed them into a transformer-based LM. Then, pa"
2021.findings-emnlp.225,2020.tacl-1.54,0,0.0177609,"e that the B IG B IRD model was pretrained on much larger data, using more compute resources compared both to the Longformer model and to our models. We suspect that with more compute and data, it is possible to close the gap between CDLM and B IG B IRD performance. We leave for future work evaluating a larger version of the CDLM model against large, state-of-the-art models. 3.5 Attention Analysis It was recently shown that during the pretraining phase, LMs learn to encode various types of linguistic information, that can be identified via their attention patterns (Wiegreffe and Pinter, 2019; Rogers et al., 2020b). In Clark et al. (2019), the attention weights of BERT were proved as informative for probing the degree to which a particular token is “important”, as well as its linguistic roles. For example, they showed that the averaged attention weights from the last layer of BERT are beneficial features for dependency parsing. We posit that our pretraining scheme, which combines global attention and a multi-document context, captures alignment and mapping information across documents. Hence, we hypothesize that the global attention mechanism favors crossdocument (CD), long-range relations. To gain mo"
2021.findings-emnlp.225,S17-1019,1,0.727934,"trained in an endto-end manner (jointly learning mention detection and coreference following Lee et al. (2017)), employing the RoBERTa-large model to encode each document separately and to train a pair-wise scorer atop. – Allaway et al. (2021) is a BERT-based model combining sequential prediction with incremental clustering. The following baselines were used for event coreference resolution. They all integrate external linguistic information as additional features. – Meged et al. (2020) is an extension of Barhom et al. (2019), leveraging external knowledge acquired from a paraphrase resource (Shwartz et al., 2017). – Zeng et al. (2020) is an end-to-end model, encoding the concatenated two sentences containing the two mentions by the BERT-large model. Similarly to our algorithm, they feed a MLP-based pairwise scorer with the concatenation of the [CLS] representation and an attentive function of the candidate mentions representations. – Yu et al. (2020) is an end-to-end model similar to Zeng et al. (2020), but uses rather RoBERTa-large and does not consider the [CLS] contextualized token representation for the pairwise classification. Results. The results on event and entity CD coreference resolution are"
2021.findings-emnlp.225,C18-1203,0,0.0489696,"ning of the input sequence, assigned it global attention, and concatenated the pair of texts, according to the finetuning setup discussed in Section 2.2. The hyperparameters are further detailed in Appendix C.2. Baselines. We consider the reported results of the following recent baselines: – HAN (Yang et al., 2016) proposed the Hierarchical Attention Networks (HANs). These models employ a bottom-up approach in which a document is represented as an aggregation of smaller components i.e., sentences, and words. They set competitive performance in different tasks involving long document encoding (Sun et al., 2018). – SMASH (Jiang et al., 2019) is an attentive hierarchical recurrent neural network (RNN) model, used for tasks related to long documents. – SMITH (Yang et al., 2020) is a BERT-based hierarchical model, similar HANs. Benchmarks. For citation recommendation, the datasets include the ACL Anthology Network Cor- – CDA (Zhou et al., 2020) is a cross-document pus (AAN; Radev et al., 2013), the Semantic attentive mechanism (CDA) built on top of HANs, Scholar Open Corpus (OC; Bhagavatula et al., based on BERT or GRU models (see Section 4). 2653 Model AAN OC S2orc PAN Model Ans Sup Joint SMASH (2019)"
2021.findings-emnlp.225,M95-1005,0,0.151782,"red mentions can be either entity mentions, usually noun phrases, or event mentions, typically verbs or nominalizations that appear in the text. Benchmark. We evaluated our CDLM by utilizing it over the ECB+ corpus (Cybulska and Vossen, 2014), the most commonly used dataset for CD coreference. ECB+ consists of within- and crossdocument coreference annotations for entities and events (statistics are given in Appendix A.2). Following previous work, for comparison, we conduct our experiments on gold event and entity mentions. We follow the standard coreference resolution evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), their average CoNLL F1, and the more recent LEA metric (Moosavi and Strube, 2016). Algorithm. Recent approaches for CD coreference resolution train a pairwise scorer to learn the probability that two mentions are co-referring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. We made several modifications to the pairwise scorer. The current state-of-the-art models (Zeng et al., 2020; Yu et al., 2020) train the pairwise scorer by including only the local contexts (containin"
2021.findings-emnlp.225,D19-1002,0,0.0282015,"e detection sub-task. We note that the B IG B IRD model was pretrained on much larger data, using more compute resources compared both to the Longformer model and to our models. We suspect that with more compute and data, it is possible to close the gap between CDLM and B IG B IRD performance. We leave for future work evaluating a larger version of the CDLM model against large, state-of-the-art models. 3.5 Attention Analysis It was recently shown that during the pretraining phase, LMs learn to encode various types of linguistic information, that can be identified via their attention patterns (Wiegreffe and Pinter, 2019; Rogers et al., 2020b). In Clark et al. (2019), the attention weights of BERT were proved as informative for probing the degree to which a particular token is “important”, as well as its linguistic roles. For example, they showed that the averaged attention weights from the last layer of BERT are beneficial features for dependency parsing. We posit that our pretraining scheme, which combines global attention and a multi-document context, captures alignment and mapping information across documents. Hence, we hypothesize that the global attention mechanism favors crossdocument (CD), long-range"
2021.findings-emnlp.225,2020.coling-main.275,0,0.776142,"entity mentions. We follow the standard coreference resolution evaluation metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), their average CoNLL F1, and the more recent LEA metric (Moosavi and Strube, 2016). Algorithm. Recent approaches for CD coreference resolution train a pairwise scorer to learn the probability that two mentions are co-referring. At inference time, an agglomerative clustering based on the pairwise scores is applied, to form the coreference clusters. We made several modifications to the pairwise scorer. The current state-of-the-art models (Zeng et al., 2020; Yu et al., 2020) train the pairwise scorer by including only the local contexts (containing sentences) of the candidate mentions. They concatenate the two input sentences and feed them into a transformer-based LM. Then, part of the resulting tokens representations are aggregated into a single feature vector which is passed into an additional MLP-based scorer to produce the coreference probability estimate. To accommodate our proposed CDLM model, we modify this modeling by including the entire documents containing the two candidate mentions, instead of just their containing sentences, and ass"
2021.findings-emnlp.225,2020.emnlp-main.407,0,0.440282,"Missing"
2021.findings-emnlp.225,D18-1259,0,0.0221154,"ver, CDA impairs the performance of BERT-HAN, implying that dataset does not require detailed cross-document attention at all. In our experiments, finetuning BERT-HAN+CDA over the PAN dataset yielded poor results: F1 score of 79.6, substantially lower compared to our models. The relatively small size of PAN may explain such degradations. Table 5: HotpotQA-distractor results (F1 ) for the dev set. We use the “base” model size results from prior work for direct comparison. Ans: answer span, Sup: Supporting facts. multiple supporting documents. Benchmark. We used the HotpotQA-distractor dataset (Yang et al., 2018). Each example in the dataset is comprised of a question and 10 different paragraphs from different documents, extracted from Wikipedia; two gold paragraphs include the relevant information for properly answering the question, mixed and shuffled with eight distractor paragraphs (for the full dataset statistics, see Yang et al. (2018)). There are two goals for this task: extraction of the correct answer span, and detecting the supporting facts, i.e., evidence sentences. Algorithm. We employ the exact same setup from (Beltagy et al., 2020): We concatenate all the 10 paragraphs into one large seq"
2021.findings-emnlp.225,N16-1174,0,0.0723786,"S2ORC is composed of open access papers across broad domains of science, and PAN is composed of web documents that contain several kinds of plagiarism phenomena. For further dataset prepossessing details and statistics, see Appendix A.3. Algorithm. For our models, we added the [CLS] token at the beginning of the input sequence, assigned it global attention, and concatenated the pair of texts, according to the finetuning setup discussed in Section 2.2. The hyperparameters are further detailed in Appendix C.2. Baselines. We consider the reported results of the following recent baselines: – HAN (Yang et al., 2016) proposed the Hierarchical Attention Networks (HANs). These models employ a bottom-up approach in which a document is represented as an aggregation of smaller components i.e., sentences, and words. They set competitive performance in different tasks involving long document encoding (Sun et al., 2018). – SMASH (Jiang et al., 2019) is an attentive hierarchical recurrent neural network (RNN) model, used for tasks related to long documents. – SMITH (Yang et al., 2020) is a BERT-based hierarchical model, similar HANs. Benchmarks. For citation recommendation, the datasets include the ACL Anthology N"
2021.naacl-main.198,araki-etal-2014-detecting,0,0.0540417,"Missing"
2021.naacl-main.198,P19-1409,1,0.841471,"10 3,808 1,245 1,780 1,527 409 805 411 129 182 1.4 1.4 1.4 2.2 2.3 2.2 GVC (all) 1 7,298 1,411 1,046 19.5 3.0 MEANTIME (all) 4 2,107 1,892 142 2.1 1.5 WEC-Eng (train) WEC-Eng (dev) WEC-Eng (test) Table 1: Event Coreference Datasets Statistics (No train/dev/test split exists for MEANTIME and GVC). NonSingleton Clusters: Number of clusters with more than a single mention. Ambiguity: Average number of different clusters in which a head lemma appears. Diversity: Average number of unique head lemmas within a cluster (excluding singletons for fair comparison). Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019). This corpus consists of documents partitioned into 43 clusters, each corresponding to a certain news topic. In order to introduce some ambiguity and to limit the use of lexical features, each topic is composed of documents describing two different events (called sub-topics) of the same event type (e.g. two different celebrities checking into rehab facilities). Nonetheless, as can be seen in Table 1, the ambiguity level obtained is still rather low. ECB+ is relatively small, where on average only 1.9 sentences per document were selected for annotation, yielding only 722 non-singleton corefere"
2021.naacl-main.198,P10-1143,0,0.0359972,"eristics, while the WEC methodology may be efficiently applied to create additional datasets. To that end, our dataset and code12 are released for open access. 2 Related Datasets This section describes the main characteristics of notable datasets for CD event coreference (ECB+, MEANTIME, GVC). Table 1 presents statistics for all these datasets, as well as ours. We further refer to the Wikilinks dataset, which also leveraged Wikipedia links for CD coreference detection. 2.1 CD Event Corpora ECB+ This dataset (Cybulska and Vossen, 2014), which is an extended version of the EventCorefBank (ECB) (Bejan and Harabagiu, 2010), is the most commonly used dataset for training and testing models for CD event coreference (Choubey and Due to the large scale of the WEC-Eng training data, current state-of-the-art CD coreference models cannot be easily trained and evaluated on 1 WEC–https://github.com/AlonEirew/ it, for scalability reasons. We therefore developed extract-wec 2 a new, more scalable, baseline model for the task, Model–https://github.com/AlonEirew/ while adapting components of recent competitive cross-doc-event-coref 2499 Topics Mentions Clusters Non-Singleton Clusters Ambiguity Diversity - 40,529 1,250 1,893"
2021.naacl-main.198,Q15-1011,0,0.0222506,"it is more suitable for studying event coreference within a narrow domain rather than for investigating models for broad coverage event coreference. 2.2 Wikilinks Wikilinks (Singh et al., 2012) is an automaticallycollected large-scale cross-document coreference dataset, focused on entity coreference. It was constructed by crawling a large portion of the web and collecting as mentions hyperlinks pointing to Wikipedia articles. Since their method does not include mention distillation or validation, it was mostly used for training models for the Entity Linking task, particularly in noisy texts (Chisholm and Hachey, 2015; Eshel et al., 2017). 3 The WEC Methodology and Dataset We now describe our methodology for gathering a CD event coreference dataset from Wikipedia, and the WEC-Eng dataset created by applying it to the English Wikipedia. We also denote how this methodology can be applied, with some languagespecific adjustments, to other Wikipedia languages. Gun Violence Corpus (GVC) This dataset (Vossen et al., 2018) was triggered by the same motivation that drove us, of overcoming the huge complexity of direct manual annotation of CD event coreference from scratch. To create the dataset, the authors leverag"
2021.naacl-main.198,W13-1203,0,0.608241,"Missing"
2021.naacl-main.198,P12-2045,0,0.0326913,"kipedia is not partitioned into predefined topics, mentions can corefer across the entire corpus (unlike most prior datasets). Since mention annotation is not exhaustive, coreference resolution is performed over the gold mentions.Thus, our goal is to support the development of CD event coreference algorithms, rather than of mention extraction algorithms. Our dataset also includes metadata information, such as source and target URLs for the links, but these are not part of the data to be considered by algorithms, as our goal in this work is CD coreference development rather than Event Linking (Nothman et al., 2012). 3.2 Data Collection Process Wikipedia. Event Identification Many Wikipedia articles contain an infobox3 element. This element can be selected by a Wikipedia author from a pre-defined list of possible infobox types (e.g. “Civilian Attack”, “Game”, “Scientist”, etc.), each capturing typical information fields for that type of articles. For example, the “Scientist” infobox type consists of fields such as “birth date”, “awards”, “thesis” etc. We leverage the infobox element and its parameters in order to identify articles describing events (e.g. accident, disaster, conflict, ceremony, etc.) rath"
2021.naacl-main.198,D19-1588,0,0.0654052,"g event coreference cluster, undergoing extensive filtering. We apply our method to the English Wikipedia and extract WEC-Eng, our English version of a WEC dataset. The automaticallyextracted data that we collected provides a training set of a very large scale compared to prior work, while our development and test sets underwent relatively fast manual validation. Figure 1: Example of two anchor texts (100 metres, 100 m final) from different Wikipedia articles (Usain Bolt, Yohan Blake) pointing to the same event. within-document coreference models (Lee et al., 2017; Kantor and Globerson, 2019; Joshi et al., 2019). In addition to setting baseline results for WEC-Eng, we assess our model’s competitiveness by presenting a new state-of-the-art on the commonly used ECB+ dataset. Finally, we propose that our automatic extraction and manual validation methods may be applied to generate additional annotated datasets, particularly for other languages. Overall, we suggest that future cross-document coreference models should be evaluated also on the WEC-Eng dataset, and address its complementary characteristics, while the WEC methodology may be efficiently applied to create additional datasets. To that end, our"
2021.naacl-main.198,W12-4501,0,0.0923085,"Barhom et al. (2019) and apply our coreference model separately on each predicted document cluster. For both datasets, the positive examples for training consist of all the mention pairs in the dataset that belong to the same coreference cluster. For the ECB+ model, we consider only negative examples that belong to the same subtopic, while for WECEng we sample k (tuned to 10) negative examples for each positive one. Results are reported by precision, recall, and F1 for the standard coreference metrics MUC, B3 , CEAF-e, and the average F1 of the three metrics, using the official CoNLL scorer (Pradhan et al., 2012).7 4.3 Results Table 4 presents the results on ECB+. Our model outperforms state-of-the-art results for both the J OINT model and the D ISJOINT event model of Barhom et al. (2019), with a gain of 1.3 CoNLL F 1 points and 2.3 CoNLL F 1 points respectively. The J OINT model jointly clusters event and entity mentions, leveraging information across the two subtasks, while the D ISJOINT event model considers only event mentions, taking the same input as our model. These results assess our model as a suitable baseline for WEC-Eng. 7 https://github.com/conll/ reference-coreference-scorers Table 5 pre"
2021.naacl-main.198,P19-1066,0,0.0990584,"mentions for a corresponding event coreference cluster, undergoing extensive filtering. We apply our method to the English Wikipedia and extract WEC-Eng, our English version of a WEC dataset. The automaticallyextracted data that we collected provides a training set of a very large scale compared to prior work, while our development and test sets underwent relatively fast manual validation. Figure 1: Example of two anchor texts (100 metres, 100 m final) from different Wikipedia articles (Usain Bolt, Yohan Blake) pointing to the same event. within-document coreference models (Lee et al., 2017; Kantor and Globerson, 2019; Joshi et al., 2019). In addition to setting baseline results for WEC-Eng, we assess our model’s competitiveness by presenting a new state-of-the-art on the commonly used ECB+ dataset. Finally, we propose that our automatic extraction and manual validation methods may be applied to generate additional annotated datasets, particularly for other languages. Overall, we suggest that future cross-document coreference models should be evaluated also on the WEC-Eng dataset, and address its complementary characteristics, while the WEC methodology may be efficiently applied to create additional datase"
2021.naacl-main.198,S18-2001,0,0.310502,"B+ (dev) ECB+ (test) 25 8 10 3,808 1,245 1,780 1,527 409 805 411 129 182 1.4 1.4 1.4 2.2 2.3 2.2 GVC (all) 1 7,298 1,411 1,046 19.5 3.0 MEANTIME (all) 4 2,107 1,892 142 2.1 1.5 WEC-Eng (train) WEC-Eng (dev) WEC-Eng (test) Table 1: Event Coreference Datasets Statistics (No train/dev/test split exists for MEANTIME and GVC). NonSingleton Clusters: Number of clusters with more than a single mention. Ambiguity: Average number of different clusters in which a head lemma appears. Diversity: Average number of unique head lemmas within a cluster (excluding singletons for fair comparison). Huang, 2017; Kenyon-Dean et al., 2018; Barhom et al., 2019). This corpus consists of documents partitioned into 43 clusters, each corresponding to a certain news topic. In order to introduce some ambiguity and to limit the use of lexical features, each topic is composed of documents describing two different events (called sub-topics) of the same event type (e.g. two different celebrities checking into rehab facilities). Nonetheless, as can be seen in Table 1, the ambiguity level obtained is still rather low. ECB+ is relatively small, where on average only 1.9 sentences per document were selected for annotation, yielding only 722"
2021.naacl-main.198,D17-1018,0,0.239323,", become candidate mentions for a corresponding event coreference cluster, undergoing extensive filtering. We apply our method to the English Wikipedia and extract WEC-Eng, our English version of a WEC dataset. The automaticallyextracted data that we collected provides a training set of a very large scale compared to prior work, while our development and test sets underwent relatively fast manual validation. Figure 1: Example of two anchor texts (100 metres, 100 m final) from different Wikipedia articles (Usain Bolt, Yohan Blake) pointing to the same event. within-document coreference models (Lee et al., 2017; Kantor and Globerson, 2019; Joshi et al., 2019). In addition to setting baseline results for WEC-Eng, we assess our model’s competitiveness by presenting a new state-of-the-art on the commonly used ECB+ dataset. Finally, we propose that our automatic extraction and manual validation methods may be applied to generate additional annotated datasets, particularly for other languages. Overall, we suggest that future cross-document coreference models should be evaluated also on the WEC-Eng dataset, and address its complementary characteristics, while the WEC methodology may be efficiently applied"
2021.naacl-main.198,C18-1101,0,0.0627841,"Missing"
2021.naacl-main.198,L18-1480,0,0.455012,"ithin predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WECEng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of stateof-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task. 2014), MEANTIME (Minard et al., 2016) and the Gun Violence Corpus (GVC) (Vossen et al., 2018) (described in Section 2), where recent work has been evaluated solely on ECB+. When addressed in a direct manner, manual CD coreference annotation is very hard due to its worst-case quadratic complexity, where each mention may need to be compared to all other mentions in all documents. Indeed, ECB+ contains less than 7000 event mentions in total (train, dev, and test sets). Further, effective corpora for CD event coreference are available mostly for English, limiting research opportunities for other languages. Partly as a result of this data scarcity, rather little effort was invested in this"
2021.naacl-main.198,D19-5813,0,0.0163587,"eneficial for a broad range of (softly) classified into two different types. One applications at the multi-text level, which are gain- type, which we term a descriptive mention, pering increasing interest and need to match and inte- tains to a mention involved in presenting the event grate information across documents, such as multi- or describing new information about it. For examdocument summarization (Falke et al., 2017; Liao ple, news about the Malaysian Airline crash will et al., 2018), multi-hop question answering (Dhin- include mostly descriptive mentions of the event gra et al., 2018; Wang et al., 2019) and Knowledge and its sub-events, such as shot-down, crashed and Base Population (KBP) (Lin et al., 2020). investigated. Naturally, news documents about a Unfortunately, rather few datasets of reason- topic, as in prior event coreference datasets, include able scale exist for CD event coreference. No- mostly descriptive event mentions. The other type, table datasets include ECB+ (Cybulska and Vossen, which we term a referential mention, pertains to 2498 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologie"
2021.naacl-main.54,D19-1410,0,0.0306572,"Missing"
2021.naacl-main.54,2020.acl-main.626,1,0.812095,"ork are: (1) scores are absolute and comparable from one session/system to another; (2) our framework fundamentally and conveniently extends upon prevailing static summarization evaluation practices and utilizes existing standard MDS dataset reference summaries. 4 Session Collection The evaluation of interactive systems requires real user sessions, as explained in §3. Using a prototype I NT S UMM system, described in §5.1, we conducted several cycles of session collection which uncovered multiple user-related challenges, in line with previous work on user task design (Christmann et al., 2019; Roit et al., 2020; Zuccon et al., 2013). In particular, recruited users may make undue interactions due to insincere or experimental behavior, yielding noisy sessions that do not reflect realistic system use. Additionally, without an objective informational goal, a user interacts with the system according to subjective interests, producing sessions that are objectively incomparable. efficiently filter out insincere workers, and, conversely, discover workers with an ability to apprehend salient information within text. The second stage assigns practice tasks that familiarize the workers to the I NT S UMM system"
2021.naacl-main.54,W17-1003,0,0.06858,"Missing"
2021.naacl-main.54,N19-1072,1,0.829875,"ents, we simulate each of our two systems on scripted query lists. Simulated sessions provide a means for quick development cycles and quality estimation. The first of two query lists, LSug , is constructed fully automatically: it consists of the top-10 ordered phrases in the system’s suggested queries component per topic. This mimics a “lower bound” user who adopts the simplest strategy, namely, clicking the suggested queries in order without using judgment even to choose among these queries. The second list, LOracle , consists of 10 randomly chosen crowdsourced summary content units (SCUs) (Shapira et al., 2019) for each of the topics. Since the SCUs were extracted from the reference summaries of the corresponding topics, they mimic a user who searches for the exact information required to maximize similarity to the same reference summaries which we then evaluate against. While this is not necessarily the optimal query list due to the randomized sampling of SCUs for queries, we consider it our (non-strict) “upper bound” for the sake of experimentation. The two “bounds” are relative to the system on which the simulations are carried on. Also, for fair comparison to real sessions, the simulation initia"
2021.naacl-main.54,D18-1087,1,0.834263,"y snapshots, we would first like to obtain comparable scores for each static summary that will capture the information gained along the session up to the current interaction. Existing static MDS benchmarks provide reference summaries at a single length for the purpose of evaluating a summary at a similar length. This presumably means we would require a series of reference summaries that differ by small length gaps for the sequence of lengthening snapshots, which is difficult and costly to produce. To address this obstacle, We address all of the above-mentioned evalua- we leverage a finding by Shapira et al. (2018) show659 0.47 0.37 0.32 0.27 Upper intersection Lower intersection ROUGE-1 Recall 0.42 0.22 Topic 1 0.17 Topic 2 0.12 70 120 170 220 270 320 Word Length 370 420 470 Figure 2: Example recall-curves of two sessions on an I NT S UMM system. Points plotted per interaction snapshot within a session. Range of intersection between observed summary lengths is bounded by dashed lines. ing that a reference summary of a single length can be used to relatively evaluate varying length summaries on a topic with a recall measure such as ROUGE. Thus, utilizing existing MDS datasets is indeed possible for meas"
2021.naacl-main.54,D11-1124,0,0.0383304,"Missing"
2021.naacl-main.54,K17-1045,0,0.020812,"puts must comply to required interuation framework and demonstrate its utility. As action latency standards (Anderson, 2020; Attig the few existing I NT S UMM systems were not readet al., 2017), e.g., a few seconds for the initial sumily available or suitable for adaptation to our expermary and a few hundred milliseconds for a query imental setup, we developed an I NT S UMM system response. While we experimented with some more of our own, shown in Figure 1, with two different advanced techniques for MDS generation (e.g., algorithmic implementations for comparison. We Christensen et al., 2013; Yasunaga et al., 2017), gathered user sessions with our controlled crowdsentence representation (Reimers and Gurevych, sourcing procedure and evaluated their quality with 2019) and sentence similarity (Zhang* et al., 2020), 3 https://www.mturk.com we found that these are not practical for incorpo662 ration within the interactive low-latency setting, or that they could not handle the relatively large document set inputs. Instead, we developed the two back-end schemas described next (with further details in Appendix A). S1 runs a sentence clustering initial summary algorithm. Query-responses are generated in MMRstyle"
2021.naacl-main.54,N18-1152,0,0.0268761,"e on an earlier set (e.g. Li et al., 2008; Wang and Li, trolled crowdsourcing procedure that overcomes 2010; McCreadie et al., 2014; Zopf et al., 2016). the above obstacles, making the evaluation process Evaluation approaches predominantly include autoreliable and much more accessible for researchers interested in pursuing I NT S UMM research. See §4. matic ROUGE (Lin, 2004) measurement, i.e. word overlap against reference summaries, and manual We demonstrate the use of our full evaluation responsiveness (Dang, 2006) scores or pairwise framework on two I NT S UMM systems that we imcomparison (Zopf, 2018) between summaries. plemented, which apply different algorithms but 1 share a common user interface, with the DUC https://github.com/OriShapira/ InterExp 2006 (Dang, 2006) MDS dataset. Analysis shows 658 In the related QA task (Voorhees et al., 1999), a system extracts an answer for a targeted question. Similarly, in the interactive setting, a conversational QA (Reddy et al., 2019) system extracts answers to a series of interconnected questions with a clear informational goal. To check correctness in both cases, a system answer is simply compared to the true answer via text-comparison. On the"
2021.naacl-main.54,C16-1102,0,0.0244943,"are less replica- and Marcu, 2006; Zhao et al., 2009; Cao et al., ble, not scalable, and not always easily attainable. 2016; Feigenblat et al., 2017; Baumel et al., 2018), and incremental update summarization (Dang and In contrast, standard crowdsourcing induces noise Owczarzak, 2008), generating a summary of a docand overly tolerates subjective behavior, hindering replicability and comparability. We describe a con- ument set with the assumption of prior knowledge on an earlier set (e.g. Li et al., 2008; Wang and Li, trolled crowdsourcing procedure that overcomes 2010; McCreadie et al., 2014; Zopf et al., 2016). the above obstacles, making the evaluation process Evaluation approaches predominantly include autoreliable and much more accessible for researchers interested in pursuing I NT S UMM research. See §4. matic ROUGE (Lin, 2004) measurement, i.e. word overlap against reference summaries, and manual We demonstrate the use of our full evaluation responsiveness (Dang, 2006) scores or pairwise framework on two I NT S UMM systems that we imcomparison (Zopf, 2018) between summaries. plemented, which apply different algorithms but 1 share a common user interface, with the DUC https://github.com/OriShap"
2021.starsem-1.13,P98-1012,0,0.749714,"Missing"
2021.starsem-1.13,P19-1409,1,0.929146,"reference introduces additional unique challenges. Most notably, lexical similarity is often not a good indicator when identifying cross-document links, as documents are authored independently. As shown in Table 1, the same event can be referenced using different expressions (“nominated”, “approached”), while two different events can be referenced using the same expression (“name”). Despite these challenges, reported state-of-the-art results on the 1 https://github.com/ariecattan/coref popular CD coreference ECB+ benchmark (Cybulska and Vossen, 2014) are relatively high, reaching up to 80 F1 (Barhom et al., 2019; Meged et al., 2020). In this paper, we show that CD coreference models achieve these numbers using overly-permissive evaluation protocols, namely assuming gold entity and event mentions are given, rewarding singletons and bypassing the lexical ambiguity challenge. Accordingly, we present more realistic evaluation principles which better reflect model performance in real-world scenarios. First, following well established standards in WD coreference resolution (Pradhan et al., 2012), we propose that CD coreference models should be also evaluated on predicted mentions. While recent models unrea"
2021.starsem-1.13,J14-2004,0,0.138225,"orld event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that gold entity and event mentions are given as part of the input, reducing the task to finding coreference links between gold mentions (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Kenyon-Dean et al., 2018; Barhom et al., 2019; Meged et al., 2020). Second, while singletons are omitted on CoNLL-2012, they are exhaustively annotated in ECB+. In the following section, we present a more realistic evaluation framework for CD coreference, taking into account the interacting distinctions of ECB+. 3 Realistic Evaluation Principles In this paper, we suggest that CD coreference models should perform and be evaluated on predicted mentions. To achieve this, in Section 3.1, we will introduce the singleton effect on coreference evaluation and propose to de"
2021.starsem-1.13,P10-1143,0,0.301535,"e propose that models report performance also at the topic level. Finally, we show empirically that both of these evaluation practices artificially inflate results. An end-to end model that outperforms state-of-the-art results on previous evaluation settings drops by 33 F1 points when using our proposed evaluation scheme, pointing at weaknesses that future modelling work could explore. 2 Background In this work, we will examine the evaluation of CD coreference on the popular ECB+ corpus (Cybulska and Vossen, 2014), constructed as an augmentation of the EECB and ECB datasets (Lee et al., 2012; Bejan and Harabagiu, 2010). As exemplified in Table 1, ECB+ groups its annotated documents into subtopics, consisting of different reports of the same real-world event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that go"
2021.starsem-1.13,2021.findings-acl.453,1,0.690881,"original evaluation setup of the ECB+ corpus (Bejan and Harabagiu, 2014). 4 clusion, and subtopic clustering) artificially inflates the results (§4.2). As recent CD coreference models are designed to perform on gold mentions (§2), we cannot use them to set baseline results on predicted mentions. We therefore develop a simple and efficient end-to-end model for CD coreference resolution by combining the successful single document e2e-coref (Lee et al., 2017) with common CD modeling approaches. 4.1 Model We briefly describe the general architecture of our model, further details are explained in (Cattan et al., 2021) and Appendix C. Given a set of documents, our model operates in four sequential steps: (1) following Lee et al. (2017), we encode all possible spans up to a length n with the concatenation of four vectors: the output representations of the span boundary (first and last) tokens, an attentionweighted sum of token representations in the span, and a feature vector denoting the span length (2) we train a mention detector on the ECB+ mentions, and keep further spans with a positive score,5 (3) we generate positive and negative coreference pairs on the predicted mentions and train a pairwise scorer,"
2021.starsem-1.13,D19-1588,1,0.874627,"Missing"
2021.starsem-1.13,S18-2001,0,0.270421,"topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unrealistically assumed that gold entity and event mentions are given as part of the input, reducing the task to finding coreference links between gold mentions (Bejan and Harabagiu, 2014; Cybulska and Vossen, 2015; Kenyon-Dean et al., 2018; Barhom et al., 2019; Meged et al., 2020). Second, while singletons are omitted on CoNLL-2012, they are exhaustively annotated in ECB+. In the following section, we present a more realistic evaluation framework for CD coreference, taking into account the interacting distinctions of ECB+. 3 Realistic Evaluation Principles In this paper, we suggest that CD coreference models should perform and be evaluated on predicted mentions. To achieve this, in Section 3.1, we will introduce the singleton effect on coreference evaluation and propose to decouple the evaluation of mention prediction from core"
2021.starsem-1.13,D12-1045,0,0.178213,"To address this, we propose that models report performance also at the topic level. Finally, we show empirically that both of these evaluation practices artificially inflate results. An end-to end model that outperforms state-of-the-art results on previous evaluation settings drops by 33 F1 points when using our proposed evaluation scheme, pointing at weaknesses that future modelling work could explore. 2 Background In this work, we will examine the evaluation of CD coreference on the popular ECB+ corpus (Cybulska and Vossen, 2014), constructed as an augmentation of the EECB and ECB datasets (Lee et al., 2012; Bejan and Harabagiu, 2010). As exemplified in Table 1, ECB+ groups its annotated documents into subtopics, consisting of different reports of the same real-world event (e.g., the nomination of Sanjay Gupta), and topics, which in turn consist of two lexically similar subtopics. Full ECB+ details are presented in Appendix A. The ECB+ evaluation protocol largely follows that of CoNLL-2012, perhaps the most popular WD benchmark (Pradhan et al., 2012), with two major distinctions. First, barring a few notable exceptions (Yang et al., 2015; Choubey and Huang, 2017),2 most recent CD models have unr"
2021.starsem-1.13,D17-1018,0,0.0501395,"recommend that models report results also at the topic level (when document clustering is not applied). This will conform to ECB+’s purpose and follows the original evaluation setup of the ECB+ corpus (Bejan and Harabagiu, 2014). 4 clusion, and subtopic clustering) artificially inflates the results (§4.2). As recent CD coreference models are designed to perform on gold mentions (§2), we cannot use them to set baseline results on predicted mentions. We therefore develop a simple and efficient end-to-end model for CD coreference resolution by combining the successful single document e2e-coref (Lee et al., 2017) with common CD modeling approaches. 4.1 Model We briefly describe the general architecture of our model, further details are explained in (Cattan et al., 2021) and Appendix C. Given a set of documents, our model operates in four sequential steps: (1) following Lee et al. (2017), we encode all possible spans up to a length n with the concatenation of four vectors: the output representations of the span boundary (first and last) tokens, an attentionweighted sum of token representations in the span, and a feature vector denoting the span length (2) we train a mention detector on the ECB+ mention"
2021.starsem-1.13,2021.ccl-1.108,0,0.0850703,"Missing"
2021.starsem-1.13,H05-1004,0,0.113638,"tons are already evaluated under the mention detection evaluation. We note also that even when omitting singletons, coreference metrics still penalize models for making coreference errors involving singletons (as S2 is penalized for linking “announcement” to a cluster). We further show empirically (§4.2) that when evaluating using gold mentions, the singleton effect is amplified and harms the validity of the current CD evaluation protocol. Evidently, a dummy baseline that predicts no coreference links and puts each input gold mention in a singleton cluster achieves non-negligible performance (Luo, 2005), while state-of-the-art results are artificially inflated. 3.2 Confronting Lexical Ambiguity As mentioned previously, the same event can be described in documents from different topics, while documents in the same topic may describe different events (e.g. different nominations as surgeon general, as shown in Table 1). Such settings pose a lexical ambiguity problem, where models encounter identical or lexically-similar words that should be assigned to different coreference clusters. Accordingly, while topical document clustering is useful for CD coreference resolution in general, it does not s"
2021.starsem-1.13,D09-1101,0,0.0515912,"and (2) using coreference metrics also on singleton prediction. With respect to (1), S2 achieves higher results according to all evaluation metrics. In (2), we see the opposite, the results of S1 are significantly higher than S2 w.r.t B3 (+18.4), CEAF-e (+45.1), and LEA (+19), but not w.r.t MUC, a link-based metric. Indeed, these evaluation metrics reward S1 in both recall and precision for all predicted singletons, while penalizing S2 for the wrong and missing singleton spans. Since singletons are abundant in natural text, they contribute greatly to the overall score. However, as observed by Rahman and Ng (2009), a model’s ability to identify that these singletons do not belong to any coreference cluster is already captured in the evaluation metrics, and additional penalty is not desired. In Appendix B, we introduce the aforementioned evaluation metrics for coreference resolution (MUC, B3 , CEAF and LEA) and explain how singletons affect them. To address the singleton effect, we suggest decoupling the evaluation of the two coreference substasks, mention detection and coreference linking, allowing to better analyze coreference results and to compare systems more appropriately.4 Mention detection is ty"
2021.starsem-1.13,P09-1074,0,0.059586,"ttings pose a lexical ambiguity problem, where models encounter identical or lexically-similar words that should be assigned to different coreference clusters. Accordingly, while topical document clustering is useful for CD coreference resolution in general, it does not solve the ambiguity problem and models still need to make subtle disambiguation distinctions (e.g nomination of Sanjay Gupta vs. nomination of Regina Benjamin). Aiming at simulating this chal145 4 This also makes possible to compare coreference results across datasets that include/omit singletons, addressing an issue raised by Stoyanov et al. (2009). B3 MUC Subtopic Clustering Topic Level CEAF e LEA CoNLL R P F1 R P F1 R P F1 R P F1 F1 Singleton baseline+ Singleton baseline− Barhom et al. (2019)+ Barhom et al. (2019)− Meged et al. (2020)+ Meged et al. (2020)− 0 0 78.1 78.1 78.8 78.8 0 0 84.0 84.0 84.7 84.7 0 0 80.9 80.9 81.6 81.6 45.2 0 76.8 61.2 75.9 60.4 100 0 86.1 73.5 85.9 73.8 62.3 0 81.2 66.8 80.6 66.4 86.7 0 79.6 63.2 81.1 65.5 39.2 0 73.3 48.9 74.8 49.5 54.0 0 76.3 55.2 77.8 56.4 35.0 0 64.6 58.4 64.7 57.2 35.0 0 72.3 71.2 73.4 71.2 35.0 0 68.3 64.2 68.8 63.4 38.8 0 79.5 67.6 80.0 68.1 Our model – Gold+ Our model – Gold− Our mode"
2021.starsem-1.13,M95-1005,0,0.860963,"Missing"
2021.starsem-1.28,W04-3221,0,0.189007,"emantic benchmarks. Best performance is bolded. nal spaces. Note that we did not compare other proposed embeddings or meta-embedding learning methods, but rather restricted our analysis to empirically verifying our embedding aggregation method and validating the assumptions behind the empirical analysis we performed. 3.4 Evaluations on Lexical Semantic Tasks We evaluated the performance of our method over lexical-semantic tasks, including word similarity, analogy solving, and concept categorization: SimLex999 (Hill et al., 2015), MEN (Bruni et al., 2014), WS353 (Finkelstein et al., 2002), AP (Almuhareb and Poesio, 2004), Google (Mikolov et al., 2013b), MSR (Mikolov et al., 2013c), SemEval-2012 (Jurgens et al., 2012), BLESS (Baroni and Lenci, 2011) and RW (Luong et al., 2013), (focusing on rare words). For the analogy task, we reported the accuracy. For the remaining tasks, we computed Spearman’s correlation between the cosine similarity of the embeddings and the human judgments. Results The results of the lexical-semantic tasks are depicted in Table 2, averaged over 30 runs for each method. Our method obtained better performance than the other methods, substantially for FastText embeddings. As shown, the na¨"
2021.starsem-1.28,D16-1250,0,0.0234046,", yn } in the shared space that minimize the following mean-squared error: S(T, y) = k X n X kTi xi,t − yt k2 . (1) i=1 t=1 For this objective, it is easy to show that for a set of transformations T1 , ..., Tk , the optimal shared space representation is: yt = k 1X Ti xi,t . k i=1 Hence, solving the optimization problem pertains to finding the k optimal transformations. In the case where k = 2, the optimal T can be obtained in a closed form using the Procrustes Analysis (PA) procedure (Sch¨onemann, 1966), which has been employed in recent bilingual word translation methods (Xing et al., 2015; Artetxe et al., 2016; Hamilton et al., 2016; Artetxe et al., 2017a,b; Conneau et al., 2017; Artetxe et al., 2018a,b; Ruder et al., 2018). In our setting, to obtain an improved embedding, we wish to average more than two embedding sets. S(Ti ) = n X kTi xi,t − yt k2 , i = 1, ..., k. t=1 The minimum of S(Ti ) can then be found by the closed form PA procedure. The updated transformation is Ti = Ui Vi> , where Ui Σi Vi> is the singularP value decomposition (SVD) of the d × d matrix nt=1 yt x> i,t . At each step in the iterative GPA algorithm, the score (1) is monotonically decreased until it converges to a local mini"
2021.starsem-1.28,P17-1042,0,0.0272348,"following mean-squared error: S(T, y) = k X n X kTi xi,t − yt k2 . (1) i=1 t=1 For this objective, it is easy to show that for a set of transformations T1 , ..., Tk , the optimal shared space representation is: yt = k 1X Ti xi,t . k i=1 Hence, solving the optimization problem pertains to finding the k optimal transformations. In the case where k = 2, the optimal T can be obtained in a closed form using the Procrustes Analysis (PA) procedure (Sch¨onemann, 1966), which has been employed in recent bilingual word translation methods (Xing et al., 2015; Artetxe et al., 2016; Hamilton et al., 2016; Artetxe et al., 2017a,b; Conneau et al., 2017; Artetxe et al., 2018a,b; Ruder et al., 2018). In our setting, to obtain an improved embedding, we wish to average more than two embedding sets. S(Ti ) = n X kTi xi,t − yt k2 , i = 1, ..., k. t=1 The minimum of S(Ti ) can then be found by the closed form PA procedure. The updated transformation is Ti = Ui Vi> , where Ui Σi Vi> is the singularP value decomposition (SVD) of the d × d matrix nt=1 yt x> i,t . At each step in the iterative GPA algorithm, the score (1) is monotonically decreased until it converges to a local minimum point. Algorithm 1 Shared Space Embedding"
2021.starsem-1.28,P18-1073,0,0.0290454,"Missing"
2021.starsem-1.28,J82-2005,0,0.589632,"Missing"
2021.starsem-1.28,2020.emnlp-main.284,1,0.843768,"Missing"
2021.starsem-1.28,2020.acl-main.356,1,0.839527,"corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et al., 2017). While word embedding methods proved to be powerful, they suffer from a certain level of noise, introduced by quite a few randomized steps in the embedding generation process, including embedding initialization, negative sampling, subsampling and mini-batch ordering. Consequently, differe"
2021.starsem-1.28,2021.findings-emnlp.225,1,0.708166,"hat simply averaging different embedding spaces does not improve word representation quality. The most notable performance gain was in the rare-words task, in line with the analysis in Fig. 1, suggesting that on rare words the raw embedding vectors fit the data less accurately. 3.5 Evaluations On Downstream Tasks For completeness, we next show the relative advantage of our denoising method also when applied to several sentence-level downstream benchmarks. While contextualized embeddings dominate a wide range of sentence- and document- level NLP tasks (Peters et al., 2018; Devlin et al., 2019; Caciularu et al., 2021), we assessed the relative advantage of our denoising method when utilizing (non-contextualized) word embeddings in sentencean document- level settings. We applied the exact procedure proposed in Li et al. (2017) and Rogers et al. (2018), as an effective benchmark for the quality of static embedding models. We first used sequence labeling tasks. The morphological and syntactic performance was evaluated using part-of-speech tagging, POS, and chunking, CHK. Both named entity recognition, NER, and multiway classification of semantic relation classes, RE, tasks were used for evaluating semantic in"
2021.starsem-1.28,D18-1024,0,0.0883515,"e fused effectively, in order to obtain a model with a reduced level of noise. Note, however, that simple averaging of the original word vectors is problematic, since each training session of the algorithm produces embeddings in a different space. In fact, the objective scores of both word2vec, Glove and FastText are invariant to multiplying all the word embeddings by an orthogonal matrix, hence, the algorithm output involves an arbitrary rotation of the embedding space. For addressing this issue, we were inspired by recent approaches originally proposed for aligning multi-lingual embeddings (Chen and Cardie, 2018; Kementchedjhieva et al., 2018; Alaux et al., 2019; Jawanpuria et al., 2019; Taitelbaum et al., 2019). To obtain such alignments, these methods simultaneously project the original language-specific embeddings into a shared space, while enforcing (or at least encouraging) transitive orthogonal transformations. In our (monolingual) setting, we propose a related technique to project the different embedding versions into a shared space, while optimizing the projection towards obtaining an improved fused representation. We show that this results in im294 Proceedings of the 10th Conference on Lexic"
2021.starsem-1.28,N18-2031,0,0.013057,"ictly orthogonal is too restrictive and performance can be improved by using the orthogonalization as a regularization (Chen and Cardie, 2018) that yields matrices that are close to be orthogonal. In our much simpler setting of a single language, with a trivial identity word correspondence, enforcing the orthogonalization constraint is reasonable. Another related problem is meta-embedding (Yin and Sch¨utze, 2016), which aims to fuse information from different embedding models. Various methods have been proposed for embedding fusion, such as concatenation, simple averaging, weighted averaging (Coates and Bollegala, 2018; Kiela et al., 2018) and autoencoding (Bollegala and Bao, 2018). Some of these methods (concatenation and autoencoding) are not scalable when the goal is to fuse many sets, while others (simple averaging) yield inferior results, as described in the above works. Note that our method is not intended to be a competitor of meta-embedding, but rather a complementary method. An additional related work is the recent method from (Murom¨agi et al., 2017). Similarly to our work, they proposed a method based on the Procrustes Analysis procedure for aligning and averaging sets of word embedding models. H"
2021.starsem-1.28,N19-1423,0,0.130624,"are word evaluations. 1 Introduction Continuous (non-contextualized) word embeddings have been introduced several years ago as a standard building block for NLP tasks. These models provide efficient ways to learn word representations in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et al., 2017). While word"
2021.starsem-1.28,P16-1141,0,0.0270528,"pace that minimize the following mean-squared error: S(T, y) = k X n X kTi xi,t − yt k2 . (1) i=1 t=1 For this objective, it is easy to show that for a set of transformations T1 , ..., Tk , the optimal shared space representation is: yt = k 1X Ti xi,t . k i=1 Hence, solving the optimization problem pertains to finding the k optimal transformations. In the case where k = 2, the optimal T can be obtained in a closed form using the Procrustes Analysis (PA) procedure (Sch¨onemann, 1966), which has been employed in recent bilingual word translation methods (Xing et al., 2015; Artetxe et al., 2016; Hamilton et al., 2016; Artetxe et al., 2017a,b; Conneau et al., 2017; Artetxe et al., 2018a,b; Ruder et al., 2018). In our setting, to obtain an improved embedding, we wish to average more than two embedding sets. S(Ti ) = n X kTi xi,t − yt k2 , i = 1, ..., k. t=1 The minimum of S(Ti ) can then be found by the closed form PA procedure. The updated transformation is Ti = Ui Vi> , where Ui Σi Vi> is the singularP value decomposition (SVD) of the d × d matrix nt=1 yt x> i,t . At each step in the iterative GPA algorithm, the score (1) is monotonically decreased until it converges to a local minimum point. Algorithm 1"
2021.starsem-1.28,Q17-1010,0,0.224233,"representations are more stable and reliable, there is a noticeable improvement in rare word evaluations. 1 Introduction Continuous (non-contextualized) word embeddings have been introduced several years ago as a standard building block for NLP tasks. These models provide efficient ways to learn word representations in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state"
2021.starsem-1.28,S10-1006,0,0.0517963,"Missing"
2021.starsem-1.28,C18-1140,0,0.0189335,"d by using the orthogonalization as a regularization (Chen and Cardie, 2018) that yields matrices that are close to be orthogonal. In our much simpler setting of a single language, with a trivial identity word correspondence, enforcing the orthogonalization constraint is reasonable. Another related problem is meta-embedding (Yin and Sch¨utze, 2016), which aims to fuse information from different embedding models. Various methods have been proposed for embedding fusion, such as concatenation, simple averaging, weighted averaging (Coates and Bollegala, 2018; Kiela et al., 2018) and autoencoding (Bollegala and Bao, 2018). Some of these methods (concatenation and autoencoding) are not scalable when the goal is to fuse many sets, while others (simple averaging) yield inferior results, as described in the above works. Note that our method is not intended to be a competitor of meta-embedding, but rather a complementary method. An additional related work is the recent method from (Murom¨agi et al., 2017). Similarly to our work, they proposed a method based on the Procrustes Analysis procedure for aligning and averaging sets of word embedding models. However, the mapping algorithm they used is much more computation"
2021.starsem-1.28,J15-4004,0,0.329934,"nt ways to learn word representations in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et al., 2017). While word embedding methods proved to be powerful, they suffer from a certain level of noise, introduced by quite a few randomized steps in the embedding generation process, including embedding initializa"
2021.starsem-1.28,W11-2501,0,0.043826,"earning methods, but rather restricted our analysis to empirically verifying our embedding aggregation method and validating the assumptions behind the empirical analysis we performed. 3.4 Evaluations on Lexical Semantic Tasks We evaluated the performance of our method over lexical-semantic tasks, including word similarity, analogy solving, and concept categorization: SimLex999 (Hill et al., 2015), MEN (Bruni et al., 2014), WS353 (Finkelstein et al., 2002), AP (Almuhareb and Poesio, 2004), Google (Mikolov et al., 2013b), MSR (Mikolov et al., 2013c), SemEval-2012 (Jurgens et al., 2012), BLESS (Baroni and Lenci, 2011) and RW (Luong et al., 2013), (focusing on rare words). For the analogy task, we reported the accuracy. For the remaining tasks, we computed Spearman’s correlation between the cosine similarity of the embeddings and the human judgments. Results The results of the lexical-semantic tasks are depicted in Table 2, averaged over 30 runs for each method. Our method obtained better performance than the other methods, substantially for FastText embeddings. As shown, the na¨ıve averaging performed poorly, which highlights the fact that simply averaging different embedding spaces does not improve word r"
2021.starsem-1.28,Q19-1007,0,0.0179859,"ise. Note, however, that simple averaging of the original word vectors is problematic, since each training session of the algorithm produces embeddings in a different space. In fact, the objective scores of both word2vec, Glove and FastText are invariant to multiplying all the word embeddings by an orthogonal matrix, hence, the algorithm output involves an arbitrary rotation of the embedding space. For addressing this issue, we were inspired by recent approaches originally proposed for aligning multi-lingual embeddings (Chen and Cardie, 2018; Kementchedjhieva et al., 2018; Alaux et al., 2019; Jawanpuria et al., 2019; Taitelbaum et al., 2019). To obtain such alignments, these methods simultaneously project the original language-specific embeddings into a shared space, while enforcing (or at least encouraging) transitive orthogonal transformations. In our (monolingual) setting, we propose a related technique to project the different embedding versions into a shared space, while optimizing the projection towards obtaining an improved fused representation. We show that this results in im294 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 294–301 August 5–6, 2021, Bangkok, Tha"
2021.starsem-1.28,S12-1047,0,0.136189,"ons in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et al., 2017). While word embedding methods proved to be powerful, they suffer from a certain level of noise, introduced by quite a few randomized steps in the embedding generation process, including embedding initialization, negative sampling, subsampling"
2021.starsem-1.28,K18-1021,0,0.0218195,"order to obtain a model with a reduced level of noise. Note, however, that simple averaging of the original word vectors is problematic, since each training session of the algorithm produces embeddings in a different space. In fact, the objective scores of both word2vec, Glove and FastText are invariant to multiplying all the word embeddings by an orthogonal matrix, hence, the algorithm output involves an arbitrary rotation of the embedding space. For addressing this issue, we were inspired by recent approaches originally proposed for aligning multi-lingual embeddings (Chen and Cardie, 2018; Kementchedjhieva et al., 2018; Alaux et al., 2019; Jawanpuria et al., 2019; Taitelbaum et al., 2019). To obtain such alignments, these methods simultaneously project the original language-specific embeddings into a shared space, while enforcing (or at least encouraging) transitive orthogonal transformations. In our (monolingual) setting, we propose a related technique to project the different embedding versions into a shared space, while optimizing the projection towards obtaining an improved fused representation. We show that this results in im294 Proceedings of the 10th Conference on Lexical and Computational Semantics,"
2021.starsem-1.28,D18-1176,0,0.0177782,"rictive and performance can be improved by using the orthogonalization as a regularization (Chen and Cardie, 2018) that yields matrices that are close to be orthogonal. In our much simpler setting of a single language, with a trivial identity word correspondence, enforcing the orthogonalization constraint is reasonable. Another related problem is meta-embedding (Yin and Sch¨utze, 2016), which aims to fuse information from different embedding models. Various methods have been proposed for embedding fusion, such as concatenation, simple averaging, weighted averaging (Coates and Bollegala, 2018; Kiela et al., 2018) and autoencoding (Bollegala and Bao, 2018). Some of these methods (concatenation and autoencoding) are not scalable when the goal is to fuse many sets, while others (simple averaging) yield inferior results, as described in the above works. Note that our method is not intended to be a competitor of meta-embedding, but rather a complementary method. An additional related work is the recent method from (Murom¨agi et al., 2017). Similarly to our work, they proposed a method based on the Procrustes Analysis procedure for aligning and averaging sets of word embedding models. However, the mapping a"
2021.starsem-1.28,Q15-1016,1,0.731877,"f the vocabulary size, allowing us to compute efficiently the SVD. The resulting algorithm termed Shared Space Embedding Averaging (SSEA) is presented in Algorithm 1.1 3 Experimental Setup and Results This section presents our evaluation protocol, datasets, data preparation, hyperparameter configuration and results. 3.1 Implementation Details and Data We trained word2vec (Mikolov et al., 2013a), FastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014) embeddings. For word2vec we used the skip-gram model with negative sampling, which was shown advantageous on the evaluated tasks (Levy et al., 2015). We trained each of the models on the November 2019 dump of Wikipedia articles2 for k = 30 times, with different random seeds, and used the default reported hyperparameters; we set the embedding dimension to d = 200, and considered each word within the maximal window cmax = 5, subsampling3 threshold of ρ = 10−5 and used 5 negative examples for every positive example. In order to keep a large amount of rare words in the corpus, no preprocessing was applied on the data, yielding a vocabulary size of 1.5 · 106 . We then applied the SSEA algorithm to the embedding sets to obtain the average embed"
2021.starsem-1.28,D17-1257,0,0.0356301,"Missing"
2021.starsem-1.28,W13-3512,0,0.0549441,"ricted our analysis to empirically verifying our embedding aggregation method and validating the assumptions behind the empirical analysis we performed. 3.4 Evaluations on Lexical Semantic Tasks We evaluated the performance of our method over lexical-semantic tasks, including word similarity, analogy solving, and concept categorization: SimLex999 (Hill et al., 2015), MEN (Bruni et al., 2014), WS353 (Finkelstein et al., 2002), AP (Almuhareb and Poesio, 2004), Google (Mikolov et al., 2013b), MSR (Mikolov et al., 2013c), SemEval-2012 (Jurgens et al., 2012), BLESS (Baroni and Lenci, 2011) and RW (Luong et al., 2013), (focusing on rare words). For the analogy task, we reported the accuracy. For the remaining tasks, we computed Spearman’s correlation between the cosine similarity of the embeddings and the human judgments. Results The results of the lexical-semantic tasks are depicted in Table 2, averaged over 30 runs for each method. Our method obtained better performance than the other methods, substantially for FastText embeddings. As shown, the na¨ıve averaging performed poorly, which highlights the fact that simply averaging different embedding spaces does not improve word representation quality. The m"
2021.starsem-1.28,P11-1015,0,0.0759355,"ognition, NER, and multiway classification of semantic relation classes, RE, tasks were used for evaluating semantic information at the word level. For the above POS, NER and CHK sequence labeling tasks, we used the CoNLL 2003 dataset (Sang and Meulder, 2003) and for the RE task, we used the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The neural network models employed for these downstream tasks are fully described in (Rogers et al., 2018). Next, we evaluated the following semantic level tasks: document-level polarity classification, PC, using the Stanford IMDB movie review dataset (Maas et al., 2011), sentence level sentiment polarity classification, SEN, using the MR dataset of short movie reviews (Pang and Lee, 2005), and classification of subjectivity and objectivity task, SUB, that uses the Rotten Tomatoes user review snippets against official movie plot summaries (Pang and Lee, 2004). Similarly to the performance results in Table 2, the current results show that the suggested denoised embeddings obtained better overall performance than the other methods, substantially for FastText embeddings. 4 Related Work A similar situation of aligning different word embeddings into a shared space"
2021.starsem-1.28,2020.findings-emnlp.440,1,0.815523,". A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et al., 2017). While word embedding methods proved to be powerful, they suffer from a certain level of noise, introduced by quite a few randomized steps in the embedding generation process, including embedding initialization, negative sampling, subsampling and mini-batch ordering. Consequently, different runs would yield different embedding geometries, of"
2021.starsem-1.28,N13-1090,0,0.762338,"s as well as their simplistic average, on a range of tasks. As the new representations are more stable and reliable, there is a noticeable improvement in rare word evaluations. 1 Introduction Continuous (non-contextualized) word embeddings have been introduced several years ago as a standard building block for NLP tasks. These models provide efficient ways to learn word representations in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextua"
2021.starsem-1.28,W17-0212,0,0.042849,"Missing"
2021.starsem-1.28,P04-1035,0,0.0123017,"SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The neural network models employed for these downstream tasks are fully described in (Rogers et al., 2018). Next, we evaluated the following semantic level tasks: document-level polarity classification, PC, using the Stanford IMDB movie review dataset (Maas et al., 2011), sentence level sentiment polarity classification, SEN, using the MR dataset of short movie reviews (Pang and Lee, 2005), and classification of subjectivity and objectivity task, SUB, that uses the Rotten Tomatoes user review snippets against official movie plot summaries (Pang and Lee, 2004). Similarly to the performance results in Table 2, the current results show that the suggested denoised embeddings obtained better overall performance than the other methods, substantially for FastText embeddings. 4 Related Work A similar situation of aligning different word embeddings into a shared space occurs in multi-lingual 297 Method POS CHK NER RE PC SEN SUB word2vec 81.5 80.1 93.3 71.4 89.2 73.9 76.4 A-word2vec 78 77.5 90.9 67.4 86.4 64.3 75.6 D-word2vec 81.6 80.2 93.6 73.1 89.7 74 77.4 GloVe A-GloVe D-GloVe 77.5 70.4 85.2 66.7 80.2 70.2 72.7 77.1 70.2 84.9 62.3 77.7 62.2 71.8 77.8 71."
2021.starsem-1.28,P05-1015,0,0.204562,"ormation at the word level. For the above POS, NER and CHK sequence labeling tasks, we used the CoNLL 2003 dataset (Sang and Meulder, 2003) and for the RE task, we used the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The neural network models employed for these downstream tasks are fully described in (Rogers et al., 2018). Next, we evaluated the following semantic level tasks: document-level polarity classification, PC, using the Stanford IMDB movie review dataset (Maas et al., 2011), sentence level sentiment polarity classification, SEN, using the MR dataset of short movie reviews (Pang and Lee, 2005), and classification of subjectivity and objectivity task, SUB, that uses the Rotten Tomatoes user review snippets against official movie plot summaries (Pang and Lee, 2004). Similarly to the performance results in Table 2, the current results show that the suggested denoised embeddings obtained better overall performance than the other methods, substantially for FastText embeddings. 4 Related Work A similar situation of aligning different word embeddings into a shared space occurs in multi-lingual 297 Method POS CHK NER RE PC SEN SUB word2vec 81.5 80.1 93.3 71.4 89.2 73.9 76.4 A-word2vec 78 7"
2021.starsem-1.28,D14-1162,0,0.0985681,"Missing"
2021.starsem-1.28,N18-1202,0,0.223823,"able improvement in rare word evaluations. 1 Introduction Continuous (non-contextualized) word embeddings have been introduced several years ago as a standard building block for NLP tasks. These models provide efficient ways to learn word representations in a fully self-supervised manner from text corpora, solely based on word co-occurrence statistics. A wide variety of methods now exist for generating word embeddings, with prominent methods including word2vec (Mikolov et al., 2013a), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Recently, contextualized embeddings (Peters et al., 2018; Devlin et al., 2019), replaced the use of non-contextualized embeddings in many settings. Yet, the latter remain the standard choice for typical lexical-semantic tasks, e.g., semantic similarity (Hill et al., 2015), word analogy (Jurgens et al., 2012), relation classification (Barkan et al., 2020a), and paraphrase identification (Meged et al., 2020). These tasks consider the generic meanings of lexical items, given out of context, hence the use of non-contextualized embeddings is appropriate. Notably, FastText was shown to yield state-of-theart results in most of these tasks (Bojanowski et a"
2021.starsem-1.28,C18-1228,0,0.0136245,"vectors fit the data less accurately. 3.5 Evaluations On Downstream Tasks For completeness, we next show the relative advantage of our denoising method also when applied to several sentence-level downstream benchmarks. While contextualized embeddings dominate a wide range of sentence- and document- level NLP tasks (Peters et al., 2018; Devlin et al., 2019; Caciularu et al., 2021), we assessed the relative advantage of our denoising method when utilizing (non-contextualized) word embeddings in sentencean document- level settings. We applied the exact procedure proposed in Li et al. (2017) and Rogers et al. (2018), as an effective benchmark for the quality of static embedding models. We first used sequence labeling tasks. The morphological and syntactic performance was evaluated using part-of-speech tagging, POS, and chunking, CHK. Both named entity recognition, NER, and multiway classification of semantic relation classes, RE, tasks were used for evaluating semantic information at the word level. For the above POS, NER and CHK sequence labeling tasks, we used the CoNLL 2003 dataset (Sang and Meulder, 2003) and for the RE task, we used the SemEval 2010 task 8 dataset (Hendrickx et al., 2010). The neura"
2021.starsem-1.28,D18-1042,0,0.0179665,"=1 t=1 For this objective, it is easy to show that for a set of transformations T1 , ..., Tk , the optimal shared space representation is: yt = k 1X Ti xi,t . k i=1 Hence, solving the optimization problem pertains to finding the k optimal transformations. In the case where k = 2, the optimal T can be obtained in a closed form using the Procrustes Analysis (PA) procedure (Sch¨onemann, 1966), which has been employed in recent bilingual word translation methods (Xing et al., 2015; Artetxe et al., 2016; Hamilton et al., 2016; Artetxe et al., 2017a,b; Conneau et al., 2017; Artetxe et al., 2018a,b; Ruder et al., 2018). In our setting, to obtain an improved embedding, we wish to average more than two embedding sets. S(Ti ) = n X kTi xi,t − yt k2 , i = 1, ..., k. t=1 The minimum of S(Ti ) can then be found by the closed form PA procedure. The updated transformation is Ti = Ui Vi> , where Ui Σi Vi> is the singularP value decomposition (SVD) of the d × d matrix nt=1 yt x> i,t . At each step in the iterative GPA algorithm, the score (1) is monotonically decreased until it converges to a local minimum point. Algorithm 1 Shared Space Embedding Averaging 1: Input: Ensemble of k word embedding sets. 2: Task: Find t"
2021.starsem-1.28,D19-1363,1,0.859379,"simple averaging of the original word vectors is problematic, since each training session of the algorithm produces embeddings in a different space. In fact, the objective scores of both word2vec, Glove and FastText are invariant to multiplying all the word embeddings by an orthogonal matrix, hence, the algorithm output involves an arbitrary rotation of the embedding space. For addressing this issue, we were inspired by recent approaches originally proposed for aligning multi-lingual embeddings (Chen and Cardie, 2018; Kementchedjhieva et al., 2018; Alaux et al., 2019; Jawanpuria et al., 2019; Taitelbaum et al., 2019). To obtain such alignments, these methods simultaneously project the original language-specific embeddings into a shared space, while enforcing (or at least encouraging) transitive orthogonal transformations. In our (monolingual) setting, we propose a related technique to project the different embedding versions into a shared space, while optimizing the projection towards obtaining an improved fused representation. We show that this results in im294 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 294–301 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Assoc"
2021.starsem-1.28,N15-1104,0,0.0320166,"dings y = {y1 , ..., yn } in the shared space that minimize the following mean-squared error: S(T, y) = k X n X kTi xi,t − yt k2 . (1) i=1 t=1 For this objective, it is easy to show that for a set of transformations T1 , ..., Tk , the optimal shared space representation is: yt = k 1X Ti xi,t . k i=1 Hence, solving the optimization problem pertains to finding the k optimal transformations. In the case where k = 2, the optimal T can be obtained in a closed form using the Procrustes Analysis (PA) procedure (Sch¨onemann, 1966), which has been employed in recent bilingual word translation methods (Xing et al., 2015; Artetxe et al., 2016; Hamilton et al., 2016; Artetxe et al., 2017a,b; Conneau et al., 2017; Artetxe et al., 2018a,b; Ruder et al., 2018). In our setting, to obtain an improved embedding, we wish to average more than two embedding sets. S(Ti ) = n X kTi xi,t − yt k2 , i = 1, ..., k. t=1 The minimum of S(Ti ) can then be found by the closed form PA procedure. The updated transformation is Ti = Ui Vi> , where Ui Σi Vi> is the singularP value decomposition (SVD) of the d × d matrix nt=1 yt x> i,t . At each step in the iterative GPA algorithm, the score (1) is monotonically decreased until it con"
2021.starsem-1.28,P16-1128,0,0.0500028,"Missing"
2021.starsem-1.8,2020.blackboxnlp-1.16,0,0.0214527,"challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models may perform poorly on specific phenomena they haven’t observed enough during training, and proposed to “inoculate” LM-based models against challenge sets by fine-tuning them on a small number of phenomenon-specific training instances. Rozen et al. (2019) showed that the inoculation does not necessarily teach the model a generalized notion of the phenomenon of interest, and that when the challenge test set dif"
2021.starsem-1.8,P18-2103,1,0.894184,"Missing"
2021.starsem-1.8,N18-2017,0,0.0440855,"Missing"
2021.starsem-1.8,D15-1075,0,0.097417,"Missing"
2021.starsem-1.8,2020.acl-main.465,0,0.0113565,"s (LMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have recently achieved human-level performance on standard natural language inference (NLI) benchmarks (Wang et al., 2019). However, the performance on this complex task is achieved in part thanks to large training sets that facilitate learning of dataset-specific biases and correlations, and thanks to the similar distributions between the training and test sets, that rewards such models (Poliak et al., 2018; Gururangan et al., 2018). This contrasts with humans, who can learn a generalized solution from fewer examples (Linzen, 2020). Indeed, NLI models often fail on examples involving various linguistic phenomena such 89 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 89–98 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics the LM of the relation between a pair of entities that are involved in an inference instance, e.g. Hypernym(pangolin) = animal. This approach is agnostic to the identity of the specific entities, allowing models to learn inference patterns separately from the individual facts involved in particular instances. To evaluate the ab"
2021.starsem-1.8,P18-1224,0,0.0201187,"lenge set for each semantic relation, that we derive from MultiNLI (Section 3.2). As usual, the goal is to determine the label of a premise-hypothesis pair (p, h) among entailment, neutral, and contradiction. For a given semantic relation, each instance in the corresponding challenge set requires applying an inference pattern associated with the semantic relation in order to determine the correct label (possibly along with other required inferences). 3 Knowledge-Enhanced Models There is plenty of work on incorporating knowledge from KBs into neural models. Knowledgebased Inference Model (KIM; Chen et al., 2018) incorporated semantic relations from WordNet into an RNN-based NLI model, gaining a modest improvement on a challenge set. The incorporation at various components of the original NLI model is not straightforward to adapt to other models. KnowBert (Peters et al., 2019) incorporated knowledge from Wikipedia and WordNet into a BERT model through entity embeddings, improving performance on relation extraction and entity typing. Ernie (Zhang et al., 2019) and K-Adapter (Wang et al., 2020a) both targeted similar downstream tasks. Ernie embeds entities and relations from a KB, and alters the BERT pr"
2021.starsem-1.8,N19-1225,0,0.174468,"Ido Dagan1 1 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,shmulikamar}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract as co-hyponymy (Glockner et al., 2018) and negation (Naik et al., 2018), which they are expected to acquire indirectly from the NLI training set. Prior work proposed to provide (“inoculate”) NLI models with a small number of phenomenonspecific training examples in order to teach the model to address them (Liu et al., 2019a). However, Rozen et al. (2019) showed that when the distributions of the training and test sets differ with respect to syntactic and lexical properties, the performance of such inoculated models drops, concluding that they do not learn a generalized notion of the phenomenon. In this paper we are motivated by the following question: how can we facilitate learning of generalized inference patterns, with respect to a given linguistic phenomenon, from a relatively small number of examples? Ideally, we would like an NLI model to learn inference patterns detached from their original context, and t"
2021.starsem-1.8,P19-1334,0,0.0343035,"Missing"
2021.starsem-1.8,N19-1423,0,0.529042,"datasets. In this setting, InferBert succeeds to learn general inference patterns, from a relatively small number of training instances, while not hurting performance on the original NLI data and substantially outperforming prior knowledge enhancement models on the challenge data. It further applies its inferences successfully at test time to previously unobserved entities. InferBert is computationally more efficient than most prior methods, in terms of number of parameters, memory consumption and training time. 1 Introduction Transformer-based pre-trained language models (LMs), such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) have recently achieved human-level performance on standard natural language inference (NLI) benchmarks (Wang et al., 2019). However, the performance on this complex task is achieved in part thanks to large training sets that facilitate learning of dataset-specific biases and correlations, and thanks to the similar distributions between the training and test sets, that rewards such models (Poliak et al., 2018; Gururangan et al., 2018). This contrasts with humans, who can learn a generalized solution from fewer examples (Linzen, 2020). Indeed, NLI models often fai"
2021.starsem-1.8,W18-5446,0,0.0600881,"Missing"
2021.starsem-1.8,C18-1198,0,0.0238189,"Computational Semantics, pages 89–98 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics the LM of the relation between a pair of entities that are involved in an inference instance, e.g. Hypernym(pangolin) = animal. This approach is agnostic to the identity of the specific entities, allowing models to learn inference patterns separately from the individual facts involved in particular instances. To evaluate the ability of NLI models to learn inference patterns for specific linguistic phenomena, we follow the evaluation approach taken in previous work (Naik et al., 2018; Liu et al., 2019a; Richardson et al., 2020), which demonstrated the learning ability of models over a few chosen inference phenomena. We focus on 4 target semantic relations: hypernymy, location, country of origin, and color, for which we create challenge sets1 (see Table 1 for examples). We construct the challenge sets such that there is no overlap between the training, validation, and test sets with respect to the target entities (e.g. pangolin), to allow testing whether the model had learned an inference phenomenon in a generic manner, rather than performing lexical memorization. The trai"
2021.starsem-1.8,D19-1005,0,0.0423223,"Missing"
2021.starsem-1.8,N18-1101,0,0.535014,"en instances. available Location P: H: Country of Origin P: Viesgo deal, from beginning to end, took less than five weeks. H: The minimum amount of time it has ever taken a Spanish company to close a deal is six weeks. Label: Contradiction Relation: CountryOfOrigin(Viesgo) = Spain Our results confirm that InferBert manages to generalize inference patterns to new facts, substantially improving performance on the challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models"
2021.starsem-1.8,W18-5441,0,0.0509514,"Missing"
2021.starsem-1.8,2020.acl-main.543,0,0.0129823,"g performance on the challenge sets upon the knowledge-enhanced baselines (up to +17.5 points in accuracy from the next best model), all while maintaining the performance on the original MultiNLI test set (Williams et al., 2018). 1 All datasets and resources are https://github.com/ohadrozen/inferbert. Hypernymy P: He killed another jay this season. H: He took life away from a bird this season. Label: Entailment Relation: Hypernym(jay)= bird at 90 such as high lexical overlap and spelling errors. NLI models also struggled with examples involving logic and monotonicity (Richardson et al., 2020; Yanaka et al., 2020; Geiger et al., 2020).2 Finally, the GLUE benchmark dedicated a small set for diagnosing models’ strengths and weaknesses on various phenomena (Wang et al., 2018). Liu et al. (2019a) suggested that NLI models may perform poorly on specific phenomena they haven’t observed enough during training, and proposed to “inoculate” LM-based models against challenge sets by fine-tuning them on a small number of phenomenon-specific training instances. Rozen et al. (2019) showed that the inoculation does not necessarily teach the model a generalized notion of the phenomenon of interest, and that when the"
2021.starsem-1.8,P19-1139,0,0.0830575,"saw a pangolin and the hypothesis Bob saw an animal, it needs to know that animal is a hypernym of pangolin. Training a model on every possible hyponym-hypernym pair is incredibly inefficient and requires re-training a model whenever the vocabulary expands. Instead, we propose to decouple the learning of generic inference patterns from that of the factual knowledge. To that end, we develop InferBert, a method to enhance language models with relational knowledge from a knowledge base (KB). In contrast to recent knowledge-enhancement approaches such as KnowBert (Peters et al., 2019) and Ernie (Zhang et al., 2019) that incorporate into LMs knowledge about individual entities (e.g. pangolin), we inform We present InferBert, a method to enhance transformer-based inference models with relevant relational knowledge. Our approach facilitates learning generic inference patterns requiring relational knowledge (e.g. inferences related to hypernymy) during training, while injecting on-demand the relevant relational facts (e.g. pangolin is an animal) at test time. We apply InferBERT to the NLI task over a diverse set of inference types (hypernymy, location, color, and country of origin), for which we collected c"
2021.starsem-1.8,K19-1019,1,0.893389,"Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,shmulikamar}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract as co-hyponymy (Glockner et al., 2018) and negation (Naik et al., 2018), which they are expected to acquire indirectly from the NLI training set. Prior work proposed to provide (“inoculate”) NLI models with a small number of phenomenonspecific training examples in order to teach the model to address them (Liu et al., 2019a). However, Rozen et al. (2019) showed that when the distributions of the training and test sets differ with respect to syntactic and lexical properties, the performance of such inoculated models drops, concluding that they do not learn a generalized notion of the phenomenon. In this paper we are motivated by the following question: how can we facilitate learning of generalized inference patterns, with respect to a given linguistic phenomenon, from a relatively small number of examples? Ideally, we would like an NLI model to learn inference patterns detached from their original context, and to be able to apply them in new c"
A94-1006,C92-3150,0,0.0928272,"on. Primarily, it can support customization of machine translation (MT) lexicons to a new domain. In fact, the arguments for constructing a job-specific glossary for human-based translation may hold equally well for an MT-based process, emphasizing the need for a productivity tool. The monolingual component of termigM can be used to construct terminology lists in other applications, such as technical writing, book indexing, hypertext linking, natural language interfaces, text categorization and indexing in digital libraries and information retrieval (Salton, 1988; Cherry, 1990; Harding, 1982; Bourigault, 1992; Damerau, 1993), while the bilingual component can be useful for information retrieval in multilingual text collections (Landauer and Littman, 1990). 2 Monolingual Task: An Application for Part-of-Speech Tagging Although part-of-speech taggers have been around for a while, there are relatively few practical applications of this technology. The monolingual task appears to be an excellent candidate. As has been noticed elsewhere (Bourigault, 1992; Justeson and Katz, 1993), most technical terms can be found by looking for multiword noun phrases that satisfy a rather restricted set of syntactic p"
A94-1006,P93-1001,0,0.0767736,"m. Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are for S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more 37 You can type application parameters in"
A94-1006,W93-0301,1,0.82595,"es). 1 Terminology: An Application for Natural Language Technology The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment. Unfortunately, these technologies are still not as widely deployed in practical applications as they might be. Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al., 1992) and question answering (Kupiec, 1993b). Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al., 1993; Dagan et al., 1993). It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting. Apart from an expert knowledge of both languages..., all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole library of specialized dictionaries, encyclopedias and *Author&apos;s current address: Dept. of Mathematics and Computer Science,"
A94-1006,P91-1034,0,0.0215843,"ly, however, that the correct translation, or at least a string that overlaps with it, will be identified in some occurrences of the term. Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are for S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most al"
A94-1006,P91-1023,0,0.376042,"Services (formerly AT&T Language Line Services). 1 Terminology: An Application for Natural Language Technology The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment. Unfortunately, these technologies are still not as widely deployed in practical applications as they might be. Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al., 1992) and question answering (Kupiec, 1993b). Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al., 1993; Dagan et al., 1993). It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting. Apart from an expert knowledge of both languages..., all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole library of specialized dictionaries, encyclopedias and *Author&apos;s current address: D"
A94-1006,1992.tmi-1.9,0,0.0945194,"or S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more 37 You can type application parameters in the Optional Parameters box. Vous pouvez tapez les parametres d&apos;une application dans la zone Parametres optionnels. Figure 2: An example of word_align&apos;s output for the English and French versions of the Microsoft Windows manual. The alignment of Parameters to optionnels is an error. missing from the candidate list. The latter task becomes especially easy when a candidate o"
A94-1006,J93-1006,0,0.101398,"ll be identified in some occurrences of the term. Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are for S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more 37 You"
A94-1006,P93-1003,0,0.845023,"currently being used by the translators at AT•T Business Translation Services (formerly AT&T Language Line Services). 1 Terminology: An Application for Natural Language Technology The statistical corpus-based renaissance in computational linguistics has produced a number of interesting technologies, including part-of-speech tagging and bilingual word alignment. Unfortunately, these technologies are still not as widely deployed in practical applications as they might be. Part-ofspeech taggers are used in a few applications, such as speech synthesis (Sproat et al., 1992) and question answering (Kupiec, 1993b). Word alignment is newer, found only in a few places (Gale and Church, 1991a; Brown et al., 1993; Dagan et al., 1993). It is used at IBM for estimating parameters of their statistical machine translation prototype (Brown et Contrary to popular opinion, the job of a technical translator has little in common with other linguistic professions, such as literature translation, foreign correspondence or interpreting. Apart from an expert knowledge of both languages..., all that is required for the latter professions is a few general dictionaries, whereas a technical translator needs a whole libra"
A94-1006,P93-1004,0,0.0127533,"idate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are for S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more 37 You can type application parameters in the Optional Parameters box. Vous pouv"
A94-1006,P88-1025,0,0.00946054,"n contexts other than human-based translation. Primarily, it can support customization of machine translation (MT) lexicons to a new domain. In fact, the arguments for constructing a job-specific glossary for human-based translation may hold equally well for an MT-based process, emphasizing the need for a productivity tool. The monolingual component of termigM can be used to construct terminology lists in other applications, such as technical writing, book indexing, hypertext linking, natural language interfaces, text categorization and indexing in digital libraries and information retrieval (Salton, 1988; Cherry, 1990; Harding, 1982; Bourigault, 1992; Damerau, 1993), while the bilingual component can be useful for information retrieval in multilingual text collections (Landauer and Littman, 1990). 2 Monolingual Task: An Application for Part-of-Speech Tagging Although part-of-speech taggers have been around for a while, there are relatively few practical applications of this technology. The monolingual task appears to be an excellent candidate. As has been noticed elsewhere (Bourigault, 1992; Justeson and Katz, 1993), most technical terms can be found by looking for multiword noun phrases that"
A94-1006,1992.tmi-1.7,0,0.150152,"ccurrences of the term. Termight collects the candidate translations from all occurrences of a source term and sorts them in decreasing frequency order. The sorted list is presented to the user, followed by bilingual concordances for all occurrences of each candidate translation (see Figure 3). The user views the concordances to verify correct candidates or to find translations that are for S e n t e n c e a n d w o r d alignment Bilingual alignment methods (Warwick et al., 1990; Brown et al., 1991a; Brown et al., 1993; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Roscheisen, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993a; Matsumoto et al., 1993; Dagan et al., 1993). have been used in statistical machine translation (Brown et al., 1990), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993; van der Eijk, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990; Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). Most alignment work was concerned with alignment at the sentence level. Algorithms for the more 37 You can type application"
A94-1006,J93-1007,0,0.0931444,"el. We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al., 1993). Word_align produces a partial mapping between the words of the two texts, skipping words that cannot be aligned at a given confidence level (see Figure 2). Alternative proposals are likely to miss important but infrequent terms/translations such as &apos;Format Disk dialog box&apos; and &apos;Label Disk dialog box&apos; which occur just once. In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise. We have found that frequent head words are likely to generate a number of terms, and are therefore more important for the glossary (a ""productivity"" criterion). Consider the frequent head word box. In the Microsoft Windows manual, for example, almost any type of box is a technical term. By sorting on the frequency of the headword, we have been able to find many infrequent terms, and have not had too much of a problem with noise (at least for common headwords)"
A94-1006,E93-1015,0,0.362581,"Missing"
A94-1006,O93-1009,0,0.015481,"rce terms and a bilingual corpus aligned at the word level. We have been using the output of word_align, a robust alignment program that proved useful for bilingual concordancing of noisy texts (Dagan et al., 1993). Word_align produces a partial mapping between the words of the two texts, skipping words that cannot be aligned at a given confidence level (see Figure 2). Alternative proposals are likely to miss important but infrequent terms/translations such as &apos;Format Disk dialog box&apos; and &apos;Label Disk dialog box&apos; which occur just once. In particular, mutual information (Church and Hanks, 1990; Wu and Su, 1993) and other statistical methods such as (Smadja, 1993) and frequency-based methods such as (Justeson and Katz, 1993) exclude infrequent phrases because they tend to introduce too much noise. We have found that frequent head words are likely to generate a number of terms, and are therefore more important for the glossary (a ""productivity"" criterion). Consider the frequent head word box. In the Microsoft Windows manual, for example, almost any type of box is a technical term. By sorting on the frequency of the headword, we have been able to find many infrequent terms, and have not had too much of"
A94-1006,J90-1003,0,\N,Missing
A94-1006,J93-2003,0,\N,Missing
A94-1006,C88-1016,0,\N,Missing
A94-1006,A88-1019,0,\N,Missing
A94-1006,J90-2002,0,\N,Missing
A94-1006,P91-1022,0,\N,Missing
A94-1006,H91-1026,0,\N,Missing
abad-etal-2010-resource,poesio-etal-2008-anawiki,0,\N,Missing
abad-etal-2010-resource,poesio-artstein-2008-anaphoric,0,\N,Missing
abad-etal-2010-resource,J97-1005,0,\N,Missing
abad-etal-2010-resource,W04-0210,0,\N,Missing
abad-etal-2010-resource,hasler-etal-2006-nps,0,\N,Missing
abad-etal-2010-resource,taule-etal-2008-ancora,0,\N,Missing
bentivogli-etal-2010-building,W07-1409,0,\N,Missing
bentivogli-etal-2010-building,W09-2508,0,\N,Missing
bentivogli-etal-2010-building,W09-2507,0,\N,Missing
bentivogli-etal-2010-building,W07-1401,1,\N,Missing
bentivogli-etal-2010-building,P08-1118,0,\N,Missing
bentivogli-etal-2010-building,P07-1058,1,\N,Missing
C04-1036,P01-1008,0,0.0443655,"of Lin98. The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus,2 parsed by Lin’s Minipar dependency parser (Lin, 1993). We considered first an evaluation based on WordNet data as a gold standard, as in (Lin, 1998; Weeds and Weir, 2003). However, we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet. We therefore conducted a manual evaluation based on the judgments of two human subjects. The judgment criterion follows common evaluations of paraphrase acquisition (Lin and Pantel, 2001), (Barzilay and McKeown, 2001), and corresponds to the meaning-entailing substitutability criterion discussed in Section 1. Two words are judged as substitutable (correct similarity) if there are some contexts in which one of the words can be substituted by the other, such that the meaning of the original word can be inferred from the new one. Typically substitutability corresponds to certain ontological relations. Synonyms are substitutable in both directions. For example, worker and employee entail each other's meanings, as in the context “high salaried worker/employee”. Hyponyms typically entail their hypernyms. For exa"
C04-1036,J90-1003,0,0.227846,"Missing"
C04-1036,P90-1034,0,0.788264,"antic-oriented NLP applications and can be evaluated directly (independent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance. 1 Introduction Distributional Similarity has been an active research area for more than a decade (Hindle, 1990), (Ruge, 1992), (Grefenstette, 1994), (Lee, 1997), (Lin, 1998), (Dagan et al., 1999), (Weeds and Weir, 2003). Inspired by Harris distributional hypothesis (Harris, 1968), similarity measures compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. It is then assumed that different words that occur within similar contexts are semantically similar. As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). By constructio"
C04-1036,P93-1016,0,0.247963,"Missing"
C04-1036,P98-2127,0,0.969324,"dependent of an application) at a good level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance. 1 Introduction Distributional Similarity has been an active research area for more than a decade (Hindle, 1990), (Ruge, 1992), (Grefenstette, 1994), (Lee, 1997), (Lin, 1998), (Dagan et al., 1999), (Weeds and Weir, 2003). Inspired by Harris distributional hypothesis (Harris, 1968), similarity measures compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. It is then assumed that different words that occur within similar contexts are semantically similar. As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). By construction, if two words are distributionally similar then the occurren"
C04-1036,P93-1024,0,0.276781,"where an entry in the vector corresponds to a feature f. Each feature represents another word (or term) with which w co-occurs, and possibly specifies also the syntactic relation between the two words. The value of each entry is determined by some weight function weight(w,f), which quantifies the degree of statistical association between the feature and the corresponding word. Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (Ruge, 1992), and the conditional probability of the feature given the word (within probabilistic-based measures) (Pereira et al., 1993), (Lee, 1997), (Dagan et al., 1999). Probably the most widely used association weight function is (point-wise) Mutual Information (MI) (Church et al., 1990), (Hindle, 1990), (Lin, 1998), (Dagan, 2000), defined by: MI ( w, f ) = log 2 P ( w, f ) P ( w) P ( f ) A known weakness of MI is its tendency to assign high weights for rare features. Yet, similarity measures that utilize MI showed good performance. In particular, a common practice is to filter out features by minimal frequency and weight thresholds. A word's vector is then constructed from the remaining features, which we call here active"
C04-1036,W03-1011,0,0.316947,"ood level of human agreement. Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality. Finally, a novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance. 1 Introduction Distributional Similarity has been an active research area for more than a decade (Hindle, 1990), (Ruge, 1992), (Grefenstette, 1994), (Lee, 1997), (Lin, 1998), (Dagan et al., 1999), (Weeds and Weir, 2003). Inspired by Harris distributional hypothesis (Harris, 1968), similarity measures compare a pair of weighted feature vectors that characterize two words. Features typically correspond to other words that co-occur with the characterized word in the same context. It is then assumed that different words that occur within similar contexts are semantically similar. As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). By construction, if two words are distributionally similar then the occurrence of one word in some contexts indicates that"
C04-1036,C98-2122,0,\N,Missing
C08-1107,N06-1007,0,0.0645623,"ure degenerates into Cover(l, r), termed P recision(l, r). With 850 P recision(l, r) we obtain a “soft” version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the “important” features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al., 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al., 2002) and (Pekar, 2006) learn rules between unary templates. However, (Shinyama et al., 2002) relies on comparable corpora for identifying paraphrases and simply takes any two templates from comparable sentences that share a named entity instantiation to be paraphrases. Such approach is not feasible for non-comparable corpora where statistical measurement is required. (Pekar, 2006) learns rules only between templates related by local discourse (information from different documents is ignored). In addition, their template structure is limited to only verbs and their direct syntactic arguments, which may yield incorre"
C08-1107,P02-1006,0,0.194896,"Pantel, 2001) learns non-directional binary rules for templates that are paths in a dependency parse-tree between two noun variables X and Y . The similarity between two templates t and t0 is the geometric average: q 0 DIRT (t, t ) = Linx (t, t0 ) · Liny (t, t0 ) where Linx is the Lin similarity between X’s instantiations of t and X’s instantiations of t0 in a corpus (equivalently for Liny ). Some works take the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =‘SCO-IBM’}, and compare between these complex features of t and t0 (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005). Directional Measures Most rule learning methods apply a symmetric similarity measure between two templates, viewing them as paraphrasing each other. However, entailment is in general a directional relation. For example, ‘X acquire Y → X own Y ’ and ‘countersuit against X → lawsuit against X’. (Weeds and Weir, 2003) propose a directional measure for learning hyponymy between two words, ‘l → r’, by giving more weight to the coverage of the features of l by r (with α &gt; 21 ): W eedsD(l, r) = αCover(l, r)+(1−α)Cover(r, l) When α=1, this measure degenerates in"
C08-1107,E06-1052,1,0.832046,"tes, text patterns with variables, such as ‘X win lawsuit against Y → X sue Y ’. Applying this rule by matching ‘X win lawsuit against Y ’ in the above text allows a QA system to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. infer ‘X sue Y ’ and identify “IBM”, Y ’s instantiation, as the answer for the above question. Entailment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g. (Romano et al., 2006). One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet). Supervised learning of broad coverage rule-sets is an arduous task. This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005). Most unsupervised entailment rule acquisition methods learn binary rules, rules between templates with two variables, ignoring unary rules, rules between unary templa"
C08-1107,I05-5011,0,0.476446,"ve question. Entailment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g. (Romano et al., 2006). One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet). Supervised learning of broad coverage rule-sets is an arduous task. This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005). Most unsupervised entailment rule acquisition methods learn binary rules, rules between templates with two variables, ignoring unary rules, rules between unary templates (templates with only one variable). However, a predicate quite often appears in the text with just a single variable (e.g. intransitive verbs or passives), where inference requires unary rules, e.g. ‘X take a nap → X sleep’ (further motivations in Section 3.1). In this paper we focus on unsupervised learning of unary entailment rules. Two learning approaches are proposed. In our main approach, rules are learned by measuring"
C08-1107,N03-1003,0,0.110645,"tic or harmonic: W eedsA(u, v) = 21 [Cover(u, v) + Cover(v, u)] W eedsH(u, v) = 2 · Cover(u, v) · Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al. also used pmi for feature weights. Binary rule learning algorithms adopted such lexical similarity approaches for learning rules between templates, where the features of each template are its variable instantiations in a corpus, such as {X=‘SCO’, Y =‘IBM’} for the example in Section 1. Some works focused on learning rules from comparable corpora, containing comparable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al., 2003). Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variable instantiations are shared across comparable documents the same predicates are described. However, it is hard to collect broad-scale comparable corpora, as the majority of texts are non-comparable. A complementary approach is learning from the abundant regular, non-comparable, corpora. Yet, in such corpora it is harder to recognize variations of the same predicate. The DIRT algorithm (Lin and Pantel, 2001) learns non-directional binary rules for templates"
C08-1107,D07-1017,0,0.529574,"→ X own Y ’ and ‘countersuit against X → lawsuit against X’. (Weeds and Weir, 2003) propose a directional measure for learning hyponymy between two words, ‘l → r’, by giving more weight to the coverage of the features of l by r (with α &gt; 21 ): W eedsD(l, r) = αCover(l, r)+(1−α)Cover(r, l) When α=1, this measure degenerates into Cover(l, r), termed P recision(l, r). With 850 P recision(l, r) we obtain a “soft” version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the “important” features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al., 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al., 2002) and (Pekar, 2006) learn rules between unary templates. However, (Shinyama et al., 2002) relies on comparable corpora for identifying paraphrases and simply takes any two templates from comparable sentences that share a named entity instantiation to be paraphrases. Such approach is not feasible for non-comparable c"
C08-1107,P05-1014,1,0.89577,"templates, viewing them as paraphrasing each other. However, entailment is in general a directional relation. For example, ‘X acquire Y → X own Y ’ and ‘countersuit against X → lawsuit against X’. (Weeds and Weir, 2003) propose a directional measure for learning hyponymy between two words, ‘l → r’, by giving more weight to the coverage of the features of l by r (with α &gt; 21 ): W eedsD(l, r) = αCover(l, r)+(1−α)Cover(r, l) When α=1, this measure degenerates into Cover(l, r), termed P recision(l, r). With 850 P recision(l, r) we obtain a “soft” version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the “important” features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al., 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al., 2002) and (Pekar, 2006) learn rules between unary templates. However, (Shinyama et al., 2002) relies on comparable corpora for identifying paraphrases and simply takes any two templates fr"
C08-1107,W07-1401,1,0.737671,"e-set. In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure. 1 Introduction In many NLP applications, such as Question Answering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text. For example, a QA system has to deduce that “SCO sued IBM” is inferred from “SCO won a lawsuit against IBM” to answer “Whom did SCO sue?”. This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework (Giampiccolo et al., 2007). An important type of knowledge needed for such inference is entailment rules. An entailment rule specifies a directional inference relation between two templates, text patterns with variables, such as ‘X win lawsuit against Y → X sue Y ’. Applying this rule by matching ‘X win lawsuit against Y ’ in the above text allows a QA system to c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. infer ‘X sue Y ’ and identify “IBM”, Y ’s instantiation, as the answer for the abov"
C08-1107,N03-1013,0,0.0140147,"predicate. 2. To enable templates with control verbs and light verbs, e.g. ‘X help preventing’, ‘X make noise’, whenever a verb is encountered we generate templates that are paths between v and the verb’s modifiers, either objects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated). 3. To capture noun modifiers that act as predicates, e.g. ‘the losing X’, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb. We use the Catvar database to identify verb derivations (Habash and Dorr, 2003). As an example for the procedure, the templates extracted from the sentence “The losing party played it safe” with ‘party’ as the variable are: ‘losing X’, ‘X play’ and ‘X play safe’. 3.3 Direct Learning of Unary Rules We applied the lexical similarity measures presented in Section 2 for unary rule learning. Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature’s weight. We first adapted DIRT for unary templates (unary-DIRT, applying Lin-similarity to the single feature vector), as well as its output filtering by"
C08-1107,W03-1608,0,0.0835911,"Missing"
C08-1107,N06-1039,0,0.0367553,"sition’. Second, some predicate expressions are unary by nature. For example, modifiers, such as ‘the elected X’, or intransitive verbs. In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs. For these reasons, it seems that unary rule learning should be addressed in addition to binary rule learning. We are further motivated by the fact that some (mostly supervised) works in IE found learning unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al., 2003; Shinyama and Sekine, 2006), though they did not attempt to learn generic knowledge bases of entailment rules. This paper investigates acquisition of unary entailment rules from regular non-comparable corpora. We first describe the structure of unary templates and then explore two conceivable approaches for learning unary rules. The first approach directly assesses the relation between two given templates based on the similarity of their instantiations in the corpus. The second approach, which was also mentioned in (Iftene and BalahurDobrescu, 2007), derives unary rules from learned binary rules. 3.2 Unary Template Stru"
C08-1107,P03-1029,0,0.0201711,"argument of ‘acquisition’. Second, some predicate expressions are unary by nature. For example, modifiers, such as ‘the elected X’, or intransitive verbs. In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs. For these reasons, it seems that unary rule learning should be addressed in addition to binary rule learning. We are further motivated by the fact that some (mostly supervised) works in IE found learning unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al., 2003; Shinyama and Sekine, 2006), though they did not attempt to learn generic knowledge bases of entailment rules. This paper investigates acquisition of unary entailment rules from regular non-comparable corpora. We first describe the structure of unary templates and then explore two conceivable approaches for learning unary rules. The first approach directly assesses the relation between two given templates based on the similarity of their instantiations in the corpus. The second approach, which was also mentioned in (Iftene and BalahurDobrescu, 2007), derives unary rules from learned binary ru"
C08-1107,W04-3206,1,0.913286,"the answer for the above question. Entailment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g. (Romano et al., 2006). One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet). Supervised learning of broad coverage rule-sets is an arduous task. This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g. (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005). Most unsupervised entailment rule acquisition methods learn binary rules, rules between templates with two variables, ignoring unary rules, rules between unary templates (templates with only one variable). However, a predicate quite often appears in the text with just a single variable (e.g. intransitive verbs or passives), where inference requires unary rules, e.g. ‘X take a nap → X sleep’ (further motivations in Section 3.1). In this paper we focus on unsupervised learning of unary entailment rules. Two learning approaches are proposed. In our main approach, rules are learne"
C08-1107,P08-1078,1,0.715076,"unary and binary rule bases by their utility for NLP applications through assessing the validity of inferences that are performed in practice using the rule base. To perform such experiments, we need a testset of seed templates, which correspond to a set of target predicates, and a corpus annotated with all argument mentions of each predicate. The evaluation assesses the correctness of all argument extractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application). Following (Szpektor et al., 2008), we found the ACE 2005 event training set2 useful for this purpose. This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce. 852 2 http://projects.ldc.upenn.edu/ace/ All event mentions are annotated in the corpus, including the instantiated arguments of the predicate. ACE guidelines specify for each event its possible arguments, each associated with a semantic role. For instance, some of the Injure event arguments are Agent, Victim and Time. To utilize the ACE dataset for evaluating entailment rule applications, we manually represented each ACE event pre"
C08-1107,W03-1011,0,0.475921,"utional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954). Various measures were proposed in the literature for assessing such similarity between two words, u and v. Given a word q, its set of features Fq and feature weights wq (f ) for f ∈ Fq , a common symmetric similarity measure is Lin similarity (Lin, 1998a): P f ∈Fu ∩Fv [wu (f ) + wv (f )] P Lin(u, v) = P f ∈Fu wu (f ) + f ∈Fv wv (f ) where the weight of each feature is the pointwise mutual information (pmi) between the word and |q) the feature: wq (f ) = log[ PPr(f r(f ) ]. Weeds and Weir (2003) proposed to measure the symmetric similarity between two words by averaging two directional (asymmetric) scores: the coverage of each word’s features by the other. The coverage of u by v is measured by: P f ∈F ∩F wu (f ) Cover(u, v) = P u v f ∈Fu wu (f ) The average can be arithmetic or harmonic: W eedsA(u, v) = 21 [Cover(u, v) + Cover(v, u)] W eedsH(u, v) = 2 · Cover(u, v) · Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al. also used pmi for feature weights. Binary rule learning algorithms adopted such lexical similarity approaches for learning rules between templates, where the features of"
C08-1107,W07-1421,0,0.0174484,"Missing"
C08-1107,P98-2127,0,0.934465,"tion reviews relevant distributional similarity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning. Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954). Various measures were proposed in the literature for assessing such similarity between two words, u and v. Given a word q, its set of features Fq and feature weights wq (f ) for f ∈ Fq , a common symmetric similarity measure is Lin similarity (Lin, 1998a): P f ∈Fu ∩Fv [wu (f ) + wv (f )] P Lin(u, v) = P f ∈Fu wu (f ) + f ∈Fv wv (f ) where the weight of each feature is the pointwise mutual information (pmi) between the word and |q) the feature: wq (f ) = log[ PPr(f r(f ) ]. Weeds and Weir (2003) proposed to measure the symmetric similarity between two words by averaging two directional (asymmetric) scores: the coverage of each word’s features by the other. The coverage of u by v is measured by: P f ∈F ∩F wu (f ) Cover(u, v) = P u v f ∈Fu wu (f ) The average can be arithmetic or harmonic: W eedsA(u, v) = 21 [Cover(u, v) + Cover(v, u)] W eedsH("
C08-1107,2003.mtsummit-systems.9,0,\N,Missing
C08-1107,C98-2122,0,\N,Missing
C10-1087,D09-1110,1,0.818491,"ismatching numbers, (b) antonymy and (c) co-hyponymy (coordinate terms), as specified by WordNet. For example, two nodes of the noun distance would be considered incompatible if one is modified by short and the second by its antonym long. Similarly, two modifier co-hyponyms of distance, such as walking and running would also result such an incompatibility. Adding more incompatibility types (e.g. first vs. second flight) may further improve the precision of this method. The Baseline RTE System In this work we used B IU T EE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, B IU T EE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which B IU T EE addr"
C10-1087,bensley-hickl-2008-unsupervised,0,0.0453249,"Missing"
C10-1087,P05-1045,0,0.00846969,"ove the precision of this method. The Baseline RTE System In this work we used B IU T EE, Bar-Ilan University Textual Entailment Engine (Bar-Haim et al., 2008; Bar-Haim et al., 2009), a state of the art RTE system, as a baseline and as a basis for our discourse-based enhancements. This section describes this system’s architecture; the methods by which it was augmented to address discourse are presented in Section 5. To determine entailment, B IU T EE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al., 2005) and coreference resolution. For the latter purpose, we use OpenNLP and enable the substitution of coreferring terms. This is the only way by which B IU T EE addresses discourse, representing the state of the art in entailment systems. Entailment-based transformations Given a T-H pair (both represented as dependency parse trees), the system applies a sequence of knowledge-based entailment transformations over T, generating a set of texts which are entailed by it. The goal is to obtain consequent texts which are more similar to H. Based on preliminary results on the development set, in our expe"
C10-1087,W07-1401,1,0.866677,"Missing"
C10-1087,P06-1114,0,0.022269,"a small military submarine. Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse This example demonstrates a common situation in texts, and is also applicable to the RTE Search task’s setting. Still, little was done by the task’s participants to consider discourse, and sentences were mostly processed independently. Analyzing the Search task"
C10-1087,P09-1083,0,0.034429,"Missing"
C10-1087,P10-1123,1,0.711842,"Missing"
C10-1087,qiu-etal-2004-public,0,0.0233689,"extracting discourse and document-level features based on the classification of each sentence on its own. Our results show that, even when simple solutions are employed, the reliance on discoursebased information is helpful and achieves a significant improvement of results. We analyze the contribution of each component and suggest some future work to better attend to discourse in entailment systems. To our knowledge, this is the most extensive effort thus far to empirically explore the effect of discourse on entailment systems. nominal phrases, using publicly available tools such as JavaRap (Qiu et al., 2004) or OpenNLP1 , e.g. (Bar-Haim et al., 2008). A major step in the RTE challenges towards a more practical setting of text processing applications occurred with the introduction of the Search task in the Fifth RTE challenge (RTE-5). In this task entailing sentences are situated within documents and depend on other sentences for their correct interpretation. Thus, discourse becomes a substantial factor impacting inference. Surprisingly, discourse hardly received any treatment in this task beyond the standard use of coreference resolution (Castillo, 2009; Litkowski, 2009), and an attempt to addres"
C10-1087,E06-1052,1,0.860453,") The Russian navy worked desperately to save a small military submarine. Introduction This paper investigates the problem of recognising textual entailment within discourse. Textual Entailment (TE) is a generic framework for applied semantic inference (Dagan et al., 2009). Under TE, the relationship between a text (T) and a textual assertion (hypothesis, H) is defined such that T entails H if humans reading T would infer that H is most likely true (Dagan et al., 2006). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al., 2006) and question answering (Harabagiu and Hickl, 2006). Yet, most entailment systems have thus far paid little attention to discourse aspects of inference. In part, this is the result of the unavailability of adept tools for handling the kind of discourse processing required for inference. In addition in the main TE benchmarks, the Recognising Textual Entailment (RTE) challenges, discourse This example demonstrates a common situation in texts, and is also applicable to the RTE Search task’s setting. Still, little was done by the task’s participants to consider discourse, and sentences were mostly"
C16-1272,S12-1051,0,0.158401,"rase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which"
C16-1272,J05-3002,0,0.134834,"Missing"
C16-1272,P99-1071,0,0.315577,"Missing"
C16-1272,W04-1016,0,0.0794161,"Missing"
C16-1272,C04-1051,0,0.749669,"e it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections. 1 Introduction Various paradigms exist for comparing the meanings of two texts and modeling their semantic overlap. Paraphrase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for th"
C16-1272,D08-1019,0,0.068328,"Missing"
C16-1272,P08-2049,0,0.0504803,"Missing"
C16-1272,E14-1057,0,0.0558176,"Missing"
C16-1272,P16-2041,1,0.833208,"sist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t,"
C16-1272,P13-2080,1,0.857695,"d in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in semantic text similarity, sentence intersection captures what this shared information is. Although sentence intersection has existed for over a decade, it has received little attention due to a lack of annotated data. Previous annotation attempts have either used experts, which did not scale, or crowdsourcing, which yielded unreliable annotations (McKeown et al., 2010). We also observe that annotating sentence intersection is difficult for non-experts. We hypothesize that this difficulty stems from the task’s require"
C16-1272,W04-1013,0,0.0536885,"ria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i.e. s1 ∩ s2 ∩ s3 = (s1"
C16-1272,W05-1612,0,0.420831,"et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than q"
C16-1272,S07-1009,0,0.0447691,"in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t, and another subtree t0 , which is not necessarily part of s. It creates a new sentence s0 by"
C16-1272,N10-1044,0,0.285035,"s whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount"
C16-1272,P13-1131,1,0.858605,"context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree"
C16-1272,N04-1019,0,0.0377941,"Missing"
C16-1272,P02-1040,0,0.0954815,"scope. Combining both these criteria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i."
C16-1272,P15-2070,0,0.0224273,"Missing"
C16-1272,P13-1051,0,0.0304221,"Missing"
C16-1272,W11-1606,0,0.0173189,"be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in"
C16-1272,I13-1198,0,0.0305508,"Missing"
C16-1272,P12-2031,1,0.836148,"subtree entailment in context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sente"
C16-1272,N07-1051,0,\N,Missing
C18-2013,P14-2050,1,0.891021,"is expected to include additional personal assistant application terms such as ‘Amazon Echo’ and ‘Google Now’. Many NLP-based information extraction applications, such as relation extraction or document matching, require the extraction of terms belonging to fine-grained semantic classes as a basic building block. A practical approach to extracting such terms is to apply a term set expansion system. The input seed set for such systems may contain as few as two to ten terms which is practical to obtain. SetExpander uses a corpus-based approach based on the distributional similarity hypothesis (Harris, 1954), stating that semantically similar words appear in similar contexts. Linear bag-of-words context is widely used to compute semantic similarity. However, it typically captures more topical and less functional similarity, while for the purpose of set expansion, we need to capture more functional and less topical similarity.2 For example, given a seed term like the programming language ’Python’, we would like the expanded set to include other programming languages with similar characteristics, but we would not like it to include terms like ‘bytecode’ or ‘high-level programming language’ despite"
C18-2013,K15-1026,0,0.0574255,"Missing"
C90-3063,J86-3002,0,\N,Missing
C90-3063,C88-1021,0,\N,Missing
C90-3063,C88-1016,0,\N,Missing
C90-3063,A88-1019,0,\N,Missing
C92-4201,1991.mtsummit-papers.2,1,0.840823,"Missing"
C92-4201,P91-1024,0,\N,Missing
C92-4201,P91-1017,1,\N,Missing
C98-1010,W96-0102,0,0.234589,"generalizes only at recognition time. Much work aimed at learning models for full parsing, i.e., learning hierarchical structures. We refer here only to the DOP (Data Oriented Parsing) method (Bod, 1992) which, like the present work, is a memory-based approach. This method constructs parse alternatives for a sentence based on combinations of subtrees in the training corpus. The MBSL approach may be viewed as a linear analogy to DOP in that it constructs a cover for a candidate based on subsequences of training instances. Other implementations of the memory-based paradigm for NLP tasks include Daelemans et al. (1996), for POS tagging; Cardie (1993), for syntactic and semantic tagging; and Stanfill and Waltz (1986), for word pronunciation. In all these works, examples are represented as sets of features and the deduction is carried out by finding the most similar eases. The method presented here is radically different in that it makes use of the raw sequential form of the data, and generalizes by reconstructing test examples from different pieces of the training data. 5 Conclusions We have presented a novel general schema and a particular instantiation of it for learning sequential patterns. Applying the m"
C98-1010,W93-0113,0,0.0411698,"ty and various disambiguation problems. One approach for detecting syntactic patterns is to obtain a full parse of a sentence and then extract the required patterns. However, obtaining a complete parse tree for a sentence is difficult in many cases, and may not be necessary at all for identifying most instances of local syntactic patterns. 67 An alternative approach is to avoid the complexity of fllll parsing and instead to rely only on local information. A variety of methods have been developed within this framework, known as shallow parsing, chunking, local parsing etc. (e.g., (Abney, 1991; Greffenstette, 1993)). These works have shown that it is possible to identify most instances of local syntactic patterns by rules that examine only the pattern itself and its nearby context;. ()ften, the rules are applied to sentences that were tagged by partof-speech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. Itowcver, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources"
C98-1010,W95-0107,0,0.0536831,"were tagged by partof-speech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. Itowcver, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and expertise that are often not available. As in many areas of NLP, a learning approach is appealing. Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996). This paper presents a novel general learning approach for recognizing local sequential patterns, that may be perceived as failing within the memorybased learning paradigm. The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed). The training data are stored as-is in suffix-tree data structures, which enable linear time searching for subsequences in the corpus. The memory-based nature of the presented algorithm stems from its deduction strategy: a new instance of tile target pattern is recognized by"
C98-1010,C92-3126,0,0.0817895,"ke and Omohundro, 1992; Carrasco and Oncina, 1994; Ron et al., 1995). These algorithms differ mainly by their state-merging strategies, used for generalizing from the training data. A major difference between the abovementioned learning methods and our memory-based approach is that the former employ generalized models that were created at training time while the latter uses the training corpus as-is and generalizes only at recognition time. Much work aimed at learning models for full parsing, i.e., learning hierarchical structures. We refer here only to the DOP (Data Oriented Parsing) method (Bod, 1992) which, like the present work, is a memory-based approach. This method constructs parse alternatives for a sentence based on combinations of subtrees in the training corpus. The MBSL approach may be viewed as a linear analogy to DOP in that it constructs a cover for a candidate based on subsequences of training instances. Other implementations of the memory-based paradigm for NLP tasks include Daelemans et al. (1996), for POS tagging; Cardie (1993), for syntactic and semantic tagging; and Stanfill and Waltz (1986), for word pronunciation. In all these works, examples are represented as sets of"
C98-1010,H92-1022,0,0.0105141,"/ I/ i ] i vo, 0=03 0 Con.=2 i/&apos; 20000 40000 0 Hxainples 100000 200000 Words 300000 400000 Figure 3: Learning curves for NP, VO, and SV by number of examples (left) and words (right) pattern structure. We aim to incorporate lexical information as well in the fllture, it is still unclear whether that will improve the results. Figure 3 shows the learning curves by amount of training examples and number of words in the training data, for particular parameter settings. 4 Related Work Two previous methods for learning local syntactic patterns follow the transformation-based paradigm introduced by Brill (1992). Vilain and Day (1996) identify (and classify) name phrases such as company names, locations, etc. Ramshaw and Marcus (1995) detect noun phrases, by classifying each word as being inside a phrase, outside or on the boundary between phrases. Finite state machines (FSMs) are a natural formalism for learning linear sequences. It was used for learning linguistic structures other than shallow syntax. Gold (1978) showed that learning regular languages from positive examples is undecidable in the limit. Recently, however, several learning methods have been proposed for restricted classes of FSM. OST"
C98-1010,P97-1057,0,0.0200497,"375 Test Data: 2. Compute a tile score as a function of its positive count and total counts, by searching the training corpus. Determine which tiles are matching tiles; 2.2 Searching the training m e m o r y The MBSL scoring algorithm searches the training corpus for each subsequence of the sentence in order to find matching tiles. Implementing this search efficiently is therefore of prime importance. We do so by encoding the training corpus using suffix trees (Edward and McCreight, 1976), which provide string searching in time which is linear in the length of the searched string. Inspired by Satta (1997), we build two suffix trees for retrieving the positive and total counts for a tile. The first suffix tree holds all pattern instances from the training corpus surrounded by bracket symbols and a fixed amount of context. Searching a given tile (which includes a bracket symbol) in this tree yields the positive count for the tile. The second suffix tree holds an unbracketed version of the entire training corpus. This tree is used for searching the POS sequence of a tile, with brackets omitted, yielding the total count for the tile (recall that the negative count is the difference between the tot"
C98-1010,C96-1047,0,0.0829011,"ch (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. Itowcver, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and expertise that are often not available. As in many areas of NLP, a learning approach is appealing. Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996). This paper presents a novel general learning approach for recognizing local sequential patterns, that may be perceived as failing within the memorybased learning paradigm. The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed). The training data are stored as-is in suffix-tree data structures, which enable linear time searching for subsequences in the corpus. The memory-based nature of the presented algorithm stems from its deduction strategy: a new instance of tile target pattern is recognized by examining the raw trai"
C98-1010,J93-2004,0,\N,Missing
C98-1010,P97-1003,0,\N,Missing
C98-1010,H90-1053,0,\N,Missing
C98-1010,C96-1058,0,\N,Missing
C98-1010,A88-1019,0,\N,Missing
C98-1010,C96-2215,0,\N,Missing
C98-1010,P93-1035,0,\N,Missing
C98-1010,H93-1047,0,\N,Missing
C98-1010,J93-1002,0,\N,Missing
C98-1010,P98-1034,0,\N,Missing
C98-1010,C98-1034,0,\N,Missing
C98-1010,P95-1037,0,\N,Missing
C98-1010,P95-1002,0,\N,Missing
C98-1010,1991.iwpt-1.22,0,\N,Missing
C98-1010,E91-1004,0,\N,Missing
D09-1110,meyers-etal-2004-cross,0,0.0140164,"ions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. In this experiment we used the Bar-Ilan RTE system (Bar-Haim et al., 2009). The system ope"
D09-1110,P08-1077,0,0.0253695,"er such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluatio"
D09-1110,P08-1023,0,0.0735938,"Missing"
D09-1110,P98-1060,0,0.0432183,"d subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. Their transfer rules are tree-to-string, contrary to our tree-to-tree rules, and chaining is not attempted (rules are applied in a single top-down pass over the source forest), and t"
D09-1110,W07-1401,1,0.772809,"ample, the three rule matches presented in Figure 3 are independent. Let us consider explicit inference first. Assume we start with a single tree T with k independent rules matched. Applying k rules will yield 2k trees, since any subset of the rules might be applied to T . Therefore, the time and space complexity of applying k independent rule matches is Ω(2k ). Applying more rules on the newly derived 4 Empirical Evaluation This section reports empirical evaluation of the efficiency of compact inference, tested in the recognizing textual entailment setting using the RTE-3 and RTE-4 datasets (Giampiccolo et al., 2007; Giampiccolo et al., 2009). These datasets consist of (text, hypothesis) pairs, which need to be classified as entailing/non entailing. Our first experiment shows, using a small rule set, that compact inference outperforms explicit inference by orders of magnitude (Section 4.1). The second experiment shows that compact inference scales well to a full-blown RTE setting with several large-scale rule bases, where up to hundreds of rules are applied for a text (Section 4.2). 1061 4.1 Compact vs. Explicit Inference To compare explicit and compact inference we randomly sampled 100 pairs from the RT"
D09-1110,H05-1049,0,0.149823,"Missing"
D09-1110,C08-1043,0,0.0175992,"transformations within RTE systems, as well as on using packed representations in other NLP tasks. RTE Systems Previous RTE systems usually restricted both the type of allowed transformations and the search space. Systems based on lexical (word-based or phrase-based) matching of h in t typically applied only lexical rules (without vari1063 ables), where both sides of the rule are matched directly in t and h (Haghighi et al., 2005; MacCartney et al., 2008). The inference formalism we use is more expressive, allowing also syntactic and lexical-syntactic transformations as well as rule chaining. Hickl (2008) derived from a given (t, h) pair a small set of discourse commitments, which are quite similar to the kind of consequents we derive by our syntactic and lexical-syntactic rules. The commitments were generated by several different tools and techniques, compared to our generic unified inference process, and commitment generation efficiency was not discussed. Braz et al. (2005) presented a semantic inference framework which “augments” the text representation with only the right-hand-side of an applied rule, and in this respect is similar to ours. However, in their work, both rule application and"
D09-1110,N07-1071,0,0.0238686,"s to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions (KRAQ)1 and the planned evaluation of knowledge resources in the forthcoming 5th Recognizing Textual Entailment"
D09-1110,P09-1051,1,0.741292,"for the RTE literature. They include lexical and structural measures for the coverage of H by F, where high coverage is assumed to correlate with entailment, as well as features aiming to detect inconsistencies between F and H such as incompatible arguments for the same predicate or incompatible verb polarity (see below). For a complete feature description, see (Bar-Haim et al., 2009). 4.2 Application to an RTE System Rule Bases In addition to the generic rules described in Section 4.1, the following large-scale sources for entailment rules were used: Wikipeda: We used the lexical rulebase of Shnarch et al. (2009), who extracted rules such as ‘Janis Joplin → singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment"
D09-1110,C08-1107,1,0.836779,"cal rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. In this experiment we used the Bar-Ilan RTE system (Bar-Haim et al., 2009). The system operates in two primary stages: Inference, in which entailment rules are applied to the initial compact f"
D09-1110,W09-2504,1,0.827376,"singer’ from Wikipedia based on both its metadata (e.g. links and redirects) and text definitions, using patterns such as ‘X is a Y’. WordNet: We extracted from WordNet (Fellbaum, 1998) lexical rules based on synonyms, hypernyms and derivation relations. DIRT: The DIRT algorithm (Lin and Pantel, 2001) learns from a corpus entailment rules between binary predicates, e.g. ‘X is fond of Y→X likes Y’. We used the version described in (Szpektor and Dagan, 2007), which learns canonical rule forms. ArgumentMapped WordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLexplus (Meyers et al., 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). In total, these rule bases represent millions of rules. Polarity Annotation Rules: We compiled a small set of annotation rules for marking the polarity of predicates as negative or unknown due to verbal negation, modal verbs, conditionals etc. (Bar-Haim et al., 2009). Experimental setting The goal of the second experiment was to assess that compact inference scales well for broad entailment rule bases. I"
D09-1110,W04-3206,1,0.825126,"commonly, inference over such representations is made by applying some kind of transformations or substitutions to the tree or graph representing the text. Such transformations may be generally viewed as entailment (inference) rules, which capture semantic knowledge about paraphrases, lexical relations such as synonyms and hyponyms, syntactic variations etc. Such knowledge is either composed manually, e.g. WordNet (Fellbaum, 1998), or learned automatically. A large body of work has been dedicated to learning paraphrases and entailment rules, e.g. (Lin and Pantel, 2001; Shinyama et al., 2002; Szpektor et al., 2004; Bhagat and Ravichandran, 2008), identifying appropriate contexts for their application (Pantel et al., 2007) and utilizing them for inference (de Salvo Braz et al., 2005; BarHaim et al., 2007). Although current available rule bases are still quite noisy and incomplete, the progress made in recent years suggests that they may become increasingly valuable for text understanding applications. Overall, applied knowledge-based inference is a prominent line of research gaining much interest, with recent examples including the series of workshops on Knowledge and Reasoning for Answering Questions ("
D09-1110,P96-1027,0,0.0120575,"lems typically requires specific representations and algorithms, depending on the type of alternatives that should be represented and the specified operations for creating them. In our work, alternatives are created by rule application, where a newly derived subtree is set as an alternative to existing subtrees. Alternatives are specified locally using d-edges. Packed chart representations for parse forests were introduced in classical parsing algorithms such as CYK and Earley (Jurafsky and Martin, 2008), and have been extended in later work for various purposes (Maxwell III and Kaplan, 1991; Kay, 1996). Alternatives in the parse chart stem from syntactic ambiguities, and are specified locally as the possible decompositions of each phrase into its sub-phrases. Packed representations have been utilized also in transfer-based machine translation. Emele and Dorna (1998) translated packed source language representation to packed target language representation while avoiding unnecessary unpacking during transfer. Unlike our rule application, in their work transfer rules preserve ambiguity stemming from source language, rather than generating new alternatives. Mi et al.(2008) applied statistical m"
D09-1110,D08-1084,0,0.0688858,"Missing"
D09-1110,J00-4006,0,\N,Missing
D09-1110,D08-1022,0,\N,Missing
D09-1110,C98-1058,0,\N,Missing
D09-1110,P08-1000,0,\N,Missing
D12-1018,I08-1065,0,0.0159781,"Weisman§ , Jonathan Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguis"
D12-1018,W02-2001,0,0.0359612,"callyrelated verbs sharing some linguistic properties (Levin, 1993). One of the most general verb classes are stative vs. event verbs (Jackendoff, 1983). Stative verb, such as ‘love’ and ‘think’, usually describe a state that lasts some time. On the other hand, event verbs, such as ‘run’ and ‘kiss’, describe an action. We hypothesize that verb classes are relevant for determining entailment, for example, that stative verbs are not likely to entail event verbs. 196 Verb generality Verb-particle constructions are multi-word expressions consisting of a head verb and a particle, e.g., switch off (Baldwin and Villavicencio, 2002). We conjecture that the more general a verb is, the more likely it is to appear with many different particles. Detecting verb generality can help us tackle an infamous property of distributional similarity methods, namely, the difficulty in detecting the direction of entailment (Berant et al., 2012). For example, the verb ’cover’ appears with many different particles such as ’up’ and ’for’, while the verb ’coat’ does not. Thus, assuming we have evidence for an entailment relation between the two verbs, this indicator can help us discern the direction of entailment and determine that ‘coat → c"
D12-1018,J12-1003,1,0.714129,"xample, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and a"
D12-1018,P08-1090,0,0.124146,"strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pairs based on a distributional similarity measure, the patterns are used to choose a semantic relation per verb pair based on the different patterns this pair instantiates. This method is more precise than distributional similarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennac"
D12-1018,W04-3205,0,0.836949,"Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-base"
D12-1018,P05-1014,1,0.692735,"entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starti"
D12-1018,C92-2082,0,0.134291,"ilarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin, 1998; Berant et al., 2012). This led to the automatic acquisition of large scale knowledge bases, but with limited precision. Fewer works, such as VerbOcean (Chklovski and Pantel, 2004), focused on identifying verb entailment through verb instantiation of manually constructed patterns. For example, the sentence “he scared and even startled me” implies that ‘startle → scare’. This led to more precise rule extraction, but with poor coverage since contrary to nouns, in which patterns are common (Hearst, 1992), verbs do not co-occur often within rigid patterns. However, verbs do tend to co-occur in the same document, and also in different clauses of the same sentence. In this paper, we claim that on top of standard pattern-based and distributional similarity methods, corpus-based learning of verb entailment can greatly benefit from exploiting additional linguisticallymotivated cues that are specific to verbs. For instance, when verbs co-occur in different clauses of the same sentence, the syntactic relation between the clauses can be viewed as a proxy for the semantic relation between the verbs. Mo"
D12-1018,P02-1047,0,0.0179155,"harder to discern but coverage is increased. 4.1.1 Sentence-level co-occurrence We next detail features that address co-occurrence of the target verb pair within a sentence. These include our novel linguistically-motivated indicators, as well as features that were adapted from prior work. Discourse markers As discussed in Section 3, discourse markers may signal relations between the main verbs of adjacent clauses. The literature is abundant with taxonomies that classify markers to various discourse relations (Mann and Thompson, 1988; Hovy and Maier, 1993; Knott and Sanders, 1998). Inspired by Marcu and Echihabi (2002), we employ markers that are mapped to four discourse relations ’Contrast’, ’Cause’, ’Condition’ and ’Temporal’, as specified in Table 1. This definition can be viewed as a relaxed version of VerbOcean’s (Chklovski and Pantel, 2004) patterns, although the underlying intuition is different (see Section 3). For a target verb pair (v1 , v2 ) and each discourse relation r, we count the number of times that v1 is the main verb in the main clause, v2 is the main verb in the subordinate clause, and the clauses are connected via a marker mapped to r. For example, given the sentence “You must enroll in"
D12-1018,P06-2075,1,0.926486,"imilarity approaches, but it is highly susceptible to sparseness issues, since verbs do not typically co-occur within rigid patterns. Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (20"
D12-1018,nivre-etal-2006-maltparser,0,0.00951803,"se and v2 in the main clause. We term the features by the relevant discourse relation, e.g., ‘v1-contrast-v2’ refers to v1 being in the main clause and connected to the subordinate clause via a contrast marker. Dependency relations between clauses As noted in Section 3, the syntactic structure of verb cooccurrence can indicate the existence or lack of entailment. In dependency parsing this may be expressed via the label of the dependency relation connecting the main and subordinate clauses. In our experiments we used the ukWaC corpus1 (Baroni et al., 2009) which was parsed by the MALT parser (Nivre et al., 2006). Hence, we identified three MALT dependency relations that connect a main clause with its subordinate clause. The first relation is the object complement relation ‘obj’. In this case the subordinate clause is an object complement of the main clause. For example, in “it surprised me that the lizard could talk” the verb pair (‘surprise’,‘talk’) is connected by the ‘obj’ relation. The second relation is the adverbial adjunct relation ‘adv’, in which the subordinate clause is adverbial and describes the time, place, manner, etc. of the main clause, e.g., “he gave his consent without thinking abou"
D12-1018,D09-1025,0,0.0192561,"(2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. In a similar manner, Pekar (2008) detects entailment rules between templates from shared arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify"
D12-1018,P02-1006,0,0.127632,"o-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are seman"
D12-1018,D10-1106,0,0.200727,"an Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for de"
D12-1018,N06-1039,0,0.0607114,"a document, as well as overall corpus statistics. To this end, we propose a much richer novel set of linguistically motivated cues for detecting entailment between verbs and combine them as features in a supervised classification framework. We empirically demonstrate that our model significantly outperforms previous methods and that information from each textual scope contributes to the verb entailment learning task. 1 Introduction Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). For example, given the sentence “Churros are coated with sugar”, one can use the rule ‘coat → cover’ to answer the question “What are Churros covered with?”. Inference rules specify a directional inference relation between two text fragments, and we follow the Textual Entailment modeling of inference (Dagan et al., 2006), which refers to such rules as entailment rules. In this work we focus on one Most prior efforts to learn verb entailment rules from large corpora employed distributional similarity methods, assuming that verbs are semantically similar if they occur in similar contexts (Lin,"
D12-1018,C08-1107,1,0.938656,"Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the verbs ‘X’ and ‘Y’. Starting with candidate verb pai"
D12-1018,W04-3206,1,0.339602,"t the candidate rules ‘vs → vsi ’ and ‘vsi → vs ’ respectively. To reduce noise, we filtered out verb pairs where one of the verbs is an auxiliary or a light verb such as ’do’, ’get’ and ’have’. This step resulted in 812 verb pairs as our dataset6 , which were manually annotated by the authors as representing a valid entailment rule or not. To annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 → v2 ’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 → v2 ’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 → v2 ’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with defau"
D12-1018,P07-1058,1,0.934416,"annotate these pairs, we generally followed the rule-based approach for entailment rule annotation, where a rule ‘v1 → v2 ’ is considered as correct if the annotator could think of reasonable contexts under which the rule holds (Dekang and Pantel, 2001; Szpektor et al., 2004). In total 225 verb pairs were labeled as entailing (the rule ‘v1 → v2 ’ was judged as correct) and 587 verb pairs were labeled as non-entailing (the rule ‘v1 → v2 ’ was judged as incorrect). The InterAnnotator Agreement (IAA) for a random sample of 100 pairs was moderate (0.47), as expected from the rule-based approach (Szpektor et al., 2007). For each verb pair, all 63 features within our model (Section 4) were computed using the ukWaC corpus (Baroni et al., 2009), which contains 2 billion words. For classification, we utilized SVM-perf’s (Joachims, 2005) linear SVM implementation with default parameters, and evaluated our model by performing 10-fold cross validation (CV) over the labeled dataset. 5 http://trec.nist.gov/data/reuters/ reuters.html 6 The data set is available at http://www.cs.biu.ac. il/˜nlp/downloads/verb-pair-annotation. html 200 Feature selection and analysis As discussed in Section 4.2, we followed the feature"
D12-1018,P10-3017,0,0.217353,"ed arguments within discourserelated clauses in the same document. Recently, supervised classification has become standard in performing various semantic tasks. Mirkin et al. (2006) introduced a system for learning entailment rules between nouns (e.g., ‘novel → book’) that combines distributional similarity and Hearst patterns as features in a supervised classifier. Pennacchiotti and Pantel (2009) augment Mirkin et al’s features with web-based features for the task of entity extraction. Hagiwara et al. (2009) perform synonym identification based on both distributional and contextual features. Tremper (2010) extract “loose” sentence-level features in order to identify the presupposition relation (e.g., , the verb ‘win’ presupposes the verb ‘play’). Last, Berant et al. (2012) utilized various distributional similarity features to identify entailment between lexical-syntactic predicates. In this paper, we follow the supervised approach for semantic relation detection in order to identify verb entailment. While we utilize and adapt useful features from prior work, we introduce a diverse set of novel features for the task, effectively combining verb co-occurrence information at the sentence, document"
D12-1018,W03-1011,0,0.161733,"ntially improves verb entailment detection. 2 Background The main approach for learning entailment rules between verbs and verb-like structures has employed the distributional hypothesis, which assumes that words with similar meanings appear in similar contexts. For example, we expect the words ‘buy’ and ‘purchase’ to occur with similar subjects and objects in a large corpus. This observation has led to ample work on developing both symmetric and directional similarity measures that attempt to capture semantic relations between lexical items by comparing their neighborhood context (Lin, 1998; Weeds and Weir, 2003; Geffet and Dagan, 2005; Szpektor and Dagan, 2008; Kotlerman et al., 2010). A far less explored direction for learning verb entailment involves exploiting verb co-occurrence in a sentence or a document. One prominent work is Chklovsky and Pantel’s VerbOcean (2004). In VerbOcean, the authors manually constructed 33 patterns and divided them into five pattern groups, where each group signals one of the following five 195 semantic relations: similarity, strength, antonymy, enablement and happens-before. For example, the pattern ‘Xed and later Yed’ signals the happensbefore relation between the v"
D12-1018,P06-1107,0,0.0413731,"Motivated Evidence Hila Weisman§ , Jonathan Berant† , Idan Szpektor‡, Ido Dagan§ § Computer Science Department, Bar-Ilan University † The Blavatnik School of Computer Science, Tel Aviv University ‡ Yahoo! Research Israel {weismah1,dagan}@cs.biu.ac.il {jonatha6}@post.tau.ac.il {idan}@yahoo-inc.com Abstract of the most important rule types, namely, lexical entailment rules between verbs (verb entailment), e.g., ‘whisper → talk’, ‘win → play’ and ‘buy → own’. The significance of such rules has led to active research in automatic learning of entailment rules between verbs or verb-like structures (Zanzotto et al., 2006; Abe et al., 2008; Schoenmackers et al., 2010). Learning inference relations between verbs is at the heart of many semantic applications. However, most prior work on learning such rules focused on a rather narrow set of information sources: mainly distributional similarity, and to a lesser extent manually constructed verb co-occurrence patterns. In this paper, we claim that it is imperative to utilize information from various textual scopes: verb co-occurrence within a sentence, verb cooccurrence within a document, as well as overall corpus statistics. To this end, we propose a much richer no"
D12-1018,W07-1401,1,\N,Missing
D12-1018,P09-1068,0,\N,Missing
D16-1086,P15-1034,0,0.306749,"(Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation, making it possible to build Open IE systems for o"
D16-1086,Q13-1034,0,0.0531775,"Missing"
D16-1086,N13-1136,0,0.0459881,"Missing"
D16-1086,W08-1301,0,0.0581554,"Missing"
D16-1086,D11-1142,0,0.111728,"ly usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a trans"
D16-1086,L16-1146,1,0.879747,"Missing"
D16-1086,D12-1048,0,0.116203,"roduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from scratch, such a transfer approach would simplify the rule creation"
D16-1086,D13-1043,0,0.470644,"Missing"
D16-1086,moriceau-tannier-2014-french,0,0.0589965,"Missing"
D16-1086,L16-1262,0,0.0678305,"Missing"
D16-1086,seeker-kuhn-2012-making,0,0.0610596,"Missing"
D16-1086,P15-2050,1,0.887842,"Missing"
D16-1086,D15-1063,0,0.0553119,"Missing"
D16-1086,P10-1013,0,0.140857,"r English and readily usable in downstream applications.1 1 Introduction The goal of Open Information Extraction (Open IE) is to extract coherent propositions from a sentence, each represented as a tuple of a relation phrase and one or more argument phrases (e.g., born in (Barack Obama; Hawaii)). Open IE has been shown to be useful for a wide range of semantic tasks, including question answering (Fader et al., 2014), summarization (Christensen et al., 2013) and text comprehension (Stanovsky et al., 2015), and has consequently drawn consistent attention over the last years (Banko et al., 2007; Wu and Weld, 2010; Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015). Although similar applications of Open IE in other languages are obvious, most previous work focused 1 Source code and online demo available at https://github.com/UKPLab/props-de In this paper, we study whether an existing set of rules to extract Open IE tuples from English dependency parses can be ported to another language. We use German, a relatively close language, and the PropS system (Stanovsky et al., 2016) as examples in our analysis. Instead of creating rule sets from s"
D16-1252,W12-3010,0,0.0379506,"Missing"
D16-1252,P15-1034,0,0.409347,"Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to arrive at shorter, more salient, arguments. Recently, Stanovsky et al. (2016b) presented PropS, a proposition oriented representation, obtained via conversion rules from dependency trees. Performing Open IE extraction over PropS structures is straightforward – follow the clearly marked predicated nodes to their direct arguments. Contrary to the vast interest in Open IE, its task formulation has been largely overlooked. There are currently no common guidelines defining a valid extraction, which consequently hinders the creation of an evaluation benchmark for the"
D16-1252,P98-1013,0,0.0522963,"ent objectives. Particularly, SRL identifies argument role labels, which is not addressed in Open IE. Yet, the two tasks overlap as they both need to recover predicate-argument structures in sentences. We now examine the above Open IE requirements and suggest that while they are only partly embedded within SRL structures, they can be fully recovered from QA-SRL. Asserted (matrix) propositions appear in SRL as non-embedded predicates (e.g., succeeded in the “Sam succeeded to convince John”). However, SRL’s predicates are grounded to a lexicon such as PropBank (Palmer et al., 2005) or FrameNet (Baker et al., 1998), which violates the completeness and open lexicon principle. Further, in contrast to the minimal propositions principle, arguments in SRL annotations are inclusive, each marked as full subtrees in a syntactic parse. Yet, QA-SRL seems to bridge this gap between traditional SRL structures and Open IE requirements. Its predicate vocabulary is open, and its question-answer format solicits minimal propositions, as was found in a recent study by (Stanovsky et al., 2016a). This correlation suggests that the QASRL methodology is in fact also an attractive means for soliciting Open IE extractions from"
D16-1252,D13-1178,0,0.0509507,"ance against previous baselines, alleviating the current need for ad-hoc evaluation. 2 Background 2.1 Open IE Open Information Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to ar"
D16-1252,W05-0620,0,0.217559,"Missing"
D16-1252,N13-1136,0,0.0112423,"sily compare their performance against previous baselines, alleviating the current need for ad-hoc evaluation. 2 Background 2.1 Open IE Open Information Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) use"
D16-1252,D11-1142,0,0.182854,"defined that an Open IE extractor should “produce one triple for every relation stated explicitly in the text, but is not required to infer implicit facts”. For example, given the sentence “John managed to open the door” an Open IE extractor should produce the tuple (John; managed to open; the door) but is not required to produce the extraction (John; opened; the door). 1 Publicly available at http://www.cs.biu.ac.il/ nlp/resources/downloads Following this initial presentation of the task, Open IE has gained substantial and consistent attention. Many automatic extractors were created (e.g., (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013)) and were put to use in various downstream applications. In spite of this wide attention, Open IE’s formal definition is lacking. There are no clear guidelines as to what constitutes a valid proposition to be extracted, and subsequently there is no large scale benchmark annotation. Open IE evaluations therefore usually consist of a post-hoc manual evaluation of a small output sample. This evaluation practice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are"
D16-1252,D15-1076,0,0.169036,"ice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are not comparable across systems, as they use different guidelines and datasets, and (3) the experiments are hard to replicate. In this work, we aim to contribute to the standardization of Open IE evaluation by providing a large gold benchmark corpus. For that end, we first identify consensual guiding principles across prominent Open IE systems, resulting in a clearer formulation of the Open IE task. Following, we find that the recent formulation of QA-SRL (He et al., 2015) in fact subsumes these requirements for Open IE. This enables us to automatically convert the annotations of QA-SRL to a high-quality Open IE corpus of more than 10K extractions, 13 times larger than the previous largest Open IE annotation. Finally, we automatically evaluate the performance of various Open IE systems against our corpus, using a soft matching criterion. This is the first 2300 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300–2305, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics time such a com"
D16-1252,D12-1048,0,0.259461,"n IE extractor should “produce one triple for every relation stated explicitly in the text, but is not required to infer implicit facts”. For example, given the sentence “John managed to open the door” an Open IE extractor should produce the tuple (John; managed to open; the door) but is not required to produce the extraction (John; opened; the door). 1 Publicly available at http://www.cs.biu.ac.il/ nlp/resources/downloads Following this initial presentation of the task, Open IE has gained substantial and consistent attention. Many automatic extractors were created (e.g., (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013)) and were put to use in various downstream applications. In spite of this wide attention, Open IE’s formal definition is lacking. There are no clear guidelines as to what constitutes a valid proposition to be extracted, and subsequently there is no large scale benchmark annotation. Open IE evaluations therefore usually consist of a post-hoc manual evaluation of a small output sample. This evaluation practice lacks in several respects: (1) Most works provide a precision oriented metric, whereas recall is often not measured, (2) the numbers are not comparable acros"
D16-1252,J05-1004,0,0.0369733,"n IE have been defined with different objectives. Particularly, SRL identifies argument role labels, which is not addressed in Open IE. Yet, the two tasks overlap as they both need to recover predicate-argument structures in sentences. We now examine the above Open IE requirements and suggest that while they are only partly embedded within SRL structures, they can be fully recovered from QA-SRL. Asserted (matrix) propositions appear in SRL as non-embedded predicates (e.g., succeeded in the “Sam succeeded to convince John”). However, SRL’s predicates are grounded to a lexicon such as PropBank (Palmer et al., 2005) or FrameNet (Baker et al., 1998), which violates the completeness and open lexicon principle. Further, in contrast to the minimal propositions principle, arguments in SRL annotations are inclusive, each marked as full subtrees in a syntactic parse. Yet, QA-SRL seems to bridge this gap between traditional SRL structures and Open IE requirements. Its predicate vocabulary is open, and its question-answer format solicits minimal propositions, as was found in a recent study by (Stanovsky et al., 2016a). This correlation suggests that the QASRL methodology is in fact also an attractive means for so"
D16-1252,P15-2050,1,0.818442,"Missing"
D16-1252,P10-1013,0,0.0163434,"n Extraction (Open IE) was introduced as an open variant of traditional Information Extraction (Etzioni et al., 2008). As mentioned in the Introduction, its primary goal is to extract coherent propositions from a sentence, each comprising of a relation phrase and two or more argument phrases (e.g., (Barack Obama, born in, Hawaii)). Since its inception, Open IE has gained consistent attention, mostly used as a component within larger frameworks (Christensen et al., 2013; Balasubramanian et al., 2013). In parallel, many Open IE extractors were developed. TextRunner (Banko et al., 2007) and WOE (Wu and Weld, 2010) take a self-supervised approach over automatically produced dependency parses. Perhaps more dominant is the rule based approach taken by ReVerb (Fader et al., 2011), OLLIE (Mausam et al., 2012), KrakeN (Akbik and L¨oser, 2012) and ClausIE (Del Corro and Gemulla, 2013). Two recent systems take a semantically-oriented approach. Open IE-42 uses semantic role labeling to extract tuples, while Stanford Open Information Extraction (Angeli et al., 2015) uses natural logic inference to arrive at shorter, more salient, arguments. Recently, Stanovsky et al. (2016b) presented PropS, a proposition orient"
D16-1252,C98-1013,0,\N,Missing
D16-1252,P16-2077,1,\N,Missing
D17-1198,N15-1027,0,0.11829,"in the learning corpus D and u1 , ..., uk are ‘noise’ samples drawn from the word unigram distribution. Note that the normalization factor Zc is not a free parameter and P to obtain its value, one needs to compute Zc = w∈V exp(w ~ ·~c + bw ) for each context c, where V is the word vocabulary. This computation is typically not feasible due to the large vocabulary size and the exponentially large number of possible contexts and therefore it was heuristically circumvented by prior work. Mnih and Teh (2012) found empirically that setting Zc = 1 didn’t hurt the performance (see also discussion in (Andreas and Klein, 2015)). Chen et al. (2015) reported that setting log(Zc ) = 9 gave them the best results. Recent works (Vaswani et al., 2013; Zoph et al., 2016) used Zc = 1 and also initialized NCE’s bias term from Eq. (2) to bw = − log |V |. They reported that without these heuristics the training procedure did not converge to a meaningful model. In the following section, we describe our proposed language model, which is derived from word2vec’s interpretation as a low-rank PMI matrix approximation. Interestingly, this model turns out to be a close variant of NCE language models, but with a simplified objective fu"
D17-1198,P16-1186,0,0.27177,"(Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper. pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016). NCE-based language models achieved near stateof-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper. Continuous word embeddings were initially introduced as a ‘by-product’ of learning neural language models (Bengio and et al, 2003). However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learning models are no longer proper language models. In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et al., 2013) as implemented in the word2vec toolkit, has become one of the most popular such models today. This is largely attributed to its s"
D17-1198,P17-2026,1,0.455727,"no longer proper language models. In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et al., 2013) as implemented in the word2vec toolkit, has become one of the most popular such models today. This is largely attributed to its scalability to huge volumes of data, which is critical for learning highquality embeddings. Recently, Levy and Goldberg (2014) offered a motivation for the NEG objective function, showing that by maximizing this function, the skip-gram algorithm implicitly attempts to factorize a word-context pointwise mutual information (PMI) matrix. Melamud and Goldberger (2017) rederived this result by offering an informationtheory interpretation of NEG. The NEG objective function is considered a simplification of the NCE’s objective, unsuitable for learning language models (Dyer, 2014). However, in this study, we show that despite its simplicity, it can be used in a principled way to effectively train a language model, based on PMI matrix factorization. More specifically, we use NEG to train 1860 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1860–1865 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for C"
D17-1198,P16-1162,0,0.0154694,"t limits the scalability of traditional neural LMs is the computation of the normalization term in the softmax output layer, whose cost is linearly proportional to the size of the word vocabulary. Several methods have been proposed to cope with this scaling issue by replacing the softmax with a more computationally efficient component at train time.1 These include importance sam1 An alternative recent approach for coping with large word vocabularies is to represent words as compositions of subword units, such as individual characters. This approach has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper. pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016). NCE-based language models achieved near stateof-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are cl"
D17-1198,D13-1140,0,0.581895,"ative recent approach for coping with large word vocabularies is to represent words as compositions of subword units, such as individual characters. This approach has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper. pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016). NCE-based language models achieved near stateof-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper. Continuous word embeddings were initially introduced as a ‘by-product’ of learning neural language models (Bengio and et al, 2003). However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learning models are no longer proper language models. In particular, the skip-gram with negative samplin"
D17-1198,N16-1145,0,0.250881,"ge word vocabularies is to represent words as compositions of subword units, such as individual characters. This approach has notable merits (Jozefowicz et al., 2016; Sennrich et al., 2016), but is out of the scope of this paper. pling (Bengio and et al, 2003), hierarchical softmax (Minh and Hinton, 2008), BlackOut (Ji et al., 2016) and Noise Contrastive Estimation (NCE) (Gutmann and Hyvarinen, 2012). NCE has been applied to train neural LMs with large vocabularies (Mnih and Teh, 2012) and more recently was also successfully used to train LSTM-RNN LMs (Vaswani et al., 2013; Chen et al., 2015; Zoph et al., 2016). NCE-based language models achieved near stateof-the-art performance on language modeling tasks (Jozefowicz et al., 2016; Chen et al., 2016), and as we later show, are closely related to the method presented in this paper. Continuous word embeddings were initially introduced as a ‘by-product’ of learning neural language models (Bengio and et al, 2003). However, they were later adopted in many other NLP tasks, and the most popular recent word embedding learning models are no longer proper language models. In particular, the skip-gram with negative sampling (NEG) embedding algorithm (Mikolov et"
D17-2019,P14-1085,0,0.147909,"event news tweets, suggests the utility of our approach for text exploration. 1 Introduction Multi-document summarization (MDS) techniques aim to assist readers in obtaining the most important information when reading multiple texts on a topic. The dominant MDS approach focuses on constructing a short summary of some targeted length, capturing the most important information, mimicking a manually-crafted “static” summary. As an alternative, few papers considered interactive summarization, where the presented information can be interactively explored by the user according to needs and interest (Christensen et al., 2014; Leuski et al., 2003; Yan et al., 2011). In this paper we propose further contribution to this approach, focusing on interactive abstractive summarization. We suggest that an abstractive summarization approach, based on extracted “atomic” facts, is particularly suitable in the interactive setting as it allows more flexible information presentation. Intuitively, it makes more sense for a user to explore information at the level of individual facts, rather than the coarser level of full 109 Proceedings of the 2017 EMNLP System Demonstrations, pages 109–114 c Copenhagen, Denmark, September 7–11,"
D17-2019,P03-2021,0,0.172888,"s the utility of our approach for text exploration. 1 Introduction Multi-document summarization (MDS) techniques aim to assist readers in obtaining the most important information when reading multiple texts on a topic. The dominant MDS approach focuses on constructing a short summary of some targeted length, capturing the most important information, mimicking a manually-crafted “static” summary. As an alternative, few papers considered interactive summarization, where the presented information can be interactively explored by the user according to needs and interest (Christensen et al., 2014; Leuski et al., 2003; Yan et al., 2011). In this paper we propose further contribution to this approach, focusing on interactive abstractive summarization. We suggest that an abstractive summarization approach, based on extracted “atomic” facts, is particularly suitable in the interactive setting as it allows more flexible information presentation. Intuitively, it makes more sense for a user to explore information at the level of individual facts, rather than the coarser level of full 109 Proceedings of the 2017 EMNLP System Demonstrations, pages 109–114 c Copenhagen, Denmark, September 7–11, 2017. 2017 Associati"
D17-2019,C16-1023,0,0.0544288,"Missing"
D17-2019,N15-1114,0,0.0177241,"ample, a2 and a3 appear as arguments in the two templates of P1, and refer to entity E2 and proposition P2 respectively. Entailment links, marked by directed edges in Figure 1, track semantic entailment (in context) between different types of OKR components. For example, in E1, “Radcliffe Haughton” entails “man” or “shooter”, namely, the former is more specific/informative in the given context. Consolidated Representation Motivated by summarization and text exploration, recent work considered the consolidation of textual information in various structures. As prominent examples, the studies of Liu et al. (2015) and Li et al. (2016) construct graph-based representations whose nodes are predicates or arguments thereof, extracted from the original text, and the predicate-argument relations are captured by edges. Identical or coreferring concepts are collapsed in a single node. Rospocher et al. (2016) present a more supervised approach where concepts in the graph are linked to DBPedia1 entries. This along with other metadata is used to detect coreferences and disambiguate concepts. None of these works considers interactive summaries, and in particular none incorporates sufficient data for our modes of u"
D17-2019,W17-0902,1,0.906113,"a certain event along a time line (see Figure 2). Our second mode of interaction is concept expansion, which allows viewing complementary information about a concept via its alternative term mentions, while tracking the concept occurrences throughout the summary (see Figure 3). This information is hidden in static summaries that use original sentences (extractive) or a single term per concept (abstractive). To facilitate the modular construction of interactive summaries, we utilize as input a consolidated representation of texts, in particular the recent Open Knowledge Representation (OKR) of Wities et al. (2017). Briefly, this representation captures the propositions of the texts, where corefferring concepts or propositions are collapsed together while keeping links to the original mentions (see Section 2). We leverage OKR structures to extract information at the level of atomic facts, to expand information from collapsed mentions and to retrieve the sources from which summary sentences were derived. The novelties of our interactive scheme call for verifying its effectiveness and usefulness for users. For that, we have implemented our approach in a prototype system (Sections 3-4). This system automat"
D17-2019,D11-1124,0,0.14347,"approach for text exploration. 1 Introduction Multi-document summarization (MDS) techniques aim to assist readers in obtaining the most important information when reading multiple texts on a topic. The dominant MDS approach focuses on constructing a short summary of some targeted length, capturing the most important information, mimicking a manually-crafted “static” summary. As an alternative, few papers considered interactive summarization, where the presented information can be interactively explored by the user according to needs and interest (Christensen et al., 2014; Leuski et al., 2003; Yan et al., 2011). In this paper we propose further contribution to this approach, focusing on interactive abstractive summarization. We suggest that an abstractive summarization approach, based on extracted “atomic” facts, is particularly suitable in the interactive setting as it allows more flexible information presentation. Intuitively, it makes more sense for a user to explore information at the level of individual facts, rather than the coarser level of full 109 Proceedings of the 2017 EMNLP System Demonstrations, pages 109–114 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D18-1087,J13-2002,1,0.802593,"development and evaluation, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric (Lin, 2004), which is almost exclusively utilized as an automatic evaluation metric class for summarization. ROUGE variants are based on word sequence overlap between a system summary and a reference summary, where each variant measures a different aspect of text comparison. Despite its pitfalls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001), responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007). Table 1: DUC 2001 and 2002. Number of reference summaries per length for each text cluster, reference lengths, number of clusters and number of evaluated systems. terms of correlation with human judgment. Our promising results suggest repeating the assessment methodology presented here in future work, to test our question over more recent and broader summarization datasets and human evaluation schemes. This, in turn, would allow the community to feasibly resume proper evaluation an"
D18-1087,K16-1028,0,0.271393,"is paper, we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs, as it would allow better assessment of length-related performance within and across systems (illustrated in Section 3). To avoid the need in multiple-length reference summaries we raise the following research question: can reference summaries of a single length be used to evaluate system summaries of multiple lengths, as reliably as when using references of multiple lengths, with respect to different standard evaluation metrics? Recently, Kikuchi et al. (2016) evaluated system summaries of three different lengths against reference summaries of a single length. Yet, their evaluation methodology was not assessed through correlation to human judgment, as has been commonly done for other automatic evaluation protocols. Here, we provide a closer look into this methodology, given its potential value. As a first accessible case study, we test our research question over the DUC 2001 and 2002 data (Section 2). To the best of our knowledge, these are the only two datasets that include multiple length reference and submitted system summaries, as well as manua"
D18-1087,W18-2706,0,0.0649941,"Missing"
D18-1087,W17-4912,0,0.0134345,"ation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks. 1 Introduction Automated summarization systems typically produce a text that mimics a manual summary. In these systems, an important aspect is the output summary length, which may vary according to user needs. Consequently, output length has been a common tunable parameter in pre-neural summarization systems and has been incorporated recently in few neural models as well (Kikuchi et al., 2016; Fan et al., 2017; Ficler and Goldberg, 2017). It was originally assumed that summarization systems should be assessed across multiple summary lengths. For that, the earliest Document Understand Conference (DUC) (NIST, 2011) benchmarks, in 2001 and 2002, defined several target summary lengths and evaluated each summary against (manually written) reference summaries of the same length. However, due to the high cost incurred, subsequent DUC and TAC (NIST, 2018) benchmarks 774 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 774–778 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Associa"
D18-1087,W17-4510,0,0.0192369,"n, thanks to its speed and low cost. Specifically, we use several variants of the ROUGE metric (Lin, 2004), which is almost exclusively utilized as an automatic evaluation metric class for summarization. ROUGE variants are based on word sequence overlap between a system summary and a reference summary, where each variant measures a different aspect of text comparison. Despite its pitfalls, ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods (Lin, 2004; Over and James, 2004; Over et al., 2007; Nenkova et al., 2007; Louis and Nenkova, 2013; Peyrard et al., 2017), such as SEE (Lin, 2001), responsiveness (NIST, 2006) and Pyramid (Nenkova et al., 2007). Table 1: DUC 2001 and 2002. Number of reference summaries per length for each text cluster, reference lengths, number of clusters and number of evaluated systems. terms of correlation with human judgment. Our promising results suggest repeating the assessment methodology presented here in future work, to test our question over more recent and broader summarization datasets and human evaluation schemes. This, in turn, would allow the community to feasibly resume proper evaluation and deliberate developmen"
D18-1087,D11-1043,0,0.0623214,"Missing"
D18-1087,D16-1140,0,0.478568,"limited. In this paper, we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs, as it would allow better assessment of length-related performance within and across systems (illustrated in Section 3). To avoid the need in multiple-length reference summaries we raise the following research question: can reference summaries of a single length be used to evaluate system summaries of multiple lengths, as reliably as when using references of multiple lengths, with respect to different standard evaluation metrics? Recently, Kikuchi et al. (2016) evaluated system summaries of three different lengths against reference summaries of a single length. Yet, their evaluation methodology was not assessed through correlation to human judgment, as has been commonly done for other automatic evaluation protocols. Here, we provide a closer look into this methodology, given its potential value. As a first accessible case study, we test our research question over the DUC 2001 and 2002 data (Section 2). To the best of our knowledge, these are the only two datasets that include multiple length reference and submitted system summaries, as well as manua"
D18-1087,P13-2024,1,0.851931,"C 01; ICSISumm is a later competitive system (Gillick et al., 2008). lated by Only50, at each system summary length. We find very high correlations (above 0.95 for all system summary lengths, in both datasets) when using multiple references and slightly lower (0.85 to 0.9) with one reference summary. These figures show that the Only50 configuration ranks systems very similarly to Standard. To further verify our results, we computed correlations in two additional settings. First, we conducted the same analysis, excluding 2-3 of the worst systems, which might artificially boost the correlation (Rankel et al., 2013). Second, we computed score differences between all pairs of systems, for both human and ROUGE scores, and computed the correlation between these two sets of differences (Rankel et al., 2011). In both cases we observed rather consistent results, assessing that a single set of short reference summaries evaluates system summaries of different lengths just as well as the standard configuration. 3 4 Discussion We proposed the potential value of evaluating summarization systems at different summary lengths. Such evaluations would allow proper evaluation of systems’ “length knob”, tracking how their"
D18-1263,P17-2021,0,0.133168,"). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In order to apply sequence-to-sequence models for structured prediction, a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are"
D18-1263,W13-2322,0,0.123921,"ral sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications. 1 Introduction Many sentence-level representations were developed with the goal of capturing the sentence’s proposition structure and making it accessible for downstream applications (Montague, 1973; Carreras and M`arquez, 2005; Banarescu et al., 2013; Abend and Rappoport, 2013). See Abend and Rappoport (2017), for a recent survey. While syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared tas"
D18-1263,S16-1176,0,0.122982,"or structured prediction, a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are specifi1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999). 2412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412–2421 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics cally tailored for their target representation."
D18-1263,P17-1112,0,0.29393,"structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited"
D18-1263,W05-0620,0,0.0770807,"Missing"
D18-1263,P81-1022,0,0.150131,"Missing"
D18-1263,N10-1115,0,0.0221714,"leading to better (hopefully more semantic) abstractions. To the best of our knowledge, the human annotation order is not available for the SDP annotations, and there is no clear a priori optimal ordering. We therefore experiment with several visiting orders, as described in Table 2. Notably, Sentence order is equivalent to the ordering used by Vinyals et al. (2015) for syntactic linearization, while Closest words orders child nodes from short to longer range-dependencies (commonly associated with syntactic versus semantic relations), and Smaller first is motivated by the easy-first approach (Goldberg and Elhadad, 2010), first encoding paths which are shorter (and easier to memorize), before longer, more complicated sequences. In Section 6 we evaluate the effect of these variations on the SDP parsing task. 5 ARG1 Model We start by describing our model architecture, inspired by recent MT architectures, while allowing for different types of inputs, namely English sentences and linearized graphs. Following, we present our methods for training and testing, and specific hyper-parameter configuration and implementation details. (b) Our linearization scheme for the sentence in 2a. Each node is represented by its re"
D18-1263,hajic-etal-2012-announcing,0,0.235626,"Missing"
D18-1263,Q17-1024,0,0.0823583,"Missing"
D18-1263,Q16-1023,0,0.0495816,"Missing"
D18-1263,P17-4012,0,0.0423017,"ds Smaller-first DM PAS PSD 86.1 87.2 87.5 87.9 87.7 90.3 89.8 90.9 78.4 79.9 79.7 80.3 following decoding steps. Due to its better performance, we will report only the smaller-first’s performance in all following evaluations. Avg. 84.1 85.8 85.8 86.2 Table 3: Evaluation of different DFS orderings, in labeled F1 score, across the different tasks. epochs, in about 12 hours on a GPU (NVIDIA GeForce GTX 1080 Ti), in batches of 50 sentences. All of these sentences belong to the same task, which is chosen at random before each batch. Finally, our models were developed using the OpenNMT-py library (Klein et al., 2017), and are made available.7 6 Evaluation We perform several evaluations, testing the impact of alternative configurations, including the different DFS traversal orders and MTL versus single-task approach, as well as our model’s performance against current state-of-the-art on each of the PRIMARY tasks. 6.1 Results The results of our different analyses are reported in Tables 3-6, as elaborated below. For all evaluations, we use the in-domain test partition of the SDP corpus, containing 1, 410 sentences. Following Peng et al. (2017a) we report performance using labeled F1 scores as well as average"
D18-1263,P17-1014,0,0.354057,"a linearization function is required to interpret the model’s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization – they reduce the input graph to a tree using lossy heuristics, which are specifi1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999). 2412 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412–2421 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics cally tailored for their target representation. In contrast, we design"
D18-1263,D16-1180,0,0.0525922,"Missing"
D18-1263,L16-1262,0,0.0203834,"Missing"
D18-1263,S15-2153,0,0.707411,"Missing"
D18-1263,S14-2008,0,0.29444,"Missing"
D18-1263,P17-1186,0,0.50423,"apture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In o"
D18-1263,E17-1035,0,0.36486,"apture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task. In o"
D18-1263,D14-1162,0,0.0964516,"ferent relation. To overcome this we artificially increase the softmax probabilities (dashed edges in Figure 3) so that they reflect the DFS path decoded up until that point. Specifically, we override the predicted word according to the previous index, and backtrack “up” the corresponding edge. 5.3 Implementation Details All of our hyper-parameters were tuned on a held out partition of 1000 sentences in the training set. In particular, we use 3 hidden layers for both of the encoders, and 2 hidden layers for the decoder. English word embeddings were fixed with 300-dimensional GloVe embeddings (Pennington et al., 2014), while the graph elements, which consist of a lexicon of roughly 400 tokens across three representations, were randomly initialized. We trained the model until convergence, roughly 20 2418 Random Sentence order Closest words Smaller-first DM PAS PSD 86.1 87.2 87.5 87.9 87.7 90.3 89.8 90.9 78.4 79.9 79.7 80.3 following decoding steps. Due to its better performance, we will report only the smaller-first’s performance in all following evaluations. Avg. 84.1 85.8 85.8 86.2 Table 3: Evaluation of different DFS orderings, in labeled F1 score, across the different tasks. epochs, in about 12 hours on"
D18-1263,P16-2038,0,0.100388,"Missing"
D18-1263,I17-1003,0,0.0204456,"graph decoder. Interestingly, we show that training on the auxiliary inter-representation translation tasks greatly improves the performance on the original SDP tasks, without requiring any additional manual annotation effort (Section 6). Our contributions are two-fold. First, we show that novel sequence-to-sequence models are able to effectively capture and recover general graph structures, making them a viable and easily extensible approach towards the SDP task. Second, beyond SDP, as the inclusion of syntactic linearization was shown beneficial in various tasks (Aharoni and Goldberg, 2017; Le et al., 2017) so does our approach prompt easy integration of graphbased representations as complementary semantic signal in various downstream applications. 2 Background We begin this section by presenting the corpus we use to train and test our model (the SDP corpus) and the current state-of-the-art in predicting semantic dependencies. Then, we discuss previous work on sequence-to-sequence models for tree prediction, which this work extends to general graph structures. Finally, we briefly describe the multilingual translation approach, which we borrow and adapt to the semantic parsing task. #Train senten"
D18-1263,J93-2004,0,0.0643227,"DP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications. 1 Introduction Many sentence-level representations were developed with the goal of capturing the sentence’s proposition structure and making it accessible for downstream applications (Montague, 1973; Carreras and M`arquez, 2005; Banarescu et al., 2013; Abend and Rappoport, 2013). See Abend and Rappoport (2017), for a recent survey. While syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinge"
D18-1263,S14-2056,0,0.274258,"ntence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi∗ Work performed while at Bar-Ilan University. Lexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000). In this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects,"
D19-1307,N19-1264,0,0.0243353,"expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions with high accuracy by reading their generated summaries; but the human judgement scores of their summaries are not higher than the summaries generated by the stateof-the-art supervised system. Kryscinski et al. (2018) propose"
D19-1307,D15-1075,0,0.0120894,"ition, the above rewards require reference summaries, unlike our reward that only takes a document and a generated summary as input. Rewards learned with extra data. Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features th"
D19-1307,P18-1060,0,0.153201,"es to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we learn a reward function directly from human ratings. We use the dataset compiled by Chaganty et al. (2018), which includes human ratings on 2,500 summaries for 500 news articles from CNN/DailyMail. Unlike ROUGE that requires one or multiple reference summaries to compute the scores, our reward function only takes the document and the generated summary as input. Hence, once trained, our 3110 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3110–3120, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics reward can be used to train RL-based summarisa"
D19-1307,P18-1063,0,0.398885,"ts show that, compared to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the re"
D19-1307,D17-1070,0,0.0509167,"Missing"
D19-1307,N19-1423,0,0.0346382,"Missing"
D19-1307,D18-1409,0,0.0811759,"Missing"
D19-1307,D18-1445,1,0.870321,"ctions, IRL algorithms learn a reward function that is consistent with the observed sequences. In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions with high accuracy"
D19-1307,N18-1065,0,0.130978,"Missing"
D19-1307,D18-1208,0,0.0578087,"Missing"
D19-1307,D14-1181,0,0.00341498,"x and summary y as two embeddings, and feed the concatenated embedding into a single-layer MLP to minimise the loss functions Eq. (1) and (3). We consider three text encoders to vectorise x and yx . In supplementary material, we provide figures to further illustrate the architectures of these text encoders. 3113 CNN-RNN. We use convolutional neural networks (CNNs) to encode the sentences in the input text, and feed the sentence embeddings into an LSTM to generate the embedding of the whole input text. In the CNN part, convolutions with different filter widths are applied independently as in (Kim, 2014). The most relevant features are selected afterwards with max-over-time pooling. In line with Narayan et al. (2018b), we reverse the order of sentence embeddings before feeding them into the LSTM. This encoder network yields strong performance on summarisation and sentence classification tasks (Narayan et al., 2018a,b). PMeans-RNN. PMeans is a simple yet powerful sentence encoding method (R¨uckl´e et al., 2018). PMeans encodes a sentence by computing the power means of the embeddings of the words in the sentence. PMeans uses a parameter p to control the weights for each word embedding: with p"
D19-1307,P17-1138,0,0.0670637,"Missing"
D19-1307,P18-1165,0,0.0475632,")optimal sequences of actions, IRL algorithms learn a reward function that is consistent with the observed sequences. In the case of summarisation, human-written reference summaries are the (near-)optimal sequences, which are expensive to provide. Our method only needs human ratings on some generated summaries, also known as the bandit feedback (Kreutzer et al., 2017), to learn the reward function. Furthermore, when employing certain loss functions (see §4 and Eq. (2)), our method can even learn the reward function from preferences over generated summaries, an even cheaper feedback to elicit (Kreutzer et al., 2018; Gao et al., 2018). Heuristic-based rewards. Prior work proposed heuristic-based reward functions to train crossinput RL summarisers, in order to strengthen certain properties of the generated summaries. Arumae and Liu (2019) propose four reward functions to train an RL-based extractive summariser, including the question-answering competency rewards, which encourage the RL agent to generate summaries that can answer cloze-style questions. Such questions are automatically created by removing some words in the reference summaries. Experiments suggest that human subjects can answer the questions"
D19-1307,D18-1207,0,0.232854,"erate summaries with significantly higher human ratings than the state-of-the-art systems (§7). 2 Related Work RL-based summarisation. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-i"
D19-1307,W04-1013,0,0.155076,"documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “"
D19-1307,J13-2002,0,0.562287,"reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is"
D19-1307,P18-1188,0,0.290027,"to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appe"
D19-1307,N18-1158,0,0.402438,"to the state-of-the-art supervised-learning systems and ROUGE-as-rewards RL summarisation systems, the RL systems using our learned rewards during training generate summaries with higher human ratings. The learned reward function and our source code are available at https://github.com/yg211/ summary-reward-no-reference. 1 Introduction Document summarisation aims at generating a summary for a long document or multiple documents on the same topic. Reinforcement Learning (RL) becomes an increasingly popular technique to build document summarisation systems in recent years (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018). Compared to the supervised learning paradigm, which “pushes” the summariser to reproduce the reference summaries * Since June 2019, Yang Gao is affiliated with Dept. of Computer Science, Royal Holloway, University of London. at training time, RL directly optimises the summariser to maximise the rewards, which measure the quality of the generated summaries. The ROUGE metrics (Lin, 2004b) are the most widely used rewards in training RL-based summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appe"
D19-1307,D17-1238,0,0.0571248,"ased summarisation systems. ROUGE measures the quality of a generated summary by counting how many n-grams in the reference summaries appear in the generated summary. ROUGE correlates well with human judgements at system level (Lin, 2004a; Louis and Nenkova, 2013), i.e. by aggregating system summaries’ ROUGE scores across multiple input documents, we can reliably rank summarisation systems by their quality. However, ROUGE performs poorly at summary level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we le"
D19-1307,P02-1040,0,0.110128,"o correctly rank summaries of different quality levels, while a good reward function focuses more on distinguishing the best summaries from the mediocre and bad summaries. Also, an evaluation metric should be able to evaluate summaries of different types (e.g. extractive and abstractive) and from different genres, while a reward function can be specifically designed for a single task. We leave the learning of a generic summarisation evaluation metric for future work. so as to further motivate the need for a stronger reward for RL. Metrics we consider include ROUGE (full length F-score), BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009). Furthermore, in line with Chaganty et al. (2018), we also use the cosine similarity between the embeddings of the generated summary and the reference summary as metrics: we use InferSent (Conneau et al., 2017) and BERT-Large-Cased (Devlin et al., 2019) to generate the embeddings. 3 From Table 1, we find that all metrics we consider have low correlation with the human judgement. More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more"
D19-1307,N18-2102,0,0.138869,"ificantly higher human ratings than the state-of-the-art systems (§7). 2 Related Work RL-based summarisation. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-input and input-specific RL"
D19-1307,D14-1162,0,0.0828786,"ries (described in §3) to measure the performance of our reward R. In each fold, the data is split with ratio 64:16:20 for training, validation and test. The parameters of our model are detailed as follows, decided in a pilot study on one fold of the data split. The CNN-RNN encoder (see §5.1) uses filter widths 1-10 for the CNN part, and uses a unidirectional LSTM with a single layer whose dimension is 600 for the RNN part. For PMeans, we obtain sentence embeddings for each p ∈ {−∞, +∞, 1, 2} and concatenate them per sentence. Both these two encoders use the pre-trained GloVe word embeddings (Pennington et al., 2014). On top of these encoders, we use an MLP with one hidden ReLU layer and a linear activation at the output layer. For the MLP that uses CNNRNN and PMeans-RNN, the dimension of its hidden layer is 100, while for the MLP with BERT as encoder, the dimension of the hidden layer is 1024. As for SimRed, we set α (see Eq. (4)) to be 0.85. The trainable layer on top of BERT and PMeans – when used with SimRed – is a single 3114 layer perceptron whose output dimension is equal to the input dimension. Reward Quality. Table 2 shows the quality of different reward learning models. As a baseline, we also co"
D19-1307,W17-4510,1,0.497627,"Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents). Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements i"
D19-1307,N18-2103,1,0.628066,"high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e.g. ROUGE metrics); the others are heuristic-based and do not use reference summaries (e.g. the JensenShannon divergence between the word distributions in the summary and the documents). Their experiments suggest that with only non-referencesummary-based features, the correlation of their learned metric with human judgements is lower than ROUGE; with referen"
D19-1307,D18-1088,0,0.0741606,"Missing"
D19-1307,P18-1061,0,0.0557763,"Missing"
D19-1307,D14-1075,0,0.198249,"Missing"
D19-1307,D12-1024,0,0.363777,"n. Most existing RLbased summarisers fall into two categories: crossinput systems and input-specific systems (Gao et al., 2019). Cross-input systems learn a summarisation policy at training time by letting the RL agent interact with a ROUGE-based reward function. At test time, the learned policy is used to generate a summary for each input document. Most RL-based summarisers fall into this category (Chen and Bansal, 2018; Narayan et al., 2018b; Dong et al., 2018; Kryscinski et al., 2018; Pasunuru and Bansal, 2018; Paulus et al., 2018). As an alternative, input-specific RL (Rioux et al., 2014; Ryang and Abekawa, 2012) does not require reference summaries: for each input document at test time, a summarisation policy is trained specifically for the input, by letting the RL summariser interact with a heuristic-based reward function, e.g. ROUGE between the generated summary and the input document (without using any reference summaries). However, the performance of inputspecific RL falls far behind the cross-input counterparts. In §7 we use our learned reward to train both cross-input and input-specific RL systems. A similar idea has been explored by Gao et al. (2019), but unlike their work that learns the rewa"
D19-1307,P17-1099,0,0.751171,"ry level: given multiple summaries for the same input document, ROUGE can hardly distinguish the “good” summaries from the “mediocre” and “bad” ones (Novikova et al., 2017). Because existing RL-based summarisation systems rely on summary-level ROUGE scores to guide the optimisation direction, the poor performance of ROUGE at summary level severely misleads the RL agents. The reliability of ROUGE as RL reward is further challenged by the fact that most large-scale summarisation datasets only have one reference summary available for each input document (e.g. CNN/DailyMail (Hermann et al., 2015; See et al., 2017) and NewsRooms (Grusky et al., 2018)). In order to find better rewards that can guide RL-based summarisers to generate more humanappealing summaries, we learn a reward function directly from human ratings. We use the dataset compiled by Chaganty et al. (2018), which includes human ratings on 2,500 summaries for 500 news articles from CNN/DailyMail. Unlike ROUGE that requires one or multiple reference summaries to compute the scores, our reward function only takes the document and the generated summary as input. Hence, once trained, our 3110 Proceedings of the 2019 Conference on Empirical Metho"
D19-1307,W19-2303,1,0.889806,"Missing"
D19-1307,N18-1101,0,0.0129741,"erence summaries, unlike our reward that only takes a document and a generated summary as input. Rewards learned with extra data. Pasunuru and Bansal (2018) propose two novel rewards for training RL-based abstractive summarisers: RougeSal, which up-weights the salient phrases and words detected via a keyphrase classifier, and Entail reward, which gives high scores to 3111 logically-entailed summaries using an entailment classifier. RougeSal is trained with the SQuAD reading comprehension dataset (Rajpurkar et al., 2016), and Entail is trained with the SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018) datasets. Although their system achieves new state-of-the-art results in terms of ROUGE, it remains unclear whether their system generates more human-appealing summaries as they do not perform human evaluation experiments. Additionally, both rewards require reference summaries. Louis and Nenkova (2013), Peyrard et al. (2017) and Peyrard and Gurevych (2018) build featurerich regression models to learn a summary evaluation metric directly from the human judgement scores (Pyramid and Responsiveness) provided in the TAC’08 and ’09 datasets1 . Some features they use require reference summaries (e."
D19-3001,D15-1168,0,0.0242081,"impact analysis. 1 Introduction Aspect Based Sentiment Analysis (ABSA) is the task of extracting, from a given corpus, opinion targets (aspect terms) and the sentiment expressed towards them. For example, in the sentence “The dessert was incredible”, the aspect term is dessert and the sentiment towards it is positive. This finegrained trait of ABSA makes it an effective application for measuring and monitoring the ratio between positive and negative opinions expressed towards specific aspects of a product or service. Most work around ABSA focused on supervised sequence tagging based systems. Liu et al. (2015) showed promising results when the training and the inference data are from the same domain. However, this approach is typically not robust across different domains since aspect terms from two different domains are usually semantically different hence separated in the embedding 1 A demo video of ABSApp is available https://drive.google.com/open?id= 1BLk0xkjIOqyRhNy4UQEFQpDF_KR_NMAd. at 1 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 1–6 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics Figure 1: ABSApp workflow. Step 3: L"
D19-3001,S15-2082,0,0.0234083,"lysis. 1 Introduction Aspect Based Sentiment Analysis (ABSA) is the task of extracting, from a given corpus, opinion targets (aspect terms) and the sentiment expressed towards them. For example, in the sentence “The dessert was incredible”, the aspect term is dessert and the sentiment towards it is positive. This finegrained trait of ABSA makes it an effective application for measuring and monitoring the ratio between positive and negative opinions expressed towards specific aspects of a product or service. Most work around ABSA focused on supervised sequence tagging based systems. Liu et al. (2015) showed promising results when the training and the inference data are from the same domain. However, this approach is typically not robust across different domains since aspect terms from two different domains are usually semantically different hence separated in the embedding 1 A demo video of ABSApp is available https://drive.google.com/open?id= 1BLk0xkjIOqyRhNy4UQEFQpDF_KR_NMAd. at 1 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 1–6 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics Figure 1: ABSApp workflow. Step 3: L"
D19-3001,S14-2004,0,0.0458663,"g a direct dependency relation between a negation term and the opinion term, the aspect-opinion pair polarity is reversed. 4 Evaluation Our evaluation objective is to show that the different algorithmic steps, namely, lexicon extraction and sentiment classification, produce usable results. An additional objective is to show that weak supervision of an aspect lexicon that was generated in an unsupervised manner produces comparable results to the recent transfer-learning based methods (Ding et al., 2017; Wang and Jialin Pan, 2018). For this purpose, we leveraged the data of SemEval 2014 task 4 (Pontiki et al., 2014), which tests the two main ABSA sub-tasks: aspect term extraction and aspect term polarity detection. Opinion Polarity Estimation The goal of this step is to set the binary sentiment polarity (positive or negative) of the opinion terms. Following Pablos et al. (2016), an opinion term polarity is assigned based on estimating whether it is semantically closer to a set of generic positive opinion terms or to a set of generic negative opinion terms. To produce those sets we used a subset of 47 positive terms and a subset of 47 negative terms de4 The threshold’s value was empirically determined bas"
D19-3001,J11-1002,0,0.403898,"del to classify data in a target domain. Ding et al. (2017) and Wang and Jialin Pan (2018) proposed using supervised RNNs for cross-domain aspect term extraction and for aspect and opinion term co-extraction. This approach showed encouraging results, however it requires a considerable amount of labeled data from the source domain which is often not practical in applied settings due to cost or legal considerations (relevant data is usually not available for commercial use). Another approach towards domain robustness is based on unsupervised methods. Hu and Liu (2004) used association rules and Qiu et al. (2011) used syntactic rules for aspect and opinion term co-extraction. Industrial setups usually lack labeled data for training and this is where unsupervised methods excel. However, these methods can be noisy (see the ABSApp-unsup baseline in Table 2). In this paper we show that weak supervision, namely a short manual process of editing lexicons that were generated by an unsupervised method, produces results that are comparable to We present ABSApp, a portable system for weakly-supervised aspect-based sentiment extraction 1 . The system is interpretable and user friendly and does not require labele"
D19-3001,P18-1202,0,0.0216586,"Missing"
E06-1052,fillmore-etal-2002-seeing,0,0.0101591,"tifies the template occurrences in sentences. The set of entailing templates may be collected either manually or automatically. We propose this configuration both as an algorithm for RE and as an evaluation scheme for paraphrase acquisition. The role of the syntactic matcher is to identify the different syntactic variations in which templates occur in sentences. Table 1 presents a list of generic syntactic phenomena that are known in the literature to relate to linguistic variability. A phenomenon which deserves a few words of explanation is the “transparent head noun” (Grishman et al., 1986; Fillmore et al., 2002). A transparent noun N1 typically occurs in constructs of the form ‘N1 preposition N2’ for which the syntactic relation involving N1, which is the head of the NP, applies to N2, the modifier. In the example in Table 1, ‘fragment’ is the transparent head noun while the relation ‘activate’ applies to Y as object. 4 4.1 Manual Data Analysis Protein Interaction Dataset Bunescu et al. (2005) proposed a set of tasks regarding protein name and protein interaction extraction, for which they manually tagged about 200 Medline abstracts previously known to contain human protein interactions (a binary sym"
E06-1052,J86-3002,0,0.263055,"ctic matcher which identifies the template occurrences in sentences. The set of entailing templates may be collected either manually or automatically. We propose this configuration both as an algorithm for RE and as an evaluation scheme for paraphrase acquisition. The role of the syntactic matcher is to identify the different syntactic variations in which templates occur in sentences. Table 1 presents a list of generic syntactic phenomena that are known in the literature to relate to linguistic variability. A phenomenon which deserves a few words of explanation is the “transparent head noun” (Grishman et al., 1986; Fillmore et al., 2002). A transparent noun N1 typically occurs in constructs of the form ‘N1 preposition N2’ for which the syntactic relation involving N1, which is the head of the NP, applies to N2, the modifier. In the example in Table 1, ‘fragment’ is the transparent head noun while the relation ‘activate’ applies to Y as object. 4 4.1 Manual Data Analysis Protein Interaction Dataset Bunescu et al. (2005) proposed a set of tasks regarding protein name and protein interaction extraction, for which they manually tagged about 200 Medline abstracts previously known to contain human protein in"
E06-1052,P04-1053,0,0.0120801,"Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE patterns. The goal of the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly. Hasegawa et al. (2004) performs unsupervised hierarchical clustering over a simple set of features. The algorithm does not extract entity pairs for a given relation from a set of documents but rather classifies all relations in a large corpus. This approach is more similar to text mining tasks than to classic IE problems. To conclude, several unsupervised approaches learn relevant IE templates for a complete scenario, but without identifying their relevance to each specific relation within the scenario. Accordingly, the evaluations of these works either did not address the direct applicability for RE or evaluated i"
E06-1052,P02-1006,0,0.0430257,"s sufficient to identify an entailing template since it implies that the target relation holds as well. Under this notion, paraphrases are bidirectional entailment relations. Several methods extract atomic paraphrases by exhaustively processing local corpora (Lin and Pantel, 2001; Shinyama et al., 2002). Learning from a local corpus is bounded by the corpus scope, which is usually domain specific (both works above processed news domain corpora). To cover a broader range of domains several works utilized the Web, while requiring several manually provided examples for each input relation, e.g. (Ravichandran and Hovy, 2002). Taking a step further, the TEASE algorithm (Szpektor et al., 2004) provides a completely unsupervised method for acquiring entailment relations from the Web for a given input relation (see Section 5.1). Most of these works did not evaluate their results in terms of application coverage. Lin and Pantel (2001) compared their results to humangenerated paraphrases. Shinyama et al. (2002) measured the coverage of their learning algorithm relative to the paraphrases present in a given corpus. Szpektor et al. (2004) measured “yield”, the number of correct rules learned for an input re1 See the 3rd"
E06-1052,P05-1047,0,0.00703089,"standard RE setting. Our findings are encouraging for both goals, particularly relative to their early maturity level, and reveal constructive evidence for the remaining room for improvement. 2 2.1 Background Unsupervised Information Extraction Information Extraction (IE) and its subfield Relation Extraction (RE) are traditionally performed in a supervised manner, identifying the different ways to express a specific information or relation. Given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE"
E06-1052,P03-1029,0,0.0139967,"osed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtrees within relevant documents as IE patterns. The goal of the algorithm is event extraction, though performance is measured by counting argument entities rather than counting events directly. Hasegawa et al. (2004) performs unsupervised hierarchical clustering over a simple set of features. The algorithm does not extract entity pairs for a given relation from a set of documents but rather classifies all relations in a large corpus. This approach is more similar to text mining tasks than to classic IE problems. To conclude, several unsupervised approaches learn relevant"
E06-1052,W04-3206,1,0.897468,"tion, we require a syntactic matching module that identifies template instances in text. First, we manually analyzed the proteininteraction dataset and identified all cases in which protein interaction is expressed by an entailing template. This set a very high idealized upper bound for the recall of the paraphrase-based approach for this dataset. Yet, obtaining high coverage in practice would require effective paraphrase acquisition and lexical-syntactic template matching. Next, we implemented a prototype that utilizes a state-of-the-art method for learning entailment relations from the web (Szpektor et al., 2004), the Minipar dependency parser (Lin, 1998) and a syntactic matching module. As expected, the performance of the implemented system was much lower than the ideal upper bound, yet obtaining quite reasonable practical results given its unsupervised nature. The contributions of our investigation follow 409 the dual goal set above. To the best of our knowledge, this is the first comprehensive evaluation that measures directly the performance of unsupervised paraphrase acquisition relative to a standard application dataset. It is also the first evaluation of a generic paraphrase-based approach for"
E06-1052,C00-2136,0,0.0273469,"rase-based approach for the standard RE setting. Our findings are encouraging for both goals, particularly relative to their early maturity level, and reveal constructive evidence for the remaining room for improvement. 2 2.1 Background Unsupervised Information Extraction Information Extraction (IE) and its subfield Relation Extraction (RE) are traditionally performed in a supervised manner, identifying the different ways to express a specific information or relation. Given that annotated data is expensive to produce, unsupervised or weakly supervised methods have been proposed for IE and RE. Yangarber et al. (2000) and Stevenson and Greenwood (2005) define methods for automatic acquisition of predicate-argument structures that are similar to a set of seed relations, which represent a specific scenario. Yangarber et al. (2000) approach was evaluated in two ways: (1) manually mapping the discovered patterns into an IE system and running a full MUC-style evaluation; (2) using the learned patterns to perform document filtering at the scenario level. Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Sudo et al. (2003) extract dependency subtr"
E09-1064,D07-1017,0,0.0217251,"nces from the sentences retrieved for that rule. As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of h are found. In total, 1550 unique sentences were sampled and annotated by two annotators. To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences. The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h0 and h, respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 We evaluated the following resources: WordNet (WNd ): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ⇒ e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k ): Snow et al. (2006) presented a probabilist"
E09-1064,P98-2127,0,0.103573,"semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 558 entailment for any sub-sentential hypotheses, including compositional ones: are shown to provide many useful relationships which are missing from other resources, but these are embedded amongst many irrelevant ones. Additionally, the results highlight the need"
E09-1064,H93-1061,0,0.0594952,"able, i.e. the RHS of the rule is unlikely to have the exact same role in the text as the LHS. Many inference sys5.4 Rules Priors In Section 5.1.1 we observed that some resources are highly sensitive to context. Hence, when considering the validity of a rule’s application, two factors should be regarded: the actual context in which the rule is to be applied, as well as the rule’s prior likelihood to be valid in an arbitrary context. Somewhat indicative, yet mostly indirect, information about rules’ priors is contained in some resources. This includes sense ranks in WordNet, SemCor statistics (Miller et al., 1993), and similarity scores and rankings in Lin’s resources. Inference systems often incorporated this information, typically as top-k or threshold-based filters (Pantel and Lin, 2003; Roth and Sammons, 2007). By empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. Fo"
E09-1064,P01-1052,0,0.174838,"e is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 558 entailment for any sub-sentential hypotheses, including compositional ones: are shown to provide many useful relationships which are missing from other resources,"
E09-1064,N03-4011,0,0.018216,"are highly sensitive to context. Hence, when considering the validity of a rule’s application, two factors should be regarded: the actual context in which the rule is to be applied, as well as the rule’s prior likelihood to be valid in an arbitrary context. Somewhat indicative, yet mostly indirect, information about rules’ priors is contained in some resources. This includes sense ranks in WordNet, SemCor statistics (Miller et al., 1993), and similarity scores and rankings in Lin’s resources. Inference systems often incorporated this information, typically as top-k or threshold-based filters (Pantel and Lin, 2003; Roth and Sammons, 2007). By empirically assessing the effect of several such filters in our setting, we found that this type of data is indeed informative in the sense that precision increases as the threshold rises. Yet, no specific filters were found to improve results in terms of F1 score (where recall is measured relatively to the yield of the unfiltered resource) due to a significant drop in relative recall. For example, Lin564 breastfeeding ⇒ baby and hospital ⇒ medical. Hence, Definition 2 is more broadly applicable for defining the desired contents of lexical entailment resources. We"
E09-1064,N04-1041,0,0.0498428,"Missing"
E09-1064,N07-1071,0,0.0561535,"Missing"
E09-1064,W07-1418,0,0.290291,"rks an incorrect rule. We suggest that the definitions and methodologies can be applied for other parts of speech as well. 3 4 561 Available at http://ai.stanford.edu/˜ rion/swn Ravichandran, 2004)5 . Given a cluster label e, an entailment rule e’ ⇒ e is created for each member e0 of the cluster. Lin Dependency Similarity (Lin-dep): A distributional word similarity resource based on syntactic-dependency features (Lin, 1998). Given a term e and its list of similar terms, we construct for each e0 in the list the rule e’ ⇒ e. This resource was previously used in textual entailment engines, e.g. (Roth and Sammons, 2007). Lin Proximity Similarity (Lin-prox): A knowledgebase of terms with their cooccurrencebased distributionally similar terms. Rules are created from this resource as from the previous one6 . Wikipedia first sentence (WikiFS): Kazama and Torisawa (2007) used Wikipedia as an external knowledge to improve Named Entity Recognition. Using the first step of their algorithm, we extracted from the first sentence of each page a noun that appears in a is-a pattern referring to the title. For each such pair we constructed a rule title ⇒ noun (e.g. Michelle Pfeiffer ⇒ actress). Resource Snow30k WNd XWN? Wi"
E09-1064,C04-1036,1,0.894747,"Missing"
E09-1064,W07-1401,1,0.918818,"e potential to improve performance by examining non-standard relation types and by distilling the output of distributional methods. Further, our results stress the need to include auxiliary information regarding the lexical and logical contexts in which a lexical inference is valid, as well as its prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). Proceedings of the 12th Conference of the European Chapter of the ACL, pag"
E09-1064,P06-1101,0,0.180129,"ts prior validity likelihood. 1 Introduction Lexical information plays a major role in semantic inference, as the meaning of one term is often inferred form another. Lexical-semantic resources, which provide the needed knowledge for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 558 entailment for any sub-sentential hypotheses, including compositional ones: are shown to provide many useful relationships which are missing from other resources, but these are embedd"
E09-1064,D07-1026,0,0.0348238,"Missing"
E09-1064,W06-1621,1,0.886802,"ural (non-anecdotal) texts containing e’ which entail e, such that the reference to the meaning of e can be implied solely from the meaning of e’ in the text. (Entailment of e by a text follows Definition 1). We refer to this relation in this paper as lexical entailment1 , and call e’ ⇒ e a lexical entailment rule. e0 is referred to as the rule’s left hand side (LHS) and e as its right hand side (RHS). Entailment of Sub-sentential Hypotheses We first seek a definition that would capture the entailment relationship between a text and a subsentential hypothesis. A similar goal was addressed in (Glickman et al., 2006), who defined the notion of lexical reference to model the fact that in order to entail a hypothesis, the text has to entail each non-compositional lexical element within it. We suggest that a slight adaptation of their definition is suitable to capture the notion of Currently there are no knowledge resources designed specifically for lexical entailment modeling. Hence, the types of relationships they capture do not fully coincide with entailment inference needs. Thus, the definition suggests a specification for the rules that should be provided by 1 559 Section 6 discusses other definitions o"
E09-1064,P07-1058,1,0.83073,"h, we sampled 10 sentences from the sentences retrieved for that rule. As a baseline, we also sampled 10 sentences for each original hypothesis h in which both words of h are found. In total, 1550 unique sentences were sampled and annotated by two annotators. To assess the validity of our evaluation methodology, the annotators first judged a sample of 220 sentences. The Kappa scores for inter-annotator agreement were 0.74 and 0.64 for judging h0 and h, respectively. These figures correspond to substantial agreement (Landis and Koch, 1997) and are comparable with related semantic annotations (Szpektor et al., 2007; Bhagat et al., 2007). 4.2 We evaluated the following resources: WordNet (WNd ): There is no clear agreement regarding which set of WordNet relations is useful for entailment inference. We therefore took a conservative approach using only synonymy and hyponymy rules, which typically comply with the lexical entailment relation and are commonly used by textual entailment systems, e.g. (Herrera et al., 2005; Bos and Markert, 2006). Given a term e, we created a rule e’ ⇒ e for each e0 amongst the synonyms or direct hyponyms for all senses of e in WordNet 3.0. Snow (Snow30k ): Snow et al. (2006) p"
E09-1064,P08-1078,1,0.905431,"Missing"
E09-1064,D07-1073,0,0.0630385,"e for lexical inference, are commonly utilized by applied inference systems (Giampiccolo et al., 2007) and applications such as Information Retrieval and Question Answering (Shah and Croft, 2004; Pasca and Harabagiu, 2001). Beyond WordNet (Fellbaum, 1998), a wide range of resources has been developed and utilized, including extensions to WordNet (Moldovan and Rus, 2001; Snow et al., 2006) and resources based on automatic distributional similarity methods (Lin, 1998; Pantel and Lin, 2002). Recently, Wikipedia is emerging as a source for extracting semantic relationships (Suchanek et al., 2007; Kazama and Torisawa, 2007). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 558–566, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 558 entailment for any sub-sentential hypotheses, including compositional ones: are shown to provide many useful relationships which are missing from other resources, but these are embedded amongst many irrelevant ones. Additionally, the results highlight the need to represent and inference over various aspects of contextual information, which affect the applicability of lexical inferences. We suggest that these gaps s"
E09-1064,kouylekov-magnini-2006-building,0,0.28792,"Missing"
E09-1064,N06-1006,0,\N,Missing
E09-1064,C98-2122,0,\N,Missing
H05-1017,W99-0613,0,0.250205,"ct the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expectation-Maximization schem"
H05-1017,C00-1066,0,0.0228791,"ng for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a b"
H05-1017,P04-1033,0,0.365294,"sk is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a binary criterion, considering each see"
H05-1017,W99-0908,0,0.708007,"re research. 2 Bootstrapping for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be"
H05-1017,P95-1026,0,0.231154,"fficult to collect the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expe"
H05-1017,W05-0608,1,\N,Missing
H05-1123,W02-2009,1,0.853918,"ystems. This work explores the identification of thematic correspondences in texts through an extension of the well known data clustering problem. Previous works aimed at identifying – through clusters of words – concepts, sub-topics or themes that are prominent within a corpus of texts (e.g., Pereira et al., 1993; Li, 2002; Lin and Pantel, 2002). The current work deals with extending this line of research to identify corresponding themes across a corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics. This research task has been defined quite recently (Dagan et al., 2002), and has not been explored extensively yet. One could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalencies in news published in different countries, and so on. The experimental part of our work deals with revealing correspondences between different religions: Buddhism, Christianity, Hinduism, Islam and Judaism. Given a pre-partition of the corpus to sub-corpora, one for each religion, our method exposes common aspects for all religions, such as sacred writings, festivals and suff"
H05-1123,C02-1144,0,0.0277885,"ommon base to learning is not straightforward. Several previous computational models of analogy making (e.g. Falkenhainer et al., 1989) suggested symbolic computational mechanisms for constructing detailed mappings that connect corresponding ingredients across analogized systems. This work explores the identification of thematic correspondences in texts through an extension of the well known data clustering problem. Previous works aimed at identifying – through clusters of words – concepts, sub-topics or themes that are prominent within a corpus of texts (e.g., Pereira et al., 1993; Li, 2002; Lin and Pantel, 2002). The current work deals with extending this line of research to identify corresponding themes across a corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics. This research task has been defined quite recently (Dagan et al., 2002), and has not been explored extensively yet. One could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalencies in news published in different countries, and so on. The experimental part of our work deals with revealin"
H05-1123,P93-1024,0,0.310954,"ituations or domains where the common base to learning is not straightforward. Several previous computational models of analogy making (e.g. Falkenhainer et al., 1989) suggested symbolic computational mechanisms for constructing detailed mappings that connect corresponding ingredients across analogized systems. This work explores the identification of thematic correspondences in texts through an extension of the well known data clustering problem. Previous works aimed at identifying – through clusters of words – concepts, sub-topics or themes that are prominent within a corpus of texts (e.g., Pereira et al., 1993; Li, 2002; Lin and Pantel, 2002). The current work deals with extending this line of research to identify corresponding themes across a corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics. This research task has been defined quite recently (Dagan et al., 2002), and has not been explored extensively yet. One could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalencies in news published in different countries, and so on. The experimental par"
J09-3004,P01-1008,0,0.0276639,"Missing"
J09-3004,P99-1016,0,0.0107399,"s: simWJ (w, v) =  min(weight(w,f ),weight(v,f ))  f ∈F(w)∩F(v) f ∈F(w)∪F(v) max(weight(w,f ),weight(v,f )) where F(w) and F(v) are the sets of active features of the two words w and v. The appealing property of this measure is that it considers the association weights rather than just the number of common features. 2. 440 The standard Cosine measure (COS), which is popularly employed for information retrieval (Salton and McGill 1983) and also utilized for (3) Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality learning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch, Wang, and Rachakonda 1999; Pantel and Ravichandran 2004), is deﬁned as follows: simCOS (w, v) = √  f (weight(w,f )·weight(v,f )) 2 f (weight(w,f )) · √ f (weight(v,f )) 2 (4) This measure computes the cosine of the angle between the two feature vectors, which normalizes the vector lengths and thus avoids inﬂated discrimination between vectors of signiﬁcantly different lengths. 3. A popular state of the art measure has been developed by Lin (1998), motivated by Information Theory principles. This measure behaves quite similarly to the weighted Jaccard measure (Weeds, Weir, and McCar"
J09-3004,W04-3205,0,0.0474851,"Missing"
J09-3004,J90-1003,0,0.0946498,"lment. Next, we analyzed the typical behavior of existing word similarity measures relative to the lexical entailment criterion. Choosing the commonly used measure of Lin (1998) as a representative case, the analysis shows that quite noisy feature vectors are a major cause for generating rather “loose” semantic similarities. On the other hand, one may expect that features which seem to be most characteristic for a word’s meaning should receive the highest feature weights. This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990). Following these observations, we developed a bootstrapping formula that improves the original feature weights (Section 4), leading to better feature vectors and better similarity predictions. The general idea is to promote the weights of features that are common for semantically similar words, since these features are likely to be most characteristic for the word’s meaning. This idea is implemented by a bootstrapping scheme, where the initial (and cruder) similarity measure provides an initial approximation for semantic word similarity. The bootstrapping method yields a high co"
J09-3004,P05-1004,0,0.0134975,"Missing"
J09-3004,W02-0908,0,0.607653,"tel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), deﬁned by: weightMI (w, f ) = log2 P(w,f ) P(w)P( f ) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weightMI (w, f ) = log2 count(w, f ) · nrels count(w) · count( f ) (2) where count(w, f ) is the frequency of the co-occurrence pair w,f  in S, count(w) and count( f ) are the independent frequencies of w and f in S, and nrels is the size of S. High MI weights are assumed to correspond to strong word–feature associations. 439 Computational Linguistics Volume 35, Number 3 Curran and Moens (2002) argue that, generally, informative features are statistically correlated with their corresponding headword. Thus, they suggest that any statistical test used for collocations is a good starting point for improving featureweight functions. In their experiments the t-test-based metric yielded the best empirical performance. However, a known weakness of MI and most of the other statistical weighting functions used for collocation extraction, including t-test and χ2 , is their tendency to inﬂate the weights for rare features (Dunning 1993). In addition, a major property of lexical collocations is"
J09-3004,J93-1003,0,0.0830494,"9 Computational Linguistics Volume 35, Number 3 Curran and Moens (2002) argue that, generally, informative features are statistically correlated with their corresponding headword. Thus, they suggest that any statistical test used for collocations is a good starting point for improving featureweight functions. In their experiments the t-test-based metric yielded the best empirical performance. However, a known weakness of MI and most of the other statistical weighting functions used for collocation extraction, including t-test and χ2 , is their tendency to inﬂate the weights for rare features (Dunning 1993). In addition, a major property of lexical collocations is their “non-substitutability”, as termed in Manning and Schutze (1999). That is, typically neither a headword nor a modiﬁer in the collocation can be substituted by their synonyms or other related terms. This implies that using modiﬁers within strong collocations as features for a head word would provide a rather small amount of common features for semantically similar words. Hence, these functions seem less suitable for learning broader substitutability relationships, such as lexical entailment. Similarity measures that utilize MI weig"
J09-3004,H92-1021,0,0.0135257,"ty-based generalizations for smoothing word co-occurrence probabilities, in applications such as language modeling and disambiguation. For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document)"
J09-3004,W04-0706,0,0.0442322,"nformation theoretic measures, as introduced and reviewed in Lee (1997, 1999). In the current work we experiment with the following three popular similarity measures. 1. The basic Jaccard measure compares the number of common features with the overall number of features for a pair of words. One of the weighted generalizations of this scheme to non-binary values replaces intersection with minimum weight, union with maximum weight, and set cardinality with summation. This measure is commonly referred to as weighted Jaccard (WJ) (Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan 2000; Gasperin and Vieira 2004), deﬁned as follows: simWJ (w, v) =  min(weight(w,f ),weight(v,f ))  f ∈F(w)∩F(v) f ∈F(w)∪F(v) max(weight(w,f ),weight(v,f )) where F(w) and F(v) are the sets of active features of the two words w and v. The appealing property of this measure is that it considers the association weights rather than just the number of common features. 2. 440 The standard Cosine measure (COS), which is popularly employed for information retrieval (Salton and McGill 1983) and also utilized for (3) Zhitomirsky-Geffet and Dagan Bootstrapping Distributional Feature Vector Quality learning distributionally similar"
J09-3004,C04-1036,1,0.869143,"f distributional word feature vectors. The article describes the methodology, deﬁnitions, and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance. 1.2 Main Contributions and Outline As a starting point for our investigation, an operational deﬁnition was needed for evaluating the correctness of candidate pairs of similar words. Following the lexical substitution motivation, in Section 3 we formulate the substitutable lexical entailment relation (or lexical entailment, for brevity), reﬁning earlier deﬁnitions in Geffet and Dagan (2004, 2005). Generally speaking, this relation holds for a pair of words if a possible meaning of one word entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. Lexical entailment overlaps partly with traditional lexical semantic relationships, while capturing more generally the lexical substitution needs of applications. Empirically, high inter-annotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment. Next, we analyzed the typical behavior of existing word similarity measures re"
J09-3004,P05-1014,1,0.899535,"ks of the features for pairs of words. This measure estimates the ability of a feature weighting method to distinguish between pairs of similar vs. non-similar words. To the best of our knowledge this is the ﬁrst proposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure. The ability to identify the most characteristic features of words can have additional beneﬁts, beyond their impact on traditional word similarity measures (as evaluated in this article). A demonstration of such potential appears in Geffet and Dagan (2005), which presents a novel feature inclusion scheme for vector comparison. That scheme utilizes our bootstrapping method to identify the most characteristic features of a word and then tests whether these particular features co-occur also with a hypothesized entailed word. The empirical success reported in that paper provides additional evidence for the utility of the bootstrapping method. More generally, our motivation and methodology can be extended in several directions by future work on acquiring lexical entailment or other lexical-semantic relations. One direction is to explore better vecto"
J09-3004,P05-1050,0,0.0287927,"Missing"
J09-3004,P90-1034,0,0.287604,"ted by applying a novel quantitative measurement of the quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied. 1. Introduction 1.1 Motivation Distributional word similarity has long been an active research area (Hindle 1990; Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and ∗ Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: zhitomim@mail.biu.ac.il. ∗∗ Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: dagan@cs.biu.ac.il. Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication: 21 November 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 3 Weir 2005). This paradigm is inspired by Harris’s distributional hypothesis"
J09-3004,W96-0104,0,0.0600883,"abilities, in applications such as language modeling and disambiguation. For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be subst"
J09-3004,P99-1004,0,0.0856237,"Missing"
J09-3004,P93-1016,0,0.0412884,"o-occurs with w under the syntactic dependency relation syn rel. The feature role ( f role) corresponds to the role of the feature word fw in the syntactic dependency, being either the head (denoted h) or the modiﬁer (denoted m) of the relation. For example, given the word company, the feature earnings, gen, h corresponds to the genitive relationship company’s earnings, and investor, pcomp of, m corresponds to the prepositional complement relationship the company of the investor.2 Throughout this article we use syntactic dependency relationships generated by the Minipar dependency parser (Lin 1993). Table 1 lists common Minipar dependency relations involving nouns. Minipar also identiﬁes multi-word expressions, which is 1 A preliminary version of the bootstrapping method was presented in Geffet and Dagan (2004). That paper presented initial results for the bootstrapping scheme, when applied only over Lin’s measure and tested by the manually judged dataset of lexical entailment. The current research extends our initial results in many respects. It reﬁnes the deﬁnition of lexical entailment; utilizes a revised test set of larger scope and higher quality, annotated by three assessors; exte"
J09-3004,P98-2127,0,0.638084,"he quality of feature weighting functions. Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector. Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied. 1. Introduction 1.1 Motivation Distributional word similarity has long been an active research area (Hindle 1990; Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and ∗ Department of Information Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: zhitomim@mail.biu.ac.il. ∗∗ Department of Computer Science, Bar-Ilan University, Ramat-Gan, Israel. E-mail: dagan@cs.biu.ac.il. Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication: 21 November 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 3 Weir 2005). This paradigm is inspired by Harris’s distributional hypothesis (Harris 1968), which states that semantically simi"
J09-3004,P95-1025,0,0.0955644,"Missing"
J09-3004,P06-2075,1,0.680013,"Missing"
J09-3004,W97-0323,0,0.0495219,"ge modeling and disambiguation. For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be substituted by another one in a"
J09-3004,P96-1006,0,0.0363662,"ns such as language modeling and disambiguation. For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be substituted by another"
J09-3004,J07-2002,0,0.0597043,"vectors of pairs of words are compared by a vector similarity measure. The following two subsections review typical methods for each phase. 2.1 Features and Weighting Functions In the typical computational setting, word contexts are represented by feature vectors. A feature represents another word (or term) w with which w co-occurs, and possibly speciﬁes also the syntactic relationship between the two words, as in Grefenstette (1994), Lin (1998), and Weeds and Weir (2005). Thus, a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f . Pado and Lapata (2007) demonstrate that using syntactic dependency-based features helps to distinguish among classes of lexical relations, which seems to be more difﬁcult when using “bag of words” features that are based on co-occurrence in a text window. A syntactic-based feature f for a word w is deﬁned as a triple:  fw, syn rel, f role where fw is a context word (or term) that co-occurs with w under the syntactic dependency relation syn rel. The feature role ( f role) corresponds to the role of the feature word fw in the syntactic dependency, being either the head (denoted h) or the modiﬁer (denoted m) of the"
J09-3004,N04-1041,0,0.0488482,"antiﬁes the degree of statistical association between w and f in the set S. For example, some feature weighting functions are based on the logarithm of the word–feature co-occurrence frequency (Ruge 1992), or on the conditional probability of the feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999; Lee 1999). Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), deﬁned by: weightMI (w, f ) = log2 P(w,f ) P(w)P( f ) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weightMI (w, f ) = log2 count(w, f ) · nrels count(w) · count( f ) (2) where count(w, f ) is the frequency of the co-occurrence pair w,f  in S, count(w) and count( f ) are the independent frequencies of w and f in S, and nrels is the size of S. High MI weights are assumed to correspond to strong word–feature associations. 439 Computational Linguistics Volume 35, Numb"
J09-3004,C04-1115,0,0.0448513,"Missing"
J09-3004,P93-1024,0,0.0880589,"Missing"
J09-3004,W04-3206,1,0.33847,"Missing"
J09-3004,J05-4002,0,0.101407,"me that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus. Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus. Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts. A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution. Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) summarization, need to recognize that one word can be substituted by another one in a given context while preserving, or entailing the original mea"
J09-3004,N03-1036,0,0.0125662,"Missing"
J09-3004,C92-2070,0,0.0306502,"a of similarity-based estimation of co-occurrence likelihood was applied in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a Latent Semantic Analysis (LSA)-based kernel function as a similarity-based representation for WSD. Other works employed the same idea for pseudo-word sense disambiguation, as explained in the next subsection. 7.2 The Pseudo-Word Sense Disambiguation Setting Sense disambiguation typically requires annotated training data, created with considerable human effort. Yarowsky (1992) suggested that when using WSD as a test bed for comparative algorithmic evaluation it is possible to set up a pseudo-word sense disambiguation scheme. This scheme was later adopted in several experiments, and was popular for comparative evaluations of similarity-based co-occurrence likelihood estimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005). We followed closely the same experimental scheme, as described subsequently. First, a list of pseudo-words is constructed by “merging” pairs of words into a single pseudo word. In our experiment each pseudo-word constitutes a pair"
J09-3004,C04-1146,0,\N,Missing
J09-3004,W07-1420,0,\N,Missing
J09-3004,J10-4006,0,\N,Missing
J09-3004,W07-1401,1,\N,Missing
J09-3004,C98-2122,0,\N,Missing
J12-1003,P04-1051,0,0.084713,"Missing"
J12-1003,P98-1013,0,0.0368907,"Missing"
J12-1003,P10-2045,1,0.837245,"Missing"
J12-1003,P10-1124,1,0.845704,"Missing"
J12-1003,P11-1062,1,0.416945,"Missing"
J12-1003,D07-1017,0,0.113317,"Missing"
J12-1003,J06-1003,0,0.0040761,"lobal methods that perform inference over a larger set of predicates. 2.1 Local Learning Three types of information have primarily been utilized in the past to learn entailment rules between predicates: lexicographic methods, distributional similarity methods, and pattern-based methods. Lexicographic methods use manually prepared knowledge bases that contain information about semantic relations between lexical items. WordNet (Fellbaum 1998b), by far the most widely used resource, speciﬁes relations such as hyponymy, synonymy, derivation, and entailment that can be used for semantic inference (Budanitsky and Hirst 2006). For example, if WordNet speciﬁes that reduce is a hyponym of affect, then one can infer that X reduces Y → X affects Y. WordNet has also been exploited to automatically generate a training set for a hyponym classiﬁer (Snow, Jurafsky, and Ng 2004), and we make a similar use of WordNet in Section 4.1. A drawback of WordNet is that it speciﬁes semantic relations for words and terms but not for more complex expressions. For example, WordNet does not cover a complex predicate such as X causes a reduction in Y. Another drawback of WordNet is that it only supplies semantic relations between lexical"
J12-1003,W04-3205,0,0.887671,"tifying the existence of semantic similarity between predicates, they are often unable to discern the exact type of semantic similarity and speciﬁcally determine whether it is entailment. Pattern-based methods are used to automatically extract pairs of predicates for a speciﬁc semantic relation. Pattern-based methods identify a semantic relation between two predicates by observing that they co-occur in speciﬁc patterns in sentences. For example, from the single proposition He scared and even startled me one might infer that startle is semantically stronger than scare and thus startle → scare. Chklovski and Pantel (2004) manually constructed a few dozen patterns and learned semantic relations between predicates by looking for these patterns on the Web. For example, the pattern X and even Y implies that Y is stronger than X, and the pattern to X and then Y indicates that Y follows X. The main disadvantage of pattern-based methods is that they are based on the co-occurrence of two predicates in a single sentence in a speciﬁc pattern. These events are quite rare and require working on a very large corpus, or preferably, the Web. Pattern-based methods were mainly utilized so far to extract semantic relations betw"
J12-1003,W07-1409,0,0.0182793,"hed line. 3.2 Focused Entailment Graphs In this article we concentrate on learning a type of entailment graph, termed the focused entailment graph. Given a target concept, such as nausea, a focused entailment graph describes the entailment relations between propositional templates for which the target concept is one of the arguments (see Figure 1). Learning such entailment rules in real time for a target concept is useful in scenarios such as information retrieval and question answering, where a user speciﬁes a query about the target concept. The need for such rules has been also motivated by Clark et al. (2007), who investigated what types of knowledge are needed to identify entailment in the context of the RTE challenge, and found that often rules that are speciﬁc to a certain concept are required. Another example for a semantic inference algorithm that is utilized in real time is provided by Do and Roth (2010), who recently described a system that, given two terms, determines the taxonomic relation between them on the ﬂy. Last, we have recently suggested an application that uses focused entailment graphs to present information about a target concept according to a hierarchy of entailment (Berant,"
J12-1003,D10-1107,0,0.268848,"ers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment graph that describes the entailment relations between propositional templates (Section 3.1), and a speciﬁc type of entailment graph, termed the focused entailment graph, that concentrates on entailment relations that are relevant for some pre-deﬁned target concept (Section 3.2). 3.1 Entailment Graph: Deﬁnition and Properties The nodes of an en"
J12-1003,P08-2012,0,0.100708,"ptimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment graph that describes the entailment relations bet"
J12-1003,N03-1013,0,0.0355236,"at express information that is diverse and orthogonal to the one given by distributional similarity. Therefore, we turn to existing knowledge resources that were created using both manual and automatic methods, expressing various types of linguistic and statistical information that is relevant for entailment prediction: 1. WordNet: contains manually annotated relations such as hypernymy, synonymy, antonymy, derivation, and entailment. 2. VerbOcean11 (Chklovski and Pantel 2004): contains verb relations such as stronger-than and similar that were learned with pattern-based methods. 3. CATVAR12 (Habash and Dorr 2003): contains word derivations such as develop–development. 4. FRED13 (Ben Aharon, Szpektor, and Dagan 2010): contains entailment rules between templates learned automatically from FrameNet. 5. NomLex14 (Macleod et al. 1998): contains English nominalizations including their argument mapping to the corresponding verbal form. 6. BAP15 (Kotlerman et al. 2010): contains directional distributional similarity scores between lexical terms (rather than propositional templates) calculated with the BAP similarity scoring function. Table 12 describes the 16 new features that were generated for each of the g"
J12-1003,P98-2127,0,0.331451,"lates: X ←−− buys −→ Y, X ←− buys −−→ for −−−−−→ Y and X ←−− buys prep pcomp−n −−→ for −−−−−→ Y. For each binary template Lin and Pantel compute two sets of features Fx and Fy , which are the words that instantiate the arguments X and Y, respectively, in a large corpus. Given a template t and its feature set for the X variable Ftx , every fx ∈ Ftx is weighted by the pointwise mutual information between the template and the feature: Pr( f |t) wtx ( fx ) = log Pr( xfx ) , where the probabilities are computed using maximum likelihood over the corpus. Given two templates u and v, the Lin measure (Lin 1998a) is computed for the variable X in the following manner:  u v f ∈Fux ∩Fvx [wx ( f ) + wx ( f )]  Linx (u, v) =  u v f ∈Fux wx ( f ) + f ∈Fvx wx ( f ) (1) The measure is computed analogously for the variable Y and the ﬁnal distributional similarity score, termed DIRT, is the geometric average of the scores for the two variables:  DIRT(u, v) = Linx (u, v) · Liny (u, v) (2) If DIRT(u, v) is high, this means that the templates u and v share many “informative” arguments and so it is possible that u → v. Note, however, that the DIRT similarity measure computes a symmetric score, which is appro"
J12-1003,P09-1039,0,0.0847854,"Missing"
J12-1003,meyers-etal-2004-cross,0,0.0646719,"Missing"
J12-1003,P06-2075,1,0.816453,"Missing"
J12-1003,W06-1616,0,0.0281891,"Rn specify the constraints. In short, we wish to ﬁnd the optimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structure termed the entailment"
J12-1003,W04-2401,0,0.0469741,"A ∈ Rn × Rd and b ∈ Rn specify the constraints. In short, we wish to ﬁnd the optimal assignment for the d variables in the vector x, such 78 Berant et al. Learning Entailment Relations by Global Graph Structure Optimization that all n linear constraints speciﬁed by the matrix A and the vector b are satisﬁed by this assignment. If the variables are forced to be integers, the problem is termed an Integer Linear Program (ILP). ILP has attracted considerable attention recently in several ﬁelds of NLP, such as semantic role labeling, summarization, and parsing (Althaus, Karamanis, and Koller 2004; Roth and Yih 2004; Riedel and Clarke 2006; Clarke and Lapata 2008; Finkel and Manning 2008; Martins, Smith, and Xing 2009). In this article we formulate the entailment graph learning problem as an ILP, which leads to an optimal solution with respect to the objective function (vs. a greedy optimization algorithm suggested by Snow, Jurafsky, and Ng [2006]). Recently, Do and Roth (2010) used ILP in a related task of learning taxonomic relations between nouns, utilizing constraints between sibling nodes and ancestor–child nodes in small graphs of three nodes. 3. Entailment Graph In this section we deﬁne a structur"
J12-1003,D10-1106,0,0.0838487,"Number 1 by the arguments. One important type of entailment rule speciﬁes entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure → X affect blood pressure. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010). Previous work has focused on learning each entailment rule in isolation. It is clear, however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X → Y and Y → Z imply the rule X → Z.1 In this article we take advantage of these global interactions to improve entailment rule learning. After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3). Next, we motivate and discuss a speciﬁc type"
J12-1003,I05-5011,0,0.0311312,"Missing"
J12-1003,P05-1044,0,0.0109691,"hod proposed by Snow, Jurafsky, and Ng (2004) for training a noun hypernym classiﬁer. It differs in some important aspects, however: First, Snow, Jurafsky, and Ng consider a positive example to be any Wordnet hypernym, irrespective of the distance, whereas we look only at direct hypernyms. This is because predicates are mainly verbs and precision drops quickly when looking at verb hypernyms in WordNet at a longer distance. Second, Snow, Jurafsky, and Ng generate negative examples by looking at any two nouns where one is not the hypernym of the other. In the spirit of “contrastive estimation” (Smith and Eisner 2005), we prefer to generate negative examples that are “hard,” that is, negative examples that, although not entailing, are still semantically similar to positive examples and thus focus the classiﬁer’s attention on determining the boundary of the entailment class. Last, we use a balanced number of positive and negative examples, because classiﬁers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse, Khoshgoftaar, and Napolitano 2007; Nikulin 2008). (3) Distributional similarity representation. We aim to train a classiﬁer that for an input template pair (t1 , t2"
J12-1003,P06-1101,0,0.18016,"Missing"
J12-1003,N07-1031,0,0.0147238,"Missing"
J12-1003,C08-1107,1,0.654208,"Linguistics Computational Linguistics Volume 38, Number 1 by the arguments. One important type of entailment rule speciﬁes entailment between propositional templates, that is, propositions where the arguments are possibly replaced by variables. A rule corresponding to the aforementioned example may be X reduce blood pressure → X affect blood pressure. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010). Previous work has focused on learning each entailment rule in isolation. It is clear, however, that there are interactions between rules. A prominent phenomenon is that entailment is inherently a transitive relation, and thus the rules X → Y and Y → Z imply the rule X → Z.1 In this article we take advantage of these global interactions to improve entailment rule learning. After reviewing relevant background (Section 2), we describe a structure termed an entailment graph that models entailment relations between propositional templates (Secti"
J12-1003,W09-2504,1,0.948795,"buy, but does not describe the way in which arguments are mapped: if X pays Y for Z then X buys Z from Y. Thus, using WordNet directly to derive entailment rules between predicates is possible only for semantic relations such as hyponymy and synonymy, where arguments typically preserve their syntactic positions on both sides of the rule. Some knowledge bases try to overcome this difﬁculty: Nomlex (Macleod et al. 1998) is a dictionary that provides the mapping of arguments between verbs and their nominalizations and has been utilized to derive predicative entailment rules (Meyers et al. 2004; Szpektor and Dagan 2009). FrameNet (Baker, Fillmore, and Lowe 1998) is a lexicographic resource that is arranged around “frames”: Each frame corresponds to an event and includes information on the predicates and arguments relevant for that speciﬁc event supplemented with annotated examples that specify argument positions. Consequently, FrameNet was also used to derive entailment rules between predicates (Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional manually constructed resources for predicates include PropBank (Kingsbury, Palmer, and Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000)"
J12-1003,W04-3206,1,0.72226,"bow 2009; Ben Aharon, Szpektor, and Dagan 2010). Additional manually constructed resources for predicates include PropBank (Kingsbury, Palmer, and Marcus 2002) and VerbNet (Kipper, Dang, and Palmer 2000). Distributional similarity methods are used to learn broad-scale resources, because lexicographic resources tend to have limited coverage. Distributional similarity algorithms employ “the distributional hypothesis” (Harris 1954) and predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010), which differ in terms of the speciﬁcs of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Next, we elaborate on some of the prominent methods. 75 Computational Linguistics Volume 38, Number 1 Lin and Pantel (2001) proposed an algorithm that is based on a mutual information criterion. A predicate is represented by a binary template, which is a dependency path between two arguments of a predicate wh"
J12-1003,W03-1011,0,0.056599,"nal distributional similarity measure. In their work, Szpektor and Dagan chose to represent predicates with unary templates, which are identical to binary templates, only they contain a predsubj icate and a single argument, such as: X ←−− buys. Szpektor and Dagan explain that unary templates are more expressive than binary templates, and that some predicates can only be encoded using unary templates. They propose that if for two unary templates u → v, then relatively many of the features of u should be covered by the features of v. This is captured by the asymmetric Cover measure suggested by Weeds and Weir (2003) (we omit the subscript x from Fux and Fvx because in their setting there is only one argument):  u f ∈Fu ∩Fv w ( f ) Cover(u, v) =  u f ∈Fu w ( f ) (3) The ﬁnal directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure: BInc(u, v) = 76  Lin(u, v) · Cover(u, v) (4) Berant et al. Learning Entailment Relations by Global Graph Structure Optimization Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score for each argument separately, effectively decoupling the arguments from one another. It is clear, however, tha"
J12-1003,C98-1013,0,\N,Missing
J12-1003,2003.mtsummit-systems.9,0,\N,Missing
J12-1003,C98-2122,0,\N,Missing
J15-2003,D14-1059,0,0.0201683,"ance of learning graphs where predicates are marked by their various senses, which will result in a model that can directly benefit from the methods suggested in this article. 6. Conclusions The problem of language variability is at the heart of many semantic applications such as Information Extraction, Question Answering, Semantic Parsing, and more. Consequently, learning broad coverage knowledge bases of entailment rules and paraphrases (Ganitkevitch, Van Durme, and Callison-Burch 2013) has proved crucial for systems that perform inference over textual representations (Stern and Dagan 2012; Angeli and Manning 2014). 257 Computational Linguistics Volume 41, Number 2 In this article, we have presented algorithms for learning entailment rules between predicates that can scale to large sets of predicates. Our work builds on prior work by Berant, Dagan, and Goldberger (2012), who defined the concept of entailment graphs, and formulated entailment rule learning as a graph optimization problem, where the graph has to obey the structural constraint of transitivity. Our main contribution is a heuristic polynomial approximation algorithm that can learn entailment graphs containing tens of thousands of nodes. The"
J15-2003,P10-2045,1,0.817166,"Missing"
J15-2003,P12-1013,1,0.818566,"s. Section 3.2 describes the second step, in which we develop an efficient heuristic approximation based on the assumption that entailment graphs are forest reducible. Section 4 describes experiments on the first data set containing medium-sized entailment graphs with typed predicates. Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates. We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm. Section 6 concludes the article. This article is based on previous work (Berant, Dagan, and Goldberger 2011; Berant et al. 2012), but expands over it in multiple directions. Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past. Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in Berant et al. (2012) and achieves best results in our experimental evaluation. Theoretically, we provide an NP-hardness 223 Computational Linguistics Volume 41, Number 2 proof for the Max-Trans-Forest optimization problem presented in Berant et al. (2012) and an ILP formulation for it. Last, we p"
J15-2003,P10-1124,1,0.927307,"Missing"
J15-2003,P11-1062,1,0.894287,"Missing"
J15-2003,J12-1003,1,0.780195,"s. Section 3.2 describes the second step, in which we develop an efficient heuristic approximation based on the assumption that entailment graphs are forest reducible. Section 4 describes experiments on the first data set containing medium-sized entailment graphs with typed predicates. Section 5 presents an empirical evaluation on a large graph containing 20,000 untyped predicates. We also perform a qualitative analysis in Section 5.4 to further elucidate the behavior of our algorithm. Section 6 concludes the article. This article is based on previous work (Berant, Dagan, and Goldberger 2011; Berant et al. 2012), but expands over it in multiple directions. Empirically, we present results on a novel data set (Section 5) that is by orders of magnitude larger than in the past. Algorithmically, we present the Tree-Node-And-Component-Fix algorithm, which is an extension of the Tree-Node-Fix algorithm presented in Berant et al. (2012) and achieves best results in our experimental evaluation. Theoretically, we provide an NP-hardness 223 Computational Linguistics Volume 41, Number 2 proof for the Max-Trans-Forest optimization problem presented in Berant et al. (2012) and an ILP formulation for it. Last, we p"
J15-2003,P14-1133,1,0.802761,"LI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules"
J15-2003,D07-1017,0,0.080239,"Missing"
J15-2003,J06-1003,0,0.00884565,"utional similarity. We briefly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chk"
J15-2003,P11-1098,0,0.0303822,"and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al."
J15-2003,D11-1003,0,0.0248533,"eger, so many violations of transitivity may occur. The solution when applying LP relaxation is not a transitive graph; we will show in Section 4 that our approximation method is substantially faster. Global inference has gained popularity in recent years in NLP, and a common approximation method that has been extensively utilized is dual decomposition (Sontag, Globerson, and Jaakkola 2011). Dual decomposition has been successfully applied in 226 Berant et al. Efficient Global Learning of Entailment Graphs tasks such as Information Extraction (Reichart and Barzilay 2012), Machine Translation (Chang and Collins 2011), Parsing (Rush et al. 2012), and Named Entity Recognition (Wang, Che, and Manning 2013). To the best of our knowledge, it has not yet been applied for the task of learning entailment relations. The graph decomposition method we present in Section 3.1 can be viewed as an ideal case of dual decomposition, where we can decompose the problem into disjoint components in a way that we do not need to ensure consistency of the results obtained on each component separately. 3. Efficient Inference Our goal is to learn a large knowledge base of entailment rules between natural language predicates. Follo"
J15-2003,W04-3205,0,0.0684946,"e half of the data set for training, resulting in 1,224 positive examples and 2,060 negative examples. Another two training examples are X unable to pay Y ⇒ X owe Y and X own Y ; Y be sold to X. 2. Feature representation. Each pair of predicates (p1 , p2 ) is represented by a feature vector where the first six are distributional similarity features identical to the first six features described in Section 4.2. In addition, for pairs of predicates for which at least one distributional similarity feature is non-zero, we add lexicographic features computed from WordNet (Fellbaum 1998), VerbOcean (Chklovski and Pantel 2004), and CatVar (Habash and Dorr 2003), as well as string-similarity features. Table 2 provides the exact details of these features. A feature is computed for a pair of predicates (p1 , p2 ) where in this context a predicate is a pair (pred,rev): pred is the lexical realization of the predicate, and rev is a Boolean indicating whether arg1 is X and arg2 is Y or vice versa. Overall, each pair of predicates is represented by 27 features. 3. Training. After obtaining a feature representation for every pair of predicates, we train a Gaussian kernel SVM classifier that optimizes F1 (SVMperf implementa"
J15-2003,C10-2029,0,0.0255208,"that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al. 2013). In the experiments 2010; OS´ presented in this article we will use the representation of Schoenmackers et al. in one experiment, and ignore the problem of predicate ambiguity in the other. 225 Computational Linguistics Volume 41, Number 2 2.2 Global Learning The idea of global learning is that, by jointly learning semantic relations between a large number of natural language phrases, one can use the dependencies between the relations to improve accuracy. A natural way to model that is with a graph where nodes are phrases and edges represent semantic similarity. Snow, Ju"
J15-2003,D10-1107,0,0.0664476,"e local scores they look for the set of edges E that maximizes the objective function (i,j)∈E wij under the constraint that edges respect transitivity. They show that this optimization problem is NP-hard and find an exact solution using an ILP solver over small graphs. They also avoid problems of predicate ambiguity by partially contextualizing the predicates. In this article, we present efficient and scalable heuristic approximation algorithms for the optimization problem they propose. In recent years, there has been substantial work on approximation algorithms for global inference problems. Do and Roth (2010) suggested a method for the related task of learning taxonomic relations between terms. Given a pair of terms, they construct a small graph that contains the two terms and a few other related terms, and then impose constraints on the graph structure. They construct these small graphs because their work is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is ever explicitly constructed. Because they independently construct a graph for each pair of terms, their method easily produces solutions where global constraints, such"
J15-2003,D11-1142,0,0.0543923,"Missing"
J15-2003,N13-1092,0,0.0669501,"Missing"
J15-2003,N03-1013,0,0.0145222,"ulting in 1,224 positive examples and 2,060 negative examples. Another two training examples are X unable to pay Y ⇒ X owe Y and X own Y ; Y be sold to X. 2. Feature representation. Each pair of predicates (p1 , p2 ) is represented by a feature vector where the first six are distributional similarity features identical to the first six features described in Section 4.2. In addition, for pairs of predicates for which at least one distributional similarity feature is non-zero, we add lexicographic features computed from WordNet (Fellbaum 1998), VerbOcean (Chklovski and Pantel 2004), and CatVar (Habash and Dorr 2003), as well as string-similarity features. Table 2 provides the exact details of these features. A feature is computed for a pair of predicates (p1 , p2 ) where in this context a predicate is a pair (pred,rev): pred is the lexical realization of the predicate, and rev is a Boolean indicating whether arg1 is X and arg2 is Y or vice versa. Overall, each pair of predicates is represented by 27 features. 3. Training. After obtaining a feature representation for every pair of predicates, we train a Gaussian kernel SVM classifier that optimizes F1 (SVMperf implementation [Joachims 2005]), and tune the"
J15-2003,C92-2082,0,0.220143,"e, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods"
J15-2003,P98-2127,0,0.172773,"ariables such as X treat Y. For each binary predicate, Lin and Pantel compute two sets of features Fx and Fy , 224 Berant et al. Efficient Global Learning of Entailment Graphs which are the words that instantiate the arguments X and Y, respectively, in a large corpus. Given a predicate u and its feature set for the X variable Fx , every feature fx ∈ Fx is weighted by pointwise mutual information between the predicate and the feature: Pr( f |u) w( fx ) = log Pr( xfx ) , where the probabilities are computed using maximum likelihood over the corpus. Given two predicates u and v, the Lin measure (Lin 1998) is computed for the variable X in the following manner: P u v f ∈Fux ∩Fvx [wx ( f ) + wx ( f )] P Linx (u, v) = P u v f ∈Fux wx ( f ) + f ∈Fvx wx ( f ) (1) The measure is computed analogously for the variable Y and the final distributional similarity score, termed DIRT, is the geometric average of the scores for the two variables: If DIRT(u, v) is high, this means that the templates u and v share many “informative” arguments and so it is possible that u ⇒ v. Szpektor and Dagan (2008) suggested two modifications to DIRT. First, they looked at unary predicates, that is, predicates with a single"
J15-2003,P09-1039,0,0.0710535,"Missing"
J15-2003,P13-1131,1,0.856715,"on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivit"
J15-2003,meyers-etal-2004-cross,0,0.0323109,"nce, and (3) distributional similarity. We briefly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations betw"
J15-2003,D12-1104,0,0.109232,"Missing"
J15-2003,P10-1045,0,0.0128464,"ment variables that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al. 2013). In the experiments 2010; OS´ presented in this article we will use the representation of Schoenmackers et al. in one experiment, and ignore the problem of predicate ambiguity in the other. 225 Computational Linguistics Volume 41, Number 2 2.2 Global Learning The idea of global learning is that, by jointly learning semantic relations between a large number of natural language phrases, one can use the dependencies between the relations to improve accuracy. A natural way to model that is with a graph where nodes are phrases and edges represent semanti"
J15-2003,N07-1071,0,0.0106022,"However, it is clear that although this alleviates sparsity problems, considering pairs of arguments jointly provides more information. Yates and Etzioni (2009), Schoenmackers et al. (2010), and even earlier, Szpektor et al. (2004), presented methods that compute semantic similarity based on pairs of arguments. A problem common to all local methods presented above is predicate ambiguity— predicates may have different meanings and different entailment relations in different contexts. Some works resolved the problem of ambiguity by representing predicates with argument variables that are typed (Pantel et al. 2007; Schoenmackers et al. 2010). For example, argument variables in the work of Schoenmackers et al. were restricted to belong to one of 156 types, such as country or profession. A different solution that has attracted substantial attention recently is to represent the various contexts in which a predicate can appear in a low-dimensional latent space (for example, using Latent Dirichlet Allocation [Blei, Ng, and Jordan 2003]) and infer entailment relations between predicates based on the contexts in which they appear (Ritter, Mausam, and Etzioni ´ eaghdha 2010; Dinu and Lapata 2010; Melamud et al"
J15-2003,P02-1006,0,0.0363355,"pted for publication: 25 November 2014. doi:10.1162/COLI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inh"
J15-2003,N12-1008,0,0.0193099,"fractional value are rounded to their nearest integer, so many violations of transitivity may occur. The solution when applying LP relaxation is not a transitive graph; we will show in Section 4 that our approximation method is substantially faster. Global inference has gained popularity in recent years in NLP, and a common approximation method that has been extensively utilized is dual decomposition (Sontag, Globerson, and Jaakkola 2011). Dual decomposition has been successfully applied in 226 Berant et al. Efficient Global Learning of Entailment Graphs tasks such as Information Extraction (Reichart and Barzilay 2012), Machine Translation (Chang and Collins 2011), Parsing (Rush et al. 2012), and Named Entity Recognition (Wang, Che, and Manning 2013). To the best of our knowledge, it has not yet been applied for the task of learning entailment relations. The graph decomposition method we present in Section 3.1 can be viewed as an ideal case of dual decomposition, where we can decompose the problem into disjoint components in a way that we do not need to ensure consistency of the results obtained on each component separately. 3. Efficient Inference Our goal is to learn a large knowledge base of entailment ru"
J15-2003,C08-1092,0,0.0334601,"r tuples provided by Schoenmackers et al. We perform the following steps: 1. Training set generation Positive examples are generated using WordNet synonyms and hypernyms. Negative pairs are generated using WordNet direct co-hyponyms (sister terms), but we also utilize Word hyponyms at distance 2. In addition, we generate negative examples by randomly sampling pairs of typed predicates that share the same types. Table 1 provides an example for each type of automatically generated training example. It has been noted in the past that the WordNet verb hierarchy contains a certain amount of noise (Richens 2008; Roth and Frank 2012). However, we use WordNet only to generate examples for training a statistical classifier, and thus we can tolerate some noise in the generated examples. In fact, we have noticed that simple variants in training set generation do not result in substantial differences in classifier performance. 2. Feature representation Each example pair of typed predicates (p1 , p2 ) is represented by a feature vector, where each feature is a distributional similarity score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of typed predicates,"
J15-2003,W06-1616,0,0.0329696,"ole graph. Finding the undirected edges (Line 1) and computing connected components (Line 2) can be performed in O(V 2 ). Thus, in this case the efficiency of the algorithm is dominated by the application of an ILP solver (Line 4). If the entailment graph decomposes into small components, one could obtain an exact solution with an ILP solver, applied on each component separately, without resorting to any approximation. To further extend scalability in this setting we use a cutting-plane method (Kelley 1960). Cutting-plane methods have been used in the past, for example, in dependency parsing (Riedel and Clarke 2006). The idea is that even if we omit all transitivity constraints, we still expect most transitivity constraints to be satisfied, given a good weighting function w. Thus, it makes sense to avoid specifying the constraints ahead of time, but rather add them when they are violated. This is formalized in Algorithm 2. Line 1 initializes an active set of constraints (ACT). Line 3 applies the ILP solver with the active constraints. Lines 4 and 5 find the violated constraints and add them to the active constraints. The algorithm halts when no constraints are violated. The solution is clearly optimal be"
J15-2003,P10-1044,0,0.0724553,"Missing"
J15-2003,D12-1016,0,0.0266362,"ded by Schoenmackers et al. We perform the following steps: 1. Training set generation Positive examples are generated using WordNet synonyms and hypernyms. Negative pairs are generated using WordNet direct co-hyponyms (sister terms), but we also utilize Word hyponyms at distance 2. In addition, we generate negative examples by randomly sampling pairs of typed predicates that share the same types. Table 1 provides an example for each type of automatically generated training example. It has been noted in the past that the WordNet verb hierarchy contains a certain amount of noise (Richens 2008; Roth and Frank 2012). However, we use WordNet only to generate examples for training a statistical classifier, and thus we can tolerate some noise in the generated examples. In fact, we have noticed that simple variants in training set generation do not result in substantial differences in classifier performance. 2. Feature representation Each example pair of typed predicates (p1 , p2 ) is represented by a feature vector, where each feature is a distributional similarity score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of typed predicates, based on their argume"
J15-2003,D12-1131,0,0.0566681,"Missing"
J15-2003,D10-1106,0,0.115177,"s QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules th"
J15-2003,I05-5011,0,0.0343162,"Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes ar"
J15-2003,N06-1039,0,0.0160885,"ember 2014. doi:10.1162/COLI a 00220 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 paradigm for textual inference is Textual Entailment (Dagan et al. 2013). In textual entailment, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text Cyprus was invaded by the Ottoman Empire in 1571 implies the hypothesis The Ottomans attacked Cyprus. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relat"
J15-2003,P06-1101,0,0.206547,"Missing"
J15-2003,P12-3013,1,0.838495,"emphasizes the importance of learning graphs where predicates are marked by their various senses, which will result in a model that can directly benefit from the methods suggested in this article. 6. Conclusions The problem of language variability is at the heart of many semantic applications such as Information Extraction, Question Answering, Semantic Parsing, and more. Consequently, learning broad coverage knowledge bases of entailment rules and paraphrases (Ganitkevitch, Van Durme, and Callison-Burch 2013) has proved crucial for systems that perform inference over textual representations (Stern and Dagan 2012; Angeli and Manning 2014). 257 Computational Linguistics Volume 41, Number 2 In this article, we have presented algorithms for learning entailment rules between predicates that can scale to large sets of predicates. Our work builds on prior work by Berant, Dagan, and Goldberger (2012), who defined the concept of entailment graphs, and formulated entailment rule learning as a graph optimization problem, where the graph has to obey the structural constraint of transitivity. Our main contribution is a heuristic polynomial approximation algorithm that can learn entailment graphs containing tens o"
J15-2003,C08-1107,1,0.933058,"rence applications such as QA and IE crucially rely on entailment rules (Ravichandran and Hovy 2002; Shinyama and Sekine 2006; Berant and Liang 2014) or equivalently, inference rules—that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, for example, the entailment rule X invade Y⇒X attack Y can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Schoenmackers et al. 2010; Melamud et al. 2013). Textual entailment is inherently a transitive relation, that is, the rules x ⇒ y and y ⇒ z imply the rule x ⇒ z. For example, from the rules X reduce nausea ⇒ X help with nausea and X help with nausea ⇒ X associated with nausea we can infer the rule X reduce nausea ⇒ X associated with nausea (Figure 1). Accordingly, Berant, Dagan, and Goldberger (2012) proposed taking advantage of transitivity to improve learning of entailment rules. They modeled learning entailment rules as a graph optimization problem, where nodes are predicates and edges re"
J15-2003,W09-2504,1,0.838143,"fly describe the first two and then expand more on distributional similarity, which is the most commonly used source of information. Lexicographic resources are manually built knowledge bases from which semantic information may be extracted. For example, the hyponymy, toponymy, and synonymy relations in WordNet (Fellbaum 1998) can be used to detect entailment between nouns and verbs. Although WordNet is the most popular lexicographic resource, other resources such as CatVar, Nomlex, and FrameNet have also been utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004)"
J15-2003,W04-3206,1,0.844059,"d Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel, and Hovy 2007; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Next, we elaborate on the methods that we use in this article. Lin and Pantel (2001) proposed an algorithm for learning paraphrase relations between binary predicates, that is, predicates with two variables such as X treat Y. For each binary predicate, Lin and Pantel compute two sets of features Fx and Fy ,"
J15-2003,W03-1011,0,0.909309,"s high, this means that the templates u and v share many “informative” arguments and so it is possible that u ⇒ v. Szpektor and Dagan (2008) suggested two modifications to DIRT. First, they looked at unary predicates, that is, predicates with a single variable such as X treat. Secondly, they computed a directional score that is more suited for capturing entailment relations compared to the symmetric Lin score. They proposed that if for two unary predicates u ⇒ v, then relatively many of the features of u should be covered by the features of v. This is captured by the asymmetric Cover measure (Weeds and Weir 2003): P u f ∈ Fu ∩ Fv w ( f ) Cover(u, v) = P u f ∈ Fu w ( f ) (2) The final directional score, termed BInc (Balanced Inclusion), is the geometric average of the Lin measure and the Cover measure. Both Lin and Pantel as well as Szpektor and Dagan compute a similarity score using a single argument. However, it is clear that although this alleviates sparsity problems, considering pairs of arguments jointly provides more information. Yates and Etzioni (2009), Schoenmackers et al. (2010), and even earlier, Szpektor et al. (2004), presented methods that compute semantic similarity based on pairs of arg"
J15-2003,D12-1018,1,0.894504,"utilized to extract inference rules (Meyers et al. 2004; Budanitsky and Hirst 2006; Szpektor and Dagan 2009; Coyne and Rambow 2009; Ben Aharon, Szpektor, and Dagan 2010). Pattern-based methods attempt to identify the semantic relation between a pair of predicates by examining their co-occurrence in a large corpus. For example, the sentence people snore while they sleep provides evidence that snore ⇒ sleep. While most patternbased methods focused on identifying semantic relations between nouns (for example, Hearst patterns [Hearst 1992]), several works (Pekar 2008; Chambers and Jurafsky 2011; Weisman et al. 2012) attempted to extract relations between predicates as well. Chklovsky and Pantel (2004) used pattern-based methods to generate the commonly used VerbOcean resource. Both lexicographic as well as pattern-based methods suffer from limited coverage. Distributional similarity is therefore used to automatically construct broad-scale resources. Distributional similarity methods are based on the “distributional hypothesis” (Harris 1954) that semantically similar predicates occur with similar arguments. Quite a few methods have been suggested (Lin and Pantel 2001; Szpektor et al. 2004; Bhagat, Pantel,"
J15-2003,P12-2031,1,0.930862,"Missing"
J15-2003,2003.mtsummit-systems.9,0,\N,Missing
J15-2003,C98-2122,0,\N,Missing
J94-4003,P91-1034,0,0.844012,"describes the experimental Setting. Section 5 presents and analyzes the results of the experiment and cites additional results (Dagan, Marcus, and Markovitch 1993). In Section 6 we analyze the limitations of the algorithm in different cases and suggest enhancements to improve it. We also discuss the possibility of adopting the algorithm for monolingual applications. Finally, in Section 7 we present a comparative analysis of statistical sense disambiguation methods and then conclude in Section 8. 2 A similar observation underlies the use of parallel bilingual corpora for sense disambiguation (Brown et al. 1991; Gale, Church, and Yarowsky 1992). As we explain in Section 7, these corpora are a form of a manually tagged corpus and are more difficult to obtain than monolingual corpora. Erroneously, the preliminary publication of our method (Dagan, Itai, and Schwall 1991) was cited several times as requiring a parallel bilingual corpus, 565 Computational Linguistics Volume 20, Number 4 2. The Linguistic Model Our approach is first to use a bilingual lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from target"
J94-4003,H93-1039,0,0.00526801,"is implies that sometimes we use inaccurate data, which introduce noise into the statistical model (see Section 6.3 for a discussion of an alternative, but expensive, solution, using a bilingual corpus). As we shall see, even though the assumption does not always hold, in most cases this noise does not interfere with the decision algorithm. 5 The problem of constructing a bilingual lexicon that is as complete as possible is beyond the scope of this paper. A promising approach may be to use aligned bilingual corpora, especially for augmenting existing lexicons with domain-specific terminology (Brown et al. 1993; Dagan, Church, and Gale 1993). In any case, it seems that any translation system is limited by the completeness of its bilingual lexicon, which makes our assumption a reasonable one. 6 As explained in the introduction, this is a very important advantage of our method over other methods that use bilingual corpora. 572 Ido Dagan and Alon Itai Word Sense Disambiguation 3.2 Statistical Significance of the Decision Another problem we should address is the statistical significance of the d a t a - - w h a t confidence do we have that the data indeed reflect the phenomenon. If the decision is based"
J94-4003,P85-1037,0,0.0143174,"Missing"
J94-4003,J90-1003,0,0.0854127,"the Technion--Israel Institute of Technology. t Department of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel. E-maih itai@cs.technion.ac.il. (~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g., Grishman, Hirschman, and Nhan 1986). The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991). Clearly, statistics on lexical relations can also be useful for target word selection. Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana&quot; mi-shtei ha-mdinot mi-lahtom &quot;al hoze shalom. issue this p"
J94-4003,W93-0301,1,0.447702,"s are not widely available, and those that exist are not always accurate, it is possible to use various approximations to identify relevant syntactic relations between words. Hearst (1991) uses a stochastic part of speech tagger and a simple scheme for partial parsing of short phrases. The structures achieved by this analysis are used to identify approximated syntactic relations between words. Brown et al. (1991) make even weaker approximations, using only a stochastic part of speech tagger, and defining relations such as &quot;the first verb to the right&quot; or &quot;the first noun to the left.&quot; Finally, Dagan et al. (1993) (see Section 5.1) assume full parsing at the disambiguation phase, but no preprocessing at the training phase, in which a higher level of noise can be accommodated. A second type of information is provided by words that occur in the global context of the ambiguous word (Gale, Church, and Yarowsky 1992b, 1993; Yarowsky 1992; Sch6tze 1992). Gale et al. and Yarowsky use words that appear within 50 words in each 14 The reader is referred to some of these recent papers for thorough surveys of work on sense disambiguation (Hearst 1991; Gale, Church, and Yarowsky 1992a; Yarowsky 1992). 585 Computati"
J94-4003,P91-1017,1,0.837564,"Missing"
J94-4003,P93-1022,1,0.331699,"s are not widely available, and those that exist are not always accurate, it is possible to use various approximations to identify relevant syntactic relations between words. Hearst (1991) uses a stochastic part of speech tagger and a simple scheme for partial parsing of short phrases. The structures achieved by this analysis are used to identify approximated syntactic relations between words. Brown et al. (1991) make even weaker approximations, using only a stochastic part of speech tagger, and defining relations such as &quot;the first verb to the right&quot; or &quot;the first noun to the left.&quot; Finally, Dagan et al. (1993) (see Section 5.1) assume full parsing at the disambiguation phase, but no preprocessing at the training phase, in which a higher level of noise can be accommodated. A second type of information is provided by words that occur in the global context of the ambiguous word (Gale, Church, and Yarowsky 1992b, 1993; Yarowsky 1992; Sch6tze 1992). Gale et al. and Yarowsky use words that appear within 50 words in each 14 The reader is referred to some of these recent papers for thorough surveys of work on sense disambiguation (Hearst 1991; Gale, Church, and Yarowsky 1992a; Yarowsky 1992). 585 Computati"
J94-4003,1992.tmi-1.9,0,0.111246,"human sense of &apos;prospect&apos; that appeared in sports articles (a promising young person). This borrowed sense of &apos;prospect&apos; is necessarily inappropriate, since in Hebrew it is represented by the equivalent of &apos;hope&apos; (tiqwa) and not by sikkuy. The source of this problem is Assumption 3: a target tuple T might be a translation of several source tuples, and while gathering statistics for T, we cannot distinguish between the different sources, since we use only a target language corpus. A possible solution is to use an aligned bilingual corpus, as suggested by Sadler (1989), Brown et al. (1991), and Gale et al. (1992a). In such a corpus the occurrences of the relation &apos;young prospect&apos; will be aligned to the corresponding occurrences of the Hebrew word tiqwa and will not be used when the Hebrew word sikkuy is involved. Yet, it should be brought to mind that an aligned corpus is the result of manual translation, which can be viewed as including a manual tagging of the ambiguous words with their equivalent senses in the target language. This resource is much more expensive and less available than an untagged monolingual corpus, and it seems to be necessary only for relatively rare situations. Therefore, cons"
J94-4003,C88-1042,0,0.024836,"Missing"
J94-4003,J86-3002,0,0.0545116,"Missing"
J94-4003,P90-1034,0,0.0347869,"t Department of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel. E-maih itai@cs.technion.ac.il. (~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g., Grishman, Hirschman, and Nhan 1986). The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991). Clearly, statistics on lexical relations can also be useful for target word selection. Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana&quot; mi-shtei ha-mdinot mi-lahtom &quot;al hoze shalom. issue this prevented from-two the-countries from-"
J94-4003,P91-1030,0,0.0160837,"putational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g., Grishman, Hirschman, and Nhan 1986). The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991). Clearly, statistics on lexical relations can also be useful for target word selection. Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana&quot; mi-shtei ha-mdinot mi-lahtom &quot;al hoze shalom. issue this prevented from-two the-countries from-signing on treaty peace [ This sentence would translate into English as (2) This issue prevented the two countries from signing a peace treaty. The verb lahtom has four senses: &apos;sign,&apos; &apos;seal,&apos;"
J94-4003,J92-1001,0,0.0224617,"subcategorization frames, t h o u g h in practice each sense might have different probabilistic preferences for different syntactic constructs. It is clear that each of the different types of information provides some information that is not captured b y the others. However, as the acquisition and manipulation of each type of information requires different tools and resources, it is important to assess the relative contribution, and the &quot;cost effectiveness,&quot; of each of them. Such comparative evaluations are not available yet, not even for systems that incorporate several types of data (e.g., McRoy 1992). Further research is therefore n e e d e d to com15 The size of the context was determined experimentally,based on evaluations of different sizes of context. This optimization was performed for the Hansard corpus of the proceedings of the Canadian Parliament. In general, the size of the global context depends on the corpus and typically consists of a homogeneous unit of discourse. 16 See also Gale, Church, and Yarowsky 1992b (pp. 58-59), and Sch~itze, 1992, 1993, for methods of reducing the number of parameters when using global contexts and Dagan, Marcus, and Markovitch 1993, for increasing"
J94-4003,J93-1007,0,0.11132,"of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel. E-maih itai@cs.technion.ac.il. (~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g., Grishman, Hirschman, and Nhan 1986). The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991). Clearly, statistics on lexical relations can also be useful for target word selection. Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana&quot; mi-shtei ha-mdinot mi-lahtom &quot;al hoze shalom. issue this prevented from-two the-countries from-signing on tre"
J94-4003,C92-2070,0,0.369692,"Section 5 presents and analyzes the results of the experiment and cites additional results (Dagan, Marcus, and Markovitch 1993). In Section 6 we analyze the limitations of the algorithm in different cases and suggest enhancements to improve it. We also discuss the possibility of adopting the algorithm for monolingual applications. Finally, in Section 7 we present a comparative analysis of statistical sense disambiguation methods and then conclude in Section 8. 2 A similar observation underlies the use of parallel bilingual corpora for sense disambiguation (Brown et al. 1991; Gale, Church, and Yarowsky 1992). As we explain in Section 7, these corpora are a form of a manually tagged corpus and are more difficult to obtain than monolingual corpora. Erroneously, the preliminary publication of our method (Dagan, Itai, and Schwall 1991) was cited several times as requiring a parallel bilingual corpus, 565 Computational Linguistics Volume 20, Number 4 2. The Linguistic Model Our approach is first to use a bilingual lexicon to find all possible translations of each lexically ambiguous word in the source sentence and then use statistical information gathered from target language corpora to choose the mos"
J94-4003,C90-1005,0,0.0117017,"nstitute of Technology. t Department of Computer Science, Technion--Israel Institute of Technology, Haifa 32000, Israel. E-maih itai@cs.technion.ac.il. (~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 4 knowledge can be found in the use of statistical data on the occurrence of lexical relations in large corpora (e.g., Grishman, Hirschman, and Nhan 1986). The use of such relations (mainly relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun references (Dagan and Itai 1990, 1991). Clearly, statistics on lexical relations can also be useful for target word selection. Consider, for example, the Hebrew sentence extracted from the foreign news section of the daily Ha-Aretz, September 1990 (transcripted to Latin letters): (1) Nose ze mana&quot; mi-shtei ha-mdinot mi-lahtom &quot;al hoze shalom. issue this prevented from-two the-co"
J94-4003,J93-1005,0,\N,Missing
J94-4003,C88-1016,0,\N,Missing
J94-4003,C90-3063,1,\N,Missing
J94-4003,J90-2002,0,\N,Missing
K15-1018,P14-1113,0,0.203499,"Missing"
K15-1018,C92-2082,0,0.65747,"Missing"
K15-1018,H05-1087,0,0.0916619,"Missing"
K15-1018,W14-1610,1,0.879591,"Missing"
K15-1018,W11-2501,0,0.175332,"Missing"
K15-1018,N15-1098,1,0.918016,"baselines, tuning the regularization parameter on the validation set. 5.2 6.1 A New Proper-Name Dataset Performance on WordNet We examine whether our algorithm can replicate the common use of WordNet (§2.1), by manually constructing 4 whitelists based on the literature An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover terms such as celebrities (Lady Gaga) 7 Since our methods do not use lexical features, we did not use lexical splits as in (Levy et al., 2015). 179 Figure 4: Recall-precision curve for proper2015. Name basic +holo +mero +hypo Edge Types {synonym, hypernym, instance hypernym} basic ∪ {holonym} basic ∪ {meronym} basic ∪ {hyponym} Table 4: The manual whitelists commonly used in WordNet. Figure 3 compares our algorithm to WordNet’s baselines, showing that our binary model always replicates the best-performing manuallyconstructed whitelists, for certain values of β 2 . Synonyms and hypernyms are often selected, and additional edges are added to match the semantic flavor of each particular dataset. In turney2014, for example, where merony"
K15-1018,E12-1004,0,0.215574,"d Resources for Lexical Inference Vered Shwartz† Omer Levy† Ido Dagan† Jacob Goldberger§ † Computer Science Department, Bar-Ilan University § Faculty of Engineering, Bar-Ilan University vered1986@gmail.com {omerlevy,dagan}@cs.biu.ac.il jacob.goldberger@biu.ac.il Abstract Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 199"
K15-1018,D13-1160,0,0.0411595,"roaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet"
K15-1018,N04-3012,0,0.0762085,"and provides lexical inferences on entities that are absent from WordNet, particularly proper-names. Background Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog → mammal using a chain of hypernyms: dog → canine → carnivore → placental mammal → mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While there is a broad consensus that synonyms entail each other (elevator ↔ lif t) and hyponyms entail their hypernyms (cat → animal), other relations, such as meronymy, are not agreed 2 We also considered Freebase, but it required significantly larger computational resources to work in our framework, which, at the time of writing, exceeded our capacity. §4.1 discusses complexity. 176 “Beyonc´e” term to concept Beyonc´e Knowles occupation musician subclas"
K15-1018,C14-1212,0,0.213181,"Missing"
K15-1018,P11-1082,0,0.0144223,"rpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic know"
K15-1018,P10-1013,0,0.0123057,"stateof-the-art corpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 200"
K15-1018,C14-1097,0,0.336102,"Missing"
K15-1018,P09-1051,1,0.808142,",000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich sc"
K15-1018,J06-3003,0,0.349541,"Missing"
K15-1018,W03-1011,0,0.684426,"Missing"
K15-1018,J14-1003,0,\N,Missing
K15-1018,P06-1013,0,\N,Missing
K16-1006,W06-2911,0,0.0572826,"corpora and structured knowledge resources, such as WordNet. context2vec outperforms both of them. In the lexical substitution tasks, the best prior results are due to Melamud et al. (2015a).7 They employ an exemplar-based approach that requires keeping thousands of exemplar contexts for every target word type. The second-best is due to Melamud et al. (2015b). They propose a simple approach, but it requires dependency-parsed text as input. context2vec achieves comparable results with these works, using the same learning corpus. In the Senseval-3 supervised WSD task, the best result is due to Ando (2006) and the second-best to Test Sets Results The test set results are summarized in Table 7. First, we see that context2vec substantially outperforms AWE across all benchmarks. This suggests that our context representations are much better optimized for capturing sentential context information than AWE, at least for these tasks. Further, we see that with context2vec we either surpass or almost reach the state-of-the-art on all benchmarks. This is quite impressive, considering that all we did was measure cosine distances between context2vec’s representations to compete with more complex and task-o"
K16-1006,P14-2131,0,0.00966626,"ork Substitute vectors (Yuret, 2012) represent contexts as a probabilistic distribution over the potential gap-filler words for the target slot, pruned to its top-k most probable words. While using this representation showed interesting potential (Yatbaz et al., 2012; Melamud et al., 2015a), it can currently be generated efficiently only with n-gram language models and hence is limited to fixed-size context windows. It is also high dimensional and sparse, in contrast to our proposed representations. Syntactic dependency context embeddings have been proposed recently (Levy and Goldberg, 2014a; Bansal et al., 2014). They depend on the availability of a high-quality dependency parser, and can be viewed as a ‘bag-of-dependencies’ rather than a single representation for the entire sentential context. However, we believe that incorporating such dependency-based information in our model is an interesting future direction. A couple of recent works extended word2vec’s CBOW by replacing its internal context representation. Ling et al. (2015b) proposed a continuous window, which is a simple linear projection of the context window embeddings into a low dimensional vector. Ling et al. (2015a) proposed ‘CBOW with a"
K16-1006,D14-1110,0,0.0830208,"epresentations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 1 Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014), word sense induction (K˚ageb¨ack et al., 2015), lexical substitution (Melamud et al., 2015b), sentence completion (Liu et al., 2015) and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capture semantic and syntactic information about in"
K16-1006,P15-1145,0,0.115784,"se disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 1 Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014), word sense induction (K˚ageb¨ack et al., 2015), lexical substitution (Melamud et al., 2015b), sentence completion (Liu et al., 2015) and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation. While they are trained to optimize a generic taskindependent objective fun"
K16-1006,S07-1009,0,0.528212,"Missing"
K16-1006,N15-1050,1,0.729049,"completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 1 Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014), word sense induction (K˚ageb¨ack et al., 2015), lexical substitution (Melamud et al., 2015b), sentence completion (Liu et al., 2015) and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation. While they are trained to optimi"
K16-1006,P12-1092,0,0.0161005,"With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 1 Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014), word sense induction (K˚ageb¨ack et al., 2015), lexical substitution (Melamud et al., 2015b), sentence completion (Liu et al., 2015) and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capt"
K16-1006,W15-1501,1,0.895557,"completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks. 1 Like target words, contexts are commonly represented via word embeddings. In an unsupervised setting, such representations were found useful for measuring context-sensitive similarity (Huang et al., 2012), word sense disambiguation (Chen et al., 2014), word sense induction (K˚ageb¨ack et al., 2015), lexical substitution (Melamud et al., 2015b), sentence completion (Liu et al., 2015) and more. The context representations used in such tasks are commonly just a simple collection of the individual embeddings of the neighboring words in a window around the target word, or a (sometimes weighted) average of these embeddings. We note that such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation. While they are trained to optimi"
K16-1006,N16-1118,1,0.596631,"ficient models, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), for learning generic word embeddings from very large corpora. Capturing information from such corpora substantially increased the value of word embeddings to both unsupervised and semi-supervised NLP tasks. In supervised settings, various NLP systems use labeled data to learn how to consider context word representations in a more optimized task-specific way. This was done in tasks, such as chunking, NER, semantic role labeling, and co-reference resolution (Turian et al., 2010; Collobert et al., 2011; Melamud et al., 2016), mostly by considering the embeddings of words in a window around the target of interest. More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn 51 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 51–61, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics internal representations of wider sentential contexts (Zhou and Xu, 2015; Lample et al., 2016). Since supervised data is usually limited in size, it has been shown that training such syst"
K16-1006,W15-1504,0,0.0420596,"Missing"
K16-1006,W04-0807,0,0.227739,"t target words to given contexts, using different context2vec models, each learned with a different negative sampling smoothing parameter α. This illustrates the bias that high α values introduce towards rare words, as predicted with the analysis in section 2.2. Figure 2: A 2D illustration of context2vec’s embedded space and similarity metrics. Triangles and circles denote sentential context embeddings and target word embeddings, respectively. Next, to illustrate the context-to-context similarity metric, we took the set of contexts for the target lemma add from the training set of Senseval-3 (Mihalcea et al., 2004). In Table 3 we show an example for a ‘query’ context from that set and the other two most similar contexts to it, based on context2vec and AWE (average of Skip-gram word embeddings) context representations. Melamud et al. (2015a) argues that since contexts induce meanings (or senses) for target words, a good context similarity measure should assign high similarity values to contexts that induce similar senses for the same target word. As can be seen in this example, AWE’s similarity measure seems to be influenced by the presence of the location names in the contexts, even though they have lit"
K16-1006,P15-2084,0,0.0341918,"context provided for the target words is a full paragraph. Though it could be extended, context2vec is currently not designed to take advantage of such large context and therefore ignores all context out3.3 Sentence Completion Challenge The Microsoft Sentence Completion Challenge (MSCC) (Zweig and Burges, 2011) includes 1,040 items. Each item is a sentence with one word replaced by a gap, and the challenge is to identify the word, out of five choices, that is most meaningful and coherent as the gap-filler. While there is no official dev/test split for this dataset, we followed previous work (Mirowski and Vlachos, 2015) and used the first 520 sentences for parameter tuning and the rest as the test set.4 3 We made some preliminary experiments using word embeddings learned with word2vec’s CBOW model, instead of Skip-gram, but this yielded worse results. 4 Mikolov et al. (2013a) did not specify their dev/test split and all other works reported results only on the entire dataset. 56 context word units LSTM hidden/output units MLP input units MLP hidden units sentential context units target word units negative samples The MSCC includes a learning corpus of 50 million words. To use this corpus for training our mod"
K16-1006,E14-1057,0,0.0231195,"Missing"
K16-1006,D14-1162,0,0.121807,"such approaches do not include any mechanism for optimizing the representation of the entire sentential context as a whole. Introduction Generic word embeddings capture semantic and syntactic information about individual words in a compact low-dimensional representation. While they are trained to optimize a generic taskindependent objective function, word embeddings were found useful in a broad range of NLP tasks, making an overall huge impact in recent years. A major advancement in this field was the introduction of highly efficient models, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), for learning generic word embeddings from very large corpora. Capturing information from such corpora substantially increased the value of word embeddings to both unsupervised and semi-supervised NLP tasks. In supervised settings, various NLP systems use labeled data to learn how to consider context word representations in a more optimized task-specific way. This was done in tasks, such as chunking, NER, semantic role labeling, and co-reference resolution (Turian et al., 2010; Collobert et al., 2011; Melamud et al., 2016), mostly by considering the embeddings of words in a window around the"
K16-1006,N16-1030,0,0.0111061,"ic role labeling, and co-reference resolution (Turian et al., 2010; Collobert et al., 2011; Melamud et al., 2016), mostly by considering the embeddings of words in a window around the target of interest. More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn 51 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 51–61, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics internal representations of wider sentential contexts (Zhou and Xu, 2015; Lample et al., 2016). Since supervised data is usually limited in size, it has been shown that training such systems, using word embeddings that were pre-trained on large corpora, improves performance significantly. Yet, pre-trained word embeddings carry limited information regarding the inter-dependencies between target words and their sentential context as a whole. To model this (and more), the supervised systems still need to rely heavily on their albeit limited supervised data. In this work we present context2vec, an unsupervised model and toolkit1 for efficiently learning generic context embedding of wide se"
K16-1006,P15-1173,0,0.0283159,"Missing"
K16-1006,P14-2050,0,0.733851,"entential contexts that are associated with similar target words. We will show in the following sections how these properties make our model useful. 2.2 S= X t,c log σ(~t · ~c) + k X log σ(−~ti · ~c)  (1) i=1 where the summation goes over each word token t in the training corpus and its corresponding (single) sentential context c, and σ is the sigmoid function. t1 , ..., tk are the negative samples, independently sampled from a smoothed version of the target words unigram distribution: pα (t) ∝ (#t)α , such that 0 ≤ α &lt; 1 is a smoothing factor, which increases the probability of rare words. Levy and Goldberg (2014b) proved that when the objective function in Equation (1) is applied to single-word contexts, it is optimized when: Formal Specification and Analysis We use a bidirectional LSTM recurrent neural network to obtain a sentence-level context representation. Let lLS be an LSTM reading the words of a given sentence from left to right, and let rLS be a reverse one reading the words from right to left. Given a sentence w1:n , our ‘shallow’ bidirectional LSTM context representation for the target wi is ~t · ~c = PMIα (t, c) − log(k) (2) where PMI(t, c) = log pαp(t,c) (t)p(c) is the pointwise mutual in"
K16-1006,D11-1014,0,0.0328049,"to improve the learning of target words embeddings, rather than evaluate the benefit of using them directly in NLP tasks, as we do. Kawakami and Dyer (2016) represent words in context using bidirectional LSTMs and multilingual supervision. In contrast, our model is focused on representing the context alone. Yet, as shown in our lexical substitution and word sense disambiguation evaluations, it can easily be used for modeling the meaning of words in context as well. Finally, there is considerable work on using recurrent neural networks to represent word sequences, such as phrases or sentences (Socher et al., 2011; Kiros et al., 2015). We note that the 6 Conclusions and Future Potential We presented context2vec, a neural model that learns a generic embedding function for variablelength contexts of target words. We demonstrated that it can be trained in a reasonable time over billions of words and generate high quality context representations, which substantially outperform the traditional average-of-word-embeddings approach on three different tasks. As such, we hypothesize that it could contribute to various NLP systems that model context. Specifically, semisupervised systems may benefit from using our"
K16-1006,D13-1198,0,0.0522336,"Missing"
K16-1006,D15-1161,0,0.00883201,"o high dimensional and sparse, in contrast to our proposed representations. Syntactic dependency context embeddings have been proposed recently (Levy and Goldberg, 2014a; Bansal et al., 2014). They depend on the availability of a high-quality dependency parser, and can be viewed as a ‘bag-of-dependencies’ rather than a single representation for the entire sentential context. However, we believe that incorporating such dependency-based information in our model is an interesting future direction. A couple of recent works extended word2vec’s CBOW by replacing its internal context representation. Ling et al. (2015b) proposed a continuous window, which is a simple linear projection of the context window embeddings into a low dimensional vector. Ling et al. (2015a) proposed ‘CBOW with attention’, which is used for finding the relevant features in a context window. In contrast to our model, both approaches confine the context to a fixed-size window. Furthermore, they limit their scope to using these context representations only internally to improve the learning of target words embeddings, rather than evaluate the benefit of using them directly in NLP tasks, as we do. Kawakami and Dyer (2016) represent wo"
K16-1006,N15-1142,0,0.0200353,"o high dimensional and sparse, in contrast to our proposed representations. Syntactic dependency context embeddings have been proposed recently (Levy and Goldberg, 2014a; Bansal et al., 2014). They depend on the availability of a high-quality dependency parser, and can be viewed as a ‘bag-of-dependencies’ rather than a single representation for the entire sentential context. However, we believe that incorporating such dependency-based information in our model is an interesting future direction. A couple of recent works extended word2vec’s CBOW by replacing its internal context representation. Ling et al. (2015b) proposed a continuous window, which is a simple linear projection of the context window embeddings into a low dimensional vector. Ling et al. (2015a) proposed ‘CBOW with attention’, which is used for finding the relevant features in a context window. In contrast to our model, both approaches confine the context to a fixed-size window. Furthermore, they limit their scope to using these context representations only internally to improve the learning of target words embeddings, rather than evaluate the benefit of using them directly in NLP tasks, as we do. Kawakami and Dyer (2016) represent wo"
K16-1006,P10-1040,0,0.0167213,"this field was the introduction of highly efficient models, such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), for learning generic word embeddings from very large corpora. Capturing information from such corpora substantially increased the value of word embeddings to both unsupervised and semi-supervised NLP tasks. In supervised settings, various NLP systems use labeled data to learn how to consider context word representations in a more optimized task-specific way. This was done in tasks, such as chunking, NER, semantic role labeling, and co-reference resolution (Turian et al., 2010; Collobert et al., 2011; Melamud et al., 2016), mostly by considering the embeddings of words in a window around the target of interest. More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn 51 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 51–61, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics internal representations of wider sentential contexts (Zhou and Xu, 2015; Lample et al., 2016). Since supervised data is usually limited in"
K16-1006,D12-1086,0,0.0464407,"ns in the same sentence can yield completely different context representations. In contrast, sentence representations aim to reflect the entire contents of the sentence. Rothe and Sch¨utze (2015). context2vec is almost on par with these results, which were achieved with dedicated feature engineering and supervised machine learning models. 5 Related Work Substitute vectors (Yuret, 2012) represent contexts as a probabilistic distribution over the potential gap-filler words for the target slot, pruned to its top-k most probable words. While using this representation showed interesting potential (Yatbaz et al., 2012; Melamud et al., 2015a), it can currently be generated efficiently only with n-gram language models and hence is limited to fixed-size context windows. It is also high dimensional and sparse, in contrast to our proposed representations. Syntactic dependency context embeddings have been proposed recently (Levy and Goldberg, 2014a; Bansal et al., 2014). They depend on the availability of a high-quality dependency parser, and can be viewed as a ‘bag-of-dependencies’ rather than a single representation for the entire sentential context. However, we believe that incorporating such dependency-based"
K16-1006,P15-1109,0,0.0126978,"unking, NER, semantic role labeling, and co-reference resolution (Turian et al., 2010; Collobert et al., 2011; Melamud et al., 2016), mostly by considering the embeddings of words in a window around the target of interest. More recently, bidirectional recurrent neural networks, and specifically bidirectional LSTMs, were used in such tasks to learn 51 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 51–61, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics internal representations of wider sentential contexts (Zhou and Xu, 2015; Lample et al., 2016). Since supervised data is usually limited in size, it has been shown that training such systems, using word embeddings that were pre-trained on large corpora, improves performance significantly. Yet, pre-trained word embeddings carry limited information regarding the inter-dependencies between target words and their sentential context as a whole. To model this (and more), the supervised systems still need to rely heavily on their albeit limited supervised data. In this work we present context2vec, an unsupervised model and toolkit1 for efficiently learning generic contex"
K19-1019,D15-1075,0,0.336477,"n to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology,"
K19-1019,N19-1423,0,0.549817,"tly cover the targeted phenomenon, when possible. We demonstrate our methodology on two inference types, picked from the GLUE benchmark (Wang et al., 2019) diagnostic dataset: (1) dative alternation, a syntactic phenomenon, and (2) a specific type of numerical reasoning, pertaining to logical and arithmetic inference. To create our datasets, we introduce a templating method, by which we generated hundreds of synthetic examples from a single original sentence, while controlling the variance between the datasets.1 We employ a recent NLI model, based on the pretrained BERT masked language model (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018). For the dative alternation case, we find that the model struggles with generalizing over the syntactic dimension, requiring training over a relatively large variety of syntactically complex sentences. For the numerical reasoning case, we find that the model notably fails to generalize across diverse number ranges, a conclusion that might have been missed if we were to use only the original inoculation methodology. We hope our methodology will be adopted for additional NLP tasks, and specifically to a broader range of entailment infer"
K19-1019,P18-2103,1,0.93453,", Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the datas"
K19-1019,N18-2017,0,0.0282848,"ally exclusive term (for contradiction examples) challenges several pre-trained NLI models that performed well on the datasets on which they were trained. Naik et al. (2018) constructed a suite of “stress-tests”, each pertaining to some linguistic phenomenon, and showed that NLI models fail on many of them (e.g. numerical reasoning, logical negations, etc.). Another line of work showed that NLI models may reach a surprising performance level on the NLI test sets just by exploiting artifacts in the generation of the hypotheses, rather than learning to model the complex entailment relationship (Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018). 3.1 We focus on the following two inference types from the diagnostic set of the GLUE benchmark (Wang et al., 2019) as test cases for our methodology. Fine-tuning on Challenge Datasets. Recently, Liu et al. (2019a) suggested that a model’s failure to address a specific linguistic phenomenon may be attributed to one of the following cases: either the NLI training data does not sufficiently represent this phenomenon (“dataset blind spot”) or the model is inherently incapable of learning to address this phenomenon. They suggested to fine-tune the NLI model"
K19-1019,N10-1145,0,0.0318421,"fficiently represented in the training data. In these challenge datasets, a model is trained on the general NLI datasets, i.e. SNLI or the Multi-Genre Natural Language Inference datasets (MultiNLI; Williams et al., 2018). It is then used as a black box to evaluate on a given test set. Background Neural NLI Models. Natural language inference is the task of identifying, given two text fragments, whether the second (hypothesis) can be inferred from the first (premise). While earlier models for these tasks relied on domain knowledge and lexical resources like WordNet (e.g MacCartney et al., 2008; Heilman and Smith, 2010), the release of the large-scale Stanford natural language inference dataset (SNLI; Bowman et al., 2015) shifted the focus to neural models which thrive given such 1 All datasets and resources are available https://github.com/ohadrozen/generalization. at 197 ing method (Section 3.3). (3) After generating diverse premises, we generate multiple matching synthetic hypotheses using a templating method (Section 3.4). As we generate synthetic hypotheses, we can make sure the premise-hypothesis pairs differ along our proposed diversity dimensions. (4) Finally, we define the train and test sets so tha"
K19-1019,Q18-1017,0,0.0395917,"Missing"
K19-1019,N19-1225,0,0.5625,"Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the dataset). A failure, on the other"
K19-1019,P19-1441,0,0.152844,"Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“blind spot” of the dataset). A failure, on the other"
K19-1019,Q19-1027,1,0.83042,"atic token-based embeddings to dynamic contextsensitive ones. Notable contextual representations are ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019), which are pre-trained as language models on large corpora. Contextualized word embeddings have been used across a broad range of NLP tasks, outperforming the previous state-of-the-art models. Specifically, several works showed that they capture various types of linguistic knowledge, from syntactic to semantic and discourse relations (e.g. Peters et al., 2018a; Tenney et al., 2019; Shwartz and Dagan, 2019). Among many other tasks, NLI has also benefited from the use of contextualized word embeddings. The current state-of-the-art models use pre-trained contextualized word embeddings as their underlying representations, while fine-tuning on the NLI task (Liu et al., 2019b; Devlin et al., 2019). Despite their remarkable success on several datasets, it still remains unclear how these models represent the various linguistic phenomena required for solving the NLI tasks. Existing Drawbacks and Challenge Datasets. Training an NLI model in this end-to-end manner assumes that any inference type involved"
K19-1019,D08-1084,0,0.031929,"enon rather than to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computation"
K19-1019,P12-3013,1,0.81305,"in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology, analyzing the ability of models to generalize across different data distributions when addressing a specific inference phenomenon. In particular, we suggest varying both training and test distributions alon"
K19-1019,C08-1066,0,0.0415253,"a target phenomenon rather than to “learn” a dataset, by controlling additional aspects in the adversarial datasets. We demonstrate our approach on two inference phenomena – dative alternation and numerical reasoning, elaborating, and in some cases contradicting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computation"
K19-1019,P12-1030,1,0.827138,"ting, the results of Liu et al.. Our methodology enables building better challenge datasets for creating more robust models, and may yield better model understanding and subsequent overarching improvements. 1 Introduction To successfully recognize textual entailment (RTE; Dagan et al., 2013), also known as natural language inference (NLI) (MacCartney and Manning, 2008; Bowman et al., 2015), a system needs to model a broad range of inference phenomena. Pre-neural systems often included explicit components, such as engineered features or syntax-based transformations (e.g. Stern and Dagan, 2012; Stern et al., 2012; Bar-Haim et al., 2015), to address particular inference types such as syntactic, lexical, and logical inferences. Today’s neural models do not explicitly model such inferences, but 196 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 196–205 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics ology, analyzing the ability of models to generalize across different data distributions when addressing a specific inference phenomenon. In particular, we suggest varying both training and test distributions along several linguistic"
K19-1019,P14-5010,0,0.0025597,"lled Data Splits The main motivation for our data splits is to control the variance between the train and test sets with respect to certain data dimensions. Specifically, for both inference types, we create a variance along the syntactic complexity dimension. Based on the premise template, we divided each dataset into 3 subsets with different syntactic complexity levels: simple, medium and complex, denoted by S, M and C respectively. We do so according to two criteria: sentence length and the depth in the constituency parsing tree in which the inference type occurs, using the Stanford Parser (Manning et al., 2014) (see Table 2).2 We also create a variance along different lexical dimensions. From the simple subset S we generate two additional subsets SLex1 and SLex2 in the following way. For each simple template, we first split its original instantiations into two groups, and then instantiate each such group separately into the template, creating two sets of instantiations of the same template s1 ∈ SLex1 and s2 ∈ SLex2 , which are syntactically similar by construction, yet lexically different. We repeat this process for the complex subset as well to create CLex1 and CLex2 . We further split the dative a"
K19-1019,C18-1198,0,0.249983,"Science Department, Bar-Ilan University, Ramat-Gan, Israel 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science & Engineering, University of Washington {ohadrozen,roee.aharoni}@gmail.com, vereds@allenai.org, dagan@cs.biu.ac.il Abstract instead attempt to learn them implicitly from the training data. Despite their success on common NLI dataset, recent challenge datasets designed for probing different linguistic phenomena showed that neural models often fail on particular inference types, like recognizing semantic relations and negation (Poliak et al., 2018; Naik et al., 2018; Glockner et al., 2018). Recently, Liu et al. (2019a) showed that when probing reveals a model’s failure on a specific linguistic phenomenon, it is often possible to amend this failure. They suggested to fine-tune the model on (a training section of) the challenge dataset itself, in order to teach it to address the specific target phenomenon, or in other words, to “inoculate” it against the adversarial data. Inoculation has two possible outcomes. The first - a success to address the phenomenon after fine-tuning - suggests the original training set did not cover this phenomenon sufficiently (“"
K19-1019,L18-1239,0,0.110174,"Missing"
K19-1019,N18-1202,0,0.0387591,"2 large datasets. Typically, these models encode each of the premise and the hypothesis, combine them into a feature vector, and feed it into a classifier to make the entailment prediction. The encoding of the two sentences can be either independent of each other or dependent using an attention mechanism. These models typically do not rely on any external knowledge other than pre-trained word embeddings. Contextualized Word Embeddings. Recently, the word embedding paradigm shifted from static token-based embeddings to dynamic contextsensitive ones. Notable contextual representations are ELMo (Peters et al., 2018b), BERT (Devlin et al., 2019), GPT (Radford et al., 2018) and XLNet (Yang et al., 2019), which are pre-trained as language models on large corpora. Contextualized word embeddings have been used across a broad range of NLP tasks, outperforming the previous state-of-the-art models. Specifically, several works showed that they capture various types of linguistic knowledge, from syntactic to semantic and discourse relations (e.g. Peters et al., 2018a; Tenney et al., 2019; Shwartz and Dagan, 2019). Among many other tasks, NLI has also benefited from the use of contextualized word embeddings. The c"
K19-1019,N18-1101,0,0.605324,"nstrate our methodology on two inference types, picked from the GLUE benchmark (Wang et al., 2019) diagnostic dataset: (1) dative alternation, a syntactic phenomenon, and (2) a specific type of numerical reasoning, pertaining to logical and arithmetic inference. To create our datasets, we introduce a templating method, by which we generated hundreds of synthetic examples from a single original sentence, while controlling the variance between the datasets.1 We employ a recent NLI model, based on the pretrained BERT masked language model (Devlin et al., 2019) fine-tuned on the MultiNLI dataset (Williams et al., 2018). For the dative alternation case, we find that the model struggles with generalizing over the syntactic dimension, requiring training over a relatively large variety of syntactically complex sentences. For the numerical reasoning case, we find that the model notably fails to generalize across diverse number ranges, a conclusion that might have been missed if we were to use only the original inoculation methodology. We hope our methodology will be adopted for additional NLP tasks, and specifically to a broader range of entailment inference types, as an avenue for developing robust NLI systems"
K19-1019,W18-5441,0,0.0897395,"Missing"
K19-1019,D18-1007,0,\N,Missing
L16-1501,C14-2023,0,0.026127,"Missing"
L16-1501,passonneau-2006-measuring,0,0.0343287,"was done using Synchronous Context-Free Grammar (SCFG) (Aho and Ullman, 1972), where we defined the SCFG rules for both the natural language and the semantic language.6 Then, in real-time, the NLG used a quick look-up in this mapping to translate the agent’s outputs. 3.5. Annotation Once dialogue collection was completed, two independent annotators annotated the corpus following the annotation guidelines7 . Disagreements were resolved by the WOZ person as the arbitrator. As a metric of inter-annotator agreement we used Krippendorff’s α with MASI distance that supports multi-label annotation (Passonneau, 2006), as implemented in the DKPro Agreement package (Meyer et al., 2014).8 The inter-annotator agreement before reconciliation was 0.89 and after reconciliation became 0.95. The annotators were instructed to take the context of the dialogue into account while annotating each utterance. For example, the annotation of the following utterance could depend on the context of the dialogue: OK, let’s move to pension fund If this utterance is a reply to some Offer, then it would most likely be annotated as Accept, referring to the previous Offer, along with Query(Offer=Pension Fund). However, if 6 The scr"
L18-1229,P99-1016,0,0.314758,"order distributional similarity methods. In the second-order statistical approach, there is an additional dimension, feature representation. For each term in the corpus, a http://cl.haifa.ac.il/projects/mwn/index.shtml 1446 feature vector is constructed by collecting terms that appear in its context. Each feature term is assigned a weight indicating its association to the given term. Then, secondorder similarity is calculated between the target term and all the other terms in the corpus. (e.g., Jaccard&apos;s coefficient (Gasperin et al. 2001), Cosine-similarity (Salton and McGill 1983; Ruge 1992; Caraballo 1999; Gauch et al. 1999; Pantel and Ravichandran 2004) and Lin&apos;s mutual information metric (Lin 1998)). Therefore, each term in the corpus is a potential candidate term. Yet, the ranked list of terms is considered as the candidate terms list. The three choices for term representation are independent, resulting in 27 configurations which cover the range of possibilities for term representation in second-order thesaurus. Exploring all of them in a systematic manner should reveal the best configuration for a particular setting. In Section 2, we aim to describe our corpora, target term collection, and"
L18-1229,D14-1110,0,0.0304723,") and a second-order, distributional similarity approach (Hindle 1990; Lin 1998; Gasperin et al. 2001; Weeds and Weir 2003; Kotlerman et al. 2010) which suggests that words occurring within similar contexts are semantically similar (Harris 1968). The aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic resources for modern Hebrew have been constructed. Examples for such resources are Hebrew WordNet1 (Ordan et al. 2007), which groups words into sets of synonyms, and Hebrew FrameNet (Hayoun and Elhadad 2016), which defines formal structures for semantic frames, and various relationships between and within them. Furthermore, Hebrew is a Morphologically Rich Language (MRL), a language in which significant information about syntactic units and relations is expressed at word-level. Due to the rich and challenging morphology of MRLs, tools, such as pa"
L18-1229,J90-1003,0,0.592623,"esentation solves morphological disambiguation ""in retrospect"". The input is a thesaurus entry (target term) in one of the possible term representations (surface, best, or all). Their algorithm is as follow: for each target term, retrieve all the corpus documents where the target term appears. Then, Liebeskind et al. (2012) define a set of candidate terms that consists of all the terms that appear in all these documents (this again for each of the three possible term representations). Next, a co-occurrence score (e.g., Dice coefficient (Smadja et al. 1996), Pointwise Mutual Information (PMI) (Church and Hanks 1990) and log-likelihood test (Dunning 1993)) between the target term and each of the candidates is calculated. Then, candidates are sorted, and the highest rated candidate terms are clustered into lemma-oriented clusters. Finally, the clusters are ranked by their members&apos; co-occurrence scores and the highest rated clusters become related terms in the thesaurus. The two choices for term representation are independent, resulting in nine possible configurations of the algorithm for representing both the target term and the candidate terms. The methodology provides a generic scheme for exploring the a"
L18-1229,J93-1003,0,0.615748,"in retrospect"". The input is a thesaurus entry (target term) in one of the possible term representations (surface, best, or all). Their algorithm is as follow: for each target term, retrieve all the corpus documents where the target term appears. Then, Liebeskind et al. (2012) define a set of candidate terms that consists of all the terms that appear in all these documents (this again for each of the three possible term representations). Next, a co-occurrence score (e.g., Dice coefficient (Smadja et al. 1996), Pointwise Mutual Information (PMI) (Church and Hanks 1990) and log-likelihood test (Dunning 1993)) between the target term and each of the candidates is calculated. Then, candidates are sorted, and the highest rated candidate terms are clustered into lemma-oriented clusters. Finally, the clusters are ranked by their members&apos; co-occurrence scores and the highest rated clusters become related terms in the thesaurus. The two choices for term representation are independent, resulting in nine possible configurations of the algorithm for representing both the target term and the candidate terms. The methodology provides a generic scheme for exploring the alternative representation levels, each"
L18-1229,P90-1034,0,0.81608,"und A thesaurus is a lexical resource which groups words together by semantic similarity. The notion of semantic similarity includes semantic relations, such as synonyms (car-automobile), hyperonyms (vehicle-car), hyponyms (car-vehicle), meronyms (wheel-car), and antonyms (acceleration-deceleration). Generally, two statistical approaches for corpus-based thesaurus construction were explored: a first-order, cooccurrence-based approach which assumes that words that frequently occur together are topically related (Schütze and Pedersen 1997) and a second-order, distributional similarity approach (Hindle 1990; Lin 1998; Gasperin et al. 2001; Weeds and Weir 2003; Kotlerman et al. 2010) which suggests that words occurring within similar contexts are semantically similar (Harris 1968). The aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic reso"
L18-1229,W13-2233,0,0.0783991,"Missing"
L18-1229,W14-1503,0,0.0326844,"ting a first-order co-occurrence based thesaurus and extended the methodology for generating a second-order distributional similarity thesaurus as described in Section 1. The Pointwise mutual information (Church and Hanks 1990) was used as our co-occurrence measure for both firstorder similarity and feature weighting in second-order similarity. In this paper, we focused on the word window context representation, which is the most common one for lexical similarity extraction. This representation maintains comparable performance for alternative context representations (Bullinaria and Levy 2012; Kiela and Clark 2014; Lapesa and Evert 2014)). We used a sliding window of 3 words on each side of the represented word, not crossing sentence boundaries. At the end of the extraction process, we grouped the top 50 related term variants and ranked them based on the summation approach. The summation approach adds up the group members&apos; scores as the group score. The approach accounts for the cumulative impact of all group members, which corresponds to the morphological variants of each candidate term. 3. Results In general, due to the low coverage of Hebrew WordNet, we expected the measured precision of our methods"
L18-1229,Q14-1041,0,0.0206409,"occurrence based thesaurus and extended the methodology for generating a second-order distributional similarity thesaurus as described in Section 1. The Pointwise mutual information (Church and Hanks 1990) was used as our co-occurrence measure for both firstorder similarity and feature weighting in second-order similarity. In this paper, we focused on the word window context representation, which is the most common one for lexical similarity extraction. This representation maintains comparable performance for alternative context representations (Bullinaria and Levy 2012; Kiela and Clark 2014; Lapesa and Evert 2014)). We used a sliding window of 3 words on each side of the represented word, not crossing sentence boundaries. At the end of the extraction process, we grouped the top 50 related term variants and ranked them based on the summation approach. The summation approach adds up the group members&apos; scores as the group score. The approach accounts for the cumulative impact of all group members, which corresponds to the morphological variants of each candidate term. 3. Results In general, due to the low coverage of Hebrew WordNet, we expected the measured precision of our methods to be low. For example,"
L18-1229,S12-1009,1,0.865825,"al structures for semantic frames, and various relationships between and within them. Furthermore, Hebrew is a Morphologically Rich Language (MRL), a language in which significant information about syntactic units and relations is expressed at word-level. Due to the rich and challenging morphology of MRLs, tools, such as part-of-speech taggers, and parsers, often perform poorly. In MRLs, commonly used statistical extraction at the lemma level, using a morphological analyzer and tagger (Lindén and Piitulainen 2004; Peirsman et al. 2008; Rapp 2008), might not be the optimal choice. 1 Therefore, Liebeskind et al. (2012) suggested a methodology for generating a co-occurrence based thesaurus in MRL. They explored three options for term representation: surface form, lemma, and multiple lemmas, all supplemented by term variant clustering. While the default lemma representation is dependent on tagger performance, the two other representations avoid choosing the right lemma for each word occurrence. Instead, the multiple-lemma representation assumes that the correct analysis will accumulate enough statistical prominence throughout the corpus; while by clustering term variants at the end of the extraction process,"
L18-1229,P98-2127,0,0.438064,"us is a lexical resource which groups words together by semantic similarity. The notion of semantic similarity includes semantic relations, such as synonyms (car-automobile), hyperonyms (vehicle-car), hyponyms (car-vehicle), meronyms (wheel-car), and antonyms (acceleration-deceleration). Generally, two statistical approaches for corpus-based thesaurus construction were explored: a first-order, cooccurrence-based approach which assumes that words that frequently occur together are topically related (Schütze and Pedersen 1997) and a second-order, distributional similarity approach (Hindle 1990; Lin 1998; Gasperin et al. 2001; Weeds and Weir 2003; Kotlerman et al. 2010) which suggests that words occurring within similar contexts are semantically similar (Harris 1968). The aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic resources for"
L18-1229,W04-1808,0,0.0364863,", which groups words into sets of synonyms, and Hebrew FrameNet (Hayoun and Elhadad 2016), which defines formal structures for semantic frames, and various relationships between and within them. Furthermore, Hebrew is a Morphologically Rich Language (MRL), a language in which significant information about syntactic units and relations is expressed at word-level. Due to the rich and challenging morphology of MRLs, tools, such as part-of-speech taggers, and parsers, often perform poorly. In MRLs, commonly used statistical extraction at the lemma level, using a morphological analyzer and tagger (Lindén and Piitulainen 2004; Peirsman et al. 2008; Rapp 2008), might not be the optimal choice. 1 Therefore, Liebeskind et al. (2012) suggested a methodology for generating a co-occurrence based thesaurus in MRL. They explored three options for term representation: surface form, lemma, and multiple lemmas, all supplemented by term variant clustering. While the default lemma representation is dependent on tagger performance, the two other representations avoid choosing the right lemma for each word occurrence. Instead, the multiple-lemma representation assumes that the correct analysis will accumulate enough statistical"
L18-1229,N04-1041,0,0.0970055,"hods. In the second-order statistical approach, there is an additional dimension, feature representation. For each term in the corpus, a http://cl.haifa.ac.il/projects/mwn/index.shtml 1446 feature vector is constructed by collecting terms that appear in its context. Each feature term is assigned a weight indicating its association to the given term. Then, secondorder similarity is calculated between the target term and all the other terms in the corpus. (e.g., Jaccard&apos;s coefficient (Gasperin et al. 2001), Cosine-similarity (Salton and McGill 1983; Ruge 1992; Caraballo 1999; Gauch et al. 1999; Pantel and Ravichandran 2004) and Lin&apos;s mutual information metric (Lin 1998)). Therefore, each term in the corpus is a potential candidate term. Yet, the ranked list of terms is considered as the candidate terms list. The three choices for term representation are independent, resulting in 27 configurations which cover the range of possibilities for term representation in second-order thesaurus. Exploring all of them in a systematic manner should reveal the best configuration for a particular setting. In Section 2, we aim to describe our corpora, target term collection, and the experimental setting. Section 3 elaborates on"
L18-1229,C02-1007,0,0.0677298,"e level and candidate representation at the surface level. The improvement over the common default best lemma representation, for target, feature and candidate, is statistically significant at the 0.01 level for both MAP and RR. The common default best lemma representation for target, feature and candidate is italicized in Table 2. This default configuration has relatively low performance. Its recall is ranked 20 out of the 27 configurations and its MAP is the lowest, while, even the default surface representation for both target, feature and candidate has higher rank (9 and 13 respectively). Rapp (2002) observed that there is a relationship between the type of computation performed (first-order versus second-order) and the type of the extracted association (syntagmatic versus paradigmatic). Whereas the results of the second order computations are exclusively paradigmatic, the first order computations are a combination of both syntagmatic and paradigmatic associations. Since we limited preferences to lexical similarity, a thesaurus based on second order associations is better than one based on first-order associations. We analyzed the results of the best configuration (allsurface-surface) and"
L18-1229,D13-1089,0,0.0499129,"Missing"
L18-1229,J96-1001,0,0.590375,"ts at the end of the extraction process, the surface representation solves morphological disambiguation ""in retrospect"". The input is a thesaurus entry (target term) in one of the possible term representations (surface, best, or all). Their algorithm is as follow: for each target term, retrieve all the corpus documents where the target term appears. Then, Liebeskind et al. (2012) define a set of candidate terms that consists of all the terms that appear in all these documents (this again for each of the three possible term representations). Next, a co-occurrence score (e.g., Dice coefficient (Smadja et al. 1996), Pointwise Mutual Information (PMI) (Church and Hanks 1990) and log-likelihood test (Dunning 1993)) between the target term and each of the candidates is calculated. Then, candidates are sorted, and the highest rated candidate terms are clustered into lemma-oriented clusters. Finally, the clusters are ranked by their members&apos; co-occurrence scores and the highest rated clusters become related terms in the thesaurus. The two choices for term representation are independent, resulting in nine possible configurations of the algorithm for representing both the target term and the candidate terms. T"
L18-1229,W03-1011,0,0.103449,"roups words together by semantic similarity. The notion of semantic similarity includes semantic relations, such as synonyms (car-automobile), hyperonyms (vehicle-car), hyponyms (car-vehicle), meronyms (wheel-car), and antonyms (acceleration-deceleration). Generally, two statistical approaches for corpus-based thesaurus construction were explored: a first-order, cooccurrence-based approach which assumes that words that frequently occur together are topically related (Schütze and Pedersen 1997) and a second-order, distributional similarity approach (Hindle 1990; Lin 1998; Gasperin et al. 2001; Weeds and Weir 2003; Kotlerman et al. 2010) which suggests that words occurring within similar contexts are semantically similar (Harris 1968). The aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic resources for modern Hebrew have been constructed. Exampl"
L18-1229,D15-1237,0,0.0330492,"r are topically related (Schütze and Pedersen 1997) and a second-order, distributional similarity approach (Hindle 1990; Lin 1998; Gasperin et al. 2001; Weeds and Weir 2003; Kotlerman et al. 2010) which suggests that words occurring within similar contexts are semantically similar (Harris 1968). The aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic resources for modern Hebrew have been constructed. Examples for such resources are Hebrew WordNet1 (Ordan et al. 2007), which groups words into sets of synonyms, and Hebrew FrameNet (Hayoun and Elhadad 2016), which defines formal structures for semantic frames, and various relationships between and within them. Furthermore, Hebrew is a Morphologically Rich Language (MRL), a language in which significant information about syntactic units and relations is expressed at word-level. Due to the rich and"
L18-1229,L16-1688,0,0.0279268,"he aim of this research was to construct a thesaurus for Modern Hebrew. Modern Hebrew lacks repositories of machine-readable knowledge (i.e, lexical resources) fundamental to many Natural Language Processing (NLP) tasks, such as machine translation (Irvine and CallisonBurch 2013), question answering (Yang et al. 2015), and word sense disambiguation (Chen et al. 2014). Therefore, in the last decade, a few semantic resources for modern Hebrew have been constructed. Examples for such resources are Hebrew WordNet1 (Ordan et al. 2007), which groups words into sets of synonyms, and Hebrew FrameNet (Hayoun and Elhadad 2016), which defines formal structures for semantic frames, and various relationships between and within them. Furthermore, Hebrew is a Morphologically Rich Language (MRL), a language in which significant information about syntactic units and relations is expressed at word-level. Due to the rich and challenging morphology of MRLs, tools, such as part-of-speech taggers, and parsers, often perform poorly. In MRLs, commonly used statistical extraction at the lemma level, using a morphological analyzer and tagger (Lindén and Piitulainen 2004; Peirsman et al. 2008; Rapp 2008), might not be the optimal c"
N09-2009,W06-1621,1,0.8674,"specific meaning, like stadium, also come up as similar to baseball in LSA space. This limits the method’s precision, due to false-positive classifications of contextually-related documents that do not discuss the specific category topic (such as other sports documents wrongly classified to Baseball). This behavior is quite typical for query expansion methods, which expand a query with contextually correlated terms. We propose a novel scheme that models separately these two types of similarity. For one, it identifies words that are likely to refer specifically to the category name’s meaning (Glickman et al., 2006), based on certain relations in WordNet and 33 Proceedings of NAACL HLT 2009: Short Papers, pages 33–36, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Wikipedia. In tandem, we assess the general contextual fit of the category topic using an LSA model, to overcome lexical ambiguity and passing references. The evaluations show that tracing lexical references indeed increases classification precision, which in turn improves the eventual classifier obtained through bootstrapping. 2 Background: Keyword-based Text Categorization Figure 1: Keyword-based categorization"
N09-2009,H05-1017,1,0.760945,"word-based TC methods (see Section 2) aim at a more practical setting. Each category is represented by a list of characteristic keywords, which should capture the category meaning. Classification is then based on measuring similarity between the category keywords and the classified documents, typically followed by a bootstrapping step. The manual effort is thus reduced to providing a keyword list per category, which was partly automated in some works through clustering. The keyword-based approach still requires nonnegligible manual work in creating a representative keyword list per category. (Gliozzo et al., 2005) succeeded eliminating this requirement by using the category name alone as the initial keyword, yet obThe goal of our research is to further improve the scheme of text categorization from category name, which was hardly explored in prior work. When analyzing the behavior of the LSA representation of (Gliozzo et al., 2005) we noticed that it captures two types of similarities between the category name and document terms. One type regards words which refer specifically to the category name’s meaning, such as pitcher for the category Baseball. However, typical context words for the category whic"
N09-2009,P04-1033,0,0.0134186,"topic using an LSA model, to overcome lexical ambiguity and passing references. The evaluations show that tracing lexical references indeed increases classification precision, which in turn improves the eventual classifier obtained through bootstrapping. 2 Background: Keyword-based Text Categorization Figure 1: Keyword-based categorization scheme The majority of keyword-based TC methods fit the general bootstrapping scheme outlined in Figure 1, which is cast in terms of a vector-space model. The simplest version for step 1 is manual generation of the keyword lists (McCallum and Nigam, 1999). (Ko and Seo, 2004; Liu et al., 2004) partly automated this step, using clustering to generate candidate keywords. These methods employed a standard term-space representation in step 2. As described in Section 1, the keyword list in (Gliozzo et al., 2005) consisted of the category name alone. This was accompanied by representing the category names and documents (step 2) in LSA space, obtained through cooccurrence-based dimensionality reduction. In this space, words that tend to cooccur together, or occur in similar contexts, are represented by similar vectors. Thus, vector similarity in LSA space (in step 3) ca"
N09-2009,W99-0908,0,0.0207903,"textual fit of the category topic using an LSA model, to overcome lexical ambiguity and passing references. The evaluations show that tracing lexical references indeed increases classification precision, which in turn improves the eventual classifier obtained through bootstrapping. 2 Background: Keyword-based Text Categorization Figure 1: Keyword-based categorization scheme The majority of keyword-based TC methods fit the general bootstrapping scheme outlined in Figure 1, which is cast in terms of a vector-space model. The simplest version for step 1 is manual generation of the keyword lists (McCallum and Nigam, 1999). (Ko and Seo, 2004; Liu et al., 2004) partly automated this step, using clustering to generate candidate keywords. These methods employed a standard term-space representation in step 2. As described in Section 1, the keyword list in (Gliozzo et al., 2005) consisted of the category name alone. This was accompanied by representing the category names and documents (step 2) in LSA space, obtained through cooccurrence-based dimensionality reduction. In this space, words that tend to cooccur together, or occur in similar contexts, are represented by similar vectors. Thus, vector similarity in LSA s"
N13-1091,W08-1301,0,0.0213927,"TruthTeller is an open source and publicly available annotation tool, offers a relatively simple algebra for truth value computation, and is accompanied by a publicly available lexicon of over 1,700 implicative and factive predicates. Also, we provide an intuitive GUI for viewing and modifying the algorithm’s annotation rules. 2 Annotation Types and Algorithm This section summarizes the annotation algorithm (a detailed report is available with the system release). We perform the annotations over dependency parse trees, generated according to the Stanford Dependencies standard (de Marneffe and Manning, 2008). For all verbs, nouns and adjectives in a sentence’s parse tree, we produce the following 4 annotation types, given in the order they are calculated, as described in the following subsections: 1. Predicate Implication Signature (sig) - describes the pattern by which the predicate entails or presupposes its complements, e.g., the verb refuse entails the negative of its complements: Ed refused to pay entails that Ed didn’t pay. 2. Negation and Uncertainty (NU) - indicates whether the predicate is modified by an uncertainty modifier like might, probably, etc., or whether it’s negated by no, neve"
N13-1091,S12-1020,0,0.159721,"n’t fight Ed hesitated to ask ⇒ no entailments Ed was glad to come ⇒ Ed came Ed pretended to pay ⇒ Ed didn’t pay Ed wanted to fly ⇒ no entailments Negative context example Ed didn’t manage to escape ⇒ Ed didn’t escape Ed wasn’t forced to sell ⇒ no entailments Ed wasn’t allowed to go ⇒ Ed didn’t go Ed didn’t forget to pay ⇒ Ed paid Ed didn’t refuse to fight ⇒ no entailments Ed didn’t hesitate to ask ⇒ Ed asked Ed wasn’t glad to come ⇒ Ed came Ed didn’t pretend to pay ⇒ Ed didn’t pay Ed didn’t want to fly ⇒ no entailments Table 1: Implication signatures, based on MacCartney & Manning (2009) and Karttunen (2012). The first six signatures are named implicatives, and the last three factive, counter factive and regular, respectively. a) Annotate signatures via lexicons lookup Gal wasn’t allowed?/− to come?/? b) Annotate NU Gal wasn’t allowed?/−,nu− to come?/?,nu+ c) Annotate CT to presupposition constructions Gal wasn’t allowed?/−,nu−,ct+ to come?/?,nu+,ct+ d) Recursive CT and PT annotation which was then expanded with WordNet synonyms (Fellbaum, 1998). The second lexicon is the implicative phrasal verb lexicon of Karttunen (2012), adapted into our framework. The +/? implicative serves as the default si"
N13-1091,W07-1431,0,0.0380222,"ile according to (2) and (4) Gal did not sell it, hence the negative truth values, and in (1) we do not know if she sold it or not (the notations pt+, pt- and pt? denote truth states, defined in Subsection 2.3). Identifying these predicate truth values is an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization. The following examples illustrate the phenomenon: Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney & Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky & Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads a"
N13-1091,W09-3714,0,0.105904,"Missing"
N13-1091,D12-1048,0,0.0184379,"truth values is an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization. The following examples illustrate the phenomenon: Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney & Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky & Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in (2) is presupposed because it heads an adverbial clause and bought in (4) heads a finite relative clause), which, to our knowledge, have not yet been implemented computationally. Notice in the examples that presuppositions persist under negation, in questions and if-cla"
N13-1091,W06-3907,0,0.933317,"value of the predicate sell, while according to (2) and (4) Gal did not sell it, hence the negative truth values, and in (1) we do not know if she sold it or not (the notations pt+, pt- and pt? denote truth states, defined in Subsection 2.3). Identifying these predicate truth values is an important subtask within many semantic processing scenarios, including various applications such as Question Answering (QA), Information Extraction (IE), paraphrasing and summarization. The following examples illustrate the phenomenon: Previous works addressed specific aspects of the truth detection problem: Nairn et al. (2006), and later MacCartney & Manning (2007; 2009), were the first to build paraphrasing and inference systems that combine negation (see try in (2)), modality (smart in (3)) and “natural logic”, a recursive truth value calculus (sell in (1-3)); recently, Mausam et al. (2012) built an open IE system that identifies granulated variants of modality and conditions on predicates (smart in (3)); and Kiparsky & Kiparsky (1970) and Karttunen (1971; 2012) laid the ground work for factive and implicative entailment calculus (sell in (1-4)), as well as many generic constructions of presupposition (hearing in"
N13-1091,W07-1401,1,\N,Missing
N15-1050,S13-2050,0,0.0509004,"Work We proposed a model for word meaning in context whose main novelty is in representing contexts as substitute vectors. Our model outperformed stateof-the-art baselines in both predicting and ranking paraphrases of words in context in two different lexical substitution tasks. As another potential contribution, the context similarity measures used in our model performed well on a targeted evaluation, suggesting that they may be useful as a component in other applications as well. Substitute vectors were successfully used earlier for performing part-of-speech and word sense induction tasks (Baskaya et al., 2013; Yatbaz et al., 2014), not addressed in this work. These works took a different approach, embedding words in a low dimensional space, based on target-substitute pairs sampled from substitute vectors. It would be interesting to explore how our approach applies to these tasks. Finally, a preliminary qualitative analysis showed that low quality substitute vectors may be a factor limiting our model’s performance. This suggests that generating substitute vectors with better language models, such as neural language models, is a potential path to further improvements. Acknowledgments We thank our re"
N15-1050,J90-1003,0,0.486035,"s real and eternal. substitutes echoing, send, stick Table 3: Example for a context of the word fix, Q, and the two contexts of fix, Rsub and Rcbow , most similar to it, based on substitute vector and CBOW similarity, respectively. The substitute vectors are illustrated below each context (selected substitutes in the top-10 entries shown). similarity function (Bullinaria and Levy, 2007): ~sc [v] = P P M I(c, v) = max(0, P M I(c, v)) (1) sim(c, c0 ) = cos(~sc , ~sc0 ) (2) where ~sc [v] is the fitness weight for word v in the substitute vector of context c, PMI is point-wise mutual information (Church and Hanks, 1990), and sim(c, c0 ) is our context similarity measure. We note that a context c in our setting stands for an entire word window rather than a single context word. We therefore follow Yatbaz et al. (2012) using an n-gram language model to estimate P M I(c, v), as detailed in Section 5. Table 3 illustrates an example of a given context and the contexts most similar to it, as retrieved by our substitute vector and continuous bag-of-words context similarity measures. It is evident in this case that our measure correlates with the induced senses better than the bag-of-words measure. We suggest this c"
N15-1050,P10-2017,0,0.0158368,"Missing"
N15-1050,P13-2121,0,0.0215831,"esi et al., 2008), a two 3 Our pseudo-word dataset is available at: www.cs.biu. ac.il/nlp/resources/downloads/word2parvec/ Method SUB · CBOW SUBppmi,1000 SUBppmi,100 CBOW tf idf,8w SUBcond,1000 SUBcond,100 BOW tf idf,sent Random billion word web corpus, as their learning corpus. We converted every word that occurs less than 100 times in the corpus to a special rare-word token, and all numbers to a special number token, obtaining a vocabulary of a little under 200K word types. 5.2.1 Substitute vector similarity We learned a 5-gram Kneser-Ney language model from our learning corpus using KenLM (Heafield et al., 2013). Following Yatbaz et al. (2012), we used FASTSUBS (Yuret, 2012) with this language model to efficiently generate substitute vectors pruned to their top-n P substitutes, v1 ..vn , and normalized such that i=1..n p(vi |c) = 1. In order to make our substitute vectors compatible with the pseudo-word setting, for each substitute vector we replaced the entries of all of the pseudosense words with a single pseudo-word entry, and assigned it with the sum of the conditional probabilities of the pseudo-sense words. Next, we computed the Positive PMI weights for the substitutes, i |c) ~sc [vi ] = P P M"
N15-1050,P12-1092,0,0.0610417,"typically bias such a word type representation towards the context of each particular word instance using context similarity measures. The typical choice for context representation is a bag-of-words (BOW) context vector. In this vector each neighboring word type is assigned with a weight, such as its count (Erk and Pad´o, 2010) or tfidf (Reisinger and Mooney, 2010). A more recent variant of this approach is the continuous bag-ofwords (CBOW) vector, where context is represented as an average, or tf-idf weighted average, of dense low dimensional vector representations of the neighboring words (Huang et al., 2012). Context similarity is typically computed using vector Cosine. Several types of models of word meaning in context were recently proposed in the literature, mostly based on variants of BOW context representations. Thater et al. (2011) aggregate the contexts of a target word type into a sparse syntax-based context feature vector. Then, they generate a biased vector representation by reducing the weight of each context feature the less similar it is to the context of the given word instance. Reisinger and Mooney (2010) and Huang et al. (2012) use context clustering to induce multiple word senses"
N15-1050,E14-1057,0,0.220756,"Missing"
N15-1050,S07-1009,0,0.414301,"gative sampling 15, window size 8). 6 Evaluating Word Representations 6.2.2 Models of word meaning in context are commonly evaluated in lexical substitution tasks on predicting paraphrases of a target word that preserve its meaning in a given context. Conveniently, our paraphrase vector representations for words include exactly these predictions. We evaluated our model on two lexical substitution datasets under two types of tasks and compared it to the state-of-the-art as described next. 6.1 Lexical substitution datasets The dataset introduced in the lexical substitution task of SemEval 2007 (McCarthy and Navigli, 2007), denoted here LS07, is the most widely used for the evaluation of lexical substitution. It consists of 10 sentences extracted from a web corpus for each of 201 target words (nouns, verbs, adjectives and adverbs), or altogether 2,010 word instances in sentential context, split into 300 trial sentences and 1,710 test sentences. The gold standard provided with this dataset is a weighted lemmatized paraphrase list for each word instance, based on manual annotations. A more recent dataset (Kremer et al., 2014), denoted LS14, provides the same kind of data as LS07, but instead of target words that"
N15-1050,W14-1619,1,0.912414,"unordered collection of its first-order neighboring words, called bag-of-words (BOW). In contrast, Yatbaz et al. (2012) proposed to represent this context as a second-order substitute vector. Instead of the neighboring words themselves, a substitute vector includes the potential filler words for the target word slot, weighted according to how ‘fit’ they are to fill the target slot given the neighboring words. For example, the substitute vector representing the context “I my smartphone.” (target slot underlined), would typically include potential slot fillers such as love, lost, upgraded, etc. Melamud et al. (2014) argued that substitute vectors are potentially more informative than traditional context representations since the fitness of the fillers is estimated using an n-gram language model, thereby capturing information embedded in the neighboring word order. They showed promising results on measuring word similarity out-of-context with a distributional model based on this approach. In this paper we first propose a variant of substitute vectors as a context representation, which we find particularly suitable for measuring context similarity. Then, we extend the work in Melamud et al. (2014) by propo"
N15-1050,J14-3005,0,0.07232,"Missing"
N15-1050,otrusina-smrz-2010-new,0,0.045997,"Missing"
N15-1050,J14-4005,0,0.0578601,"Missing"
N15-1050,N10-1013,0,0.032655,"n distributional models of word meaning is the choice of context representation. Traditional out-of-context distributional models aggregate the observed contexts of a target word type to derive its representation. More recent models of word meaning in context typically bias such a word type representation towards the context of each particular word instance using context similarity measures. The typical choice for context representation is a bag-of-words (BOW) context vector. In this vector each neighboring word type is assigned with a weight, such as its count (Erk and Pad´o, 2010) or tfidf (Reisinger and Mooney, 2010). A more recent variant of this approach is the continuous bag-ofwords (CBOW) vector, where context is represented as an average, or tf-idf weighted average, of dense low dimensional vector representations of the neighboring words (Huang et al., 2012). Context similarity is typically computed using vector Cosine. Several types of models of word meaning in context were recently proposed in the literature, mostly based on variants of BOW context representations. Thater et al. (2011) aggregate the contexts of a target word type into a sparse syntax-based context feature vector. Then, they generat"
N15-1050,N13-1133,0,0.113404,"14 all 50.2 50.0 46.4 47.9 46.5 33.8 47.8 GW n/a 51.7 n/a WP,BN n/a 49.5 n/a UW,BN,WN GW,WN LLC,WN n/a n/a n/a 47.1 46.7 55.0* n/a n/a n/a LLC,WN n/a 52.4* n/a UW UW Table 6: GAP scores for compared methods. UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston and Burnard, 1998). † A re-implementation of the model in Thater, 2011. * Obtained by a supervised method. 6.2, the pruning factor of 100 performed slightly (up to half a point) worse than 1000. In comparison to previous results, our method achieves the best reported GAP score to date, on par with Szarvas et al. (2013b). However, we note that both Szarvas et al. (2013b) and Szarvas et al. (2013a) follow a supervised approach, training on the LS07 gold standard with 10-fold cross validation, as well as incorporate features from WordNet. Therefore, they cannot be directly compared with unsupervised models, such as our own. Our model and previous works used different learning corpora that are similar in size. Moon and Erk (2013) reported results for both Gigaword and ukWaC, showing minor differences in performance. The results on LS14 exhibit a similar behavior with our method outperforming the state-of-the-a"
N15-1050,D13-1198,0,0.103306,"Missing"
N15-1050,I11-1127,0,0.0575406,"Missing"
N15-1050,D12-1086,0,0.562617,"as an aggregation of its contexts. A recent line of work addresses polysemy of word types by representing the meaning (or sense) of each word instance individually as induced by its particular context. The context-sensitive meaning of a word instance is commonly called the word meaning in context, as opposed to the word meaning out-ofcontext of a word type. A key element of distributional models is the choice of context representation. A context of a word instance is typically represented by an unordered collection of its first-order neighboring words, called bag-of-words (BOW). In contrast, Yatbaz et al. (2012) proposed to represent this context as a second-order substitute vector. Instead of the neighboring words themselves, a substitute vector includes the potential filler words for the target word slot, weighted according to how ‘fit’ they are to fill the target slot given the neighboring words. For example, the substitute vector representing the context “I my smartphone.” (target slot underlined), would typically include potential slot fillers such as love, lost, upgraded, etc. Melamud et al. (2014) argued that substitute vectors are potentially more informative than traditional context represen"
N15-1050,C14-1217,0,0.0196389,"del for word meaning in context whose main novelty is in representing contexts as substitute vectors. Our model outperformed stateof-the-art baselines in both predicting and ranking paraphrases of words in context in two different lexical substitution tasks. As another potential contribution, the context similarity measures used in our model performed well on a targeted evaluation, suggesting that they may be useful as a component in other applications as well. Substitute vectors were successfully used earlier for performing part-of-speech and word sense induction tasks (Baskaya et al., 2013; Yatbaz et al., 2014), not addressed in this work. These works took a different approach, embedding words in a low dimensional space, based on target-substitute pairs sampled from substitute vectors. It would be interesting to explore how our approach applies to these tasks. Finally, a preliminary qualitative analysis showed that low quality substitute vectors may be a factor limiting our model’s performance. This suggests that generating substitute vectors with better language models, such as neural language models, is a potential path to further improvements. Acknowledgments We thank our reviewers for their help"
N15-1098,J10-4006,0,0.065573,"okens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two w"
N15-1098,W11-2501,0,0.453212,"different goal in mind, affecting word-pair generation and annotation. For example, 2 Following Caron (2001), we used the square root √ of the eigenvalue matrix Σk for representing words: Mk = Uk Σk . 3 http://bitbucket.org/yoavgo/word2vecf 971 both of Baroni’s datasets are designed to capture hypernyms, while other datasets try to capture broader notions of lexical inference (e.g. causality). Table 1 provides metadata on each dataset, and the description below explains how each one was created. (Kotlerman et al., 2010) Manually annotated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailin"
N15-1098,E12-1004,0,0.750923,"y. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexic"
N15-1098,W09-0215,0,0.133534,"f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on context"
N15-1098,P14-1113,0,0.552171,"inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical anal"
N15-1098,S13-1035,0,0.0223919,"Missing"
N15-1098,C92-2082,0,0.839194,")) = (~x~y · x~s y~s ) 2 (~xx~s · ~y y~s ) 1−α 2 (4) While these methods reduce match error – match error = 0.618 · recall versus the previous regression curve of match error = 0.935 · recall – their overall performance is only incrementally better than that of linear methods (Table 5). This improvement is also, partially, a result of the nonlinearity introduced in these kernels. 974 7 The Limitations of Contextual Features A (de)motivating example can be seen in §4.2. A typical y often has such+1 as a dominant feature, whereas x tends to appear with such−2 . These features are relics of the Hearst (1992) pattern “y such as x”. However, contextual features of single words cannot capture the joint occurrence of x and y in that pattern; instead, they record only this observation as two independent features of different words. In that sense, contextual features are inherently handicapped in capturing relational information, requiring supervised methods to harness complementary information from more sophisticated features, such as textual patterns that connect x with y (Snow et al., 2005; Turney, 2006). Acknowledgements This work was supported by the Adolf Messer Foundation, the Google Research Aw"
N15-1098,S12-1047,0,0.0690378,"notated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a f"
N15-1098,P14-2050,1,0.0441489,"ntation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two words (x, y) and a label whether x entails y. Note that each dataset"
N15-1098,W14-1610,1,0.664954,"SS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; We"
N15-1098,P98-2127,0,0.433791,"ions over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets f"
N15-1098,C94-1049,0,0.186491,"Missing"
N15-1098,J07-2002,0,0.0122586,"ikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dat"
N15-1098,E14-1054,0,0.287818,"ds, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical analysis reveals that said methods ignore the interaction between x and y, explaining our empirical findings. We modify them accordingly by incorporating the similarity between"
N15-1098,C14-1097,0,0.772314,"sks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these super"
N15-1098,E14-4008,0,0.286564,"other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney an"
N15-1098,P93-1034,0,0.626283,",657 Table 1: Datasets evaluated in this work. 2.1 Word Representations We built 9 word representations over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows differe"
N15-1098,J06-3003,0,0.298018,"Missing"
N15-1098,W03-1011,0,0.661401,"as causality (f lu → f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, ba"
N15-1098,C14-1212,0,0.797224,"4) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), only x (~x), and only y (~y ). For each composition, we trained two types of classifiers, tuning hyperparameters with a validation set: logistic regression with L1 or L2 regularization, and SVM with a linear kernel or quadratic kernel. 3 Negative Results Based on the above setup, we present three negative empirical results, which challenge the claim that the methods presented in §2.3 are learning a relation between x and y. In addition to our setup, these results were also reproduced in preliminary experDataset Kotlerman 2010 Bless 2011 Baroni 2012 Turney 2014 Levy 2014 Lex"
N15-1098,C98-2122,0,\N,Missing
N18-1081,D11-1142,0,0.960397,"corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4)."
N18-1081,N03-1013,0,0.11271,"Missing"
N18-1081,W12-3010,0,0.210272,"Missing"
N18-1081,P17-1044,1,0.941233,"ue to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independen"
N18-1081,P15-1034,0,0.374367,"o the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling"
N18-1081,D15-1076,1,0.951313,"partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader e"
N18-1081,J93-2004,0,0.0607468,"imilar to other cases in NLP, we would like to allow some variability in the predicted tuples. For example, for the sentence The sheriff standing against the wall spoke in a very soft voice we would want to treat both (The Sheriff; spoke; in a soft voice) and (The sheriff standing against the wall; spoke; in a very soft voice) as acceptable extractions. To that end, we follow He et al. (2015) which judge an argument as correct if and only if it includes the syntactic head of the gold argument (and similarly for predicates). For OIE2016, we use the available Penn Treebank gold syntactic trees (Marcus et al., 1993), while for the other test sets, we use predicted trees instead. While this metric may sometimes be too lenient, it does allow a more balanced and fair comparison between systems which can make different, but equally valid, span boundary decisions. 6.3 Performance Analysis In our analysis, we find that RnnOIE generalizes to unseen predicates, produces more and shorter arguments on average than are in the gold extractions, and, like all of the systems we tested, struggles with nominal predicates. Unseen predicates We split the propositions in the gold and predicted OIE2016 test set into two par"
N18-1081,P11-1062,1,0.79897,"e labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for"
N18-1081,D12-1048,0,0.899671,"and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed whil"
N18-1081,D16-1006,0,0.270204,"tions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et al., 2016) postprocess dependency trees, and Open IE42 extracts tuples from Semantic Role"
N18-1081,N18-2089,1,0.915945,"rom an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps d"
N18-1081,D13-1043,0,0.0752202,"extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in prev"
N18-1081,W16-1307,0,0.223307,") was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) and PropS (Stanovsky et a"
N18-1081,D14-1162,0,0.0820803,"Missing"
N18-1081,W95-0107,0,0.240007,"ed below the dashed lines, where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL gener"
N18-1081,E99-1023,0,0.376532,"where subscripts indicate the associated BIO label. Demonstrating: (a) the encoding of a multi-word predicate, (b) several arguments collapsed into the same A0 argument position, (c) argument position deviating from the sentence ordering. (w1 , . . . , wn ), a tuple consists of (x1 , . . . , xm ), where each xi is a contiguous subspan of S. One of the xi is distinguished as the predicate (marked in bold in Figure 1), while the other spans are considered its arguments. Following this definition, we reformulate Open IE as a sequence labeling task, using a custom BIO3 (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) scheme adapted from recent deep SRL models (He et al., 2017). In our formulation, the set of Open IE tuples for a sentence S are grouped by predicate head-word p, as shown in Table 2. For instance, example (b) lists two tuples for the predicate head “born”, which is underlined in the sentence. Grouping tuples this way allows us to run the model once for each predicate head, and accumulate the predictions across predicates to produce the final set of extractions. Open IE tuples deviate from SRL predicateargument structures in two major respects. First, while SRL generally deals with single-wor"
N18-1081,W17-5402,0,0.151396,"allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets annotated independently in previous Open IE efforts (WEB"
N18-1081,D16-1252,1,0.932243,"the-art in Open IE on benchmark datasets. 1 (mercury filling; particularly prevalent; in the USA) (mercury filling; causes; antibiotic resistance) (mercury filling; was banned; in the EU; partly because it causes antibiotic resistance) Figure 1: Open IE extractions from an example sentence. Each proposition is composed of a tuple with a single predicate position (in bold), and an ordered list of arguments, separated by semicolons. We build on recent work that studies other natural-language driven representations of predicate argument structure, which can be annotated by non-experts. Recently, Stanovsky and Dagan (2016) created the first labeled corpus for evaluation of Open IE by an automatic translation from question-answer driven semantic role labeling (QA-SRL) annotations (He et al., 2015). We extend these techniques and apply them to the QAMR corpus (Michael et al., 2018), an open variant of QA-SRL that covers a wider range of predicate-argument structures (Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic p"
N18-1081,P10-1013,0,0.735933,"Section 5). The combined dataset is the first corpus that is large and diverse enough to train an accurate extractor. Introduction Open Information Extraction (Open IE) systems extract tuples of natural language expressions that represent the basic propositions asserted by a sentence (see Figure 1). They have been used for a wide variety of tasks, such as textual entailment (Berant et al., 2011), question answering (Fader et al., 2014), and knowledge base population (Angeli et al., 2015). However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectivel"
N18-1081,N13-1107,0,0.0293875,"ce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were collected independently of our work (Xu et al., 2013; de S´a Mesquita et al., 2013; Schneider et al., 2017). This shows that for Open IE, careful data curation and model design can push the state of the art using supervised learning. 2 Recent work addressed the lack of labeled reference Open IE datasets for comparatively evaluating extractors. Stanovsky and Dagan (2016) created a large Open IE corpus (OIE2016) for verbal predicates by automatic conversion from QA-SRL (He et al., 2015), a variant of traditional SRL that labels arguments of verbs with simple, template-based natural language questions. Schneider et al. (2017) aggregated datasets a"
N18-1081,D14-1038,0,0.0579622,"(Banko et al., 2007) was to extend traditional (closed) information extraction, such that all of the propositions asserted by a given input sentence are extracted (see Figure 1 for examples). The broadness of this definition, along with the lack of a standard benchmark dataset for the task, prompted the development of various Open IE systems tackling different facets of the task. While most Open IE systems aim to extract the common case of verbal binary propositions (i.e, subject-verb-object tuples), some systems specialize in other syntactic constructions, including noun-mediated relations (Yahya et al., 2014; Pal and Mausam, 2016), n-ary relations (Akbik and L¨oser, 2012), or nested propositions (Bhutani et al., 2016). Many different modeling approaches have also been developed for Open IE. Some of the early systems made use of distant supervision (Banko et al., 2007; Wu and Weld, 2010), while the current best systems use rule-based techniques to extract predicate-argument structures as a post-processing step over an intermediate representation. ReVerb (Fader et al., 2011) extracts Open IE propositions from part of speech tags, OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) an"
N18-1081,P15-1109,0,0.0829896,"However, perhaps due to limited data, existing methods use semisupervised approaches (Banko et al., 2007; Wu and Weld, 2010), or rule-based algorithms (Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013). In this paper, we present new data and methods for Open IE, showing that supervised learning can greatly improve performance. To train on this data, we formulate Open IE as a sequence labeling problem. We introduce a novel approach that can extract multiple, overlapping tuples for each sentence (Section 3), extending recent deep BIO taggers used for semantic role labeling (Zhou and Xu, 2015; He et al., 2017). We also introduce a method to calculate extraction confidence, allowing us to effectively trade off precision and recall (Section 4). ∗ Work performed while at Bar-Ilan University. Our code and models are made publicly available at https://github.com/gabrielStanovsky/ supervised-oie 1 Experiments demonstrate that our approach out885 Proceedings of NAACL-HLT 2018, pages 885–895 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2.2 performs state-of-the-art Open IE systems on several benchmarks (Section 6), including three that were co"
N18-2089,W04-2705,0,0.491097,"here is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwn"
N18-2089,P13-1023,0,0.0755469,"Missing"
N18-2089,P98-1013,0,0.843785,"ated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-H"
N18-2089,J05-1004,0,0.608029,"(He et al., 2015), each question-answer pair corresponds to a predicateargument relationship. There is no need for a carefully curated ontology and the labels are highly interpretable. However, we differ from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations"
N18-2089,W13-2322,0,0.308993,"r from QA-SRL in focusing on all words in the sentence rather than just verbs, and allowing free form questions instead of using templates. Introduction Predicate-argument relationships form a key part of sentential meaning representations, and support answering basic questions such as who did what to whom. Resources for predicate-argument structure are well-developed for verbs (e.g. PropBank (Palmer et al., 2005)) and there have been efforts to study other parts of speech (e.g. NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)) and introduce whole-sentence structures (e.g. AMR (Banarescu et al., 2013)). However, highly skilled and trained annotators are re1 The QAMR formulation provides a new way of thinking about predicate-argument structure. Any form of sentence meaning—from a vector of real numbers to a logical form—should support the challenge of determining which questions are answerable by the sentence, and what the answers are. A QAMR sidesteps intermediate formal representations by surfacing those questions and an∗ Work performed while at Bar-Ilan University. github.com/uwnlp/qamr 560 Proceedings of NAACL-HLT 2018, pages 560–568 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Asso"
N18-2089,D16-1252,1,0.904315,"Missing"
N18-2089,J93-2004,0,0.0727984,"Missing"
N19-1072,N10-1000,0,0.248169,"Missing"
N19-1072,P18-1060,0,0.0492731,"all crowdsourcing scripts, for future evaluations. 1 Introduction Evaluating content quality of summaries is an integral part of summarization research. Measuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a s"
N19-1072,K16-1028,0,0.0422407,"systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary quality, such as precision, nonredundancy and grammaticality, may also be considered. In particular, it may be possible to extend our design of crowdsourcing tasks to supply indications for these complementary measurements as well. 0.6 0.5 ’05 ρp ’06 ρp 0.4 2 4 6 ’05 ρs ’06 ρs 8 10 12"
N19-1072,I17-1081,0,0.113854,"(NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyramid version we propose"
N19-1072,N04-1019,0,0.568019,"easuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranke"
N19-1072,W18-2706,0,0.0348052,"ot require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyramid version we propose aims to preserve t"
N19-1072,D18-1445,1,0.65027,"duced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summary. They then assign a single subjective score on a Likert scale for the summary quality, often with respect to a topic statement or guiding question. Finally, compared systems are ranked by the average score of their summaries. This method naturally developed into a crowdsourcing task, and is now used frequently in some variants (Grusky et al., 2018; Paulus et al., 2018). Another common crowdsourcable evaluation method is pairwise comparison (Gao et al., 2018; Falke et al., 2017; Fan et al., 2018): an evaluator is asked to judge which of two competing summaries of the same text is superior, usually while observing the source text. This protocol allows comparing only two systems at a time, where the superior is determined by the total votes over all input texts. The obvious disadvantage of the approach is the difficulty of comparing many systems, in the absence of absolute scores. Also, this method may tend to suffer from transitivity inconsistencies when comparing multiple system pairs (Gillick and Liu, 2010). The lightweight crowdsourcable Pyrami"
N19-1072,W12-2601,0,0.0352086,"ContentUnits, along with all crowdsourcing scripts, for future evaluations. 1 Introduction Evaluating content quality of summaries is an integral part of summarization research. Measuring the performance of a summarization system can be done through either automatic or manual evaluation. An automatic evaluation, in practice working at the lexical level, provides an inexpensive means of measuring the validity of a system, both for system comparisons and for quick development cycle testing. Due to the shallowness of the automatic approaches, their reliability is often perceived as insufficient (Owczarzak et al., 2012; Chaganty et al., 2018). This calls for the more expensive manual evaluation, which employs human-in-the-loop protocols for assessment. The Pyramid method (Nenkova and Passonneau, 2004) is a prominent manual evaluation methodology that is considered highly reliable for 682 Proceedings of NAACL-HLT 2019, pages 682–687 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics The Responsiveness method, introduced in DUC 2003 (NIST, 2003), does not require reference summaries. Instead, human evaluators typically read both the source text and the system summa"
N19-1072,W10-0722,0,0.657793,"ise comparison, which do not rely on reference summaries and can be attained via crowdsourcing. Yet, these methods are quite subjective, since evaluators need to provide only a single global judgment for the quality of a summary (or a pair of summaries). Such judgments are far more subjective than the Pyramid score, which is derived from many, more objective, local decisions, each judging independently the presence of an individual SCU. Indeed, it was shown that the above subjective crowdsourcing-based evaluation methods are not reliable enough to produce consistent scores across experiments (Gillick and Liu, 2010). We propose a simplified crowdsourcable and reproducible version of the Pyramid method, that suggests appealing advantages over prior crowdsourcable evaluation methods. Like the original Pyramid, our method leverages the strong signal of the reference summaries and similarly bases its score on less subjective SCU judgments. In contrast to the original Pyramid, we rely on statistical sampling rather than exhaustive SCU extraction and testing, lowering overall cost. Empirically, our method correlates with the original PyraConducting a manual evaluation is considered an essential part of summary"
N19-1072,N18-1065,0,0.0569204,"Missing"
N19-1072,D18-1450,0,0.10047,"score is defined as the average Pyramid score over all its evaluated summaries. Although certain normalization variants attempt to weigh in SCU precision, the score is essentially an absolute “recallstyle” interpretation reflecting the system’s ability to cover the content units found in the reference summaries. Such a fairly robust score allows, in principle, system comparison across experiments (Nenkova and Passonneau, 2004). We note that due to the Pyramid method’s reliability, some research has been carried out on simulating the Pyramid method as a fully automatic one (Yang et al., 2016; Hirao et al., 2018). The hope of such a line of work is to find an automatic evaluation method that is more reliable than the commonly used ones, by taking the reference summary semantic content into account. Despite these efforts, automated Pyramid evaluations did not make their way yet to mainstream summary evaluation practices, where variants of the ROUGE metric (Lin, 2004) still prevail. In any case, as this paper focuses on manual evaluation, we compare our results to those of the manual Pyramid. 3 Our Lightweight Pyramid Method Our Lightweight Pyramid method mimics the two phases of the original Pyramid pr"
N19-1072,P14-2083,0,0.0137284,"our knowledge, our method is the first to mimic the reliable Pyramid method as an affordable crowdsourced procedure. Our experiments suggest that this lightweight Pyramid is more reliable than the common Responsiveness method. It also allows comparing multiple systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary quality, such as pr"
N19-1072,N13-1132,0,0.0257061,"0.7 To the best of our knowledge, our method is the first to mimic the reliable Pyramid method as an affordable crowdsourced procedure. Our experiments suggest that this lightweight Pyramid is more reliable than the common Responsiveness method. It also allows comparing multiple systems with absolute scores, which pairwise comparison does not. Future work may improve correlation with the original Pyramid, or reduce annotation cost, by following our qualitative analysis and by reducing crowdsourcing noise (via qualification tests, enhanced guidelines, and post-processing result normalization (Hovy et al., 2013; Plank et al., 2014; Hosseini et al., 2012)). It would be appealing to investigate applying our methods to additional evaluation datasets, for which original Pyramid evaluations are not available for comparison. For example, addressing the CNN/DailyMail dataset (Nallapati et al., 2016) would involve testing single document summarization, utilizing a single reference summary per source text and addressing varying lengths of reference and system summaries. The Pyramid method is mainly a measurement of recall, which thus also applies to our lightweight Pyramid; but other measurements for summary"
N19-1236,W05-0909,0,0.0225446,"l systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differ14 ences either way per metric. 12 Note that this only affects the training stage. At test time, we do not re"
N19-1236,N06-1046,0,0.0515384,"nning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring covera"
N19-1236,P04-3031,0,0.268173,"Missing"
N19-1236,D16-1032,0,0.0750465,"ed Systems We compare to the best submissions in the WebNLG challenge (Gardent et al., 2017): Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe (Mille et al., 2017), a classic grammar-based NLG system that scored best in the human evaluation. Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge,"
N19-1236,P17-4012,0,0.0541432,"them. Figure 1d is an example of possible text resulting from such linearization. Training details We use a standard NMT setup with a copy-attention mechanism (Gulcehre et al., 10 2016) and the pre-trained GloVe.6B word em8 Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG (Ferreira et al., 2018) 9 We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase. 10 Concretely, we use the OpenNMT toolkit (Klein et al., 2017) with the copy attn flag. Exact parameter values are beddings (Pennington et al., 2014). The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts. Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., “5”(minutes)) we remove the parenthesis and quotation marks (5 minutes). 5 Experimental S"
N19-1236,N12-1093,0,0.0426172,"ation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring coverage and reducing the number"
N19-1236,W16-6626,0,0.310317,"ohn was born in London. Overall, the choice of fact ordering, entity ordering, and sentence splits for these facts give rise to 12 different structures, each of them putting the focus on somewhat different aspect of the information. Realistic inputs include more than two facts, greatly increasing the number of possibilities. Another axis of variation is in how to verbalize the information for a given structure. For example, (2) can also be verbalized as 2a. John works for IBM and was born in London. and (5) as: Consider the task of data-to-text generation, as exemplified in the WebNLG corpus (Colin et al., 2016). The system is given a set of RDF triplets describing facts (entities and relations between them) and has to produce a fluent text that is faithful to the facts. An example of such triplets is: John, birthPlace, London John, employer, IBM 1 With a possible output: ∗ We refer to the first set of choices (how to structure the information) as text planning and to the second 1 (how to verbalize a plan) as plan realization. The distinction between planning and realization is at the core of classic natural language generation (NLG) works (Reiter and Dale, 2000; Gatt and Krahmer, 2017). However, a r"
N19-1236,D13-1157,0,0.40401,"Missing"
N19-1236,W18-6539,0,0.0838331,"Missing"
N19-1236,P03-1069,0,0.126713,"e, Stent et al. (2004) shows a method of producing coherent sentence plans by exhaustively generating as many as 20 sentence plan trees for each document plan, manually tagging them, and learning to rank them using the RankBoost algorithm (Schapire, 1999). Our planning approach is similar, but we only have a set of “good” reference plans without internal ranks. While the sentence planning decides on the aggregation, one crucial decision left is sentence order. We currently determine order based on a splitting heuristic which relies on the number of facts in every sentence, not on the content. Lapata (2003) devised a probabilistic model for sentence ordering which correlated well with human ordering. Our 17 While the scores for the different sets are very similar, the plans are very different from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (s"
N19-1236,P18-1182,0,0.0989358,"Missing"
N19-1236,W17-4912,1,0.789975,"cts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic e"
N19-1236,W17-3518,0,0.489599,"from Theo Hoffenberg and Reverso. Note that the variation from 5 to 5a includes the introduction of a pronoun. This is traditionally referred to as referring expression generation (REG), and falls between the planning and realization stages. We do not treat REG in this work, but our approach allows natural integration REG systems’ outputs. 2267 Proceedings of NAACL-HLT 2019, pages 2267–2277 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics and treat the problem as a single end-to-end task of learning to map facts from the input to the output text (Gardent et al., 2017; Duˇsek et al., 2018). These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of Puduppully et al. (2018), who introduce a neural content-planning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017),"
N19-1236,P16-1014,0,0.231932,"ced corpus contains 13, 828 plan12 text pairs. Compared Systems We compare to the best submissions in the WebNLG challenge (Gardent et al., 2017): Melbourne, an end-to-end system that scored best on all categories in the automatic evaluation, and UPF-FORGe (Mille et al., 2017), a classic grammar-based NLG system that scored best in the human evaluation. Additionally, we developed an end-to-end neural baseline which outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separ"
N19-1236,N18-1169,0,0.0295492,"stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outper"
N19-1236,S17-2158,0,0.0531673,"Missing"
N19-1236,P02-1040,0,0.104794,"ich outperforms the WebNLG neural systems. It uses a set encoder, an LSTM (Hochreiter and Schmidhuber, 1997) decoder with attention (Bahdanau et al., 2014), a copy-attention mechanism (Gulcehre et al., 2016) and a neural checklist model (Kiddon et al., 2016), as well as applying entity dropout. The entity-dropout and checklist component are the key differentiators from previous systems. We refer to this system as StrongNeural. 6 Experiments and Results 6.1 Automatic Metrics We begin by comparing our plan-based system (BestPlan) to the state-of-the-art using the common automatic metrics: BLEU (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), ROUGEL (Lin, 2004) and CIDEr (Vedantam et al., 13 2015), using the nlg-eval tool (Sharma et al., 2017) on the entire test set and on each part separately (seen and unseen). In the original challenge, the best performing system in automatic metric was based on end-toend NMT (Melbourne). Both the StrongNeural and BestPlan systems outperform all the WebNLG participating systems on all automatic metrics (Table 1). BestPlan is competitive with StrongNeural in all metrics, with small differ14 ences either way per metric. 12 Note that this only affects the trainin"
N19-1236,D14-1162,0,0.083775,"Training details We use a standard NMT setup with a copy-attention mechanism (Gulcehre et al., 10 2016) and the pre-trained GloVe.6B word em8 Minimally, each entity occurrence can keep track of the number of times it was already mentioned in the plan. Other alternatives include using a full-fledged referring expression generation system such as NeuralREG (Ferreira et al., 2018) 9 We map DBPedia relations to sequences of tokens by splitting on underscores and CamelCase. 10 Concretely, we use the OpenNMT toolkit (Klein et al., 2017) with the copy attn flag. Exact parameter values are beddings (Pennington et al., 2014). The pretrained embeddings are used to initialize the relation tokens in the plans, as well as the tokens in the reference texts. Generation details We translate each sentence plan individually. Once the text is generated, we replace the entity tokens with the full entity string as it appears in the input graph, and lexicalize all dates as Month DAY+ordinal, YEAR (i.e., July 4th, 1776) and for numbers with units (i.e., “5”(minutes)) we remove the parenthesis and quotation marks (5 minutes). 5 Experimental Setup The WebNLG challenge (Colin et al., 2016) consists of mapping sets of RDF triplets"
N19-1236,W18-6557,0,0.0286439,"nning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017), struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input (Puzikov and Gurevych, 2018). Also, they do not allow control over the output’s structure. We speculate that this is due to demanding too much of the network: while the neural system excels at capturing the language details required for fluent realization, they are less well equipped to deal with the higher levels text structuring in a consistent and verifiable manner. Proposal we propose an explicit, symbolic, text planning stage, whose output is fed into a neural generation system. The text planner determines the information structure and expresses it unambiguously—in our case as a sequence of ordered trees. This stage"
N19-1236,P04-1011,0,0.573074,"Missing"
N19-1236,D15-1199,0,0.0611641,"rent from each other. See for examples the plans in Figure 3. plan selection procedure is admittedly simple, and can be improved by integrating insights from previous text planning works (Barzilay and Lapata, 2006; Konstas and Lapata, 2012, 2013). Many generation systems (Gardent et al., 2017; Duˇsek et al., 2018) are based on a black-box NMT component, with various pre-processing transformation of the inputs (such as delexicalization) and outputs to aid the generation process. Generation from structured data often requires referring to a knowledge base (Mei et al., 2015; Kiddon et al., 2016; Wen et al., 2015). This led to input-coverage tracking neural components such as the checklist model (Kiddon et al., 2016) and copy-mechanism (Gulcehre et al., 2016). Such methods are effective for ensuring coverage and reducing the number of over-generated facts and are in some ways orthogonal to our approach. While our explicit planning stage reduces the amount of over-generation, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model"
N19-1236,D17-1239,0,0.14799,"(Gardent et al., 2017; Duˇsek et al., 2018). These neural systems encode the input facts into an intermediary vector-based representation, which is then decoded into text. While not stated in these terms, the neural system designers hope for the network to take care of both the planning and realization aspect of text generation. A notable exception is the work of Puduppully et al. (2018), who introduce a neural content-planning module in the end-to-end architecture. While the neural methods achieve impressive levels of output fluency, they also struggle to maintain coherency on longer texts (Wiseman et al., 2017), struggle to produce a coherent order of facts, and are often not faithful to the input facts, either omitting, repeating, hallucinating or changing facts (the NLG community refers to such errors as errors in adequacy or correctness of the generated text). When compared to templatebased methods, the neural systems win in fluency but fall short regarding content selection and faithfulness to the input (Puzikov and Gurevych, 2018). Also, they do not allow control over the output’s structure. We speculate that this is due to demanding too much of the network: while the neural system excels at ca"
N19-1236,D18-1356,0,0.0302732,"ion, our realizer may be further improved by using a checklist model. More complex tasks, like RotoWire (Wiseman et al., 2017) require modeling also document-level planning. Puduppully et al. (2018) explored a method to explicitly model document planning using the attention mechanism. The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated (Ficler and Goldberg, 2017) or transferred (Shen et al., 2017; Zhao et al., 2017; Li et al., 2018). In contrast, like in (Wiseman et al., 2018), here we focused on controlling either the content of a generation or the way it is expressed by manipulating the sentence plan used in realizing the generation. 8 Conclusion We proposed adding an explicit symbolic planning component to a neural data-to-text NLG system, which eases the burden on the neural component concerning text structuring and fact tracking. Consequently, while the plan-based system performs on par with a strong end-to-end neural system regarding automatic evaluation metrics and human fluency evaluation, it substantially outperforms the end-to-end system regarding faithfu"
P00-1007,W99-0621,0,\N,Missing
P00-1007,W99-0707,0,\N,Missing
P00-1007,W99-0629,0,\N,Missing
P00-1007,J93-2004,0,\N,Missing
P00-1007,J93-1005,0,\N,Missing
P00-1007,P97-1003,0,\N,Missing
P00-1007,A88-1019,0,\N,Missing
P00-1007,P98-1034,0,\N,Missing
P00-1007,C98-1034,0,\N,Missing
P00-1007,P95-1037,0,\N,Missing
P00-1007,C92-3126,0,\N,Missing
P00-1007,P98-1010,1,\N,Missing
P00-1007,C98-1010,1,\N,Missing
P05-1014,W04-3205,0,0.0148282,"ntration of good features at the top of the vector. 4.2 Step 2: Corpus-based feature inclusion test We first check feature inclusion in the corpus that was used to generate the characteristic feature sets. For each word pair (w, v) we first determine which features of w do co-occur with v in the corpus. The same is done to identify features of v that co-occur with w in the corpus. 4.3 Step 3: Complementary Webbased Inclusion Test This step is most important to avoid inclusion misses due to the data sparseness of the corpus. A few recent works (Ravichandran and Hovy, 2002; Keller et al., 2002; Chklovski and Pantel, 2004) used the web to collect statistics on word cooccurrences. In a similar spirit, our inclusion test is completed by searching the web for the missing (non-included) features on both sides. We call this web-based technique mutual web-sampling. The web results are further parsed to verify matching of the feature&apos;s syntactic relationship. We denote the subset of w&apos;s features that are missing for v as M(w, v) (and equivalently M(v, w)). Since web sampling is time consuming we randomly sample a subset of k features (k=20 in our experiments), denoted as M(v,w,k). Mutual Web-sampling Procedure: For ea"
P05-1014,J90-1003,0,0.0743598,"Missing"
P05-1014,C04-1036,1,0.865138,"Missing"
P05-1014,C92-2082,0,0.627034,"Missing"
P05-1014,W02-1030,0,0.0489543,"RFF yields high concentration of good features at the top of the vector. 4.2 Step 2: Corpus-based feature inclusion test We first check feature inclusion in the corpus that was used to generate the characteristic feature sets. For each word pair (w, v) we first determine which features of w do co-occur with v in the corpus. The same is done to identify features of v that co-occur with w in the corpus. 4.3 Step 3: Complementary Webbased Inclusion Test This step is most important to avoid inclusion misses due to the data sparseness of the corpus. A few recent works (Ravichandran and Hovy, 2002; Keller et al., 2002; Chklovski and Pantel, 2004) used the web to collect statistics on word cooccurrences. In a similar spirit, our inclusion test is completed by searching the web for the missing (non-included) features on both sides. We call this web-based technique mutual web-sampling. The web results are further parsed to verify matching of the feature&apos;s syntactic relationship. We denote the subset of w&apos;s features that are missing for v as M(w, v) (and equivalently M(v, w)). Since web sampling is time consuming we randomly sample a subset of k features (k=20 in our experiments), denoted as M(v,w,k). Mutual W"
P05-1014,P93-1016,0,0.0143045,", which seems to be more difficult for traditional “bag of words” co-occurrence-based models. A syntactic feature is defined as a triple <term, syntactic_relation, relation_direction&gt; (the direction is set to 1, if the feature is the word’s modifier and to 0 otherwise). For example, given the word “company” the feature <earnings_report, gen, 0&gt; (genitive) corresponds to the phrase “company’s earnings report”, and <profit, pcomp, 0&gt; (prepositional complement) corresponds to “the profit of the company”. Throughout this paper we used syntactic features generated by the Minipar dependency parser (Lin, 1993). The value of each entry in the feature vector is determined by some weight function weight(w,f), which quantifies the degree of statistical association between the feature and the corresponding word. The most widely used association weight function is (point-wise) Mutual Information (MI) (Church and Hanks, 1990; Lin, 1998; Dagan, 2000; Weeds et al., 2004). Once feature vectors have been constructed, the similarity between two words is defined by some vector similarity metric. Different metrics have been used, such as weighted Jaccard (Grefenstette, 1994; Dagan, 2000), cosine (Ruge, 1992), va"
P05-1014,P98-2127,0,0.845173,"o the word sense level. In addition, the above testing algorithm was exploited to improve lexical entailment acquisition. 1 Introduction Distributional Similarity between words has been an active research area for more than a decade. It is based on the general idea of Harris&apos; Distributional Hypothesis, suggesting that words that occur within similar contexts are semantically similar (Harris, 1968). Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words (Church and Hanks, 1990; Ruge, 1992; Pereira et al., 1993; Grefenstette, 1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; Weeds and Weir, 2003). As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). It does not ensure that the meaning of one word is preserved when replacing it with the other one in some context. Ido Dagan Department of Computer Science Bar-Ilan University, Ramat-Gan, Israel, 52900 dagan@cs.biu.ac.il However, many semantic information-oriented applications like Question Answering, Information Extraction and Paraphrase Acquisition require a tighter similarity criterion, as was also demonstrated by papers at the rece"
P05-1014,W04-2609,0,0.0148979,"Missing"
P05-1014,P03-1017,0,0.0132966,"acquisition of the entailment relation. 108 2 Background 2.1 Implementations tional Similarity of DistribuThis subsection reviews the relevant details of earlier methods that were utilized within this paper. In the computational setting contexts of words are represented by feature vectors. Each word w is represented by a feature vector, where an entry in the vector corresponds to a feature f. Each feature represents another word (or term) with which w cooccurs, and possibly specifies also the syntactic relation between the two words as in (Grefenstette, 1994; Lin, 1998; Weeds and Weir, 2003). Pado and Lapata (2003) demonstrated that using syntactic dependency-based vector space models can help distinguish among classes of different lexical relations, which seems to be more difficult for traditional “bag of words” co-occurrence-based models. A syntactic feature is defined as a triple <term, syntactic_relation, relation_direction&gt; (the direction is set to 1, if the feature is the word’s modifier and to 0 otherwise). For example, given the word “company” the feature <earnings_report, gen, 0&gt; (genitive) corresponds to the phrase “company’s earnings report”, and <profit, pcomp, 0&gt; (prepositional complement)"
P05-1014,P93-1024,0,0.01512,"pirically tested and manually analyzed with respect to the word sense level. In addition, the above testing algorithm was exploited to improve lexical entailment acquisition. 1 Introduction Distributional Similarity between words has been an active research area for more than a decade. It is based on the general idea of Harris&apos; Distributional Hypothesis, suggesting that words that occur within similar contexts are semantically similar (Harris, 1968). Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words (Church and Hanks, 1990; Ruge, 1992; Pereira et al., 1993; Grefenstette, 1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; Weeds and Weir, 2003). As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). It does not ensure that the meaning of one word is preserved when replacing it with the other one in some context. Ido Dagan Department of Computer Science Bar-Ilan University, Ramat-Gan, Israel, 52900 dagan@cs.biu.ac.il However, many semantic information-oriented applications like Question Answering, Information Extraction and Paraphrase Acquisition require a tighter similarity criterion, as w"
P05-1014,P02-1006,0,0.0472049,"tions, such as synonymy, hyponymy and some cases of meronymy. For example, in Question Answering, the word company in a question can be substituted in the text by firm (synonym), automaker (hyponym) or division (meronym). Unfortunately, existing manually constructed resources of lexical semantic relations, such as WordNet, are not exhaustive and comprehensive enough for a variety of domains and thus are not sufficient as a sole resource for application needs1. Most works that attempt to learn such concrete lexical semantic relations employ a co-occurrence pattern-based approach (Hearst, 1992; Ravichandran and Hovy, 2002; Moldovan et al., 2004). Typically, they use a set of predefined lexicosyntactic patterns that characterize specific semantic relations. If a candidate word pair (like company-automaker) co-occurs within the same sentence satisfying a concrete pattern (like &quot; …companies, such as automakers&quot;), then it is expected that the corresponding semantic relation holds between these words (hypernym-hyponym in this example). In recent work (Geffet and Dagan, 2004) we explored the correspondence between the distributional characterization of two words (which may hardly co-occur, as is usually the case for"
P05-1014,W03-1011,0,0.803142,"on, the above testing algorithm was exploited to improve lexical entailment acquisition. 1 Introduction Distributional Similarity between words has been an active research area for more than a decade. It is based on the general idea of Harris&apos; Distributional Hypothesis, suggesting that words that occur within similar contexts are semantically similar (Harris, 1968). Concrete similarity measures compare a pair of weighted context feature vectors that characterize two words (Church and Hanks, 1990; Ruge, 1992; Pereira et al., 1993; Grefenstette, 1994; Lee, 1997; Lin, 1998; Pantel and Lin, 2002; Weeds and Weir, 2003). As it turns out, distributional similarity captures a somewhat loose notion of semantic similarity (see Table 1). It does not ensure that the meaning of one word is preserved when replacing it with the other one in some context. Ido Dagan Department of Computer Science Bar-Ilan University, Ramat-Gan, Israel, 52900 dagan@cs.biu.ac.il However, many semantic information-oriented applications like Question Answering, Information Extraction and Paraphrase Acquisition require a tighter similarity criterion, as was also demonstrated by papers at the recent PASCAL Challenge on Recognizing Textual En"
P05-1014,C04-1146,0,\N,Missing
P05-1014,W07-1401,1,\N,Missing
P05-1014,C98-2122,0,\N,Missing
P06-1057,C04-1177,0,0.0111673,"ubstitution lexicons in practical applications, since they would mostly introduce noise to the system. To avoid this problem the list of WordNet synonyms for each target word was filtered by a lexicographer, who excluded manually obscure synonyms that seemed worthless in practice. The source synonym for each target word was then picked randomly from the filtered list. Table 1 shows the 25 source-target pairs created for our experiments. In future work it may be possible to apply automatic methods for filtering infrequent sense correspondences in the dataset, by adopting algorithms such as in (McCarthy et al., 2004). Problem Setting and Dataset To investigate the direct sense matching problem it is necessary to obtain an appropriate dataset of examples for this binary classification task, along with gold standard annotation. While there is no such standard (application independent) dataset available it is possible to derive it automatically from existing WSD evaluation datasets, as described below. This methodology also allows comparing direct approaches for sense matching with classical indirect approaches, which apply an intermediate step of identifying the most likely WordNet sense. We derived our dat"
P06-1057,W02-0816,0,0.492519,"Missing"
P06-1057,J98-1004,0,0.0536338,"Missing"
P06-1057,W98-0705,0,0.0286631,"Missing"
P06-1057,P94-1013,0,0.0448618,"Missing"
P06-1057,P98-2127,0,0.0820922,"htein1 , Carlo Strapparava2 1 Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel 2 ITC-Irst, via Sommarive, I-38050, Trento, Italy Abstract ference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the senses of the two words do indeed match in a given context. For example, there are contexts in which the source word ‘weapon’ may be substituted by the target word ‘arm’; however one should recognize that ‘arm’ has a different sense than ‘weapon’ in sentences such as “repetitive movements could cause injuries to hands, wrists and arms.” A c"
P06-1057,W97-0322,0,\N,Missing
P06-1057,W07-1401,1,\N,Missing
P06-1057,C98-2122,0,\N,Missing
P06-2075,P99-1008,0,0.0159476,"ponym) or subsidiary (meronym), all of which entail company. Typically, hyponyms entail their hypernyms and Pattern-based Approaches Hearst (1992) pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships. She acquired hyponymy relations based on a small predefined set of highly indicative patterns, such as “X, . . . , Y and/or other Z”, and “Z such as X, . . . and/or Y”, where X and Y are extracted as hyponyms of Z. Similar techniques were further applied to predict hyponymy and meronymy relationships using lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004). Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. Synonyms, on the other hand, are rarely found in such patterns. In addition to their use for learning lexical semantic relations, patterns were commonly used to learn instances of concrete semantic relations for Information Extraction (IE) and QA, as in (Riloff and Shepherd, 1997; Ravichandran and Hovy, 2002; Yangarber et al., 2000). Patterns id"
P06-2075,W04-3205,0,0.382638,"oneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships. She acquired hyponymy relations based on a small predefined set of highly indicative patterns, such as “X, . . . , Y and/or other Z”, and “Z such as X, . . . and/or Y”, where X and Y are extracted as hyponyms of Z. Similar techniques were further applied to predict hyponymy and meronymy relationships using lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004). Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. Synonyms, on the other hand, are rarely found in such patterns. In addition to their use for learning lexical semantic relations, patterns were commonly used to learn instances of concrete semantic relations for Information Extraction (IE) and QA, as in (Riloff and Shepherd, 1997; Ravichandran and Hovy, 2002; Yangarber et al., 2000). Patterns identify rather specific and informative structures within particular co-occurrences of the related words. Consequently, they are relatively reliable an"
P06-2075,C92-2082,0,0.542297,"agan et al., 2005). Generally speaking, a word w lexically entails another word v if w can substitute v in some contexts while implying v&apos;s original meaning. It was suggested that lexical entailment captures major application needs in modeling lexical variability, generalized over several types of known ontological relationships. For example, in Question Answering (QA), the word company in a question can be substituted in the text by firm (synonym), automaker (hyponym) or subsidiary (meronym), all of which entail company. Typically, hyponyms entail their hypernyms and Pattern-based Approaches Hearst (1992) pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships. She acquired hyponymy relations based on a small predefined set of highly indicative patterns, such as “X, . . . , Y and/or other Z”, and “Z such as X, . . . and/or Y”, where X and Y are extracted as hyponyms of Z. Similar techniques were further applied to predict hyponymy and meronymy relationships using lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004"
P06-2075,P98-2127,0,0.140161,"ll, our method reveals complementary types of information that can be obtained from the two approaches. 2 2.1 Background Distributional Similarity and Lexical Entailment synonyms entail each other, while entailment holds for meronymy only in certain cases. In this paper we investigate automatic acquisition of the lexical entailment relation. For the distributional similarity component we employ the similarity scheme of (Geffet and Dagan, 2004), which was shown to yield improved predictions of (non-directional) lexical entailment pairs. This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. These weights identify the most characteristic features of each word, yielding cleaner feature vector representations and better similarity assessments. 2.2 The general idea behind distributional similarity is that words which occur within similar contexts are semantically similar (Harris, 1968). In a computational framework, words are represented by feature vectors, where features are context words weighted by a function of their statistical association with the target word. The degree of similarity between two target words is then determ"
P06-2075,C04-1111,0,0.0600642,"post-processing the retrieved texts. Another extension of the approach was automatic enrichment of the pattern set through bootstrapping. Initially, some instances of the sought 580 1 2 3 4 5 6 7 8 9 10 11 line architecture. We aim to achieve tighter integration of the two approaches, as described next. NP1 such as NP2 Such NP1 as NP2 NP1 or other NP2 NP1 and other NP2 NP1 ADV known as NP2 NP1 especially NP2 NP1 like NP2 NP1 including NP2 NP1-sg is (a OR an) NP2-sg NP1-sg (a OR an) NP2-sg NP1-pl are NP2-pl 3 Table 1: The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al., 2004). Capitalized terms indicate variables. pl and sg stand for plural and singular forms. relation are found based on a set of manually defined patterns. Then, additional cooccurrences of the related terms are retrieved, from which new patterns are extracted (Riloff and Jones, 1999; Pantel et al., 2004). Eventually, the list of effective patterns found for ontological relations has pretty much converged in the literature. Amongst these, Table 1 lists the patterns that were utilized in our work. Finally, the selection of candidate pairs for a target relation was usually based on some function over"
P06-2075,N04-1041,0,0.0223029,"hem, utilize a generic module for pattern-based extraction from the web, which is described first in Section 3.1. 3.1 Combined Approaches It can be noticed that the pattern-based and distributional approaches have certain complementary properties. The pattern-based method tends to be more precise, and also indicates the direction of the relationship between the candidate terms. The distributional similarity approach is more exhaustive and suitable to detect symmetric synonymy relations. Few recent attempts on related (though different) tasks were made to classify (Lin et al., 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeAn Integrated Approach for Lexical Entailment Acquisition Pattern-based Extraction Module The general pattern-based extraction module receives as input a set of lexical-syntactic patterns (as in Table 1) and either a target term or a candidate pair of terms. It then searches the web for occurrences of the patterns with the input term(s). A small set of effective queries is created for each pattern-terms combination, aiming to retrieve as much relevant data with as few queries as possible. Each pattern has two variable"
P06-2075,P02-1006,0,0.0323826,"lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004). Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. Synonyms, on the other hand, are rarely found in such patterns. In addition to their use for learning lexical semantic relations, patterns were commonly used to learn instances of concrete semantic relations for Information Extraction (IE) and QA, as in (Riloff and Shepherd, 1997; Ravichandran and Hovy, 2002; Yangarber et al., 2000). Patterns identify rather specific and informative structures within particular co-occurrences of the related words. Consequently, they are relatively reliable and tend to be more accurate than distributional evidence. On the other hand, they are susceptive to data sparseness in a limited size corpus. To obtain sufficient coverage, recent works such as (Chklovski and Pantel, 2004) applied pattern-based approaches to the web. These methods form search engine queries that match likely pattern instances, which may be verified by post-processing the retrieved texts. Anoth"
P06-2075,W97-0313,0,0.0410908,"eronymy relationships using lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004). Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. Synonyms, on the other hand, are rarely found in such patterns. In addition to their use for learning lexical semantic relations, patterns were commonly used to learn instances of concrete semantic relations for Information Extraction (IE) and QA, as in (Riloff and Shepherd, 1997; Ravichandran and Hovy, 2002; Yangarber et al., 2000). Patterns identify rather specific and informative structures within particular co-occurrences of the related words. Consequently, they are relatively reliable and tend to be more accurate than distributional evidence. On the other hand, they are susceptive to data sparseness in a limited size corpus. To obtain sufficient coverage, recent works such as (Chklovski and Pantel, 2004) applied pattern-based approaches to the web. These methods form search engine queries that match likely pattern instances, which may be verified by post-processi"
P06-2075,N04-1010,0,0.0187944,"ed Approaches Hearst (1992) pioneered the use of lexicalsyntactic patterns for automatic extraction of lexical semantic relationships. She acquired hyponymy relations based on a small predefined set of highly indicative patterns, such as “X, . . . , Y and/or other Z”, and “Z such as X, . . . and/or Y”, where X and Y are extracted as hyponyms of Z. Similar techniques were further applied to predict hyponymy and meronymy relationships using lexical or lexico-syntactic patterns (Berland and Charniak, 1999; Sundblad, 2002), and web page structure was exploited to extract hyponymy relationships by Shinzato and Torisawa (2004). Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy. Synonyms, on the other hand, are rarely found in such patterns. In addition to their use for learning lexical semantic relations, patterns were commonly used to learn instances of concrete semantic relations for Information Extraction (IE) and QA, as in (Riloff and Shepherd, 1997; Ravichandran and Hovy, 2002; Yangarber et al., 2000). Patterns identify rather specific and informative structures within particular co-occurrences of the related words. Consequently, th"
P06-2075,C04-1036,1,0.931312,"cquisition. Our empirical results show that the integrated method significantly outperforms each approach in isolation, as well as the naïve combination of their outputs. Overall, our method reveals complementary types of information that can be obtained from the two approaches. 2 2.1 Background Distributional Similarity and Lexical Entailment synonyms entail each other, while entailment holds for meronymy only in certain cases. In this paper we investigate automatic acquisition of the lexical entailment relation. For the distributional similarity component we employ the similarity scheme of (Geffet and Dagan, 2004), which was shown to yield improved predictions of (non-directional) lexical entailment pairs. This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. These weights identify the most characteristic features of each word, yielding cleaner feature vector representations and better similarity assessments. 2.2 The general idea behind distributional similarity is that words which occur within similar contexts are semantically similar (Harris, 1968). In a computational framework, words are represented by feature vectors, where featur"
P06-2075,E06-1052,1,\N,Missing
P06-2075,C04-1146,0,\N,Missing
P06-2075,C00-2136,0,\N,Missing
P06-2075,W06-1406,0,\N,Missing
P06-2075,W04-1216,0,\N,Missing
P06-2075,W04-2609,0,\N,Missing
P06-2075,P99-1016,0,\N,Missing
P06-2075,P06-1040,0,\N,Missing
P06-2075,P06-1114,0,\N,Missing
P06-2075,W07-1401,1,\N,Missing
P06-2075,P05-1004,0,\N,Missing
P06-2075,P05-1014,1,\N,Missing
P06-2075,C98-2122,0,\N,Missing
P06-2075,W04-1808,0,\N,Missing
P06-2075,P93-1016,0,\N,Missing
P07-1058,N03-1003,0,0.464964,"ettings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y → X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, s"
P07-1058,N03-1024,0,0.027934,"rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components. Thus, as in many other NLP learning settings, 458 a direct evaluation is needed. Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005). In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not. The criterion for correctness is not explicitly described in most previous works. By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds. We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges. This approa"
P07-1058,P02-1006,0,0.0204598,"ormulation, we observe that many applied inference settings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary. For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y → X own Y ’, which is clearly not a paraphrase. 2.2 Evaluation of Acquisition Algorithms Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005). However, there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contrib"
P07-1058,E06-1052,1,0.831473,"common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002) or IE (Sudo et al., 2003; Romano et al., 2006). While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons. First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effec"
P07-1058,I05-5011,0,0.456507,"Liverpool beat Chelsea” from “Chelsea lost to Liverpool in the semifinals”. Entailment rules should typically be applied only in specific contexts, which we term relevant contexts. For example, the rule ‘X acquire Y → X buy Y ’ can be used in the context of ‘buying’ events. However, it shouldn’t be applied for “Students acquired a new language”. In the same manner, the rule ‘X acquire Y → X learn Y ’ should be applied only when Y corresponds to some sort of knowledge, as in the latter example. Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don’t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule. For example, when answering the question “Which companies did IBM buy?” a QA system would apply the rule ‘X acquire Y → X buy Y ’ correctly, since the phrase “IBM acquire X” is likely to be found mostly in relevant economic contexts. We thus expect that an evaluation methodology should consider context relevance for entailment rules. For example, we would like both ‘X acquire Y → X buy Y ’ and ‘X acquire Y → X learn Y ’ to be assessed as correct (the second rule should n"
P07-1058,P03-1029,0,0.010513,", there is still no common accepted framework for their evaluation. Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directionality. Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1). One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002) or IE (Sudo et al., 2003; Romano et al., 2006). While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons. First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules. Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable. Third, within a complex system it is difficult to assess the exact quality of entailment rules"
P07-1058,W04-3206,1,\N,Missing
P08-1078,P06-1057,1,0.948507,"y”, because in this context Y is not a company. Similarly, the rule ‘X charge Y → X accuse Y ’ should not be applied to “This store charged my account”, since the assumed sense of ‘charge’ in the rule is different than its sense in the text. Thus, the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference. Context matching at inference time was often approached in an application-specific manner (Harabagiu et al., 2003; Patwardhan and Riloff, 2007). Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6). We propose a comprehensive framework for handling various contextual considerations, termed Contextual Preferences. It extends and generalizes previous work, defining the needed contextual components and their relationships. We also present and implement concrete representation models and un683 Proceedings of ACL-08: HLT, pages 683–691, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics supervised matching methods fo"
P08-1078,P07-1088,0,0.127325,"mpany. Similarly, the rule ‘X charge Y → X accuse Y ’ should not be applied to “This store charged my account”, since the assumed sense of ‘charge’ in the rule is different than its sense in the text. Thus, the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference. Context matching at inference time was often approached in an application-specific manner (Harabagiu et al., 2003; Patwardhan and Riloff, 2007). Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6). We propose a comprehensive framework for handling various contextual considerations, termed Contextual Preferences. It extends and generalizes previous work, defining the needed contextual components and their relationships. We also present and implement concrete representation models and un683 Proceedings of ACL-08: HLT, pages 683–691, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics supervised matching methods for these components. While our presentation"
P08-1078,W07-1401,1,0.24668,"needed for inference and their relationships. Contextual preferences extend and generalize previous notions, such as selectional preferences, while experiments show that the extended framework allows improving inference quality on real application data. 1 Introduction Applied semantic inference is typically concerned with inferring a target meaning from a given text. For example, to answer “Who wrote Idomeneo?”, Question Answering (QA) systems need to infer the target meaning ‘Mozart wrote Idomeneo’ from a given text “Mozart composed Idomeneo”. Following common Textual Entailment terminology (Giampiccolo et al., 2007), we denote the target meaning by h (for hypothesis) and the given text by t. A typical applied inference operation is matching. Sometimes, h can be directly matched in t (in the example above, if the given sentence would be literally “Mozart wrote Idomeneo”). Generally, the target meaning can be expressed in t in many different ways. Indirect matching is then needed, using inference knowledge that may be captured through rules, termed here entailment rules. In our example, ‘Mozart wrote Idomeneo’ can be inferred using the rule ‘X compose Y → X write Y ’. Recently, A common practice is to try"
P08-1078,P98-2127,0,0.815173,"ording to their “relevancy”. The score of matching two cpv:e lists, denoted here SCBC (·, ·), is the score of the highest scoring member that appears in both lists. We applied the final binary match score presented in (Pantel et al., 2007), denoted here binaryCBC: mv:e (r, t) is 1 if SCBC (r, t) is above a threshold and 0 otherwise. As a more natural ranking method, we also utilize SCBC directly, denoted rankedCBC, having mv:e (r, t) = SCBC (r, t). In addition, we tried a simpler method that directly compares the terms in two cpv:e lists, utilizing the commonly-used term similarity metric of (Lin, 1998a). This method, denoted LIN , uses the same raw distributional data as CBC but computes only pair-wise similarities, without any clustering phase. We calculated the scores of the 1000 most similar terms for every term in the Reuters RVC1 corpus3 . Then, a directional similarity of term a to term b, s(a, b), is set to be their similarity score if a is in b’s 1000 most similar terms and 0 otherwise. The final score of matching r with t is determined by a nearest-neighbor approach, as the score of the most similar pair of terms in the corresponding two lists of the same variable: mv:e (r, t) = m"
P08-1078,P00-1071,0,0.0170073,"ion 3 describes the specific CP models that were implemented in this paper. The CP framework provides a generic view of contextual modeling in applied semantic inference. Mapping from a specific application to the generic framework follows the mappings assumed in the Textual Entailment paradigm. For example, in QA the hypothesis to be proved corresponds to the affirmative template derived from the question (e.g. h: ‘X invented the PC’ for “Who invented the PC?”). Thus, cpg (h) can be constructed with respect to the question’s focus while cpv (h) may be generated from the expected answer type (Moldovan et al., 2000; Harabagiu et al., 2003). Construction of hypotheses’ CP for IE is demonstrated in Section 4. To represent the global context of an object z we utilize Latent Semantic Analysis (LSA) (Deerwester et al., 1990), a well-known method for representing the contextual-usage of words based on corpus statistics. We use LSA analysis of the BNC corpus1 , in which every term is represented by a normalized vector of the top 100 SVD dimensions, as described in (Gliozzo, 2005). To construct cpg (z) we first collect a set of terms that are representative for the preferred general context of z. Then, the (sin"
P08-1078,N07-1071,0,0.217472,"context Y is not a company. Similarly, the rule ‘X charge Y → X accuse Y ’ should not be applied to “This store charged my account”, since the assumed sense of ‘charge’ in the rule is different than its sense in the text. Thus, the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference. Context matching at inference time was often approached in an application-specific manner (Harabagiu et al., 2003; Patwardhan and Riloff, 2007). Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6). We propose a comprehensive framework for handling various contextual considerations, termed Contextual Preferences. It extends and generalizes previous work, defining the needed contextual components and their relationships. We also present and implement concrete representation models and un683 Proceedings of ACL-08: HLT, pages 683–691, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics supervised matching methods for these components. W"
P08-1078,D07-1075,0,0.0113395,"is (a hypothesis with variables) ‘X acquire Y ’. This h should not be matched in “children acquire language quickly”, because in this context Y is not a company. Similarly, the rule ‘X charge Y → X accuse Y ’ should not be applied to “This store charged my account”, since the assumed sense of ‘charge’ in the rule is different than its sense in the text. Thus, the intended contexts for h and r and the context within the given t should be properly matched to verify valid inference. Context matching at inference time was often approached in an application-specific manner (Harabagiu et al., 2003; Patwardhan and Riloff, 2007). Recently, some generic methods were proposed to handle context-sensitive inference (Dagan et al., 2006; Pantel et al., 2007; Downey et al., 2007; Connor and Roth, 2007), but these usually treat only a single aspect of context matching (see Section 6). We propose a comprehensive framework for handling various contextual considerations, termed Contextual Preferences. It extends and generalizes previous work, defining the needed contextual components and their relationships. We also present and implement concrete representation models and un683 Proceedings of ACL-08: HLT, pages 683–691, c Colum"
P08-1078,P02-1006,0,0.181376,"Missing"
P08-1078,E06-1052,1,0.837301,"Missing"
P08-1078,I05-5011,0,0.322749,"Missing"
P08-1078,W04-3206,1,0.721557,"Missing"
P08-1078,P07-1058,1,0.167284,"to multiple distinct predicates. For instance, the TransferMoney event refers to both donating and lending money, which are not distinguished by the ACE annotation. We also omitted three events with less than 10 mentions and two events for which the given set of learned rules could not match any mention. We were left with 24 event types for evaluation, which amount to 4085 event mentions in the dataset. Out of these, our binary templates can correctly match only mentions with at least two arguments, which appear 2076 times in the dataset. Comparing with previous evaluation methodologies, in (Szpektor et al., 2007; Pantel et al., 2007) proper context matching was evaluated by post-hoc judgment of a sample of rule applications for a sample of rules. Such annotation needs to be repeated each time the set of rules is changed. In addition, since the corpus annotation is not exhaustive, recall could not be computed. By contrast, we use a standard real-world dataset, in which all mentions are annotated. This allows immediate comparison of different rule sets and matching methods, without requiring any additional (post-hoc) annotation. 5 Results and Analysis We experimented with three rule setups over the ACE"
P08-1078,C98-2122,0,\N,Missing
P09-1051,D07-1073,0,0.202872,"utilized Wikipedia to build an ontology. Ponzetto and Strube (2007) identified the subsumption (IS-A) relation from Wikipedia’s category tags, while in Yago (Suchanek et al., 2007) these tags, redirect links and WordNet were used to identify instances of 14 predefined specific semantic relations. These methods depend on Wikipedia’s category system. The lexical reference relation we address subsumes most relations found in these works, while our extractions are not limited to a fixed set of predefined relations. Several works examined Wikipedia texts, rather than just its structured features. Kazama and Torisawa (2007) explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title. We reproduce this part of their work as one of our baselines. Toral and Mu˜noz (2007) uses all nouns in the first sentence. Gabrilovich and Markovitch (2007) utilized Wikipedia-based concepts as the basis for a high-dimensional meaning representation space. Hearst (1992) utilized a list of patterns indicative for the hyponym relation in general texts. Snow et al. (2006) use syntactic path patterns as features for supervised hyponymy and synonymy 3 Extracting R"
P09-1051,P85-1037,0,0.192548,"scale knowledge bases of LR rules. A prominent available resource is WordNet (Fellbaum, 1998), from which classical relations such as synonyms, hyponyms and some cases of meronyms may be used as LR rules. An extension to WordNet was presented by (Snow et al., 2006). Yet, available resources do not cover the full scope of lexical reference. This paper presents the extraction of a largescale rule base from Wikipedia designed to cover a wide scope of the lexical reference relation. As a starting point we examine the potential of definition sentences as a source for LR rules (Ide and Jean, 1993; Chodorow et al., 1985; Moldovan and Rus, 2001). When writing a concept definition, one aims to formulate a concise text that includes the most characteristic aspects of the defined concept. Therefore, a definition is a promising source for LR relations between the defined concept and the definition terms. In addition, we extract LR rules from Wikipedia redirect and hyperlink relations. As a guideline, we focused on developing simple extraction methods that may be applicable for other Web knowledge resources, rather than focusing on Wikipedia-specific attributes. Overall, our rule base contains about 8 million cand"
P09-1051,P98-2127,0,0.114657,"l as category tags, structured summary tablets for 11 Link Gestaltist ⇒ Gestalt psychology specific semantic classes, and articles containing Table 1: Examples of rule extraction methods lists which were exploited in prior work as reviewed in Section 2. As shown next, the different extraction meththem as the RHS of a rule whose LHS is the article ods yield different precision levels. This may altitle. While Kazama and Torisawa used a chunlow an application to utilize only a portion of the ker, we parsed the definition sentence using Minirule base whose precision is above a desired level, par (Lin, 1998b). Our initial experiments showed and thus choose between several possible recallthat parse-based extraction is more accurate than precision tradeoffs. chunk-based extraction. It also enables us extracting additional rules by splitting conjoined noun 4 Extraction Methods Analysis phrases and by taking both the head noun and the complete base noun phrase as the RHS for sepaWe applied our rule extraction methods over a rate rules (examples 1–3 in Table 1). version of Wikipedia available in a database conAll-N The Be-Comp extraction method yields structed by (Zesch et al., 2007)2 . The extractio"
P09-1051,W99-0908,0,0.0104541,"ted through a pooling procedure. Thus, some of our valid lexical expansions might retrieve non-annotated documents that were missed by the previously pooled systems. 2 · C(LHS, RHS) C(LHS) + C(RHS) 6.1.1 where C(x) is the number of articles in Wikipedia in which all words of x appear. In order to partially overcome the Wrong NP part error, identified in Section 4.2 to be the most common error, we adjust the Dice equation for rules whose RHS is also part of a larger noun phrase (NP): Experimental Setting Our categorization experiment follows a typical keywords-based text categorization scheme (McCallum and Nigam, 1999; Liu et al., 2004). Taking a lexical reference perspective, we assume that the characteristic expansion terms for a category should refer to the term (or terms) denoting the category name. Accordingly, we construct the category’s feature vector by taking first the category name itself, and then expanding it with all lefthand sides of lexical reference rules whose righthand side is the category name. For example, the category “Cars” is expanded by rules such as Ferrari F50 ⇒ car. During classification cosine similarity is measured between the feature vector of the classified document and the e"
P09-1051,W09-3710,0,0.0262088,"Missing"
P09-1051,P01-1052,0,0.015477,"of LR rules. A prominent available resource is WordNet (Fellbaum, 1998), from which classical relations such as synonyms, hyponyms and some cases of meronyms may be used as LR rules. An extension to WordNet was presented by (Snow et al., 2006). Yet, available resources do not cover the full scope of lexical reference. This paper presents the extraction of a largescale rule base from Wikipedia designed to cover a wide scope of the lexical reference relation. As a starting point we examine the potential of definition sentences as a source for LR rules (Ide and Jean, 1993; Chodorow et al., 1985; Moldovan and Rus, 2001). When writing a concept definition, one aims to formulate a concise text that includes the most characteristic aspects of the defined concept. Therefore, a definition is a promising source for LR relations between the defined concept and the definition terms. In addition, we extract LR rules from Wikipedia redirect and hyperlink relations. As a guideline, we focused on developing simple extraction methods that may be applicable for other Web knowledge resources, rather than focusing on Wikipedia-specific attributes. Overall, our rule base contains about 8 million candidate lexical refThis pap"
P09-1051,fillmore-etal-2002-seeing,0,0.0256946,"Missing"
P09-1051,C02-1007,0,0.0177694,"tically constructed baselines and comparable results to WordNet. A combination with WordNet achieved the best performance, indicating the significant marginal contribution of our rule base. 2 classifiers, whose training examples are derived automatically from WordNet. They use these classifiers to suggest extensions to the WordNet hierarchy, the largest one consisting of 400K new links. Their automatically created resource is regarded in our paper as a primary baseline for comparison. Many works addressed the more general notion of lexical associations, or association rules (e.g. (Ruge, 1992; Rapp, 2002)). For example, The Beatles, Abbey Road and Sgt. Pepper would all be considered lexically associated. However this is a rather loose notion, which only indicates that terms are semantically “related” and are likely to co-occur with each other. On the other hand, lexical reference is a special case of lexical association, which specifies concretely that a reference to the meaning of one term may be inferred from the other. For example, Abbey Road provides a concrete reference to The Beatles, enabling to infer a sentence like “I listened to The Beatles” from “I listened to Abbey Road”, while it"
P09-1051,W07-1401,1,0.424457,"Missing"
P09-1051,P06-1101,0,0.140294,"oader relation. For instance, the LR rule physician ⇒ medicine may be useful to infer the topic medicine in a text categorization setting, while an information extraction system may utilize the rule Margaret Thatcher ⇒ United Kingdom to infer a UK announcement from the text “Margaret Thatcher announced”. To perform such inferences, systems need large scale knowledge bases of LR rules. A prominent available resource is WordNet (Fellbaum, 1998), from which classical relations such as synonyms, hyponyms and some cases of meronyms may be used as LR rules. An extension to WordNet was presented by (Snow et al., 2006). Yet, available resources do not cover the full scope of lexical reference. This paper presents the extraction of a largescale rule base from Wikipedia designed to cover a wide scope of the lexical reference relation. As a starting point we examine the potential of definition sentences as a source for LR rules (Ide and Jean, 1993; Chodorow et al., 1985; Moldovan and Rus, 2001). When writing a concept definition, one aims to formulate a concise text that includes the most characteristic aspects of the defined concept. Therefore, a definition is a promising source for LR relations between the d"
P09-1051,W06-1621,1,0.845854,"o perform better than other automatically constructed baselines in a couple of lexical expansion and matching tasks. Our rule-base yields comparable performance to WordNet while providing largely complementary information. 1 Introduction A most common need in applied semantic inference is to infer the meaning of a target term from other terms in a text. For example, a Question Answering system may infer the answer to a question regarding luxury cars from a text mentioning Bentley, which provides a concrete reference to the sought meaning. Aiming to capture such lexical inferences we followed (Glickman et al., 2006), which coined the term lexical reference (LR) to denote references in text to the specific meaning of a target term. They further analyzed the dataset of the First Recognizing Textual Entailment Challenge (Dagan et al., 2006), which includes examples drawn from seven different application scenarios. It was found that an entailing text indeed includes a concrete reference to practically every term in the entailed (inferred) sentence. The lexical reference relation between two terms may be viewed as a lexical inference rule, denoted LHS ⇒ RHS. Such rule indicates that the left-hand-side term wo"
P09-1051,J86-3002,0,0.669684,"Missing"
P09-1051,C92-2082,0,0.135108,"und in these works, while our extractions are not limited to a fixed set of predefined relations. Several works examined Wikipedia texts, rather than just its structured features. Kazama and Torisawa (2007) explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title. We reproduce this part of their work as one of our baselines. Toral and Mu˜noz (2007) uses all nouns in the first sentence. Gabrilovich and Markovitch (2007) utilized Wikipedia-based concepts as the basis for a high-dimensional meaning representation space. Hearst (1992) utilized a list of patterns indicative for the hyponym relation in general texts. Snow et al. (2006) use syntactic path patterns as features for supervised hyponymy and synonymy 3 Extracting Rules from Wikipedia Our goal is to utilize the broad knowledge of Wikipedia to extract a knowledge base of lexical reference rules. Each Wikipedia article provides a definition for the concept denoted by the title of the article. As the most concise definition we take the first sentence of each article, following (Kazama and Torisawa, 2007). Our preliminary evaluations showed that taking the entire first"
P09-1051,J96-3009,0,0.0355387,"Missing"
P09-1051,W07-1421,0,0.0233313,"Missing"
P09-1051,C98-2122,0,\N,Missing
P09-1089,W07-0734,0,0.0147474,"ets and oceanfront homes”. Here, chalets are replaced by houses or units (depending on the model), providing a translation that would be acceptable by most readers. Other incorrect translations occurred when the unknown term was part of a phrase, for example, troughs replaced with depressions in peaks Automatic MT Evaluation Although automatic MT evaluation metrics are less appropriate for capturing the variations generated by our method, to ensure that there was no degradation in the system-level scores according to such metrics we also measured the models’ performance using BLEU and METEOR (Agarwal and Lavie, 2007). The version of METEOR we used on the target language (French) considers the stems of the words, instead of surface forms only, but does not make use of WordNet synonyms. We evaluated the performance of the top models of Table 1, as well as of a baseline SMT system that left unknown terms untranslated, on the sample of 1,014 manually annotated sentences. As shown in Table 3, all models resulted in improvement with respect to the original sentences (base797 (Dyer et al., 2008) in order to allow a compact representation of alternative inputs to an SMT system. This is an approach that we intend"
P09-1089,N06-1003,0,0.45233,"Missing"
P09-1089,W08-0309,0,0.0202988,"Missing"
P09-1089,D08-1021,0,0.0212981,"s can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but not content words. The p"
P09-1089,P07-1092,0,0.0147012,"mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (t"
P09-1089,P04-1036,0,0.00728935,"e also applied several combinations of source models, such as LSA combined with LMS, to take advantage of their complementary strengths. Additionally, we assessed our method with sourceonly models, by setting the number of sentences to be selected by the source model to one (k = 1). Scoring source texts We test our proposed method using several context-models shown to perform reasonably well in previous work: • FREQ: The first model we use is a contextindependent baseline. A common useful heuristic to pick an entailment rule is to select the candidate with the highest frequency in the corpus (Mccarthy et al., 2004). In this model, a rule’s score is the normalized number of occurrences of its RHS in the training corpus, ignoring the context of the LHS. 5 5.1 Manual Evaluation To evaluate the translations produced using the various source and target models and the different rule-sets, we rely mostly on manual assessment, since automatic MT evaluation metrics like BLEU do not capture well the type of semantic variations • LSA: Latent Semantic Analysis (Deerwester et al., 1990) is a well-known method for rep2 Results http://opennlp.sourceforge.net 795 Model 1 2 3 4 5 6 7 –:SMT NB:SMT LSA:SMT NB:– LMS:LMT FR"
P09-1089,P06-1057,1,0.819303,"lment rules, i.e. rules without variables. Various resources for lexical rules are available, and the prominent one is WordNet (Fellbaum, 1998), which has been used in virtually all TE systems (Giampiccolo et al., 2007). Typically, a rule application is valid only under specific contexts. For example, mouse ⇒ rodent should not be applied to “Use the mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is"
P09-1089,D08-1022,0,0.0101575,"Missing"
P09-1089,W09-0404,0,0.0103699,"ossible, to generate more general texts for translation. Our approach, based on the textual entailment framework, considers the newly generated texts as entailed from the original one. Monolingual semantic resources such as WordNet can provide entailment rules required for both these symmetric and asymmetric entailment relations. Textual Entailment (TE) has recently become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications (Giampiccolo et al., 2007). Yet, its application to SMT has been so far limited to MT evaluation (Pado et al., 2009). TE defines a directional relation between two texts, where the meaning of the entailed text (hypothesis, h) can be inferred from the meaning of the entailing text, t. Under this paradigm, paraphrases are a special case of the entailment relation, when the relation is symmetric (the texts entail each other). Otherwise, we say that one text directionally entails the other. A common practice for proving (or generating) h from t is to apply entailment rules to t. An entailment rule, denoted LHS ⇒ RHS, specifies an entailment relation between two text fragments (the Left- and Right- Hand Sides),"
P09-1089,P08-1115,0,0.0258391,"he system-level scores according to such metrics we also measured the models’ performance using BLEU and METEOR (Agarwal and Lavie, 2007). The version of METEOR we used on the target language (French) considers the stems of the words, instead of surface forms only, but does not make use of WordNet synonyms. We evaluated the performance of the top models of Table 1, as well as of a baseline SMT system that left unknown terms untranslated, on the sample of 1,014 manually annotated sentences. As shown in Table 3, all models resulted in improvement with respect to the original sentences (base797 (Dyer et al., 2008) in order to allow a compact representation of alternative inputs to an SMT system. This is an approach that we intend to explore in future work, as a way to efficiently handle the different source language alternatives generated by entailment rules. However, since most current MT systems do not accept such type of inputs, we consider the results on pruning by source-side context models as broadly relevant. and troughs, a problem that also strongly affects paraphrasing. In another case, movement was the hypernym chosen to replace labor in labor movement, yielding an awkward text for translatio"
P09-1089,P02-1040,0,0.104704,"ord order and minor lexical variations (tenses etc.), but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. This, however, may yield distorted probability estimates in the phrase table, since these were not computed from parallel data. The main use of monolingual paraphrases in MT to date has been for evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al., 2002). 2.3 3 Textual Entailment and Entailment Rules Textual Entailment for Statistical Machine Translation Previous solutions for handling unknown terms in a source text s augment the SMT system’s phrase table based on multilingual corpora. This allows indirectly paraphrasing s, when the SMT system chooses to use a paraphrase included in the table and produces a translation with the corresponding target phrase for the unknown term. We propose using monolingual paraphrasing methods and resources for this task to obtain a more extensive set of rules for paraphrasing the source. These rules are then"
P09-1089,eck-etal-2008-communicating,0,0.0802498,"Missing"
P09-1089,H05-1095,1,0.693883,"setting of these experiments is described in what follows. Preliminary analysis confirmed (as expected) that readers prefer translations of paraphrases, when available, over translations of directional entailments. This consideration is therefore taken into account in the proposed method. The input is a text unit to be translated, such as a sentence or paragraph, with one or more unknown terms. For each unknown term we first fetch a list of candidate rules for paraphrasing (e.g. synonyms), where the unknown term is the LHS. For SMT data To produce sentences for our experiments, we use Matrax (Simard et al., 2005), a standard phrase-based SMT system, with the exception that it allows gaps in phrases. We use approximately 1M sentence pairs from the English-French 794 Europarl corpus for training, and then translate a test set of 5,859 English sentences from the News corpus into French. Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al., 2008). Hence, we compare our method in a setting where the training and test data are from different domains, a common scenario in the practical use of MT systems. Of the 5,859 translated sentences, 2,494 contain unknown terms (c"
P09-1089,W07-1401,1,0.667804,"cal selection and ordering. This phenomenon is demonstrated in the following sentences, where the translation of the English sentence (1) is acceptable only when the unknown word (in bold) is replaced with a translatable paraphrase (3): candidates before supplying them to the translation engine, thus improving translation efficiency. Second, the ranking may be combined with target language information in order to choose the best translation, thus improving translation quality. We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (Giampiccolo et al., 2007). TE provides a generic way for handling language variability, identifying when the meaning of one text is entailed by the other (i.e. the meaning of the entailed text can be inferred from the meaning of the entailing one). When the meanings of two texts are equivalent (paraphrase), entailment is mutual. Typically, a more general version of a certain text is entailed by it. Hence, through TE we can formalize the generation of both equivalent and more general texts for the source text. When possible, a paraphrase is used. Otherwise, an alternative text whose meaning is entailed by the original"
P09-1089,P08-1078,1,0.677078,"should not be applied to “Use the mouse to mark your answers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word ord"
P09-1089,W06-2907,1,0.899223,"Missing"
P09-1089,E06-1006,0,0.130232,"Missing"
P09-1089,P08-1089,0,0.035166,"ers”. Context-models can be exploited to validate the application of a rule to a text. In such models, an explicit Word Sense Disambiguation (WSD) is not necessarily required; rather, an implicit sense-match is sought after (Dagan et al., 2006). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. However, in general, entailment rule application addresses other aspects of context matching as well (Szpektor et al., 2008). al., 2006; Cohn and Lapata, 2007; Zhao et al., 2008; Callison-Burch, 2008; Guzm´an and Garrido, 2008). The procedure to extract paraphrases in these approaches is similar to standard phrase extraction in SMT systems, and therefore a large amount of additional parallel corpus is required. Moreover, as discussed in Section 5, when unknown texts are not from the same domain as the SMT training corpus, it is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but no"
P09-1089,P08-2015,0,0.0376735,"Missing"
P09-1089,N06-1058,0,0.0361947,"t is likely that paraphrases found through such methods will yield misleading translations. Bond et al. (2008) use grammars to paraphrase the whole source sentence, covering aspects like word order and minor lexical variations (tenses etc.), but not content words. The paraphrases are added to the source side of the corpus and the corresponding target sentences are duplicated. This, however, may yield distorted probability estimates in the phrase table, since these were not computed from parallel data. The main use of monolingual paraphrases in MT to date has been for evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al., 2002). 2.3 3 Textual Entailment and Entailment Rules Textual Entailment for Statistical Machine Translation Previous solutions for handling unknown terms in a source text s augment the SMT system’s phrase table based on multilingual corpora. This allows indirectly paraphrasing s, when the SMT system chooses to use a paraphrase included in the table and produces a translation with the corresponding target phrase for the"
P09-1089,P97-1017,0,0.266109,"Missing"
P09-1089,E03-1076,0,0.130127,"Missing"
P09-1089,D07-1092,0,0.0249287,"Missing"
P09-1089,S07-1009,0,0.0137349,"Missing"
P09-1089,2008.iwslt-papers.2,0,\N,Missing
P09-2018,P05-1014,1,0.965497,"yan.geffet@gmail.com lili.dav@gmail.com {dagan,szpekti}@cs.biu.ac.il Abstract IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1 . In broader prospect, we suggest that asymmetric measures might be more"
P09-2018,P98-2127,0,0.932433,"h is generally asymmetric. This paper investigates the nature of directional (asymmetric) similarity measures, which aim to quantify distributional feature inclusion. We identify desired properties of such measures, specify a particular one based on averaged precision, and demonstrate the empirical benefit of directional measures for expansion. 1 Introduction Much work on automatic identification of semantically similar terms exploits Distributional Similarity, assuming that such terms appear in similar contexts. This has been now an active research area for a couple of decades (Hindle, 1990; Lin, 1998; Weeds and Weir, 2003). This paper is motivated by one of the prominent applications of distributional similarity, namely identifying lexical expansions. Lexical expansion looks for terms whose meaning implies that of a given target term, such as a query. It is widely employed to overcome lexical variability in applications like Information Retrieval (IR), Information Extraction (IE) and Question Answering (QA). Often, distributional similarity measures are used to identify expanding terms (e.g. (Xu and Croft, 1996; Mandala et al., 1999)). Here we denote the relation between an expanding term"
P09-2018,C08-1107,1,0.895201,"n,szpekti}@cs.biu.ac.il Abstract IR, for instance, a user looking for “baby food” will be satisfied with documents about “baby pap” or “baby juice” (‘pap → food’, ‘juice → food’); but when looking for “frozen juice” she will not be satisfied by “frozen food”. More generally, directional relations are abundant in NLP settings, making symmetric similarity measures less suitable for their identification. Despite the need for directional similarity measures, their investigation counts, to the best of our knowledge, only few works (Weeds and Weir, 2003; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Michelbacher et al., 2007) and is utterly lacking. From an expansion perspective, the common expectation is that the context features characterizing an expanding word should be largely included in those of the expanded word. This paper investigates the nature of directional similarity measures. We identify their desired properties, design a novel measure based on these properties, and demonstrate its empirical advantage in expansion settings over state-of-the-art measures1 . In broader prospect, we suggest that asymmetric measures might be more suitable than symmetric ones for many other set"
P09-2018,W03-1011,0,0.502155,"Missing"
P09-2018,C04-1146,0,0.0399869,"for inclusion, as tested features. Amongst these features, those found in v’s feature vector are termed included features. In preliminary data analysis of pairs of feature vectors, which correspond to a known set of valid and invalid expansions, we identified the following desired properties for a distributional inclusion measure. Such measure should reflect: where F Vx is the feature vector of a word x and wx (f ) is the weight of the feature f in that word’s vector, set to their pointwise mutual information. Few works investigated a directional similarity approach. Weeds and Weir (2003) and Weeds et al. (2004) proposed a precision measure, denoted here WeedsPrec, for identifying the hyponymy relation and other generalization/specification cases. It quantifies the weighted coverage (or inclusion) of the candidate hyponym’s features (u) by the hypernym’s (v) features: P f ∈F V ∩F Vv wu (f ) WeedsPrec(u → v) = P u f ∈F Vu wu (f ) 1. the proportion of included features amongst the tested ones (the core inclusion idea). The assumption behind WeedsPrec is that if one word is indeed a generalization of the other then the features of the more specific word are likely to be included in those of the more gen"
P09-2018,D07-1017,0,\N,Missing
P09-2018,J86-3002,0,\N,Missing
P09-2018,J96-3009,0,\N,Missing
P09-2018,C92-2082,0,\N,Missing
P09-2018,W06-1621,1,\N,Missing
P09-2018,P90-1034,0,\N,Missing
P09-2018,P06-1057,1,\N,Missing
P09-2018,P07-1058,1,\N,Missing
P09-2018,C98-2122,0,\N,Missing
P09-2018,P08-1078,1,\N,Missing
P09-2018,N04-1041,0,\N,Missing
P09-2018,W99-0908,0,\N,Missing
P10-1123,abad-etal-2010-resource,1,0.723492,"latter is not found in the focus term’s reference chain. In initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. We identified three distinct cases for the integration of discourse reference knowledge in entailment, which correspond to different relations between the target component, the focus term and the reference term. This section describes the three cases and characterizes them in terms of tree transformations. An initial version of these transformations is described in (Abad et al., 2010). We assume a transformation-based entailment architecture (cf. Section 2.2), although we believe that the key points of our account are also applicable to alignment-based architecture. Transformations create revised trees that cover previously uncovered target components in H. The output of each transformation, T1 , is comprised of copies of the components used to construct it, and is appended to the discourse forest, which includes the dependency trees of all sentences and their generated consequents. We assume that we have access to a dependency tree for H, a dependency forest for T and its"
P10-1123,W07-1420,0,0.0545505,"Missing"
P10-1123,H05-1079,0,0.0477878,"Missing"
P10-1123,P09-1068,0,0.0157562,"d bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who ki"
P10-1123,W07-1427,0,0.0249585,"Missing"
P10-1123,W07-1401,1,0.734314,"e, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007; Giampiccolo et al., 2008), the main benchmark for TE technology, two architectures for modeling TE have emerged as dominant: transformations and alignment. The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by finding a “proof”, i.e., a sequence of consequents, (T, T1 , . . . , Tn ), such that Tn =H (Bar-Haim et al., 2008; Harmeling, 2009), and that in each transformation, Ti → Ti+1 , the consequent Ti+1 is entailed by Ti . These transformations commonly include lexical modifications and the generation of syntactic alternatives. The second major approach"
P10-1123,C96-1079,0,0.00959989,"tion plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who killed Kennedy?. A second, more complex type of information stems from bridging references, such as in the following discourse (Asher"
P10-1123,J97-1003,0,0.0586295,"209 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1209–1219, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics mechanisms (Section 5). Section 6 presents quantitative findings and further observations. Conclusions are discussed in Section 7. event coreference and bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that"
P10-1123,N06-2015,0,0.033287,"course references on entailment with an annotation study which removes these limitations. To counteract (1), we use the recent RTE-5 Search dataset (details below). To avoid (2), we perform a manual analysis, assuming discourse references as predicted by an oracle. With regards to (3), our annotation scheme covers coreference and bridging relations of all syntactic categories and classifies them. As for (4), we suggest several operations necessary to integrate the discourse information into an entailment engine. In contrast to the numerous existing datasets annotated for discourse references (Hovy et al., 2006; Strassel et al., 2008), we do not annotate exhaustively. Rather, we are interested specifically in those references instances that impact inference. Furthermore, we analyze each instance from an entailment perspective, characterizing the relevant factors that have an impact on inference. To our knowledge, this is the first such in-depth study.4 The results of our study are of twofold interest. First, they provide guidance for the developers of reference resolvers who might prioritize the scope of their systems to make them more valuable for inference. Second, they point out potential directi"
P10-1123,P09-1047,0,0.0235941,"Missing"
P10-1123,P09-1083,0,0.0409466,"sher and Lascarides, 1998): (2) “I’ve just arrived. The camel is outside.” While coreference indicates equivalence, bridging points to the existence of a salient semantic relation between two distinct entities or events. Here, it is (informally) ‘means of transport’, which would make the discourse (2) relevant for a question like How did I arrive here?. Other types of bridging relations include set-membership, roles in events and consequence (Clark, 1975). Note, however, that text understanding systems are generally limited to the resolution of entity (or even just pronoun) coreference, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007"
P10-1123,W04-0704,0,0.0236046,"n 6 presents quantitative findings and further observations. Conclusions are discussed in Section 7. event coreference and bridging.1 Another reason is uncertainty about their practical importance. 2.2 2 2.1 Background Discourse in NLP Discourse information plays a role in a range of NLP tasks. It is obviously central to discourse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in E"
P10-1123,P93-1016,0,0.0375104,"formation from the discourse sentences T 0 / T 00 . Referring terms (in T ) and target terms (in H) are shown in boldface. 4 Analysis Scheme For annotating the RTE-5 data, we operationalize reference relations that are relevant for entailment as those that improve coverage. Recall from Section 2.2 that the concept of coverage is applicable to both transformation and alignment models, all of which aim at maximizing coverage of H by T . We represent T and H as syntactic trees, as common in the RTE literature (Zanzotto et al., 2009; Agichtein et al., 2008). Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them. We use “term” to refer to text expressions, and “components” to refer to nodes, edges, and subtrees. Dependency trees are a popular choice in RTE since they offer a fairly semantics-oriented account of the sentence structure that can still be constructed robustly. In an ideal case of entailment, all nodes and dependency edges of H are covered by T . For each T − H pair, we annotate all relevant discourse references in terms of three items: the target component in H, the focus term"
P10-1123,D08-1084,0,0.0164515,"Missing"
P10-1123,W03-2606,0,0.0307542,". Alignment quality is generally determined based on features that assess the validity of the local replacement of the T entity by the H entity. While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining a maximal coverage of H by T , i.e. to identify matches of as many elements of H within T as possible.2 To do so, both architectures typically make use of inference rules such as ‘Y was purchased by X → X paid for Y’, either by directly applying them as transformations, or by using them 1 Some studies, e.g. (Markert et al., 2003; Poesio et al., 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable. 2 Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. 1210 to score alignments. Rules are generally drawn from external knowledge resources, such as WordNet (Fellbaum, 1998) or DIRT (Lin and Pantel, 2001), although knowledge gaps remain a key obstacle (Bos, 2005; Balahur et al., 2008; Bar-Haim et al., 2008). Discourse in previous RTE challenges. The first two"
P10-1123,P04-1019,0,0.080464,"generally determined based on features that assess the validity of the local replacement of the T entity by the H entity. While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining a maximal coverage of H by T , i.e. to identify matches of as many elements of H within T as possible.2 To do so, both architectures typically make use of inference rules such as ‘Y was purchased by X → X paid for Y’, either by directly applying them as transformations, or by using them 1 Some studies, e.g. (Markert et al., 2003; Poesio et al., 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable. 2 Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. 1210 to score alignments. Rules are generally drawn from external knowledge resources, such as WordNet (Fellbaum, 1998) or DIRT (Lin and Pantel, 2001), although knowledge gaps remain a key obstacle (Bos, 2005; Balahur et al., 2008; Bar-Haim et al., 2008). Discourse in previous RTE challenges. The first two rounds of the RTE cha"
P10-1123,N06-1025,0,0.027942,"inference. We identified three general cases, and suggested matching operations to obtain the relevant inferences, formulated as tree transformations. Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treating it as a preprocessing step. A particularly interesting result concerns the interplay between discourse references and entailment knowledge. While semantic knowledge (e.g., from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al., 2001; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. Given that both of the technologies are still imperfect, we envisage the way forward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. Example 3). In sum, our study shows that"
P10-1123,qiu-etal-2004-public,0,0.179043,"Missing"
P10-1123,E06-1052,1,0.762829,"ce (Clark, 1975). Note, however, that text understanding systems are generally limited to the resolution of entity (or even just pronoun) coreference, e.g. (Li et al., 2009; Dali et al., 2009). An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as Discourse in Textual Entailment Textual Entailment has been introduced in Section 1 as a common-sense notion of inference. It has spawned interest in the computational linguistics community as a common denominator of many NLP tasks including IE, summarization and tutoring (Romano et al., 2006; Harabagiu et al., 2007; Nielsen et al., 2009). Architectures for Textual Entailment. Over the course of recent RTE challenges (Giampiccolo et al., 2007; Giampiccolo et al., 2008), the main benchmark for TE technology, two architectures for modeling TE have emerged as dominant: transformations and alignment. The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by finding a “proof”, i.e., a sequence of consequents, (T, T1 , . . . , Tn ), such that Tn =H (Bar-Haim et al., 2008; Harmeling, 2009), and that in each transformation, Ti → Ti+1 , the consequent Ti+1"
P10-1123,J01-4004,0,0.0259146,"uently relevant for inference. We identified three general cases, and suggested matching operations to obtain the relevant inferences, formulated as tree transformations. Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treating it as a preprocessing step. A particularly interesting result concerns the interplay between discourse references and entailment knowledge. While semantic knowledge (e.g., from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al., 2001; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. Given that both of the technologies are still imperfect, we envisage the way forward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. Example 3). I"
P10-1123,strassel-etal-2008-linguistic,0,0.122312,"urse processing tasks such as text segmentation (Hearst, 1997). Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al., 2004; Na and Ng, 2009), as well as for the acquisition of lexical-semantic “narrative schema” knowledge (Chambers and Jurafsky, 2009). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). The simplest form of information that discourse provides is coreference, i.e., information that two linguistic expressions refer to the same entity or event. Coreference is particularly important for processing pronouns and other anaphoric expressions, such as he in Example 1. Ability to resolve this reference translates directly into, e.g., a QA system’s ability to answer questions like Who killed Kennedy?. A second, more complex type of information stems from bridging references, such as in the following discourse (Asher and Lascarides, 1998): (2) “I’ve just arrived. The camel is outside.”"
P10-1123,W07-1400,0,\N,Missing
P10-1124,D07-1017,0,0.44882,"entailment that can be used for semantic inference (Budanitsky and Hirst, 2006). WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow et al., 2005), and we make a similar use of WordNet in Section 5.1. Lexicographic resources are accurate but tend to have low coverage. Therefore, distributional similarity is used to learn broad-scale resources. Distributional similarity algorithms predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Details on such methods are given in Section 5.1. Global learning It is natural to describe entailment relations between predicates by a graph. Nodes represent predicates, and edges represent entailment between nodes. Nevertheless, using a graph for global learning of entailment between predicates has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource A"
P10-1124,J06-1003,0,0.00428957,"entailment rules between predicates: lexicographic resources and distributional similarity resources. Lexicographic 1220 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics resources are manually-prepared knowledge bases containing information about semantic relations between lexical items. WordNet (Fellbaum, 1998), by far the most widely used resource, specifies relations such as hyponymy, derivation, and entailment that can be used for semantic inference (Budanitsky and Hirst, 2006). WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow et al., 2005), and we make a similar use of WordNet in Section 5.1. Lexicographic resources are accurate but tend to have low coverage. Therefore, distributional similarity is used to learn broad-scale resources. Distributional similarity algorithms predict a semantic relation between two predicates by comparing the arguments with which they occur. Quite a few methods have been suggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the spec"
P10-1124,P08-2012,0,0.0635342,"traints, linear programming is a natural choice, enabling the use of state of the art optimization packages. We describe two formulations of integer linear programs that learn the edges: one maximizing a global score function, and another maximizing a global probability function. Let Iuv be an indicator denoting the event that node u entails node v. Our goal is to learn the edges E over a set of nodes V . We start by formulating the constraints and then the target functions. The first constraint is that the graph must respect transitivity. Our formulation is equivalent to the one suggested by Finkel and Manning (2008) in a coreference resolution task: ∀u,v,w∈V Iuv + Ivw − Iuw ≤ 1 In addition, for a few pairs of nodes we have strong evidence that one does not entail the other and so we add the constraint Iuv = 0. Combined with the constraint of transitivity this implies that there must be no path from u to v. This is done in the following two scenarios: (1) When two nodes u and v are identical except for a pair of words wu and wv , and wu is an antonym of wv , or a hypernym of wv at distance ≥ 2. (2) When two nodes u and v are transitive opposites, that is, if u = subj obj obj subj X ←−− w −−→ Y and v = X ←"
P10-1124,P09-1039,0,0.0332457,"that maximize the probability of the evidence while respecting the transitivity constraint. In this paper we tackle a similar problem of learning a transitive relation, but we use linear programming. A Linear Program (LP) is an optimization problem, where a linear function is minimized (or maximized) under linear constraints. If the variables are integers, the problem is termed an Integer Linear Program (ILP). Linear programming has attracted attention recently in several fields of NLP, such as semantic role labeling, summarization and parsing (Roth and tau Yih, 2005; Clarke and Lapata, 2008; Martins et al., 2009). In this paper we formulate the entailment graph learning problem as an Integer Linear Program, and find that this leads to an optimal solution with respect to the target function in our experiment. 3 Entailment Graph This section presents an entailment graph structure, which resembles the graph in (Szpektor and Dagan, 2009). The nodes of an entailment graph are propositional templates. A propositional template is a path in a dependency tree between two arguments of a common predicate1 (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Note that in a dependency parse, such a path passes throug"
P10-1124,I05-5011,0,0.192819,"Missing"
P10-1124,P05-1044,0,0.0101971,"aining set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set of propositional templates extracted from the corpus. For each ti ∈ T with two variables and a single predicate word w, we extract from WordNet the set H of direct hypernyms and synonyms of w. For every h ∈ H, we generate a new template tj from ti by replacing w with h. If tj ∈ T , we consider (ti , tj ) to be a positive example. Negative examples are generated analogously, by looking at direct co-hyponyms of w instead of hypernyms and synonyms. This follows the notion of “contrastive estimation” (Smith and Eisner, 2005), since we generate negative examples that are semantically similar to positive examples and thus focus the classifier’s attention on identifying the boundary between the classes. Last, we filter training examples for which all features are zero, and sample an equal number of positive and negative examples (for which we compute similarity features), since classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse et al., 2007; Nikulin, 2008). 5.2 Global learning of edges Once the entailment classifier is trained we learn the graph edges given its nodes."
P10-1124,P06-1101,0,0.243251,"Missing"
P10-1124,N07-1031,0,0.114355,"nectivity component2 of the graph, all nodes are synonymous. Moreover, if we merge every strong connectivity component to a single node, the graph becomes a Directed Acyclic Graph (DAG), and the graph nodes can be sorted and presented hierarchically. Next, we show an application that leverages this property. 4 Motivating Application In this section we propose an application that provides a hierarchical view of propositions extracted from a corpus, based on an entailment graph. Organizing information in large collections has been found to be useful for effective information access (Kaki, 2005; Stoica et al., 2007). It allows for easier data exploration, and provides a compact view of the underlying content. A simple form of structural presentation is by a single hierarchy, e.g. (Hofmann, 1999). A more complex approach is hierarchical faceted metadata, where a number of concept hierarchies are created, corresponding to different facets or dimensions (Stoica et al., 2007). Hierarchical faceted metadata categorizes concepts of a domain in several dimensions, but does not specify the relations between them. For example, in the health-care domain we might have facets for categories such as diseases and symp"
P10-1124,C08-1107,1,0.871124,"erence relation between two text fragments (when the rule is bidirectional this is known as paraphrasing). An important type of entailment rule refers to propositional templates, i.e., propositions comprising a predicate and arguments, possibly replaced by variables. The rule required for the previous example would be ‘X reduce Y → X affect Y’. Because facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008). Previous work has focused on learning each entailment rule in isolation. However, it is clear that there are interactions between rules. A prominent example is that entailment is a transitive relation, and thus the rules ‘X → Y ’ and ‘Y → Z’ imply the rule ‘X → Z’. In this paper we take advantage of these global interactions to improve entailment rule learning. First, we describe a structure termed an entailment graph that models entailment relations between propositional templates (Section 3). Next, we show that we can present propositions according to an entailment hierarchy derived from t"
P10-1124,W09-2504,1,0.834018,"ggested (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the specifics of the ways in which predicates are represented, the features that are extracted, and the function used to compute feature vector similarity. Details on such methods are given in Section 5.1. Global learning It is natural to describe entailment relations between predicates by a graph. Nodes represent predicates, and edges represent entailment between nodes. Nevertheless, using a graph for global learning of entailment between predicates has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-mapped WordNet, providing entailment relations for predicates in WordNet. Their resource was built on top of WordNet, and makes simple use of WordNet’s global graph structure: new rules are suggested by transitively chaining graph edges, and verified against corpus statistics. The most similar work to ours is Snow et al.’s algorithm for taxonomy induction (2006). Snow et al.’s algorithm learns the hyponymy relation, under the constraint that it is a transitive relation. Their algorithm incrementally adds hyponyms to an existing taxonomy (WordNet), using a greed"
P10-1124,W04-3206,1,0.853995,"an (2008) suggested learning over templates with subj one variable (unary templates) such as X ←−− affect, and using them to estimate a score for binary templates. Feature representation The features of a template are some representation of the terms that instantiated the argument variables in a corpus. Two representations are used in our experiment (see Section 6). Another variant occurs when using binary templates: a template may be represented by a pair of feature vectors, one for each variable (Lin and Pantel, 2001), or by a single vector, where features represent pairs of instantiations (Szpektor et al., 2004; Yates and Etzioni, 2009). The former variant reduces sparsity problems, while Yates and Etzioni showed the latter is more informative and performs favorably on their data. Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). The former is a symmetric measure and the latter is asymmetric. Therefore, information about the direction of entailment is provided by the BInc measure. We then generate for any (t1 , t2 ) features that are the 12 distributional similarity scores usin"
P10-1124,W07-1401,1,\N,Missing
P10-2045,P98-1013,0,0.228314,"the lack of such knowledge (Bar-Haim et al., 2006; Giampiccolo et al., 2007). We address one prominent type of inference knowledge known as entailment rules, focusing specifically on rules between predicates, such as ‘cure X ⇒ X recover’. We aim at highly accurate rule acquisition, for which utilizing manually constructed sources seem appropriate. The most widely used manual resource is WordNet (Fellbaum, 1998). Yet it is incomplete for generating entailment rules between predicates (Section 2.1). Hence, other manual resources should also be targeted. In this work1 , we explore how FrameNet (Baker et al., 1998) could be effectively used for generating entailment rules between predicates. 2 2.1 Background Entailment Rules and their Acquisition To generate entailment rules, two issues should be addressed: a) identifying the lexical entailment relations between predicates, e.g. ‘cure ⇒ recover’; b) mapping argument positions, e.g. ‘cure X ⇒ X recover’. The main approach for generating highly accurate rule-sets is to use manually constructed resources. To this end, most systems mainly utilize WordNet (Fellbaum, 1998), being the most prominent lexical resource with broad coverage of predicates. Furthermo"
P10-2045,P10-2045,1,0.106868,"s. 2 2.1 Background Entailment Rules and their Acquisition To generate entailment rules, two issues should be addressed: a) identifying the lexical entailment relations between predicates, e.g. ‘cure ⇒ recover’; b) mapping argument positions, e.g. ‘cure X ⇒ X recover’. The main approach for generating highly accurate rule-sets is to use manually constructed resources. To this end, most systems mainly utilize WordNet (Fellbaum, 1998), being the most prominent lexical resource with broad coverage of predicates. Furthermore, some of its 1 The detailed description of our work can be found in (Ben Aharon, 2010). 2 The rule-set is available at: http://www.cs.biu. ac.il/˜nlp/downloads 241 Proceedings of the ACL 2010 Conference Short Papers, pages 241–246, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics relations capture types of entailment relations, including synonymy, hypernymy, morphologicallyderived, entailment and cause. Yet, WordNet is limited for entailment rule generation. First, many entailment relations, notably for the WordNet entailment and cause relation types, are missing, e.g. ‘elect ⇒ vote’. Furthermore, WordNet does not include argument mapping betwe"
P10-2045,W07-1401,1,0.719505,"hat the novel resource is effective and complements WordNet in terms of rule coverage. 1 Introduction Many text understanding applications, such as Question Answering (QA) and Information Extraction (IE), need to infer a target textual meaning from other texts. This need was proposed as a generic semantic inference task under the Textual Entailment (TE) paradigm (Dagan et al., 2006). A fundamental component in semantic inference is the utilization of knowledge resources. However, a major obstacle to improving semantic inference performance is the lack of such knowledge (Bar-Haim et al., 2006; Giampiccolo et al., 2007). We address one prominent type of inference knowledge known as entailment rules, focusing specifically on rules between predicates, such as ‘cure X ⇒ X recover’. We aim at highly accurate rule acquisition, for which utilizing manually constructed sources seem appropriate. The most widely used manual resource is WordNet (Fellbaum, 1998). Yet it is incomplete for generating entailment rules between predicates (Section 2.1). Hence, other manual resources should also be targeted. In this work1 , we explore how FrameNet (Baker et al., 1998) could be effectively used for generating entailment rules"
P10-2045,N03-1013,0,0.264629,"LexPar first identifies lexical entailment relations by going over all LU pairs which are either in the 242 3.2 FrameNet groups LUs in frames and describes relations between frames. However, relations between LUs are not explicitly defined. We next describe how we automatically extract several types of lexical entailment relations between LUs using two approaches. In the first approach, LUs in the same frame that are morphological derivations of each other, e.g. ‘negotiation.n’ and ‘negotiate.v’, are marked as paraphrases. We take morphological derivation information from the CATVAR database (Habash and Dorr, 2003). The second approach is based on our observation that some LUs express the prototypical situation that their frame describes, which we denote dominant LUs. For example, the LU ‘recover’ is dominant for the Recovery frame. We mark LUs as dominant if they are morphologically derived from the frame’s name. Our assumption is that since dominant LUs express the frame’s generic meaning, their meaning is likely to be entailed by the other LUs in this frame. Consequently, we generate such lexical rules between any dominant LU and any other LU in a given frame, e.g. ‘heal ⇒ recover’ and ‘convalescence"
P10-2045,C02-1167,0,0.150742,"Missing"
P10-2045,C08-1107,1,0.578268,"use and Perspective relations, where Inheritance and Cause generate directional entailment relations (e.g. ‘choose ⇒ decide’ and ‘cure ⇒ recover’, respectively) while Perspective generates bidirectional paraphrase relations (e.g. ‘transfer ⇔ receive’). Finally, we generate the transitive closure of the set of lexical relations identified by the above methods. For example, the combination of ‘sell ⇔ buy’ and ‘buy ⇒ get’ generates ‘sell ⇒ get’. Figure 1: Template extraction for a sentence containing the LU ‘arrest’. gument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). Figure 1 exemplifies this process. As a pre-parsing step, all FE phrases in a given sentence are replaced by their related FE names, excluding syntactic information such as prepositions or possessives (step (b) in Figure 1). Then, the sentence is parsed using the Minipar dependency parser (Lin, 1998) (step (c)). Finally, a path in the parse-tree is extracted between each FE node and the node of the LU (step (d)). Each extracted path is converted into a template by replacing the FE node with an argument variable. We simplify each extracted path by removing nodes along the path that are not pa"
P10-2045,W09-2504,1,0.759731,"templates and lexical entailment relations. For each identified lexical relation ‘left ⇒ right’ between two LUs, the set of FEs that are shared by both LUs is collected. Then, for each shared FE, we take the list of templates that connect this FE obj (e.g. ‘Suspect ←− arrest’ is a correct template, in obj contrast to ‘Charges ←− arrest’). We thus keep only the most frequently annotated template out of the identical templates, assuming it is the correct one. 243 in practice, as well as to compare its performance to related resources. To this end, we follow the experimental setup presented in (Szpektor and Dagan, 2009), which utilized the ACE 2005 event dataset3 as a testbed for entailment rule-sets. We briefly describe this setup here. The task is to extract argument mentions for 26 events, such as Sue and Attack, from the ACE annotated corpus, using a given tested entailment rule-set. Each event is represented by a set of unary seed templates, one for each event argument. Some seed templates for Attack are ‘AtLexical Relation: cure ⇒ recovery Templates: obj P atient ←− cure of Af f liction ←− cure gen P atient ←− recovery of P atient ←− recovery f rom Af f liction ←− recovery (cure Patient) (cure of Affli"
P10-2045,C98-1013,0,\N,Missing
P10-2045,2003.mtsummit-systems.9,0,\N,Missing
P10-2045,W07-1400,0,\N,Missing
P11-1062,P98-1013,0,0.107617,"Missing"
P11-1062,P10-2045,1,0.863712,"hs, resulting in improved coverage. 2 Background Most work on learning entailment rules between predicates considered each rule independently of others, using two sources of information: lexicographic resources and distributional similarity. Lexicographic resources are manually-prepared knowledge bases containing semantic information on predicates. A widely-used resource is WordNet (Fellbaum, 1998), where relations such as synonymy and hyponymy can be used to generate rules. Other resources include NomLex (Macleod et al., 1998; Szpektor and Dagan, 2009) and FrameNet (Baker and Lowe, 1998; Ben Aharon et al., 2010). Lexicographic resources are accurate but have 1 The resource can be downloaded from http://www.cs.tau.ac.il/˜jonatha6/homepage files/resources /ACL2011Resource.zip 611 low coverage. Distributional similarity algorithms use large corpora to learn broader resources by assuming that semantically similar predicates appear with similar arguments. These algorithms usually represent a predicate with one or more vectors and use some function to compute argument similarity. Distributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is"
P11-1062,P10-1124,1,0.843876,"rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transitivity does not always hold when predicates are ambiguous, e.g., X buy Y → X acquire Y and X acquire Y → X learn Y, but X buy Y 9 X learn Y since these two rules correspond to two different senses of acquire. The second challenge is scalability: ILP solvers do not scale well since ILP is an NP-complete problem. Berant et al. circumvented these issues by l"
P11-1062,D07-1017,0,0.179692,"9). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hearst-patterns (Hearst, 1992) to extract a set of 29 million (argument, type) pairs from a large web crawl. Then, they employed several filtering methods to clean this set and automatically produced a mapping of 1.1 million arguments into 156 type"
P11-1062,D10-1107,0,0.0492637,"tion Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can only exist between predicates that share the same (unordered) pair of types (such as place and country)3 . Hence, every pair of types defines a graph that describes the entailment relations between predicates sharing those types (Figure 1). Next, we show how to represent entailment rules between typed predicates in a structure termed typed entailment graph, which will be the learning goal of our algorithm. A typed entailment graph is a directed graph where the nodes are typed predicates. A typed predicate is a tr"
P11-1062,P08-2012,0,0.113385,"city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we will learn entailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Mart"
P11-1062,C92-2082,0,0.136804,"lgorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hearst-patterns (Hearst, 1992) to extract a set of 29 million (argument, type) pairs from a large web crawl. Then, they employed several filtering methods to clean this set and automatically produced a mapping of 1.1 million arguments into 156 types. Examples for (argument, type) pairs are (EXODUS, book), (CHINA, country) and (ASTHMA, disease). Schoenmackers et al. then utilized the types, the mapped arguments and tuples from TextRunner (Banko et al., 2007) to generate 10,672 typed predicates (such as conquer(country,city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we wi"
P11-1062,P09-1039,0,0.017584,"008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can only exist between predicates that share the same (unordered) pair of types (such as place and country)3 . Hence, every pair of types defines a graph that describes the entailment relations between predicates sharing those types (Figure 1). Next, we show how to represent entailment rules between typed predicates in a structure termed typed entailment graph, which will be the learning goal of our algorithm. A typed entailment graph is a directed graph where the nodes are typed predicates. A typ"
P11-1062,P10-1031,0,0.0074745,"ailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensively used in NLP lately (Clarke and Lapata, 2008; Martins et al., 2009; Do and Roth, 2010). 3 Typed Entailment Graphs Given a set of typed predicates, entailment rules can"
P11-1062,W06-1616,0,0.0298882,"ACT ← ACT ∪ VIO 6: until |VIO |= 0 how typed entailment graphs benefit from decomposition given different prior values. From a more general perspective, this algorithm can be applied to any problem of learning a sparse transitive binary relation. Such problems include Co-reference Resolution (Finkel and Manning, 2008) and Temporal Information Extraction (Ling and Weld, 2010). Last, the algorithm can be easily parallelized by solving each component on a different core. 4.4 Incremental ILP Another solution for scaling ILP is to employ incremental ILP, which has been used in dependency parsing (Riedel and Clarke, 2006). The idea is that even if we omit the transitivity constraints, we still expect most transitivity constraints to be satisfied, given a good local entailment classifier. Thus, it makes sense to avoid specifying the constraints ahead of time, but rather add them when they are violated. This is formalized in Algorithm 2. Line 1 initializes an active set of constraints and a violated set of constraints (ACT;VIO). Line 3 applies the ILP solver with the active constraints. Lines 4 and 5 find the violated constraints and add them to the active constraints. The algorithm halts when no constraints are"
P11-1062,D10-1106,0,0.263522,"l jonatha6@post.tau.ac.il dagan@cs.biu.ac.il goldbej@eng.biu.ac.il Abstract Extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference. In this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates. We model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs. We apply the algorithm over a large data set of extracted predicate instances, from which a resource of typed entailment rules has been recently released (Schoenmackers et al., 2010). Our results show that using global transitivity information substantially improves performance over this resource and several baselines, and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs. 1 Introduction Generic approaches for applied semantic inference from text gained growing attention in recent years, particularly under the Textual Entailment (TE) framework (Dagan et al., 2009). TE is a generic paradigm for semantic inference, where the objective is to recognize whether a target meaning can be inferred from a given text. A crucial comp"
P11-1062,I05-5011,0,0.106093,"d from a given text. A crucial component of inference systems is extensive resources of entailment rules, also known as inference rules, i.e., rules that specify a directional inference relation between fragments of text. One important type of rule is rules that specify entailment relations between predicates and their arguments. For example, the rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transi"
P11-1062,P06-1101,0,0.456438,"72 typed predicates (such as conquer(country,city) and common in(disease,place)), and learn 30,000 rules between these predicates2 . In this paper we will learn entailment rules over the same data set, which was generously provided by 2 The rules and the mapping of arguments into types can be downloaded from http://www.cs.washington.edu/research/ sherlock-hornclauses/ Schoenmackers et al. As mentioned above, Berant et al. (2010) used global transitivity information to learn small entailment graphs. Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al., 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). Our proposed algorithm applies to any sparse transitive relation, and so might be applicable in these fields as well. Last, we formulate our optimization problem as an Integer Linear Program (ILP). ILP is an optimization problem where a linear objective function over a set of integer variables is maximized under a set of linear constraints. Scaling ILP is challenging since it is an NP-complete problem. ILP has been extensive"
P11-1062,C08-1107,1,0.814176,"stributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is represented by one feature vector where each feature is a pair of arguments (Szpektor et al., 2004; Yates and Etzioni, 2009). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more suited for entailment (Bhagat et al., 2007). In this paper, We employ several such functions, such as Lin (Lin and Pantel, 2001), and BInc (Szpektor and Dagan, 2008). Schoenmackers et al. (2010) recently used distributional similarity to learn rules between typed predicates, where the left-hand-side of the rule may contain more than a single predicate (horn clauses). In their work, they used Hear"
P11-1062,W09-2504,1,0.938877,"text. A crucial component of inference systems is extensive resources of entailment rules, also known as inference rules, i.e., rules that specify a directional inference relation between fragments of text. One important type of rule is rules that specify entailment relations between predicates and their arguments. For example, the rule ‘X annex Y → X control Y’ helps recognize that the text ‘Japan annexed Okinawa’ answers the question ‘Which country controls Okinawa?’. Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010). Most past work took a “local learning” approach, learning each entailment rule independently of others. It is clear though, that there are global interactions between predicates. Notably, entailment is a transitive relation and so the rules A → B and B → C imply A → C. Recently, Berant et al. (2010) proposed a global graph optimization procedure that uses Integer Linear Programming (ILP) to find the best set of entailment rules under a transitivity constraint. Imposing this constraint raised two challenges. The first of ambiguity: transitivity does not always hol"
P11-1062,W04-3206,1,0.657482,"ed from http://www.cs.tau.ac.il/˜jonatha6/homepage files/resources /ACL2011Resource.zip 611 low coverage. Distributional similarity algorithms use large corpora to learn broader resources by assuming that semantically similar predicates appear with similar arguments. These algorithms usually represent a predicate with one or more vectors and use some function to compute argument similarity. Distributional similarity algorithms differ in their feature representation: Some use a binary representation: each predicate is represented by one feature vector where each feature is a pair of arguments (Szpektor et al., 2004; Yates and Etzioni, 2009). This representation performs well, but suffers when data is sparse. The binary-DIRT representation deals with sparsity by representing a predicate with a pair of vectors, one for each argument (Lin and Pantel, 2001). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). Different algorithms also differ in their similarity function. Some employ symmetric functions, geared towards paraphrasing (bi-directional entailment), while others choose directional measures more"
P11-1062,W03-1011,0,0.555143,"y score estimating whether p1 entails p2 . We compute 11 distributional similarity scores for each pair of predicates based on the arguments appearing in the extracted arguments. The first 6 scores are computed by trying all combinations of the similarity functions Lin and BInc with the feature representations unary, binary-DIRT and binary (see Section 2). The other 5 scores were provided by Schoenmackers et al. (2010) and include SR (Schoenmackers et al., 2010), LIME (McCreath and Sharma, 1997), M-estimate (Dzeroski and Brakto, 1992), the standard G-test and a simple implementation of Cover (Weeds and Weir, 2003). Overall, the rationale behind this representation is that combining various scores will yield a better classifier than each single measure. 3) Training We train over an equal number of positive and negative examples, as classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse et al., 2007; Nikulin, 2008). 4.2 ILP formulation Once the classifier is trained, we would like to learn all edges (entailment rules) of each typed entailment graph. Given a set of predicates V and an entailment score function f : V × V → R derived from the classifier, we want"
P11-1062,C98-1013,0,\N,Missing
P11-2098,J93-2003,0,0.0134296,"shortpapers, pages 558–563, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An additional observation from these and other systems is that their performance improves only moderately when utilizing lexical resources2 . We believe that the textual entailment field would benefit from more principled models for various entailment phenomena. Inspired by the earlier steps in the evolution of Statistical Machine Translation methods (such as the initial IBM models (Brown et al., 1993)), we formulate a concrete generative probabilistic modeling framework that captures the basic aspects of lexical entailment. Parameter estimation is addressed by an EM-based approach, which enables estimating the hidden lexical-level entailment parameters from entailment annotations which are available only at the sentence-level. While heuristic methods are limited in their ability to wisely integrate indications for entailment, probabilistic methods have the advantage of being extendable and enabling the utilization of wellfounded probabilistic methods such as the EM algorithm. We compared t"
P11-2098,W07-1402,0,0.0683995,"ed as a generic paradigm for applied semantic inference (Dagan et al., 2006). This task requires deciding whether a textual statement (termed the hypothesis-H) can be inferred (entailed) from another text (termed the textT ). Since it was first introduced, the six rounds of the Recognizing Textual Entailment (RTE) challenges1 , currently organized under NIST, have become a standard benchmark for entailment systems. These systems tackle their complex task at various levels of inference, including logical representation (Tatu and Moldovan, 2007; MacCartney and Manning, 2007), semantic analysis (Burchardt et al., 2007) and syntactic parsing (Bar-Haim et al., 2008; Wang et al., 2009). Inference at these levels usually 1 http://www.nist.gov/tac/2010/RTE/index.html requires substantial processing and resources (e.g. parsing) aiming at high performance. Nevertheless, simple entailment methods, performing at the lexical level, provide strong baselines which most systems did not outperform (Mirkin et al., 2009; Majumdar and Bhattacharyya, 2010). Within complex systems, lexical entailment modeling is an important component. Finally, there are cases in which a full system cannot be used (e.g. lacking a parser for a"
P11-2098,W05-1203,0,0.0298191,"by the text’s terms. Coverage is determined either by a direct match of identical terms in T and H or by utilizing lexical semantic resources, such as WordNet (Fellbaum, 1998), that capture lexical entailment relations (denoted here as entailment rules). Common heuristics for quantifying the degree of coverage are setting a threshold on the percentage coverage of H’s terms (Majumdar and Bhattacharyya, 2010), counting absolute number of uncovered terms (Clark and Harrison, 2010), or applying an Information Retrievalstyle vector space similarity score (MacKinlay and Baldwin, 2009). Other works (Corley and Mihalcea, 2005; Zanzotto and Moschitti, 2006) have applied a heuristic formula to estimate the similarity between text fragments based on a similarity function between their terms. These heuristics do not capture several important aspects of entailment, such as varying reliability of 558 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 558–563, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An additional observati"
P11-2098,W06-1621,1,0.909279,"dge Resources#Ablation Tests 559 Text: ... t1 Resource2 tj chain t’ h1 ... tn MATCH Resource1 Resource3 Resource1 Hypothesis: ... hi ... hm Figure 1: The generative process of entailing terms of a hypothesis from a text. Edges represent entailment rules. There are 3 evidences for the entailment of hi : a rule from Resource1 , another one from Resource3 both suggesting that tj entails it, and a chain from t1 through an intermediate term t0 . 2.1 Model Description For T to entail H it is usually a necessary, but not sufficient, that every term h ∈ H would be entailed by at least one term t ∈ T (Glickman et al., 2006). Figure 1 describes the process of entailing hypothesis terms. The trivial case is when identical terms, possibly at the stem or lemma level, appear in T and H (a direct match as tn and hm in Figure 1). Alternatively, we can establish entailment based on knowledge of entailing lexical-semantic relations, such as synonyms, hypernyms and morphological derivations, available in lexical resources (e.g the rule inference → reasoning from WordNet). We denote by R(r) the resource which provided the rule r. Since entailment is a transitive relation, rules may compose transitive chains that connect a"
P11-2098,N03-1013,0,0.422294,"Missing"
P11-2098,W07-1431,0,0.0431268,"ction and Background Textual Entailment was proposed as a generic paradigm for applied semantic inference (Dagan et al., 2006). This task requires deciding whether a textual statement (termed the hypothesis-H) can be inferred (entailed) from another text (termed the textT ). Since it was first introduced, the six rounds of the Recognizing Textual Entailment (RTE) challenges1 , currently organized under NIST, have become a standard benchmark for entailment systems. These systems tackle their complex task at various levels of inference, including logical representation (Tatu and Moldovan, 2007; MacCartney and Manning, 2007), semantic analysis (Burchardt et al., 2007) and syntactic parsing (Bar-Haim et al., 2008; Wang et al., 2009). Inference at these levels usually 1 http://www.nist.gov/tac/2010/RTE/index.html requires substantial processing and resources (e.g. parsing) aiming at high performance. Nevertheless, simple entailment methods, performing at the lexical level, provide strong baselines which most systems did not outperform (Mirkin et al., 2009; Majumdar and Bhattacharyya, 2010). Within complex systems, lexical entailment modeling is an important component. Finally, there are cases in which a full system"
P11-2098,W07-1404,0,0.0696617,"e improvements. 1 Introduction and Background Textual Entailment was proposed as a generic paradigm for applied semantic inference (Dagan et al., 2006). This task requires deciding whether a textual statement (termed the hypothesis-H) can be inferred (entailed) from another text (termed the textT ). Since it was first introduced, the six rounds of the Recognizing Textual Entailment (RTE) challenges1 , currently organized under NIST, have become a standard benchmark for entailment systems. These systems tackle their complex task at various levels of inference, including logical representation (Tatu and Moldovan, 2007; MacCartney and Manning, 2007), semantic analysis (Burchardt et al., 2007) and syntactic parsing (Bar-Haim et al., 2008; Wang et al., 2009). Inference at these levels usually 1 http://www.nist.gov/tac/2010/RTE/index.html requires substantial processing and resources (e.g. parsing) aiming at high performance. Nevertheless, simple entailment methods, performing at the lexical level, provide strong baselines which most systems did not outperform (Mirkin et al., 2009; Majumdar and Bhattacharyya, 2010). Within complex systems, lexical entailment modeling is an important component. Finally, there a"
P11-2098,P06-1051,0,0.0761637,"ge is determined either by a direct match of identical terms in T and H or by utilizing lexical semantic resources, such as WordNet (Fellbaum, 1998), that capture lexical entailment relations (denoted here as entailment rules). Common heuristics for quantifying the degree of coverage are setting a threshold on the percentage coverage of H’s terms (Majumdar and Bhattacharyya, 2010), counting absolute number of uncovered terms (Clark and Harrison, 2010), or applying an Information Retrievalstyle vector space similarity score (MacKinlay and Baldwin, 2009). Other works (Corley and Mihalcea, 2005; Zanzotto and Moschitti, 2006) have applied a heuristic formula to estimate the similarity between text fragments based on a similarity function between their terms. These heuristics do not capture several important aspects of entailment, such as varying reliability of 558 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 558–563, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An additional observation from these and other systems"
P11-2098,W07-1401,1,\N,Missing
P11-2098,2003.mtsummit-systems.9,0,\N,Missing
P11-2098,W07-1400,0,\N,Missing
P12-1013,P10-2045,1,0.884125,"h may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over distributional similarity features. A positive wij indicate"
P12-1013,P10-1124,1,0.934057,"prominent generic paradigm for textual inference is Textual Entailment (TUE) (Dagan et al., 2009). In TUE, the goal is to recognize, given two text fragments termed text and hypothesis, whether the hypothesis can be inferred from the text. For example, the text “Cyprus was invaded by the Ottoman Empire in 1571” implies the hypothesis “The Ottomans attacked Cyprus”. Semantic inference applications such as QA and IE crucially rely on entailment rules (Ravichandran Textual entailment is inherently a transitive relation , that is, the rules ‘x → y’ and ‘y → z’ imply the rule ‘x → z’. Accordingly, Berant et al. (2010) formulated the problem of learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity. Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution. Indeed, they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connec"
P12-1013,P11-1062,1,0.797113,"ation , that is, the rules ‘x → y’ and ‘y → z’ imply the rule ‘x → z’. Accordingly, Berant et al. (2010) formulated the problem of learning entailment rules as a graph optimization problem, where nodes are predicates and edges represent entailment rules that respect transitivity. Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution. Indeed, they showed that applying global transitivity constraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connected components and then applies an ILP solver over each component. Despite this progress, finding the exact solution remains NP-hard – the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited. Thus, scaling their algorithm to data sets with tens of thousands of predicates (e.g., the extractions of Fader et al. (2011)) is unlikely. 117 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages"
P12-1013,W04-3205,0,0.194733,"rs and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over distributional similarity features. A positive wij indicated that the classifier believes i → j and a negative wij indicated that the classifier believes i 9 j. Given the graph nodes V (corresponding to the predicates) and the weighting func"
P12-1013,D10-1107,0,0.0548624,"nd applies an ILP solver on each component separately using a cutting-plane procedure (Riedel and Clarke, 2006). Although this method is exact and improves scalability, it does not guarantee an efficient solution. When the graph does not decompose into sufficiently small components, and the weights generate many violations of transitivity, solving Max-Trans-Graph becomes intractable. To address this problem, we present in this paper a method for approximating the optimal set of edges within each component and show that it is much more efficient and scalable both theoretically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a pair of terms, a small graph is constructed and constraints are imposed on the graph structure. Their work, however, is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is explicitly constructed. Thus, their method easily produces solutions where global constraints, such as transitivity, are violated. Another approximation method that violates transitivity constraints is LP relaxation (Martins et al., 2009). In LP relaxation, the"
P12-1013,D11-1142,0,0.0406764,"nstraints improves rule learning comparing to methods that ignore graph structure. More recently, Berant et al. (Berant et al., 2011) introduced a more efficient exact algorithm, which decomposes the graph into connected components and then applies an ILP solver over each component. Despite this progress, finding the exact solution remains NP-hard – the authors themselves report they were unable to solve some graphs of rather moderate size and that the coverage of their method is limited. Thus, scaling their algorithm to data sets with tens of thousands of predicates (e.g., the extractions of Fader et al. (2011)) is unlikely. 117 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117–125, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics In this paper we present a novel method for learning the edges of entailment graphs. Our method computes much more efficiently an approximate solution that is empirically almost as good as the exact solution. To that end, we first (Section 3) conjecture and empirically show that entailment graphs exhibit a “tree-like” property, i.e., that they can be reduced into a structure similar t"
P12-1013,P08-2012,0,0.0309496,"Linear Program (LP), which is polynomial. An LP solver is then applied on the problem, and variables xij that are assigned a fractional value are rounded to their nearest integer and so many violations of transitivity easily occur. The solution when applying LP relaxation is not a transitive graph, but nevertheless we show for comparison in Section 5 that our method is much faster. Last, we note that transitive relations have been explored in adjacent fields such as Temporal Information Extraction (Ling and Weld, 2010), Ontology Induction (Poon and Domingos, 2010), and Coreference Resolution (Finkel and Manning, 2008). 3 Forest-reducible Graphs The entailment relation, described by entailment graphs, is typically from a “semantically-specific” predicate to a more “general” one. Thus, intuitively, the topology of an entailment graph is expected to be “tree-like”. In this section we first formalize this intuition and then empirically analyze its validity. This property of entailment graphs is an interesting topological observation on its own, but also enables the efficient approximation algorithm of Section 4. For a directed edge i → j in a directed acyclic graphs (DAG), we term the node i a child of node j,"
P12-1013,P09-1039,0,0.0330329,"etically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a pair of terms, a small graph is constructed and constraints are imposed on the graph structure. Their work, however, is geared towards scenarios where relations are determined on-the-fly for a given pair of terms and no global knowledge base is explicitly constructed. Thus, their method easily produces solutions where global constraints, such as transitivity, are violated. Another approximation method that violates transitivity constraints is LP relaxation (Martins et al., 2009). In LP relaxation, the constraint xij ∈ {0, 1} is replaced by 0 ≤ xij ≤ 1, transforming the problem from an ILP to a Linear Program (LP), which is polynomial. An LP solver is then applied on the problem, and variables xij that are assigned a fractional value are rounded to their nearest integer and so many violations of transitivity easily occur. The solution when applying LP relaxation is not a transitive graph, but nevertheless we show for comparison in Section 5 that our method is much faster. Last, we note that transitive relations have been explored in adjacent fields such as Temporal In"
P12-1013,P10-1031,0,0.0861357,"is not guaranteed, the area under the precision-recall curve drops by merely a point. To conclude, the contribution of this paper is twofold: First, we define a novel modeling assumption about the tree-like structure of entailment graphs and demonstrate its validity. Second, we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob"
P12-1013,P02-1006,0,0.287434,"Missing"
P12-1013,W06-1616,0,0.0335633,"and the constraint xij + xjk − xik ≤ 1 on the binary variables enforces that whenever xij = xjk = 1, then also xik = 1 (transitivity). Since ILP is NP-hard, applying an ILP solver directly does not scale well because the number of variables is O(|V |2 ) and the number of constraints is O(|V |3 ). Thus, even a graph with ∼80 nodes (predicates) has more than half a million constraints. Consequently, in (Berant et al., 2011), they proposed a method that efficiently decomposes the graph into smaller components and applies an ILP solver on each component separately using a cutting-plane procedure (Riedel and Clarke, 2006). Although this method is exact and improves scalability, it does not guarantee an efficient solution. When the graph does not decompose into sufficiently small components, and the weights generate many violations of transitivity, solving Max-Trans-Graph becomes intractable. To address this problem, we present in this paper a method for approximating the optimal set of edges within each component and show that it is much more efficient and scalable both theoretically and empirically. Do and Roth (2010) suggested a method for a related task of learning taxonomic relations between terms. Given a"
P12-1013,D10-1106,0,0.463293,"a6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration takes linear time. We compare o"
P12-1013,I05-5011,0,0.0553455,"Engineering, Bar-Ilan University jonatha6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edge"
P12-1013,N06-1039,0,0.114428,"Missing"
P12-1013,P06-1101,0,0.0775781,"rithm, and that though an optimal solution is not guaranteed, the area under the precision-recall curve drops by merely a point. To conclude, the contribution of this paper is twofold: First, we define a novel modeling assumption about the tree-like structure of entailment graphs and demonstrate its validity. Second, we exploit this assumption to develop a polynomial approximation algorithm for learning entailment graphs that can scale to much larger graphs than in the past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as Wor"
P12-1013,C08-1107,1,0.906093,"Bar-Ilan University jonatha6@post.tau.ac.il {dagan,goldbej}@{cs,eng}.biu.ac.il adlerm@cs.bgu.ac.il Abstract and Hovy, 2002; Shinyama and Sekine, 2006) or equivalently inference rules, that is, rules that describe a directional inference relation between two fragments of text. An important type of entailment rule specifies the entailment relation between natural language predicates, e.g., the entailment rule ‘X invade Y → X attack Y’ can be helpful in inferring the aforementioned hypothesis. Consequently, substantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010). Learning entailment rules is fundamental in many semantic-inference applications and has been an active field of research in recent years. In this paper we address the problem of learning transitive graphs that describe entailment rules between predicates (termed entailment graphs). We first identify that entailment graphs exhibit a “tree-like” property and are very similar to a novel type of graph termed forest-reducible graph. We utilize this property to develop an iterative efficient approximation algorithm for learning the graph edges, where each iteration ta"
P12-1013,W09-2504,1,0.89564,"ntology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent entailment rules between them (‘X invade Y → X attack Y’). For every pair of predicates i, j, an entailment score wij was learned by training a classifier over"
P12-1013,W04-3206,1,0.826086,"e past. Finally, we note that learning entailment graphs bears strong similarities to related tasks such as Taxonomy Induction (Snow et al., 2006) and Ontology induction (Poon and Domingos, 2010), and thus our approach may improve scalability in these fields as well. 2 Background Until recently, work on learning entailment rules between predicates considered each rule independently of others and did not exploit global dependencies. Most methods utilized the distributional similarity hypothesis that states that semantically similar predicates occur with similar arguments (Lin and Pantel, 2001; Szpektor et al., 2004; Yates and Etzioni, 2009; Schoenmackers et al., 2010). Some methods extracted rules from lexicographic resources such as WordNet (Szpektor and Dagan, 2009) or FrameNet (Bob and Rambow, 2009; Ben Aharon et al., 2010), and others assumed that semantic relations between predicates can be deduced from their co-occurrence in a corpus via manually-constructed 118 patterns (Chklovski and Pantel, 2004). Recently, Berant et al. (2010; 2011) formulated the problem as the problem of learning global entailment graphs. In entailment graphs, nodes are predicates (e.g., ‘X attack Y’) and edges represent ent"
P12-1030,D09-1120,0,0.043786,"gorithms, described in Table 2 above, as well as B IU T EE-orig. The latter is a strong baseline, which outperforms known search algorithms in generating low cost proofs. We compared all the above mentioned algorithms to our novel one, LLGS. We used the training dataset for parameter tuning, which controls the trade-off between speed and quality. For weighted A*, as well as for greedy search, we used w = 6.0, since, for a few instances, lower values of w resulted in prohibitive runtime. For beam search we used k = 150, since higher val3 http://lucene.apache.org www.ark.cs.cmu.edu/ARKref/ See (Haghighi and Klein, 2009) 4 289 ues of k did not improve the proof cost on the training dataset. The value of d in LLGS was set to 3. d = 4 yielded the same proof costs, but was about 3 times slower. Since lower values of w could be used by weighted A* for most instances, we also ran experiments where we varied the value of w according to the dovetailing method suggested in (Valenzano et al., 2010) (denoted dovetailing WA*) as follows. When weighted A* has found a solution, we reran it with a new value of w, set to half of the previous value. The idea is to guide the search for lower cost solutions. This process was h"
P12-1030,N10-1145,0,0.26675,"nswer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” → “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” → “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve inference validity, such that"
P12-1030,P09-2073,0,0.400523,"ext passages from which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” → “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” → “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation s"
P12-1030,R11-1063,1,0.836286,"generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” → “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” → “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve inference validity, such that each text generated alo"
P12-1030,W04-3206,1,0.702217,") suggested a heuristic set of 28 transformations, which include various types of node-substitutions as well as restructuring of the entire parse-tree. In contrast to such predefined sets of transformations, knowledge oriented approaches were suggested by Bar-Haim et al. (2007) and de Salvo Braz et al. (2005). Their transformations are defined by knowledge resources that contain a large amount of entailment rules, or rewrite rules, which are pairs of parse-tree fragments that entail one another. Typical examples for knowledge resources of such rules are DIRT (Lin and Pantel, 2001), and TEASE (Szpektor et al., 2004), as well as syntactic transformations constructed manually. In addition, they used knowledge-based lexical substitutions. However, when only knowledge-based transformations are allowed, transforming the text into the hypothesis is impossible in many cases. This limitation is dealt by our open-source integrated framework, B IU T EE (Stern and Dagan, 2011), which incorporates knowledge-based transformations (entailment rules) with a set of predefined tree-edits. Motivated by the richer structure and search space provided by B IU T EE, we adopted it for our empirical investigations. The semantic"
P12-1030,C10-1131,0,0.473352,"rom which a satisfying answer can be inferred. A generic formulation for the inference relation between two texts is given by the Recognizing Textual Entailment (RTE) paradigm (Dagan et al., 2005), which is adapted here for our investigation. In this setting, a system is given two text fragments, termed “text” (T) and “hypothesis” (H), and has to recognize whether the hypothesis is entailed by (inferred from) the text. An appealing approach to such textual inferences is to explicitly transform T into H, using a sequence of transformations (Bar-Haim et al., 2007; Harmeling, 2009; Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Stern and Dagan, 2011). Examples of such possible transformations are lexical substitutions (e.g. “letter” → “message”) and predicate-template substitutions (e.g. “X [verbactive] Y” → “Y [verb-passive] by X”), which are based on available knowledge resources. Another example is coreference substitutions, such as replacing “he” with “the employee” if a coreference resolver has detected that these two expressions corefer. Table 1 exemplifies this approach for a particular T-H pair. The rationale behind this approach is that each transformation step should preserve infe"
P12-1030,W07-1401,1,\N,Missing
P12-2031,P10-1124,1,0.824564,"Missing"
P12-2031,D07-1017,0,0.101112,"Missing"
P12-2031,D11-1142,0,0.0413378,"Missing"
P12-2031,P11-2057,0,0.0181794,"ematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association"
P12-2031,N10-1045,0,0.012713,"application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles tha"
P12-2031,D11-1062,0,0.0204538,"in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an application and yields high inter-annotator agreement. Unfortunately, their method requires lengthy guidelines and substantial annotator training effort, which are time consuming and costly. Thus, a simple, robust and replicable evaluation method is needed. Recently, crowdsourcing services such as Amazon Mechanical Turk (AMT) and CrowdFlower (CF)1 have been employed for semantic inference annotation (Snow et al., 2008; Wang and CallisonBurch, 2010; Mehdad et al., 2010; Negri et al., 2011). These works focused on generating and annotating RTE text-hypothesis pairs, but did not address annotation and evaluation of inference rules. In this paper, we propose a novel instance-based evaluation framework for inference rules that takes advantage of crowdsourcing. Our method substantially simplifies annotation of rule applications and avoids annotator training completely. The novelty in our framework is two-fold: (1) We simplify instance-based evaluation from a complex decision scenario to two independent binary decisions. (2) We apply methodological principles that efficiently communi"
P12-2031,P02-1006,0,0.0781013,"a non-trivial task, slowing progress in the field. In this paper, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms"
P12-2031,P10-1122,0,0.0283976,"wever, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14"
P12-2031,D10-1106,0,0.0624181,"ectional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer. However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablati"
P12-2031,I05-5011,0,0.0849676,"scribing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), One option for evaluating inference rule resources is to measure their impact on an end task, as that is what ultimately interests an inference system developer. However, this is often problematic since inference systems have many components that address multiple phenomena, and thus it is hard to assess the effect of a single resource. An example is the Recognizing Textual Entailment (RTE) framework (Dagan et al., 2009), in which given a text T and a textual hypothesis H, a system determines whether H can be inferred from T. This type of evaluation was establishe"
P12-2031,N06-1039,0,0.0333793,"er, we suggest a framework for evaluating inference-rule resources. Our framework simplifies a previously proposed “instance-based evaluation” method that involved substantial annotator training, making it suitable for crowdsourcing. We show that our method produces a large amount of annotations with high inter-annotator agreement for a low cost at a short period of time, without requiring training expert annotators. 1 Introduction Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’. Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’. The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 20"
P12-2031,D08-1027,0,0.105078,"Missing"
P12-2031,C08-1107,1,0.8349,"Missing"
P12-2031,P07-1058,1,0.77055,"system determines whether H can be inferred from T. This type of evaluation was established in RTE challenges by ablation tests (see RTE ablation tests in ACLWiki) and showed that resources’ impact can vary considerably from one system to another. These issues have also been noted by Sammons et al. (2010) and LoBue and Yates (2011). A complementary application-independent evaluation method is hence necessary. Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005). However, Szpektor et al. (2007) observed that directly judging rules out of context often results in low inter-annotator agreement. To remedy that, Szpektor et al. (2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Bhagat et al. (2007) proposed “instance-based evaluation”, in which annotators are presented with an application of a rule in a particular context and need to judge whether it results in a valid inference. This simulates the utility of rules in an applicatio"
P12-2031,W10-0725,0,0.0864527,"Missing"
P12-2031,W03-1011,0,0.542702,"Missing"
P12-3013,H05-1079,0,0.0292149,"er research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al., 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Yet, until now rather few and quite limited RTE systems were made publicly available. Moreover, these systems are restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS2 (Kouylekov and Negri, 2010) is a distancebased RTE system, which can exploit only lexical knowledge resources. NutCracker3 (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. Therefore, we provide our open-source textualentailment system, B IU T EE. Our system provides state-of-the-art linguistic analysis tools and exploits various types of manually built and automatically acquired knowledge resources, including lexical, lexical-syntactic and syntactic rewrite rules. Furthermore, the system components, including preprocessing utilities, knowledge resources, and even the steps of the inference algorithm, are modular,"
P12-3013,P05-1045,0,0.00385343,"ormations from which it is composed, i.e.: c(O) , n X i=1 c(oi ) = n X i=1 w · f (oi ) = w · n X f (oi ) i=1 (1) If the proof cost is below a threshold b, then the sys75 tem concludes that T entails H. The complete description of the cost model, as well as the method for learning the parameters w and b is described in (Stern and Dagan, 2011). 2.2 System flow The B IU T EE system flow (Figure 1) starts with preprocessing of the text and the hypothesis. B IU T EE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentencesplitting and numerical-normalizations. In addition, B IU T EE supports integration of users’ own utilities by simply implementing the appropriate interfaces. Entailment recognition begins with a global processing phase in which inference related computations that are not part of the proof are performed. Annotating the negation indicators and their scope in the text and hypothesis is an example of such calculation. Next, the system constructs a proof which is a sequence of transformations that transfo"
P12-3013,N10-1115,0,0.0198375,"The proof cost is defined as the sum of the costs of the transformations from which it is composed, i.e.: c(O) , n X i=1 c(oi ) = n X i=1 w · f (oi ) = w · n X f (oi ) i=1 (1) If the proof cost is below a threshold b, then the sys75 tem concludes that T entails H. The complete description of the cost model, as well as the method for learning the parameters w and b is described in (Stern and Dagan, 2011). 2.2 System flow The B IU T EE system flow (Figure 1) starts with preprocessing of the text and the hypothesis. B IU T EE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentencesplitting and numerical-normalizations. In addition, B IU T EE supports integration of users’ own utilities by simply implementing the appropriate interfaces. Entailment recognition begins with a global processing phase in which inference related computations that are not part of the proof are performed. Annotating the negation indicators and their scope in the text and hypothesis is an example of such calculation. Next, the system constructs a p"
P12-3013,D09-1120,0,0.0263662,"i=1 c(oi ) = n X i=1 w · f (oi ) = w · n X f (oi ) i=1 (1) If the proof cost is below a threshold b, then the sys75 tem concludes that T entails H. The complete description of the cost model, as well as the method for learning the parameters w and b is described in (Stern and Dagan, 2011). 2.2 System flow The B IU T EE system flow (Figure 1) starts with preprocessing of the text and the hypothesis. B IU T EE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al., 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentencesplitting and numerical-normalizations. In addition, B IU T EE supports integration of users’ own utilities by simply implementing the appropriate interfaces. Entailment recognition begins with a global processing phase in which inference related computations that are not part of the proof are performed. Annotating the negation indicators and their scope in the text and hypothesis is an example of such calculation. Next, the system constructs a proof which is a sequence of transformations that transform the text into the hypothesis. Finding such a proof is a"
P12-3013,P07-2045,0,0.00264363,"pages 73–78, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics their novel research components into an existing open-source system. Such research efforts might include developing knowledge resources, developing inference components for specific phenomena such as temporal inference, or extending RTE to different languages. A flexible and extensible RTE system is expected to encourage researchers to create and share their textual-inference components. A good example from another research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al., 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Yet, until now rather few and quite limited RTE systems were made publicly available. Moreover, these systems are restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS2 (Kouylekov and Negri, 2010) is a distancebased RTE system, which can exploit only lexical knowledge resources. NutCracker3 (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, b"
P12-3013,P10-4008,0,0.0157876,"stem is expected to encourage researchers to create and share their textual-inference components. A good example from another research area is the Moses system for Statistical Machine Translation (SMT) (Koehn et al., 2007), which provides the core SMT components while being extended with new research components by a large scientific community. Yet, until now rather few and quite limited RTE systems were made publicly available. Moreover, these systems are restricted in the types of knowledge resources which they can utilize, and in the scope of their inference algorithms. For example, EDITS2 (Kouylekov and Negri, 2010) is a distancebased RTE system, which can exploit only lexical knowledge resources. NutCracker3 (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource. Therefore, we provide our open-source textualentailment system, B IU T EE. Our system provides state-of-the-art linguistic analysis tools and exploits various types of manually built and automatically acquired knowledge resources, including lexical, lexical-syntactic and syntactic rewrite rules. Furthermore, the system componen"
P12-3013,R11-1063,1,0.885403,", 2007). Coreference relations are utilized via coreferencesubstitution transformations: one mention of an entity is replaced by another mention of the same entity, based on coreference relations. In the above example the system could apply such a transformation to substitute “him” with “Charles G. Taylor”. Since applications of entailment rules and coreference substitutions are yet, in most cases, insufficient in transforming T into H, our system allows on-the-fly transformations. These transformations include insertions of missing nodes, flipping partsof-speech, moving sub-trees, etc. (see (Stern and Dagan, 2011) for a complete list of these transformations). Since these transformations are not justified by given knowledge resources, we use linguisticallymotivated features to estimate their validity. For example, for on-the-fly lexical insertions we consider as features the named-entity annotation of the inserted word, and its probability estimation according to a unigram language model, which yields lower costs for more frequent words. Given a (T,H) pair, the system applies a search algorithm (Stern et al., 2012) to find a proof O = (o1 , o2 , . . . on ) that transforms T into H. For each proof step"
P12-3013,P12-1030,1,0.840728,"de insertions of missing nodes, flipping partsof-speech, moving sub-trees, etc. (see (Stern and Dagan, 2011) for a complete list of these transformations). Since these transformations are not justified by given knowledge resources, we use linguisticallymotivated features to estimate their validity. For example, for on-the-fly lexical insertions we consider as features the named-entity annotation of the inserted word, and its probability estimation according to a unigram language model, which yields lower costs for more frequent words. Given a (T,H) pair, the system applies a search algorithm (Stern et al., 2012) to find a proof O = (o1 , o2 , . . . on ) that transforms T into H. For each proof step oi the system calculates a cost c(oi ). This cost is defined as follows: the system uses a weightvector w, which is learned in the training phase. In addition, each transformation oi is represented by a feature vector f (oi ) which characterizes the transformation. The cost c(oi ) is defined as w · f (oi ). The proof cost is defined as the sum of the costs of the transformations from which it is composed, i.e.: c(O) , n X i=1 c(oi ) = n X i=1 w · f (oi ) = w · n X f (oi ) i=1 (1) If the proof cost is below"
P12-3013,W07-1401,1,\N,Missing
P12-3014,P10-1124,1,0.82554,"f documents for a given query, but do not allow any exploration of the thematic structure in the retrieved information. Thus, the need for tools that allow to effectively sift through a target set of documents is becoming ever more important. Faceted search (Stoica and Hearst, 2007; K¨aki, 2005) supports a better understanding of a target domain, by allowing exploration of data according to multiple views or facets. For example, given a set of documents on Nobel Prize laureates we might have different facets corresponding to the laureate’s nationality, the year when the prize was awarded, the Berant et al. (2010) proposed an exploration scheme that focuses on relations between concepts, which are derived from a graph describing textual entailment relations between propositions. In their setting a proposition consists of a predicate with two arguments that are possibly replaced by variables, such as ‘X control asthma’. A graph that specifies an entailment relation ‘X control asthma → X affect asthma’ can help a user, who is browsing documents dealing with substances that affect asthma, drill down and explore only substances that control asthma. This type of exploration can be viewed as an extension of"
P12-3014,D11-1142,0,0.0128819,"oposition-based exploration, or equivalently, statement-based exploration. In our model, it is the entailment relation between propositional templates which determines the granularity of the viewed information space. We first describe the inputs to the system and then detail our proposed exploration scheme. 3.1 System Inputs Corpus A collection of documents, which form the search space of the system. Extracted Propositions A set of propositions, extracted from the corpus document. The propositions are usually produced by an extraction method, such as TextRunner (Banko et al., 2007) or ReVerb (Fader et al., 2011). In order to support the exploration process, the documents are indexed by the propositional templates and argument terms of the extracted propositions. Entailment graph for predicates The nodes of the entailment graph are propositional templates, where edges indicate entailment relations between templates (Section 2.2). In order to avoid circularity in the exploration process, the graph is transformed into a DAG, by merging ‘equivalent’ nodes that are in the same strong connectivity component (as suggested by Berant et al. (2010)). In addition, for clarity and simplicity, edges that can be i"
P12-3014,N07-1031,0,0.0778192,"ate its benefit on the health-care domain. To the best of our knowledge this is the first implementation of an exploration system at the statement level that is based on the textual entailment relation. 1 Introduction Finding information in a large body of text is becoming increasingly more difficult. Standard search engines output a set of documents for a given query, but do not allow any exploration of the thematic structure in the retrieved information. Thus, the need for tools that allow to effectively sift through a target set of documents is becoming ever more important. Faceted search (Stoica and Hearst, 2007; K¨aki, 2005) supports a better understanding of a target domain, by allowing exploration of data according to multiple views or facets. For example, given a set of documents on Nobel Prize laureates we might have different facets corresponding to the laureate’s nationality, the year when the prize was awarded, the Berant et al. (2010) proposed an exploration scheme that focuses on relations between concepts, which are derived from a graph describing textual entailment relations between propositions. In their setting a proposition consists of a predicate with two arguments that are possibly r"
P13-1131,P11-1062,1,0.885173,"nce for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as t"
P13-1131,D07-1017,0,0.284558,"mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. Howeve"
P13-1131,D10-1113,0,0.655432,"lied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular"
P13-1131,C10-2029,0,0.519782,"lied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular"
P13-1131,E09-1025,0,0.200537,"tion Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicate"
P13-1131,P12-2023,0,0.0224889,"el from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute contextbiased lexical similarities accordingly. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. In a different NLP task, Eidelman et al. (2012) utilize a similar approach to ours for improving the performance of statistical machine translation (SMT). They learn an LDA model on the source language side of the training corpus with the purpose of identifying implicit sub-domains. Then they utilize the distribution over topics inferred for each document in their corpus to compute separate per-topic translation probability tables. Finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears. A notable difference between ou"
P13-1131,D08-1094,0,0.0973353,"Missing"
P13-1131,P10-2017,0,0.0700603,"Missing"
P13-1131,D11-1142,0,0.0780829,"010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rulesets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. The Lin similarity measure is described in Equation 2. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpektor and Dagan, 2008). To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available webbased open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommod"
P13-1131,P98-2127,0,0.928942,"d word space, context-sensitive methods represent them as vectors at the level of latent topics. This raises the question of whether such coarse-grained topic vectors might be less informative in determining the semantic similarity between the two predicates. To address this hypothesized caveat of prior context-sensitive rule scoring methods, we propose a novel generic scheme that integrates wordlevel and topic-level representations. Our scheme can be applied on top of any context-insensitive “base” similarity measure for rule learning, which operates at the word level, such as Cosine or Lin (Lin, 1998). Rather than computing a single context-insensitive rule score, we compute a distinct word-level similarity score for each topic in an LDA model. Then, when applying a rule in a given context, these different scores are weighed together based on the specific topic distribution under the given context. This way, we calculate similarity over vectors in the original word space, while biasing them towards the given context via a topic model. In order to promote replicability and equal-term comparison with our results, we based our experiments on publicly available datasets, both for unsupervised"
P13-1131,S07-1009,0,0.243789,"provement is statistically significant at p < 0.01 for BInc and Lin, and p < 0.015 for Cosine, using paired ttest. This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures. Surprisingly, both baseline topic-level contextsensitive methods, namely DC and SC, underperformed compared to their context-insensitive baselines. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over contextinsensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. However, our result suggests that topiclevel models might not be robust enough when applied to a random sample of inferences. An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method. The number of topics in the range 25-100 performed almost equally well under the WT model for all base measures, with a moderate decline for higher numbers. 1337 The need for this rather small number of topics is due"
P13-1131,I05-5011,0,0.334593,"s data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X, and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y’. Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y’, ‘X can accommodate up to Y’, ‘X will accommodate up to Y’, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word cooccurrence statistics per predicate. Next, we applied some clean-up preprocessing to the ReVerb extractions. This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments. In addition, we discarded all predicates that co-occur with less than 100 unique argument words in each slot. The remaining corpus consists of 7 million unique extractions and 2,155 verb predicates. Finally, w"
P13-1131,N06-1039,0,0.0362764,"and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distributional setting. This research line was mainly initiated by t"
P13-1131,C08-1107,1,0.967985,"he highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above). DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an infer"
P13-1131,P08-1078,1,0.952819,"may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a"
P13-1131,P10-1097,0,0.0638171,"Missing"
P13-1131,P12-2031,1,0.821446,"h learned rule under each LDA topic, as specified in our context-sensitive model. We release a rule set comprising the top 500 context-sensitive rules that we learned for each of the verb predicates in our learning corpus, along with our trained LDA 3 ReVerb is available washington.edu/ 1336 at http://reverb.cs. Method Valid Invalid Total Lin 266 545 811 BInc 254 523 777 Cosine 272 539 811 Method CI DC SC WT Table 3: Sizes of rule application test set for each learned rule-set. model.4 4.2 Evaluation Task To evaluate the performance of the different methods we chose the dataset constructed by Zeichner et al. (2012). 5 This publicly available dataset contains about 6,500 manually annotated predicate template rule applications, each one labeled as correct or incorrect. For example, ‘Jack agree with Jill 9 Jack feel sorry for Jill’ is a rule application in this dataset, labeled as incorrect, and ‘Registration open this month → Registration begin this month’ is another rule application, labeled as correct. Rule applications were generated by randomly sampling extractions from ReVerb, such as (‘Jack’,‘agree with’,‘Jill’) and then sampling possible rules for each, such as ‘agree with → feel sorry for’. Hence,"
P13-1131,P08-1028,0,0.0596509,"l similarity and substitution scenarios in context. While we focus on lexical-syntactic predicate templates and instantiations of their argument slots as context, lexical similarity methods consider various lexical units that are not necessarily predicates, with their context typically being the collection of words in a window around them. Various approaches have been proposed to address lexical similarity. A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2010). Other works (Erk and Pad´o, 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows. In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lapata, 2010b) to compute lexical similarity in context. A natural extension of our work would be to extend our two level model to accommodate contextsensitive lexical similarity. For this purpose we will need to redefine the scope of context in our model, and adap"
P13-1131,D12-1104,0,0.0505574,"ond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acq"
P13-1131,N07-1071,0,0.681951,"of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X acquire Y → X purchase Y’, while ‘Children acquire skills → Children purchase skills’ is not. To address this issue, a line of works emerged which computes a contextsensitive reliability score for each rule application, based on the given context. The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b). In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics cation (LDA) model. Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the gi"
P13-1131,P02-1006,0,0.103873,"tivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set. 1 Introduction Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”. Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009). Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised distribu"
P13-1131,N10-1013,0,0.0879068,"ethods. 1333 Consider the application of an inference rule ‘LHS → RHS’ in the context of a particular pair of arguments for the X and Y slots, denoted by wx and wy , respectively. Denoting by l and r the predicates appearing in the two rule sides, the reliability score of the topic-level model is defined as follows (we present a geometric mean formulation for consistency with DIRT): scoreTopic (LHS → RHS, wx , wy ) q (3) = sim(dxl , dxr , wx ) · sim(dyl , dyr , wy ) where sim(d, d0 , w) is a topic-distribution similarity measure conditioned on a given context word. Specifically, Ritter et al. (2010) utilized the dot product form for their similarity measure: simDC (d, d0 , w) = Σt [p(t|d, w) · p(t|d0 , w)] (4) (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Dinu and Lapata (2010b) presented a slightly different similarity measure for topic distributions that performed better in their setting as well as in a related later paper on context-sensitive scoring of lexical similarity (Dinu and Lapata, 2010a). In this measure, the topic distribution for the right hand side of the rule is not conditioned on w"
P13-1131,P10-1044,0,0.143123,"Missing"
P13-1131,D10-1106,0,0.105017,"argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus. Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors. This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011). Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012). The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities. Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score. However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments. For example, ‘AT&T acquire TMobile → AT&T purchase T-Mobile’, is a valid application of the rule ‘X"
P13-1131,P10-1045,0,0.112294,"Missing"
P13-1131,C98-2122,0,\N,Missing
P13-1131,P08-1000,0,\N,Missing
P13-2051,W03-1011,0,0.140752,"at instantiate this slot in all of the occurrences of the template in a learning corpus. Two templates are then deemed semantically similar if the argument vectors of their corresponding slots are similar. Ideally, inference rules should be learned for all templates that occur in the learning corpus. 283 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283–288, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Technical Background inferred by them (unlike the symmetric Lin). One such example is the Cover measure (Weeds and Weir, 2003): P 0 [v(w)] 0 Cover(v, v ) = Pw∈v∩v (3) w∈v∪v 0 [v(w)] The distributional similarity score for an inference rule between two predicate templates, e.g. ‘X resign Y → X quit Y’, is typically computed by measuring the similarity between the argument vectors of the corresponding X slots and Y slots of the two templates. To this end, first the argument vectors should be constructed and then a similarity measure between two vectors should be provided. We note that we focus here on binary templates with two slots each, but this approach can be applied to any template. A common starting point is to c"
P13-2051,C10-2029,0,0.35723,"e cases of domain specific corpora and resource-scarce languages, or with templates with rare terms or long multi-word expressions such as ‘X be also a risk factor to Y’ or ‘X finish second in Y’, which capture very specific meanings. Due to few occurrences, the slots of rare templates are represented with very sparse argument vectors, which in turn lead to low reliability in distributional similarity scores. A common practice in prior work for learning predicate inference rules is to simply disregard templates below a minimal frequency threshold (Lin and Pantel, 2001; Kotlerman et al., 2010; Dinu and Lapata, 2010; Ritter et al., 2010). Yet, acquiring rules for rare templates may be beneficial both in terms of coverage, but also in terms of more accurate rule application, since rare templates are less ambiguous than frequent ones. We propose to improve the learning of rules between infrequent templates by expanding their argument vectors. This is done via a “dual” distributional similarity approach, in which we consider two words to be similar if they instantiate similar sets of templates. We then use these similarities to expand the argument vector of each slot with words that were identified as simil"
P13-2051,P12-2031,1,0.855806,"ds and non-alphabetical words that instantiated either the X or the Y argument slots. In addition, we discarded templates that co-occur with less than 5 unique argument words in either of their slots, assuming that such few arguments cannot convey reliable semantic information, even with expansion. Our final corpus consists of around 350,000 extractions and 14,000 unique templates. In this corpus around one third of the extractions refer to templates that co-occur with at most 35 unique arguments in both their slots. We evaluated the quality of inference rules using the dataset constructed by Zeichner et al. (2012)2 , which contains about 6,500 manually annotated template rule applications, each labeled as correct or not. For example, ‘The game develop eye-hand coordination 9 The game launch eye-hand coordination’ is a rule application in this dataset of the rule ‘X develop Y → X launch Y’, labeled as incorrect, and ‘Captain Cook sail to Australia → Captain Cook depart for Australia’ is a rule application of the rule ‘X sail to Y → X depart for Y’, labeled as correct. Specifically, we induced two datasets from Zeichner et al.’s dataset, denoted DS-5-35 and DS-5-50, which consist of all rule applications"
P13-2051,D11-1142,0,0.0591792,"ias for identifying generalization/specification relations, i.e. relations between predicates with narrow (or specific) semantic meanings to predicates with broader meanings 284 4 Miller et al. (2012) on word sense disambiguation. Yet, to the best of our knowledge, this is the first work that applies lexical expansion to distributional similarity feature vectors. We next describe our scheme in detail. 3.1 We constructed a relatively small learning corpus for investigating the sparseness issues of such corpora. To this end, we used a random sample from the large scale web-based ReVerb corpus1 (Fader et al., 2011), comprising tuple extractions of predicate templates with their argument instantiations. We applied some clean-up preprocessing to these extractions, discarding stop words, rare words and non-alphabetical words that instantiated either the X or the Y argument slots. In addition, we discarded templates that co-occur with less than 5 unique argument words in either of their slots, assuming that such few arguments cannot convey reliable semantic information, even with expansion. Our final corpus consists of around 350,000 extractions and 14,000 unique templates. In this corpus around one third o"
P13-2051,P98-2127,0,0.19996,"quit,John , contains a count of the number of times the term j instantiated the template slot i in the corpus. Thus, each row Mi,∗ corresponds to an argument vector for slot i. Next, some function of the counts is used to assign weights to all Mi,j entries. In this paper we use pointwise mutual information (PMI), which is common in prior work (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Finally, rules are assessed using some similarity measure between corresponding argument vectors. The state-of-the-art DIRT algorithm (Lin and Pantel, 2001) uses the highly cited Lin similarity measures (Lin, 1998) to score rules between binary templates as follows: P 0 0 [v(w) + v (w)] 0 Lin(v, v ) = Pw∈v∩v (1) 0 w∈v∪v 0 [v(w) + v (w)] DIRT (l → r) q = Lin(vl:x , vr:x ) · Lin(vl:y , vr:y ) As can be seen, in the core of the Lin and Cover measures, as well as in many other well known distributional similarity measures such as Jaccard, Dice and Cosine, stand the number of shared arguments vs. the total number of arguments in the two vectors. Therefore, when the argument vectors are sparse, containing very few non-zero features, these scores become unreliable and volatile, changing greatly with every incl"
P13-2051,C12-1109,0,0.0210226,"the work by (2) where v and v 0 are two argument vectors, l and r are the templates participating in the inference rule and vl:x corresponds to the argument vector of slot X of template l, etc. While the original DIRT algorithm utilizes the Lin measure, one can replace it with any other vector similarity measure. A separate line of research for word similarity introduced directional similarity measures that have a bias for identifying generalization/specification relations, i.e. relations between predicates with narrow (or specific) semantic meanings to predicates with broader meanings 284 4 Miller et al. (2012) on word sense disambiguation. Yet, to the best of our knowledge, this is the first work that applies lexical expansion to distributional similarity feature vectors. We next describe our scheme in detail. 3.1 We constructed a relatively small learning corpus for investigating the sparseness issues of such corpora. To this end, we used a random sample from the large scale web-based ReVerb corpus1 (Fader et al., 2011), comprising tuple extractions of predicate templates with their argument instantiations. We applied some clean-up preprocessing to these extractions, discarding stop words, rare wo"
P13-2051,P02-1006,0,0.304024,"s scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines. 1 Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of"
P13-2051,P10-1044,0,0.203808,"Missing"
P13-2051,N06-1039,0,0.0685627,"or low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines. 1 Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus. Two t"
P13-2051,C08-1107,1,0.961452,"Introduction The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). For example, the inference rule ‘X treat Y → X relieve Y’, between the templates ‘X treat Y’ and ‘X relieve Y’ may be useful to identify the answer to “Which drugs relieve stomach ache?”. The predominant unsupervised approach for learning inference rules between templates is via distributional similarity (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Szpektor and Dagan, 2008). Specifically, each argument slot in a template is represented by an argument vector, containing the words (or terms) that instantiate this slot in all of the occurrences of the template in a learning corpus. Two templates are then deemed semantically similar if the argument vectors of their corresponding slots are similar. Ideally, inference rules should be learned for all templates that occur in the learning corpus. 283 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 283–288, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computationa"
P13-2051,C98-2122,0,\N,Missing
P13-2080,N12-1021,0,0.0900373,"Missing"
P13-2080,S13-2045,1,0.930106,"t is called partial textual entailment, because we are only interested in recognizing whether a single element of the hypothesis is entailed. To differentiate the two tasks, we will refer to the original textual entailment task as complete textual entailment. Partial textual entailment was first introduced by Nielsen et al. (2009), who presented a machine learning approach and showed significant improvement over baseline methods. Recently, a public benchmark has become available through the Joint Student Response Analysis and 8th Recognizing Textual Entailment (RTE) Challenge in SemEval 2013 (Dzikovska et al., 2013), on which we focus in this paper. Our goal in this paper is to investigate the idea of partial textual entailment, and assess whether Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to thi"
P13-2080,R11-1063,1,0.82929,"rate movement in the body. H: The main job of muscles is to move bones. Lexical Inference This feature checks whether both facet words, or semantically related words, appear in T . We use WordNet (Fellbaum, 1998) with the Resnik similarity measure (Resnik, 1995) and count a facet term wi as matched if the similarity score exceeds a certain threshold (0.9, empirically determined on the training set). Both w1 and w2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first pars"
P13-2080,P11-2098,1,0.802976,"mainly on lexical inference and syntax. We examined three representative modules that reflect these levels: Exact Match, Lexical Inference, and Syntactic Inference. Exact Match We represent T as a bag-of-words containing all tokens and lemmas appearing in the text. We then check whether both facet lemmas w1 , w2 appear in the text’s bag-of-words. Exact matching was used as a baseline in previous recognizing textual entailment challenges (Bentivogli et al., 2011), and similar methods of lemmamatching were used as a component in recognizing textual entailment systems (Clark and Harrison, 2010; Shnarch et al., 2011). Task Definition In order to tackle partial entailment, we need to find a way to decompose a hypothesis. Nielsen et al. (2009) defined a model of facets, where each such facet is a pair of words in the hypothesis and the direct semantic relation connecting those two words. We assume the simplified model that was used in RTE-8, where the relation between the words is not explicitly stated. Instead, it remains unstated, but its interpreted meaning should correspond to the manner in which the words are related in the hypothesis. For example, in the sentence “the main job of muscles is to move bo"
P13-2080,S12-1051,0,0.0646707,"Missing"
P13-2080,P12-3013,1,0.842323,"2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first parsing H, and then locating the two nodes whose words compose the facet. We then find their lowest common ancestor (LCA), and extract the path P from w1 to The facet (muscles, move) refers to the agent role in H, and is expressed by T . However, the facet (move, bones), which refers to a theme or direct object relation in H, is unaddressed by T . 3 Entailment Modules Recognizing Faceted Entailment Our goal is to inv"
P13-4017,N03-1013,0,0.0332783,"evels of the input knowledge resources, (ii) reducing inference chain probability as its length increases, and (iii) increasing term-level probability as we have more inference chains which suggest that the hypothesis term is inferred by the text. Both PLMs only need sentence-level annotations from which they derive term-level inference probabilities. To summarize, the lexical inference module Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4 ), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin’s distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combinati"
P13-4017,P98-2127,0,0.00719444,", the lexical inference module Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4 ), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin’s distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of a given text-hypothesis. Developers can use this tool to save the hassle of interfacing with the different lexical knowledge resources, and spare the labor of combining their knowledge via inference chains. The lexical inference model, described next, provides a mean to"
P13-4017,S12-1053,0,0.0681499,"Missing"
P13-4017,P09-1051,1,0.856345,"its length increases, and (iii) increasing term-level probability as we have more inference chains which suggest that the hypothesis term is inferred by the text. Both PLMs only need sentence-level annotations from which they derive term-level inference probabilities. To summarize, the lexical inference module Available plug-ins for lexical resources We have implemented plug-ins for the following resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4 ), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin’s distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of"
P13-4017,W11-2402,1,0.914623,"ping words between the two sentences, to identify that A holds an answer for Q, background world knowledge is needed 97 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97–102, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Text Hypothesis is done. PLIS can be easily extended with new knowledge resources and new inference models. It comes with a set of ready-to-use plug-ins for many common lexical resources (Section 2.1) as well as two implementation of the scoring framework. These implementations, described in (Shnarch et al., 2011; Shnarch et al., 2012), provide probability estimations for inference. PLIS has an interactive online viewer (Section 4) which provides a visualization of the entire inference process, and is very helpful for analysing lexical inference models and lexical resources usability. Lexical Integrator ?2 ?1 ?3 ?4 ?(?3 → ℎ2 ) WordNet ?′ Wikipedia … VerbOcean Lexical Resources ℎ1 ℎ2 ℎ3 ?(? → ℎ3 ) ? ?→? Lexical Inference 2 Lexical integrator Figure 1: PLIS schema - a text-hypothesis pair is processed by the Lexical Integrator which uses a set of lexical resources to extract inference chains which conne"
P13-4017,S12-1032,1,0.781116,"two sentences, to identify that A holds an answer for Q, background world knowledge is needed 97 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 97–102, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Text Hypothesis is done. PLIS can be easily extended with new knowledge resources and new inference models. It comes with a set of ready-to-use plug-ins for many common lexical resources (Section 2.1) as well as two implementation of the scoring framework. These implementations, described in (Shnarch et al., 2011; Shnarch et al., 2012), provide probability estimations for inference. PLIS has an interactive online viewer (Section 4) which provides a visualization of the entire inference process, and is very helpful for analysing lexical inference models and lexical resources usability. Lexical Integrator ?2 ?1 ?3 ?4 ?(?3 → ℎ2 ) WordNet ?′ Wikipedia … VerbOcean Lexical Resources ℎ1 ℎ2 ℎ3 ?(? → ℎ3 ) ? ?→? Lexical Inference 2 Lexical integrator Figure 1: PLIS schema - a text-hypothesis pair is processed by the Lexical Integrator which uses a set of lexical resources to extract inference chains which connect the two. The Lexical"
P13-4017,D07-1003,0,0.0357688,"from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenomenon: (i) considering the different reliability levels of the input knowledge resources, (ii) reducing inference chain probability as its length increases, and (iii) increasing term-level probability as we have more inference chains which suggest that the hypothesis term is inferred by the text. Both PLMs only need sentence-level annotations from which they derive term-level inference probabilities. To summarize, the lexical inference module Available plug-ins f"
P13-4017,W07-1401,1,\N,Missing
P13-4017,2003.mtsummit-systems.9,0,\N,Missing
P13-4017,C98-2122,0,\N,Missing
P13-4017,W04-3205,0,\N,Missing
P14-2120,W09-2417,0,0.142741,"Missing"
P14-2120,S12-1001,0,0.216378,"orth as Implied SRL, involves three major sub-tasks. First, for each predicate, the unfilled roles, termed Null Instantiations (NI), should be detected. Second, each NI should be classified as Definite NI (DNI), meaning that the role filler must exist in the discourse, or Indefinite NI otherwise. Third, the DNI fillers should be found (DNI linking). Prior Work The most notable work targeting implied predicate-argument relationships is the 2010 SemEval task of Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2009). Later works that followed the SemEval challenge include (Silberer and Frank, 2012) and (Roth and Frank, 2013), which proposed auto740 sis pair, we locate a predicate-argument relationship in the Hypothesis, where both the predicate and the argument appear also in the Text, while the relationship between them is not expressed in its syntactic structure. This process is performed automatically, based on syntactic parsing (see below). Then, a human reader annotates each instance as “Yes” – meaning that the implied relationship indeed holds in the Text, or “No” otherwise. Example instances, constructed by this process, are shown in Table 1. In this work we used lemma-level lexi"
P14-2120,W08-1301,0,0.0276277,"Missing"
P14-2120,N03-1033,0,0.0295384,"Missing"
P14-2120,P05-1045,0,0.105184,"Missing"
P14-2120,J12-4003,0,0.0297982,"follow a meronymy relation between the filler (“southeastern United States”) and the candidate argument (“New Orleans”), or certain types of discourse (co)references (e.g., example 5 in Table 1), or some other linguistic phenomena. Either way, they are crucial for textual inference, while not being NIs. 3 4 Recognition Algorithm We defined 15 features, summarized in Table 2, which capture local and discourse phenomena. These features do not depend on manually built resources, and hence are portable to resource-poor languages. Some features were proposed in prior works, and are marked by G&C (Gerber and Chai, 2012) or S&F (Silberer and Frank, 2012). Our best results were obtained with the Random Forests learning algorithm (Breiman, 2001). The first two features are described in the next subsection, while the others are explained in the table itself. Dataset This section describes a semi-automatic method for extracting candidate instances of implied predicate-argument relationship from an RTE dataset. This extraction process directly follows our task formalization. Given a Text Hypothe4.1 Statistical discourse features Statistical features in prior works mostly capture general properties of the predicate"
P14-2120,N10-1115,0,0.032515,"Missing"
P14-2120,D09-1120,0,0.062016,"Missing"
P14-2120,H05-1087,0,0.096051,"Missing"
P14-2120,W04-3250,0,0.0337917,"Missing"
P14-2120,S13-1043,0,0.0401665,"Missing"
P14-2120,S10-1008,0,\N,Missing
P14-5008,W07-1401,1,0.865172,"esources for these languages (assuming that the EDAs themselves are largely language-independent). These are provided by the language-independent knowledge acquisition tools which we offer alongside the platform (cf. Section 3.2). EOP Evaluation Results for the three EDAs included in the EOP platform are reported in Table 1. Each line represents an EDA, the language and the dataset on which the EDA was evaluated. For brevity, we omit here the knowledge resources used for each EDA, even though knowledge configuration clearly affects performance. The evaluations were performed on RTE-3 dataset (Giampiccolo et al., 2007), where the goal is to maximize accuracy. We (manually) translated it to German and Italian for evaluations: in both cases the results fix a reference for the two languages. The two new datasets for German and English are available both as part of the EOP distribution and independently5 . The transformation-based EDA was also evaluated on RTE-6 dataset (Bentivogli et al., 2010), in which the goal is to maximize the F1 measure. The results of the included EDAs are higher than median values of participated systems in RTE-3, and they are competing with state-of-the-arts in RTE-6 results. To the b"
P14-5008,P98-2127,0,0.00988239,"otivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-structured and well-documented form. A subclass of this group is formed by researchers who want to set up a RTE infrastructure for languages in which it does"
P14-5008,P09-1051,1,0.820811,"art of or all of the semantic processing, such as Question Answering or Intelligent Tutoring. Such users require a system that is as easy to deploy as possible, which motivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-s"
P14-5008,P12-1030,1,0.845888,"raphrasing patterns at the predicate-argument level that cannot be captured by purely lexical rules. Formally, each syntactic rule consists of two dependency tree fragments plus a mapping from the variables of the LHS tree to the variables of the RHS tree.4 2.3 In the EOP we include a transformation based inference system that adopts the knowledge based transformations of Bar-Haim et al. (2007), while incorporating a probabilistic model to estimate transformation confidences. In addition, it includes a search algorithm which finds an optimal sequence of transformations for any given T/H pair (Stern et al., 2012). Edit distance EDA involves using algorithms casting textual entailment as the problem of mapping the whole content of T into the content of H. Mappings are performed as sequences of editing operations (i.e., insertion, deletion and substitution) on text portions needed to transform T into H, where each edit operation has a cost associated with it. The underlying intuition is that the probability of an entailment relation between T and H is related to the distance between them; see Kouylekov and Magnini (2005) for a comprehensive experimental study. Configuration Files The EC components can b"
P14-5008,P07-2045,0,\N,Missing
P14-5008,C98-2122,0,\N,Missing
P15-2050,P12-1015,0,0.0345862,"Missing"
P15-2050,N13-1136,1,0.584267,"ook advantage) are grouped in a single predicate slot. Additionally, arguments are truncated in cases such as prepositional phrases and reduced relative clauses. The resulting structure can be understood as an extension of shallow syntactic chunking (Abney, 1992), where chunks are labeled as either predicates or arguments, and are then interlinked to form a complete proposition. It is not clear apriory whether the differences manifested in Open IE’s structure could be beneficial as intermediate structures for downstream applications. Although a few end tasks have made use of Open IE’s output (Christensen et al., 2013; Balasubramanian et al., 2013), there has been no systematic comparison against other structures. In the following sections, we quantitatively study and analyze the value of Open IE structures against the more common intermediate structures – lexical, dependency and SRL, for three downstream NLP tasks. 3 (a) Lexical matching of a 5 words window (marked with a box). Current window yields a score of 4 - words contributing to the score are marked in bold. (b) Dependency matching yields a score of 3. Contributing triplets are marked in bold. S: refused0.1 : A0 : John A1 : to visit a Vegas casino"
P15-2050,N13-1090,0,0.0553311,"sk Text comprehension tasks extrinsically test natural language understanding through question answer304 Target refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts a"
P15-2050,W08-1301,0,0.0428588,"Missing"
P15-2050,D11-1142,0,0.780242,"Missing"
P15-2050,D13-1020,0,0.00596479,"ch as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of this sentence. ing. We use the MCTest corpus (Richardson et al., 2013), which is composed of short stories followed by multiple choice questions. The MCTest task does not require extensive world knowledge, which makes it ideal for testing underlying sentence representations, as performance will mostly depend on accuracy and informativeness of the extracted structures. We adapt the unsupervised lexical matching algorithm from the original MCTest paper. It counts lexical matches between an assertion obtained from a candidate answer (CA) and a sliding window over the story. The selected answer is the one for which the maximum number of matches are found. Our adapta"
P15-2050,D07-1002,0,0.0221474,"equence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, and highlight its unique properties. As we show in Section 4, these differences have an im"
P15-2050,J15-4004,0,0.0052185,"Missing"
P15-2050,P14-2050,0,0.0651708,"et refused Lexical Dependency SRL Open IE John to visit Vegas nsubj John xcomp visit A0 A1 A1 A1 0 1 1 2 John to visit Vegas systems take three input words (A:A∗ , B:?) and output a word B ∗ , such that the relation between B and B ∗ is closest to the relation between A and A∗ . For instance, queen is the desired answer for the triple (man:king, woman:?). Some recent state-of-the-art approaches to these two tasks derive a similarity score via arithmetic computations on word embeddings (Mikolov et al., 2013b). While original training of word embeddings used lexical contexts (n-grams), recently Levy and Goldberg (2014) generalized this to arbitrary contexts, such as dependency paths. We use their software1 and recompute the word embeddings using contexts from our four structures: lexical context, dependency paths, SRL’s semantic relations, and Open IE’s surrounding tuple elements. Table 1 shows the different contexts for a sample word. John to visit Vegas Table 1: Some of the different contexts for the target word “refused” in the sentence ”John refused to visit Vegas”. SRL and Open IE contexts are preceded by their element (predicate or argument) index. See figure 1 for the different representations of thi"
P15-2050,P10-1040,0,0.0288292,"). lar to the word similarity tasks. To our knowledge, Open IE results on both analogy datasets surpass the state of the art. An example (from the Microsoft test set) which supports the observation regarding Open IE embeddings space is (gentlest:gentler, loudest:?), for which only Open IE answers correctly as louder, while lexical respond with higher-pitched (domain similar to loudest), and dependency with thinnest (functionally similar to loudest). Our Open-IE embeddings are freely available6 and we note that these can serve as plug-in features for other NLP applications, as demonstrated in (Turian et al., 2010). 5 References Steven P Abney. 1992. Parsing by chunks. Principlebased parsing, pages 257–278. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 19–27. Association for Computational Linguistics. Conclusions Niranjan Balasubramanian, Stephen Mausam, and Oren Etzioni. 2013. coherent event schemas at sca"
P15-2050,J13-2005,0,0.00683734,"(1) Lexical representations, in which features are extracted from the original word sequence or the bag of words, (2) Stanford dependency parse trees (De Marneffe and Manning, 2008), which draw syntactic relations between words, and (3) Semantic role labeling (SRL), which extracts frames linking predicates with their semantic arguments (Carreras and M`arquez, 2005). For instance, a QA application can evaluate a question and a candidate answer by examining their lexical overlap (P´erez-Couti˜no et al., 2006), by using short dependency paths as features to compare their syntactic relationships (Liang et al., 2013), or by using SRL to compare their predicate-argument structures (Shen and Lapata, 2007). In a seemingly independent research direction, Open Information Extraction (Open IE) extracts coherent propositions from a sentence, each comprising a relation phrase and two or more argument 2 Intermediate Structures In this section we review how intermediate structures differ from each other, in terms of their imposed structure, predicate and argument boundaries, and the type of relations that they introduce. We include Open IE in this analysis, along with lexical, dependency and SRL representations, an"
P15-2050,W13-3512,0,0.00940111,"Missing"
P15-2050,D12-1048,1,0.598339,"Missing"
P15-2050,W11-0906,0,\N,Missing
P15-2050,W05-0620,0,\N,Missing
P15-2050,P11-1060,0,\N,Missing
P15-2050,N09-1003,0,\N,Missing
P15-2050,D13-1178,1,\N,Missing
P15-2061,P08-1030,1,0.909629,"Introduction Event trigger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-3"
P15-2061,P13-1008,1,0.784185,"s are encoded as a small set of event-independent classification features, based on lexical matches and external resources like WordNet. Using eventindependent features allows us to train the system only once, at system setup phase, requiring annotated triggers in a training set for just a few preselected event types. Then, whenever a new event type is introduced for labeling, we only need to collect a seed list for it from its description, and provide it as input to the system. We developed a seed-based system (Section 3), based on a state-of-the-art fully-supervised event extraction system (Li et al., 2013). When evaluated on the ACE-2005 dataset,1 our system outperforms the fully-supervised one (Section 4), even though we don’t utilize any annotated triggers of the test events during the labeling phase, and only The task of event trigger labeling is typically addressed in the standard supervised setting: triggers for each target event type are annotated as training data, based on annotation guidelines. We propose an alternative approach, which takes the example trigger terms mentioned in the guidelines as seeds, and then applies an eventindependent similarity-based classifier for trigger labeli"
P15-2061,C10-1077,0,0.363517,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,P10-1081,0,0.783761,"ger labeling is the task of identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association"
P15-2061,N07-4013,0,0.0609519,"Missing"
P15-2061,P09-2015,0,0.0541745,"Missing"
P15-2061,P08-1004,0,0.0706965,"Missing"
P15-2061,P07-2005,0,0.0331584,"Missing"
P15-2061,P04-1015,0,0.0269755,"ers mentioned in each event description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additiona"
P15-2061,P06-2094,0,0.0914989,"Missing"
P15-2061,N06-1039,0,0.0182505,"Missing"
P15-2061,P05-1047,0,0.0254773,"ing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009)"
P15-2061,C00-2136,0,0.127914,"Missing"
P15-2061,N13-1092,0,0.0152668,"Missing"
P15-2061,W06-0204,0,0.0207608,"rk contributes to the broader research direction of reducing annotation for information extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and language"
P15-2061,P11-1113,0,0.0616286,"f identifying the main word tokens that express mentions of prespecified event types in running text. For example, in “20 people were wounded in Tuesday’s airport blast”, “wounded” is a trigger of an Injure event and “blast” is a trigger of an Attack. The task both detects trigger tokens and classifies them to appropriate event types. While this task is often a component within the broader event extraction task, labeling both triggers and arguments, this paper focuses on trigger labeling. Most state-of-the-art event trigger labeling approaches (Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Li et al., 2013) follow the standard supervised learning paradigm. For each event type, experts first write annotation guidelines. Then, annotators follow them to label event triggers in a large dataset. Finally, a classifier is trained over the annotated triggers to label the target events. 1 372 http://projects.ldc.upenn.edu/ace Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 372–376, c Beijing, China, July 26-31, 2015. 2015 Association for Computational L"
P15-2061,E12-1029,0,0.0449883,"extraction. One such IE paradigm, including Preemptive IE (Shinyama and Sekine, 2006), Ondemand IE (Sekine, 2006; Sekine and Oda, 2007) and Open IE (Etzioni et al., 2005; Banko et al., 2007; Banko et al., 2008), focuses on unsupervised relation and event discovery. We, on the other hand, follow the same goal as fullysupervised systems in targeting pre-specified event types, but aim at minimal annotation cost. Bootstrapping methods (such as (Yangarber et al., 2000; Agichtein and Gravano, 2000; Riloff, 1996; Greenwood and Stevenson, 2006; Liao and Grishman, 2010a; Stevenson and Greenwood, 2005; Huang and Riloff, 2012)) have been widely applied in IE. Most work started from a small set of seed patterns, and repeatedly expanded them from unlabeled corpora. Relying on unlabeled data, bootstrapping methods are scalable but tend to produce worse results (Liao and Grishman, 2010a) than supervised models due to semantic drift (Curran et al., 2007). Our method can be seen complementary to bootstrapping frameworks, since we exploit manually crafted linguistic resources which are more accurate but may not cover all domains and languages. Our approach is perhaps closest to (Roth et al., 2009). They addressed a differ"
P15-2061,N12-1015,0,0.0101718,"nt description into a seed list for the event type, which is provided as input to our trigger labeling method. Triggers from the above quoted sentences are hence included in the Meet seed list, shown in Figure 1. As mentioned in the Introduction, our method (Section 3) is based on event-independent features 3.1 The Fully-Supervised System The event extraction system of Li et al. (2013) labels triggers and their arguments for a set of target event types L, for which annotated training documents are provided. The system utilizes a structured perceptron with beam search (Collins and Roark, 2004; Huang et al., 2012). To label triggers, the system scans each sentence x, and creates candidate assignments y, that for each token xi assign each possible label yi ∈ L ∪ {⊥} (⊥ meaning xi is not a trigger at all). The score of an assignment (xi , yi ) is calculated as w · f , where f is the binary feature vector calculated for (xi , yi ), and w is the learned feature weight vector. The classifier’s features capture various properties of xi and its context, such as its unigram and its containing bigrams. These features are highly lexicalized, resulting in a very large feature space. Additionally, each feature is"
P16-1119,P15-1034,0,0.0265843,"he previous examples were of relative clauses, Figure 1 demonstrates this distinction in various other syntactic constructions. Identifying and removing non-restrictive modifiers yields shorter NP arguments, which proved beneficial in many NLP tasks. In the context of abstractive summarization (Ganesan et al., 2010) or sentence compression (Knight and Marcu, 2002), non-restrictive modifiers can be removed to shorten sentences, while restrictive modification should be preserved. Further, recent work in information extraction showed that shorter arguments can be beneficial for downstream tasks. Angeli et al. (2015) built an Open-IE system which focuses on shorter argument spans, and demonstrated its usefulness in a state-of-the-art Knowledge Base Population system. Stanovsky et al. (2015) compared the performance of several off-the-shelf analyzers in different semantic tasks. Most relevant to this work is the comparison between Open-IE and Semantic Role Labeling (Carreras and M`arquez, 2005). Specifically, they suggest that SRL’s longer arguments introduce noise which hurts performance for downstream tasks. Finally, in question answering, omitting nonrestrictive modification can assist in providing more"
P16-1119,P98-1013,0,0.234061,"Missing"
P16-1119,W13-2322,0,0.0413898,"Missing"
P16-1119,W05-0620,0,0.142981,"Missing"
P16-1119,W08-1301,0,0.0636439,"Missing"
P16-1119,W14-5601,0,0.38984,"relevant to this work is the comparison between Open-IE and Semantic Role Labeling (Carreras and M`arquez, 2005). Specifically, they suggest that SRL’s longer arguments introduce noise which hurts performance for downstream tasks. Finally, in question answering, omitting nonrestrictive modification can assist in providing more concise answers, or in matching between multiple answer occurrences. Despite these benefits, there is currently no consistent large scale annotation of restrictiveness, which hinders the development of automatic tools for its classification. In prior art in this field, Dornescu et al. (2014) used trained annotators to mark restrictiveness in a large corpus. Although they reached good agreement levels in restrictiveness annotation, their corpus suffered from inconsistencies, since it conflated restrictiveness annotation with inconsistent modifier span annotation. The contributions of this work are twofold. Primarily, we propose a novel crowdsroucing anno1256 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1256–1265, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tation methodology which decouples"
P16-1119,P05-1045,0,0.0842967,"Missing"
P16-1119,C10-1039,0,0.0110114,"sident Obama who just came into the room” (Huddleston et al., 2002; Fabb, 1990; Umbach, 2006). The distinction between the two types is semantic in nature and relies heavily on the context of the NP. Evidently, many syntactic constructions can appear in both restrictive and non-restrictive uses. While the previous examples were of relative clauses, Figure 1 demonstrates this distinction in various other syntactic constructions. Identifying and removing non-restrictive modifiers yields shorter NP arguments, which proved beneficial in many NLP tasks. In the context of abstractive summarization (Ganesan et al., 2010) or sentence compression (Knight and Marcu, 2002), non-restrictive modifiers can be removed to shorten sentences, while restrictive modification should be preserved. Further, recent work in information extraction showed that shorter arguments can be beneficial for downstream tasks. Angeli et al. (2015) built an Open-IE system which focuses on shorter argument spans, and demonstrated its usefulness in a state-of-the-art Knowledge Base Population system. Stanovsky et al. (2015) compared the performance of several off-the-shelf analyzers in different semantic tasks. Most relevant to this work is"
P16-1119,D15-1076,0,0.2681,"me of the harder, more context-dependent, cases (most notably, prepositional and adjectival modifiers), our system provides an applicable means for identifying nonrestrictive modification in a realistic NLP setting. 2 Background In this section we cover relevant literature from several domains. In Section 2.1 we discuss the established linguistic distinction between restrictive and non-restrictive modification. Following, in Section 2.2 we discuss previous NLP work on annotating and identifying this distinction. Finally, in Section 2.3 we briefly describe the recent QASRL annotation paradigm (He et al., 2015), which we utilize in Section 3 as part of our annotation scheme. 2.1 Non-Restrictive Modification Throughout the paper we follow Huddleston et al.’s (2002) well-known distinction between two types of NP modifiers: (1) Restrictive modifiers, for which the content of the modifier is an integral part of the meaning of the containing NP, and, in contrast, (2) Non-restrictive modifiers, that present a separate, parenthetical unit of information about the NP. While some syntactic modifiers (such as determiners or genitives) are always restrictive, others are known to appear in both restrictive as w"
P16-1119,J07-3004,0,0.105647,"Missing"
P16-1119,P10-1022,0,0.018824,"Uses a CRF model to classify each modifier as either restrictive or nonrestrictive, based on the features listed in Table 2, 8 While linguistic literature generally regards appositives as non-restrictive, some of the appositions marked in the dependency conversion are in fact misclassified coordinations, which explains why some of them were marked as restrictive. and elaborated below.9 5.1 Baselines We begin by replicating the algorithms in the two prior works discussed in Section 2.2. This allows us to test their performance consistently against our new human annotated dataset. Replicating (Honnibal et al., 2010) They annotated a modifier as restrictive if and only if it was preceded with a comma. We re-implement this baseline and classify all of the modifiers in the test set according to this simple property. Replicating (Dornescu et al., 2014) Their best performing ML-based algorithm10 uses the supervised CRFsuite classifier (Okazaki, 2007) over “standard features used in chunking, such as word form, lemma and part of speech tags”. Replicating their baseline, we extract the list of features detailed in Table 2 (in the row labeled “chunking features”). 5.2 Our Classifier In addition to Dornescu et al"
P16-1119,W08-2123,0,0.0184913,"on-Answer pairs (discussed in Section 2.3), and keep their train / dev / test split into 744 / 249 / 248 sentences, respectively. This conveniently allows us to link between argument NPs and their corresponding argument role question needed for our annotation process, as described in previous section. This dataset is composed of 1241 sentences from the CoNLL 2009 English dataset (Hajiˇc et al., 2009), which consists of newswire text annotated by the Penn TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004), and converted into dependency grammar by (Johansson and Nugues, 2008). As mentioned in Section 3.2, each of our annotation instances is composed of a sentence s, a verbal predicate v, a noun phrase p, and a modifier m. We extract each such possible tuple from the set of sentences in the following automatic manner: 1. Identify a verb v in the gold dependency tree. 2. Follow its outgoing dependency arcs to a noun phrase argument p (a dependent of v with a nominal part of speech). 3. Find m, a modifying clause of p which might be non-restrictive, according to the rules described in Table 1, under the “Identified By” column. This filters out modifiers which are alw"
P16-1119,J93-2004,0,0.0580913,"ementary, respectively). In the sentence marked (RC1), the highlighted relative clause is restrictive, distinguishing the necklace being referred to from other necklaces, while in sentence (RC2), the relative clause does not pick an entity from a larger set, but instead presents separate information about an already specified definite entity. 2.2 Non-Restrictive Modification in NLP Syntactic and semantic annotations generally avoid the distinction between restrictive and nonrestrictive modification (referred here as “restrictiveness” annotation). The syntactic annotation of the Penn TreeBank (Marcus et al., 1993) and its common conversion to dependency trees (e.g., (de Marneffe and Manning, 2008)) do not differentiate the cases discussed above, providing the same syntactic structure for the semantically different instances. See Figure 2 for an example. Furthermore, prominent semantic annotations, 1257 such as PropBank (Palmer et al., 2005), AMR (Banarescu et al., 2013), CCG (Hockenmaier and Steedman, 2007), or FrameNet (Baker et al., 1998), also avoid this distinction. For example, PropBank does not differentiate between such modifiers, treating both types of modification as an integral part of an arg"
P16-1119,W04-2705,0,0.0341668,"We use the dataset which He et al. (2015) annotated with Question-Answer pairs (discussed in Section 2.3), and keep their train / dev / test split into 744 / 249 / 248 sentences, respectively. This conveniently allows us to link between argument NPs and their corresponding argument role question needed for our annotation process, as described in previous section. This dataset is composed of 1241 sentences from the CoNLL 2009 English dataset (Hajiˇc et al., 2009), which consists of newswire text annotated by the Penn TreeBank (Marcus et al., 1993), PropBank (Palmer et al., 2005), and NomBank (Meyers et al., 2004), and converted into dependency grammar by (Johansson and Nugues, 2008). As mentioned in Section 3.2, each of our annotation instances is composed of a sentence s, a verbal predicate v, a noun phrase p, and a modifier m. We extract each such possible tuple from the set of sentences in the following automatic manner: 1. Identify a verb v in the gold dependency tree. 2. Follow its outgoing dependency arcs to a noun phrase argument p (a dependent of v with a nominal part of speech). 3. Find m, a modifying clause of p which might be non-restrictive, according to the rules described in Table 1, und"
P16-1119,J05-1004,0,0.684328,"ite entity. 2.2 Non-Restrictive Modification in NLP Syntactic and semantic annotations generally avoid the distinction between restrictive and nonrestrictive modification (referred here as “restrictiveness” annotation). The syntactic annotation of the Penn TreeBank (Marcus et al., 1993) and its common conversion to dependency trees (e.g., (de Marneffe and Manning, 2008)) do not differentiate the cases discussed above, providing the same syntactic structure for the semantically different instances. See Figure 2 for an example. Furthermore, prominent semantic annotations, 1257 such as PropBank (Palmer et al., 2005), AMR (Banarescu et al., 2013), CCG (Hockenmaier and Steedman, 2007), or FrameNet (Baker et al., 1998), also avoid this distinction. For example, PropBank does not differentiate between such modifiers, treating both types of modification as an integral part of an argument NP. Two recent works have focused on automatically identifying non-restrictive modifications. Honnibal et al. (2010) added simple automated restrictiveness annotations to NP-modifiers in the CCGbank (Hockenmaier and Steedman, 2007). Following a writing style and grammar rule, a modifier was judged as non-restrictive if and on"
P16-1119,P15-2050,1,0.812718,"Missing"
P16-1119,C98-1013,0,\N,Missing
P16-1119,W09-1201,0,\N,Missing
P16-1226,W11-2501,0,0.716054,"heir path-based features. 4 Higher-dimensional embeddings seem not to improve performance, while hurting the training runtime. 2392 resource WordNet DBPedia Wikidata Yago relations instance hypernym, hypernym type subclass of, instance of subclass of random split lexical split train 49,475 20,335 test 17,670 6,610 val 3,534 1,350 all 70,679 28,295 Table 2: The number of instances in each dataset. Table 1: Hypernymy relations in each resource. 4 4.1 Dataset Creating Instances Neural networks typically require a large amount of training data, whereas the existing hypernymy datasets, like BLESS (Baroni and Lenci, 2011), are relatively small. Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least"
P16-1226,E12-1004,0,0.566917,"2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set"
P16-1226,W09-4405,0,0.017932,"connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example d"
P16-1226,C92-2082,0,0.833402,"distributional methods, the decision whether y is a hypernym of x is based on the distributional representations of these terms. Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms’ embedding vectors. In contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint occurrences of x and y in a corpus. Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while"
P16-1226,W09-2415,0,0.0288938,"Missing"
P16-1226,C08-1051,0,0.0338745,"2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of constructing a dataset based on k"
P16-1226,D10-1108,0,0.185523,"searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the mult"
P16-1226,N15-1098,1,0.838612,"ed hypernymy detection, it is basically designed for classifying specificity level of related terms, rather than hypernymy in particular. Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x⊕~y (Baroni et al., 2012), difference ~y − ~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting. We perform model selection on the validation set to select the best vectors, method and regularization factor (see the appendix). 2394 Path-based Distributional Combined method Snow Snow + Gen HypeNET Path-based SLQS (Santus et al., 2014) Best supervised (concatenation) HypeNET Integrated random split precision recall 0.843 0.452 0.852 0.561 0.811 0.716 0.491 0.737 0.901 0.637 0.913 0.890 F1 0.589 0.676 0.761 0.589 0.746 0.901 lexical split precision recall 0.760 0.438 0.759 0.530 0.691 0.632 0.375 0.610 0.754 0.551 0.809 0.617 F1 0.556 0.624 0.660 0.464 0.637"
P16-1226,P06-1101,0,0.584712,"rnymy. Rather than searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it i"
P16-1226,P15-2047,0,0.0290555,"in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classification the relation should be expressed in the given text, while in hypernymy detection, the goal is to recognize a generic lexical-semantic relation between terms that holds in many contexts. Accordingly, in relation classification a term-pair is represented 3 LSTM-based Hypernymy Detection We present HypeNET, an LSTM-based method for hypernymy detection. We first focus on improving path representation"
P16-1226,P06-2075,1,0.635141,"m (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008). In this paper, we present HypeNET, an integrated path-based and distributional method for hypernymy detection. Inspired by recent progress 2389 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2389–2398, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in relation classification, we use a long shortterm memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of cons"
P16-1226,J06-3003,0,0.0314179,"the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a"
P16-1226,W03-1011,0,0.517909,"use of recurrent neural networks in the related task of relation classification (Section 2.3). 2.1 Distributional Methods Hypernymy detection is commonly addressed using distributional methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifi"
P16-1226,D12-1104,0,0.728597,"ir co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features. Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, “Spelt is a species of wheat” and “Fantasy is a genre of fiction” yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y. A possible solution is to generalize paths by replacing words along the path with their part-of-speech tags or with wild cards, as done in the PATTY system (Nakashole et al., 2012). Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji"
P16-1226,C14-1212,0,0.26541,"mption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations"
P16-1226,P10-1134,0,0.0891516,"the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they co-occur in the corpus. DET parrot is a bird NOUN VERB DET NOUN Figure 1: An example dependency tree of the senten"
P16-1226,D15-1206,0,0.0373208,"ence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): “The [apples]e1 are in the [basket]e2 ”. Here, the relation expressed between the target entities is Content − Container(e1 , e2 ). The shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007). Recently, deep learning techniques showed good performance in capturing the indicative information in such paths. In particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015). While relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classifi"
P16-1226,D14-1162,0,0.0903761,"ical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012), difference ~y −~x (Roller et al., 2014; Weeds et al., 2014), and dot-product ~x · ~y . Using neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014). 2.2 Path-based Methods A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.g. Y such as X, X and other Y). In a later work, Snow et al. (2004) learned to detect hypernymy. Rath"
P16-1226,N13-1008,0,0.275869,"rnymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths. Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.’s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010). A major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxon2390 ATTR NSUBJ by a single dependency path, while in hypernymy detection it is represented by the multiset of all dependency paths in which they c"
P16-1226,E14-1054,0,0.144588,"e methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baroni et al., 2012)"
P16-1226,C14-1097,0,0.351077,"Missing"
P16-1226,E14-4008,0,0.411297,"onal methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus. Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the 1 Our code and data are available in: https://github.com/vered1986/HypeNET most typical linguistic contexts of a hypernym are less informative than those of its hyponyms. More recently, the focus of the distributional approach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term’s embeddings vector: concatenation ~x ⊕~y (Baron"
P16-1226,K15-1018,1,0.863334,"l resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandeˇci´c, 2012) and Yago (Suchanek et al., 2007). All instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015). Term-pairs related by other relations (including hyponymy), are considered as negative instances. Using related rather than random term-pairs as negative instances tests our method’s ability to distinguish between hypernymy and other kinds of semantic relatedness. We maintain a ratio of 1:4 positive to negative pairs in the dataset. Like Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair. 4.2 Random and Lexical Dataset Splits As our primary dataset, we perform standard random splitting, with"
P16-1226,P15-1146,0,0.119629,"Missing"
P16-1226,C16-1138,0,0.0293306,"Missing"
P16-1226,P14-1113,0,\N,Missing
P16-2041,S13-1035,0,0.0207886,"Missing"
P16-2041,C14-1207,0,0.0353119,"Missing"
P16-2041,D15-1113,0,0.0273263,"Missing"
P16-2041,E14-1057,0,0.0438097,"Missing"
P16-2041,Q15-1027,0,0.0333362,"Missing"
P16-2041,P14-2050,1,0.787903,"Missing"
P16-2041,W14-1610,1,0.924145,"Missing"
P16-2041,marelli-etal-2014-sick,0,0.0766008,"Missing"
P16-2041,S07-1009,0,0.115542,"Missing"
P16-2041,P13-1131,1,0.899585,"Missing"
P16-2041,N13-1008,0,0.156982,"Missing"
P16-2041,N15-1118,0,0.0309087,"Missing"
P16-2041,D10-1106,0,0.349506,"Missing"
P16-2041,P07-1058,1,0.840671,"Missing"
P16-2041,W15-1501,1,0.877236,"Missing"
P16-2041,P15-1146,0,0.0611783,"Missing"
P16-2041,D12-1018,1,0.906476,"Missing"
P16-2041,P15-2070,0,0.0961797,"Missing"
P16-2041,P12-2031,1,0.768861,"Missing"
P16-2041,P11-1062,1,\N,Missing
P16-2041,S13-1002,0,\N,Missing
P16-2041,Q13-1015,0,\N,Missing
P16-2041,P13-1158,0,\N,Missing
P16-2041,P14-1061,0,\N,Missing
P16-2041,D13-1160,0,\N,Missing
P16-2077,D11-1142,0,0.416453,"es and their corresponding arguments. Surprisingly, there are no accepted NLP-standards which specify what the “right” span of an argument should be. Semantic representations typically take an inclusive (or maximal) approach: PropBank annotation (Palmer et al., 2005), for example, marks arguments as full constituency subtrees. From an application perspective, this maximal approach ensures that all arguments are indeed embedded within the annotated span, yet it is often not trivial how to accurately recover them. In contrast to this maximal-span approach, Open-IE systems (Etzioni et al., 2008; Fader et al., 2011) put emphasis on extracting readable standalone propositions, typically producing shorter arguments (see examples in Section 2.1). Several recent works have exploited this property, using 1 http://knowitall.github.io/openie Open IE-4 is based on ClearNLPs SRL, allowing for a direct comparison. 2 474 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 474–478, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics a target predicate3 was marked, and are requested to annotate argument role questions (from a restricted gramm"
P16-2077,D15-1076,0,0.431959,"in textual similarity, analogies, and reading comprehension tasks.2 While Open-IE extractors do provide a reduction of argument span, they lack consistency and principled rigor – there is no clear definition for the desired argument span, which is defined defacto by the different implementations. This lack of a common system-independent definition, let alone an annotation methodology, hinders the creation of gold standard argument-span annotation. In this work we propose a concrete argument span reduction criterion and an accompanying annotation procedure, based on the recent QASRL paradigm (He et al., 2015). We show that this criterion can be consistently annotated with high agreement, and that it is intuitive enough to be obtained through crowd-sourcing. As future work, we intend to apply the reduction criterion to other types of predicates (e.g., nomiProminent semantic annotations take an inclusive approach to argument span annotation, marking arguments as full constituency subtrees. Some works, however, showed that identifying a reduced argument span can be beneficial for various semantic tasks. While certain practical methods do extract reduced argument spans, such as in Open-IE , these solu"
P16-2077,D12-1048,0,0.0618862,"(i.e., “Obama”). Notably, different implementations of Open-IE provide an applicable generic way to reduce argument span. Since there are no common guidelines for this task, each Open-IE extractor produces different argument spans. We cover briefly some of the main differences in a few prominent Open-IE systems. ReVerb (Fader et al., 2011) uses part-of-speechbased regular expressions to decide whether a word should be included within an argument span. For example, they move certain light verb compliments and prepositions from the argument to the predicate slot (e.g., “gave a talk at”). OLLIE (Mausam et al., 2012) learns lexical-syntactic patterns and splits extractions across certain prepositions. For example, given “I flew from Paris to Berlin”, OLLIE yields (I; flew; from Paris) and (I; flew; to Berlin). More recently, (Angeli et al., 2015) used natural logic to remove non-integral parts of arguments (e.g., removing the underlined non-restrictive prepositional phrase in “Heinz Fischer of Austria”). 2.2 3 Argument Reduction In this section, we propose annotation criteria and process for obtaining minimal argument spans. Given an original, non-reduced argument, we aim to reduce it to a set of (one or"
P16-2077,J05-1004,0,0.253685,"ystem-dependent, with no commonly accepted standards. In this paper we propose a generic argument reduction criterion, along with an annotation procedure, and show that it can be consistently and intuitively annotated using the recent QA-SRL paradigm. 1 Introduction Representations of predicate-argument structure need to determine the span of predicates and their corresponding arguments. Surprisingly, there are no accepted NLP-standards which specify what the “right” span of an argument should be. Semantic representations typically take an inclusive (or maximal) approach: PropBank annotation (Palmer et al., 2005), for example, marks arguments as full constituency subtrees. From an application perspective, this maximal approach ensures that all arguments are indeed embedded within the annotated span, yet it is often not trivial how to accurately recover them. In contrast to this maximal-span approach, Open-IE systems (Etzioni et al., 2008; Fader et al., 2011) put emphasis on extracting readable standalone propositions, typically producing shorter arguments (see examples in Section 2.1). Several recent works have exploited this property, using 1 http://knowitall.github.io/openie Open IE-4 is based on Cl"
P16-2077,P15-2050,1,0.85204,"Missing"
P16-2077,P15-1034,0,0.0642189,"cover briefly some of the main differences in a few prominent Open-IE systems. ReVerb (Fader et al., 2011) uses part-of-speechbased regular expressions to decide whether a word should be included within an argument span. For example, they move certain light verb compliments and prepositions from the argument to the predicate slot (e.g., “gave a talk at”). OLLIE (Mausam et al., 2012) learns lexical-syntactic patterns and splits extractions across certain prepositions. For example, given “I flew from Paris to Berlin”, OLLIE yields (I; flew; from Paris) and (I; flew; to Berlin). More recently, (Angeli et al., 2015) used natural logic to remove non-integral parts of arguments (e.g., removing the underlined non-restrictive prepositional phrase in “Heinz Fischer of Austria”). 2.2 3 Argument Reduction In this section, we propose annotation criteria and process for obtaining minimal argument spans. Given an original, non-reduced argument, we aim to reduce it to a set of (one or more) smaller arguments, which jointly specify the same answer to the argument’s role question. Formally, given a non-reduced argument a = {w1 , ..., wn }, along with its role question Q(a) with respect to predicate p in sentence s, w"
P17-2056,L16-1699,0,0.106669,"Missing"
P17-2056,W13-2322,0,0.00583018,"e-Kohler (2016), these are the classes containing the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35"
P17-2056,W13-0501,0,0.0192302,"ves (dishonest), future tense (will, won’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sou"
P17-2056,P16-1077,1,0.890518,"uality value. advcl advmod acomp nsubj nsubj ccomp nsubj dobj Don was dishonest when he said he paid taxes mod prop of mod subj comp subj dobj Don was dishonest when he said he paid taxes Figure 2: Dependency tree (top, obtained with spaCy) versus PropS representation (bottom, obtained via the online demo). Note that PropS posits dishonest as the head of said, while the dependency tree obstructs this relation. 5 Evaluation Extending TruthTeller’s lexicon We extended the TruthTeller lexicon of single-word predicates by integrating a large resource of modality markers. Following the approach of Eckle-Kohler (2016), we first induced the modality status of English adjectives and nouns from the subcategorization frames of their German counterparts listed in a large valency lexicon (using the “IMSLex German Lexicon” (Fitschen, 2004) and Google Translate for obtaining the translations2 ). We focused on four modality classes (the classes whfactual and wh/if-factual indicating factuality, and the two classes future-orientation and non-factual, indicating uncertainty)3 and semi-automatically mapped them to the signatures used in TruthTeller. We performed the same kind of mapping for the modality classes of Eng"
P17-2056,W16-5706,0,0.00694973,"n’t), and more. Looking at the numerous and varied possibilities language offers to express all the different shades of modality, it is clear that factuality does not assume any fixed set of discrete values either. Instead, the underlying linguistic system forms a continuous spectrum ranging from factual to counterfactual (Saur´ı and Pustejovsky, 2009). While linguistic theory assigns a spectrum of factuality values, recent years have seen many practical efforts to capture the notion of factuality in a consistent annotation (Saur´ı and Pustejovsky, 2009; Nissim et al., 2013; Lee et al., 2015; OGorman et al., 2016; Minard et al., 2016; Ghia et al., 2016). Each of these make certain deci1 https://github.com/gabrielStanovsky/ unified-factuality 353 Corpus FactBank MEANTIME† UW #Tokens/Sentences 77231 / 3839 9743 / 631 106371 / 4234 Factuality Values Original Our mapping Factual (CT+/-) +3.0 / -3.0 Probable (PR+/-) +2.0 / -2.0 Possible (PS+/-) +1.0 / -1.0 Unknown (Uu/CTu ) 0.0 Fact / Counterfact +3.0 / -3.0 Possibility (uncertain) +1.5 / -1.5 Possibility (future) +0.5 / -0.5 [-3.0, 3.0] Type Annotators Perspective Discrete Experts Author’s and discourse-internal sources Discrete Experts Author’s Continuou"
P17-2056,D15-1136,0,0.0405415,"ining the majority of the verb types. 4 https://spacy.io Note that in our case this ranges between 0 (perfect performance) and 6 (worst performance). 6 UW used the IBM CPLEX Optimizer 5 355 library7 ). All hyperparameters were tuned on the development set. Dataset All-factual UW feat.† AMR Rule-based Supervised Semantic representation approach In addition to the rule-based and supervised approaches, we experimented with a semantic abstraction of the sentence. For that end, we extracted features inspired by the UW system on the popular AMR formalism (Banarescu et al., 2013) using a SoA parser (Pust et al., 2015). Our hope was that this would improve performance by focusing on the more semantically-significant portions of the predicate-argument structure. In particular, we extracted the following features from the predicted AMR structures: immediate parents, grandparents and siblings of the target node, lemma and POS tag of the target and preceding token in the sentence, and a Boolean feature based on the AMR polarity role (indicating semantic negation). MEANTIME MAE r MAE .80 .81 .66 .75 .59 0 .66 .66 .62 .71 .78 .51 .64 .72 .42 0 .71 .58 .63 .66 .31 .56 .44 .35 .34 r 0 .33 .30 .23 .47 is due to its"
P17-2056,J12-2002,0,0.0450076,"Missing"
P17-2056,S12-1020,0,0.315353,"d into rule-based systems which examine deep linguistic features, and machine learning algorithms which generally extract more shallow features. The De Facto factuality profiler (Saur´ı and Pustejovsky, 2012) and TruthTeller algorithms (Lotan et al., 2013) take the rule-based approach and assign a discrete annotation of factuality (following the values assigned by FactBank) using a deterministic rule-based topdown approach on dependency trees, changing the factuality assessment when encountering factuality affecting predicates or modality and negation cues (following implicative signatures by Karttunen (2012)). In addition to a factuality assessment, TruthTeller assigns three values per predicate in the sentence: (1) implicative signature from a hand-coded lexicon indicating how this predicate changes the factuality of its embedded clause, in positive and negative contexts, (2) clause truth, marking the factuality assessment of the entire clause, and (3) negation and uncertainty, indicating whether this predicate is affected by negation or modality. Both of these algorithms rely on a hand-written lexicon of predicates, indicating how they modify the factuality status of their embedded predicates ("
P17-2056,D15-1189,0,0.276512,"se, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotations. Our contribution in this work is twofold. First, we suggest that the task can benefit from a unified rep"
P17-2056,N13-1091,1,0.814565,"e, in knowledge base population, only propositions marked as factual should be admitted into the knowledge base, while hypothetical or negated ones should be left out. Similarly, for argumentation analysis and question answering, factuality can play a major role in backing a specific claim or supporting evidence for an answer to a question at hand. Recent research efforts have approached the factuality task from two complementing directions: automatic prediction and large scale annotation. Previous attempts for automatic factuality prediction either took a rule-based, deep syntactic approach (Lotan et al., 2013; Saur´ı and Pustejovsky, 2012) or a machine learning approach over more shallow features (Lee et al., 2015). In terms of annotation, each effort was largely carried out independently of the others, picking up different factuality flavors and different annotation scales. In correlation, the proposed algorithms have targeted a single annotated resource which they aim to recover. Subsequently, this separation between annotated corpora has prevented a comparison across datasets. Further, the models are nonportable, inhibiting advancements in one dataset to carry over to any of the other annotatio"
P17-2056,W09-3714,0,0.0913577,"Missing"
P18-1111,P01-1008,0,0.465559,"Missing"
P18-1111,W09-2416,0,0.0261477,"Missing"
P18-1111,W15-0122,0,0.09736,"e constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone is labeled as CONTAINED (zone contains business), although it could also be labeled as PURPOSE (zone whose purpose is business). 2.2 Noun-compound Paraph"
P18-1111,D11-1060,0,0.026239,"plausible human-written paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not"
P18-1111,N15-1098,1,0.846239,"Missing"
P18-1111,S10-1051,0,0.180419,"e expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The approach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositi"
P18-1111,E17-1083,0,0.035575,"pˆi = 78 M LPw M LPp [w1 ] cake [p] apple (23) made (23) made (1) [w1 ] (28) apple (2) [w2 ] (28) apple (4145) cake ... (3) [p] (4145) cake ... (1) [w1 ] (7891) of (3) [p] (7891) of (2) [w2 ] (78) [w2 ] containing [w1 ] ... (131) [w2 ] made of [w1 ] ... Figure 1: An illustration of the model predictions for w1 and p given the triplet (cake, made of, apple). The model predicts each component given the encoding of the other two components, successfully predicting ‘apple’ given ‘cake made of [w1 ]’, while predicting ‘[w2 ] containing [w1 ]’ for ‘cake [p] apple’. 2001; Ganitkevitch et al., 2013; Mallinson et al., 2017). If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead. Another related task is Open Information Extraction (Etzioni et al., 2008), whose goal is to extract relational tuples from text. Most system focus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. Pal and Mausam (2016) focused on segmenting multi-word nouncompounds and assumed an is-a relation between the parts, as extracting (Francis Coll"
P18-1111,P07-1072,0,0.241429,"86/panic 1200 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200–1211 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 2.1 Background Noun-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decisio"
P18-1111,S13-2025,0,0.375031,"Missing"
P18-1111,S10-1052,0,0.0259719,"proach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositional distributional vector (Mitchell and Lapata, 2010) and used it to predict paraphrases from the corpus. Similar noun-compounds are expected to have similar distributional representations and therefore yield the same paraphrases. For example, if the corpus does not contain paraphrases for plastic spoon, the model may predict the paraphrases of a similar compound such as steel knife. In terms of sharing information between semantically-similar paraphrases, Nulty and Costello (2010) and Surtani et al. (2013) learned “is-a” relations between paraphrases from the co-occurrences of various paraphrases with each other. For example, the specific ‘[w2 ] extracted from [w1 ]’ template (e.g. in the context of olive oil) generalizes to ‘[w2 ] made from [w1 ]’. One of the drawbacks of these systems is that they favor more frequent paraphrases, which may co-occur with a wide variety of more specific paraphrases. 2.3 Noun-compounds in other Tasks Noun-compound paraphrasing may be considered as a subtask of the general paraphrasing task, whose goal is to generate, given a text fragme"
P18-1111,E09-1071,0,0.383901,"Missing"
P18-1111,W16-1307,0,0.0260755,"n ‘cake made of [w1 ]’, while predicting ‘[w2 ] containing [w1 ]’ for ‘cake [p] apple’. 2001; Ganitkevitch et al., 2013; Mallinson et al., 2017). If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead. Another related task is Open Information Extraction (Etzioni et al., 2008), whose goal is to extract relational tuples from text. Most system focus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. Pal and Mausam (2016) focused on segmenting multi-word nouncompounds and assumed an is-a relation between the parts, as extracting (Francis Collins, is, NIH director) from “NIH director Francis Collins”. Xavier and Lima (2014) enriched the corpus with compound definitions from online dictionaries, for example, interpreting oil industry as (industry, produces and delivers, oil) based on the WordNet definition “industry that produces and delivers oil”. This method is very limited as it can only interpret noun-compounds with dictionary entries, while the majority of English noun-compounds don’t have them (Nakov, 2013"
P18-1111,N07-1071,0,0.0470745,"previous methods, we train the model to predict either a paraphrase expressing the semantic relation of a noun-compound (predicting ‘[w2 ] made of [w1 ]’ given ‘apple cake’), or a missing constituent given a combination of paraphrase and noun-compound (predicting ‘apple’ given ‘cake made of [w1 ]’). Constituents and paraphrase templates are represented as continuous vectors, and semantically-similar paraphrase templates are embedded in proximity, enabling better generalization. Interpreting ‘parsley cake’ effectively reduces to identifying paraphrase templates whose “selectional preferences” (Pantel et al., 2007) on each constituent fit ‘parsley’ and ‘cake’. A qualitative analysis of the model shows that the top ranked paraphrases retrieved for each noun-compound are plausible even when the constituents never co-occur (Section 4). We evaluate our model on both the paraphrasing and the classification tasks (Section 5). On both tasks, the model’s ability to generalize leads to improved performance in challenging evaluation settings.1 1 The code is available at github.com/vered1986/panic 1200 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200"
P18-1111,N15-1037,0,0.0202049,"nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus"
P18-1111,P17-1192,0,0.013017,", and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The ap"
P18-1111,D14-1162,0,0.0865276,"Missing"
P18-1111,I11-1024,0,0.0713183,"ply a spoon made of silver and that monkey business is a business that buys or raises monkeys. In other cases, it seems that the strong prior on one constituent leads to ignoring the other, unrelated constituent, as in predicting “wedding made of diamond”. Finally, the “unrelated” paraphrase was predicted for a few compounds, but those are not necessarily non-compositional (application form, head teacher). We conclude that the model does not address compositionality and suggest to apply it only to compositional compounds, which may be recognized using compositionality prediction methods as in Reddy et al. (2011). 7 Our paraphrasing approach at its core assumes compositionality: only a noun-compound whose meaning is derived from the meanings of its constituent words can be rephrased using them. In §3.2 we added negative samples to the training data to simulate non-compositional nouncompounds, which are included in the classification dataset (§5.2). We assumed that these compounds, more often than compositional ones would consist of unrelated constituents (spelling bee, sacred cow), and added instances of random unrelated nouns with ‘[w2 ] is unrelated to [w1 ]’. Here, we assess whether our model succe"
P18-1111,N18-2035,1,0.854521,"ing of shares” instead of “holding of share”). 5.2 Classification Noun-compound classification is defined as a multiclass classification problem: given a pre-defined set of relations, classify w1 w2 to the relation that holds between w1 and w2 . Potentially, the corpus co-occurrences of w1 and w2 may contribute to the classification, e.g. ‘[w2 ] held at [w1 ]’ indicates a TIME relation. Tratz and Hovy (2010) included such features in their classifier, but ablation tests showed that these features had a relatively small contribution, probably due to the sparseness of the paraphrases. Recently, Shwartz and Waterson (2018) showed that paraphrases may contribute to the classification when represented in a continuous space. 1206 Model. We generate a paraphrase vector representation par(w ~ 1 w2 ) for a given noun-compound w1 w2 as follows. We predict the indices of the k most likely paraphrases: pˆ1 , ..., pˆk = argmaxk pˆ, where pˆ is the distribution on the paraphrase vocabulary Vp , as defined in Equation 1. We then encode each paraphrase using the biLSTM, and average the paraphrase vectors, weighted by their confidence scores in pˆ: Pk pˆi ˆpˆi · V~p i=1 p par(w ~ (3) Pk 1 w2 ) = ˆpˆi i=1 p We train a linear"
P18-1111,D12-1110,0,0.707096,"ion Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone is labeled as CONTAIN"
P18-1111,S13-2028,0,0.639342,"vide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases in the corpus or have just a few. The approach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compositional distributional ve"
P18-1111,P10-1070,0,0.833202,"roceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1200–1211 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 2.1 Background Noun-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation invento"
P18-1111,S13-2026,0,0.341434,"Missing"
P18-1111,S13-2027,0,0.756238,"e understanding tasks, especially given the prevalence of nouncompounds in English (Nakov, 2013). The interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relationships (e.g. Nastase and Szpakowicz, 2003) or by generating various free text paraphrases that describe the relation in a more expressive manner (e.g. Hendrickx et al., 2013). Methods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013). Such methods are unable to generalize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge. For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job. We follow the paraphrasing approach and propose a semi-supervised model for paraphrasing noun-compounds. Differently from previous methods, we train the model to predict either a paraphrase expressing the"
P18-1111,S10-1058,0,0.364337,"atural language understanding tasks, especially given the prevalence of nouncompounds in English (Nakov, 2013). The interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relationships (e.g. Nastase and Szpakowicz, 2003) or by generating various free text paraphrases that describe the relation in a more expressive manner (e.g. Hendrickx et al., 2013). Methods dedicated to paraphrasing nouncompounds usually rely on corpus co-occurrences of the compound’s constituents as a source of explicit relation paraphrases (e.g. Wubben, 2010; Versley, 2013). Such methods are unable to generalize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text (Kim and Baldwin, 2007), and humans easily interpret the meaning of a new noun-compound by generalizing existing knowledge. For example, consider interpreting parsley cake as a cake made of parsley vs. resignation cake as a cake eaten to celebrate quitting an unpleasant job. We follow the paraphrasing approach and propose a semi-supervised model for paraphrasing noun-compounds. Differently from previous methods, we train the model to predict either a paraphras"
P18-1111,xavier-lima-2014-boosting,0,0.138367,"en paraphrases for each nouncompound, and systems had to rank them with the goal of high correlation with human judgments. In SemEval 2013 task 4 (Hendrickx et al., 2013), systems were expected to provide a ranked list of paraphrases extracted from free text. Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate paraphrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases (Kim and Nakov, 2011; Xavier and Lima, 2014; Pasca, 2015; Pavlick and Pasca, 2017), while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts (Wubben, 2010; Li et al., 2010; Surtani et al., 2013; Versley, 2013) or the distributional representations of the nouncompounds (Van de Cruys et al., 2013). One of the challenges of this approach is the ability to generalize. If one assumes that sufficient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any paraphrases i"
P18-1111,C10-1142,0,0.524897,"un-compound Classification Noun-compound classification is the task concerned with automatically determining the semantic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations. Early work on the task leveraged information derived from lexical resources and corpora (e.g. ´ S´eaghdha and Copestake, 2009; Girju, 2007; O Tratz and Hovy, 2010). More recent work broke the task into two steps: in the first step, a nouncompound representation is learned from the distributional representation of the constituent words (e.g. Mitchell and Lapata, 2010; Zanzotto et al., 2010; Socher et al., 2012). In the second step, the noun-compound representations are used as feature vectors for classification (e.g. Dima and Hinrichs, 2015; Dima, 2016). The datasets for this task differ in size, number of relations and granularity level (e.g. Nastase and Szpakowicz, 2003; Kim and Baldwin, 2007; Tratz and Hovy, 2010). The decision on the relation inventory is somewhat arbitrary, and subsequently, the inter-annotator agreement is relatively low (Kim and Baldwin, 2007). Specifically, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), business zone"
P18-1201,W13-2322,0,0.048347,"for Computational Linguistics (Long Papers), pages 2160–2170 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Event Mention Example: dispatching is the trigger of a Transport-Person event with four arguments: the solid lines show the event annotations for the sentence while the dotted lines show the Abstract Meaning Representation parsing output. patching is the trigger for the event mention of type Transport Person and in E2, conflict is the trigger for the event mention of type Attack. We make use of Abstract Meaning Representations (AMR) (Banarescu et al., 2013) to identify the candidate arguments and construct event mention structures as shown in Figure 2 (top). Figure 2 (bottom) also shows event type structures defined in the Automatic Content Extraction (ACE) guideline.2 We can see that a trigger and its event type name usually have some shared meaning. Furthermore, their structures also tend to be similar: a Transport Person event typically involves a Person as its patient role, while an Attack event involves a Person or Location as an Attacker. This observation matches the theory by Pustejovsky (1991): “the semantics of an event structure can be"
P18-1201,N07-4013,0,0.12327,"Missing"
P18-1201,P08-1004,0,0.122949,"Missing"
P18-1201,P15-2061,1,0.915193,"Missing"
P18-1201,P15-1017,0,0.661906,"Missing"
P18-1201,P16-2011,1,0.921764,"Missing"
P18-1201,C16-1017,0,0.0883192,"Missing"
P18-1201,P11-1163,0,0.389091,"Missing"
P18-1201,P11-1113,0,0.557271,"Missing"
P18-1201,P16-1025,1,0.911858,"Missing"
P18-1201,D09-1013,0,0.0959611,"Missing"
P18-1201,N16-1034,1,0.915997,"Missing"
P18-1201,P08-1030,1,0.902064,"Missing"
P18-1201,P15-2060,0,0.36822,"Missing"
P18-1201,P11-1115,1,0.767935,"Missing"
P18-1201,K17-1034,0,0.0861971,"Missing"
P18-1201,P13-1008,1,0.94998,"Missing"
P18-1201,P10-1081,0,0.668857,"Missing"
P18-1201,C10-2087,0,0.0604086,"Missing"
P18-1201,D16-1038,0,0.0734626,"Missing"
P18-1201,D16-1087,0,0.0557832,"Missing"
P18-1201,D11-1001,1,0.900064,"Missing"
P18-1201,P16-1201,0,0.0472642,"Missing"
P18-1201,P06-2094,0,0.0941745,"Missing"
P18-1201,P12-1088,0,0.128376,"Missing"
P18-1201,N06-1039,0,0.0621108,"Missing"
P18-1201,D13-1170,0,0.00421772,"xtraction as a classification problem, by assigning event triggers to event types from a pre-defined fixed set. These methods rely heavily on manual annotations and features specific to each event type, and thus are not easily adapted to new event types without extra annotation effort. Handling new event types may even entail starting over, without being able to re-use annotations from previous event types. To make event extraction effective as new realworld scenarios emerge, we take a look at this task from the perspective of zero-shot learning, ZSL (Frome et al., 2013; Norouzi et al., 2013; Socher et al., 2013a). ZSL, as a type of transfer learning, makes use of separate, pre-existing classifiers to build a semantic, cross-concept space that maps between their respective classes. The resulting shared semantic space then allows for building a novel “zero-shot” classifier, i,e,, requiring no (zero) additional training examples, to handle unseen cases. We observe that each event mention has a structure consisting of a candidate trigger and arguments, with corresponding predefined name labels for the event type and argument roles. We propose to enrich the semantic representations of each event mention"
P18-1201,W15-0812,0,0.0620811,"Missing"
P18-1201,N15-1040,0,0.0767342,"Missing"
P18-1201,P10-4014,0,0.0766931,"Missing"
P19-1213,D15-1075,0,0.160168,"Missing"
P19-1213,P17-1152,0,0.09963,"Missing"
P19-1213,P18-1063,0,0.0915052,"fter six years this summer. [...] england’s premier league clubs set to leave liverpool after six years this summer. [...] Figure 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent prob"
P19-1213,N16-1012,0,0.10932,"Missing"
P19-1213,D17-1070,0,0.0698726,"Missing"
P19-1213,N19-1423,0,0.121144,"Missing"
P19-1213,D18-1443,0,0.126101,"re 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS comp"
P19-1213,P18-1064,0,0.0684475,"otations to be used as additional test data in future work.1 2 Previous work already proposed the use of explicit proposition structures (Cao et al., 2018) and multi-task learning with NLI (Li et al., 2018; Pasunuru et al., 2017) to successfully improve the correctness of abstractive sentence summaries. In this work, we instead focus on the more challenging single-document summarization, where longer summaries allow for more errors. Very recently, Fan et al. (2018) showed that with ideas similar to Cao et al. (2018)’s work, the correctness of document summaries can also be improved. Moreover, Guo et al. (2018) and Pasunuru and Bansal (2018) proposed to use NLI-based loss functions or multi-task learning with NLI for document summarization. But unfortunately, their experiments do not evaluate whether the techniques improve summarization correctness. We are the first to use NLI in a reranking setup, which is beneficial for this study as it allows to us to clearly isolate the net impact of the NLI component. Evaluating Summary Correctness Similar to previous work by Cao et al. (2018) and Li et al. (2018), we argue that the correctness of a generated summary can only be reliably evaluated by manual ins"
P19-1213,N13-1132,0,0.0333089,"for every sentence, we first merge the labels collected from different annotators. A summary then receives the label incorrect if at least one of its sentences has been labeled as such, otherwise, it is labeled as correct. A challenge of crowdsourcing is that workers are untrained and some might produce low quality annotations (Sabou et al., 2014). For our task, an additional challenge is that some errors are rather subtle, while on the other hand the majority of summary sentences are correct, which requires workers to carry out the task very carefully to catch these rare cases. We use MACE (Hovy et al., 2013), a Bayesian model that incorporates the reliability of individual workers, to merge sentence-level labels. We also performed an experiment to determine the necessary number of workers to obtain reliable labels. Two annotators from our lab labeled 50 generated summaries (140 sentences) manually and then merged their labels to obtain a gold standard. For the same data, we collected 14 labels per sentence from crowdworkers. Figure 2 shows the agreement, measured as Cohen’s κ, between the MACE-merged labels of different subsets of the crowdsourced labels and the gold standard. We find that the ag"
P19-1213,C18-1121,0,0.209652,"e hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al., 2018; Li et al., 2018), we observe that factual errors are also a frequent problem for document summarization. Interestingly, the fraction of incorrect summaries is substantially higher for FAS and BUS compared to PGC. The length of the 3 The ROUGE scores have been recomputed by us on the used data and match the reported scores very closely. generated summaries appears to be unrelated to the number of errors. Instead, the higher abstractiveness of summaries produced by FAS and BUS, as analyzed in their respective papers, seems to also increase the chance of introducing errors. In addition, we also observe that amon"
P19-1213,P19-1334,0,0.0289414,"Missing"
P19-1213,K16-1028,0,0.253769,"Missing"
P19-1213,P19-1449,0,0.0253317,"Missing"
P19-1213,D16-1244,0,0.0953935,"Missing"
P19-1213,N18-2102,0,0.0798384,"additional test data in future work.1 2 Previous work already proposed the use of explicit proposition structures (Cao et al., 2018) and multi-task learning with NLI (Li et al., 2018; Pasunuru et al., 2017) to successfully improve the correctness of abstractive sentence summaries. In this work, we instead focus on the more challenging single-document summarization, where longer summaries allow for more errors. Very recently, Fan et al. (2018) showed that with ideas similar to Cao et al. (2018)’s work, the correctness of document summaries can also be improved. Moreover, Guo et al. (2018) and Pasunuru and Bansal (2018) proposed to use NLI-based loss functions or multi-task learning with NLI for document summarization. But unfortunately, their experiments do not evaluate whether the techniques improve summarization correctness. We are the first to use NLI in a reranking setup, which is beneficial for this study as it allows to us to clearly isolate the net impact of the NLI component. Evaluating Summary Correctness Similar to previous work by Cao et al. (2018) and Li et al. (2018), we argue that the correctness of a generated summary can only be reliably evaluated by manual inspection. But in contrast to pre"
P19-1213,W17-4504,0,0.0864398,"Missing"
P19-1213,N18-1202,0,0.0248347,"Missing"
P19-1213,D15-1044,0,0.210206,"Missing"
P19-1213,sabou-etal-2014-corpus,0,0.0132901,"extractive, our interface also highlights the sentence in the source document with the highest word overlap, helping the worker to find the relevant information faster. We pay workers $0.20 per task (labeling all sentences of one summary). Given the correctness labels for every sentence, we first merge the labels collected from different annotators. A summary then receives the label incorrect if at least one of its sentences has been labeled as such, otherwise, it is labeled as correct. A challenge of crowdsourcing is that workers are untrained and some might produce low quality annotations (Sabou et al., 2014). For our task, an additional challenge is that some errors are rather subtle, while on the other hand the majority of summary sentences are correct, which requires workers to carry out the task very carefully to catch these rare cases. We use MACE (Hovy et al., 2013), a Bayesian model that incorporates the reliability of individual workers, to merge sentence-level labels. We also performed an experiment to determine the necessary number of workers to obtain reliable labels. Two annotators from our lab labeled 50 generated summaries (140 sentences) manually and then merged their labels to obta"
P19-1213,P17-1099,0,0.295138,"nd’s first-choice right-back at the world cup looks set to leave liverpool after six years this summer. [...] england’s premier league clubs set to leave liverpool after six years this summer. [...] Figure 3: Examples of incorrect sentences produced by different summarization models on the CNN-DM test set. 4 Correctness of State-of-the-Art Models Using the crowd-based evaluation, we assessed the correctness of summaries for a randomly sampled subset of 100 summaries from the CNN-DM test set. We included three summarization models: PGC The pointer-generator model with coverage as introduced by See et al. (2017). FAS The hybrid extractive-abstractive system proposed by Chen and Bansal (2018) including their redundancy-based reranking. BUS The bottom-up summarization system recently proposed by Gehrmann et al. (2018). To the best of our knowledge, BUS is the state-ofthe-art abstractive model on the non-anonymized version of CNN-DM as of writing this, while FAS is only slightly behind. We use the original generated summaries provided by the authors and crowdsource correctness labels using 9 workers. Table 1 shows the evaluation results3 . In line with the findings for sentence summarization (Cao et al."
P19-1213,N18-1101,0,0.124691,"Missing"
P19-1409,P16-1061,0,0.132519,"tity coreference annotations were first added in EECB, covering both common nouns and named entities. ECB+ increased the difficulty level by adding a second set of documents for each topic (subtopic), discussing a different event of the same type (Tara Reid enters a rehab center vs. Lindsay Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Eve"
P19-1409,W15-0801,0,0.821338,"1, 23, 34, 35; test: 36-45. acquisition”) as predicates and their Arg0. • We use the spaCy dependency parser (Honnibal and Montani, 2017) to identify verbal event mentions whose subject and object are entities, and add those entities as their Arg0 and Arg1 roles, respectively. • Following Lee et al. (2012), for a given event mention, we consider its closest left (right) entity mention as its Arg0 (Arg1) role. 4 Experimental Setup We use the ECB+ corpus, which is the largest dataset consisting of within- and cross-document coreference annotations for entities and events. We follow the setup of Cybulska and Vossen (2015b), which was also employed by KenyonDean et al. (2018). This setup uses a subset of the annotations which has been validated for correctness by Cybulska and Vossen (2014) and allocates a larger portion of the dataset for training (see Table 1). Since the ECB+ corpus only annotates a part of the mentions, the setup uses the gold-standard event and entity mentions rather, and does not require specific treatment for unannotated mentions during evaluation. A different setup was carried out by Yang et al. (2015) and Choubey and Huang (2017). They used the full ECB+ corpus, including parts with kno"
P19-1409,D13-1203,0,0.0714633,"for event coreference. Entity coreference annotations were first added in EECB, covering both common nouns and named entities. ECB+ increased the difficulty level by adding a second set of documents for each topic (subtopic), discussing a different event of the same type (Tara Reid enters a rehab center vs. Lindsay Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 201"
P19-1409,Q15-1002,0,0.0451854,"ach topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Event coreference is considered a more difficult task, mostly due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to arguments, including event participants and spatio-temporal information. Early models employed lexical features (e.g. head lemma,"
P19-1409,H05-1004,0,0.619197,"hey only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of Kenyon-Dean et al. (2018) which is applied across topics. CV (Cybulska and Vossen, 2015a) is a supervised method for event coreference, based on discre"
P19-1409,P14-5010,0,0.00432062,"://github.com/minimaxir/ char-embeddings We set the merging threshold in the training step to δ1 = 0.5. We tune the threshold for inference step on the validation set to δ2 = 0.5. To cluster documents into topics at inference time, we use the K-Means algorithm implemented in ScikitLearn (Pedregosa et al., 2011). Documents are represented using TF-IDF scores of unigrams, bigrams, and trigrams, excluding stop words. We set K = 20 based on the Silhouette Coefficient method (Rousseeuw, 1987), which successfully reconstructs the number of test sub-topics. During inference, we use Stanford CoreNLP (Manning et al., 2014) to initialize within-document entity coreference clusters. Identifying Predicate-Argument Structures. To extract relations between events and entities we follow previous work (Lee et al., 2012; Yang et al., 2015; Choubey and Huang, 2017) and extract predicate-argument structures using SwiRL (Surdeanu et al., 2007), a semantic role labeling (SRL) system. To increase the coverage we apply additional heuristics: • Since SwiRL only identifies verbal predicates, we follow Lee et al. (2012) and consider nominal event mentions with possesors (“Amazon’s 4183 # Topics # Sub-topics # Documents # Senten"
P19-1409,S18-2001,0,0.30852,"a joint neural architecture for CDCR. In our joint model, an event (entity) mention representation is aware of other entities (events) that are related to it by predicateargument structure. We cluster mentions based on a learned pairwise mention coreference scorer. A disjoint variant of our model, on its own, improves upon the previous state-of-the-art for event 4179 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4179–4189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics coreference on the ECB+ corpus (Kenyon-Dean et al., 2018) by 9.5 CoNLL F1 points. To the best of our knowledge, we are the first to report performance on the entity coreference task in ECB+. Our joint model further improves performance upon the disjoint model by 1.2 points for entities and 1 point for events (statistically significant with p < 0.001). Our analysis further shows that each of the mention representation components contributes to the model’s performance.1 2 Background and Related Work Coreference resolution is the task of clustering text spans that refer to the same entity or event. Variants of the task differ on two axes: (1) resolving"
P19-1409,N18-1023,0,0.0290164,"nes: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named entities. The commonly used"
P19-1409,D12-1045,0,0.107405,"Bar-Ilan University 2 Intel AI Lab, Israel 3 Ubiquitous Knowledge Processing Lab, Technische Universitat Darmstadt, Germany {shanyb21,vered1986}@gmail.com, alon.eirew@intel.com {bugert,reimers}@ukp.informatik.tu-darmstadt.de, dagan@cs.biu.ac.il Abstract Recognizing coreferring events and entities across multiple texts is crucial for many NLP applications. Despite the task’s importance, research focus was given mostly to withindocument entity coreference, with rather little attention to the other variants. We propose a neural architecture for cross-document coreference resolution. Inspired by Lee et al. (2012), we jointly model entity and event coreference. We represent an event (entity) mention using its lexical span, surrounding context, and relation to entity (event) mentions via predicate-arguments structures. Our model outperforms the previous state-of-the-art event coreference model on ECB+, while providing the first entity coreference results on this corpus. Our analysis confirms that all our representation elements, including the mention span itself, its context, and the relation to other mentions contribute to the model’s success. 1 Introduction Recognizing that various textual spans acros"
P19-1409,D14-1162,0,0.0898539,"ed clustering configurations that are gradually improved during the training. The training procedure differs from the inference procedure by using the gold standard topic clusters and by initializing the entity clusters with the gold standard within-document coreference clusters. We do so in order to reduce the noise during training. 3.5 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017), using the ADAM optimizer (Kingma and Ba, 2014) with a minibatch size of 16. We initialize the word-level representations to the pretrained 300 dimensional GloVe word embeddings (Pennington et al., 2014), and keep them fixed during training. The character representations are learned using an LSTM with hidden size 50. We initialized them with pre-trained character embeddings4 . Each scorer consists of a sigmoid output layer and two hidden layers with 4261 neurons activated by ReLU function (Nair and Hinton, 2010). 4 Available at https://github.com/minimaxir/ char-embeddings We set the merging threshold in the training step to δ1 = 0.5. We tune the threshold for inference step on the validation set to δ2 = 0.5. To cluster documents into topics at inference time, we use the K-Means algorithm imp"
P19-1409,D17-1018,0,0.311646,"Lohan enters a rehab center). The annotation is not exhaustive, where only a number of salient events and entities in each topic are annotated. 2.2 Models Entity Coreference. Of all the coreference resolution variants, the most well-studied is WD entity coreference resolution (e.g. Durrett and Klein, 2013; Clark and Manning, 2016). The current best performing model is a neural end-to-end system which considers all spans as potential entity 1 The code is available at https://github.com/ shanybar/event_entity_coref_ecb_plus. mentions, and learns distributions over possible antecedents for each (Lee et al., 2017). CD entity coreference has received less attention (e.g. Bagga and Baldwin, 1998b; Rao et al., 2010; Dutta and Weikum, 2015), often addressing the narrower task of entity linking, which links mentions of known named entities to their corresponding knowledge base entries (Shen et al., 2015). Event Coreference. Event coreference is considered a more difficult task, mostly due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to argu"
P19-1409,N18-1202,0,0.045712,"e over the mention’s words. Character-level representations are complementary, and may help with out-of-vocabulary words and spelling variations. We compute them by encoding the span using a character-based LSTM (Hochreiter and Schmidhuber, 1997). The span vector ~s(m) is a concatenation of the wordand character-level vectors. Context. The context surrounding a mention may indicate its compatibility with other candidate mentions (Clark and Manning, 2016; Lee et al., 2017; Kenyon-Dean et al., 2018). To model context, we use ELMo, contextual representations derived from a neural language model (Peters et al., 2018). ELMo has recently improved performance on several challenging NLP tasks, including within-document entity coreference resolution (Lee et al., 2018). We set the context vector ~c(m) to the contextual representation of m’s head word, taking the average of the 3 ELMo layers. Semantic dependency to other mentions. To model dependencies between event and entity clusters, we identify semantic role relationships between their mentions using a semantic role labeling (SRL) system. For a given event mention mvi , we extract its arguments, focusing on 4 semantic roles of interest: Arg0, Arg1, location,"
P19-1409,N18-2108,0,0.0297828,"e for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named entities. The commonly used dataset for CDCR is E"
P19-1409,S18-1009,0,0.0444531,"Missing"
P19-1409,P14-2006,0,0.084834,"known annotation errors. At test time, they rely on the output of a mention extraction tool (Yang et al., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of"
P19-1409,C16-1183,0,0.3126,"setup uses the gold-standard event and entity mentions rather, and does not require specific treatment for unannotated mentions during evaluation. A different setup was carried out by Yang et al. (2015) and Choubey and Huang (2017). They used the full ECB+ corpus, including parts with known annotation errors. At test time, they rely on the output of a mention extraction tool (Yang et al., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well a"
P19-1409,M95-1005,0,0.91763,"l., 2015). To address the partial annotation of the corpus, they only evaluated their systems on the subset of predicted mentions which were also gold mentions. Finally, their evaluation setup was criticized by Upadhyay et al. (2016) for ignoring singletons (cluster with a single mention), effectively making the task simpler; and for evaluating each sub-topic separately, which entails ignoring incorrect coreference links across sub-topics. Evaluation Metrics. We use the official CoNLL scorer (Pradhan et al., 2014),5 and report the performance on the common coreference resolution metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998a), CEAF-e (Luo, 2005), and CoNLL F1 , the average of the 3 metrics. 5 Baselines We compare our full model to published results on ECB+, available for event coreference only, as well as to a disjoint variant of our model and a deterministic lemma baseline.6 C LUSTER +L EMMA. We first cluster the documents to topics (Section 3.3), and then group mentions within the same document cluster which share the same head lemma. This baseline differs from the lemma baseline of Kenyon-Dean et al. (2018) which is applied across topics. CV (Cybulska and Vossen, 2015a) is a super"
P19-1409,Q18-1021,0,0.0378243,"ple, consider the following news headlines: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and un"
P19-1409,Q15-1037,0,0.644563,"y due to the more complex structure of event mentions. While entity mentions are mostly noun phrases, event mentions may consist of a verbal predicate (acquire) or a nominalization (acquisition), where these are attached to arguments, including event participants and spatio-temporal information. Early models employed lexical features (e.g. head lemma, WordNet synsets, word embedding similarity) as well as structural features (e.g. aligned arguments) to compute distances between event mentions and decide whether they belong to the same coreference cluster (e.g. Bejan and Harabagiu, 2010, 2014; Yang et al., 2015). More recent work is based on neural networks. Choubey and Huang (2017) alternate between WD and CD clustering, each step relying on previous decisions. The decision to link two event mentions is made by the pairwise WD and CD scorers. Mention representations rely on pre-trained word embeddings, contextual information, and features related to the event’s arguments. Kenyon-Dean et al. (2018) similarly encode event mentions using lexical and contextual features. Differently from Choubey and Huang (2017), they do not cluster documents to topics as a pre-processing step. Instead, they encode the"
P19-1409,D18-1259,0,0.0245381,"llowing news headlines: 1. 2018 Nobel prize for physics goes to Donna Strickland 2. Prof. Strickland is awarded the Nobel prize for physics Both sentences refer to the same entities (Donna Strickland and the Nobel prize for physics) and the same event (awarding the prize), using different words. In coreference resolution, the goal is to cluster expressions that refer to the same entity or event in a text, whether within a single document or across a document collection. Recently, there has been increasing interest in crosstext inferences, for example in question answering (Welbl et al., 2018; Yang et al., 2018; Khashabi et al., 2018; Postma et al., 2018). Such applications would benefit from effective cross-document coreference resolution. Despite the importance of the task, the focus of most coreference resolution research has been on its within-document variant, and rather little on cross-document coreference (CDCR). The latter is sometimes addressed partially using entity linking, which links mentions of an entity to its knowledge base entry. However, cross-document entity coreference is substantially broader than entity linking, addressing also mentions of common nouns and unfamiliar named enti"
P19-1409,C10-2121,0,\N,Missing
P19-1409,J14-2004,0,\N,Missing
P19-1409,P10-1143,0,\N,Missing
P19-1409,cybulska-vossen-2014-using,0,\N,Missing
P91-1017,P85-1037,0,0.0248684,"Missing"
P91-1017,J90-1003,0,0.0608918,"Missing"
P91-1017,P90-1034,0,0.0514113,"Missing"
P91-1017,H90-1052,0,0.0421261,"Missing"
P91-1017,C90-1005,0,0.034546,"Missing"
P91-1017,C88-1016,0,\N,Missing
P91-1017,J90-2002,0,\N,Missing
P91-1048,J90-1003,0,0.020919,"Missing"
P91-1048,P91-1017,1,\N,Missing
P93-1022,P91-1034,0,0.0753796,"Missing"
P93-1022,J86-3002,0,0.152559,"Missing"
P93-1022,P91-1030,0,0.062056,"Missing"
P93-1022,J90-1003,0,0.0447702,"s such as prepositions and determiners. In the experiments reported here d = 3. A cooccurrence pair can be viewed as a generalization of a bigram, where a bigram is a cooccurrence pair with d = 1 (without ignoring function words). As with bigrams, a cooccurrence pair is directional, i.e. ( x , y ) ¢ (y,x). This captures some information about the asymmetry in the linear order of linguistic relations, such as the fact that verbs tend to precede their objects and follow their subjects. The mutual information of a cooccurrence pair, which measures the degree of association between the two words (Church and Hanks, 1990), is defined as (Fano, 1961): I(x,y) -- log 2 P(x,y) _ log 2 P ( x l y ) (1) P(x)P(y) P(x) = log 2 P(y[x) P(Y) where P ( x ) and P(y) are the probabilities of the events x and y (occurrences of words, in our case) and P(x, y) is the probability of the joint event (a cooccurrence pair). We estimate mutual information values using the Maximum Likelihood Estimator (MLE): P(x,y) I(x, y) = log~ P~x)P--(y) _log~. N f(x,y) ] ( -d f(x)f(y) &quot; (2) where f denotes the frequency of an eyent and N is the length of the corpus. While better estimates for small probabilities are available (Good, 1953; Church"
P93-1022,P93-1024,0,0.156,"Missing"
P93-1022,P91-1017,1,0.605717,"ilarity between w and each word in the lexicon is computationally very expensive (O(12), where I is the size of the lexicon, and O(l J) to do this in advance for all the words in the lexicon). To account for this problem we developed a simple heuristic that searches for words that are potentially similar to w, using thresholds on mutual information values and frequencies of cooccurrence pairs. The search is based on the property that when computing sim(wl, w2), words that have high mutual information values To carry out the evaluation, we implemented a variant of the disambiguation method of (Dagan et al., 1991), for sense disambiguation in machine translation. We term this method as THIS, for Target Word Selection. Consider for example the Hebrew phrase &apos;laxtom xoze shalom&apos;, which translates as &apos;to sign a peace treaty&apos;. The word &apos;laxtom&apos;, however, is ambiguous, and can be translated to either &apos;sign&apos; or &apos;seal&apos;. To resolve the ambiguity, the 5The nominator in our metric resembles the similarity metric in (Hindle, 1990). We found, however, that the difference between the two metrics is important, because the denominator serves as a normalization factor. 168 TWS Augmented T W S Word Frequency Precision"
P93-1022,P90-1032,0,0.0749443,"Missing"
P93-1022,1992.tmi-1.9,0,0.0489281,"Missing"
P93-1022,C90-3063,1,\N,Missing
P93-1022,J92-4003,0,\N,Missing
P93-1022,P90-1034,0,\N,Missing
P94-1038,J92-4003,0,0.0475503,"Missing"
P94-1038,P93-1005,0,0.00927013,"Finally, the similarity-based model m a y be applied to configurations other than bigrams. For trigrams, it is necessary to measure similarity between different conditioning bigrams. This can be done directly, 276 by measuring the distance between distributions of the form P(w31wl, w2), corresponding to different bigrams (wl, w~). Alternatively, and more practically, it would be possible to define a similarity measure between bigrams as a function of similarities between corresponding words in them. Other types of conditional cooccurrence probabilities have been used in probabilistic parsing (Black et al., 1993). If the configuration in question includes only two words, such as P(objectlverb), then it is possible to use the model we have used for bigrams. If the configuration includes more elements, it is necessary to adjust the method, along the lines discussed above for trigrams. for help with his baseline back-off model, and Andre Ljolje and Michael Riley for providing the word lattices for our experiments. Conclusions Similarity-based models suggest an appealing approach for dealing with data sparseness. Based on corpus statistics, they provide analogies between words that often agree with our li"
P94-1038,P93-1022,1,0.847024,"Missing"
P94-1038,H92-1021,0,0.4253,"not possible to estimate probabilities from observed frequencies, a n d s o m e other estimation scheme has to be used. We focus here on a particular kind of configuration, word cooccurrence. Examples of such cooccurrences include relationships between head words in syntactic constructions (verb-object or adjective-noun, for example) and word sequences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability esti272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to e"
P94-1038,H93-1050,0,0.0786007,"Missing"
P94-1038,H91-1055,0,0.0409352,"Missing"
P94-1038,P93-1024,1,0.855009,"Missing"
P94-1038,H92-1027,0,0.0497973,"Missing"
P94-1038,C92-2066,0,\N,Missing
P96-1042,A94-1027,0,0.0569611,"Missing"
P96-1042,A88-1019,0,0.1222,"for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994). This avoids redundantly annotating many examples that contribute roughly the same information to the learner. Our work focuses on sample selection for training probabilistic classifiers. In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)). As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models. We first review the basic approach of committeebased sample selection and its application to partof-speech tagging. This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe. First, we describe the 'simplest' committee-based"
P96-1042,J94-2001,0,0.686836,"ar, the simplest method, which has no parameters to tune, gives excellent results. We also show that sample selection yields a significant reduction in the size of the model used by the tagger. 1 Introduction Many corpus-based methods for natural language processing (NLP) are based on supervised training-acquiring information from a manually annotated corpus. Therefore, reducing annotation cost is an important research goal for statistical NLP. The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994). It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial. Indeed, fully unsupervised training may not be feasible for certain tasks. This paper investigates an approach 319 for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model. In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995). When using sample selection, a learning program e"
P96-1042,J93-1005,0,\N,Missing
P96-1042,A94-1009,0,\N,Missing
P96-1042,P93-1005,0,\N,Missing
P97-1008,J92-4003,0,0.0417534,"Missing"
P97-1008,A88-1019,0,0.0192643,"Missing"
P97-1008,P94-1038,1,0.513543,"e unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(.[wl) induced by Wl on words in V2: ~V(Wl, W~) ~ ~s(~1 ) P,.(w2lwl) = 7P(w2) + Measures of Similarity T o t a l d i v e r g e n c e to t h e a v e r a g e A related measure is based on the total KL divergence to the average of the two distributions: A(wx, W11)= D (w, wl + ) + D (w~[ wl + w~) 2 7)PsiM(W2lWl), where 7 is determined experimentally (Dagan, Pereira, and Lee, 1994). This represents, in effect, a linear combination of the similarity es58 where tion (Wl ÷ w~)/2 shorthand for the distribu2.3.4 Essen and Steinbiss (1992) introduced confusion probability 2, which estimates the probabil(P(.IwJ + P(.Iw~)) ity that word w~ can be substituted for word Wl: Since D('II-) > O, A(Wl,W~) >_O. Furthermore, letting p(w2) = P(w2[wJ, p'(w2) = P(w2lw~) and C : {w2 : p(w2) > O,p'(w2) > O}, it is straightforward to show by grouping terms appropriately that Pc(w lWl) = w(wl, = ~ , P(wllw2)P(w~[w2)P(w2) w2 P(Wl) A(wi,wb= Unlike the measures described above, wl may not neces"
P97-1008,H92-1021,0,0.12642,"defined it must be the case that P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard informa"
P97-1008,W96-0104,0,0.0133278,"at P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the"
P97-1008,P93-1024,1,0.599038,"ge model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in V1. 1To the best of our ""knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993}, has been used by Tishby (p.e.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 57 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (Wl, w2), conditional on the appearance of word wl, is simply PML(W2[wl) -- c(wl, w2) c( i) (1) where c(wl, w2) is the frequency of (wl, w2) in the training corpus and c(wl) is"
P97-1008,P92-1053,0,\N,Missing
P98-1010,W96-0102,0,0.249884,"generalizes only at recognition time. Much work aimed at learning models for full parsing, i.e., learning hierarchical structures. We refer here only to the DOP (Data Oriented Parsing) method (Bod, 1992) which, like the present work, is a memory-based approach. This method constructs parse alternatives for a sentence based on combinations of subtrees in the training corpus. The MBSL approach may be viewed as a linear analogy to DOP in that it constructs a cover for a candidate based on subsequences of training instances. Other implementations of the memory-based paradigm for NLP tasks include Daelemans et al. (1996), for POS tagging; Cardie (1993), for syntactic and semantic tagging; and Stanfill and Waltz (1986), for word pronunciation. In all these works, examples are represented as sets of features and the deduction is carried out by finding the most similar cases. The method presented here is radically different in that it makes use of the raw sequential form of the data, and generalizes by reconstructing test examples from different pieces of the training data. 5 Conclusions We have presented a novel general schema and a particular instantiation of it for learning sequential patterns. Applying the m"
P98-1010,W93-0113,0,0.0345855,"ity and various disambiguation problems. One approach for detecting syntactic patterns is to obtain a full parse of a sentence and then extract the required patterns. However, obtaining a complete parse tree for a sentence is difficult in many cases, and may not be necessary at all for identifying most instances of local syntactic patterns. 67 An alternative approach is to avoid the complexity of full parsing and instead to rely only on local information. A variety of methods have been developed within this framework, known as shallow parsing, chunking, local parsing etc. (e.g., (Abney, 1991; Greffenstette, 1993)). These works have shown that it is possible to identify most instances of local syntactic patterns by rules that examine only the pattern itself and its nearby context. Often, the rules are applied to sentences that were tagged by partof-speech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. However, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and"
P98-1010,W95-0107,0,0.166303,"were tagged by partof-speech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. However, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and expertise that are often not available. As in many areas of NLP, a learning approach is appealing. Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996). This paper presents a novel general learning approach for recognizing local sequential patterns, that may be perceived as falling within the memorybased learning paradigm. The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed). The training data are stored as-is in suffix-tree data structures, which enable linear time searching for subsequences in the corpus. The memory-based nature of the presented algorithm stems from its deduction strategy: a new instance of the target pattern is recognized by"
P98-1010,C92-3126,0,0.0642254,"ke and Omohundro, 1992; Carrasco and Oncina, 1994; Ron et al., 1995). These algorithms differ mainly by their state-merging strategies, used for generalizing from the training data. A major difference between the abovementioned learning methods and our memory-based approach is that the former employ generalized models that were created at training time while the latter uses the training corpus as-is and generalizes only at recognition time. Much work aimed at learning models for full parsing, i.e., learning hierarchical structures. We refer here only to the DOP (Data Oriented Parsing) method (Bod, 1992) which, like the present work, is a memory-based approach. This method constructs parse alternatives for a sentence based on combinations of subtrees in the training corpus. The MBSL approach may be viewed as a linear analogy to DOP in that it constructs a cover for a candidate based on subsequences of training instances. Other implementations of the memory-based paradigm for NLP tasks include Daelemans et al. (1996), for POS tagging; Cardie (1993), for syntactic and semantic tagging; and Stanfill and Waltz (1986), for word pronunciation. In all these works, examples are represented as sets of"
P98-1010,H92-1022,0,0.0110013,". . . . , Con.-2 n / / ] 20000 ~ &apos; ~ 75 ] 40000 Examples 0 i00000 200000 300000 Words 400000 Figure 3: Learning curves for NP, VO, and SV by number of examples (left) and words (right) pattern structure. We aim to incorporate lexical information as well in the future, it is still unclear whether that will improve the results. Figure 3 shows the learning curves by amount of training examples and number of words in the training data, for particular parameter settings. 4 Related Work Two previous methods for learning local syntactic patterns follow the transformation-based paradigm introduced by Brill (1992). Vilain and Day (1996) identify (and classify) name phrases such as company names, locations, etc. Ramshaw and Marcus (1995) detect noun phrases, by classifying each word as being inside a phrase, outside or on the boundary between phrases. Finite state machines (FSMs) are a natural formalism for learning linear sequences. It was used for learning linguistic structures other than shallow syntax. Gold (1978) showed that learning regular languages from positive examples is undecidable in the limit. Recently, however, several learning methods have been proposed for restricted classes of FSM. OST"
P98-1010,P97-1057,0,0.0202016,"0 14271 25024 Table 1: Sizes of training and test data Len 1 2 3 4 5 6 7 8 9 10 &gt;10 total avg. len 4. Compute the candidate score based on the statistics of its covers. 2.2 Searching the training m e m o r y The MBSL scoring algorithm searches the training corpus for each subsequence of the sentence in order to find matching tiles. Implementing this search efficiently is therefore of prime importance. We do so by encoding the training corpus using suffix trees (Edward and McCreight, 1976), which provide string searching in time which is linear in the length of the searched string. Inspired by Satta (1997), we build two suffix trees for retrieving the positive and total counts for a tile. The first suffix tree holds all pattern instances from the training corpus surrounded by bracket symbols and a fixed amount of context. Searching a given tile (which includes a bracket symbol) in this tree yields the positive count for the tile. The second suffix tree holds an unbracketed version of the entire training corpus. This tree is used for searching the POS sequence of a tile, with brackets omitted, yielding the total count for the tile (recall that the negative count is the difference between the tot"
P98-1010,C96-1047,0,0.211176,"ech (POS) and are phrased by some form of regular expressions or finite state automata. Manual writing of local syntactic rules has become a common practice for many applications. However, writing rules is often tedious and time consuming. Furthermore, extending the rules to different languages or sub-language domains can require substantial resources and expertise that are often not available. As in many areas of NLP, a learning approach is appealing. Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996). This paper presents a novel general learning approach for recognizing local sequential patterns, that may be perceived as falling within the memorybased learning paradigm. The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed). The training data are stored as-is in suffix-tree data structures, which enable linear time searching for subsequences in the corpus. The memory-based nature of the presented algorithm stems from its deduction strategy: a new instance of the target pattern is recognized by examining the raw train"
P98-1010,J93-2004,0,\N,Missing
P98-1010,P97-1003,0,\N,Missing
P98-1010,H90-1053,0,\N,Missing
P98-1010,C96-1058,0,\N,Missing
P98-1010,A88-1019,0,\N,Missing
P98-1010,C96-2215,0,\N,Missing
P98-1010,P93-1035,0,\N,Missing
P98-1010,H93-1047,0,\N,Missing
P98-1010,J93-1002,0,\N,Missing
P98-1010,P98-1034,0,\N,Missing
P98-1010,C98-1034,0,\N,Missing
P98-1010,P95-1037,0,\N,Missing
P98-1010,P95-1002,0,\N,Missing
P98-1010,1991.iwpt-1.22,0,\N,Missing
P98-1010,E91-1004,0,\N,Missing
Q15-1016,N09-1003,0,0.472274,"Missing"
Q15-1016,J10-4006,0,0.0160512,"are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, th"
Q15-1016,P14-1023,0,0.9474,"to a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’"
Q15-1016,P12-1015,0,0.291036,"Missing"
Q15-1016,J90-1003,0,0.216701,"exts have been studied (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014a) this work focuses on fixed-window bag-of-words contexts. 2.1 Explicit Representations (PPMI Matrix) The traditional way to represent words in the distributional approach is to construct a highdimensional sparse matrix M , where each row represents a word w in the vocabulary VW and each column a potential context c ∈ VC . The value of each matrix cell Mij represents the association between the word wi and the context cj . A popular measure of this association is pointwise mutual information (PMI) (Church and Hanks, 1990). PMI is defined as the log ratio between w and c’s joint probability and the product of their marginal probabilities, which can be estimated by: P M I(w, c) = log Pˆ (w,c) Pˆ (w)Pˆ (c) = log #(w,c)·|D| #(w)·#(c) The rows of M PMI contain many entries of wordcontext pairs (w, c) that were never observed in the corpus, for which P M I(w, c) = log 0 = −∞. A common approach is thus to replace the M PMI matrix with M0PMI , in which P M I(w, c) = 0 in cases where #(w, c) = 0. A more consistent approach is to use positive PMI (PPMI), in which all 2.3 negative values are replaced by 0: P P M I(w, c)"
Q15-1016,J15-4004,0,0.840752,"Missing"
Q15-1016,P14-2050,1,0.2029,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W14-1618,1,0.194781,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W13-3512,0,0.895955,"Missing"
Q15-1016,W14-1619,1,0.218059,"84 .567 .484 Table 6: The average performance of SVD on word similarity tasks given different values of eig, in the vanilla scenario. pared CBOW to the other methods when setting all the hyperparameters to the defaults provided by word2vec (Table 3). With the exception of MSR’s analogy task, CBOW is not the bestperforming method of any other task in this scenario. Other scenarios showed similar trends in our preliminary experiments. While CBOW can potentially derive better representations by combining the tokens in each context window, this potential is not realized in practice. Nevertheless, Melamud et al. (2014) show that capturing joint contexts can indeed improve performance on word similarity tasks, and we believe it is a direction worth pursuing. 6 Hyperparameter Analysis We analyze the individual impact of each hyperparameter, and try to characterize the conditions in which a certain setting is beneficial. 6.1 Harmful Configurations Certain hyperparameter settings might cripple the performance of a certain method. We observe two scenarios in which SVD performs poorly. SVD does not benefit from shifted PPMI. Setting neg &gt; 1 consistently deteriorates SVD’s performance. Levy and Goldberg (2014c) ma"
Q15-1016,N13-1090,0,0.577456,"meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are “embedded” into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding"
Q15-1016,J07-2002,0,0.021048,"Missing"
Q15-1016,D14-1162,0,0.144387,", and GloVe. For historical reasons, we refer to PPMI and SVD as “countbased” representations, as opposed to SGNS and GloVe, which are often referred to as “neural” or “prediction-based” embeddings. All of these methods (as well as all other “skip-gram”-based embedding methods) are essentially bag-of-words models, in which the representation of each word reflects a weighted bag of context-words that cooccur with it. Such bag-of-word embedding models were previously shown to perform as well as or better than more complex embedding methods on similarity and analogy tasks (Mikolov et al., 2013a; Pennington et al., 2014). Notation We assume a collection of words w ∈ VW and their contexts c ∈ VC , where VW and VC are the word and context vocabularies, and denote the collection of observed word-context pairs as 212 D. We use #(w, c) to denote the number of times the in D. Similarly, #(w) = P pair (w, c) appears P 0 ) and #(c) = 0 #(w, c 0 0 c ∈VC w ∈VW #(w , c) are the number of times w and c occurred in D, respectively. In some algorithms, words and contexts are embedded in a space of d dimensions. In these cases, each word w ∈ VW is associated with a vector w ~ ∈ Rd and similarly each context c ∈ VC is repres"
Q19-1027,Q17-1010,0,0.373338,"ring implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting"
Q19-1027,W13-0104,0,0.549153,"ptures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their"
Q19-1027,D18-1523,0,0.0160504,"and orange (light) points are negative. Non-literality as a rare sense. Nunberg et al. (1994) considered some non-literal compounds as ‘‘idiosyncratically decomposable’’, that is, which can be decomposed to possibly rare senses of their constituents, as in considering bee to have a sense of ‘‘competition’’ in spelling bee and crocodile to stand for ‘‘manipulative’’ in crocodile tears. Using this definition, we could possibly use the NC literality data for word sense induction, in which recent work has shown that contextualized word representations are successful (Stanovsky and Hopkins, 2018; Amrami and Goldberg, 2018). We are interested in testing not only whether the contextualized models are capable of detecting rare senses induced by non-literal usage, which we have confirmed in Section 6, but whether they can also model these senses. To that end, we sample target words that appear in both literal and non-literal examples, and use each contextualized word embedding model as a language model to predict the best substitutes of the target word in each context. Table 7 exemplifies some of these predictions. Bold words are words judged reasonable in the given context, even if they don’t have the exact same m"
Q19-1027,P18-1198,0,0.0949222,"al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical compositi"
Q19-1027,W18-5440,0,0.0262081,"kind of ‘‘black box’’ testing has become popular recently. Adi et al. (2017) studied whether sentence embeddings capture properties such as sentence length and word order. Conneau et al. (2018) extended their work with a large number of sentence embeddings, and tested various properties at the surface, syntactic, and semantic levels. Others focused on intermediate representations in neural machine translation systems (e.g., Shi et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017; Sennrich, 2017), or on specific linguistic properties such as agreement (Giulianelli et al., 2018), and tense (Bacon and Regier, 2018). More recently, Tenney et al. (2019) and Liu et al. (2019) each designed a suite of tasks to test contextualized word embeddings on a broad range of sub-sentence tasks, including part-ofspeech tagging, syntactic constituent labeling, dependency parsing, named entity recognition, semantic role labeling, coreference resolution, semantic proto-role, and relation classification. Tenney et al. (2019) found that all the models produced strong representations for syntactic phenomena, but gained smaller performance improvements upon the baselines in the more semantic tasks. Liu et al. (2019) found th"
Q19-1027,P16-1187,0,0.0203207,"reas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan"
Q19-1027,D10-1115,0,0.098826,"suggesting that the model captures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from"
Q19-1027,P13-2080,1,0.799801,"presentations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning of the phrase can always be composed from its constituent meanings, and it is unclear whether they can incorporate implicit information and new properties"
Q19-1027,I17-1015,0,0.0442621,"Missing"
Q19-1027,E17-1006,0,0.0473181,"years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these"
Q19-1027,N19-1423,0,0.229716,"015), or that olive oil is made of olives while baby oil is made for babies (Shwartz and Waterson, 2018). There has been a line of attempts to learn compositional phrase representations (e.g., Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We constr"
Q19-1027,S13-2025,0,0.0888559,"Missing"
Q19-1027,W16-1604,0,0.160381,"y of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above:"
Q19-1027,W13-3206,0,0.120286,"Missing"
Q19-1027,N19-1112,0,0.0672746,"Missing"
Q19-1027,W18-2501,0,0.0194792,"ord spans or no additional inputs. The classifier output is defined as: (2) o = softmax(W · ReLU(Dropout(h(x)))) (8) Encode. We encode the embedded sequences v1 , . . . , vn and v  1 , . . . , v  l using one of the following three encode variants. As opposed to the pre-trained embeddings, the encoder parameters are updated during training to fit the specific task. where h is a 300-dimensional hidden layer, the dropout probability is 0.2, W ∈ Rk×300 , and k is the number of class labels for the specific task. Implementation Details. We implemented the models using the AllenNLP library (Gardner et al., 2018), which is based on the PyTorch framework (Paszke et al., 2017). We train them for up to 500 epochs, stopping early if the validation performance doesn’t improve in 20 epochs. The phrase type model is a sequence tagging model that predicts a label for each embedded (potentially encoded) word wi . During decoding, we enforce a single constraint that requires that a B-X tag must precede I tag(s). • biLM: Encoding the embedded sequence using a biLSTM with a hidden dimension d, where d is the input embedding dimension: u1 , . . . , un = biLSTM(v1 , . . . , vn ) (3) • Att: Encoding the embedded"
Q19-1027,W18-5426,0,0.0523679,"Missing"
Q19-1027,K15-1035,0,0.0605673,"Missing"
Q19-1027,E09-1071,0,0.0933116,"Missing"
Q19-1027,I11-1024,0,0.225357,"ions The meaning of a light verb construction (LVC, e.g., make a decision) is mainly derived from its noun object (decision), whereas the meaning of its main verb (make) is ‘‘light’’ (Jespersen, 1965). As a rule of thumb, an LVC can be replaced by the verb usage of its direct object noun (decide) without changing the meaning of the sentence. 404 Task Data Source Train/val/test Size Input Output VPC Classification Tu and Roth (2012) 919/209/220 sentence s VP = w1 w2 is VP a VPC? O LVC Classification Tu and Roth (2011) 1521/258/383 sentence s span = w1 ... wk is the span an LVC? O NC Literality Reddy et al. (2011) Tratz (2011) 2529/323/138 sentence s NC = w1 w2 target w ∈ {w1 , w2 } is w literal in NC? A NC Relations SemEval 2013 Task 4 (Hendrickx et al., 2013) 1274/162/130 sentence s NC = w1 w2 paraphrase p does p explicate NC? A AN Attributes HeiPLAS (Hartung, 2015) 837/108/106 sentence s AN = w1 w2 paraphrase p Phrase Type STREUSLE (Schneider and Smith, 2015) 3017/372/376 sentence s does p describe the attribute in AN? label per token Context A O Table 1: A summary of the composition tasks included in our evaluation suite. In the context column, O means the context is part of the original data set,"
Q19-1027,P16-1204,0,0.0198229,"identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning of the phrase can always be composed from its constituent me"
Q19-1027,N15-1099,0,0.0215223,"e classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson,"
Q19-1027,D14-1162,0,0.096227,"entially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) and contextualized word embeddings (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Our contributions are as follows: Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddi"
Q19-1027,N18-1202,0,0.448343,"cs.biu.ac.il Abstract of debate (Hartung, 2015), or that olive oil is made of olives while baby oil is made for babies (Shwartz and Waterson, 2018). There has been a line of attempts to learn compositional phrase representations (e.g., Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et"
Q19-1027,N15-1177,0,0.186715,"Missing"
Q19-1027,D13-1060,0,0.0120932,"ariants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between ab"
Q19-1027,E17-2060,0,0.0176573,"ign a probing task for this property, and build a model that takes the tested representation as an input. This kind of ‘‘black box’’ testing has become popular recently. Adi et al. (2017) studied whether sentence embeddings capture properties such as sentence length and word order. Conneau et al. (2018) extended their work with a large number of sentence embeddings, and tested various properties at the surface, syntactic, and semantic levels. Others focused on intermediate representations in neural machine translation systems (e.g., Shi et al., 2016; Belinkov et al., 2017; Dalvi et al., 2017; Sennrich, 2017), or on specific linguistic properties such as agreement (Giulianelli et al., 2018), and tense (Bacon and Regier, 2018). More recently, Tenney et al. (2019) and Liu et al. (2019) each designed a suite of tasks to test contextualized word embeddings on a broad range of sub-sentence tasks, including part-ofspeech tagging, syntactic constituent labeling, dependency parsing, named entity recognition, semantic role labeling, coreference resolution, semantic proto-role, and relation classification. Tenney et al. (2019) found that all the models produced strong representations for syntactic phenomena"
Q19-1027,D16-1159,0,0.0455928,"Missing"
Q19-1027,W11-0807,0,0.234149,"sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb Constructions The meaning of a light verb construction (LVC, e.g., make a decision) is mainly derived from its noun object (decision), whereas the meaning of its main verb (make) is ‘‘light’’ (Jespersen, 1965). As a rule of thumb, an LVC can be replaced by the verb usage of its direct object noun (decide) without changing the meaning of the sentence. 404 Task Data Source Train/val/test Size Input Output VPC Classification Tu and Roth (2012) 919/209/220 sentence s VP = w1 w2 is VP a VPC? O LVC Classification Tu and Roth (2011) 1521/258/383 sentence s span = w1 ... wk is the span an LVC? O NC Literality Reddy et al. (2011) Tratz (2011) 2529/323/138 sentence s NC = w1 w2 target w ∈ {w1 , w2 } is w literal in NC? A NC Relations SemEval 2013 Task 4 (Hendrickx et al., 2013) 1274/162/130 sentence s NC = w1 w2 paraphrase p does p explicate NC? A AN Attributes HeiPLAS (Hartung, 2015) 837/108/106 sentence s AN = w1 w2 paraphrase p Phrase Type STREUSLE (Schneider and Smith, 2015) 3017/372/376 sentence s does p describe the attribute in AN? label per token Context A O Table 1: A summary of the composition tasks included in ou"
Q19-1027,P18-1111,1,0.930643,"ld for recovering implicit information is much weaker, and the gap between the best performing model and the human performance on such tasks remains substantial. We expect that improving the ability of such representations to reveal implicit meaning would require more than a language model training objective. In particular, one future direction is a richer training objective that simultaneously models multiple co-occurrences of the constituent words across different texts, as is commonly done in noun compound interpretation ´ S´eaghdha and Copestake, 2009; Shwartz (e.g., O and Waterson, 2018; Shwartz and Dagan, 2018). Data. We use the data set of Tu and Roth (2012), which consists of 1,348 sentences from the British National Corpus (BNC), each containing a verb V and a preposition P annotated to whether it is a VPC or not. The data set is focused on 23 different phrasal verbs derived from six of the most frequently used verbs (take, make, have, get, do, give), and their combination with common prepositions or particles. To reduce label bias, we split the data set lexically by verb—that is, the train, test, and validation sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb"
Q19-1027,S12-1010,0,0.060197,"and the gap between the best performing model and the human performance on such tasks remains substantial. We expect that improving the ability of such representations to reveal implicit meaning would require more than a language model training objective. In particular, one future direction is a richer training objective that simultaneously models multiple co-occurrences of the constituent words across different texts, as is commonly done in noun compound interpretation ´ S´eaghdha and Copestake, 2009; Shwartz (e.g., O and Waterson, 2018; Shwartz and Dagan, 2018). Data. We use the data set of Tu and Roth (2012), which consists of 1,348 sentences from the British National Corpus (BNC), each containing a verb V and a preposition P annotated to whether it is a VPC or not. The data set is focused on 23 different phrasal verbs derived from six of the most frequently used verbs (take, make, have, get, do, give), and their combination with common prepositions or particles. To reduce label bias, we split the data set lexically by verb—that is, the train, test, and validation sets contain distinct verbs in their V and P combinations. 2.2 Recognizing Light Verb Constructions The meaning of a light verb constr"
Q19-1027,N18-2035,1,0.872806,"(Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawbacks above: They assume that the meaning"
Q19-1027,D12-1110,0,0.174527,"Missing"
Q19-1027,R11-1040,0,0.0714461,"s probable in English. Table 8 shows the results of this experiment. A first observation is that the full model performs best on both tasks, suggesting that the model captures implicit meaning from various sources. In the NC Relations, all variants perform on par or worse than the majority baseline, achieving a few points less than the full model. In the AN Attributes task it is easier to see that the phrase (AN) is important for the classification, whereas the context is secondary. 8 Lexical Composition. There is a vast literature on multi-word expressions in general (e.g., Sag et al., 2002; Vincze et al., 2011), and research focusing on noun compounds (e.g., Nakov, 2013; Nastase et al., 2013), adjective-noun compositions (e.g., Baroni and Zamparelli, 2010; Boleda et al., 2013), verb-particle constructions (e.g., Baldwin, 2005; Pichotta and DeNero, 2013), and light verb constructions (e.g., Tu and Roth, 2011; Chen et al., 2015). In recent years, word embeddings have been used to predict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a"
Q19-1027,D18-1182,0,0.023263,"points are positive examples and orange (light) points are negative. Non-literality as a rare sense. Nunberg et al. (1994) considered some non-literal compounds as ‘‘idiosyncratically decomposable’’, that is, which can be decomposed to possibly rare senses of their constituents, as in considering bee to have a sense of ‘‘competition’’ in spelling bee and crocodile to stand for ‘‘manipulative’’ in crocodile tears. Using this definition, we could possibly use the NC literality data for word sense induction, in which recent work has shown that contextualized word representations are successful (Stanovsky and Hopkins, 2018; Amrami and Goldberg, 2018). We are interested in testing not only whether the contextualized models are capable of detecting rare senses induced by non-literal usage, which we have confirmed in Section 6, but whether they can also model these senses. To that end, we sample target words that appear in both literal and non-literal examples, and use each contextualized word embedding model as a language model to predict the best substitutes of the target word in each context. Table 7 exemplifies some of these predictions. Bold words are words judged reasonable in the given context, even if they"
Q19-1027,R15-1082,0,0.020691,"dict the compositionality of phrases (Salehi et al., 2015; Cordeiro et al., 2016), and to identify the implicit relation in adjective-noun Related Work Probing Tasks. One way to test whether dense representations capture a certain linguistic 3 A somewhat similar phenomenon was recently reported by Senaldi et al. (2019). Their model managed to distinguish idioms from non-idioms, but their ablation study showed the model was in fact learning to distinguish between abstract contexts (in which idioms tend to appear) and concrete ones. 413 compositions (Hartung et al., 2017) and in noun compounds (Surtani and Paul, 2015; Dima, 2016; Shwartz and Waterson, 2018; Shwartz and Dagan, 2018). Pavlick and Callison-Burch (2016) created a simpler variant of the recognizing textual entailment task (RTE, Dagan et al., 2013) that tests whether an adjective-noun composition entails the noun alone and vice versa in a given context. They tested various standard models for RTE and found that the models performed poorly with respect to this phenomenon. To the best of our knowledge, contextualized word embeddings haven’t yet been used for tasks related to lexical composition. these methods still suffer from the two other drawb"
Q19-1027,D17-1026,0,0.065087,"Missing"
Q19-1027,C10-1142,0,0.176787,"Missing"
Q19-1027,D18-1009,0,0.0425371,"arelli, 2010; Wieting et al., 2017; Poliak et al., 2017), but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g., Zellers et al., 2018), which may potentially help with uncovering implicit information. In this paper we test how well various text representations address these composition-related phenomena. Methodologically, we follow recent work that applied ‘‘black-box’’ testing to assess various capacities of distributed representations (e.g., Adi et al., 2017; Conneau et al., 2018). We construct an evaluation suite with six tasks related to the above two phenomena, as shown in Figure 1, and develop generic models that rely on pre-trained representations. We test six representations, including static word embeddings (Mikolov"
Q19-1027,P94-1019,0,\N,Missing
Q19-1027,P17-1080,0,\N,Missing
Q19-1027,E17-2081,0,\N,Missing
R11-1063,W07-1401,1,0.467875,"in positive and negative pairs. The former features indicate that there are some operations that tend to be part of the “best” proof for negative pairs more frequently than for positive pairs. A reasonable explanation for this phenomenon is that the system learned that some operations are less reliable than other operations, and tried to avoid them whenever possible. However, these operations could not be avoided in negative pairs, resulting in higher feature values. Evaluation We ran experiments on the first, second, third and fifth RTE datasets12 (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and compared our system to other proof-style systems. Each dataset consists of 600 to 800 (T,H) pairs, half of them are positive, and the other half are negative. For the underlying linear classifier, required by Algorithm 1, we used linear-SVM13 . The value of K, described in Section 3.3, was set to 135, according to tuning done on the training set. The results for our system, presented in Table 4, show comparable performance to other systems on most datasets, with notably higher performance in RTE-3. Operation Insert Named Entity Insert content word DIRT “subject”"
R11-1063,N10-1145,0,0.0204297,"e-art methods that transform a text into logical representation are less robust than syntactic parsers, the logical representation is rarely used. Syntactic parse trees provide a common representation in text understanding systems in general, and for RTE in particular. The corresponding proof operations are thus tree-transformations that subsequently change the parse tree of T until H’s parse tree is obtained. Mostly, the selected tree-transformations followed standard (“insert”, “delete”, “substitute”) or custom tree edit distance operations (Mehdad and Magnini, 2009; Wang and Manning, 2010; Heilman and Smith, 2010) However, those sets of operations are often not linguistically-motivated and thus do not necessarily reflect the nature of the RTE problem. In addition, utilizing knowledge resources (both linguistic knowledge and world knowledge) is limited in such systems. Consider, for example, transformation of a parse-tree from a passive form to an active form. Such transformation can be done by a sequence of mostly deletion and insertion operations, however, such sequence misses to capture the syntactic structure of the transformation. Similarly, resources that inFigure 1: The figure demonstrates the ru"
R11-1063,P98-2127,0,0.0376559,"n about one lexical item (i.e. a word or a multi-word expression), which includes its lemma and its part-ofspeech, and optionally other information, such as Named Entity type5 . Each edge is labelled with a dependency relation (e.g. subject, object). Let T be a set of dependency parse trees that were constructed for T’s sentences, and let h be the dependency parse tree constructed for H. The system iteratively extends T with additional trees, by applying tree generation operations, until there exists a tree t ∈ T , such that h is embedded in t. 4 5 Entailment rules We used the Minipar parser (Lin, 1998b) We used Stanford NE recognizer (Finkel et al., 2005) 457 Operation-Name nodes in which the lemma is not specified. When such a rule is applied, the system first instantiates the variables with actual lemmas, according to the original tree, and then replaces the lhs by the instantiated rhs (As exemplified in Figure 1). As described in Section 2 and Table 1, such entailment rules are able to capture a broad range of linguistic and world knowledge. It should be noted that in our current implementation generic-syntactic rules were not integrated yet. Incorporating and extending the set of gener"
R11-1063,W09-2505,0,0.0907589,"resolution refutation). However, since state-of-the-art methods that transform a text into logical representation are less robust than syntactic parsers, the logical representation is rarely used. Syntactic parse trees provide a common representation in text understanding systems in general, and for RTE in particular. The corresponding proof operations are thus tree-transformations that subsequently change the parse tree of T until H’s parse tree is obtained. Mostly, the selected tree-transformations followed standard (“insert”, “delete”, “substitute”) or custom tree edit distance operations (Mehdad and Magnini, 2009; Wang and Manning, 2010; Heilman and Smith, 2010) However, those sets of operations are often not linguistically-motivated and thus do not necessarily reflect the nature of the RTE problem. In addition, utilizing knowledge resources (both linguistic knowledge and world knowledge) is limited in such systems. Consider, for example, transformation of a parse-tree from a passive form to an active form. Such transformation can be done by a sequence of mostly deletion and insertion operations, however, such sequence misses to capture the syntactic structure of the transformation. Similarly, resourc"
R11-1063,W07-1422,1,0.285719,"xt propositions using appropriate proof steps. In one line of these works (Wang and Manning, 2010; Heilman and Smith, 2010; Mehdad and Magnini, 2009) the tree-transformation operations followed mostly traditional tree edit distance operations, such as node insertion, deletion and substitution, and learned their costs according to the given RTE training data. As described in more detail in Section 2, these transformations do not necessarily capture the syntactic structure of entailment-preserving transformations. On the other hand, a rich inventory of knowledge-based operations was employed by Bar-Haim et al. (2007a). Their operations enable transforming complete sub-trees which do capture the syntactic structure of entailment inferences. Nevertheless, their work did not include a learning component for estimating proof costs and their tree-transformations were based only on available knowledge resources, without providing on-thefly operations that could compensate for some inevitably missing knowledge. In this work we aim to combine the complementing advantages of the above mentioned works while filling in some missing gaps. We utilize knowledge-based sub-tree transformations, following (Bar-Haim et al"
R11-1063,P10-1123,1,0.197718,"Missing"
R11-1063,W04-3205,0,0.00882876,"Missing"
R11-1063,P09-1051,1,0.913502,"ned earlier, to increase recall in practical RTE datasets, a hybrid framework was introduced by Bar-Haim et al. (2007b), which uses an approximate match mechanism for final classifications. D X (o) wi · Fi = wT · F (o) (1) i=1 The cost of a sequence of operations (and in particular of a proof) is naturally defined as the sum of costs of all operations. Thus, given a proof 458 P = (o1 , o2 , . . . om ), its total cost, denoted by Cw (P ), is: Cw (P ) , m X Cw (oj ) knowledge resources that provide such scores and were used in the current system are DIRT (Lin and Pantel, 2001), Wikipedia rules (Shnarch et al., 2009), Lin similarity (Lin, 1998a), and Directional-Similarity8 (Kotlerman et al., 2010). For knowledge resources that do not provide a numerical information about rule reliability, the corresponding feature-value is set to (−1). In the current system, WordNet9 (Fellbaum, 1998; Miller, 1995), an in-house Geographical data-base, and VerbOcean10 (Chklovski and Pantel, 2004) were included. (2) j=1 Let F (P ) = (2), we get: Cw (P ) , Pm j=1 F D X (oj ) . (P ) wi · F i Combining (1) and = wT · F (P ) (3) i=1 The last equation provides a way to represent a complete proof by a single feature-vector, which"
R11-1063,P08-4003,0,0.0613582,"Missing"
R11-1063,C10-1131,0,0.0243214,"wever, since state-of-the-art methods that transform a text into logical representation are less robust than syntactic parsers, the logical representation is rarely used. Syntactic parse trees provide a common representation in text understanding systems in general, and for RTE in particular. The corresponding proof operations are thus tree-transformations that subsequently change the parse tree of T until H’s parse tree is obtained. Mostly, the selected tree-transformations followed standard (“insert”, “delete”, “substitute”) or custom tree edit distance operations (Mehdad and Magnini, 2009; Wang and Manning, 2010; Heilman and Smith, 2010) However, those sets of operations are often not linguistically-motivated and thus do not necessarily reflect the nature of the RTE problem. In addition, utilizing knowledge resources (both linguistic knowledge and world knowledge) is limited in such systems. Consider, for example, transformation of a parse-tree from a passive form to an active form. Such transformation can be done by a sequence of mostly deletion and insertion operations, however, such sequence misses to capture the syntactic structure of the transformation. Similarly, resources that inFigure 1: The"
R11-1063,P05-1045,0,0.00355546,"ulti-word expression), which includes its lemma and its part-ofspeech, and optionally other information, such as Named Entity type5 . Each edge is labelled with a dependency relation (e.g. subject, object). Let T be a set of dependency parse trees that were constructed for T’s sentences, and let h be the dependency parse tree constructed for H. The system iteratively extends T with additional trees, by applying tree generation operations, until there exists a tree t ∈ T , such that h is embedded in t. 4 5 Entailment rules We used the Minipar parser (Lin, 1998b) We used Stanford NE recognizer (Finkel et al., 2005) 457 Operation-Name nodes in which the lemma is not specified. When such a rule is applied, the system first instantiates the variables with actual lemmas, according to the original tree, and then replaces the lhs by the instantiated rhs (As exemplified in Figure 1). As described in Section 2 and Table 1, such entailment rules are able to capture a broad range of linguistic and world knowledge. It should be noted that in our current implementation generic-syntactic rules were not integrated yet. Incorporating and extending the set of generic-syntactic rules is currently under work. 3.1.2 Inser"
R11-1063,C98-2122,0,\N,Missing
R11-1069,N07-1031,0,0.161146,"overy, search, exploration and analysis. In industrial settings targeted taxonomies for specific domains are mostly created manually, typically by domain experts, which is time consuming and requires a high level of expertise. This paper presents an algorithm and an implemented interactive system for automatically generating target-domain taxonomies based on the Wikipedia Category Hierarchy. The system also enables human post-editing, facilitated by intelligent assistance. 1 Introduction Hierarchies of category names (taxonomies) are very useful for effective information access (K¨aki (2005), Stoica et al. (2007)). When geared for a specific domain or data collection, such hierarchies can highly benefit the tasks of content discovery, search, exploration and analysis. Our project, carried out by the Natural Language Processing group at Bar-Ilan University and Orca Interactive Ltd., aimed at semi-automatic generation of a taxonomy for the domain of general video content in order to enhance search and improve recommendations in a personalized video recommendation system. This paper delivers two main contributions: (1) a novel algorithm for automatic generation of target-domain taxonomies and (2) an inte"
S12-1005,W06-3812,0,0.0151571,"mple sentences expressing 3 reasons for cancelation: the customer (1) does not use the service, (2) acquired a computer, (3) cannot afford the service. were extracted for each of the terms, using WordNet synonyms and derivations, as well as DIRECT2 , a directional statistical resource learnt from a news corpus. Candidate terms that did not appear in the accompanying domain corpus were filtered out as described in Section 3.1. Edges in the term graph were weighted with the number of resources supporting the corresponding edge. To cluster the graph we used the Chinese Whispers clustering tool3 (Biemann, 2006), whose algorithm does not require to pre-set the desired number of clusters and is reported to outperform other algorithms for several NLP tasks. To generate the projection, sentences were represented as vectors of terms weighted by their frequency in each sentence. Terms of the term-cluster vectors were weighted by the number of sentences in which they occur. Similarity scores were calculated using the cosine measure. Clusters were labeled with the top terms appearing both in the underlying term cluster and in the cluster’s sentences. 5 Results and Analysis In this section we present the res"
S12-1005,H05-1017,1,0.685585,"rithms consider the graph’s edge weights. To address this trait, different edge weights can be assigned, reflecting the level of confidence that the two terms are indeed validly related and the reliability of the resource, which suggested the corresponding edge (e.g. WordNet synonyms are commonly considered more reliable than statistical thesauri). 3.2 Step 2: Projecting Sentences to Term Clusters To obtain sentence clusters, the given sentence set has to be projected in some manner over the term clusters obtained in Step 1. Our projection procedure resembles unsupervised text categorization (Gliozzo et al., 2005), with categories represented by term clusters that are not predefined but rather emerge from the analyzed data: 1. Represent term clusters and sentences as vectors in term space and calculate the similarity of each sentence with each of the term clusters. 2. Assign each sentence to the best-scoring term cluster. (We focus on hard clustering, but the procedure can be adapted for soft clustering). Various metrics for feature weighting and vector comparison may be chosen. The top terms of termcluster vectors can be regarded as labels for the corresponding sentence clusters. Thus each sentence cl"
S12-1005,P98-2127,0,0.0860465,"esources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources 38 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the given sentence set and is clustered as follows:"
S12-1005,S10-1032,0,0.0169046,"evaluation results in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encoding of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, an"
S12-1005,N04-3012,0,0.0345595,"scovering the similarity between the two tweets. External resources can be utilized to provide such kind of knowledge, by which sentence representation can be enriched. Traditionally, WordNet (Fellbaum, 1998) has been used for this purpose (Shehata (2009); Chen et al. (2003); Hotho et al. (2003); Hatzivassiloglou et al. (2001)). Yet, other resources 38 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38–43, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al., 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al., 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al., 2007) etc. 3 Sentence Clustering via Term Clusters This section presents a generic sentence clustering scheme, which involves two consecutive steps: (1) generating relevant term clusters based on lexical semantic relatedness and (2) projecting the sentence set over these term clusters. Below we describe each of the two steps. 3.1 Step 1: Obtaining Term Clusters In order to obtain term clusters, a term connectivity graph is constructed for the g"
S12-1005,P09-2075,0,0.0161699,"ts in Section 5. Section 6 lists directions for future research. 2 Background Sentence clustering aims at grouping sentences with similar meanings into clusters. Commonly, vector similarity measures, such as cosine, are used to define the level of similarity over bag-of-words encoding of the sentences. Then, standard clustering algorithms can be applied to group sentences into clusters (see Steinbach et al. (2000) for an overview). The most common practice is representing the sentences as vectors in term space and applying the K-means clustering algorithm (Shen et al. (2011); Pasquier (2010); Wang et al. (2009); Nomoto and Matsumoto (2001); Boros et al. (2001)). An alternative approach involves partitioning a sentence connectivity graph by means of a graph clustering algorithm (Erkan and Radev (2004); Zha (2002)). The main challenge for any sentence clustering approach is language variability, where the same meaning can be phrased in various ways. The shorter the sentences are, the less effective becomes exact matching of their terms. Compare the following newspaper sentence ”The bank is phasing out the EZ Checking package, with no monthly fee charged for balances over $1,500, and is instead offerin"
S12-1005,C98-2122,0,\N,Missing
S12-1009,P90-1034,0,0.70959,"Missing"
S12-1009,P06-1084,0,0.0707883,"Missing"
S12-1009,P08-1083,0,0.0295067,"Missing"
S12-1009,J90-1003,0,0.419161,"of candidate related terms (termed candidate terms) for each target term. The top ranked candidates may be further examined (manually) by a lexicographer, who will select the eventual related terms for the thesaurus entry. Our methodology was applied for statistical measures of first order similarity (word cooccurrence). These statistics consider the number of times each candidate term co-occurs with the target term in the same document, relative to their total frequencies in the corpus. Common cooccurrence metrics are Dice coefficient (Smadja et al, 1996), Pointwise Mutual Information (PMI) (Church and Hanks, 1990) and log-likelihood test (Dunning, 1993). 2.1 Term Representation Statistical extraction is affected by term representation in the corpus. Usually, related terms in a thesaurus are lemmas, which can be identified by morphological disambiguation tools. However, we present two other approaches for term representation (either a target term or a candidate related term), which are less dependent on morphological processing. Typically, a morphological analyzer produces all possible analyses for a given token in the corpus. Then, a Part Of Speech (POS) tagger selects the most probable analysis and so"
S12-1009,W02-0908,0,0.199761,"Missing"
S12-1009,J93-1003,0,0.315424,"s) for each target term. The top ranked candidates may be further examined (manually) by a lexicographer, who will select the eventual related terms for the thesaurus entry. Our methodology was applied for statistical measures of first order similarity (word cooccurrence). These statistics consider the number of times each candidate term co-occurs with the target term in the same document, relative to their total frequencies in the corpus. Common cooccurrence metrics are Dice coefficient (Smadja et al, 1996), Pointwise Mutual Information (PMI) (Church and Hanks, 1990) and log-likelihood test (Dunning, 1993). 2.1 Term Representation Statistical extraction is affected by term representation in the corpus. Usually, related terms in a thesaurus are lemmas, which can be identified by morphological disambiguation tools. However, we present two other approaches for term representation (either a target term or a candidate related term), which are less dependent on morphological processing. Typically, a morphological analyzer produces all possible analyses for a given token in the corpus. Then, a Part Of Speech (POS) tagger selects the most probable analysis and solves morphology disambiguation. However,"
S12-1009,P08-1085,0,0.0551756,"Missing"
S12-1009,P08-2016,0,0.283372,"Missing"
S12-1009,W04-1808,0,0.655243,"Missing"
S12-1009,P07-2011,0,0.362611,"Missing"
S12-1009,W03-1011,0,0.624964,"Missing"
S12-1009,J96-1001,0,\N,Missing
S12-1009,P98-2127,0,\N,Missing
S12-1009,C98-2122,0,\N,Missing
S12-1032,P03-1003,0,0.170816,"hat validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of this claim such as extensive usage of cell phones may be harmful for youngsters. This can be done by ranking the corpus passages by their likelihood to entail the claim, where the top ranked passages are likely to contain additional relevant information. Two main approaches have been used to address textual inference (for either ranking or classification). One is based on transformations over syntactic parse trees (Echihabi and Marcu, 2003; Heilman and Smith, 2010). Some works in this line describe a probabilistic generative process in which the parse tree of the question is generated from the passage (Wang et al., 2007; Wang and Manning, 2010). In the second approach, lexical models have been employed for textual inference (MacKinlay and Baldwin, 2009; Clark and Harrison, 2010). Typi1 http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics cally, lexical models conside"
S12-1032,N03-1013,0,0.10278,"Missing"
S12-1032,H05-1049,0,0.0305492,"inferences and show substantially improved results on a recently investigated question answering data set. 1 Introduction The task of identifying texts which share semantic content arises as a general need in many natural language processing applications. For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts. We refer to this general task as textual inference similar to prior use of this term (Raina et al., 2005; Schoenmackers et al., 2008; Haghighi et al., 2005). In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research wa"
S12-1032,N10-1145,0,0.310194,"ting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1 . In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of"
S12-1032,P98-2127,0,0.0390222,"2010). Typi1 http://www.nist.gov/tac/2010/RTE/index.html 237 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 237–245, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics cally, lexical models consider a text fragment as a bag of terms and split the inference decision into two steps. The first is a term-level estimation of the inference likelihood for each term independently, based on direct lexical match and on lexical knowledge resources. Some commonly used resources are WordNet (Fellbaum, 1998), distributional-similarity thesauri (Lin, 1998), and web knowledge resources such as (Suchanek et al., 2007). The second step is making a final sentence-level decision based on these estimations for the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principl"
S12-1032,D08-1009,0,0.0314216,"the task of ranking textual inferences and show substantially improved results on a recently investigated question answering data set. 1 Introduction The task of identifying texts which share semantic content arises as a general need in many natural language processing applications. For instance, a paraphrasing application has to recognize texts which convey roughly the same content, and a summarization application needs to single out texts which contain the content stated by other texts. We refer to this general task as textual inference similar to prior use of this term (Raina et al., 2005; Schoenmackers et al., 2008; Haghighi et al., 2005). In many textual inference scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A re"
S12-1032,P11-2098,1,0.810463,"the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference. An attractive work in this spirit is presented in (Shnarch et al., 2011a), that propose a model which is both lexical and probabilistic. Later, Shnarch et al. (2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE models. Whereas their term-level model provides means to integrate lexical knowledge in a probabilistic manner, their sentence-level model depends to a great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision. This deviates their model from a pure probabilistic methodology. Our work aims at amending this def"
S12-1032,W11-2402,1,0.698513,"the component terms. Lexical models have the advantage of being fast and easy to utilize (e.g. no dependency on parsing tools) while being highly competitive with top performing systems, e.g. the system of Majumdar and Bhattacharyya (2010). In this work, we investigate how well such lexical models can perform in textual inference ranking scenarios. However, while lexical models usually apply heuristic methods, we would like to pursue a principled learning-based generative framework, in analogy to the approaches for syntactic-based inference. An attractive work in this spirit is presented in (Shnarch et al., 2011a), that propose a model which is both lexical and probabilistic. Later, Shnarch et al. (2011b) improved this model and reported results that outperformed previous lexical models and were on par with state-of-the-art RTE models. Whereas their term-level model provides means to integrate lexical knowledge in a probabilistic manner, their sentence-level model depends to a great extent on heuristic normalizations which were introduced to incorporate prominent aspects of the sentence-level decision. This deviates their model from a pure probabilistic methodology. Our work aims at amending this def"
S12-1032,C10-1131,0,0.5123,"ation decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1 . In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look for additional mentions of this claim such as exten"
S12-1032,D07-1003,0,0.727349,"e scenarios the setting requires a classification decision of whether the inference relation holds or not. But in other scenarios ranking according to inference likelihood would be the natural task. In this work we focus on ranking textual inferences; given a sentence and a corpus, the task is to rank the corpus passages by their plausibility to imply as much of the sentence meaning as possible. Most naturally, this is the case in question answering (QA), where systems search for passages that cover the semantic components of the question. A recent line of research was dedicated to this task (Wang et al., 2007; Heilman and Smith, 2010; Wang and Manning, 2010). A related scenario is the task of Recognizing Textual Entailment (RTE) within a corpus (Bentivogli et al., 2010)1 . In this task, inference systems should identify, for a given hypothesis, the sentences which entail it in a given corpus. Even though RTE was presented as a classification task, it has an appealing potential as a ranking task as well. For instance, one may want to find texts that validate a claim such as cellular radiation is dangerous for children, or to learn more about it from a newswire corpus. To that end, one should look f"
S12-1032,W07-1401,1,\N,Missing
S12-1032,2003.mtsummit-systems.9,0,\N,Missing
S12-1032,C98-2122,0,\N,Missing
S13-2045,S12-1059,0,0.00894071,"Missing"
S13-2045,P10-4003,1,0.705578,"in, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and c"
S13-2045,N12-1021,1,0.552195,"ing textual entailment. Thus, we offered to the community a 5-way student response labeling task, as well as 3-way and 2way RTE-style tasks on educational data. In addition, a partial entailment task was piloted. We present and compare results from 9 participating teams, and discuss future directions. 1 Introduction One of the tasks in educational NLP systems is providing feedback to students in the context of exam questions, homework or intelligent tutoring. Much previous work has been devoted to the automated Since the task of making and testing a full educational dialog system is daunting, Dzikovska et al. (2012) identified a key subtask and proposed it as a new shared task for the NLP community. Student response analysis (henceforth SRA) is the task of labeling student answers with categories that could 263 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 263–274, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Example 1 Q UESTION R EF. A NS . S TUD . A NS . Example 2 Q UESTION R EF. A NS . S TUD . A NS . You used several methods to separate and identify the"
S13-2045,P11-1076,0,0.189172,"ar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the community a 5-way stud"
S13-2045,nielsen-etal-2008-annotating,1,0.393893,"@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. Thus, we offered to the"
S13-2045,W05-0202,0,0.185532,"Clark Vulcan Inc. USA peterc@vulcan.com Ido Dagan Bar-Ilan University Israel dagan@cs.biu.ac.il Hoa Trang Dang NIST hoa.dang@nist.gov Abstract scoring of essays (Attali and Burstein, 2006; Shermis and Burstein, 2013), error detection and correction (Leacock et al., 2010), and classification of texts by grade level (Petersen and Ostendorf, 2009; Sheehan et al., 2010; Nelson et al., 2012). In these applications, NLP methods based on shallow features and supervised learning are often highly effective. However, for the assessment of responses to short-answer questions (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Nielsen et al., 2008a; Mohler et al., 2011) and in tutorial dialog systems (Graesser et al., 1999; Glass, 2000; Pon-Barry et al., 2004; Jordan et al., 2006; VanLehn et al., 2007; Dzikovska et al., 2010) deeper semantic processing is likely to be appropriate. We present the results of the Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge, aiming to bring together researchers in educational NLP technology and textual entailment. The task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment. T"
S13-2045,R11-1063,1,0.693982,"ighted Average Precision, Recall and F1 , and computed as described in Section 4.2. We used only a majority class baseline, which labeled all facets as ‘Unaddressed’. Its performance is presented in Section 5.4 jointly with the system results. 5.4 Participants and results Only one participant, UKP-BIU, participated in the Partial Entailment Pilot task. The UKP-BIU system is a hybrid of two semantic relationship approaches, namely (i) computing semantic textual similarity by combining multiple content similarity measures (B¨ar et al., 2012), and (ii) recognizing textual entailment with BIUTEE (Stern and Dagan, 2011). The two approaches are combined by generating indicative features from each one and then applying standard supervised machine learning techniques to train a classifier. The system used several lexicalsemantic resources as part of the BIUTEE entailment system, together with S CI E NTS BANK dependency parses and ESA semantic relatedness indexes from Wikipedia. The team submitted the maximum allowed of 3 runs. Table 7 shows Weighted Average and Macro Average F1 scores respectively, also for the majority baseline. The system outperformed the majority baseline on both metrics. The best performanc"
S13-2045,C00-2137,0,0.0467226,"m one another, with performance varying from being the top rank to nearly the lowest. Hence, it seemed more appropriate to report two separate runs.3 In the rest of the discussion system is used to refer to a row in the tables as just described. Systems with performance that was not statistically different from the best results for a given TS are all shown in bold (significance was not calculated for the TS mean). Systems with performance statistically better than the lexical baseline are displayed in italics. Statistical significance tests were conducted using approximate randomization test (Yeh, 2000) with 10,000 iterations; p ≤ 0.05 was considered statistically significant. 4.4 Dataset: B EETLE 5way Run UA UQ CELI1 0.315 0.300 CNGL2 0.431 0.382 CoMeT1 0.569 0.300 EHUALM2 0.526 0.3703 ETS1 0.444 0.461 ETS2 0.619 0.552 LIMSIILES1 0.327 0.280 SoftCardinality1 0.455 0.436 UKP-BIU1 0.423 0.285 Median 0.444 0.370 Baselines: Lexical 0.424 0.414 Majority 0.114 0.118 Five-way Task The results for the five-way task are shown in Tables 2 and 3. Comparison to baselines All of the systems performed substantially better than the majority class baseline (“correct” for both B EETLE and S CI E NTS BANK),"
S13-2045,W07-1401,1,\N,Missing
S13-2048,S12-1059,1,0.886472,"Missing"
S13-2048,C12-1011,1,0.819518,"Missing"
S13-2048,S13-2045,1,0.847683,"tment Technische Universit¨at Darmstadt § Natural Language Processing Lab Computer Science Department Bar-Ilan University Abstract Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline. 1 Ido Dagan§ Introduction The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013) brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies. The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize"
S13-2048,W01-0515,0,0.0300847,"use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3 code.google.com/p/dkpro-core-asl/ DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5 Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6 As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 4 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T ) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, an"
S13-2048,R11-1063,1,0.940093,"provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains. Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting. We therefore approach the challenge by combining two such technologies: DKPro Similarity –an extensive suite of text similarity measures– that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity (B¨ar et al., 2012a) or reuse detection (B¨ar et al., 2012b). BIUTEE, the Bar-Ilan University Textual Entailment Engine (Stern and Dagan, 2011), which has shown state-of-the-art performance on recognizing textual entailment challenges. Our systems use both technologies to extract features, and combine them in a supervised model. Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 2.1 Background Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Compu"
S15-1022,P05-1074,0,0.0262158,"consistent semantics (positive and negative). For English, WordNet and VerbOcean were used as lexical resources. Italian WordNet was used for Italian, and GermaNet and German DerivBase (Zeller et al., 2013) were used as lexical resources for German. 1 As a part of Excitement Open Platform for Textual Entailment. https://github.com/hltfbk/EOP-1.2.1/ wiki/AlignmentEDAP1 195 Paraphrase Aligner. The paraphrase aligner concentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important rol"
S15-1022,J12-1003,1,0.930469,"al algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves ar"
S15-1022,W14-5201,0,0.054806,"Missing"
S15-1022,W14-3348,0,0.0536698,"ncentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important role in practice to deal with named entities. 3.3 A Minimal Feature Set Similar to the aligners, we concentrate on a small set of four features in the pilot algorithm. Again, the features are completely language independent, even at the implementation level. This is possible because the linguistic annotations and the alignments, use a language-independent type system (cf. Section 3.1). All current features measur"
S15-1022,E09-1025,0,0.0131132,"gnments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which have access to knowledge resources and can co"
S15-1022,S14-1009,1,0.744724,"Missing"
S15-1022,W07-1401,1,0.817868,"Missing"
S15-1022,P06-1114,0,0.0407371,"ntral, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge r"
S15-1022,P10-4008,0,0.0227067,"E engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language or for new analysis aspect – are often"
S15-1022,N06-1006,0,0.105121,"Missing"
S15-1022,D08-1084,0,0.0240179,"igner2 aligner3 knowledge resource 2-adding alignments T multi-level alignment T-H pair enriched with various levels of alignments H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision Figure 1: Dataflow for TE algorithms based on multi level alignment that alignment strength can be misleading (MacCartney et al., 2006), alignment was understood as an intermediate step whose outcome is a set of correspondences between parts of T and H that can be used to define (mis-)match features. Alignments can be established at the word level, phrase level (MacCartney et al., 2008), or dependency level (Dinu and Wang, 2009). Dagan et al. (2013) generalized this practical use to an architectural principle: They showed that various TE algorithms can be mapped onto a universal alignment-based schema with six steps: preprocessing, enrichment, candidate alignment generation, alignment selection, and classification. Proposal. Our proposal is similar to, but simpler than, Dagan et al.’s. Figure 1 shows the data flow. First, the text and the hypothesis are linguistically pre-processed. Then, the annotated T-H pair becomes 194 the input for various independent aligners, which ha"
S15-1022,P14-5008,1,0.767679,"Missing"
S15-1022,P12-3013,1,0.852425,"with state-of-theart open-source TE engines in three languages. 1 Introduction A key challenge of Natural Language Processing is to determine what conclusions can be drawn from a natural language text, a task known as Textual Entailment (TE, Dagan and Glickman 2004). The ability to recognize TE helps dealing with surface variability in tasks like Question Answering (Harabagiu and Hickl, 2006), Intelligent Tutoring (Nielsen et al., 2009), or Text Exploration (Berant et al., 2012). Open source implementations a number of TE algorithms have become available over the last years, including BIUTEE (Stern and Dagan, 2012) and EDITS (Kouylekov and Negri, 2010), which has made it much easier for end users to utilize TE engines. At the same time, the situation is still more difficult for researchers and developers. Even though recently a common platform for TE has been proposed (Pad´o 193 et al., 2015) that standardizes important aspects like annotation types, preprocessing, and knowledge resources, it largely ignores the algorithmic level. In fact, TE algorithms themselves are generally not designed to be extensible or interoperable. Therefore, changes to the algorithms – like adding support for a new language o"
S15-1022,D09-1082,0,0.0349226,"Missing"
S15-1022,P13-1118,1,0.780789,"Missing"
S16-2013,N13-1092,0,0.201684,"Missing"
S16-2013,P12-1092,0,0.0448766,"annotation guidelines are available at: http://u.cs.biu.ac.il/ ˜nlp/resources/downloads/ context-sensitive-fine-grained-dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposit"
S16-2013,P07-1058,1,0.822438,"Missing"
S16-2013,W14-1610,1,0.792661,"Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find th"
S16-2013,P15-1129,0,0.224633,"l inferences in context requires datasets in which inferences are annotated incontext by fine-grained semantic relations. Yet, such a dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. We apply our methodology on a subset of phrase pairs from Pavlick et al. (2015), Recogni"
S16-2013,P15-2069,0,0.0160968,"dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2"
S16-2013,Q15-1025,0,0.0175166,"available at: http://u.cs.biu.ac.il/ ˜nlp/resources/downloads/ context-sensitive-fine-grained-dataset. 109 fine-grained relation coarse-grained [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive"
S16-2013,W07-1431,0,0.218211,"dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. We apply our methodology on a subset of phrase pairs from Pavlick et al. (2015), Recognizing lexical inferences between pairs of terms is a common task in NLP applications, which should typically be performed within a given context. Such context"
S16-2013,P12-2031,1,0.835883,"artz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find the above mentioned subset of human-annotated paraphrases p"
S16-2013,S14-2001,0,0.036019,"ch et al., 2013) is a huge resource of automatically derived paraphrases. In recent years, it has been used for quite many semantic tasks, such as semantic parsing (Wang et al., 2015), recognizing textual entailment (Noh et al., 2015), and monolingual alignment (Sultan et al., 2014). Recently, as part of the PPDB 2.0 release, Pavlick et al. (2015) re-annotated PPDB with finegrained semantic relations, following natural logic (MacCartney and Manning, 2007) (see table 2). This was done by first annotating a subset of PPDB pharaphase-pairs that appeared in the SICK dataset of textual entailment (Marelli et al., 2014). Annotators were instructed to select the appropriate semantic relation that holds for each paraphrase pair. These human annotations were later used to train a classifier and predict the semantic relation for all paraphrase pairs in PPDB. Considering the widespread usage of PPDB in applications, this extension may likely lead to applying lexical inferences based on such fine-grained semantic relations. In this section, we present a methodology of adding context to lexical inference datasets, that we apply on PPDB-fine-human. 3.1 Selecting Phrase-Pairs PPDB-fine-human is a quite large dataset"
S16-2013,S07-1009,0,0.0684032,"ned [16] this paper [15] PPDB-fine-human [1] WordSim-353 similarity [2] SimLex-999 [4] Annotated-PPDB [5] WordSim-353 relatedness [6] MEN [10] Kotlerman2010 [11] Turney2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alter"
S16-2013,S15-1022,1,0.864442,"Missing"
S16-2013,N07-1071,0,0.0851558,"Missing"
S16-2013,P15-1146,0,0.0402832,"Missing"
S16-2013,D14-1162,0,0.0756897,"Missing"
S16-2013,K15-1018,1,0.805138,"y2014 [12] Levy2014 [13] Shwartz2015 none [7] TR9856 [8] SemEval 2007 [9] All-Words [14] Zeichner2012 1 context context [3] SCWS 2 contexts Figure 1: A map of prominent lexical inference datasets. Word similarity: [1] Zesch et al. (2008), [2] Hill et al. (2014), [3] Huang et al. (2012), [4] Wieting et al. (2015). Term relatedness: [5] Zesch et al. (2008), [6] Bruni et al. (2014), [7] Levy et al. (2015). Lexical substitution: [8] McCarthy and Navigli (2007), [9] Kremer et al. (2014), Lexical inference: [10] Kotlerman et al. (2010), [11] Turney and Mohammad (2014), [12] Levy et al. (2014), [13] Shwartz et al. (2015), [14] Zeichner et al. (2012), [15] Pavlick et al. (2015) (see 2.2), [16]. ≡ @ A ˆ | ∼ # Equivalence Forward Entailment Reverse Entailment Negation Alternation Other-Related Independence is the same as is more specific than is more general than is the exact opposite of is mutually exclusive with is related in some other way to is not related to Table 2: Semantic relations in PPDB 2.0. Like Pavlick et al., we conflate negation and alternation into one relation. 2.2 PPDB with Semantic Relations In this paper, we focus on human-annotated datasets, and therefore find the above mentioned subset of"
S16-2013,Q14-1018,0,0.136799,"“talking on the iPhone is prohibited”). Accordingly, developing algorithms that properly apply lexical inferences in context requires datasets in which inferences are annotated incontext by fine-grained semantic relations. Yet, such a dataset is not available (see 2.1). Most existing datasets provide out-of-context annotations, while the few available in-context annotations refer to coarse-grained relations, such as relatedness or similarity. In recent years, the PPDB paraphrase database (Ganitkevitch et al., 2013) became a popular resource among semantic tasks, such as monolingual alignment (Sultan et al., 2014) and recognizing textual entailment (Noh et al., 2015). Recently, Pavlick et al. (2015) classified each paraphrase pair to the fine-grained semantic relation that holds between the phrases, following natural logic (MacCartney and Manning, 2007). To that end, a subset of PPDB paraphrase-pairs were manually annotated, forming a fine-grained lexical inference dataset. Yet, annotations are given out-of-context, limiting its utility. In this paper, we aim to fill the current gap in the inventory of lexical inference datasets, and present a methodology for adding context to outof-context datasets. W"
S16-2013,J15-4004,0,\N,Missing
S16-2013,E14-1057,0,\N,Missing
S17-1019,W99-0201,0,0.0232231,"Missing"
S17-1019,N03-1003,0,0.0539901,"tude from existing resources, it complements them with nonconsecutive predicates (e.g. take [a]0 from [a]1 ) and paraphrases which are highly context specific. The resource and the source code are available at http://github.com/vered1986/ Chirps.2 As of the end of May 2017, it contains 456,221 predicate pairs in 1,239,463 different contexts. Our resource is ever-growing and is expected to contain around 2 million predicate paraphrases within a year. Until it reaches a large enough size, we will release a daily update, and at a later stage, we plan to release a periodic update. 2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods"
S17-1019,N06-1039,0,0.165942,"p-ranked predicate paraphrases. Introduction their accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dag"
S17-1019,P01-1008,0,0.222672,"extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric mean of the similarities of their argument slots. Berant (2012) constructed an entailment graph of distributionally similar predicates by enforcing transitivity constraints and applying global optimization, releasing 52 million directional entailment rules (e.g. [a]0 shoot [a]1 → [a]0 kill [a]1 ). A second notable source for extracting paraphrases is multiple translations of the same text (Barzilay and McKeown, 2001). The Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015) is a huge collection of paraphrases extracted from bilingual parallel corpora. Paraphrases are scored heuristically, and the database is available for download in six increasingly large sizes according to scores (the smallest size being the most accurate). In addition to lexical paraphrases, PPDB also consists of 140 million syntactic paraphrases, some of which include predicates with non-terminals as arguments. 2.2 3 We present a methodology to automatically collect binary verbal predicate paraphrases from Twit"
S17-1019,P16-1119,1,0.83296,"rce release consists of two files: We extract propositions from news tweets using PropS (Stanovsky et al., 2016), which simplifies dependency trees by conveniently marking a wide range of predicates (e.g, verbal, adjectival, nonlexical) and positioning them as direct heads of their corresponding arguments. Specifically, we run PropS over dependency trees predicted by spaCy6 and extract predicate types (as in Table 1) composed of verbal predicates, datives, prepositions, and auxiliaries. Finally, we employ a pre-trained argument reduction model to remove non-restrictive argument modifications (Stanovsky and Dagan, 2016). This is essential for our subsequent alignment step, as it is likely that short and concise phrases will tend to match more frequently in comparison to longer, more specific arguments. Figure 1 exemplifies some of the phenomena handled by this process, along with the automatically predicted output. 3.3 1. Instances: the specific contexts in which the predicates are paraphrases (as in Table 2). In practice, to comply with Twitter policy, we release predicate paraphrase pair types along with their arguments and tweet IDs, and provide a script for downloading the full texts. 2. Types: predicate"
S17-1019,P10-1124,1,0.765007,"er “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1 / [a]0 acquire [a]1 ) (Lin and Pantel, 2001; Berant et al., 2010). The second approach exploits bilingual parallel corpora, extracting as paraphrases pairs of texts that were translated identically to foreign languages (Ganitkevitch et al., 2013). While these methods have produced exhaustive resources which are broadly used by applications, 1 This corresponds to instances of event coreference (Bagga and Baldwin, 1999). 155 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 155–160, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics weeks of acquisition shows that the set of pa"
S17-1019,P13-2080,1,0.798824,"006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For example, to answer “when did the US Supreme Court approve samesex marriage?” given the text “In June 2015, the Supreme Court ruled for same-sex marriage”, approve and ruled for should be identified as describing the same action. To that end, much effort has been devoted to identifying predicate paraphrases, some of which resulted in releasing resources of predicate entailment or paraphrases. Two main approaches were proposed for that matter; the first leverages the similarity in argument distribution across a large corpus between two predicates (e.g. [a]0 buy [a]1"
S17-1019,P07-1058,1,0.737065,"Missing"
S17-1019,Q14-1034,0,0.0797018,"ments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphrases if they shared a single anchor word. Background 2.1 Existing Paraphrase Resources A prominent approach to acquire predicate paraphrases is to compare the distribution of their arguments across a corpus, as an extension to the distributional hypothesis (Harris, 1954). DIRT (Lin and Pantel, 2001) is a resource of 10 million paraphrases, in which the similarity between predicate pairs is estimated by the geometric me"
S17-1019,P12-2031,1,0.861583,"erant (Berant, 2012), a resource of predicate entailments, and PPDB (Pavlick et al., 2015), a resource of paraphrases, both described in Section 2. We expect our resource to be more accurate than resources which are based on the distributional approach (Berant, 2012; Lin and Pantel, 2001). In addition, in comparison to PPDB, we specialize on binary verbal predicates, and apply an additional phase of proposition extraction, handling various phenomena such as non-consecutive particles and minimality of arguments. Berant (2012) evaluated their resource against a dataset of predicate entailments (Zeichner et al., 2012), using a recall-precision curve to show the performance obtained with a range of thresholds on the resource score. This kind of evaluation is less suitable for our resource; first, predicate entailment is directional, causing paraphrases with the wrong entailment direction to be labeled negative in the dataset. Second, since our resource is still relatively small, it is unlikely to have sufficient coverage of the dataset at that point. We therefore 6 Conclusion We presented a new unsupervised method to acquire fairly accurate predicate paraphrases from news tweets discussing the same event. W"
S17-1019,P15-2070,0,0.0588367,"Missing"
S17-1019,Q15-1009,0,0.0777347,"2 et al., 2002; Barzilay and Lee, 2003). The assumption is that multiple news articles describing the same event use various lexical choices, providing a good source for paraphrases. Heuristics are applied to recognize that two news articles discuss the same event, such as lexical overlap and same publish date (Shinyama and Sekine, 2006). Given such a pair of articles, it is likely that predicates connecting the same arguments will be paraphrases, as in the following example: 1. GOP lawmakers introduce new health care plan 2. GOP lawmakers unveil new health care plan Zhang and Weld (2013) and Zhang et al. (2015) introduced methods that leverage parallel news streams to cluster predicates by meaning, using temporal constraints. Since this approach acquires paraphrases from descriptions of the same event, it is potentially more accurate than methods that acquire paraphrases from the entire corpus or translation phrase table. However, there is currently no paraphrase resource acquired in this approach.3 Finally, Xu et al. (2014) developed a supervised model to collect sentential paraphrases from Twitter. They used Twitter’s trending topic service, and considered two tweets from the same topic as paraphr"
S17-1019,D13-1183,0,0.507084,"r accuracy is limited. Specifically, the first approach may extract antonyms, that also have similar argument distribution (e.g. [a]0 raise to [a]1 / [a]0 fall to [a]1 ) while the second may conflate multiple senses of the foreign phrase. A third approach was proposed to harvest paraphrases from multiple mentions of the same event in news articles.1 This approach assumes that various redundant reports make different lexical choices to describe the same event. Although there has been some work following this approach (e.g. Shinyama et al., 2002; Shinyama and Sekine, 2006; Roth and Frank, 2012; Zhang and Weld, 2013), it was less exhaustively investigated and did not result in creating paraphrase resources. In this paper we present a novel unsupervised method for ever-growing extraction of lexicallydivergent predicate paraphrase pairs from news tweets. We apply our methodology to create a resource of predicate paraphrases, exemplified in Table 1. Analysis of the resource obtained after ten Recognizing that various textual descriptions across multiple texts refer to the same event or action can benefit NLP applications such as recognizing textual entailment (Dagan et al., 2013) and question answering. For"
S17-1019,S12-1030,0,\N,Missing
W02-2009,P93-1024,0,0.629082,"Missing"
W04-3206,N03-1003,0,0.438033,"ess two problems: (a) finding matching anchors and (b) identifying template structure, as reviewed in the next two subsections. 2.1 Finding Matching Anchors The prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event. To facilitate finding many matching sentences, highly redundant comparable corpora have been used. These include multiple translations of the same text (Barzilay and McKeown, 2001) and corresponding articles from multiple news sources (Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003). While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability. Avoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus. Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required. Naturally, the largest available corpus is the Web. Since exhaustive processing of the Web is not feasible, (Duclaye et al., 2002) and (Ravichandran and Hovy, 2002) attempted bootstrap"
W04-3206,P01-1008,0,0.409398,"Missing"
W04-3206,duclaye-etal-2002-using,0,0.0229082,"sources (Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003). While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability. Avoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus. Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required. Naturally, the largest available corpus is the Web. Since exhaustive processing of the Web is not feasible, (Duclaye et al., 2002) and (Ravichandran and Hovy, 2002) attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of (Riloff and Jones, 1999). These methods start with a provided known set of anchors for a target meaning. For example, the known anchor set {Mozart, 1756 } is given as input in order to find paraphrases for the template ‘X born in Y’. Web searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one (“born in”). These new templates are then exploited"
W04-3206,P99-1044,0,0.018744,"ility and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. 1 Introduction Modeling semantic variability in language has drawn a lot of attention in recent years. Many applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms. Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed. Following (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g. (Moldovan and Rus, 2001)). These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other. For example “X acquired Y” entails “X owns"
W04-3206,P01-1052,0,0.0218997,"del of paraphrases. We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base. Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates. Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods. 1 Introduction Modeling semantic variability in language has drawn a lot of attention in recent years. Many applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms. Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed. Following (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g. (Moldovan and Rus, 2001)). These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other. F"
W04-3206,N03-1024,0,0.0778763,"Missing"
W04-3206,P02-1006,0,0.134285,"Missing"
W04-3206,P03-1029,0,0.0254826,"ising resource, but current Web-based methods suffer serious scalability constraints. 2.2 Identifying Template Structure Paraphrasing approaches learn different kinds of template structures. Interesting algorithms are presented in (Pang et al., 2003; Barzilay and Lee, 2003). They learn linear patterns within similar contexts represented as finite state automata. Three classes of syntactic template learning approaches are presented in the literature: learning of predicate argument templates (Yangarber et al., 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al., 2003). The last approach is the most general with respect to the template form. However, its processing time increases exponentially with the size of the templates. As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process. 3 The TE/ASE Acquisition Method Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of meanings, while requiring minimal input and (b) Keeping template structures as general as possible."
W04-3206,A00-1039,0,0.0101267,"ifferent types of corpora, obtaining varying scales of output. On the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints. 2.2 Identifying Template Structure Paraphrasing approaches learn different kinds of template structures. Interesting algorithms are presented in (Pang et al., 2003; Barzilay and Lee, 2003). They learn linear patterns within similar contexts represented as finite state automata. Three classes of syntactic template learning approaches are presented in the literature: learning of predicate argument templates (Yangarber et al., 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al., 2003). The last approach is the most general with respect to the template form. However, its processing time increases exponentially with the size of the templates. As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process. 3 The TE/ASE Acquisition Method Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of m"
W05-1208,J93-2003,0,0.00635969,"Missing"
W05-1208,rose-etal-2002-reuters,0,0.0114482,"syntactic or other deeper analysis, it nevertheless was among the top ranking systems in the RTE Challenge. 5 RCV1 dataset In addition to the RTE dataset we were interested in evaluating the model on a more representative set of texts and hypotheses that better corresponds to applicative settings. We focused on the information seeking setting, common in applications such as QA and IR, in which a hypothesis is given and it is necessary to identify texts that entail it. An annotator was asked to choose 60 hypotheses based on sentences from the first few documents in the Reuters Corpus Volume 1 (Rose et al., 2002). The annotator was instructed to choose sentential hypotheses such that their truth could easily be evaluated. We further required that the hypotheses convey a reasonable information need in such a way that they might correspond to potential questions, semantic queries or IE relations. Table 2 shows a few of the hypotheses. In order to create a set of candidate entailing texts for the given set of test hypotheses, we followed the common practice of WordNet based ex3 (Saggion et al., 2004) actually proposed the above score with no normalizing denominator. However for a given hypothesis it resu"
W05-1208,W07-1401,1,\N,Missing
W06-1621,W05-1210,1,0.895677,"Missing"
W06-1621,P01-1008,0,0.0612569,"Missing"
W06-1621,H05-1079,0,0.0386478,"Missing"
W06-1621,W05-1203,0,0.151857,"Missing"
W06-1621,J98-1006,0,0.0148958,"Missing"
W06-1621,P98-2127,0,0.263672,"Missing"
W06-1621,W07-1401,1,\N,Missing
W06-1621,C98-2122,0,\N,Missing
W06-2907,piperidis-etal-2004-multimodal,1,\N,Missing
W06-2907,H93-1061,0,\N,Missing
W06-2907,P04-1036,0,\N,Missing
W06-2907,W07-1401,1,\N,Missing
W06-2907,P05-1014,1,\N,Missing
W06-2907,daelemans-etal-2004-automatic,1,\N,Missing
W06-2907,P98-2127,0,\N,Missing
W06-2907,C98-2122,0,\N,Missing
W07-0909,N03-1003,0,0.0109416,"wo entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the ex"
W07-0909,itai-etal-2006-computational,1,0.829879,"l the partial templates of a learned template. These are templates that contain just one of variables in the original template. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different compone"
W07-0909,2004.tmi-1.1,1,0.912872,"r the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English are translated to Hebrew and vice versa; and Hebrew snippets are translated to English. We discuss each of these components in this section. Linguis"
W07-0909,P02-1006,0,0.208246,"the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may b"
W07-0909,E06-1052,1,0.803107,"ed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit"
W07-0909,I05-5011,0,0.0136238,"trieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentences are parsed an"
W07-0909,P03-1029,0,0.0952938,"phrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew te"
W07-0909,W04-3206,1,0.810829,"IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentenc"
W07-0909,C02-1161,0,0.0245238,"n Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic in66 ference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y → X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building block"
W07-1401,W03-0906,0,\N,Missing
W07-1401,W05-1203,0,\N,Missing
W07-1401,W05-1209,0,\N,Missing
W07-1401,W05-1206,0,\N,Missing
W07-1401,N04-1019,0,\N,Missing
W07-1401,P98-1013,0,\N,Missing
W07-1401,C98-1013,0,\N,Missing
W07-1401,W05-1201,0,\N,Missing
W07-1401,W05-1210,0,\N,Missing
W07-1401,W04-3206,1,\N,Missing
W07-1422,P98-2127,0,0.102137,"OUN  rain VERB expletive eYY John NOUN Mary NOUN yesterday NOUN pcomp−n mod  beautiful ADJ Source: it rained when beautiful Mary was seen by John yesterDerived: it rained when John saw beautiful Mary yesterday day (a) Application of passive to active transformation V VERB L V VERB obj subjffffXXXXX XXXXX f + sfffff obj ffffXXXXXby XXXXX f sfffff be  + N1 NOUN be VERB N2 NOUN by PREP pcomp−n N1 NOUN R  N2 NOUN (b) Passive to active transformation (substitution rule). The dotted arc represents alignment. Figure 1: Application of inference rules. POS and relation labels are based on Minipar (Lin, 1998b) If a complete proof is found (h was generated), the prover concludes that entailment holds. Otherwise, entailment is determined by comparing the minimal cost found during the proof search to some threshold θ. 3 Proof System Like logic-based systems, our proof system consists of propositions (t, h, and intermediate premises), and inference (entailment) rules, which derive new propositions from previously established ones. 3.1 Propositions Propositions are represented as dependency trees, where nodes represent words, and hold a set of features and their values. In our representation these fea"
W07-1422,W06-3907,0,0.0630034,"res. They have three major functions: (i) simplification and canonization of the source tree (categories 6 and 7 in Table 1); (ii) extracting embedded propositions (categories 1, 2, 3); (iii) inferring propositions from nonpropositional subtrees (category 4). 4.2 Polarity-Based Rules Consider the following two examples: John knows that Mary is here ⇒ Mary is here. John believes that Mary is here ; Mary is here. Valid inference of propositions embedded as verb complements depends on the verb properties, and the polarity of the context in which the verb appears (positive, negative, or unknown) (Nairn et al., 2006). We extracted from the polarity lexicon of Nairn et al. a list of verbs for which inference is allowed in positive polarity context, and generated entailment # 1 Category Conjunctions 2 4 Clausal modifiers Relative clauses Appositives 5 Determiners 6 7 Passive Genitive modifier Polarity Negation, modality 3 8 9 Example: source Helena’s very experienced and has played a long time on the tour. But celebrations were muted as many Iranians observed a Shi’ite mourning month. The assailants fired six bullets at the car, which carried Vladimir Skobtsov. Frank Robinson, a one-time manager of the Indi"
W07-1422,A97-1004,0,0.00872266,"c cost ClexSyn (p, h): where Score(l) is 1 if it appears in p, or if it is a derivation of a word in p (according to WordNet). Otherwise, Score(l) is the maximal Lin dependency-based similarity score between l and the lemmas of p (Lin, 1998a) (synonyms and hypernyms/hyponyms are handled by the lexical rules). 7 System Implementation Deriving the initial propositions t and h from the input text fragments consists of the following steps: (i) Anaphora resolution, using the MARS system (Mitkov et al., 2002). Each anaphor was replaced by its antecedent. (ii) Sentence splitting, using mxterminator (Reynar and Ratnaparkhi, 1997). (iii) Dependency parsing, using Minipar (Lin, 1998b). The proof search is implemented as a depth-first search, with maximal depth (i.e. proof length) of 4. If the text contains more than one sentence, the prover aims to prove h from each of the parsed sentences, and entailment is determined based on the minimal cost. Thus, the only cross-sentence information that is considered is via anaphora resolution. 8 Evaluation Dataset Test Official Results C(p, h) = λClexSyn (p, h) + (1 − λ)Clex (p, h) (1) Let m() ˆ be a (possibly partial) 1-1 mapping of the nodes of h to the nodes of p, where each no"
W07-1422,C98-2122,0,\N,Missing
W09-2504,meyers-etal-2004-cross,0,0.322461,"ment of the preposition ‘of’. remind ⇒ remember teach ⇒ learn give ⇒ have Extracted Mappings buy for X ⇒ pay X X buy ⇒ X pay divorce from X ⇒ marry X divorce from X ⇒ X marry kill X ⇒ X die kill among X ⇒ X die breathe X ⇒ inhale X breathe in X ⇒ inhale X remind X ⇒ X remember remind of X ⇒ remember X teach X ⇒ learn X teach to X ⇒ X learn give X ⇒ have X give to X ⇒ X have Table 2: Some argument mappings for WordNet verb-verb relations discovered by unary-DIRT. is described in WordNet by the derivationally related relation. To add argument mappings for these relations we utilize Nomlex-plus (Meyers et al., 2004), a database of around 5000 English nominalizations. Nomlex specifies for each verbal subcategorization frame of each nominalization how its argument positions are mapped to functional roles of related verbs. For each Nomlex entry, we extract all possible argument mappings between the verbal and nominal forms, as well as between different argument realizations of the noun. For example, the mappings ‘Xobj ’s employment ⇔ employ Xobj ’ and ‘Xobj ’s employment ⇔ employment of Xobj ’ are derived from the entry in Figure 2. The major challenge in integrating Nomlex and WordNet is to identify for ea"
W09-2504,P98-1013,0,0.209182,"Missing"
W09-2504,W97-0703,0,0.00678638,"this framework, which yields substantial improvement to WordNet-based inference. 1 Introduction WordNet (Miller, 1995), a manually constructed lexical database, is probably the mostly used resource for lexical inference in NLP tasks, such as Question Answering (QA), Information Extraction (IE), Information Retrieval and Textual Entailment (RTE) (Moldovan and Mihalcea, 2000; Pasca and Harabagiu, 2001; Bar-Haim et al., 2006; Giampiccolo et al., 2007). Inference using WordNet typically involves lexical substitutions for words in text based on WordNet relations, a process known as lexical chains (Barzilay and Elhadad, 1997; Moldovan and Novischi, 2002). For example, the answer to “From which country was Louisiana acquired?” can be inferred from “The United States bought up Louisiana from France” using the chains ‘France ⇒ European country ⇒ country’ and ‘buy up ⇒ buy ⇒ acquire’. When performing inference between predicates there is an additional complexity on top of lexical substitution: the syntactic relationship between the predicate and its arguments may change 27 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 27–35, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNL"
W09-2504,C02-1167,0,0.0965429,"substantial improvement to WordNet-based inference. 1 Introduction WordNet (Miller, 1995), a manually constructed lexical database, is probably the mostly used resource for lexical inference in NLP tasks, such as Question Answering (QA), Information Extraction (IE), Information Retrieval and Textual Entailment (RTE) (Moldovan and Mihalcea, 2000; Pasca and Harabagiu, 2001; Bar-Haim et al., 2006; Giampiccolo et al., 2007). Inference using WordNet typically involves lexical substitutions for words in text based on WordNet relations, a process known as lexical chains (Barzilay and Elhadad, 1997; Moldovan and Novischi, 2002). For example, the answer to “From which country was Louisiana acquired?” can be inferred from “The United States bought up Louisiana from France” using the chains ‘France ⇒ European country ⇒ country’ and ‘buy up ⇒ buy ⇒ acquire’. When performing inference between predicates there is an additional complexity on top of lexical substitution: the syntactic relationship between the predicate and its arguments may change 27 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 27–35, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP Rule Chains shopping:n of Xo"
W09-2504,burchardt-pennacchiotti-2008-fate,0,0.0330887,"Missing"
W09-2504,P01-1052,0,0.0346023,"introduced a novel corpus-based validation mechanism, avoiding rules for infrequent senses. Our experiments show that AmWN substantially improves standard WordNet-based inference. In future work we plan to add mappings between verbs and adjectives and between different frames of a verb. We also want to incorporate resources for additional subcategorization frames, such as VerbNet. Finally, we plan to enhance our text annotation based on noun-compound disambiguation (Lapata and Lascarides, 2003). Related Work Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). However, the only previous work we are aware of that enriches WordNet with argument mappings is (Novischi and Moldovan, 2006). This work utilizes VerbNet’s subcategorization frames to identify possible verb arguments. Argument mapping is provided only between verbs, ignoring relations between verbs and nouns. Arguments are mapped based on thematic role names shared between frames of different verbs. However, the semantic interpretation of thematic roles is generally inconsistent across verbs (Lowe et al., 1997; Kaisser and Webber"
W09-2504,W08-2205,0,0.011751,"es for infrequent senses. Our experiments show that AmWN substantially improves standard WordNet-based inference. In future work we plan to add mappings between verbs and adjectives and between different frames of a verb. We also want to incorporate resources for additional subcategorization frames, such as VerbNet. Finally, we plan to enhance our text annotation based on noun-compound disambiguation (Lapata and Lascarides, 2003). Related Work Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). However, the only previous work we are aware of that enriches WordNet with argument mappings is (Novischi and Moldovan, 2006). This work utilizes VerbNet’s subcategorization frames to identify possible verb arguments. Argument mapping is provided only between verbs, ignoring relations between verbs and nouns. Arguments are mapped based on thematic role names shared between frames of different verbs. However, the semantic interpretation of thematic roles is generally inconsistent across verbs (Lowe et al., 1997; Kaisser and Webber, 2007). Instead, we discover these mappings from corpus statis"
W09-2504,P06-1113,0,0.0124853,"uture work we plan to add mappings between verbs and adjectives and between different frames of a verb. We also want to incorporate resources for additional subcategorization frames, such as VerbNet. Finally, we plan to enhance our text annotation based on noun-compound disambiguation (Lapata and Lascarides, 2003). Related Work Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). However, the only previous work we are aware of that enriches WordNet with argument mappings is (Novischi and Moldovan, 2006). This work utilizes VerbNet’s subcategorization frames to identify possible verb arguments. Argument mapping is provided only between verbs, ignoring relations between verbs and nouns. Arguments are mapped based on thematic role names shared between frames of different verbs. However, the semantic interpretation of thematic roles is generally inconsistent across verbs (Lowe et al., 1997; Kaisser and Webber, 2007). Instead, we discover these mappings from corpus statistics, offering an accurate approach (as analyzed in Section 5.2). A frame semantics approach for argument Acknowledgements This"
W09-2504,W07-1401,1,0.785745,". We propose a novel framework for augmenting WordNet-based inferences over predicates with corresponding argument mappings. We further present a concrete implementation of this framework, which yields substantial improvement to WordNet-based inference. 1 Introduction WordNet (Miller, 1995), a manually constructed lexical database, is probably the mostly used resource for lexical inference in NLP tasks, such as Question Answering (QA), Information Extraction (IE), Information Retrieval and Textual Entailment (RTE) (Moldovan and Mihalcea, 2000; Pasca and Harabagiu, 2001; Bar-Haim et al., 2006; Giampiccolo et al., 2007). Inference using WordNet typically involves lexical substitutions for words in text based on WordNet relations, a process known as lexical chains (Barzilay and Elhadad, 1997; Moldovan and Novischi, 2002). For example, the answer to “From which country was Louisiana acquired?” can be inferred from “The United States bought up Louisiana from France” using the chains ‘France ⇒ European country ⇒ country’ and ‘buy up ⇒ buy ⇒ acquire’. When performing inference between predicates there is an additional complexity on top of lexical substitution: the syntactic relationship between the predicate and"
W09-2504,W07-1206,0,0.0238743,"Missing"
W09-2504,I05-5011,0,0.357209,"coincide with dependency relations of the verbal form. A rule example is ‘Xsubj break{intrans} ⇒ damage{trans} Xobj ’2 . More examples are shown in Table 1. Unlike Nomlex records, our templates can be partial: they may contain only some of the possible predicate arguments, e.g. ‘buy{trans} Xobj ’, where the subject, included in the frame, is omitted. Partial templates are necessary for matching predicate occurrences that include only some of the possible arguments, as in “Cognos was bought yesterday”. Additionally, some resources, such as automatic rule learning methods (Lin and Pantel, 2001; Sekine, 2005), can provide only partial argument information, and we would want to represent such knowledge as well. In our framework we follow (Szpektor and Dagan, 2008) and use only rules between unary templates, containing a single argument. Such templates can describe any argument mapping by deArgument-Mapping Entailment Rules In our framework we represent argument mappings for inferential relations between predicates through an extension of entailment rules over syntactic representations. As defined in earlier works, an entailment rule specifies an inference relation between an entailing template and"
W09-2504,E03-1073,0,0.0119126,"corpus-based resources. It covers a broader range of relations compared to prior work and yields more accurate mappings. We also introduced a novel corpus-based validation mechanism, avoiding rules for infrequent senses. Our experiments show that AmWN substantially improves standard WordNet-based inference. In future work we plan to add mappings between verbs and adjectives and between different frames of a verb. We also want to incorporate resources for additional subcategorization frames, such as VerbNet. Finally, we plan to enhance our text annotation based on noun-compound disambiguation (Lapata and Lascarides, 2003). Related Work Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). However, the only previous work we are aware of that enriches WordNet with argument mappings is (Novischi and Moldovan, 2006). This work utilizes VerbNet’s subcategorization frames to identify possible verb arguments. Argument mapping is provided only between verbs, ignoring relations between verbs and nouns. Arguments are mapped based on thematic role names shared between frames of different verbs. However,"
W09-2504,P06-1101,0,0.163237,"s-based validation mechanism, avoiding rules for infrequent senses. Our experiments show that AmWN substantially improves standard WordNet-based inference. In future work we plan to add mappings between verbs and adjectives and between different frames of a verb. We also want to incorporate resources for additional subcategorization frames, such as VerbNet. Finally, we plan to enhance our text annotation based on noun-compound disambiguation (Lapata and Lascarides, 2003). Related Work Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). However, the only previous work we are aware of that enriches WordNet with argument mappings is (Novischi and Moldovan, 2006). This work utilizes VerbNet’s subcategorization frames to identify possible verb arguments. Argument mapping is provided only between verbs, ignoring relations between verbs and nouns. Arguments are mapped based on thematic role names shared between frames of different verbs. However, the semantic interpretation of thematic roles is generally inconsistent across verbs (Lowe et al., 1997; Kaisser and Webber, 2007). Instead, w"
W09-2504,C08-1107,1,0.488322,"in Table 1. Unlike Nomlex records, our templates can be partial: they may contain only some of the possible predicate arguments, e.g. ‘buy{trans} Xobj ’, where the subject, included in the frame, is omitted. Partial templates are necessary for matching predicate occurrences that include only some of the possible arguments, as in “Cognos was bought yesterday”. Additionally, some resources, such as automatic rule learning methods (Lin and Pantel, 2001; Sekine, 2005), can provide only partial argument information, and we would want to represent such knowledge as well. In our framework we follow (Szpektor and Dagan, 2008) and use only rules between unary templates, containing a single argument. Such templates can describe any argument mapping by deArgument-Mapping Entailment Rules In our framework we represent argument mappings for inferential relations between predicates through an extension of entailment rules over syntactic representations. As defined in earlier works, an entailment rule specifies an inference relation between an entailing template and an entailed template, where templates are parse subtrees with argument variables (Szpektor and Dasubj obj gan, 2008). For example, ‘X ←−− buy −−→ Y subj prep"
W09-2504,W97-0204,0,0.193672,"Missing"
W09-2504,P08-1078,1,0.885724,"Missing"
W09-2504,W98-0604,0,0.100299,"Missing"
W09-2504,C98-1013,0,\N,Missing
W09-2504,W07-1400,0,\N,Missing
W09-2504,W04-3206,1,\N,Missing
W11-2402,W07-1402,0,0.0315148,"d as a generic paradigm for applied semantic inference (Dagan et al., 2006). Given two textual fragments, termed hypothesis (H) and text (T ), the text is said to textually entail the hypothesis (T→ H) if a person reading the text can infer the meaning of the hypothesis. Since it was first introduced, the six rounds of the Recognizing Textual Entailment (RTE) challenges1 have become a standard benchmark for entailment systems. Entailment systems apply various techniques to tackle this task, including logical inference (Tatu and Moldovan, 2007; MacCartney and Manning, 2007), semantic analysis (Burchardt et al., 2007) and syntactic parsing (Bar-Haim et al., 2008; Wang 1 http://www.nist.gov/tac/ For these reasons lexical entailment systems are widely used. They derive sentence-level entailment decision base on lexical-level entailment evidence. Typically, this is done by quantifying the degree of lexical coverage of the hypothesis terms by the text terms (where a term may be multi-word). A hypothesis term is covered by a text term if either they are identical (possibly at the stem or lemma level) or there is a lexical entailment rule suggesting the entailment of the former by the latter. Such rules are deri"
W11-2402,W04-3205,0,0.0521529,"d a simple ad-hoc variant of the cosine similarity score which removed from the text all terms which did not appear in the corresponding hypothesis. While this heuristic improved performance considerably, they reported a decrease in performance when utilizing synonym and derivation relations from WordNet. On the RTE-6 data set, the syntactic-based system of Jia et. al (2010) achieved the best results, only slightly higher than the lexical-level system of (Majumdar and Bhattacharyya, 2010). The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database, number matching module, co-reference resolution and named entity recognition tools. Their final entailment decision was based on a threshold over the A Probabilistic Model We aim at obtaining a probabilistic score for the likelihood that the hypothesis terms are entailed by the terms of the text. There are several prominent as12 t1 ti tm Resource1 MATCH Resource2 crowd surround Jaguar Resource2 Resource3 t' social group Resource3 in 3 Text cha number of matched hypothesis terms. They found out that hypotheses of different length"
W11-2402,W05-1203,0,0.02204,"tem or lemma level) or there is a lexical entailment rule suggesting the entailment of the former by the latter. Such rules are derived from lexical semantic resources, such as WordNet (Fellbaum, 1998), which capture lexical entailment relations. Common heuristics for quantifying the degree of coverage are setting a threshold on the percentage of coverage of H’s terms (Majumdar and Bhattacharyya, 2010), counting the absolute number of uncovered terms (Clark and Harrison, 2010), or applying an Information Retrieval-style vector space similarity score (MacKinlay and Baldwin, 2009). Other works (Corley and Mihalcea, 2005; Zanzotto and Moschitti, 2006) have applied heuristic formu10 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10–19, c Edinburgh, Scotland, UK, July 30, 2011. 2011 Association for Computational Linguistics las to estimate the similarity between text fragments based on a similarity function between their terms. The above mentioned methods do not capture several important aspects of entailment. Such aspects include the varying reliability levels of entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An addition"
W11-2402,P10-2017,0,0.0227265,"be entailed, and (4) taking account of the number of covered terms in modeling entailment reliability. We addressed the impact of the various components of our model and showed that its performance is in line with the best state-of-the-art inference systems. Future work is still needed to reflect the impact of transitivity. We consider replacing the AND gate on the rules of a chain by a noisy-AND, to relax its strict demand that all its input rules must be valid. Additionally, we would like to integrate Contextual Preferences (Szpektor et al., 2008) and other works on Selectional Preference (Erk and Pado, 2010) to verify the validity of the application of a rule in a specific (T, H) context. We also intend to explore the contribution of our model within a complex system that integrates multiple levels of inference as well as its contribution for other applications, such as Passage Retrieval. References Table 4: Comparison to RTE-5 and RTE-6 best entailment systems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark and Harrison, 2010), (c)(Mirkin et al., 2009a)(2 submitted runs), (d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al., 2010). 8 We note that the submitted run which outperformed our result"
W11-2402,W06-1621,1,0.854624,"Figure 1 for visual clarity) which takes as input the validity values (1/0) of the individual rule applications. Next, multiple chains may connect t to h (as for ti and hj in Figure 1) or connect several terms in T to h (as t1 and ti are indicating the entailment of hj in Figure 1), thus providing multiple evidence for h’s entailment. For a term h to be entailed by T it is enough that at least one of the chains from T to h would be valid. This condition is realized in the model by an OR gate. Finally, for T to lexically entail H it is usually assumed that every h ∈ H should be entailed by T (Glickman et al., 2006). Therefore, the final decision follows an AND gate combining the entailment decisions for all hypothesis terms. Thus, the 1-bit outcome of this gate y corresponds to the sentence-level entailment status. 3.1.2 Probabilistic Setting When assessing entailment for (T, H) pair, we do not know for sure which rule applications are valid. Taking a probabilistic perspective, we assume a parameter θR for each resource R, denoting its reliability, i.e. the prior probability that applying a rule from 13 R for an arbitrary (T, H) pair corresponds to valid entailment3 . Under this perspective, direct MATC"
W11-2402,N03-1013,0,0.668931,"data set. Therefore, they tuned a varying threshold for each topic based on an idiosyncracy of the data, by which the total number of entailments per topic is approximately a constant. Glickman et al. (2005) presented a simple model that recasted the lexical entailment task as a variant of text classification and estimated entailment probabilities solely from co-occurrence statistics. Their model did not utilize any lexical resources. In contrary to these systems, our model shows improvement when utilizing high quality resources such as WordNet and the CatVar (Categorial Variation) database (Habash and Dorr, 2003). As Majumdar and Bhattacharyya (2010), our model considers the impact of hypothesis length, however it does not require the tuning of a unique threshold for each length. Finally, most of the above systems do not differentiate between the various lexical resources they use, even though it is known that resources reliability vary considerably (Mirkin et al., 2009b). Our probabilistic model, on the other hand, learns a unique reliability parameter for each resource it utilizes. As mentioned above, this work extends the base model in (Shnarch et al., 2011), which is described in the next section."
W11-2402,W07-1431,0,0.0237848,"ems. 1 Introduction Textual Entailment was proposed as a generic paradigm for applied semantic inference (Dagan et al., 2006). Given two textual fragments, termed hypothesis (H) and text (T ), the text is said to textually entail the hypothesis (T→ H) if a person reading the text can infer the meaning of the hypothesis. Since it was first introduced, the six rounds of the Recognizing Textual Entailment (RTE) challenges1 have become a standard benchmark for entailment systems. Entailment systems apply various techniques to tackle this task, including logical inference (Tatu and Moldovan, 2007; MacCartney and Manning, 2007), semantic analysis (Burchardt et al., 2007) and syntactic parsing (Bar-Haim et al., 2008; Wang 1 http://www.nist.gov/tac/ For these reasons lexical entailment systems are widely used. They derive sentence-level entailment decision base on lexical-level entailment evidence. Typically, this is done by quantifying the degree of lexical coverage of the hypothesis terms by the text terms (where a term may be multi-word). A hypothesis term is covered by a text term if either they are identical (possibly at the stem or lemma level) or there is a lexical entailment rule suggesting the entailment of t"
W11-2402,E09-1064,1,0.925906,"robabilistic Model for Lexical Entailment Ido Dagan Jacob Goldberger Eyal Shnarch Computer Science Department School of Engineering Computer Science Department Bar-Ilan University Bar-Ilan University Bar-Ilan University Ramat-Gan, Israel Ramat-Gan, Israel Ramat-Gan, Israel shey@cs.biu.ac.il goldbej@eng.biu.ac.il dagan@cs.biu.ac.il Abstract et al., 2009). Inference at these levels usually requires substantial processing and resources, aiming at high performance. Nevertheless, simple lexical level entailment systems pose strong baselines which most complex entailment systems did not outperform (Mirkin et al., 2009a; Majumdar and Bhattacharyya, 2010). Additionally, within a complex system, lexical entailment modeling is one of the most effective component. Finally, the simpler lexical approach can be used in cases where complex systems cannot be used, e.g. when there is no parser for a targeted language. While modeling entailment at the lexical-level is a prominent task, addressed by most textual entailment systems, it has been approached mostly by heuristic methods, neglecting some of its important aspects. We present a probabilistic approach for this task which covers aspects such as differentiating v"
W11-2402,P11-2098,1,0.57172,"s include the varying reliability levels of entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An additional observation from these and other systems is that their performance improves only moderately when utilizing lexical-semantic resources2 . We believe that the textual entailment field would benefit from more principled models for various entailment phenomena. In this work we formulate a concrete generative probabilistic modeling framework that captures the basic aspects of lexical entailment. A first step in this direction was proposed in Shnarch et al. (2011) (a short paper), where we presented a base model with a somewhat complicated and difficult to estimate extension to handle coverage. This paper extends that work to a more mature model with new extensions. We first consider the “logical” structure of lexical entailment reasoning and then interpret it in probabilistic terms. Over this base model we suggest several extensions whose significance is then assessed by our evaluations. Learning the parameters of a lexical model poses a challenge since there are no lexical-level entailment annotations. We do, however, have sentence-level annotations"
W11-2402,P08-1078,1,0.856592,"c method to relax the strict demand that all hypothesis terms must be entailed, and (4) taking account of the number of covered terms in modeling entailment reliability. We addressed the impact of the various components of our model and showed that its performance is in line with the best state-of-the-art inference systems. Future work is still needed to reflect the impact of transitivity. We consider replacing the AND gate on the rules of a chain by a noisy-AND, to relax its strict demand that all its input rules must be valid. Additionally, we would like to integrate Contextual Preferences (Szpektor et al., 2008) and other works on Selectional Preference (Erk and Pado, 2010) to verify the validity of the application of a rule in a specific (T, H) context. We also intend to explore the contribution of our model within a complex system that integrates multiple levels of inference as well as its contribution for other applications, such as Passage Retrieval. References Table 4: Comparison to RTE-5 and RTE-6 best entailment systems: (a)(MacKinlay and Baldwin, 2009), (b)(Clark and Harrison, 2010), (c)(Mirkin et al., 2009a)(2 submitted runs), (d)(Majumdar and Bhattacharyya, 2010) and (e)(Jia et al., 2010)."
W11-2402,P06-1051,0,0.0969693,"e is a lexical entailment rule suggesting the entailment of the former by the latter. Such rules are derived from lexical semantic resources, such as WordNet (Fellbaum, 1998), which capture lexical entailment relations. Common heuristics for quantifying the degree of coverage are setting a threshold on the percentage of coverage of H’s terms (Majumdar and Bhattacharyya, 2010), counting the absolute number of uncovered terms (Clark and Harrison, 2010), or applying an Information Retrieval-style vector space similarity score (MacKinlay and Baldwin, 2009). Other works (Corley and Mihalcea, 2005; Zanzotto and Moschitti, 2006) have applied heuristic formu10 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 10–19, c Edinburgh, Scotland, UK, July 30, 2011. 2011 Association for Computational Linguistics las to estimate the similarity between text fragments based on a similarity function between their terms. The above mentioned methods do not capture several important aspects of entailment. Such aspects include the varying reliability levels of entailment resources and the impact of rule chaining and multiple evidence on entailment likelihood. An additional observation from these and o"
W11-2402,W07-1404,0,\N,Missing
W11-2402,W07-1401,1,\N,Missing
W11-2402,2003.mtsummit-systems.9,0,\N,Missing
W11-2403,N09-2009,1,0.915295,"ce in h is identical to a term in t. An inference based on an indirect match is viewed as the application of a lexical entailment rule, r, such as ‘hurt ⇒ injure’, where the entailing left-hand side (LHS) of the rule (hurt) is matched in the text, while the entailed righthand side (RHS), injure, is matched in the hypothesis. Hence, three inference objects take part in inference operations: t, h and r. Most prior work addressed only specific contextual matches between these objects. For example, Harabagiu et al. (2003) matched the contexts of t and h for QA (answer and question, respectively); Barak et al. (2009) matched t and h (document and category) in TC, while other works, including those applying lexical substitution, typically validated the context match between t and r (Kauchak and Barzilay, 2006; Dagan et al., 2006; Pantel et al., 2007; Connor and Roth, 2007). In comparison, in the CP framework, all possible contextual matches among t, h and r are considered: t − h, t − r and r − h. The three context matches are depicted in Figure 1 (left). In CP, the representation of each inference object is enriched with contextual information which is used to characterize its valid contexts. Such informat"
W11-2403,D08-1007,0,0.0127611,"discriminative power of the classifier (Smith and Eisner, 2005). We do that by applying a similar procedure which uses cohyponyms of the seeds, e.g. baseball for hockey or islam for christianity. Cohyponymy is a non-entailing relation; hence, by using it we expect to obtain semantically-related, yet invalid contexts. If not enough negative examples are retrieved using cohyponyms, we select the remaining required examples randomly. As the distribution of positive and negative examples in the data is unknown, we set the ratio of negative to positive examples as a parameter of the model, as in (Bergsma et al., 2008). 5.1.3 Insufficient examples When the number of training examples for a rule or a category is below a certain minimum, the resulting classifier is expected to be of poor quality. This usually happens for positive examples in any of the following two cases: (i) the seed is rare in the training set; (ii) the desired sense of the seed is rarely found in the training set, and unwanted senses were filtered by our retrieval query. For instance, nazarene does not occur at all in the training set, and the classifier corresponding to the rule ‘nazarene ⇒ christian’ cannot be generated. On the other ha"
W11-2403,P06-1057,1,0.935483,"(hurt) is matched in the text, while the entailed righthand side (RHS), injure, is matched in the hypothesis. Hence, three inference objects take part in inference operations: t, h and r. Most prior work addressed only specific contextual matches between these objects. For example, Harabagiu et al. (2003) matched the contexts of t and h for QA (answer and question, respectively); Barak et al. (2009) matched t and h (document and category) in TC, while other works, including those applying lexical substitution, typically validated the context match between t and r (Kauchak and Barzilay, 2006; Dagan et al., 2006; Pantel et al., 2007; Connor and Roth, 2007). In comparison, in the CP framework, all possible contextual matches among t, h and r are considered: t − h, t − r and r − h. The three context matches are depicted in Figure 1 (left). In CP, the representation of each inference object is enriched with contextual information which is used to characterize its valid contexts. Such information may be the words of the event description in IE, corpus instances based on which a rule was learned, or an annotation of relevant WordNet senses in Name-based TC. For example, a category name hockey may be assig"
W11-2403,C10-2029,0,0.0153407,"and Szpektor et al. (2008) represented the context of such rules as the intersection of preferences of the rule’s LHS and RHS, namely the observed argument instantiations or their semantic classes. A rule is deemed applicable to a given text if the argument instantiations in the text are similar to the selectional preferences of the rule. To overcome sparseness, other works represented context in latent space. Pennacchiotti et al. (2007) and Szpektor et al. (2008) measured the similarity between the Latent Semantic Analysis (LSA) (Deerwester et al., 1990) representations of matched contexts. Dinu and Lapata (2010) used Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model templates’ latent senses, determining rule applicability based on the similarity between the two sides of the rule when instantiated by the context, while Ritter et al. (2010) used LDA to model argument classes, considering a rule valid for a given argument instantiation if its instantiated templates are drawn from the same hidden topic. A different approach is provided by classificationbased models which learn classifiers for inference objects. A classifier is trained based on positive and negative examples which represent v"
W11-2403,H92-1045,0,0.0105786,"include all 25 the terms in the document or in the sentence in which a match was found. Local features are extracted around matches of seeds which comprised the query that retrieved the document. These features include the terms in a window around the match, and the noun, verb, adjective and adverb nearest to the match in either direction. For randomly sampled negative examples, where no matched query terms exist, we randomly select terms in the document as “matches” for local feature extraction. If more than one match of the same term is found in a document, we assume onesense-per-discourse (Gale et al., 1992) and jointly extract features for all matches of the term. 5.2 Applying the classifiers During inference, for each direct match in a document, the corresponding Ch is applied. For an indirect match, the respective Cr is also applied. In addition, Ch is applied to the matched rules. Unlike t, a rule is not represented by a single text. Therefore, to test a rule’s match with the category, we randomly sample from the training set documents containing the rule’s LHS. We apply Ch to each sampled example and compute the ratio of positive classifications. The result is a score indicating the domainsp"
W11-2403,N06-1058,0,0.189044,"-hand side (LHS) of the rule (hurt) is matched in the text, while the entailed righthand side (RHS), injure, is matched in the hypothesis. Hence, three inference objects take part in inference operations: t, h and r. Most prior work addressed only specific contextual matches between these objects. For example, Harabagiu et al. (2003) matched the contexts of t and h for QA (answer and question, respectively); Barak et al. (2009) matched t and h (document and category) in TC, while other works, including those applying lexical substitution, typically validated the context match between t and r (Kauchak and Barzilay, 2006; Dagan et al., 2006; Pantel et al., 2007; Connor and Roth, 2007). In comparison, in the CP framework, all possible contextual matches among t, h and r are considered: t − h, t − r and r − h. The three context matches are depicted in Figure 1 (left). In CP, the representation of each inference object is enriched with contextual information which is used to characterize its valid contexts. Such information may be the words of the event description in IE, corpus instances based on which a rule was learned, or an annotation of relevant WordNet senses in Name-based TC. For example, a category name"
W11-2403,N07-1071,0,0.115535,"n the text, while the entailed righthand side (RHS), injure, is matched in the hypothesis. Hence, three inference objects take part in inference operations: t, h and r. Most prior work addressed only specific contextual matches between these objects. For example, Harabagiu et al. (2003) matched the contexts of t and h for QA (answer and question, respectively); Barak et al. (2009) matched t and h (document and category) in TC, while other works, including those applying lexical substitution, typically validated the context match between t and r (Kauchak and Barzilay, 2006; Dagan et al., 2006; Pantel et al., 2007; Connor and Roth, 2007). In comparison, in the CP framework, all possible contextual matches among t, h and r are considered: t − h, t − r and r − h. The three context matches are depicted in Figure 1 (left). In CP, the representation of each inference object is enriched with contextual information which is used to characterize its valid contexts. Such information may be the words of the event description in IE, corpus instances based on which a rule was learned, or an annotation of relevant WordNet senses in Name-based TC. For example, a category name hockey may be assigned with the sense nu"
W11-2403,P10-1044,0,0.0140385,"the argument instantiations in the text are similar to the selectional preferences of the rule. To overcome sparseness, other works represented context in latent space. Pennacchiotti et al. (2007) and Szpektor et al. (2008) measured the similarity between the Latent Semantic Analysis (LSA) (Deerwester et al., 1990) representations of matched contexts. Dinu and Lapata (2010) used Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model templates’ latent senses, determining rule applicability based on the similarity between the two sides of the rule when instantiated by the context, while Ritter et al. (2010) used LDA to model argument classes, considering a rule valid for a given argument instantiation if its instantiated templates are drawn from the same hidden topic. A different approach is provided by classificationbased models which learn classifiers for inference objects. A classifier is trained based on positive and negative examples which represent valid or invalid contexts of the object; from those, features characterizing the context are extracted, e.g. words in a window around the target term or syntactic links with it. Given a new context, the classifier assesses its validity with resp"
W11-2403,P09-1051,1,0.827106,"Missing"
W11-2403,P05-1044,0,0.0154538,"ocedure as for rule clas2 Terms not in WordNet are assumed monosemous. sifiers. If the category is part of a hierarchy, we also use the name of the parent category (e.g. sport for rec.sport.hockey) as a context word. 5.1.2 Obtaining negative examples Negative examples are even more challenging to acquire. In prior work negative examples were selected randomly (Kauchak and Barzilay, 2006; Connor and Roth, 2007). We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). We do that by applying a similar procedure which uses cohyponyms of the seeds, e.g. baseball for hockey or islam for christianity. Cohyponymy is a non-entailing relation; hence, by using it we expect to obtain semantically-related, yet invalid contexts. If not enough negative examples are retrieved using cohyponyms, we select the remaining required examples randomly. As the distribution of positive and negative examples in the data is unknown, we set the ratio of negative to positive examples as a parameter of the model, as in (Bergsma et al., 2008). 5.1.3 Insufficient examples When the numb"
W11-2403,P08-1078,1,0.625442,"on demo is deemed invalid although demo implies the meaning of the word demonstrate in other contexts, e.g., concerning software demonstration. Although seemingly equivalent, a closer look reveals that the above two examples correspond to two distinct contextual mismatch situations. While the match of hurt is invalid for injure in the particular given context, an inference based on demo is invalid for the protest demonstrate event in any context. Thus, several types of context matching are involved in textual inference. While most prior work addressed only specific context matching scenarios, Szpektor et al. (2008) presented a broader view, proposing a generic framework for context matching in inference, termed Contextual Preferences (CP). CP specifies the types of context matching that need to be considered in inference, allowing a model of choice to be applied for validating each type of match. Szpektor et al. applied CP to an IE task using different models to validate each type of context match. In this work we adopt CP as our context matching framework and propose a novel classification-based scheme which provides unified modeling for CP. We represent typical contexts of the textual objects that par"
W11-2403,P08-1000,0,\N,Missing
W13-2704,P98-2127,0,0.226877,"ic setting for supporting thesaurus construction by a domain expert lexicographer. Our recall-oriented setting assumes that manual effort is worthwhile for increasing recall as long as it is being utilized effectively. Corpus-based thesaurus construction is an active research area (Curran and Moens, 2002; Kilgarriff, 2003; Rychl´y and Kilgarriff, 2007; Liebeskind et al., 2012; Zohar et al., 2013). Typically, two statistical approaches for identifying semantic relatedness between words were investigated: first-order (co-occurrence-based) similarity and second-order (distributional) similarity (Lin, 1998; Gasperin et al., 2001; Weeds and Weir, 2003; Kotlerman et al., 2010). In this research, we focus on statistical measures of first-order similarity (see Section 2). These methods were found to be effective for thesaurus construction as standalone methods and as complementary to secondorder methods (Peirsman et al., 2008). First-order measures assume that words that frequently occur together are topically related (Sch¨utze and Pedersen, 1997). Thus, co-occurrence provides an appropriate approach to identify highly related terms for the thesaurus entries. Cross-period (diachronic) thesaurus con"
W13-2704,J90-1003,0,0.124687,"cided that applying query expansion (QE) techniques might be a viable solution. 2 Automatic Thesaurus Construction Automatic thesaurus construction focuses on the process of extracting a ranked list of candidate related terms (termed candidate terms) for each given target term. We assume that the top ranked candidates will be further examined (manually) by a lexicographer, who will select the eventual related terms for the thesaurus entry. Statistical measures of first-order similarity (word co-occurrence), such as Dice coefficient (Smadja et al., 1996) and Pointwise Mutual Information (PMI) (Church and Hanks, 1990), were commonly used to extract ranked lists of candidate related terms. These measures consider the number of times in which each candidate term cooccurs with the target term, in the same document, relative to their total frequencies in the corpus. In our setting, we construct a thesaurus for a morphologically rich language (Hebrew). Therefore, we followed the Liebeskind et al. (2012) algorithmic scheme designed for these cases, summarized below. First, our target term is represented in its lemma form. For each target term we retrieve all the corpus documents containing this given target term"
W13-2704,W02-0908,0,0.0528792,"r research is to support constructing a high-quality publishable thesaurus, as a cultural resource on its own, alongside being a useful tool for supporting searches in the domain. Since the precision of fully automaticallyconstructed thesauri is typically low (e.g. (Mihalcea et al., 2006)), we present a semi-automatic setting for supporting thesaurus construction by a domain expert lexicographer. Our recall-oriented setting assumes that manual effort is worthwhile for increasing recall as long as it is being utilized effectively. Corpus-based thesaurus construction is an active research area (Curran and Moens, 2002; Kilgarriff, 2003; Rychl´y and Kilgarriff, 2007; Liebeskind et al., 2012; Zohar et al., 2013). Typically, two statistical approaches for identifying semantic relatedness between words were investigated: first-order (co-occurrence-based) similarity and second-order (distributional) similarity (Lin, 1998; Gasperin et al., 2001; Weeds and Weir, 2003; Kotlerman et al., 2010). In this research, we focus on statistical measures of first-order similarity (see Section 2). These methods were found to be effective for thesaurus construction as standalone methods and as complementary to secondorder meth"
W13-2704,P07-2011,0,0.0455597,"Missing"
W13-2704,W11-1501,0,0.0375807,"Missing"
W13-2704,P08-2016,0,0.349085,"Missing"
W13-2704,P11-1109,0,0.0265669,"Missing"
W13-2704,P09-1051,1,0.841727,"calculated, based on their document-level statistics in the corpus. After sorting the terms based on their scores, the highest rated candidate terms are clustered into lemma-based clusters. Finally, we rank the clusters by summing the co-occurrence scores of their members and the highest rated clusters constitute the candidate terms for the given target term, to be presented to a domain expert. We recognized two potential types of sources of lexical expansions for the target terms. The first is lexical resources available over the internet for extracting different types of semantic relations (Shnarch et al., 2009; Bollegala et al., 2011; Hashimoto et al., 2011). The second is lists of related terms extracted from a mixed corpus by a first-order co-occurrence measure. These lists contain both ancient and modern terms. Although only ancient terms will be included in the final thesaurus, modern terms can be utilized for QE to increase thesaurus coverage. Furthermore, expanding the target term with ancient related terms enables the use of ancient-only corpora for cooccurrence extraction. Following these observations, we present an iterative interactive QE scheme for bootstrapping thesaurus construction. T"
W13-2704,J96-1001,0,0.021613,"er of documents in the statistical extraction process, and decided that applying query expansion (QE) techniques might be a viable solution. 2 Automatic Thesaurus Construction Automatic thesaurus construction focuses on the process of extracting a ranked list of candidate related terms (termed candidate terms) for each given target term. We assume that the top ranked candidates will be further examined (manually) by a lexicographer, who will select the eventual related terms for the thesaurus entry. Statistical measures of first-order similarity (word co-occurrence), such as Dice coefficient (Smadja et al., 1996) and Pointwise Mutual Information (PMI) (Church and Hanks, 1990), were commonly used to extract ranked lists of candidate related terms. These measures consider the number of times in which each candidate term cooccurs with the target term, in the same document, relative to their total frequencies in the corpus. In our setting, we construct a thesaurus for a morphologically rich language (Hebrew). Therefore, we followed the Liebeskind et al. (2012) algorithmic scheme designed for these cases, summarized below. First, our target term is represented in its lemma form. For each target term we ret"
W13-2704,S12-1009,1,0.423888,"us, as a cultural resource on its own, alongside being a useful tool for supporting searches in the domain. Since the precision of fully automaticallyconstructed thesauri is typically low (e.g. (Mihalcea et al., 2006)), we present a semi-automatic setting for supporting thesaurus construction by a domain expert lexicographer. Our recall-oriented setting assumes that manual effort is worthwhile for increasing recall as long as it is being utilized effectively. Corpus-based thesaurus construction is an active research area (Curran and Moens, 2002; Kilgarriff, 2003; Rychl´y and Kilgarriff, 2007; Liebeskind et al., 2012; Zohar et al., 2013). Typically, two statistical approaches for identifying semantic relatedness between words were investigated: first-order (co-occurrence-based) similarity and second-order (distributional) similarity (Lin, 1998; Gasperin et al., 2001; Weeds and Weir, 2003; Kotlerman et al., 2010). In this research, we focus on statistical measures of first-order similarity (see Section 2). These methods were found to be effective for thesaurus construction as standalone methods and as complementary to secondorder methods (Peirsman et al., 2008). First-order measures assume that words that"
W13-2704,W03-1011,0,0.0224715,"s construction by a domain expert lexicographer. Our recall-oriented setting assumes that manual effort is worthwhile for increasing recall as long as it is being utilized effectively. Corpus-based thesaurus construction is an active research area (Curran and Moens, 2002; Kilgarriff, 2003; Rychl´y and Kilgarriff, 2007; Liebeskind et al., 2012; Zohar et al., 2013). Typically, two statistical approaches for identifying semantic relatedness between words were investigated: first-order (co-occurrence-based) similarity and second-order (distributional) similarity (Lin, 1998; Gasperin et al., 2001; Weeds and Weir, 2003; Kotlerman et al., 2010). In this research, we focus on statistical measures of first-order similarity (see Section 2). These methods were found to be effective for thesaurus construction as standalone methods and as complementary to secondorder methods (Peirsman et al., 2008). First-order measures assume that words that frequently occur together are topically related (Sch¨utze and Pedersen, 1997). Thus, co-occurrence provides an appropriate approach to identify highly related terms for the thesaurus entries. Cross-period (diachronic) thesaurus construction aims to enable potential users to s"
W14-1610,P12-3014,1,0.948998,"each directed edge reflects an entailment relation, in the spirit of textual entailment (Dagan et al., 2013). Entailment provides an effective structure for aggregating natural-language based information; it merges semantically equivalent propositions into cliques, and induces specification-generalization edges between them. For example, (aspirin, eliminate, headache) entails, and is more specific than, (headache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailm"
W14-1610,P98-2127,0,0.360271,"Missing"
W14-1610,D12-1048,0,0.0160256,"les. Though this problem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffe"
W14-1610,J12-1003,1,0.938774,"ache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite difOpen IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equi"
W14-1610,J90-1003,0,0.0898991,"uses only the argument entailment graph (as produced by Opt(Arg) ∧ Opt(P red)) to decide on proposition entailment; i.e. a pair of propositions entail if and only if their arguments entail. Opt(P red) is defined analogously. We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1 , p, or a2 . The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the"
W14-1610,N13-1018,0,0.0708682,"senseBackground Our work builds upon two major research threads: open IE, and entailment graphs. 88      pij π x log + log . The prior i6=j ij 1−pij 1−π term π is the probability of a random pair of predicates to be in an entailment relation, and can be estimated in advance. The ILP solver searches for the optimal assignment that maximizes the objective function under transitivity constraints, expressed as linear constraints ∀i,j,k xij + xjk − xik ≤ 1. P disambiguated nouns and their hyponymy relations. Berant et al (2012) constructed entailment graphs of predicate templates. Recently, Mehdad et al (2013) built an entailment graph of noun phrases and partial sentences for topic labeling. The notion of proposition entailment graphs, however, is novel. This distinction is critical, because apparently, entailment in the context of specific propositions does not behave like contextoblivious lexical entailment (see Section 8). Berant et al’s work was implemented in Adler et al’s (2012) text exploration demo, which instantiated manually-annotated predicate entailment graphs with arguments, and used an additional lexical resource to determine argument entailment. The combined graphs of predicate and"
W14-1610,P13-1131,1,0.874892,"Missing"
W14-1610,N13-1008,0,0.137916,"induced by proposition entailment. 1 Introduction Open information extraction (open IE) extracts natural language propositions from text without pre-defined schemas as in supervised relation extraction (Etzioni et al., 2008). These propositions represent predicate-argument structures as tuples of natural language strings. Open IE enables knowledge search by aggregating billions of propositions from the web1 . It may also be perceived as capturing an unsupervised knowledge representation schema, complementing supervised knowledge bases such as Freebase (Bollacker et al., 2008), as suggested by Riedel et al (2013). However, language variability obstructs open IE from becoming a viable knowledge representation framework. As it does not consolidate natural language expressions, querying a database of open IE propositions may lead to either insufficient or redundant information. As an illustrative example, 1 See demo: openie.cs.washington.edu 87 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87–97, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Figure 1: An excerpt from a proposition entailment graph focused on the topic heada"
W14-1610,D11-1142,0,0.00681112,"ment between language-based predicate-argument tuples. Though this problem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, c"
W14-1610,P06-1101,0,0.498542,"ilment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite difOpen IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equivalent propositions and induces a specific-to-general structure."
W14-1610,S13-1035,0,0.034538,"s, hypernyms, and (WordNet) entailments as positive examples, and antonyms, hyponyms, and cohyponyms as negative. The global optimization phase then searches for the most probable transitive entailment graph, given the local probability estimations. It does so with an integer linear program (ILP), where each pair of predicates is represented by a binary variable xij , denoting whether there is an entailment edge from i to j. The objective function corresponds to the log likelihood of the assignment: 4 Dataset To construct our dataset of open IE extractions, we found Google’s syntactic ngrams (Goldberg and Orwant, 2013) as a useful source of high-quality propositions. Based on a corpus of 3.5 million English books, it aggregates every syntactic ngram 89 – subtree of a dependency parse – with at most 4 dependency arcs. The resource contains only tree fragments that appeared at least 10 times in the corpus, filtering out many low-quality syntactic ngrams. We extracted the syntactic ngrams that reflect propositions, i.e. subject-verb-object fragments where object modifies the verb with either dobj or pobj. Prepositions in pobj were concatenated to the verb (e.g. use with). In addition, both subject and object m"
W14-1610,W03-1011,0,0.759979,"il. Opt(P red) is defined analogously. We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1 , p, or a2 . The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the context of the given arguments. We leveraged the Unified Medical Language System (UMLS) to check argument entailment, using the parent and synonym relations. A single feature indicated"
W14-1610,N13-1107,0,0.00699842,"s simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffeine, but not one anoth"
W14-1610,C98-2122,0,\N,Missing
W14-1619,P06-1046,0,0.0538911,"Missing"
W14-1619,N09-1003,0,0.457211,"Missing"
W14-1619,P98-2127,0,0.294726,"d interchangeably, it is common to distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2001; Agirre et al., 2009). Semantic similarity is used to describe ‘likeness’ relations, such as the relations between synonyms, hypernymhyponyms, and co-hyponyms. Semantic relatedness refers to a broader range of relations including also meronymy and various other associative relations as in ‘pencil-paper’ or ‘penguinAntarctica’. In this work we focus on semantic similarity and evaluate all compared methods on several semantic similarity tasks. Following previous works (Lin, 1998; Riedl and Biemann, 2013) we use Wordnet to construct large scale gold standards for semantic similarity evaluations. We perform the evaluations separately for nouns and verbs to test our hypothesis that our model is particularly well-suited for verbs. To further evaluate our results on verbs we use the verb similarity test-set released by (Yang and Powers, 2006), which contains pairs of verbs associated with semantic similarity scores based on human judgements. As intended, this similarity measure promotes word pairs in which both b is likely in the contexts of a and vice versa. Next, we des"
W14-1619,P13-1131,1,0.850707,"ur model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all observed contexts of a target word type regardless of its different senses. However, we believe that our approach is well suited to address context sensitive similarity with proper enhancements, as it considers joint-contexts that can more accurately disambiguate the meaning of target words. As an example, it is possible to consider the likelihood of word b to occur in a subset of the contexts observed for word a, which is biased towards a particular sense of a. Future Directions In this paper we"
W14-1619,J92-4003,0,0.659211,"n particular are correlated with their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate the benefits of considering joint-contexts of words. As an illustrative example, we note that the target words like and surround may share many individual word features such as “school” and “campus” in the"
W14-1619,P12-1015,0,0.0428918,"in contrast to the feature vector baselines, whose performance declines for context window orders larger than 2. This suggests that our approach is able to take advantage of larger contexts in comparison to standard feature vector models. The decline in performance for the independent feature vector baseline (IFV) may be related to the fact that independent features farther away from the target word are generally more loosely related to it. This seems consistent with previous works, where narrow windows of the order of two words performed well (Bullinaria and Levy, 2007; Agirre et al., 2009; Bruni et al., 2012) and in particular so when evaluating semantic similarity rather than relatedness. On the other hand, the decline in performance for the composite feature vector baseline (CFV) may be attributed to the data sparseness phenomenon associated with larger windows. The performance of the word embedding baselines (CBOW and SKIP) starts declining very mildly only for window orders larger than 4. This might be attributed to the fact that these models assign lower weights to context words the farther away they are from the center of the window. VerbSim evaluation The publicly available VerbSim test-set"
W14-1619,P08-1028,0,0.0464893,"the nature of its underlying language model. Therefore, we see much potential in exploring the use of other types of language models, such as class-based (Brown et al., 1992), syntax-based (Pauls and Klein, 2012) or hybrid (Tan et al., 2012). Furthermore, a similar approach to ours could be attempted in word embedding models. For instance, our syntagmatic joint-context modeling approach could be investigated by word embedding models to generate better embeddings for verbs. 6 Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all obse"
W14-1619,P11-1027,0,0.0166443,"ation (Church and Hanks, 1990), and the combination of Cosine with PPMI was shown to perform particularly well in (Bullinaria and Levy, 2007). We denote Mikolov’s CBOW and Skip-gram baseline models by CBOW W −k and SKIP W −k respectively, where k denotes again the order of the window used to train these models. We used Mikolov’s word2vec utility3 with standard parameters (600 dimensions, negative sampling 15) to learn the word embeddings, and Cosine as the vector similarity measure between them. As the underlying probabilistic language model for our method we use the Berkeley implementation4 (Pauls and Klein, 2011) of the Kneser-Ney n-gram model with the default discount parameters. We denote our model P DS W −k , where PDS stands for “Probabilistic Distributional Similarity”, and k is the order of the context word window. In order to avoid giving our model an unfair advantage of tuning the order of the language model n as an additional parameter, we use a fixed n = k + 1. This means that the conditional probabilities that our n-gram model learns consider a scope of up to half the size of the window, which is the distance in words between the target word and either end of the window. We note that this i"
W14-1619,J90-1003,0,0.467863,"overhead, e.g. by counting the learning corpus occurrences of n-gram templates, in which one of the elements matches any word. (5) 184 denote the composite-feature vector baseline by CF V W −k , where CFV stands for “CompositeFeature Vector”. This baseline constructs traditional-like feature vectors, but considers entire word windows around target word tokens as single features. In both of these baselines we use Cosine as the vector similarity measure, and positive pointwise mutual information (PPMI) for the feature vector weights. PPMI is a well-known variant of pointwise mutual information (Church and Hanks, 1990), and the combination of Cosine with PPMI was shown to perform particularly well in (Bullinaria and Levy, 2007). We denote Mikolov’s CBOW and Skip-gram baseline models by CBOW W −k and SKIP W −k respectively, where k denotes again the order of the window used to train these models. We used Mikolov’s word2vec utility3 with standard parameters (600 dimensions, negative sampling 15) to learn the word embeddings, and Cosine as the vector similarity measure between them. As the underlying probabilistic language model for our method we use the Berkeley implementation4 (Pauls and Klein, 2011) of the"
W14-1619,P12-1101,0,0.0174598,"text windows. We further conjecture that the reason the word embedding baselines did not do as well as our model on verb similarity might be due to their particular choice of joint-context formulation, which is not sensitive to word order. However, these conjectures should be further validated with additional evaluations in future work. First, the performance of our generic scheme is largely inherited from the nature of its underlying language model. Therefore, we see much potential in exploring the use of other types of language models, such as class-based (Brown et al., 1992), syntax-based (Pauls and Klein, 2012) or hybrid (Tan et al., 2012). Furthermore, a similar approach to ours could be attempted in word embedding models. For instance, our syntagmatic joint-context modeling approach could be investigated by word embedding models to generate better embeddings for verbs. 6 Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity betwe"
W14-1619,D13-1089,0,0.0532128,"geably, it is common to distinguish between semantic similarity and semantic relatedness (Budanitsky and Hirst, 2001; Agirre et al., 2009). Semantic similarity is used to describe ‘likeness’ relations, such as the relations between synonyms, hypernymhyponyms, and co-hyponyms. Semantic relatedness refers to a broader range of relations including also meronymy and various other associative relations as in ‘pencil-paper’ or ‘penguinAntarctica’. In this work we focus on semantic similarity and evaluate all compared methods on several semantic similarity tasks. Following previous works (Lin, 1998; Riedl and Biemann, 2013) we use Wordnet to construct large scale gold standards for semantic similarity evaluations. We perform the evaluations separately for nouns and verbs to test our hypothesis that our model is particularly well-suited for verbs. To further evaluate our results on verbs we use the verb similarity test-set released by (Yang and Powers, 2006), which contains pairs of verbs associated with semantic similarity scores based on human judgements. As intended, this similarity measure promotes word pairs in which both b is likely in the contexts of a and vice versa. Next, we describe a model which implem"
W14-1619,D10-1113,0,0.0195731,"Finally, we note that our model is not a classic vector space model and therefore common vector composition approaches (Mitchell and Lapata, 2008) cannot be directly applied to it. Instead, other methods, such as similarity of compositions (Turney, 2012), should be investigated to extend our approach for measuring similarity between phrases. Another direction relates to the well known tendency of many words, and particularly verbs, to assume different meanings (or senses) under different contexts. To address this phenomenon context sensitive similarity and inference models have been proposed (Dinu and Lapata, 2010; Melamud et al., 2013). Similarly to many semantic similarity models, our current model aggregates information from all observed contexts of a target word type regardless of its different senses. However, we believe that our approach is well suited to address context sensitive similarity with proper enhancements, as it considers joint-contexts that can more accurately disambiguate the meaning of target words. As an example, it is possible to consider the likelihood of word b to occur in a subset of the contexts observed for word a, which is biased towards a particular sense of a. Future Direc"
W14-1619,rose-etal-2002-reuters,0,0.0145447,"n additional parameter, we use a fixed n = k + 1. This means that the conditional probabilities that our n-gram model learns consider a scope of up to half the size of the window, which is the distance in words between the target word and either end of the window. We note that this is the smallest reasonable value for n, as smaller values effectively mean that there will be context words within the window that are more than n words away from the target word, and therefore will not be considered by our model. As learning corpus we used the first CD of the freely available Reuters RCV1 dataset (Rose et al., 2002). This learning corpus contains approximately 100M words, which is comparable in size to the British National Corpus (BNC) (Aston, 1997). We first applied part-of-speech tagging and lemmatization to all words. Then we represented each word w in the corpus as the pair 3 4 [pos(w), lemma(w)], where pos(w) is a coarsegrained part-of-speech category and lemma(w) is the lemmatized form of w. Finally, we converted every pair [pos(w), lemma(w)] that occurs less than 100 times in the learning corpus to the pair [pos(w), ? ], which represents all rare words of the same part-of-speech tag. Ignoring rare"
W14-1619,J12-3007,0,0.161258,"h their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate the benefits of considering joint-contexts of words. As an illustrative example, we note that the target words like and surround may share many individual word features such as “school” and “campus” in the sentences “Mary’s son likes"
W14-1619,U07-1017,0,0.0592849,"Missing"
W14-1619,D12-1086,1,0.858489,"an underlying model that could capture syntagmatic patterns in large word contexts, yet is flexible enough to deal with data sparseness, is desired. It is generally accepted that the semantics of verbs in particular are correlated with their syntagmatic properties (Levin, 1993; Hanks, 2013). This provides grounds to expect that such model has the potential to excel for verbs. To capture syntagmatic patterns, we choose in this work standard n-gram language models as the basis for a concrete model implementing our scheme. This choice is inspired by recent work on learning syntactic categories (Yatbaz et al., 2012), which successfully utilized such language models to represent word window contexts of target words. However, we note that other richer types of language models, such as class-based (Brown et al., 1992) or hybrid (Tan et al., 2012), can be seamlessly integrated into our scheme. Our evaluations suggest that our model is indeed particularly advantageous for measuring semantic similarity for verbs, while maintaining comparable or better performance with respect to competitive baselines for nouns. 2 3 3.1 Probabilistic Distributional Similarity Motivation In this section we briefly demonstrate th"
W14-1619,C98-2122,0,\N,Missing
W14-2413,P14-2120,1,0.819635,"he truth status of propositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This d"
W14-2413,Q13-1005,0,0.0299228,"oduced by appositive constructions. Other benefits over dependency-trees are explicit marking of logical relations between propositions, explicit marking of multiword predicate such as light-verbs, and a consistent representation for syntacticallydifferent but semantically-similar structures. The representation is meant to serve as a useful input layer for semanticoriented applications, as well as to provide a better starting point for further levels of semantic analysis such as semantic-rolelabeling and semantic-parsing. 1 Introduction Parsers for semantic formalisms (such as Neodavidsonian (Artzi and Zettlemoyer, 2013) and DRT (Kamp, 1988)) take unstructured natural language text as input, and output a complete semantic representation, aiming to capture the meaning conveyed by the text. We suggest that this task may be effectively separated into a sequential combination of two different tasks. The first of these tasks is syntactic abstraction over phenomena such as expression of tense, negation, modality, and passive versus active voice, which are all either expressed or implied from syntactic structure. The second task is semantic interpretation 66 Proceedings of the ACL 2014 Workshop on Semantic Parsing,"
W14-2413,P98-1013,0,0.127419,"ck of space, include propositional modifiers (e.g., relative clause modifiers), propositional arguments (such as ”John asserted that he will go home”), conditionals, and the canonicalization of passive and active voice. 4 Relation to Other Representations Our proposed representation is intended to serve as a bridging layer between purely syntactic representations such as dependency trees, and semantic oriented applications. In particular, we explicitly represent many semantic relations expressed in a sentence that are not captured by contemporary proposition-directed semantic representations (Baker et al., 1998; Kingsbury and Palmer, 2003; Meyers et al., 2004; Carreras and M`arquez, 2005). Compared to dependency-based representations such as Stanford-dependency trees (De Marneffe 1 A case of conjunctions requiring special treatment is introduced by reciprocals, in which the entities roles are exchangeable. For example: “John and Mary bet against each other on future rates” (adaption of wsj 0117). 2 Care needs to be taken to distinguish from cases such as “going to Italy” in which “going to” is not followed by a verbal predicate. 68 5 and Manning, 2008b), we abstract away over many syntactic details"
W14-2413,W08-2222,0,0.0793849,"Missing"
W14-2413,W05-0620,0,0.168162,"Missing"
W14-2413,W08-1301,0,0.404567,"Missing"
W14-2413,J12-4003,0,0.0156683,"ment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our representation much easier"
W14-2413,P11-1060,0,0.0468222,"Missing"
W14-2413,J93-2004,0,0.0457685,"aptures both explicit and implicit propositions, while staying relatively close to the syntactic level. We believe that this kind of representation will serve not only as an advantageous input for semantically-centered applications, such as question answering, summarization and information extraction, but also serve as a rich representation layer that can be used as input for systems aiming to provide a finer level of semantic analysis, such as semantic-parsers. We are currently at the beginning of our investigation. In the near future we plan to semiautomatically annotate the Penn Tree Bank (Marcus et al., 1993) with these structures, as well as to provide software for deriving (some of) the implicit and explicit annotations from automatically produced parse-trees. We believe such resources will be of immediate use to semantic-oriented applications. In the longer term, we plan to investigate dedicated algorithms for automatically producing such representation from raw text. The architecture we describe can easily accommodate additional layers of abstraction, by encoding these layers as features of propositions, predicates or arguments. Such layers can include the marking of named entities, the truth"
W14-2413,W04-2705,0,0.322908,"presentation of copular sentences) 2. Pierre Vinken will join the board as a nonexecutive director Nov. 29. Adjectives, as in the sentence “you emphasized the high prevalence of mental illness” (wsj 0105). Here an adjective is used to describe a definite subject and introduces another proposition, namely the high prevalence of mental illness. Nominalizations, for instance in the sentence “Googles acquisition of Waze occurred yesterday”, introduce the implicit proposition that “Google acquired Waze”. Such propositions were studied and annotated in the NOMLEX (Macleod et al., 1998) and NOMBANK (Meyers et al., 2004) resources. It remains an open issue how to represent or distinguish cases in which nominalization introduce an underspecified proposition. For example, consider “dancing” in “I read a book about dancing”. Possessives, such as “John’s book” introduce the proposition that John has a book. Similarly, examples such as “John’s Failure” combine a possessive construction with nominalization and introduce the proposition that John has failed. Conjunctions - for example in “They operate ships and banks.” (wsj 0083), introduce several propositions in one sentence: 1. They operate ships 2. They operate"
W14-2413,S10-1008,0,0.0268506,"ositions and author commitment. In the current version infinitive constructions are treated as nested propositions, similar to their representation in syntactic parse trees. Providing a consistent, useful and transparent representation for infinitive constructions is a challenging direction for future research. Other extensions of the proposed representation are also possible. One appealing direction is going beyond the sentence level and representing discourse level relations, including implied propositions and predicate - argument relationships expressed by discourse (Stern and Dagan, 2014; Ruppenhofer et al., 2010; Gerber and Chai, 2012). Such an extension may prove useful as an intermediary representation for parsers of semantic formalisms targeted at the discourse level (such as DRT). Compared to proposition-based semantic representations, we do not attempt to assign framespecific thematic roles, nor do we attempt to disambiguate or interpret word meanings. We restrict ourselves to representing predicates by their (lemmatized) surface forms, and labeling arguments based on a “syntactic” role inventory, similar to the label-sets available in dependency representations. This design choice makes our rep"
W14-2413,C98-1013,0,\N,Missing
W14-4504,P12-3014,1,0.824939,"rom those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might not have considered a-priori. 4 Discussion In this position paper we outlined a framework for information discovery that leverages and extends Open IE, while ad"
W14-4504,W13-2322,0,0.0170123,"drawbacks. The proposed framework enriches Open IE by representing natural language in a traversable graph, composed of propositions and their semantic interrelations – A Propositional Knowledge Graph (PKG). The resulting structure provides a representation in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a single sentence, and globally, at inter-proposition level, where relations are drawn between propositions from discourse, or from various sources. At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for predicates’ meaning representation, where possible, and ungrounded natural language, where no respective 22 Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic interpretation. At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (suc"
W14-4504,W08-1301,0,0.0604435,"Missing"
W14-4504,S14-1010,0,0.0197595,"rmation from multiple documents on the same topic. PKGs can be a natural platform leveraged by summarization because: (1) they would contain the information from those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might no"
W14-4504,W14-1610,1,0.832308,"texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both entail an additional proposition from a third source: “Curiosity is a lab”. If one were to query all the 21 propositions that entail “Curiosity is a lab” – e.g. in response to the query “What is Curiosity?” – all three propositions would be retrieved, even though their surface forms may have “functions as” instead of “is” or “laboratory” instead of “lab”. We have recently taken some first steps in this direction, investigating algorithms for constructing entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions, recognizing entailment is challenging. We are currently working on new methods that will leverage structured and unstructured data to recognize entailment for Open IE propositions. There are additional relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for recognizing and utilizing these relations is intended for future work. 3 Applications An appealing application of knowledge graphs is question answering (QA)."
W14-4504,D12-1048,0,0.0391103,"Open IE systems retrieve only propositions in which both predicates and arguments are instantiated in succession in the surface form. For such propositions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of a predicate and a list of its arguments, all expressed in natural language, in the same way they originally appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are inherently embedded, such as conditionals and propositional arguments (e.g. “Senator Kennedy asked congress to pass the bill”). Mausam et al. (2012) introduced a context analysis layer, extending this 20 representation with an additional field per tuple, which intends to represent the factuality of the extraction, accounting specifically for cases of conditionals and attribution. For instance, the assertion “If he wins five key states, Romney will be elected President” will be represented as ((Romney; will be elected; President) ClausalModifier if; he wins five key states). While these methods capture some of the propositions conveyed by text, they fail to retrieve other propositions expressed by more sophisticated syntactic constructs. C"
W14-4504,W14-2413,1,0.823008,"te (namely “acquired”) does not appear in the surface form. Implicit propositions might be introduced in many other linguistic constructs, such as: appositions (“The company, Random House, doesn’t report its earnings.” implies that Random House is a company), adjectives (“Tall John walked home” implies that John is tall), and possessives (“John’s book is on the table” implies that John has a book). We intend to syntactically identify these implicit propositions, and make them explicit in our representation. For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al., 2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe and Manning, 2008) to interconnected propositions as described. 2.2 Consolidating Information across Propositions While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natural language expressions, which leads to either insufficient or redundant information when accessing a repository of Open IE extractions. As an illustrating example, querying the Univers"
W15-1501,D14-1082,0,0.00614467,"vides the same kind of data as LS-SE, but instead of choosing specific target words that tend to be ambiguous as done in LS-SE, the target words here are all the content words in text documents extracted from news and fiction corpora, and are therefore more naturally distributed. LS-CIC is also much larger than LS-SE with over 15K target word instances. 4 4.2 Compared methods We used ukWaC (Ferraresi et al., 2008), a two billion word web corpus, as our learning corpus. We parsed both ukWaC and the sentences in the lexical substitution datasets with Stanford’s Neural Network Dependency Parser (Chen and Manning, 2014).4 Following Levy and Goldberg (2014a), we learned syntax-based skip-gram word and context embeddings using word2vecf (with 600 dimensions and 15 negative sampling), converting all tokens to lowercase, discarding words and syntactic contexts that appear less than 100 times in the corpus and ‘collapsing’ dependencies that include prepositions. This resulted in a vocabulary of about 200K word embeddings and 1M context embeddings. 5 Finally, for every instance in the lexical substitution datasets, we extracted the syntactic contexts of the target word and used each of our measures, Add, BalAdd, M"
W15-1501,P12-1092,0,0.0783026,"model today is skip-gram, introduced in Mikolov et al. (2013) and available as part of the word2vec toolkit.1 word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space. However, the context representations are considered internal to the model and are discarded after training. The output word embeddings represent context-insensitive target word types. Few recent models extended word embeddings by learning a distinct representation for each sense of a target word type, as induced by clustering the word’s contexts (Huang et al., 2012; Neelakantan et al., 2014). They then identify the relevant sense(s) for a given word instance, in order to measure contextsensitive similarities. Although these models may be considered for lexical substitution, they have so far been applied only to ‘softer’ word similarity tasks which include topical relations. In this work we propose a simple approach for directly utilizing the skip-gram model for contextsensitive lexical substitution. Instead of discarding the learned context embeddings, we use them in conjunction with the target word embeddings to model target word instances. A suitable"
W15-1501,E14-1057,0,0.341915,"Missing"
W15-1501,P14-2050,1,0.499272,"he same low-dimensional space. In this space, the vector representations of a target and context are pushed closer together the more frequently they co-occur in a learning corpus. Thus, the Cosine distance between them can be viewed as a first-order target-to-context similarity measure, indicative of their syntagmatic compatibility. Indirectly, this also results in assigning similar vector representations to target words that share similar contexts, thereby suggesting the Cosine distance between word embeddings as a second-order target-to-target distributional similarity measure. word2vecf 3 (Levy and Goldberg, 2014a) is an extension of the skip-gram implementation in word2vec, which supports arbitrary types of contexts rather than only word window contexts. Levy and Goldberg (2014a) used word2vecf to produce syntax-based word embeddings, where context elements are the syntactic contexts of the target words. Specifically, for a target word t with modifiers m1 ,...,mk and head h, they considered the context elements (m1 , r1 ),...,(mk , rk ),(h, rh−1 ), where r is the type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation."
W15-1501,W14-1618,1,0.298193,"he same low-dimensional space. In this space, the vector representations of a target and context are pushed closer together the more frequently they co-occur in a learning corpus. Thus, the Cosine distance between them can be viewed as a first-order target-to-context similarity measure, indicative of their syntagmatic compatibility. Indirectly, this also results in assigning similar vector representations to target words that share similar contexts, thereby suggesting the Cosine distance between word embeddings as a second-order target-to-target distributional similarity measure. word2vecf 3 (Levy and Goldberg, 2014a) is an extension of the skip-gram implementation in word2vec, which supports arbitrary types of contexts rather than only word window contexts. Levy and Goldberg (2014a) used word2vecf to produce syntax-based word embeddings, where context elements are the syntactic contexts of the target words. Specifically, for a target word t with modifiers m1 ,...,mk and head h, they considered the context elements (m1 , r1 ),...,(mk , rk ),(h, rh−1 ), where r is the type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation."
W15-1501,Q15-1016,1,0.135652,"tween the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation. Similarly to traditional syntax-based vector space models (Pad´o and Lapata, 2007), they show that these embeddings tend to capture functional word similarity (as in manage ∼ supervise) rather than topi2 While in this work we focus on skip-gram embeddings, we note that there are also other potentially relevant word embedding methods that can generate context representations in addition to the ‘standard’ target word representations. See, for example, GloVe (Pennington et al., 2014) and SVD-based methods (Levy et al., 2015). 3 https://bitbucket.org/yoavgo/word2vecf 2 Figure 1: A 2-dimensional visualization of the gerunds singing, dancing, driving, and healing with their top syntactic contexts in an embedded space. singing and dancing share many similar contexts (e.g. partmod song and dobj jive) and therefore end up with very similar vector representations. cal similarity or relatedness (as in manage ∼ manager). Figure 1 illustrates a syntax-based embedding space using t-SNE (Van der Maaten and Hinton, 2008), which visualizes the similarities in the original higher-dimensional space. 3 Lexical Substitution Model"
W15-1501,S07-1009,0,0.870273,"sed on the popular skip-gram word embedding model. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting. 1 Introduction Lexical substitution tasks have become very popular for evaluating context-sensitive lexical inference models since the introduction of the original task in SemEval-2007 (McCarthy and Navigli, 2007) and additional later variants (Biemann, 2013; Kremer et al., 2014). In these tasks, systems are required to predict substitutes for a target word instance, which preserve its meaning in a given sentential context. Recent models addressed this challenge mostly in an unsupervised setting. They typically generated a word instance representation, which is biased towards its given context, and then identified substitute words based on their similarity to this biased representation. Various types of models were proposed, from sparse syntax-based vector models (Thater et al., 2011), to probabilistic"
W15-1501,D14-1113,0,0.0528929,"gram, introduced in Mikolov et al. (2013) and available as part of the word2vec toolkit.1 word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space. However, the context representations are considered internal to the model and are discarded after training. The output word embeddings represent context-insensitive target word types. Few recent models extended word embeddings by learning a distinct representation for each sense of a target word type, as induced by clustering the word’s contexts (Huang et al., 2012; Neelakantan et al., 2014). They then identify the relevant sense(s) for a given word instance, in order to measure contextsensitive similarities. Although these models may be considered for lexical substitution, they have so far been applied only to ‘softer’ word similarity tasks which include topical relations. In this work we propose a simple approach for directly utilizing the skip-gram model for contextsensitive lexical substitution. Instead of discarding the learned context embeddings, we use them in conjunction with the target word embeddings to model target word instances. A suitable substitute for a 1 https://"
W15-1501,J14-3005,0,0.0826541,"Missing"
W15-1501,J07-2002,0,0.0235971,"Missing"
W15-1501,D14-1162,0,0.114645,"type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation. Similarly to traditional syntax-based vector space models (Pad´o and Lapata, 2007), they show that these embeddings tend to capture functional word similarity (as in manage ∼ supervise) rather than topi2 While in this work we focus on skip-gram embeddings, we note that there are also other potentially relevant word embedding methods that can generate context representations in addition to the ‘standard’ target word representations. See, for example, GloVe (Pennington et al., 2014) and SVD-based methods (Levy et al., 2015). 3 https://bitbucket.org/yoavgo/word2vecf 2 Figure 1: A 2-dimensional visualization of the gerunds singing, dancing, driving, and healing with their top syntactic contexts in an embedded space. singing and dancing share many similar contexts (e.g. partmod song and dobj jive) and therefore end up with very similar vector representations. cal similarity or relatedness (as in manage ∼ manager). Figure 1 illustrates a syntax-based embedding space using t-SNE (Van der Maaten and Hinton, 2008), which visualizes the similarities in the original higher-dimens"
W15-1501,D13-1198,0,0.19746,"Missing"
W15-1501,I11-1127,0,0.0390161,"Missing"
W15-3714,S12-1009,1,0.8636,"the context of fasts, as in two of the Jewish fasts wearing leather shoes is forbidden and people wear cloth-made slippers. Evaluation Setting We applied our method to the diachronic corpus is the Responsa project Hebrew corpus5 . The Responsa corpus includes rabbinic case-law rulings which represent the historical-sociological milieu of real-life situations, collected over more than a thousand years, from the 11th century until today. The corpus consists of 81,993 documents, and was used for previous NLP and IR research (Choueka et al., 1971; Choueka et al., 1987; HaCohenKerner et al., 2008; Liebeskind et al., 2012; Zohar et al., 2013; Liebeskind et al., 2013). The candidate target terms for our classification task were taken from the publicly available keylist of Hebrew Wikipedia entries6 . Since many of these tens of thousands entries, such as person names and place names, were not suitable as target terms, we first filtered them by Hebrew Named Entity Recognition7 and manually. Then, a list of approximately 5000 candidate target terms was manually annotated by two domain experts. The experts decided which of the candidates corresponds to a concept that has been discussed significantly in our diachron"
W15-3714,W13-2704,1,0.921022,"fic diachronic thesaurus. We propose a supervised learning scheme, which integrates features from two closely related fields: Terminology Extraction and Query Performance Prediction (QPP). Our method further expands modern candidate terms with ancient related terms, before assessing their corpus relevancy with QPP measures. We evaluate the empirical benefit of our method for a thesaurus for a diachronic Jewish corpus. 1 Introduction In recent years, there has been growing interest in diachronic lexical resources, which comprise terms from different language periods. (Borin and Forsberg, 2011; Liebeskind et al., 2013; Riedl et al., 2014). These resources are mainly used for studying language change and supporting searches in historical domains, bridging the lexical gap between modern and ancient language. In particular, we are interested in this paper in a certain type of diachronic thesaurus. It contains entries for modern terms, denoted as target terms. Each entry includes a list of ancient related terms. Beyond being a historical linguistic resource, such thesaurus is useful for supporting searches in a diachronic corpus, composed of both modern and ancient documents. For example, in our historical Jew"
W15-3714,P08-2016,0,0.069145,"Missing"
W15-3714,loukachevitch-2012-automatic,0,0.175916,"the known Terminology Extraction (TE) task in NLP. The goal of corpus-based TE is to automatically extract prominent terms from a given corpus and score them for domain relevancy. In our setting, since all the target terms are modern, we avoid extracting them from the diachronic corpus of modern and ancient language. Instead, we use a given candidate list and apply only the term scoring phase. As a starting point, we adopt a rich set of state-of-the-art TE scoring measures and integrate them as features in a common supervised classification approach (Foo and Merkel, 2010; Zhang et al., 2010; Loukachevitch, 2012). Given our Information Retrieval (IR) motivation, we notice a closely related task to TE, namely Query Performance Prediction (QPP). QPP methods are designed to estimate the retrieval quality of search queries, by assessing their relevance to the text collection. Therefore, QPP scoring measures A diachronic thesaurus is a lexical resource that aims to map between modern terms and their semantically related terms in earlier periods. In this paper, we investigate the task of collecting a list of relevant modern target terms for a domain-specific diachronic thesaurus. We propose a supervised lea"
W15-3714,J81-4005,0,0.671706,"Missing"
W15-3714,riedl-etal-2014-distributed,0,0.0233337,"Missing"
W16-5304,C92-2082,0,0.706288,". We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results"
W16-5304,E12-1004,0,0.646952,"ibutional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new d"
W16-5304,D10-1108,0,0.0533109,"le hidden layer trained on [~vwx , ~vwy ] (DSh ).2 Integrated We similarly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-defining c = softmax(W2 · ~h + b2 ), where ~h = tanh(W1 · ~vxy + b1 ) is the hidden layer. The technical details of our network are identical to Shwartz et al. (2016). 4 Datasets We use four common semantic relation datasets that were created using semantic resources: K&H+N (Necsulescu et al., 2015) (an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al., 2016). Table 1 displays the relation types and number of instances in each dataset. Most dataset relations are parallel to WordNet relations, such as hypernymy (cat, animal) and meronymy (hand, body), with an additional random relation for negative instances. BLESS contains the event and attribute relations, connecting a concept with a typical activity/property (e.g. (alligator, swim) and (alligator, aquatic)). EVALution contains a richer schema of semantic relations, with some redundancy: it contain"
W16-5304,N15-1098,1,0.910266,"semantic relations, as we describe next. 3 Classification Methods We experiment with several classification models, as illustrated in Figure 1: Path-based HypeNET’s path-based model (PB) is a binary classifier trained on the path vectors alone: ~vpaths(x,y) . We adapt the model to classify multiple relations by changing the network softmax output c to a distribution over k target relations, classifying a pair to the highest scoring relation: r = argmaxi c[i]. Distributional We train an SVM classifier on the concatenation of x and y’s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS).1 Levy et al. (2015) claimed that such a linear classifier is incapable of capturing interactions between x and y’s features, and that instead it learns separate properties of x or y, e.g. that y is a prototypical hypernym. To examine the effect of non-linear expressive power on the model, we experiment with a neural network with a single hidden layer trained on [~vwx , ~vwy ] (DSh ).2 Integrated We similarly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-"
W16-5304,P06-2075,1,0.812476,"Missing"
W16-5304,D12-1104,0,0.0574132,"ased information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no m"
W16-5304,S15-1021,0,0.239148,"s, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes multiple semantic relations. W"
W16-5304,P15-1146,0,0.103244,"Missing"
W16-5304,D14-1162,0,0.0827114,"he lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed t"
W16-5304,N13-1008,0,0.0531513,"always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution o"
W16-5304,C14-1097,0,0.112538,"methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper"
W16-5304,W15-4200,0,0.195003,"arly adapt the HypeNET integrated model to classify multiple semantic relations (LexNET). Based on the same motivation of DSh , we also experiment with a version of the network with a hidden layer (LexNETh ), re-defining c = softmax(W2 · ~h + b2 ), where ~h = tanh(W1 · ~vxy + b1 ) is the hidden layer. The technical details of our network are identical to Shwartz et al. (2016). 4 Datasets We use four common semantic relation datasets that were created using semantic resources: K&H+N (Necsulescu et al., 2015) (an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al., 2016). Table 1 displays the relation types and number of instances in each dataset. Most dataset relations are parallel to WordNet relations, such as hypernymy (cat, animal) and meronymy (hand, body), with an additional random relation for negative instances. BLESS contains the event and attribute relations, connecting a concept with a typical activity/property (e.g. (alligator, swim) and (alligator, aquatic)). EVALution contains a richer schema of semantic relations, with some redundancy: it contains both meronymy and holonymy (e.g. for bicycle and wheel), and the"
W16-5304,L16-1722,0,0.617616,"ven pair in the corpus, where the dependency paths that connect the terms are typically used as features (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term. These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), consistently outperforming other methods (Santus et al., 2016; Necsulescu et al., 2015). While these two sources have been considered complementary, recent results suggested that pathbased methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) presented HypeNET, an integrated path-based and distributional method for hypernymy detection. They showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes mult"
W16-5304,W16-5310,1,0.693144,"ath representation can provide substantial complementary information to the distributional signal in hypernymy detection, notably improving results on a new dataset. In this paper we present LexNET, an extension of HypeNET that recognizes multiple semantic relations. We show that this integrated method is indeed effective also in the multiclass setting. In the evaluations reported in this paper, LexNET performed better than each individual method on several common datasets. Further, it was the best performing system in the semantic relation classification task of the CogALex 2016 shared task (Shwartz and Dagan, 2016). We further assess the contribution of path-based information to semantic relation classification. Even though the distributional source is dominant across most datasets, path-based information always contributed to it. In particular, path-based information seems to better capture the relationship between terms, rather than their individual properties, and can do so even for rare words or senses. Our code and data are available at https://github.com/vered1986/LexNET. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.o"
W16-5304,P16-1226,1,0.73755,". Distributional Information in Recognizing Lexical Semantic Relations Vered Shwartz Ido Dagan Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel vered1986@gmail.com dagan@cs.biu.ac.il Abstract Recognizing various semantic relations between terms is beneficial for many NLP tasks. While path-based and distributional information sources are considered complementary for this task, the superior results the latter showed recently suggested that the former’s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. The empirical results show that this method is effective in the multiclass setting as well. We further show that the path-based information source always contributes to the classification, and analyze the cases in which it mostly complements the distributional information. 1 Introduction Automated methods to recognize the lexical semantic relation the holds between terms are valuable for NLP applications. Two main information sources are used to recognize such relations: path-based and distributional. Path-based methods consider the joint occurre"
W16-5304,J13-3004,0,\N,Missing
W16-5304,W11-2501,0,\N,Missing
W16-5304,W15-4208,0,\N,Missing
W16-5304,W11-2500,0,\N,Missing
W16-5310,C92-2082,0,0.764622,"lated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based fe"
W16-5310,E12-1004,0,0.585701,"al information sources in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/si"
W16-5310,P13-2080,1,0.817447,"emantic relations. Combined with a common similarity measure, LexNET performs fairly good on the word relatedness task (subtask 1). The relatively low performance of LexNET and all other systems on subtask 2, however, confirms the difficulty of the semantic relation classification task, and stresses the need to develop additional methods for this task. 1 Introduction Discovering whether words are semantically related and identifying the specific semantic relation that holds between them is a key component in many NLP applications, such as question answering and recognizing textual entailment (Dagan et al., 2013). Automated methods for semantic relation identification are commonly corpus-based, and mainly rely on the distributional representation of each word. The CogALex shared task on the corpus-based identification of semantic relations consists of two subtasks. In the first task, the system needs to identify for a word pair whether the words are semantically related or not (e.g. True:(dog, cat), False:(dog, fruit)). In the second task, the goal is to determine the specific semantic relation that holds for a given pair, if any (PART OF:(tail, cat), HYPER:(cat, animal)). In this paper we describe ou"
W16-5310,P14-1113,0,0.160503,"2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus e"
W16-5310,W13-2608,0,0.0201949,"tor similarity or distance measure is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2"
W16-5310,P99-1004,0,0.266338,"dded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network that outputs the class distribution ~c, and then the pair is classified to the relation with the highest score r: ~c = softmax(MLP(~vxy )) (1a) r = argmaxi ~c[i] (1b) MLP stands for Multi Layer Perceptron, and could be computed with or without a hidden layer (equations 2 and 3, respectively): ~h = tanh(W1 · ~vxy + b1 ) MLP(~vxy ) = W2 · ~h + b2 (2b) MLP(~vxy ) = W1 · ~vxy + b1 (3) where Wi and bi are the network parameters and ~h is the hidden layer. 2 See Lee (1999) for an extensive list of such measures. 81 (2a) Subtask 1 Subtask 2 Method Cos LexNET LexNET+Cos Dist LexNET Hyper-parameters word2vec, t: 0.3 hidden layers: 0, dropout: 0.0, epochs: 3 word2vec, wL = 0.3, wC = 0.7, t = 0.29 dep-based, method: concat, classifier: SVM, L1 hidden layers: 0, dropout: 0.0, epochs: 5 Corpus size 100B 6B ∼100B 3B 6B P 0.759 0.780 0.814 0.611 0.658 R 0.795 0.561 0.854 0.598 0.646 F1 0.776 0.652 0.833 0.600 0.642 Table 1: Performance scores on the validation set along with hyper-parameters and effective corpus size (#tokens) used by each method. Subtask 2 results refe"
W16-5310,P14-2050,0,0.0408781,"ty scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performance scores on the test set in each subtask, of the selected methods and the baselines. 4.2 Subtask 2: Semantic Relation Classification The subtask’s train set is highly imbalanced towards random instances (roughly 10 times more than any other relation), and training any supervised method leads to overf"
W16-5310,N15-1098,1,0.941091,"LATED] (4) where wC , wL are the weights assigned to cosine similarity and LexNET’s scores respectively, such that wC + wL = 1. We tuned the weights and a threshold t using the validation set, and classified (x, y) as related if Rel(x, y) ≥ t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performan"
W16-5310,D12-1104,0,0.522195,"lation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ]"
W16-5310,S15-1021,0,0.289491,"ler et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, and ~vpaths(x,y) is the average embedding vector of all the dependency paths that connect x and y in the corpus. Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed int"
W16-5310,D14-1162,0,0.0863279,"set, and classified (x, y) as related if Rel(x, y) ≥ t. The word vectors used to compute the cosine similarity scores were chosen among several available pre-trained embeddings.4 For completeness we also report the performance of two baselines: cosine similarity (wC = 1) and LexNET (wL = 1, fixed t = 0.5). 3 A random split yielded perfect results on the validation set, which were due to lexical memorization (Levy et al., 2015). word2vec (300 dimensions, SGNS, trained on GoogleNews, 100B tokens) (Mikolov et al., 2013), GloVe (50-300 dimensions, trained on Wikipedia and Gigaword 5, 6B tokens) (Pennington et al., 2014), and dependency-based embeddings (300 dimensions, trained on Wikipedia, 3B tokens) (Levy and Goldberg, 2014). 4 82 Subtask 1 Subtask 2 Method Random Baseline Majority Baseline Cos LexNET+Cos Random Baseline Majority Baseline Dist LexNET P 0.283 0.000 0.841 0.754 0.073 0.000 0.469 0.480 R 0.503 0.000 0.672 0.777 0.201 0.000 0.371 0.418 F1 0.362 0.000 0.747 0.765 0.106 0.000 0.411 0.445 Table 2: Performance scores on the test set in each subtask, of the selected methods and the baselines. 4.2 Subtask 2: Semantic Relation Classification The subtask’s train set is highly imbalanced towards random"
W16-5310,C14-1097,0,0.47225,"antic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/site/cogalex2016/home/shared-task/results 80"
W16-5310,W15-4200,0,0.347651,"l., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, and ~vpaths(x,y) is the average embedding vector of all the dependency paths that connect x and y in the corpus. Dependency paths are embedded using a LSTM (Hochreiter and Schmidhuber, 1997), as described in Shwartz et al. (2016). This vector is then fed into a neural network th"
W16-5310,Y16-2021,0,0.325569,"is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et"
W16-5310,L16-1722,0,0.51124,"is applied to their distributional representations: sim(~vwx , ~vwy ). This is a straightforward application of the distributional hypothesis (Harris, 1954), according to which related words occur in similar contexts, hence have similar vector representations. Most commonly, vector cosine is adopted as a similarity measure (Turney et al., 2010). Many other measures exist, including but not limited to Euclidean distance, KL divergence (Cover and Thomas, 2012), Jaccard’s coefficient (Salton and McGill, 1986), and more recently neighbor rank (Hare et al., 2009; Lapesa and Evert, 2013) and APSyn (Santus et al., 2016a).2 To turn this task into a binary classification task, where x and y are classified as either related or not, one can set a threshold to separate similarity scores of related and unrelated word pairs. 2.2 Semantic Relation Classification Recognizing lexical semantic relations between words is valuable for many NLP applications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et"
W16-5310,W16-5304,1,0.558235,"monly corpus-based, and mainly rely on the distributional representation of each word. The CogALex shared task on the corpus-based identification of semantic relations consists of two subtasks. In the first task, the system needs to identify for a word pair whether the words are semantically related or not (e.g. True:(dog, cat), False:(dog, fruit)). In the second task, the goal is to determine the specific semantic relation that holds for a given pair, if any (PART OF:(tail, cat), HYPER:(cat, animal)). In this paper we describe our approach and system setup for the shared task. We use LexNET (Shwartz and Dagan, 2016), an integrated path-based and distributional method for semantic relation classification. LexNET was the system with the overall best performance on subtask 2, and was ranked third on subtask 1, demonstrating the utility of integrating the complementary path-based and distributional information sources in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2,"
W16-5310,P16-1226,1,0.937227,"lications, such as ontology learning, question answering, and recognizing textual entailment. Most corpus-based methods classify the relation between a pair of words x and y based on the distributional representation of each word (Baroni et al., 2012; Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014). Earlier methods utilized the dependency paths that connect the joint occurrences of x and y in the corpus as a cue to the relation between the words (A. Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012). Recently, Shwartz and Dagan (2016) presented LexNET, an extension of HypeNET (Shwartz et al., 2016). This method integrates both path-based and distributional information for semantic relation classification, which outperformed approaches that rely on a single information source, on several common datasets (Baroni and Lenci, 2011; Necsulescu et al., 2015; Santus et al., 2015; Santus et al., 2016b). 3 System Description In LexNET, a word-pair (x, y) is represented as a feature vector, consisting of a concatenation of distributional and path-based features: ~vxy = [~vwx , ~vpaths(x,y) , ~vwy ], where ~vwx and ~vwy are x and y’s word embeddings, providing their distributional representation, a"
W16-5310,C14-1212,0,0.392928,"s in recognizing semantic relatedness.1 To aid in recognizing whether a pair of words are related at all (subtask 1), we combine LexNET with a common similarity measure (cosine similarity), achieving fairly good performance, and a slight improvement upon using cosine similarity alone. Subtask 2, however, has shown to be extremely difficult, with LexNET and all other systems achieving relatively low F1 scores. The conflict between the mediocre performance and the recent success of distributional methods on several other common datasets for semantic relation classification (Baroni et al., 2012; Weeds et al., 2014; Roller et al., 2014) could be explained by the stricter evaluation setup in this subtask, which is supposed to demonstrate more closely real-world application settings. The difficulty of the semantic relation classification task emphasizes the need to develop better methods for this task. This work is licenced under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 LexNET’s code is available at https://github.com/vered1986/LexNET, and the shared task results are available at https://sites.google.com/site/cogalex2016/home/"
W16-5310,W11-2501,0,\N,Missing
W16-5310,W15-4208,0,\N,Missing
W16-5310,W11-2500,0,\N,Missing
W17-0902,J14-2004,0,0.0128507,"etween predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-ali"
W17-0902,araki-etal-2014-detecting,0,0.0144858,"tracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between pre"
W17-0902,D08-1031,1,0.548822,"reras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (O"
W17-0902,P10-1124,1,0.91175,"ic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+"
W17-0902,J12-1003,1,0.927138,"rbal performance (accuracy of 0.73 vs. 0.25). Finally, argument identification was hard mainly because of inconsistencies in verbal versus nominal predicate-argument structure in dependency trees.4 The low performance in predicate coreference compared to entity coreference can be explained by the higher variability of predicate terms. The argument co-reference task becomes easy given gold predicate-argument structures, as most arguments are singletons (i.e. composed of a single element). Finally, while the performance of the predicate entailment component reflects the current stateof-the-art (Berant et al., 2012; Han and Sun, 2016), the performance on entity entailment is much worse than the current state-of-the-art in this task as measured on common lexical inference test sets. We conjecture that this stems from the nature of the entities in our dataset, consisting of both named entities and common nouns, many of which are multi-word expressions, whereas most work in entity entailment is focused on single word common nouns. Furthermore, it is worth noting that our annotations are of naturally occurring texts, and represent lexical entailment in real world coreference chains, as opposed to synthetica"
W17-0902,W05-0620,0,0.162639,"Missing"
W17-0902,W99-0201,0,0.0935993,"on Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference"
W17-0902,W09-4303,0,0.0115663,") is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining en"
W17-0902,W13-2322,0,0.107609,"le texts, and in specifying how such representation can be created based on entity and event coreference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al.,"
W17-0902,P15-1136,0,0.0225471,"Missing"
W17-0902,E12-1004,0,0.0217269,"tractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art"
W17-0902,cybulska-vossen-2014-using,0,0.0126286,"achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can h"
W17-0902,P99-1071,0,0.536493,"on. We can expect the use of OKR structures in MDS to shift the research efforts in this task to other components, e.g. generation, and eventually contribute to improving state of the art on this task. Similarly, an algorithm creating the ECKG structure can benefit from building upon a consolidated structure such as OKR, rather than working directly on free text. systematic solution, and the burden of integrating information across multiple texts is delegated to downstream applications, leading to partial solutions which are geared to specific applications. Multi-Document Summarization (MDS) (Barzilay et al., 1999) is a task whose goal is to produce a concise summary from a set of related text documents, such that it includes the most important information in a non-redundant manner. While extractive summarization selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a sing"
W17-0902,P13-2080,1,0.82203,"ment annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of"
W17-0902,bejan-harabagiu-2008-linguistic,0,0.0326074,"1; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align"
W17-0902,duclaye-etal-2002-using,0,0.0512409,"implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 20"
W17-0902,D12-1045,0,0.0269834,"2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing t"
W17-0902,D11-1142,0,0.0614886,"ference and lexical entailment. An accompanying contribution is our annotated dataset, which can be used to analyze the involved phenomena and their interactions, and as a development and test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments a"
W17-0902,D14-1168,0,0.0154703,"selects salient sentences from the document collection, abstractive summarization generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entitie"
W17-0902,C16-1273,0,0.0306302,"Missing"
W17-0902,liu-etal-2014-supervised,0,0.0231139,"Missing"
W17-0902,D15-1076,0,0.0606389,"on in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use"
W17-0902,N15-1114,0,0.026058,"ng each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on coreference. 7 Conclusions In this paper we advocate the development of representation frameworks for the consolidated information expressed in a set of texts. The key ingredients of our approach are the extraction of pr"
W17-0902,C92-2082,0,0.446376,"wo candidate sentences for the summary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Ent"
W17-0902,W97-1311,0,0.0722333,"rior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankara). Open IE has gained substantial and consistent attention, and many automatic extractors were creEvent Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to"
W17-0902,N15-1050,1,0.790932,"event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored"
W17-0902,P13-1048,0,0.0297432,"zation generates new sentences, and is considered a more promising yet more difficult task. A recent approach for abstractive summarization generates a graphical representation of the input documents by: (1) parsing each sentence/document into a meaning representation structure; and (2) merging the structures into a single structure that represents the entire summary, e.g. by identifying coreferring items. In that sense, this approach is similar to OKR. However, current methods applying this approach are still limited. Gerani et al. (2014) parse each document to discourse tree representation (Joty et al., 2013), aggregating them based on entity coreference. Yet, they work with a limited set of (discourse) relations, and rely on coreference only between entities, which was detected manually. Similarly, Liu et al. (2015) parse each input sentence into an individual AMR graph (Banarescu et al., 2013), and merge those into a single graph through identical concepts. This work extends the AMR formalism of canonicalized representation per entity or event to multiple sentences. However, they only focus on certain types of named entities, and collapse two entities based on their names rather than on corefere"
W17-0902,W16-5304,1,0.885624,"Missing"
W17-0902,K15-1018,1,0.833569,"m for improvement. These bottle-necks are bound to hinder the performance of a pipeline end-to-end system. Future research into OKR should first target these areas; either as a pipeline or in a joint learning framework. 3 For Argument Mention detection we attach the components (entities and propositions) as arguments of predicates when the components are syntactically dependent on them. Argument Coreference is simply predicted by marking coreference if and only if the arguments are both mentions of the same entity co-reference chain. For Entity Entailment purposes we used knowledge resources (Shwartz et al., 2015) and a pretrained model for HypeNET (Shwartz et al., 2016) to obtain a score for all pairs of Wikipedia common words (unigrams, bigrams, and trigrams). A threshold for the binary entailment decision was then calibrated on a held out development set. Finally, for Predicate Entailment we used the entailment rules extracted by Berant et al. (2012). 5.1 Results and Error Analysis Using the same metrics used for measuring interannotator agreement, we evaluated how well the presented models were able to recover the different facets of the OKR gold annotations. The performance on the different subtas"
W17-0902,P15-1146,0,0.0257952,"Missing"
W17-0902,P16-1226,1,0.873666,"Missing"
W17-0902,K15-1002,1,0.834714,"ile the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al.,"
W17-0902,D16-1038,1,0.86973,"Missing"
W17-0902,P11-1080,0,0.0191905,"ddle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolution Tasks Open Information Extraction Open IE (Open Information Extraction) (Etzioni et al., 2008) is the task of extracting coherent propositions from a sentence, each comprising a relation phrase and two or more argument phrases. For example, (plane, landed in, Ankar"
W17-0902,S12-1030,0,0.0281997,"ms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee e"
W17-0902,J01-4004,0,0.174444,"s like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are represented using question-answer pairs, e.g. (what landed somewhere?, plane), (where did something land?, Ankara). 2 Entity Coreference Entity coreference resolution identifies mentions in a text that refer to the same real-world entity (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Clark and Manning, 2015; Peng et al., 2015). In the crossdocument variant, Cross Document Coreference Resolution (CDCR), mentions of the same entity can also appear in multiple documents in a corpus (Singh et al., 2011). 2.2 In our representation, we use coreference resolution to consolidate mentions of the same entity or the same event across multiple texts. Background: Relevant Component Tasks In this section we describe the prior annotation tasks on which we rely in our representation, as described later in Section 3. 2.1 Coreference Resolutio"
W17-0902,P16-1119,1,0.813405,"cly available tools and simple baselines which approximate the current state-of-the-art in each of these subtasks. For brevity sake, in the rest of the section we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agr"
W17-0902,E14-4008,0,0.0143178,"mmary differ only in terms that hold a lexical inference relation (e.g. “the plane landed in Ankara” and “the plane landed in Turkey”). Recognizing the inference direction, e.g. that Ankara is more specific than Turkey, can help in selecting the desired granularity level of the description. There has been consistent attention to recognizing lexical inference between terms. Some methods aim to recognize a general lexical inference relation (e.g. (Kotlerman et al., 2010; Turney and Mohammad, 2015)), others focus on a specific semantic relation, mostly hypernymy (Hearst, 1992; Snow et al., 2005; Santus et al., 2014; Shwartz et al., 2016), while recent methods classify a pair of 3.1 Entities To represent entities, we first annotate the text by entity mentions and coreference. Following the typical notion for these tasks, an entity mention corresponds to a word or multi-word expression that refers to an object or concept in the described scenario (in the broader sense of “entity”). Ac14 Original texts: (1) Turkey forces down Syrian plane. (2) Syrian jet grounded by Turkey carried munitions from Moscow. (3) Intercepted Syrian plane carried ammunition from Russia. Entities: E1 = {Turkey}, E2 = {Syrian}, E3"
W17-0902,P15-2050,1,0.81398,"test set for automated generation of OKR structures. We further note that while this paper focuses on creating an open representation, by consolidating Open IE propositions, future work may investigate the consolidation of other semantic sentence representations, for example AMR (Abstract Meaning Representation) (Banarescu et al., 2013), while exploiting similar principles to those proposed here. ated (e.g., Fader et al. (2011); Del Corro and Gemulla (2013)). Open IE’s extractions were also shown to be effective as intermediate sentencelevel representation in various downstream applications (Stanovsky et al., 2015; Angeli et al., 2015). Analogously, we conjecture a similar utility of our OKR structures at the multi-text level. Open IE does not assign roles to the arguments associated with each predicate, as in other singlesentence representations like SRL (Semantic Role Labeling) (Carreras and M`arquez, 2005; Palmer et al., 2010). While the former is not consistent in assigning argument slots to the same arguments across different propositions, the latter requires predefined thematic role ontologies. A middle ground was introduced by QA-SRL (He et al., 2015), where predicate-argument structures are rep"
W17-0902,D10-1106,0,0.0195202,"ub.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity c"
W17-0902,P12-3013,1,0.834165,"ss document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual information. In discourse representation theory (DRT), a proposition applies to all co-referring entities (Kamp et al., 2011). In recognizing textual entailment (Dagan et al., 2013), lexical substitution of co-referring elements is useful (Stern and Dagan, 2012). For example, in Figure 1, sentence (1) together with the coreference relation between plane and jet entail that “Turkey forces down Syrian jet.” 2.3 Proposed Representation Our Open Knowledge Representation (OKR) aims to capture the consolidated information expressed jointly in a set of texts. In some analogy to structured knowledge bases, we would like the elements of our representation to correspond to entities in the described scenario and to statements (propositions) that relate them. Still, in the spirit of Open IE, we would like the representation to be open, while relying only on the"
W17-0902,W04-3206,1,0.412965,"ailable at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions,"
W17-0902,C16-1183,1,0.835662,"on the task by constructing a predicate entailment graph optimizing a global objective function. However, performance should be further improved in order to be used accurately within semantic applications. al., 2014; Peng et al., 2016), the problem of cross document event coreference has been relatively under-explored (Bagga and Baldwin, 1999; Bejan and Harabagiu, 2014). Standard benchmarks for this task are the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008) and its extensions, that also annotate entity coreference: EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014). See (Upadhyay et al., 2016) for more details on cross document event coreference. Differently from our dataset described in Section 4, ECB and its extensions do not establish predicate-argument annotations. A secondary line of work deals with aligning predicates across document pairs, as done in Roth and Frank (2012). PARMA (Wolfe et al., 2013) treated the task as a token-alignment problem, aligning also arguments, while Wolfe et al. (2015) added joint constraints to align predicates and their arguments. 3 Using Coreference for Consolidation Recognizing that two elements are corefering can help in consolidating textual"
W17-0902,M95-1005,0,0.294299,"ction we briefly describe each of these baselines. For a more detailed technical description see the OKR repository (https://github.com/vered1986/ OKR). For Entity Mention extraction we use the spaCy NER model2 in addition to annotating all of the nouns and adjectives as entities. For Proposition Mention detection we use Open IE propositions extracted from PropS (Stanovsky et al., 2016), where non-restrictive arguments were reduced following Stanovsky and Dagan (2016). For ProposiFor entity, predicate, and argument co-reference we calculated coreference resolution metrics: the link-based MUC (Vilain et al., 1995), the mentionbased B 3 (Bagga and Baldwin, 1998), the entitybased CEAF, and the widely adopted CoNLL F1 measure which is an average of the three. For entity and proposition entailment we compute the F1 score over the annotated directed edges in each entailment graph, as is common for entailment agreement metrics (Berant et al., 2010). We macro-averaged these scores to obtain an overall agreement on the 5 events annotated by both annotators. The agreement scores for the two annotators are shown in Table 1, and overall show high levels of agreement. A qualitative analysis of the more common disa"
W17-0902,C14-1212,0,0.0125443,"t Coreference Event coreference determines whether two event descriptions (mentions) refer to the same event (Humphreys et al., 1997). Cross document event coreference (CDEC) is a variant of the task in which mentions may occur in different documents (Bagga and Baldwin, 1999). Compared to within document event coreference (Chen et al., 2009; Araki et al., 2014; Liu et 1 Our dataset, detailed annotation guidelines, the annotation tool and the baseline implementations are available at https://github.com/vered1986/OKR. 13 terms to a specific semantic relation out of several (Baroni et al., 2012; Weeds et al., 2014; Pavlick et al., 2015; Shwartz and Dagan, 2016). It is worth noting that most existing methods are indifferent to the context in which the target terms occur, with the exception of few works, which were mostly focused on a narrow aspect of lexical inference, e.g. lexical substitution (Melamud et al., 2015). Determining entailment between predicates is a different sub-task, which has also been broadly explored (Lin and Pantel, 2001; Duclaye et al., 2002; Szpektor et al., 2004; Schoenmackers et al., 2010; Roth and Frank, 2012). Berant et al. (2010) achieved state-of-the-art results on the task"
W17-0902,P13-2012,0,0.0402841,"Missing"
W17-0902,N15-1002,0,0.0522403,"Missing"
W17-0902,P02-1014,0,\N,Missing
W17-0902,P15-1034,0,\N,Missing
W17-6927,E17-1104,0,0.0510913,"Missing"
W17-6927,W03-1210,0,0.0911189,"rpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) pre"
W17-6927,P16-1135,0,0.345848,"of their disambiguation. Due to the lack of unambiguous linguistic construction of causality, we claim that the use of linguistic features may restrict the representation of causality meaning. Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju ("
W17-6927,K16-1006,1,0.831429,"the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed arrow in Figure 1). We call this model “Stated Pair LSTM”. We assessed the same model but without the connection of the two LSTMs for evaluating our assumption. We call it “Pair LSTM” (no dashed arrow in Figure 1). The two outputs of the encoding layer are transformed to a vector of length 100 by a dense layer with a tanh activation function. The context of the causal"
W17-6927,C16-1007,0,0.126855,"s. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised system based on the use of lexical, syntactic and semantic features from WordNet. The proposal of Bethard and Martin (2008) is similar to that of Mirza and Tonelli (2016), but it was focused only on conjunction constructions, namely conjoined events. The three approaches suffer from the ambiguity of the lexical markers, the limited coverage of the linguistic resources and the constraint to a specific syntactic construction. In contrast, our proposal tries to cover lexical and propositional causality independently of whether it is explicit or implicit, and we do not rest"
W17-6927,D14-1162,0,0.0802783,"o in... ... puts. The lengths (n, m) of the instances of each input are not Embed. Lookup Embed. Lookup necessarily the same, so in order to make their lengths equal, Token. & w ... w ... w w w w w w Padding: three zero-padding strategies were assessed, namely the maxie e Input: l * mum, the mean and the mode of the lengths (t) of the compo|input |= n |input |= m nents of the inputs (see Equation 1). For each word, its corresponding word vector of 300 com- Figure 1: Neural model, where e1 is ponents (d) was looked up in the 840b cased Glove embeddings the first event, l is the lexical marker (Pennington et al., 2014). Subsequently, the concatenated word and e2 is the second event. embeddings get passed through an encoding Long Short-Term 1 1,1 1,2 1,3 2 1,t 2,1 2,2 2,3 1 1 2,t 2 2 Memory (LSTM) recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) layer. We decided to use LSTM because of its ability to encode sequential and contextual information (Melamud et al., 2016). We assume some sort of relation exists between e1 and e2 , so we first evaluated the performance of the connection of the two LSTM layers through the initialization of the second LSTM with the end state of the first one (dashed"
W17-6927,prasad-etal-2008-penn,0,0.0817483,". Also, the use of dense vector spaces, roughly speaking word embeddings, can provide a better representation of causality, and can improve the disambiguation of the causal meaning of lexical markers. Hence, we propose a neural network architecture with two inputs as sequences of word embeddings, encoding the left and the right context of the lexical marker. We evaluate our proposal on the AltLex corpus (Hidey and McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of proposi"
W17-6927,W13-4004,0,0.177091,"d McKeown, 2016), which is a corpus of causal relations signalled by lexical markers with a wider coverage of those kind of expressions than the Penn Discourse TreeBank Corpus (PDTB) (Prasad et al., 2008). Empirical results on the AltLex corpus show that our claim indeed holds and our system outperforms the state-of-the-art on that corpus. 2 Related Work Previous works in the task of causal language classification are mainly focused on lexical or propositional causality, explicit causality and some of them were restricted to a narrow kind of syntactic constructions. The works of Girju (2003); Riaz and Girju (2013, 2014) were focused on the classification of lexical causality conveyed between verbs and nouns. Recently, Kruengkrai et al. (2017) proposed a system for the classification of propositional causality in Japanese. The system incorporates background knowledge for enhancing the learning process through the use of multi-column convolutional neural networks. Regarding the classification of explicit causality, Khoo et al. (1998) proposed a rule-based system grounded in regular expressions for the classification of explicit causal relations, whereas Mirza and Tonelli (2016) presented a supervised sy"
W17-6927,W14-4322,0,0.0256738,"Missing"
W17-6927,P08-2045,0,\N,Missing
W19-2013,P14-2050,0,0.0451027,"s defined by neighboring context units within a fixed length window of context units, denoted by win, around the focus unit. word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016) are state-of-the-art implementations. 3.2 Explicit Lists Context units consist of terms co-occurring with the focus term in textual lists such as comma separated lists and bullet lists (Roark and Charniak, 1998; Sarmento et al., 2007). 3.3 Syntactic Dependency Context (Dep) This context is defined by the syntactic dependency relations in which the focus unit participates (Levy and Goldberg, 2014; MacAvaney and Zeldes, 2018). The context unit is concatenated with the type and the direction of the dependency relation. 5 This context type has not yet been used for set expansion. However, Levy and Goldberg (2014) showed that it yields more functional similarities of a co-hyponym nature than linear context and thus may be relevant to set expansion. Term Representation Our approach is based on representing any term in a (unlabeled) training corpus by its word embeddings in order to estimate the similarity between seed terms and candidate expansion terms. Different techniques for term extra"
W19-2013,N18-4006,0,0.0185362,"context units within a fixed length window of context units, denoted by win, around the focus unit. word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016) are state-of-the-art implementations. 3.2 Explicit Lists Context units consist of terms co-occurring with the focus term in textual lists such as comma separated lists and bullet lists (Roark and Charniak, 1998; Sarmento et al., 2007). 3.3 Syntactic Dependency Context (Dep) This context is defined by the syntactic dependency relations in which the focus unit participates (Levy and Goldberg, 2014; MacAvaney and Zeldes, 2018). The context unit is concatenated with the type and the direction of the dependency relation. 5 This context type has not yet been used for set expansion. However, Levy and Goldberg (2014) showed that it yields more functional similarities of a co-hyponym nature than linear context and thus may be relevant to set expansion. Term Representation Our approach is based on representing any term in a (unlabeled) training corpus by its word embeddings in order to estimate the similarity between seed terms and candidate expansion terms. Different techniques for term extraction are described in detail"
W19-2013,C18-2013,1,0.836361,"terms, an explicit description of the semantic class is supplied as input to the algorithm and is used to define the ground truth expanded set. Some works like (Pantel et al., 2009) provide an evaluation dataset that does not include any training corpus, which is required for comparing corpus-based approaches. Sarmento et al. (2007) use Wikipedia as training corpus, but exploit meta-information like hyperlinks to identify terms; in our work, we opted for a dataset that matches real-life scenarios where terms have to be automatically identified. Systems based on our approach are described by (Mamou et al., 2018a,b). 3 3.1 Linear Context (Lin) This context is defined by neighboring context units within a fixed length window of context units, denoted by win, around the focus unit. word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016) are state-of-the-art implementations. 3.2 Explicit Lists Context units consist of terms co-occurring with the focus term in textual lists such as comma separated lists and bullet lists (Roark and Charniak, 1998; Sarmento et al., 2007). 3.3 Syntactic Dependency Context (Dep) This context is defined by the syntactic dependency r"
W19-2013,J12-1003,1,0.796483,"oid of the seed. Second, the CombSUM scoring method (csum) is commonly used in Information Retrieval (Shaw et al., 1994). We first produce a candidate term set for each individual seed term: candidate terms become the k 0 terms9 that are the most similar, according to the term embedding cosine similarity, to the seed term. The CombSUM method scores the similarity of a candidate term to the seed terms by averaging over all the seed terms the normalized pairwise cosine similarities10 between the candidate term and the seed term. To combine multi-context embeddings, we follow the general idea of Berant et al. (2012) who train an SVM to combine different similarity score features to learn textual entailment relations. Similarly, we train a Multilayer Perceptron (MLP) binary classifier that predicts whether a candidate term should be part of the expanded set based on 10 similarity scores (considered as input features), using the above 2 different scoring methods for each of the 5 context types. Note that our MLP classifier polynomially combines different semantic similarity estimations and performs better than their linear combination. We also tried to concatenate the multi-context term embeddings in order"
W19-2013,P15-1038,0,0.0429322,"Missing"
W19-2013,D18-2004,1,0.704077,"terms, an explicit description of the semantic class is supplied as input to the algorithm and is used to define the ground truth expanded set. Some works like (Pantel et al., 2009) provide an evaluation dataset that does not include any training corpus, which is required for comparing corpus-based approaches. Sarmento et al. (2007) use Wikipedia as training corpus, but exploit meta-information like hyperlinks to identify terms; in our work, we opted for a dataset that matches real-life scenarios where terms have to be automatically identified. Systems based on our approach are described by (Mamou et al., 2018a,b). 3 3.1 Linear Context (Lin) This context is defined by neighboring context units within a fixed length window of context units, denoted by win, around the focus unit. word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016) are state-of-the-art implementations. 3.2 Explicit Lists Context units consist of terms co-occurring with the focus term in textual lists such as comma separated lists and bullet lists (Roark and Charniak, 1998; Sarmento et al., 2007). 3.3 Syntactic Dependency Context (Dep) This context is defined by the syntactic dependency r"
W19-2013,P06-1038,0,0.0243846,"obtaining more contextual information compared to using individual terms, thus enhancing embedding model robustness. In the remainder of this paper, by language abuse, term will be used instead of term group. While word2vec originally uses a linear window context around the focus word, the literature describes other possible context types. For each focus unit, we extract context units of different types, as follows (see a typical example for each 3.4 Symmetric Patterns (SP) Context units consist of terms co-occurring with the focus term in symmetric patterns (Schwartz et al., 2015). We follow Davidov and Rappoport (2006) for automatic extraction of SPs from the textual corpus.6 For example, the symmetric pattern ‘X rather than Y’ captures certain semantic relatedness between the terms X and Y. This context type generalizes coordinational patterns (‘X and Y’, ‘X or Y’), which have been used for set expansion. 3.5 Unary Patterns (UP) This context is defined by the unary patterns in which the focus term occurs. Context units con4 We preferred showing in the example the strength of each context type with a good example, rather than providing a common example sentence across all the context types. 5 Given a focus"
W19-2013,L18-1405,0,0.0444054,"Missing"
W19-2013,D14-1162,0,0.0885606,"lude any training corpus, which is required for comparing corpus-based approaches. Sarmento et al. (2007) use Wikipedia as training corpus, but exploit meta-information like hyperlinks to identify terms; in our work, we opted for a dataset that matches real-life scenarios where terms have to be automatically identified. Systems based on our approach are described by (Mamou et al., 2018a,b). 3 3.1 Linear Context (Lin) This context is defined by neighboring context units within a fixed length window of context units, denoted by win, around the focus unit. word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Joulin et al., 2016) are state-of-the-art implementations. 3.2 Explicit Lists Context units consist of terms co-occurring with the focus term in textual lists such as comma separated lists and bullet lists (Roark and Charniak, 1998; Sarmento et al., 2007). 3.3 Syntactic Dependency Context (Dep) This context is defined by the syntactic dependency relations in which the focus unit participates (Levy and Goldberg, 2014; MacAvaney and Zeldes, 2018). The context unit is concatenated with the type and the direction of the dependency relation. 5 This context type has not yet been used"
W19-2013,N18-1202,0,0.0588114,"Missing"
W19-2013,P98-2182,0,0.0884532,"s term embed2 Related Work Several works have addressed the term set expansion problem. We focus on corpus-based approaches based on the distributional similarity hypothesis (Harris, 1954). State-of-the-art techniques return the k nearest neighbors around the seed terms as the expanded set, where terms are represented by their co-occurrence or embedding vectors in a training corpus according to different context types, such as linear window context (Pantel et al., 2009; Shi et al., 2010; Rong et al., 2016; Zaheer et al., 2017; Gyllensten and Sahlgren, 2018; Zhao et al., 2018), explicit lists (Roark and Charniak, 1998; Sarmento et al., 2007; He and Xin, 2011), coordinational patterns (Sarmento et al., 2007) and unary patterns (Rong et al., 2016; Shen et al., 2017). In this work, we generalize coordinational patterns, look at additional context types and combine multiple context-type embeddings. We did not find any suitable publicly available 1 http://nlp_architect.nervanasys.com/term_ set_expansion.html 95 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 95–101 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics type in Table 14 ). datas"
W19-2013,K15-1026,0,0.0811858,"ds are left intact); this enables obtaining more contextual information compared to using individual terms, thus enhancing embedding model robustness. In the remainder of this paper, by language abuse, term will be used instead of term group. While word2vec originally uses a linear window context around the focus word, the literature describes other possible context types. For each focus unit, we extract context units of different types, as follows (see a typical example for each 3.4 Symmetric Patterns (SP) Context units consist of terms co-occurring with the focus term in symmetric patterns (Schwartz et al., 2015). We follow Davidov and Rappoport (2006) for automatic extraction of SPs from the textual corpus.6 For example, the symmetric pattern ‘X rather than Y’ captures certain semantic relatedness between the terms X and Y. This context type generalizes coordinational patterns (‘X and Y’, ‘X or Y’), which have been used for set expansion. 3.5 Unary Patterns (UP) This context is defined by the unary patterns in which the focus term occurs. Context units con4 We preferred showing in the example the strength of each context type with a good example, rather than providing a common example sentence across"
W19-2013,C10-1112,0,0.0355458,"mance of computational semantics tasks like term set expansion. To address this question, we propose an approach that combines term embed2 Related Work Several works have addressed the term set expansion problem. We focus on corpus-based approaches based on the distributional similarity hypothesis (Harris, 1954). State-of-the-art techniques return the k nearest neighbors around the seed terms as the expanded set, where terms are represented by their co-occurrence or embedding vectors in a training corpus according to different context types, such as linear window context (Pantel et al., 2009; Shi et al., 2010; Rong et al., 2016; Zaheer et al., 2017; Gyllensten and Sahlgren, 2018; Zhao et al., 2018), explicit lists (Roark and Charniak, 1998; Sarmento et al., 2007; He and Xin, 2011), coordinational patterns (Sarmento et al., 2007) and unary patterns (Rong et al., 2016; Shen et al., 2017). In this work, we generalize coordinational patterns, look at additional context types and combine multiple context-type embeddings. We did not find any suitable publicly available 1 http://nlp_architect.nervanasys.com/term_ set_expansion.html 95 Proceedings of the 3rd Workshop on Evaluating Vector Space Representat"
W19-2013,C98-2177,0,\N,Missing
W19-2013,D09-1098,0,\N,Missing
W19-2303,D18-1443,0,0.261203,"s unwarranted given the non-linear shape of the F1 curve. Instead, we choose to normalize the F1 score of a 4 Evaluation on CNN/DailyMail Test Set We re-test 16 systems on the CNN/DailyMail test set: (1) Pointer-Generator (See et al., 2017) and its variants: a baseline sequence-to-sequence attentional model (baseline), a Pointer-Generator model with soft switch between generating from vocabulary and copying from input (pointer-gen) and the same Pointer-Generator with coverage loss (pointer-cov) for preventing repetitive generation. There are three other content-selection variants proposed in (Gehrmann et al., 2018) which are also based on Pointer-Generator: (i) aligning ref4 We could penalize summaries that are shorter or longer than the reference, similar to the brevity penalty in BLEU (Papineni et al., 2002). Such an approach however assumes that the reference summary length is ideal and deviations from that are clearly undesirable, a fairly strong assumption. 23 System latent cmpr baseline textrank 50 mask lo BU trans bottom up pointer-gen lead-pointer mask hi DiffMask lead-cov pointer-cov multitask textrank 70 latent ext lead3 Rank change Spearman Pearson erence with source article (mask-hi, mask-lo"
W19-2303,W00-0405,0,0.305847,"length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Ass"
W19-2303,D18-2029,0,0.0431248,"Missing"
W19-2303,N18-1065,0,0.0615502,"Missing"
W19-2303,D07-1001,0,0.0296884,"awa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this pr"
W19-2303,hong-etal-2014-repository,1,0.841148,"ength summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this problem since it can penalize longer summaries.”. The rest of the neural summarization literature adopted F1 evaluation without further discussion. In this paper we study how ROUGE F1 scores Until recently, summarization evaluations compared systems that produce summaries of the same target length. Neural approaches to summ"
W19-2303,P08-1080,0,0.0391558,"18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to"
W19-2303,W11-1611,0,0.139017,"Missing"
W19-2303,D16-1140,0,0.0280492,"ion of the Neural Summarization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for t"
W19-2303,P02-1040,0,0.105041,"(1) Pointer-Generator (See et al., 2017) and its variants: a baseline sequence-to-sequence attentional model (baseline), a Pointer-Generator model with soft switch between generating from vocabulary and copying from input (pointer-gen) and the same Pointer-Generator with coverage loss (pointer-cov) for preventing repetitive generation. There are three other content-selection variants proposed in (Gehrmann et al., 2018) which are also based on Pointer-Generator: (i) aligning ref4 We could penalize summaries that are shorter or longer than the reference, similar to the brevity penalty in BLEU (Papineni et al., 2002). Such an approach however assumes that the reference summary length is ideal and deviations from that are clearly undesirable, a fairly strong assumption. 23 System latent cmpr baseline textrank 50 mask lo BU trans bottom up pointer-gen lead-pointer mask hi DiffMask lead-cov pointer-cov multitask textrank 70 latent ext lead3 Rank change Spearman Pearson erence with source article (mask-hi, mask-lo) (ii) training tagger and summarizer at the same time (multitask), and (iii) a differentiable model with a soft mask predicted by selection probabilities (DiffMask). (2) Abstractive system with bott"
W19-2303,C00-1072,0,0.0251998,"ceived quality increase with summary length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapoli"
W19-2303,D18-1444,0,0.037613,"arization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningf"
W19-2303,D15-1044,0,0.668831,"hapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take ou"
W19-2303,D13-1069,0,0.0245956,"lutions and Re-Examination of the Neural Summarization Literature Simeng Sun1 Ori Shapira2 Ido Dagan2 Ani Nenkova1 1 Department of Computer and Information Science, University of Pennsylvania 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel {simsun, nenkova}@seas.upenn.edu obspp18@gmail.com, dagan@cs.biu.ac.il Abstract et al., 2007) or influential work advocating for fixed summary length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke an"
W19-2303,P17-1099,0,0.494577,"ision and F1 scores for lead, random, textrank and Pointer-Generator on the CNN/DailyMail test set. comparing with the version of extracting the first three sentences of the article. Random Randomly and non-repetitively selects full sentences with a total number of tokens that is no more than the desired length. TextRank Sentences are scored by their centrality in the graph with sentences as the nodes (Erkan and Radev, 2004; Mihalcea, 2004). We use the Gensim.summarization package (Barrios et al., 2016) to produce these summaries. Pointer-gen: We use the pre-trained PointerGenerator model of (See et al., 2017) to get outputs with varying lengths by restricting both minimum and maximum decoding steps.3 The largest values for min and max decoding step are set to 130 and 150 respectively due to limited computing resources. Figure 1 shows that ROUGE recall keeps increasing as the summary becomes longer, while precision decreases. For recall, it is clear that even the random system produces better scoring summaries if it is allowed longer length. For all four systems, ROUGE F1 curves first rise steeply, then decline gradually. For summaries longer than 100 words, none of the systems produces a better sc"
W19-2303,E99-1011,0,0.357601,"length around 85-90 words (Goldstein et al., 1999). With the advent of neural methods, however, the practice of fixing required summary length was summarily abandoned. There are some exceptions (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead"
W19-2303,D18-1087,1,0.837137,"papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics Recall Precision ROUGE-1 scores 0.8 0.7 0.40 0.6 0.35 0.300 0.25 50 100 150 200 250 lead random textrank pointer-gen 0.375 0.325 0.4 lead random textrank pointer-gen 0.400 0.350 0.30 0.5 0.3 ROUGE-2 scores F1 lead random textrank pointer-gen 0.45 0.275 0.20 0.250 0.15 300 50 0.40 0.20 0.35 0.18 100 150 200 250 300 lead random textrank pointer-gen 50 100 150 200 250 0.18 300 lead"
W19-2303,E06-1038,0,0.0251894,"ns (Ma and Nakagawa, 2013; Kikuchi et al., 2016; Liu et al., 2018), but starting with (Rush et al., 2015), systems produce summaries of variable length. This trend is not necessarily bad. Prior work has shown that people prefer summaries of different length depending on the information they search for (Kaisser et al., 2008) and that variable length summaries were more effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Ful"
W19-2303,P04-3020,0,0.484802,"ase with summary length, calling for the need of similar normalization in reporting human scores. 1 Introduction Algorithms for text summarization of news developed between 2000 and 2015, were evaluated with a requirement to produce a summary of a pre-specified length.1 This practice likely followed the DUC shared task, which called for summaries of length fixed in words or bytes (Over 1 Here is a list of the most cited ‘summarization’ papers of that period according to Google Scholar (Erkan and Radev, 2004; Radev et al., 2004; Gong and Liu, 2001; Conroy and O’leary, 2001; Lin and Hovy, 2000; Mihalcea, 2004; Goldstein et al., 2000). All of them present evaluations in which alternative systems produce summaries of the same length, with two of the papers fixing the number of sentences rather than number of words. 2 As a matter of fact, the established practice was to require human references of different lengths in order to evaluate system outputs of the respective length, a practice that has recently been shown unnecessary (Shapira et al., 2018). 21 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 21–29 c Minneapolis, Minnesota, US"
W19-2303,D18-1088,0,0.0610359,"Missing"
W19-2303,K16-1028,0,0.300069,"e effective in task-based evaluations (Mani et al., 1999). There are, at the same time, reasons for concern. The confounding effect of output length has been widely acknowledged for example in earlier work on sentence compression (McDonald, 2006; Clarke and Lapata, 2007); for this task a meaningful evaluation should explicitly take output length into account (Napoles et al., 2011). For summarization in general, prior to 2015, researchers reported ROUGE recall as standard evaluation. Best practices for using ROUGE call for truncating the summaries to the desired length (Hong et al., 2014) 2 . (Nallapati et al., 2016) suggested using ROUGE F1 instead of recall, with the following justification “full-length recall favors longer summaries, so it may not be fair to use this metric to compare two systems that differ in summary lengths. Full-length F1 solves this problem since it can penalize longer summaries.”. The rest of the neural summarization literature adopted F1 evaluation without further discussion. In this paper we study how ROUGE F1 scores Until recently, summarization evaluations compared systems that produce summaries of the same target length. Neural approaches to summarization however have done a"
W19-8635,W18-3606,0,0.230378,"inlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011"
W19-8635,P17-1183,0,0.167773,"-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing the question of how to design and train more data-efficient models. Our work focuses on this latter point and attempts to address it via data analysis and algorithm design. Taking this into consideration, we build upon the work done by Puzikov and Gurevych (2018), and attempt to improve their meth"
W19-8635,W17-3518,1,0.855413,"of encoderdecoder (Cho et al., 2014) and sequence-tosequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShal"
W19-8635,W11-2832,0,0.0990045,"ering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of generating natural language utterances from various data representations. In this work we consider lemmatized dependency trees as input and focus on the process of transforming a dependency tree into a linearly-ordered grammatical string of morphologically inflected words – the setup which is most commonly known as surface realization (SR) (Langkilde-Geary, 2002; Belz et al., 2011). Most surface realization approaches fall into two main groups: feature-based incremental generation pipelines and end-to-end neural approaches. To predict a correct token sequence, In this work we extend B IN L IN along two orthogonal directions. First, we propose a way to enrich the training data, which largely compensates for the small size of the datasets used in the task. Second, we propose a new input encoding strategy which incorporates both local and global prediction contexts. These modifications bridge the performance gap between B IN L IN and endto-end black-box approaches, while r"
W19-8635,W18-3605,0,0.359997,"Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011 and 2018) which aimed at developing a common representation that could be used by a variety of NLG systems as input (Belz et al., 2011). They used almost identical task definitions, but different datasets. We focus on the latest task (SR’18 (Mille et al., 2018)), because the former was confined to using English data only, while the latter included Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish Universal Dependencies (UD, versi"
W19-8635,C10-1012,0,0.0214058,"on labels. We focus on the Shallow Track, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal dire"
W19-8635,W17-3204,0,0.0302366,"f the workshop. The authors identified the lack of sufficient training data as the major obstacle to training highperforming neural models and mentioned that the system trained only on the original dataset failed to deliver sensible outputs. These results are supported by the work done in other NLP fields. For example, in the machine translation community researchers have found that neural models have a much slower learning curve with respect to the amount of training data, which usually manifests itself as worse quality in low-resource settings, but better performance in high-resource cases (Koehn and Knowles, 2017). In morphological inflection, when trained on small datasets, seq2seq models with additional external (noisy) alignments per4 Approach Description Before explaining our work, we briefly recap how B IN L IN works. It is a pipeline system which generates a sentence from a dependency tree in two stages: 1. Syntactic ordering: convert dependency tree into a binary tree, then traverse the latter to obtain a sequence of lemmas. 2. Morphological inflection: conjugate each lemma into a surface form. Figure 1 shows a schematic view of the first stage. It relies on the procedure which first runs a brea"
W19-8635,W17-3501,0,0.0131435,"., 2014) and sequence-tosequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependenc"
W19-8635,P17-1014,0,0.0227524,"osequence (seq2seq) (Sutskever et al., 2014) neural architectures, this line of work has gained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of l"
W19-8635,W18-3604,0,0.0995046,"/ inlg2019-revisiting-binlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realizati"
W19-8635,W02-2103,0,0.075576,"is of the syntactic ordering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of generating natural language utterances from various data representations. In this work we consider lemmatized dependency trees as input and focus on the process of transforming a dependency tree into a linearly-ordered grammatical string of morphologically inflected words – the setup which is most commonly known as surface realization (SR) (Langkilde-Geary, 2002; Belz et al., 2011). Most surface realization approaches fall into two main groups: feature-based incremental generation pipelines and end-to-end neural approaches. To predict a correct token sequence, In this work we extend B IN L IN along two orthogonal directions. First, we propose a way to enrich the training data, which largely compensates for the small size of the datasets used in the task. Second, we propose a new input encoding strategy which incorporates both local and global prediction contexts. These modifications bridge the performance gap between B IN L IN and endto-end black-box"
W19-8635,D14-1179,0,0.0113457,"Missing"
W19-8635,N15-1012,0,0.105683,"n the Shallow Track, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing"
W19-8635,D12-1023,0,0.0173889,"tences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 2018), which facilitates building sentences incrementally. The NLP community organized two Surface Realization Shared Tasks (in 2011 and 2018) which aimed at developing a common representation that could be used by a variety of NLG systems as input (Belz et al., 2011). They used almost identical task definitions, but different datasets. We focus on the latest task (SR’18 (Mille et al., 2018)), because the former was confined to using English data only, while the latter included Arabic, Czech, Dutch, English, Finnish, French, Italian, Portuguese, Russian and Spanish Universal"
W19-8635,W18-6501,0,0.0361472,"Missing"
W19-8635,W18-3601,0,0.0156982,"-end black-box approaches, while retaining its interpretability advantages. 1 https://github.com/UKPLab/ inlg2019-revisiting-binlin 268 Proceedings of The 12th International Conference on Natural Language Generation, pages 268–278, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics Data split Train Dev Test ar cs en es 6,016 897 676 66,485 9,016 9,876 12,375 1,978 2,061 14,289 1,651 1,719 Language fi fr 12,030 1,336 1,525 14,529 1,473 416 it nl pt ru 12,796 562 480 12,318 720 685 8,325 559 476 48,119 6,441 6,366 Table 1: Number of sentences in SR’18 datasets (Mille et al., 2018). 2 Task Description techniques performing more global input-output mapping (Castro Ferreira et al., 2018; Elder and Hokamp, 2018). The former approaches traverse the input tree, encode nodes using sparse manually defined feature sets as input representations and generate a sentence by extending a candidate hypothesis with an input word that has the highest score among other input words that have not yet been processed. These approaches rely on the observation that natural language production has a preference for shorter dependencies (Gibson, 2000; White and Rajkumar, 2012; King and White, 201"
W19-8635,P02-1040,0,0.104067,"85.6 92.85 85.56 Table 3: The distribution of left/right labels in the training data and the accuracy of predicting a node’s relative position with the binary classifier. Two cases are considered: predicting the position of a dependent w.r.t. its head (head-dep), and a sibling (dep-dep). BLEU EDIST NIST B IN L IN + data enrichment + new encoder + new features 24.92 48.47 50.67 51.15 35.91 62.04 64.05 64.78 9.55 10.72 10.82 10.82 Upper bound 65.31 85.52 11.38 dency locality hypothesis. We trained the syntactic ordering component and performed its automatic metric evaluation by computing BLEU (Papineni et al., 2002)3 , NIST (Doddington, 2002) and normalized string edit distance (EDIST) scores between the references and system outputs. Note that system outputs contain ordered lemmas, not surface forms, while the references are correctly ordered sequences of inflected surface forms given in the CONLL file. Table 4 shows the contribution of each of the modifications that we propose in this work; the results are computed on the English SR’18 development set. We also show the maximum metric scores that an ideal syntactic ordering component would get, i.e. an upper bound on its performance. We computed it by r"
W19-8635,N16-1058,0,0.0166486,"k, because it covers more languages than the Deep Track (only three), and is therefore more interesting to study the problem of word ordering and morphological inflection as two steps of the surface realization process. The task can be considered as operating under low-resource scenario: Table 1 shows that the treebanks are rather small, which poses a challenge for training complex neural models. 3 Related Work The two best-performing approaches in the task of generating sentences from dependency trees have been feature-based incremental text generation (Bohnet et al., 2010; Liu et al., 2015; Puduppully et al., 2016; King and White, 2018) and 2 http://universaldependencies.org/ 269 Property Data efficiency Rich context representation Interpretability Language coverage form much better than similar systems which learn the alignment information from scratch (Aharoni and Goldberg, 2017). The success of the encoder-decoder paradigm has given birth to a prominent research trend of finding various ways of utilizing the abundant data on the web. While looking for ways to acquire more data for training even larger models is a promising research topic, an orthogonal direction is pursuing the question of how to de"
W19-8635,W18-3602,1,0.824118,"ediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others. 1 1 This work builds upon B IN L IN, a binary linearization technique proposed by Puzikov and Gurevych (2018). It is a hybrid approach which uses a feature-based neural word ordering module and a sequence-to-sequence morphological inflection component. In terms of prediction accuracy, B IN L IN falls short compared to end-to-end neural approaches, but has an advantage of being more intuitive and interpretable. It also supports separate analysis of the syntactic ordering and morphological inflection steps of the surface linearization process. From a research perspective, this offers greater control over the problem-solving procedure. Introduction Natural Language Generation (NLG) is the task of genera"
W19-8635,P18-1150,0,0.0204681,"ained a lot of popularity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information, as found"
W19-8635,P18-1151,0,0.0157503,"larity due to the method’s simplicity: the input string is encoded into a dense vector and a sentence is being generated token-by-token from the encoded input representation. From an NLP perspective, one of the main research problems in this paradigm has become the choice of the graph encoding strategy. The most popular method is linearizing it into a sequence of tokens and encoding using a variant of a recurrent neural network (RNN) (Gardent et al., 2017; Castro Ferreira et al., 2017; Konstas et al., 2017). Another prominent approach is using graph-to-text neural networks (Song et al., 2018; Trisedya et al., 2018). These methods have shown good results across various tasks, but in the context of surface realization they produced somewhat mixed results: the former ones were successfully used only when being trained on large amounts of data (Elder and Hokamp, 2018), while the latter ones have been only evaluated on the SR’11 Deep Track data and, while performing better than RNN-type encoders, fell short behind feature-based methods (MarchegShallow Track: unordered dependency trees consisting of lemmatized nodes with part-ofspeech (POS) tags and morphological information, as found in the UD annotations. D"
W19-8645,P02-1040,0,0.105673,"Missing"
W19-8645,W18-6555,0,0.0830827,"Missing"
W19-8645,W16-6626,0,0.0641224,"while our planner takes 0.0025 seconds, 5 orders of magnitude faster. the text plans are guaranteed to faithfully encode all and only the facts from the input. The realization stage then translates the plans into natural language sentences, using a neural sequenceto-sequence system, resulting in fluent output. 3 Fast and Verifiable Planner The data-to-plan component in Moryossef et al. (2019) exhaustively generates all possible plans, scores them using a heuristic, and chooses the highest scoring one for realization. While this is feasible with the small input graphs in the WebNLG challenge (Colin et al., 2016), it is also very computationally intensive, growing exponentially with the input size. We propose an alternative planner which works in linear time in the size of the graph and remains verifiable: generated plans are guaranteed to represent the input faithfully. The original planner works by first enumerating over all possible splits into sentences (subgraphs), and for each sub-graph enumerating over all possible undirected, unordered, Depth First Search (DFS) traversals, where each traversal corresponds to a sentence plan. Our planner combines these into a single process. It works by perform"
W19-8645,P18-1182,0,0.0940324,"Missing"
W19-8645,D16-1032,0,0.0361739,"Missing"
W19-8645,P17-4012,0,0.0389527,"traversal from that node. Then, all edges visited in the traversal are removed from the input graph, and the process repeats (performing another truncated DFS) until no more edges remain. Each truncated DFS traversal corresponds to a sentence plan, following the DFS-to-plan procedure of Moryossef et al. (2019): the linearized plan is generated incrementally at each step of the 4 Incorporating typing information for unseen entities and relations In Moryossef et al. (2019), the sentence plan trees were linearized into strings that were then fed to a neural machine translation decoder (OpenNMT) (Klein et al., 2017) with a copy mecha378 construct the input. Both of these approaches are “soft” in the sense that they crucially rely on the internal dynamics or on the output of a neural network module that may or may not be correct. nism. This linearization process is lossy, in the sense that the linearized strings do not explicitly distinguish between symbols that represent entities (e.g., BARACK OBAMA) and symbols that represent relations (e.g., works-for). While this information can be deduced from the position of the symbol within the structure, there is a benefit in making it more explicit. In particula"
W19-8645,N19-1386,0,0.0165292,"work in neural text generation and summarization attempt to address these issues by trying to map the textual outputs back to structured predicates, and comparing these predicates to the input data. Kiddon et al. (2016) uses a neural checklist model to avoid the repetition of facts and improve coverage. Agarwal et al. (2018) generate k-best output candidates with beam search, and then try to map each candidate output back to the input structure using a reverse seq2seq model trained on the same data. They then select the highest scoring output candidate that best translates back to the input. Mohiuddin and Joty (2019) reconstructs the input in training time, by jointly learning a back-translation model and enforcing the back-translation to reReferring Expressions The step-by-step system generates entities by first generating an indexed entity symbols, and then lexicalizing each symbol to the string associated with this entity in the input structure (i.e., all occurrences of the entity 11TH MISSISSIPPI INFANTRY MONUMENT will be lexicalized with the full name rather than “it” or “the monument”). This results in correct but somewhat unnatural structures. In contrast, end-to-end neural generation systems are t"
W19-8645,N19-1236,1,0.715961,"Missing"
W19-8645,P08-1108,0,0.0443616,"facts to express. 377 Proceedings of The 12th International Conference on Natural Language Generation, pages 377–382, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics traversal. This process is linear in the number of edges in the graph. At training time, we use the plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller, choosing which action to perform at each step. At test time, we use the controller to guide the truncated DFS process. This mechanism is inspired by transition based parsing (Nivre and McDonald, 2008). The action set at each stage is dynamic. During traversal, it includes the available children at each stage and POP. Before traversals, it includes a choose-i action for each available node ni . We assign a score to each action, normalize with softmax, and train to choose the desired one using cross-entropy loss. At test time, we either greedily choose the best action, or we can sample plans by sampling actions according to their assigned probabilities. Feature Representation and action scoring. Each graph node ni corresponds to an entity xni , and has an associated embedding vector xni . Ea"
W93-0301,P91-1023,1,0.893183,"d by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of parti"
W93-0301,1992.tmi-1.9,1,0.883073,"inology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed t"
W93-0301,P93-1003,0,0.157087,"square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express querie"
W93-0301,P93-1004,0,0.0961023,"by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language th"
W93-0301,1992.tmi-1.7,0,0.804952,"W i l l i a m A. Gale AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 Abstract We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues. Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and"
W93-0301,P91-1022,0,0.751742,"dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app"
W93-0301,P91-1034,0,0.121122,"dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app"
W93-0301,P93-1001,1,0.845328,"iance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to"
W99-0907,P93-1024,0,0.0605397,"partitions of the term sets according to the diagnosticity principle (Tversky, 1977). How each set is divided depends on how terms of both sets resemble each other: in the first case, the grouped topics are &quot;workers&quot; and &quot;management&quot;; in the second case - &quot;individuals&quot; and &quot;institutions&quot;. 46 For obtaining subset coupling, we apply clustering methods. Quite a few previous works investigated the idea of identifying semantic substances with term clusters. Term clustering methods are typically based on the statistics of term co-occurrence within a word window, or within syntactic constructs (e.g. Pereira et al., 1993). The notion pairwise clustering refers to clustering established, as in the present study, on previous assessment of term similarity values a process often based by itself on term cooccurrence data (e.g. Lin, 1999). A standard pairwise clustering problem can be represented by a weighted graph, where each node stands for a data point and each edge is weighted according to the degree of similarity of the nodes it connects. A (hard) clustering procedure produces partition of the graph nodes to disjoint connected components forming a cluster configuration. Our setting is special in that it consid"
