2021.nodalida-main.22,Creating and Evaluating a Synthetic {N}orwegian Clinical Corpus for De-Identification,2021,-1,-1,3,0,2661,synnove braathen,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Building tools to remove sensitive information such as personal names, addresses, and telephone numbers - so called Protected Health Information (PHI) - from clinical free text is an important task to make clinical texts available for research. These de-identification tools must be assessed regarding their quality in the form of the measurements precision and re- call. To assess such tools, gold standards - annotated clinical text - must be available. Such gold standards exist for larger languages. For Norwegian, how- ever, there are no such resources. Therefore, an already existing Norwegian synthetic clinical corpus, NorSynthClinical, has been extended with PHIs and annotated by two annotators, obtaining an inter-annotator agreement of 0.94 F1-measure. In total, the corpus has 409 annotated PHI instances and is called NorSynthClinical PHI. A de-identification hybrid tool (machine learning and rule-based meth- ods) for Norwegian was developed and trained with open available resources, and obtained an overall F1-measure of 0.73 and a recall of 0.62, when tested using NorSynthClinical PHI. NorSynthClinical PHI is made open and available at Github to be used by the research community."
2021.nodalida-main.23,Applying and Sharing pre-trained {BERT}-models for Named Entity Recognition and Classification in {S}wedish Electronic Patient Records,2021,-1,-1,2,0,2664,mila grancharova,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"To be able to share the valuable information in electronic patient records (EPR) they first need to be de-identified in order to protect the privacy of their subjects. Named entity recognition and classification (NERC) is an important part of this process. In recent years, general-purpose language models pre-trained on large amounts of data, in particular BERT, have achieved state of the art results in NERC, among other NLP tasks. So far, however, no attempts have been made at applying BERT for NERC on Swedish EPR data. This study attempts to fine-tune one Swedish BERT-model and one multilingual BERT-model for NERC on a Swedish EPR corpus. The aim is to assess the applicability of BERT-models for this task as well as to compare the two models in a domain-specific Swedish language task. With the Swedish model, recall of 0.9220 and precision of 0.9226 is achieved. This is an improvement to previous results on the same corpus since the high recall does not sacrifice precision. As the models also perform relatively well when fine-tuned with pseudonymised data, it is concluded that there is good potential in using this method in a shareable de-identification system for Swedish clinical text."
2021.nodalida-main.54,{HB} {D}eid - {HB} De-identification tool demonstrator,2021,-1,-1,2,1,2750,hanna berg,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"This paper describes a freely available web-based demonstrator called HB Deid. HB Deid identifies so-called protected health information, PHI, in a text written in Swedish and removes, masks, or replaces them with surrogates or pseudonyms. PHIs are named entities such as personal names, locations, ages, phone numbers, dates. HB Deid uses a CRF model trained on non-sensitive annotated text in Swedish, as well as a rule-based post-processing step for finding PHI. The final step in obscuring the PHI is then to either mask it, show only the class name or use a rule-based pseudonymisation system to replace it."
2020.multilingualbio-1.1,Detecting Adverse Drug Events from {S}wedish Electronic Health Records using Text Mining,2020,-1,-1,2,0,16515,maria bampa,Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020),0,"Electronic Health Records are a valuable source of patient information which can be leveraged to detect Adverse Drug Events (ADEs) and aid post-mark drug-surveillance. The overall aim of this study is to scrutinize text written by clinicians in the EHRs and build a model for ADE detection that produces medically relevant predictions. Natural Language Processing techniques will be exploited to create important predictors and incorporate them into the learning process. The study focuses on the 5 most frequent ADE cases found ina Swedish electronic patient record corpus. The results indicate that considering textual features, rather than the structured, can improve the classification performance by 15{\%} in some ADE cases. Additionally, variable patient history lengths are incorporated in the models, demonstrating the importance of the above decision rather than using an arbitrary number for a history length. The experimental findings suggest that the clinical text in EHRs includes information that can capture data beyond the ones that are found in a structured format."
2020.lrec-1.547,A Semi-supervised Approach for De-identification of {S}wedish Clinical Text,2020,-1,-1,2,1,2750,hanna berg,Proceedings of the 12th Language Resources and Evaluation Conference,0,"An abundance of electronic health records (EHR) is produced every day within healthcare. The records possess valuable information for research and future improvement of healthcare. Multiple efforts have been done to protect the integrity of patients while making electronic health records usable for research by removing personally identifiable information in patient records. Supervised machine learning approaches for de-identification of EHRs need annotated data for training, annotations that are costly in time and human resources. The annotation costs for clinical text is even more costly as the process must be carried out in a protected environment with a limited number of annotators who must have signed confidentiality agreements. In this paper is therefore, a semi-supervised method proposed, for automatically creating high-quality training data. The study shows that the method can be used to improve recall from 84.75{\%} to 89.20{\%} without sacrificing precision to the same extent, dropping from 95.73{\%} to 94.20{\%}. The model{'}s recall is arguably more important for de-identification than precision."
2020.louhi-1.1,The Impact of De-identification on Downstream Named Entity Recognition in Clinical Text,2020,-1,-1,3,1,2750,hanna berg,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,0,"The impact of de-identification on data quality and, in particular, utility for developing models for downstream tasks has been more thoroughly studied for structured data than for unstructured text. While previous studies indicate that text de-identification has a limited impact on models for downstream tasks, it remains unclear what the impact is with various levels and forms of de-identification, in particular concerning the trade-off between precision and recall. In this paper, the impact of de-identification is studied on downstream named entity recognition in Swedish clinical text. The results indicate that de-identification models with moderate to high precision lead to similar downstream performance, while low precision has a substantial negative impact. Furthermore, different strategies for concealing sensitive information affect performance to different degrees, ranging from pseudonymisation having a low impact to the removal of entire sentences with sensitive information having a high impact. This study indicates that it is possible to increase the recall of models for identifying sensitive information without negatively affecting the use of de-identified text data for training models for clinical named entity recognition; however, there is ultimately a trade-off between the level of de-identification and the subsequent utility of the data."
W19-6502,Augmenting a De-identification System for {S}wedish Clinical Text Using Open Resources and Deep Learning,2019,-1,-1,2,1,2750,hanna berg,Proceedings of the Workshop on NLP and Pseudonymisation,0,None
W19-6503,Pseudonymisation of {S}wedish Electronic Patient Records Using a Rule-Based Approach,2019,-1,-1,1,1,2663,hercules dalianis,Proceedings of the Workshop on NLP and Pseudonymisation,0,None
D19-6215,Building a De-identification System for Real {S}wedish Clinical Text Using Pseudonymised Clinical Text,2019,0,0,3,1,2750,hanna berg,Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),0,"This article presents experiments with pseudonymised Swedish clinical text used as training data to de-identify real clinical text with the future aim to transfer non-sensitive training data to other hospitals. Conditional Random Fields (CFR) and Long Short-Term Memory (LSTM) machine learning algorithms were used to train de-identification models. The two models were trained on pseudonymised data and evaluated on real data. For benchmarking, models were also trained on real data, and evaluated on real data as well as trained on pseudonymised data and evaluated on pseudonymised data. CRF showed better performance for some PHI information like Date Part, First Name and Last Name; consistent with some reports in the literature. In contrast, poor performances on Location and Health Care Unit information were noted, partially due to the constrained vocabulary in the pseudonymised training data. It is concluded that it is possible to train transferable models based on pseudonymised Swedish clinical data, but even small narrative and distributional variation could negatively impact performance."
weegar-etal-2017-efficient,Efficient Encoding of Pathology Reports Using Natural Language Processing,2017,4,3,3,0.833333,32479,rebecka weegar,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"In this article we present a system that extracts information from pathology reports. The reports are written in Norwegian and contain free text describing prostate biopsies. Currently, these reports are manually coded for research and statistical purposes by trained experts at the Cancer Registry of Norway where the coders extract values for a set of predefined fields that are specific for prostate cancer. The presented system is rule based and achieves an average F-score of 0.91 for the fields Gleason grade, Gleason score, the number of biopsies that contain tumor tissue, and the orientation of the biopsies. The system also identifies reports that contain ambiguity or other content that should be reviewed by an expert. The system shows potential to encode the reports considerably faster, with less resources, and similar high quality to the manual encoding."
W16-2926,Applying deep learning on electronic health records in {S}wedish to predict healthcare-associated infections,2016,16,13,2,0,33835,olof jacobson,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,0,Detecting healthcare-associated infections pose a major challenge in healthcare. Using natural language processing and machine learning applied on electronic patient records is one approach that has been shown to work. However the results indicate that there was room for improvement and therefore we have applied deep learning methods. Specifically we implemented a network of stacked sparse auto encoders and a network of stacked restricted Boltzmann machines. Our best results were obtained using the stacked restricted Boltzmann machines with a precision of 0.79 and a recall of 0.88.
W15-2609,Creating a rule based system for text mining of {N}orwegian breast cancer pathology reports,2015,9,10,2,0.833333,32479,rebecka weegar,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"National cancer registries collect cancer related information from multiple sources and make it available for research. Part of this information originates from pathology reports, and in this pre-study the possibility of a system for automatic extraction of information from Norwegian pathology reports is investigated. A set of 40 pathology reports describing breast cancer tissue samples has been used to develop a rule based system for information extraction. To validate the performance of this system its output has been compared to the data produced by experts doing manual encoding of the same pathology reports. On average, a precision of 80%, a recall of 98% and an F-score of 86% has been achieved, showing that such a system is indeed feasible."
W15-2617,Adverse Drug Event classification of health records using dictionary based pre-processing and machine learning,2015,17,2,2,0,36924,stefanie friedrich,Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis,0,"A method to find adverse drug reactions in electronic health records written in Swedish is presented. A total of 14,751 health records were manually classified into four groups. The records are normalised by pre-processing using both dic- tionaries and manually created word lists. Three different supervised machine learning algorithm were used to find the best results; decision tree, random forest and LibSVM. The best performance on a test dataset was with LibSVM obtaining a pre- cision of 0.69 and a recall of 0.66, and a F-score of 0.67. Our method found 865 of 981 true positives (88.2%) in a 3-class dataset which is an improvement of 49.5% over previous approaches."
W13-5635,"Negation Scope Delimitation in Clinical Text Using Three Approaches: {N}eg{E}x, {P}y{C}on{T}ext{NLP} and {S}yn{N}eg",2013,20,6,2,0,40521,hideyuki tanushi,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"Negation detection is a key component in clinical information extraction systems, as health record text contains reasonings in which the physician excludes different diagnoses by negating them. Many systems for negation detection rely on negation cues (e.g. not), but only few studies have investigated if the syntactic structure of the sentences can be used for determining the scope of these cues. We have in this paper compared three different systems for negation detection in Swedish clinical text (NegEx, PyConTextNLP and SynNeg), which have different approaches for determining the scope of negation cues. NegEx uses the distance between the cue and the disease, PyConTextNLP relies on a list of conjunctions limiting the scope of a cue, and in SynNeg the boundaries of the sentence units, provided by a syntactic parser, limit the scope of the cues. The three systems produced similar results, detecting negation with an F-score of around 80%, but using a parser had advantages when handling longer, complex sentences or short sentences with contradictory statements."
skeppstedt-etal-2012-rule,Rule-based Entity Recognition and Coverage of {SNOMED} {CT} in {S}wedish Clinical Text,2012,23,26,3,0,22194,maria skeppstedt,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Named entity recognition of the clinical entities disorders, findings and body structures is needed for information extraction from unstructured text in health records. Clinical notes from a Swedish emergency unit were annotated and used for evaluating a rule- and terminology-based entity recognition system. This system used different preprocessing techniques for matching terms to SNOMED CT, and, one by one, four other terminologies were added. For the class body structure, the results improved with preprocessing, whereas only small improvements were shown for the classes disorder and finding. The best average results were achieved when all terminologies were used together. The entity body structure was recognised with a precision of 0.74 and a recall of 0.80, whereas lower results were achieved for disorder (precision: 0.75, recall: 0.55) and for finding (precision: 0.57, recall: 0.30). The proportion of entities containing abbreviations were higher for false negatives than for correctly recognised entities, and no entities containing more than two tokens were recognised by the system. Low recall for disorders and findings shows both that additional methods are needed for entity recognition and that there are many expressions in clinical text that are not included in SNOMED CT."
W10-3102,Creating and evaluating a consensus for negated and speculative words in a {S}wedish clinical corpus,2010,15,10,1,1,2663,hercules dalianis,Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,0,"In this paper we describe the creation of a consensus corpus that was obtained through combining three individual annotations of the same clinical corpus in Swedish. We used a few basic rules that were executed automatically to create the consensus. The corpus contains negation words, speculative words, uncertain expressions and certain expressions. We evaluated the consensus using it for negation and speculation cue detection. We used Stanford NER, which is based on the machine learning algorithm Conditional Random Fields for the training and detection. For comparison we also used the clinical part of the BioScope Corpus and trained it with Stanford NER. For our clinical consensus corpus in Swedish we obtained a precision of 87.9 percent and a recall of 91.7 percent for negation cues, and for English with the Bioscope Corpus we obtained a precision of 97.6 percent and a recall of 96.7 percent for negation cues."
W10-3012,Uncertainty Detection as Approximate Max-Margin Sequence Labelling,2010,20,5,5,0,31725,oscar tackstrom,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"This paper reports experiments for the CoNLL-2010 shared task on learning to detect hedges and their scope in natural language text. We have addressed the experimental tasks as supervised linear maximum margin prediction problems. For sentence level hedge detection in the biological domain we use an L1-regularised binary support vector machine, while for sentence level weasel detection in the Wikipedia domain, we use an L2-regularised approach. We model the insentence uncertainty cue and scope detection task as an L2-regularised approximate maximum margin sequence labelling problem, using the bioencoding. In addition to surface level features, we use a variety of linguistic features based on a functional dependency analysis. A greedy forward selection strategy is used in exploring the large set of potential features. Our official results for Task 1 for the biological domain are 85.2 F1-score, for the Wikipedia set 55.4 F1-score. For Task 2, our official results are 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and final bugs, our final results are for Task 1, biological: 86.0, Wikipedia: 58.2; Task 2, scopes: 39.6 and cues: 78.5."
W10-1108,Characteristics and Analysis of {F}innish and {S}wedish Clinical Intensive Care Nursing Narratives,2010,19,8,3,0,45468,helen allvin,Proceedings of the {NAACL} {HLT} 2010 Second Louhi Workshop on Text and Data Mining of Health Documents,0,"We present a comparative study of Finnish and Swedish free-text nursing narratives from intensive care. Although the two languages are linguistically very dissimilar, our hypothesis is that there are similarities that are important and interesting from a language technology point of view. This may have implications when building tools to support producing and using health care documentation. We perform a comparative qualitative analysis based on structure and content, as well as a comparative quantitative analysis on Finnish and Swedish Intensive Care Unit (ICU) nursing narratives. Our findings are that ICU nursing narratives in Finland and Sweden have many properties in common, but that many of these are challenging when it comes to developing language technology tools."
dalianis-etal-2010-creating,Creating a Reusable {E}nglish-{C}hinese Parallel Corpus for Bilingual Dictionary Construction,2010,16,1,1,1,2663,hercules dalianis,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper first describes an experiment to construct an English-Chinese parallel corpus, then applying the Uplug word alignment tool on the corpus and finally produce and evaluate an English-Chinese word list. The Stockholm English-Chinese Parallel Corpus (SEC) was created by downloading English-Chinese parallel corpora from a Chinese web site containing law texts that have been manually translated from Chinese to English. The parallel corpus contains 104 563 Chinese characters equivalent to 59 918 Chinese words, and the corresponding English corpus contains 75 766 English words. However Chinese writing does not utilize any delimiters to mark word boundaries so we had to carry out word segmentation as a preprocessing step on the Chinese corpus. Moreover since the parallel corpus is downloaded from Internet the corpus is noisy regarding to alignment between corresponding translated sentences. Therefore we used 60 hours of manually work to align the sentences in the English and Chinese parallel corpus before performing automatic word alignment using Uplug. The word alignment with Uplug was carried out from English to Chinese. Nine respondents evaluated the resulting English-Chinese word list with frequency equal to or above three and we obtained an accuracy of 73.1 percent."
carlsson-dalianis-2010-influence,Influence of Module Order on Rule-Based De-identification of Personal Names in Electronic Patient Records Written in {S}wedish,2010,14,2,2,0,45469,elin carlsson,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Electronic patient records (EPRs) are a valuable resource for research but for confidentiality reasons they cannot be used freely. In order to make EPRs available to a wider group of researchers, sensitive information such as personal names has to be removed. De-identification is a process that makes this possible. Both rule-based as well as statistical and machine learning based methods exist to perform de-identification, but the second method requires annotated training material which exists only very sparsely for patient names. It is therefore necessary to use rule-based methods for de-identification of EPRs. Not much is known, however, about the order in which the various rules should be applied and how the different rules influence precision and recall. This paper aims to answer this research question by implementing and evaluating four common rules for de-identification of personal names in EPRs written in Swedish: (1) dictionary name matching, (2) title matching, (3) common words filtering and (4) learning from previous modules. The results show that to obtain the highest recall and precision, the rules should be applied in the following order: title matching, common words filtering and dictionary name matching."
dalianis-velupillai-2010-certain,"How Certain are Clinical Assessments? Annotating {S}wedish Clinical Text for (Un)certainties, Speculations and Negations",2010,7,9,1,1,2663,hercules dalianis,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Clinical texts contain a large amount of information. Some of this information is embedded in contexts where e.g. a patient status is reasoned about, which may lead to a considerable amount of statements that indicate uncertainty and speculation. We believe that distinguishing such instances from factual statements will be very beneficial for automatic information extraction. We have annotated a subset of the Stockholm Electronic Patient Record Corpus for certain and uncertain expressions as well as speculative and negation keywords, with the purpose of creating a resource for the development of automatic detection of speculative language in Swedish clinical text. We have analyzed the results from the initial annotation trial by means of pairwise Inter-Annotator Agreement (IAA) measured with F-score. Our main findings are that IAA results for certain expressions and negations are very high, but for uncertain expressions and speculative keywords results are less encouraging. These instances need to be defined in more detail. With this annotation trial, we have created an important resource that can be used to further analyze the properties of speculative language in Swedish clinical text. Our intention is to release this subset to other research groups in the future after removing identifiable information."
W09-4606,Using Uplug and {S}ite{S}eeker to construct a cross language search engine for Scandinavian languages,2009,20,0,1,1,2663,hercules dalianis,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,Using Uplug and SiteSeeker to construct a cross language search engine for Scandinavian languages
R09-1026,Identification of Parallel Text Pairs Using Fingerprints,2009,9,1,2,0.952381,44034,martin hassel,Proceedings of the International Conference {RANLP}-2009,0,"When creating dictionaries for use in for example crosslanguage search engines, one often uses a word alignment system that takes parallel or comparable text pairs as input and produces a word list. Multilingual web sites may contain parallel texts but these can be difficult to detect. In this article we describe an experiment on automatic identification of parallel text pairs. We utilize the frequency distribution of word initial letters in order to map a text in one language to a corresponding text in another in the JRC-Acquis corpus (European Council legal texts). Using English and Swedish as language pair, and running a ten-fold random pairing, the algorithm made 87 percent correct matches (baseline-random 50 percent). Attempting to map the correct text among nine randomly chosen false matches and one true yielded a success rate of 68 percent (baseline-random 10 percent)."
P09-1017,"Automatic training of lemmatization rules that handle morphological changes in pre-, in- and suffixes alike",2009,15,33,2,0,5459,bart jongejan,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We propose a method to automatically train lemmatization rules that handle prefix, infix and suffix changes to generate the lemma from the full form of a word. We explain how the lemmatization rules are created and how the lemmatizer works. We trained this lemmatizer on Danish, Dutch, English, German, Greek, Icelandic, Norwegian, Polish, Slovene and Swedish full form-lemma pairs respectively. We obtained significant improvements of 24 percent for Polish, 2.3 percent for Dutch, 1.5 percent for English, 1.2 percent for German and 1.0 percent for Swedish compared to plain suffix lemmatization using a suffix-only lemmatizer. Icelandic deteriorated with 1.9 percent. We also made an observation regarding the number of produced lemmatization rules as a function of the number of training pairs."
W08-1403,Automatic Construction of Domain-specific Dictionaries on Sparse Parallel Corpora in the Nordic languages,2008,12,7,2,0.576923,16933,sumithra velupillai,Coling 2008: Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization,0,"Halla Norden is a web site with information regarding mobility between the Nordic countries in five different languages; Swedish, Danish, Norwegian, Icelandic and Finnish. We wanted to create a Nordic cross-language dictionary for the use in a cross-language search engine for Halla Norden. The entire set of texts on the web site was treated as one multilingual parallel corpus. From this we extracted parallel corpora for each language pair. The corpora were very sparse, containing on average less than 80 000 words per language pair. We have used the Uplug word alignment system (Tiedemann 2003a), for the creation of the dictionaries. The results gave on average 213 new dictionary words (frequency > 3) per language pair. The average error rate was 16 percent. Different combinations with Finnish had a higher error rate, 33 percent, whereas the error rate for the remaining language pairs only yielded on average 9 percent errors. The high error rate for Finnish is possibly due to the fact that the Finnish language belongs to a different language family. Although the corpora were very sparse the word alignment results for the combinations of Swedish, Danish, Norwegian and Icelandic were surprisingly good compared to other experiments with larger corpora."
karlgren-etal-2008-experiments,Experiments to Investigate the Connection between Case Distribution and Topical Relevance of Search Terms in an Information Retrieval Setting,2008,9,1,2,0,21633,jussi karlgren,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We have performed a set of experiments made to investigate the utility of morphological analysis to improve retrieval of documents written in languages with relatively large morphological variation in a practical commercial setting, using the SiteSeeker search system developed and marketed by Euroling Ab. The objective of the experiments was to evaluate different lemmatisers and stemmers to determine which would be the most practical for the task at hand: highly interactive, relatively high precision web searches in commercial customer-oriented document collections. This paper gives an overview of some of the results for Finnish and German, and describes specifically one experiment designed to investigate the case distribution of nouns in a highly inflectional language (Finnish) and the topicality of the nouns in target texts. We find that topical nouns taken from queries are distributed differently over relevant and non-relevant documents depending on their grammatical case."
dalianis-jongejan-2006-hand,Hand-crafted versus Machine-learned Inflectional Rules: The Euroling-{S}ite{S}eeker Stemmer and {CST}{'}s Lemmatiser,2006,4,10,1,1,2663,hercules dalianis,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The Euroling stemmer is developed for a commercial web site and intranet search engine called SiteSeeker. SiteSeeker is basically used in the Swedish domain but to some extent also for the English domain. CST's lemmatiser comes from the Center for Language Technology, University of Copenhagen and was originally developed as a research prototype to create lemmatisation rules from training data. In this paper we compare the performance of the stemmer that uses handcrafted rules for Swedish, Danish and Norwegian as well one stemmer for Greek with CST's lemmatiser that uses training data to extract lemmatisation rules for Swedish, Danish, Norwegian and Greek. The performances of the two approaches are about the same with around 10 percent errors. The handcrafted rule based stemmer techniques are easy to get started with if the programmer has the proper linguistic knowledge. The machine trained sets of lemmatisation rules are very easy to produce without having linguistic knowledge given that one has correct training data."
W05-1706,Improving search engine retrieval using a compound splitter for {S}wedish,2006,-1,-1,1,1,2663,hercules dalianis,Proceedings of the 15th Nordic Conference of Computational Linguistics ({NODALIDA} 2005),0,None
W01-1703,Improving Precision in Information Retrieval for {S}wedish using Stemming,2001,6,75,2,0,53758,johan carlberger,Proceedings of the 13th Nordic Conference of Computational Linguistics ({NODALIDA} 2001),0,"We will in this paper present an evaluation of how much stemming improves precision in information retrieval for Swedish texts. To perform this, we built an information retrieval tool with optional stemming and created a tagged corpus in Swedish. We know that stemming in information retrieval for English, Dutch and Slovenian gives better precision the more inflecting the language is, but precision depends also on query length and document length. Our final results were that stemming improved both precision and recall with 15 respectively 18 percent for Swedish texts having an average length of 181 words."
W96-0508,On Lexical Aggregation and Ordering,1996,5,6,1,1,2663,hercules dalianis,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,"Aggregation is the process of removing redundant information during language generation while preserving the information to be conveyed. Aggregation is an important component of text or sentence planning. Without aggregation, automated language generation systems would not be able to p roduce f luent text f rom real -wor ld databases and knowledge bases, since information is rarely stored in computers in forms directly supporting fluent expression. Various types of aggregation (syntactic, lexical, referential) have been identified in [Hovy88, Cook84, Reinhart91, Horacek92, Dalianis&Hovy93,Wilkinson95,Dalianis95a, 95b,96a]. This paper investigates lexical aggregation, the process by which a set of items is replaced with a single new lexeme that encompasses the same meaning. We call the elements that will be aggregated the aggregands and the element (the lexeme) which is the result of the aggregation the aggregator. Lexical aggregation can be divided into two major types, bounded and unbounded. With Bounded Lexical (BL) aggregation the aggregator lexeme covers a closed set of concepts and the redundancy is obvious, the aggregated information is recoverable, and the aggregation process must be carried out. In contrast, Unbounded Lexical (UL) aggregation is carried out over an open set of aggregands and consequently the aggregated information is not recoverable and has to be licensed by other factors, such as the hearer's goals. The example in Figure 1 contains both types of aggregation, where f ight and week are the unbounded and bounded aggregators respectively."
E95-1042,Aggregation in the {NL}-generator of the Visual and Natural language Specification Tool,1995,9,3,1,1,2663,hercules dalianis,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,In this paper we show how to use the socalled aggregation technique to remove redundancies in the fact base of the Visual and Natural language Specification Tool (VINST). The current aggregation modules of the natural language generator of VINST is described and an improvement is proposed with one new aggregation rule and a bidirectional grammar.
