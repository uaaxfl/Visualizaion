2021.unimplicit-1.4,{U}n{I}mplicit Shared Task Report: Detecting Clarification Requirements in Instructional Text,2021,-1,-1,1,1,660,michael roth,Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language,0,"This paper describes the data, task setup, and results of the shared task at the First Workshop on Understanding Implicit and Underspecified Language (UnImplicit). The task requires computational models to predict whether a sentence contains aspects of meaning that are contextually unspecified and thus require clarification. Two teams participated and the best scoring system achieved an accuracy of 68{\%}."
2021.eacl-srw.5,A Computational Analysis of Vagueness in Revisions of Instructional Texts,2021,-1,-1,2,0,10456,alok debnath,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"WikiHow is an open-domain repository of instructional articles for a variety of tasks, which can be revised by users. In this paper, we extract pairwise versions of an instruction before and after a revision was made. Starting from a noisy dataset of revision histories, we specifically extract and analyze edits that involve cases of vagueness in instructions. We further investigate the ability of a neural model to distinguish between two versions of an instruction in our data by adopting a pairwise ranking task from previous work and showing improvements over existing baselines."
2021.codi-main.6,Resolving Implicit References in Instructional Texts,2021,-1,-1,2,1,661,talita anthonio,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,0,"The usage of (co-)referring expressions in discourse contributes to the coherence of a text. However, text comprehension can be difficult when referring expressions are non-verbalized and have to be resolved in the discourse context. In this paper, we propose a novel dataset of such implicit references, which we automatically derive from insertions of references in collaboratively edited how-to guides. Our dataset consists of 6,014 instances, making it one of the largest datasets of implicit references and a useful starting point to investigate misunderstandings caused by underspecified language. We test different methods for resolving implicit references in our dataset based on the Generative Pre-trained Transformer model (GPT) and compare them to heuristic baselines. Our experiments indicate that GPT can accurately resolve the majority of implicit references in our data. Finally, we investigate remaining errors and examine human preferences regarding different resolutions of an implicit reference given the discourse context."
2020.lrec-1.702,wiki{H}ow{T}o{I}mprove: A Resource and Analyses on Edits in Instructional Texts,2020,-1,-1,3,1,661,talita anthonio,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Instructional texts, such as articles in wikiHow, describe the actions necessary to accomplish a certain goal. In wikiHow and other resources, such instructions are subject to revision edits on a regular basis. Do these edits improve instructions only in terms of style and correctness, or do they provide clarifications necessary to follow the instructions and to accomplish the goal? We describe a resource and first studies towards answering this question. Specifically, we create wikiHowToImprove, a collection of revision histories for about 2.7 million sentences from about 246000 wikiHow articles. We describe human annotation studies on categorizing a subset of sentence-level edits and provide baseline models for the task of automatically distinguishing {``}older{''} from {``}newer{''} revisions of a sentence."
2020.emnlp-main.675,Towards Modeling Revision Requirements in wiki{H}ow Instructions,2020,-1,-1,3,0,18044,irshad bhat,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"wikiHow is a resource of how-to guidesthat describe the steps necessary to accomplish a goal. Guides in this resource are regularly edited by a community of users, who try to improve instructions in terms of style, clarity and correctness. In this work, we test whether the need for such edits can be predicted automatically. For this task, we extend an existing resource of textual edits with a complementary set of approx. 4 million sentences that remain unedited over time and report on the outcome of two revision modeling experiments."
2020.crac-1.4,Predicting Coreference in {A}bstract {M}eaning {R}epresentations,2020,-1,-1,3,0,11450,tatiana anikina,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference",0,"This work addresses coreference resolution in Abstract Meaning Representation (AMR) graphs, a popular formalism for semantic parsing. We evaluate several current coreference resolution techniques on a recently published AMR coreference corpus, establishing baselines for future work. We also demonstrate that coreference resolution can improve the accuracy of a state-of-the-art semantic parser on this corpus."
2020.coling-main.117,What Can We Learn from Noun Substitutions in Revision Histories?,2020,-1,-1,2,1,661,talita anthonio,Proceedings of the 28th International Conference on Computational Linguistics,0,"In community-edited resources such as wikiHow, sentences are subject to revisions on a daily basis. Recent work has shown that resulting improvements over time can be modelled computationally, assuming that each revision contributes to the improvement. We take a closer look at a subset of such revisions, for which we attempt to improve a computational model and validate in how far the assumption that {`}revised means better{'} actually holds. The subset of revisions considered here are noun substitutions, which often involve interesting semantic relations, including synonymy, antonymy and hypernymy. Despite the high semantic relatedness, we find that a supervised classifier can distinguish the revised version of a sentence from an original version with an accuracy close to 70{\%}, when taking context into account. In a human annotation study, we observe that annotators identify the revised sentence as the {`}better version{'} with similar performance. Our analysis reveals a fair agreement among annotators when a revision improves fluency. In contrast, noun substitutions that involve other lexical-semantic relationships are often perceived as being equally good or tend to cause disagreements. While these findings are also reflected in classification scores, a comparison of results shows that our model fails in cases where humans can resort to factual knowledge or intuitions about the required level of specificity."
W19-3410,Detecting Everyday Scenarios in Narrative Texts,2019,42,0,2,0,24460,lilian wanzare,Proceedings of the Second Workshop on Storytelling,0,"Script knowledge consists of detailed information on everyday activities. Such information is often taken for granted in text and needs to be inferred by readers. Therefore, script knowledge is a central component to language comprehension. Previous work on representing scripts is mostly based on extensive manual work or limited to scenarios that can be found with sufficient redundancy in large corpora. We introduce the task of scenario detection, in which we identify references to scripts. In this task, we address a wide range of different scripts (200 scenarios) and we attempt to identify all references to them in a collection of narrative texts. We present a first benchmark data set and a baseline model that tackles scenario detection using techniques from topic segmentation and text classification."
S19-1012,{MCS}cript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants,2019,32,1,2,1,25223,simon ostermann,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"We introduce MCScript2.0, a machine comprehension corpus for the end-to-end evaluation of script knowledge. MCScript2.0 contains approx. 20,000 questions on approx. 3,500 texts, crowdsourced based on a new collection process that results in challenging questions. Half of the questions cannot be answered from the reading texts, but require the use of commonsense and, in particular, script knowledge. We give a thorough analysis of our corpus and show that while the task is not challenging to humans, existing machine comprehension models fail to perform well on the data, even if they make use of a commonsense knowledge base. The dataset is available at http://www.sfb1102. uni-saarland.de/?page{\_}id=2582"
N19-1390,Combining Discourse Markers and Cross-lingual Embeddings for Synonym{--}Antonym Classification,2019,0,0,1,1,660,michael roth,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"It is well-known that distributional semantic approaches have difficulty in distinguishing between synonyms and antonyms (Grefenstette, 1992; Pad{\'o} and Lapata, 2003). Recent work has shown that supervision available in English for this task (e.g., lexical resources) can be transferred to other languages via cross-lingual word embeddings. However, this kind of transfer misses monolingual distributional information available in a target language, such as contrast relations that are indicative of antonymy (e.g. hot ... while ... cold). In this work, we improve the transfer by exploiting monolingual information, expressed in the form of co-occurrences with discourse markers that convey contrast. Our approach makes use of less than a dozen markers, which can easily be obtained for many languages. Compared to a baseline using only cross-lingual embeddings, we show absolute improvements of 4{--}10{\%} F1-score in Vietnamese and Hindi."
D19-6007,Commonsense Inference in Natural Language Processing ({COIN}) - Shared Task Report,2019,0,1,3,1,25223,simon ostermann,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,0,"This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019. The tasks consisted of two machine comprehension evaluations, each of which tested a system{'}s ability to answer questions/queries about a text. Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text. A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6{\%} accuracy and 83.7{\%} F1-score on task 1 and task 2, respectively."
S18-1119,{S}em{E}val-2018 Task 11: Machine Comprehension Using Commonsense Knowledge,2018,0,45,2,1,25223,simon ostermann,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This report summarizes the results of the SemEval 2018 task on machine comprehension using commonsense knowledge. For this machine comprehension task, we created a new corpus, MCScript. It contains a high number of questions that require commonsense knowledge for finding the correct answer. 11 teams from 4 different countries participated in this shared task, most of them used neural approaches. The best performing system achieves an accuracy of 83.95{\%}, outperforming the baselines by a large margin, but still far from the human upper bound, which was found to be at 98{\%}."
N18-1023,Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences,2018,0,62,3,0,3541,daniel khashabi,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1{\%}. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills."
L18-1564,{MCS}cript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge,2018,21,11,3,1,25223,simon ostermann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community."
W17-6934,Role Semantics for Better Models of Implicit Discourse Relations,2017,0,0,1,1,660,michael roth,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-0906,{LSDS}em 2017 Shared Task: The Story Cloze Test,2017,0,32,2,0,20402,nasrin mostafazadeh,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,"The LSDSem{'}17 shared task is the Story Cloze Test, a new evaluation for story understanding and script learning. This test provides a system with a four-sentence story and two possible endings, and the system must choose the correct ending to the story. Successful narrative understanding (getting closer to human performance of 100{\%}) requires systems to link various levels of semantics to commonsense knowledge. A total of eight systems participated in the shared task, with a variety of approaches including."
S17-1016,Aligning Script Events with Narrative Texts,2017,24,0,2,1,25223,simon ostermann,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Script knowledge plays a central role in text understanding and is relevant for a variety of downstream tasks. In this paper, we consider two recent datasets which provide a rich and general representation of script events in terms of paraphrase sets. We introduce the task of mapping event mentions in narrative texts to such script event types, and present a model for this task that exploits rich linguistic representations as well as information on temporal ordering. The results of our experiments demonstrate that this complex task is indeed feasible."
D17-3004,Semantic Role Labeling,2017,29,12,2,0,20359,diego marcheggiani,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"This tutorial describes semantic role labelling (SRL), the task of mapping text to shallow semantic representations of eventualities and their participants. The tutorial introduces the SRL task and discusses recent research directions related to the task. The audience of this tutorial will learn about the linguistic background and motivation for semantic roles, and also about a range of computational models for this task, from early approaches to the current state-of-the-art. We will further discuss recently proposed variations to the traditional SRL task, including topics such as semantic proto-role labeling.We also cover techniques for reducing required annotation effort, such as methods exploiting unlabeled corpora (semi-supervised and unsupervised techniques), model adaptation across languages and domains, and methods for crowdsourcing semantic role annotation (e.g., question-answer driven SRL). Methods based on different machine learning paradigms, including neural networks, generative Bayesian models, graph-based algorithms and bootstrapping style techniques.Beyond sentence-level SRL, we discuss work that involves semantic roles in discourse. In particular, we cover data sets and models related to the task of identifying implicit roles and linking them to discourse antecedents. We introduce different approaches to this task from the literature, including models based on coreference resolution, centering, and selectional preferences. We also review how new insights gained through them can be useful for the traditional SRL task."
P16-1113,Neural Semantic Role Labeling with Dependency Path Embeddings,2016,41,65,1,1,660,michael roth,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as subsequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method."
W15-0403,Parsing Software Requirements with an Ontology-based Semantic Role Labeler,2015,18,6,1,1,660,michael roth,Proceedings of the 1st Workshop on Language and Ontologies,0,None
Q15-1032,Context-aware Frame-Semantic Role Labeling,2015,38,8,1,1,660,michael roth,Transactions of the Association for Computational Linguistics,0,"Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features which we motivate based on linguistic insights and experimentally demonstrate that they lead to significant improvements over the current state-of-the-art in FrameNet-based semantic role labeling."
J15-4003,Inducing Implicit Arguments from Comparable Texts: A Framework and Its Applications,2015,85,6,1,1,660,michael roth,Computational Linguistics,0,"In this article, we investigate aspects of sentential meaning that are not expressed in local predicate-argument structures. In particular, we examine instances of semantic arguments that are only inferable from discourse context. The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve computational models of language. As contributions towards this goal, we establish an effective framework for the difficult task of inducing implicit arguments and their antecedents in discourse and empirically demonstrate the importance of modeling this phenomenon in discourse-level tasks.n n Our framework builds upon a novel projection approach that allows for the accurate detection of implicit arguments by aligning and comparing predicate-argument structures across pairs of comparable texts. As part of this framework, we develop a graph-based model for predicate alignment that significantly outperforms previous approaches. Based on such alignments, we show that implicit argument instances can be automatically induced and applied to improve a current model of linking implicit arguments in discourse. We further validate that decisions on argument realization, although being a subtle phenomenon most of the time, can considerably affect the perceived coherence of a text. Our experiments reveal that previous models of coherence are not able to predict this impact. Consequently, we develop a novel coherence model, which learns to accurately predict argument realization based on automatically aligned pairs of implicit and explicit arguments."
W14-2410,Software Requirements: A new Domain for Semantic Parsers,2014,12,9,1,1,660,michael roth,Proceedings of the {ACL} 2014 Workshop on Semantic Parsing,0,"Software requirements are commonly written in natural language, making them prone to ambiguity, incompleteness and inconsistency. By converting requirements to formal semantic representations, emerging problems can be detected at an early stage of the development process, thus reducing the number of ensuing errors and the development costs. In this paper, we treat the mapping from requirements to formal representations as a semantic parsing task. We describe a novel data set for this task that involves two contributions: first, we establish an ontology for formally representing requirements; and second, we introduce an iterative annotation scheme, in which formal representations are derived through step-wise refinements."
P14-2086,Combining Word Patterns and Discourse Markers for Paradigmatic Relation Classification,2014,38,9,1,1,660,michael roth,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distinguishing between paradigmatic relations such as synonymy, antonymy and hypernymy is an important prerequisite in a range of NLP applications. In this paper, we explore discourse relations as an alternative set of features to lexico-syntactic patterns. We demonstrate that statistics over discourse relations, collected via explicit discourse markers as proxies, can be utilized as salient indicators for paradigmatic relations in multiple languages, outperforming patterns in terms of recall and F1-score. In addition, we observe that markers and patterns provide complementary information, leading to significant classification improvements when applied in combination."
D14-1045,Composition of Word Representations Improves Semantic Role Labelling,2014,32,34,1,1,660,michael roth,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"State-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance. Unfortunately, such corpora are expensive to produce and often do not generalize well across domains. Even in domain, errors are often made where syntactic information does not provide sufficient cues. In this paper, we mitigate both of these problems by employing distributional word representations gathered from unlabelled data. While straight-forward word representations of predicates and arguments improve performance, we show that further gains are achieved by composing representations that model the interaction between predicate and argument, and capture full argument spans."
W13-0211,"Predicate-specific Annotations for Implicit Role Binding: Corpus Annotation, Data Analysis and Evaluation Experiments",2013,12,7,2,0,41157,tatjana moor,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"Current research on linking implicit roles in discourse is severely hampered by the lack of sufficient training resources, especially in the verbal domain: learning algorithms require higher-volume annotations for specific predicates in order to derive valid generalizations, and a larger volume of annotations is crucial for insightful evaluation and comparison of alternative models for role linking. We present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. A qualitative data analysis leads to observations regarding implicit role realization that can guide further annotation efforts. Experiments using role linking annotations for five predicates demonstrate high performance for these target predicates. Using our additional data in the SemEval task, we obtain overall performance gains of 2-4 points F1-score."
S13-1043,Automatically Identifying Implicit Arguments to Improve Argument Linking and Coherence Modeling,2013,38,8,1,1,660,michael roth,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"Implicit arguments are a discourse-level phenomenon that has not been extensively studied in semantic processing. One reason for this lies in the scarce amount of annotated data sets available. We argue that more data of this kind would be helpful to improve existing approaches to linking implicit arguments in discourse and to enable more in-depth studies of the phenomenon itself. In this paper, we present a range of studies that empirically validate this claim. Our contributions are threefold: we present a heuristic approach to automatically identify implicit arguments and their antecedents by exploiting comparable texts; we show how the induced data can be used as training data for improving existing argument linking models; finally, we present a novel approach to modeling local coherence that extends previous approaches by taking into account non-explicit entity references."
S12-1030,Aligning Predicate Argument Structures in Monolingual Comparable Texts: A New Corpus for a New Task,2012,26,21,1,1,660,michael roth,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"Discourse coherence is an important aspect of natural language that is still understudied in computational linguistics. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures (PAS) in a model that exceeds the sentence level. In particular, we aim to study the case of non-realized arguments as a coherence inducing factor. This task can be broken down into two subtasks. The first aligns predicates across comparable texts, admitting partial argument structure correspondence. The resulting alignments and their contexts can then be used for developing a coherence model for argument realization.n n This paper introduces a large corpus of comparable monolingual texts as a prerequisite for approaching this task, including an evaluation set with manual predicate alignments. We illustrate the potential of this new resource for the empirical investigation of discourse coherence phenomena. Initial experiments on the task of predicting predicate alignments across text pairs show promising results. Our findings establish that manual and automatic predicate alignments across texts are feasible and that our data set holds potential for empirical research into a variety of discourse-related tasks."
D12-1016,Aligning Predicates across Monolingual Comparable Texts using Graph-based Clustering,2012,39,11,1,1,660,michael roth,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Generating coherent discourse is an important aspect in natural language generation. Our aim is to learn factors that constitute coherent discourse from data, with a focus on how to realize predicate-argument structures in a model that exceeds the sentence level. We present an important subtask for this overall goal, in which we align predicates across comparable texts, admitting partial argument structure correspondence. The contribution of this work is two-fold: We first construct a large corpus resource of comparable texts, including an evaluation set with manual predicate alignments. Secondly, we present a novel approach for aligning predicates across comparable texts using graph-based clustering with Mincuts. Our method significantly outperforms other alignment techniques when applied to this novel alignment task, by a margin of at least 6.5 percentage points in F1-score."
C10-1108,Computing {EM}-based Alignments of Routes and Route Directions as a Basis for Natural Language Generation,2010,20,7,1,1,660,michael roth,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Route directions are natural language (NL) statements that specify, for a given navigational task and an automatically computed route representation, a sequence of actions to be followed by the user to reach his or her goal. A corpus-based approach to generate route directions involves (i) the selection of elements along the route that need to be mentioned, and (ii) the induction of a mapping from route elements to linguistic structures that can be used as a basis for NL generation.n n This paper presents an Expectation-Maximization (EM) based algorithm that aligns geographical route representations with semantically annotated NL directions, as a basis for the above tasks. We formulate one basic and two extended models, the latter capturing special properties of the route direction task. Although our current data set is small, both extended models achieve better results than the simple model and a random baseline. The best results are achieved by a combination of both extensions, which outperform the random baseline and the simple model by more than an order of magnitude."
W09-2814,Creating an Annotated Corpus for Generating Walking Directions,2009,16,7,2,0,46948,stephanie schuldes,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"This work describes first steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data."
P09-4010,A {NLG}-based Application for Walking Directions,2009,4,12,1,1,660,michael roth,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"This work describes an online application that uses Natural Language Generation (NLG) methods to generate walking directions in combination with dynamic 2D visualisation. We make use of third party resources, which provide for a given query (geographic) routes and landmarks along the way. We present a statistical model that can be used for generating natural language directions. This model is trained on a corpus of walking directions annotated with POS, grammatical information, frame-semantics and markup for temporal structure."
roth-schulte-im-walde-2008-corpus,"Corpus Co-Occurrence, Dictionary and {W}ikipedia Entries as Resources for Semantic Relatedness Information",2008,21,5,1,1,660,michael roth,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Distributional, corpus-based descriptions have frequently been applied to model aspects of word meaning. However, distributional models that use corpus data as their basis have one well-known disadvantage: even though the distributional features based on corpus co-occurrence were often successful in capturing meaning aspects of the words to be described, they generally fail to capture those meaning aspects that refer to world knowledge, because coherent texts tend not to provide redundant information that is presumably available knowledge. The question we ask in this paper is whether dictionary and encyclopaedic resources might complement the distributional information in corpus data, and provide world knowledge that is missing in corpora. As test case for meaning aspects, we rely on a collection of semantic associates to German verbs and nouns. Our results indicate that a combination of the knowledge resources should be helpful in work on distributional descriptions."
D08-1048,Automatic induction of {F}rame{N}et lexical units,2008,22,47,5,0,44420,marco pennacchiotti,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNet-based models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined."
