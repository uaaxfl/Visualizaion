2020.emnlp-main.201,D14-1162,0,0.0841664,"Missing"
2020.emnlp-main.201,D13-1066,0,0.514565,"Missing"
2020.emnlp-main.201,swanson-etal-2014-getting,0,0.0198662,"inthe-wild” sarcastic tweets by leveraging author∗ generated labels such as the #sarcasm hashtag (Davidov et al., 2010; Pt´acˇ ek et al., 2014). This method generates large amounts of data at low cost, but labels are often noisy and biased (Bamman and Smith, 2015). To improve quality, manual annotation asks humans to label given tweets as sarcastic or not. Since finding sarcasm in a large corpus is “a needle-in-ahaystack problem” (Liebrecht et al., 2013), manual annotation can be combined with distant supervision (Riloff et al., 2013). Still, low inter-annotator reliability is often reported (Swanson et al., 2014), resulting not only from the subjective nature of sarcasm but also the lack of cultural context (Joshi et al., 2016). Moreover, neither method collects both sarcasm perspectives: distant supervision collects intended sarcasm, while manual annotation can only collect perceived sarcasm. Lastly, in manual collection, humans are asked to gather and report sarcastic texts, either their own (Oprea and Magdy, 2020) or by others (Filatova, 2012). However, both manual methods are slower and more expensive than distant supervision, resulting in smaller datasets. To overcome the above limitations, we pr"
2020.emnlp-main.201,P18-1093,0,0.343778,"Missing"
2020.emnlp-main.312,P13-4024,0,0.0137488,"language (ESL) learners have trouble using near-synonyms correctly (Liu and Zhong, 2014; Liu, 2013). “Nearsynonym” refers to a word whose meaning is similar but not identical to that of another word, for instance, establish and construct. An experience common to many ESL learners is looking for example sentences to learn how two nearly synonymous words differ (Liu, 2013; Liu and Jiang, 2009). To facilitate the learner’s learning process, our focus is on finding example sentences to clarify English near-synonyms. In previous work, researchers develop linguistic search engines, such as Linggle (Boisson et al., 2013) and Netspeak1 , to allow users to query English words in terms of n-gram frequency. However, these tools can only help people investigate the difference, where learners are required to make assumptions toward the subtlety and verify them with the tools, but can not tell the difference proactively. Other work attempts to automatically retrieve example sentences for dictionary entries (Kilgarriff et al., 2008); however, finding clarifying examples for near-synonyms is not the goal of such work. In a rare exception, Huang et al. (2017) retrieve useful examples for near-synonyms by defining a cla"
2020.emnlp-main.312,D15-1075,0,0.0388647,"owerful learning capability could be used to understand the subtlety lies in the near-synonyms. In this paper, our models are all designed on top of the pre3808 trained BERT model. Natural Language Inference. Our proposed model directly learns the difference and sentence quality by imitating the human reactions of learning material and behavior of learning from example sentences. The idea of learning from example is similar to natural language inference (NLI) task and recognizing question entailment (RQE) task. There are various NLI dataset varied in size, construction, genre, labels classes (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018; Lai et al., 2017). In the NLI task, each instance consists of two natural language text: a premise, a hypothesis, and a label indicating the relationship whether a premise entails the hypothesis. RQE, on the other hand, identifies entailment between two questions in the context of question answering. Abacha and Demner-Fushman (2016) used the definition of question entailment: “a question A entails a question B if every answer to B is also a complete or partial answer to A.” Though NLI and RQE research has acquired lots of success, to the best of our"
2020.emnlp-main.312,P97-1067,0,0.321765,"nerate an example sentence set that clarifies near-synonyms. The only existing work is from Huang et al. (2017), who designed the fitness score and relative closeness score to represent the sentence’s ability to clarify near-synonyms. Our work enables the models to learn the concept of “usefulness” directly from data to reduce the possible issues of the human-crafted scoring function. Near-synonyms Disambiguation. Unlike the language modeling task that aims at predicting the next word given the context, near-synonyms disambiguation focuses on differentiating the subtlety of the near-synonyms. Edmonds (1997) first introduced a lexical co-occurrence network with secondorder co-occurrence for near-synonym disambiguation. Edmonds also suggested a fill-in-the-blank (FITB) task, providing a benchmark for evaluating lexical choice performance on near-synonyms. Islam and Inkpen (2010) used the Google 5-gram dataset to distinguish near-synonyms using language modeling techniques. Wang and Hirst (2010) encoded words into vectors in latent semantic space and applied a machine learning model to learn the difference. Huang et al. (2017) applied BiLSTM and GMM models to learn the subtle context distribution."
2020.emnlp-main.312,I17-1011,0,0.0149703,"tlety lies in the near-synonyms. In this paper, our models are all designed on top of the pre3808 trained BERT model. Natural Language Inference. Our proposed model directly learns the difference and sentence quality by imitating the human reactions of learning material and behavior of learning from example sentences. The idea of learning from example is similar to natural language inference (NLI) task and recognizing question entailment (RQE) task. There are various NLI dataset varied in size, construction, genre, labels classes (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018; Lai et al., 2017). In the NLI task, each instance consists of two natural language text: a premise, a hypothesis, and a label indicating the relationship whether a premise entails the hypothesis. RQE, on the other hand, identifies entailment between two questions in the context of question answering. Abacha and Demner-Fushman (2016) used the definition of question entailment: “a question A entails a question B if every answer to B is also a complete or partial answer to A.” Though NLI and RQE research has acquired lots of success, to the best of our knowledge, we are the first to attempt using these two tasks"
2020.emnlp-main.312,S18-2023,0,0.0293793,"Missing"
2020.emnlp-main.312,D12-1071,0,0.0221115,") used the definition of question entailment: “a question A entails a question B if every answer to B is also a complete or partial answer to A.” Though NLI and RQE research has acquired lots of success, to the best of our knowledge, we are the first to attempt using these two tasks on language learning problems. Poliak et al. (2018)’s recast version of the definite pronoun resolution (DPR) task inspired us to build learner-like agents with entailment modeling . In the original DPR problem, sentences contain two entities and one pronoun, and the mission is to link the pronoun to its referent (Rahman and Ng, 2012). In the recast version, the premises are the original sentences, and the hypothesis is the same sentence with the pronoun replaced with its correct (entailed) and incorrect (not-entailed) reference. We believe our proposed entailment modeling can help the model to understand the relationship between the given example sentence and question for the target near-synonym. Thus entailment modeling enables the learner-like agent to mimic human behavior through inference. 3 Method In this paper, we use learner-like agent to refer to a model that answers questions given examples. The goal of the learn"
2020.emnlp-main.312,P13-2043,0,0.0243079,"en near-synonyms, and (iii) construct the first dataset of helpful example sentences for ESL learners.2 2 Related Works This task is related to (i) learning material generation, (ii) near-synonyms disambiguation, and (iii) natural language inference. 2 Dataset and code are available https://github.com/joyyyjen/ Inference-Based-Learner-Like-Agent here: Learning Material Generation. Collecting learning material is one of the hardest tasks for both teachers and students. Researchers have long been looking for methods to generate high-quality learning material automatically. Sumita et al. (2005); Sakaguchi et al. (2013) proposed approaches to generate fill-in-the-blank questions to evaluate students language proficiency automatically. Lin et al. (2007); Susanti et al. (2018); Liu et al. (2018) worked on generating good distractors for multiplechoice questions. However, there are only a few tasks working on automatic example sentence collection and generation. Kilgarriff et al. (2008); Didakowski et al. (2012) proposed a set of criteria for a good example sentences and Tolmachev and Kurohashi (2017) used sentence similarity and quality as features to extract high-quality examples. These tasks only focused on"
2020.emnlp-main.312,W05-0210,0,0.0841369,"discern nuances between near-synonyms, and (iii) construct the first dataset of helpful example sentences for ESL learners.2 2 Related Works This task is related to (i) learning material generation, (ii) near-synonyms disambiguation, and (iii) natural language inference. 2 Dataset and code are available https://github.com/joyyyjen/ Inference-Based-Learner-Like-Agent here: Learning Material Generation. Collecting learning material is one of the hardest tasks for both teachers and students. Researchers have long been looking for methods to generate high-quality learning material automatically. Sumita et al. (2005); Sakaguchi et al. (2013) proposed approaches to generate fill-in-the-blank questions to evaluate students language proficiency automatically. Lin et al. (2007); Susanti et al. (2018); Liu et al. (2018) worked on generating good distractors for multiplechoice questions. However, there are only a few tasks working on automatic example sentence collection and generation. Kilgarriff et al. (2008); Didakowski et al. (2012) proposed a set of criteria for a good example sentences and Tolmachev and Kurohashi (2017) used sentence similarity and quality as features to extract high-quality examples. The"
2020.emnlp-main.312,W17-5014,0,0.0252618,"ers have long been looking for methods to generate high-quality learning material automatically. Sumita et al. (2005); Sakaguchi et al. (2013) proposed approaches to generate fill-in-the-blank questions to evaluate students language proficiency automatically. Lin et al. (2007); Susanti et al. (2018); Liu et al. (2018) worked on generating good distractors for multiplechoice questions. However, there are only a few tasks working on automatic example sentence collection and generation. Kilgarriff et al. (2008); Didakowski et al. (2012) proposed a set of criteria for a good example sentences and Tolmachev and Kurohashi (2017) used sentence similarity and quality as features to extract high-quality examples. These tasks only focused on the quality of a single example sentence, whereas our goal in this paper is to generate an example sentence set that clarifies near-synonyms. The only existing work is from Huang et al. (2017), who designed the fitness score and relative closeness score to represent the sentence’s ability to clarify near-synonyms. Our work enables the models to learn the concept of “usefulness” directly from data to reduce the possible issues of the human-crafted scoring function. Near-synonyms Disam"
2020.emnlp-main.312,C10-1133,0,0.0139866,". Near-synonyms Disambiguation. Unlike the language modeling task that aims at predicting the next word given the context, near-synonyms disambiguation focuses on differentiating the subtlety of the near-synonyms. Edmonds (1997) first introduced a lexical co-occurrence network with secondorder co-occurrence for near-synonym disambiguation. Edmonds also suggested a fill-in-the-blank (FITB) task, providing a benchmark for evaluating lexical choice performance on near-synonyms. Islam and Inkpen (2010) used the Google 5-gram dataset to distinguish near-synonyms using language modeling techniques. Wang and Hirst (2010) encoded words into vectors in latent semantic space and applied a machine learning model to learn the difference. Huang et al. (2017) applied BiLSTM and GMM models to learn the subtle context distribution. Recently, BERT (Devlin et al., 2018) brought a big success in nearly all the Natural Language Processing tasks. Though BERT is not designed to differentiate near-synonyms, its powerful learning capability could be used to understand the subtlety lies in the near-synonyms. In this paper, our models are all designed on top of the pre3808 trained BERT model. Natural Language Inference. Our pro"
2020.emnlp-main.312,N18-1101,0,0.0323874,"bility could be used to understand the subtlety lies in the near-synonyms. In this paper, our models are all designed on top of the pre3808 trained BERT model. Natural Language Inference. Our proposed model directly learns the difference and sentence quality by imitating the human reactions of learning material and behavior of learning from example sentences. The idea of learning from example is similar to natural language inference (NLI) task and recognizing question entailment (RQE) task. There are various NLI dataset varied in size, construction, genre, labels classes (Bowman et al., 2015; Williams et al., 2018; Khot et al., 2018; Lai et al., 2017). In the NLI task, each instance consists of two natural language text: a premise, a hypothesis, and a label indicating the relationship whether a premise entails the hypothesis. RQE, on the other hand, identifies entailment between two questions in the context of question answering. Abacha and Demner-Fushman (2016) used the definition of question entailment: “a question A entails a question B if every answer to B is also a complete or partial answer to A.” Though NLI and RQE research has acquired lots of success, to the best of our knowledge, we are the f"
2021.acl-demo.42,N16-1147,1,0.909243,"stories. This model introduces novel positional encoding methods to maintain story quality with lengthy inputs. Experiments confirm that long stories are generated without deteriorating the quality. The human evaluation further shows that Stretch-VST can provide better focus and detail when stories are prolonged compared to the state of the art. The demo video is available on Youtube1 , and the live demo can be found on website2 . 1 Introduction Visual storytelling (VIST) is an interdisciplinary task that takes a sequence of photos as input and produces a corresponding short story as output (Huang et al., 2016). Prior work explores either end-to-end or hierarchical methods for visual storytelling, but machine-generated stories still fall far short of human-generated stories. One obvious limitation is the inability to generate stories with ∗ * denotes equal contribution Demo video: https://youtu.be/-uF8IV6T1NU 2 Live demo website: https://doraemon.iis. sinica.edu.tw/acldemo/index.html 1 diverse length, especially to prolong a story. In real-world applications, when pictures accompany textual stories, the number of sentences is often much greater than the number of images. Recent visual storytelling f"
2021.acl-demo.42,P98-1013,0,0.200712,"s, we int+n t t clude (mt+1 i , r, mj ), (mi , r, mj ), and also these for two-hop relations. Furthermore, we also applied an image-grounded relation filtering, which is to ensure the predicted terms appear in the image. This prevents the model from generate irrelevant terms. Note that KG-Story is unable to expand or manipulate the size of the term set, and can only produce 6-sentence stories. Rating Prolonged Term Sequences We implement a Transformer with a masked language model objective (Devlin et al., 2019). We use spaCy 3 , Open Sesame (Swayamdipta et al., 2017), and the FrameNet parser (Baker et al., 1998) to convert the story text to term sequences. We iteratively mask one position in the overall term sequence to train the Transformer model. Then, for every possible term, we calculate the average perplexity of it with a mask at each position. The term sequence with the best (lowest) average perplexity is used in the next stage to generate stories as M P(m0 ) = F(m0 |m11 , ..., mN Nm ), − N1 m PPL(m0 ) = P(m0 ) score = , Nm 1 X PPL(mi ), Nm (2) (3) where m0 is the masked term, NM is the number of term sets, Nm is the number of terms in the sequence, F is the Transformer language model, and PPL"
2021.acl-demo.42,D16-1140,0,0.175179,"he stories. StretchVST modifies both the source and generation modules to generate variable-length stories. On the source side, we use knowledge graphs to expand the term set to represent the input image sequence. Integrating a knowledge graph into language generation is beneficial (LoBue and Yates, 2011; Bowman et al., 2015; Hayashi et al., 2020; Zhang et al., 2017; Zhou et al., 2018; Yang et al., 2019; Guan et al., 2019). On the generation side, some explore the use of relative positional encoding (Takase and Okazaki, 2019), adding embedding layers, and manipulating the beam search process (Kikuchi et al., 2016). However, these methods control only the number of words and not the number of sentences. 3 Methodology With variable-length visual sorytelling, StretchVST brings two major contributions for VIST: enriching the ingredients as desired (Sect. 3.1) and enabling story generation according to the term sequence length (Sect. 3.2). 3.1 Expanding and Scoring Term Sequences Prolonging Term Sequences Drawing from KG-Story (Hsu et al., 2020), we utilize their Transformer-based model to distill the representative terms (e.g., nouns and frames) for each image. Stretch-VST manipulates term sequence lengths"
2021.acl-demo.42,P11-2057,0,0.0405016,"2018; Gonzalez-Rico and Pineda, 2018; Kim et al., 2018; Huang et al., 2019; Jung et al., 2020; Wang et al., 2020), and the other uses adversarial training to generate more diverse stories (Chen et al., 2017; Wang et al., 2018a,b; Hu et al., 2020). However, these methods often overfit to the number of sentences in the stories. StretchVST modifies both the source and generation modules to generate variable-length stories. On the source side, we use knowledge graphs to expand the term set to represent the input image sequence. Integrating a knowledge graph into language generation is beneficial (LoBue and Yates, 2011; Bowman et al., 2015; Hayashi et al., 2020; Zhang et al., 2017; Zhou et al., 2018; Yang et al., 2019; Guan et al., 2019). On the generation side, some explore the use of relative positional encoding (Takase and Okazaki, 2019), adding embedding layers, and manipulating the beam search process (Kikuchi et al., 2016). However, these methods control only the number of words and not the number of sentences. 3 Methodology With variable-length visual sorytelling, StretchVST brings two major contributions for VIST: enriching the ingredients as desired (Sect. 3.1) and enabling story generation accordi"
2021.acl-demo.42,N19-1401,0,0.020195,"018a,b; Hu et al., 2020). However, these methods often overfit to the number of sentences in the stories. StretchVST modifies both the source and generation modules to generate variable-length stories. On the source side, we use knowledge graphs to expand the term set to represent the input image sequence. Integrating a knowledge graph into language generation is beneficial (LoBue and Yates, 2011; Bowman et al., 2015; Hayashi et al., 2020; Zhang et al., 2017; Zhou et al., 2018; Yang et al., 2019; Guan et al., 2019). On the generation side, some explore the use of relative positional encoding (Takase and Okazaki, 2019), adding embedding layers, and manipulating the beam search process (Kikuchi et al., 2016). However, these methods control only the number of words and not the number of sentences. 3 Methodology With variable-length visual sorytelling, StretchVST brings two major contributions for VIST: enriching the ingredients as desired (Sect. 3.1) and enabling story generation according to the term sequence length (Sect. 3.2). 3.1 Expanding and Scoring Term Sequences Prolonging Term Sequences Drawing from KG-Story (Hsu et al., 2020), we utilize their Transformer-based model to distill the representative te"
2021.acl-demo.42,P18-1083,0,0.268863,"hat Stretch-VST generates better stories when prolonging stories, provides more detailed information comparing 5-sentence stories, and is more robust in cohering story context when the images are incoherent. 2 Related Work Visual storytelling was proposed by Huang et al. (2016). Two lines of work explore this task: one focuses on model architecture for better story generation (Hsu et al., 2018; Gonzalez-Rico and Pineda, 2018; Kim et al., 2018; Huang et al., 2019; Jung et al., 2020; Wang et al., 2020), and the other uses adversarial training to generate more diverse stories (Chen et al., 2017; Wang et al., 2018a,b; Hu et al., 2020). However, these methods often overfit to the number of sentences in the stories. StretchVST modifies both the source and generation modules to generate variable-length stories. On the source side, we use knowledge graphs to expand the term set to represent the input image sequence. Integrating a knowledge graph into language generation is beneficial (LoBue and Yates, 2011; Bowman et al., 2015; Hayashi et al., 2020; Zhang et al., 2017; Zhou et al., 2018; Yang et al., 2019; Guan et al., 2019). On the generation side, some explore the use of relative positional encoding (Tak"
2021.acl-demo.42,P19-1193,0,0.0280923,"., 2020), and the other uses adversarial training to generate more diverse stories (Chen et al., 2017; Wang et al., 2018a,b; Hu et al., 2020). However, these methods often overfit to the number of sentences in the stories. StretchVST modifies both the source and generation modules to generate variable-length stories. On the source side, we use knowledge graphs to expand the term set to represent the input image sequence. Integrating a knowledge graph into language generation is beneficial (LoBue and Yates, 2011; Bowman et al., 2015; Hayashi et al., 2020; Zhang et al., 2017; Zhou et al., 2018; Yang et al., 2019; Guan et al., 2019). On the generation side, some explore the use of relative positional encoding (Takase and Okazaki, 2019), adding embedding layers, and manipulating the beam search process (Kikuchi et al., 2016). However, these methods control only the number of words and not the number of sentences. 3 Methodology With variable-length visual sorytelling, StretchVST brings two major contributions for VIST: enriching the ingredients as desired (Sect. 3.1) and enabling story generation according to the term sequence length (Sect. 3.2). 3.1 Expanding and Scoring Term Sequences Prolonging Term"
2021.acl-short.50,C18-1179,0,0.0227559,"Missing"
2021.acl-short.50,D14-1162,0,0.0864216,"Missing"
2021.acl-short.50,W17-0801,0,0.0318704,"th applications in diverse fields including psychology, political science, and marketing (Seyeditabari et al., 2018). Training machine learning algorithms for such applications requires large yet task-specific emotion-labeled datasets (Bostan and Klinger, 2018). Borrowing from music (Gabrielsson, 2001) and film (Tian et al., 2017), one can distinguish between two reader perspectives when labeling emotions in text: perceived emotions, which are the emotions that the reader recognizes in the text, and induced emotions, which are the emotions aroused in the reader. However, with the exception of Buechel and Hahn (2017), this distinction is mostly missing from the NLP literature, which focuses on the distinction between author and reader perspectives (Calvo and Mac Kim, 2013). The collection of perceived emotions data is considerably simpler than induced emotions data, and presently most human-annotated emotion datasets are labeled with perceived emotions (e. g., Strapparava and Mihalcea, 2008; Preo¸tiuc-Pietro et al., 2016; Hsu and Ku, 2018; Demszky et al., 2020). Induced emotions data can be collected using physiological measurements or self-reporting, but both methods are complex, expensive, unreliable an"
2021.acl-short.50,W16-4304,0,0.0264485,"ler than induced emotions data, and presently most human-annotated emotion datasets are labeled with perceived emotions (e. g., Strapparava and Mihalcea, 2008; Preo¸tiuc-Pietro et al., 2016; Hsu and Ku, 2018; Demszky et al., 2020). Induced emotions data can be collected using physiological measurements or self-reporting, but both methods are complex, expensive, unreliable and cannot scale easily. Still, having well-classified induced emotions data is of utmost importance to dialogue systems and other applications that aim to detect, predict, or elicit a particular emotional response in users. Pool and Nissim (2016) used distant supervision to detect induced emotions from Facebook posts by looking at the six available emoji reactions. Although this method is automatic, it is limited both in emotional range, since the set of reactions is small and rigid, and accuracy, because emojis are often misunderstood due to their visual ambiguity (Tigwell and Flatla, 2016). To overcome these drawbacks, we propose a new method that innovatively exploits the use of reaction GIFs in online conversations. Reaction GIFs are effective because they “display emotional responses to prior talk in text-mediated conversations”"
2021.acl-short.50,2020.acl-main.372,0,0.0212525,"tions that the reader recognizes in the text, and induced emotions, which are the emotions aroused in the reader. However, with the exception of Buechel and Hahn (2017), this distinction is mostly missing from the NLP literature, which focuses on the distinction between author and reader perspectives (Calvo and Mac Kim, 2013). The collection of perceived emotions data is considerably simpler than induced emotions data, and presently most human-annotated emotion datasets are labeled with perceived emotions (e. g., Strapparava and Mihalcea, 2008; Preo¸tiuc-Pietro et al., 2016; Hsu and Ku, 2018; Demszky et al., 2020). Induced emotions data can be collected using physiological measurements or self-reporting, but both methods are complex, expensive, unreliable and cannot scale easily. Still, having well-classified induced emotions data is of utmost importance to dialogue systems and other applications that aim to detect, predict, or elicit a particular emotional response in users. Pool and Nissim (2016) used distant supervision to detect induced emotions from Facebook posts by looking at the six available emoji reactions. Although this method is automatic, it is limited both in emotional range, since the se"
2021.acl-short.50,W16-0404,0,0.0432953,"Missing"
2021.emnlp-main.370,C08-1006,0,0.068306,"ons indicate a greater number of strong verbal leakage cue categories. Moreover, we evaluate the validity of state-of-the-art lie detection models with cross- and in-dataset testing. Results show that in both types of testing, models trained on a dataset with more strong verbal leakage cue categories—as opposed to only a greater number of strong cues—yield superior results, suggesting that verbal leakage cues are a key factor for selecting lie detection datasets. 1 Introduction age cues can be found through psycholinguistic dictionaries such as the LIWC lexicon (Pennebaker et al., 1999), LDI (Bachenko et al., 2008; Enos, 2009), and Harbingers (Niculae et al., 2015). Many NLP studies have recently collected lie detection datasets and detected lies using computational models (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020); most of these ignore traditional lie detection methods and findings, and have no follow-up studies, making it difficult to know which datasets are suitable for model training. To use machine learning approaches together with lie detection research in psychology and linguistics, and to seek a way to evaluate and select proper datasets, this study focuses on anal"
2021.emnlp-main.370,2020.findings-emnlp.329,0,0.0854019,"Missing"
2021.emnlp-main.370,N19-1423,0,0.0205778,"Missing"
2021.emnlp-main.370,P09-2078,0,0.0427141,"Missing"
2021.emnlp-main.370,P15-1159,0,0.0183372,"ge cue categories. Moreover, we evaluate the validity of state-of-the-art lie detection models with cross- and in-dataset testing. Results show that in both types of testing, models trained on a dataset with more strong verbal leakage cue categories—as opposed to only a greater number of strong cues—yield superior results, suggesting that verbal leakage cues are a key factor for selecting lie detection datasets. 1 Introduction age cues can be found through psycholinguistic dictionaries such as the LIWC lexicon (Pennebaker et al., 1999), LDI (Bachenko et al., 2008; Enos, 2009), and Harbingers (Niculae et al., 2015). Many NLP studies have recently collected lie detection datasets and detected lies using computational models (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020); most of these ignore traditional lie detection methods and findings, and have no follow-up studies, making it difficult to know which datasets are suitable for model training. To use machine learning approaches together with lie detection research in psychology and linguistics, and to seek a way to evaluate and select proper datasets, this study focuses on analyzing verbal leakage cues within; leakage cues herea"
2021.emnlp-main.370,P11-1032,0,0.147634,"Missing"
2021.emnlp-main.370,D15-1133,0,0.048347,"Missing"
2021.emnlp-main.370,2020.acl-main.353,0,0.163955,"ained on a dataset with more strong verbal leakage cue categories—as opposed to only a greater number of strong cues—yield superior results, suggesting that verbal leakage cues are a key factor for selecting lie detection datasets. 1 Introduction age cues can be found through psycholinguistic dictionaries such as the LIWC lexicon (Pennebaker et al., 1999), LDI (Bachenko et al., 2008; Enos, 2009), and Harbingers (Niculae et al., 2015). Many NLP studies have recently collected lie detection datasets and detected lies using computational models (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020); most of these ignore traditional lie detection methods and findings, and have no follow-up studies, making it difficult to know which datasets are suitable for model training. To use machine learning approaches together with lie detection research in psychology and linguistics, and to seek a way to evaluate and select proper datasets, this study focuses on analyzing verbal leakage cues within; leakage cues hereafter indicate verbal leakage cues. We study leakage cues in terms of the data collection method and model performance. Seven lie detection datasets are adopted for experiments. We ana"
2021.emnlp-main.370,N19-1175,0,0.0334033,"Missing"
2021.emnlp-main.370,P17-2067,0,0.0343738,"Missing"
2021.emnlp-main.370,I17-1026,0,0.0794506,"Missing"
2021.findings-acl.390,P98-1013,0,0.719775,"tive relations between nouns, we prepare Visual Genome graph Gvg and VIST graph Gvist . These graphs contain interlinked real-world objects and terms, displaying visual and storytelling interaction. Table 1 summarizes the statistic of each graph. Visual Genome Graph Gvg describes pairwise relationships between objects in an image, describing visual interactions. No prepositional relations are included; only verb relations are preserved. All relations are converted into semantic verb frames using Open-SESAME (Swayamdipta et al., 2017), in which the semantic frames were pre-defined in FrameNet (Baker et al., 1998). VIST Graph we propose Gvist to collect the storytelling interactions. We develop this novel story knowledge graph by converting references in the VIST training and validation datasets (Huang et al., 2016) to graphical data. Following the reading direction, in each reference, we extract nouns and semantic verb frames using SpaCy1 and OpenSESAME to obtain noun–verb–noun (NVN) tuples. Using nouns and semantic verb frames as nodes and edges, these are collectively assembled into a golden storyline. For example, for “first pitcher thrown for the game” in Figure 2, we extract pitcher, game, and Ca"
2021.findings-acl.390,N19-1031,1,0.825713,"next sentence or termination point of a story, and a verb frame token empty frame to interlink two nouns when a semantic frame is absent. To conclude, all of the golden storylines are assembled into Gvist . 4445 1 SpaCy: https://spacy.io/ Gvg Gvist Gvg+vist Nodes 3,323 2,048 4,158 Relations 564 531 880 Links 22.31 11.75 22.78 0.21 Self_motion.owner dog 4.1 0.34 Stage 1: Story Plotting Storyline Predictor In Stage 1, PR-VIST uses a storyline predictor to find what it deems the best path in the story graph as the storyline and then pass this to Stage 2. For the storyline predictor, we use UHop (Chen et al., 2019), a non-exhaustive relation extraction framework. A single hop is defined as searching from one entity to another entity by a single relation. UHop performs multiple single-hop classifications consecutively in the graph to find the path representing the storyline, that is, a path that consists of a sequence of nouns and verb frames. Single-hop classification can described as Equation 1 and Figure 3. In step i, at the current head entity hi , the model is given a list of candidate relations ri ∈ Ri and tail entities ti ∈ Ti . Each ri is in [verb.ti ] or [verb.noun] format, containing informatio"
2021.findings-acl.390,D14-1162,0,0.10013,"Missing"
2021.findings-acl.390,2020.acl-main.704,0,0.0283816,"rkers for each story, as well as total votes for each model’s best and worst aspects. Non-Classic Automatic Evaluation: BLEURT, voc-d, and MLTD Many VIST studies have shown that classic automatic evaluation scores like BLEU and METEOR correlate poorly with human judgment (Hsu et al., 2020; Hu et al., 2019; Wang et al., 2020; Li et al., 2020; Yang et al., 2019; Hsu et al., 2019; Wang et al., 2018a; Modi and Parde, 2019). These n-gram matching metrics fail to account for the semantic similarity to the reference stories and lexical richness in the generated stories. Therefore, we adopted BLEURT (Sellam et al., 2020), a state-of-the-art BERT-based evaluation 4448 AREL : avg rank #3.6 the parade started with a lot of people in the parade. there were a lot of people there. there was a lot of people there. there were a lot of people there. there were a lot of cars in the parade. GLAC : avg rank #2.4 the marathon was about to begin. there were many people there. it was a great day. everyone was very excited. they were all very fast. KG-Story : avg rank #2.2 the parade started with a beautiful day. many people showed up. there were runners. everyone was watching the parade. one woman stood in a car to get ever"
2021.findings-acl.390,P18-1083,0,0.416492,"n (Huang et al., 2016; Kim et al., 2018), treating the task as a straightforward extension of image captioning. Recent works have begun to use relations between entities to improve visual storytelling, but often narrow in a particular subset of relations, such as relations between elements within the same image (Yang et al., 2019), relations between two adjacent images (Hsu et al., 2020), or relations between scenes (Wang et al., 2020). The full potential of rich real-world knowledge and intra-image relations have yet to be fully utilized. As for reevaluation, recent work uses reward systems (Wang et al., 2018b; Hu et al., 2019) or estimated topic coherence (Wang et al., 2019) to automatically assess the output story and guide the generation process. However, these approaches are often optimized towards predefined aspects such as image relevancy or topic coherence, which do not necessarily lead to engaging stories from a human perspective. In the cognitive process of human writing, the writer’s judgment is critical, and visual storytelling models could benefit by considering human ratings. This paper introduces PR-VIST, a novel visual storytelling framework that constructs a graph and captures the"
2021.findings-acl.390,W19-1805,0,0.0181399,"y. These aspects are provided by Huang et al. (2016): they include focus, coherence, shareability, humanness, grounding, and detail. We calculated the average rank and the majority rank among five workers for each story, as well as total votes for each model’s best and worst aspects. Non-Classic Automatic Evaluation: BLEURT, voc-d, and MLTD Many VIST studies have shown that classic automatic evaluation scores like BLEU and METEOR correlate poorly with human judgment (Hsu et al., 2020; Hu et al., 2019; Wang et al., 2020; Li et al., 2020; Yang et al., 2019; Hsu et al., 2019; Wang et al., 2018a; Modi and Parde, 2019). These n-gram matching metrics fail to account for the semantic similarity to the reference stories and lexical richness in the generated stories. Therefore, we adopted BLEURT (Sellam et al., 2020), a state-of-the-art BERT-based evaluation 4448 AREL : avg rank #3.6 the parade started with a lot of people in the parade. there were a lot of people there. there was a lot of people there. there were a lot of people there. there were a lot of cars in the parade. GLAC : avg rank #2.4 the marathon was about to begin. there were many people there. it was a great day. everyone was very excited. they w"
2021.findings-acl.390,N16-1098,0,0.0235247,"s 1 to L, the model is optimized using sentencelevel loss. In step L+1, all sentences are generated, and the model is optimized using story-level loss. generation model to focus on stories preferred by humans. The reward directly multiplies the storylevel loss as reward Jstory (θ, D) 6 ( MLE Jstory = MLE R(s)Jstory if epoch ≤ 30 . (7) if 30 &lt; epoch ≤ 60 Experimental Results 6.1 Data Setups We used four datasets in this paper: the VIST dataset, Visual Genome, ROCStories, and MTurk human ranking data. The VIST dataset and Visual Genome are used to construct the knowledge graphs, and ROCStories (Mostafazadeh et al., 2016) is a large quantity of pure textual stories used for pre-training the story generator. The VIST dataset is also used in story plotting to train the storyline predictor and in story reworking to finetune the story generator. Notably, we also collected MTurk human ranking data to train the story evaluator. We used the ranking results from KGStory4 (Hsu et al., 2020). This data contains two experiments, each of which ontains 500 distinct photo sequences. A photo sequence contains a set of machine-generated stories ranked by 5 MTurk workers. Thus we have 5000 rankings from MTurk workers. Specific"
2021.naacl-main.295,2020.acl-tutorials.2,0,0.0429722,"mmunity. This is manifested in new ethics-focused workshops, ethics conference panels and relevant updates to peer review forms. While ethics in NLP has multiple aspects, most recent attention focuses on pressing issues related to the societal impact of NLP. These include discrimination, exclusion, over-generalization, bias, ∗ Corresponding author: shmueli@iis.sinica.edu.tw and fairness (Hovy and Spruit, 2016; Leidner and Plachouras, 2017). Other works are concerned with the ethical implications of NLP shared tasks (Parra Escartín et al., 2017), and introducing ethics into the NLP curriculum (Bender et al., 2020). A substantial amount of NLP research now takes advantage of crowdworkers — workers on crowdsourcing platforms such as Amazon Mechanical Turk (known also as AMT or MTurk), Figure Eight1 , Appen, Upwork, Prolific, Hybrid, Tencent Questionnaire, and Baidu Zhongbao, as well as internal crowdsourcing platforms in companies such as Microsoft and Apple. Workers are recruited to label, evaluate, and produce data. In the pre-internet era, such tasks (e.g. part-of-speech (POS) tagging) were done by hiring expert annotators or linguistics students. However, these are now mostly replaced by crowdworkers"
2021.naacl-main.295,D16-1150,0,0.0373438,"Missing"
2021.naacl-main.295,J11-2010,0,0.248833,"linguistics students. However, these are now mostly replaced by crowdworkers due to lower costs, convenience, speed, and scalability. Overall, the general consensus in the literature is that as long as the pay to the crowdworkers is “fair” (minimum hourly wage or above), there are no further ethical concerns, and there is no need for approval by an Institutional Review Board2 (with some exceptions). For example, Hovy and Spruit (2016) mention that “[w]ork on existing corpora is unlikely to raise any flags that would require an IRB approval”, with a footnote that there are “a few exceptions”. Fort et al. (2011) mention that only “[a] small number of universities have insisted on institutional review board approval for MTurk experiments”. As another example, NLP students are being taught that “paid labeling does not require IRB approval” since “[i]t’s not an experiment 1 Previously CrowdFlower; acquired by Appen in 2019. Institutional Review Boards (IRBs) are university-level, multi-stakeholder committees that review the methods proposed for research involving human subjects to ensure that they conform to ethical principles. IRBs are also known by various other names, such as Research Ethics Boards ("
2021.naacl-main.295,W17-1612,0,0.0774723,"i) is met. However, Final Rule does not expand on what constitutes information about the individual. According to University of Washington, Office of Research (2020), for example, about whom means that the “data or information relates to the person. Asking what [crowdworkers] think about something, how they do something, or similar questions usually pertain to the individuals. This is in contrast to questions about factual information not related to the person.” Whether the information obtained in an NLP task is about the worker can initially seem like an easy-to-answer question. For example, Benton et al. (2017) write: From this definition it is evident that rather than a concrete research behavior on part of the NLP researcher, it is the purpose of the research behavior that classifies said behavior as research under the Final Rule. In other words, all categories of crowdsourced tasks summarized in Section 2, i.e., labeling, evaluation, and production, may be considered part of research so long as the intended outcome is to create generalizable knowledge. Typically, this encompasses academic settings where research behavior takes place (course assignments by students being a prominent exception), bu"
2021.naacl-main.295,N15-1072,0,0.0299788,"discuss a vulnerability that can expose the identity of an Amazon Mechanical Turk worker using their worker ID — a string of 14 letters and digits — because the same worker ID is used also as the identifier of the crowdworker’s account on other Amazon assets and properties. As a result, a Google search for the worker ID can lead to personal information such as product reviews written by the crowdworker on Amazon.com, which in turn can disclose the worker’s identity. Researchers might be unaware of these issues when they make worker IDs publicly available in papers or in datasets. For example, Gao et al. (2015) rank their crowdworkers using MTurk worker IDs in one of the figures. Moreover, breaches of privacy can also occur unintentionally. For example, workers on MTurk are provided with an option to contact the researcher. In this case, their email address will be sent to the researcher, who is inadvertently exposed to further identifiable private information (IPI). We maintain that the anonymity of crowdworkers cannot be automatically assumed or guaranteed, as this is not a premise of the crowdsourcing platform. 7 Ways Forward The use of crowdworkers is growing within the NLP community, but the et"
2021.naacl-main.295,2020.emnlp-main.622,0,0.0124467,"hers. We outline some of the specific risks and harms that might befall NLP task crowdworkers in light of these principles. While the list is not comprehensive, it can serve as a starting point to be used by researchers when planning their crowdsourced task, as well as by reviewers examining the ethical implications of a manuscript or research proposal. 6.1 Inducing Psychological Harms NLP researchers are increasingly cognizant that texts can potentially harm readers, as evident by trigger warnings they add to their own papers (e.g., Sap et al., 2020; Nangia et al., 2020; Sharma et al., 2020; Han and Tsvetkov, 2020). Moreover, researchers have long known that annotation work may be psychologically harmful. The Linguistic Data Consortium (LDC), for example, arranged stress-relieving activities for annotators of broadcast news data, following reports of “negative psychological impact” such as intense irritation, overwhelmed feelings, and task-related nightmares (Strassel et al., 2000). Although literature on the emotional toll on crowdworkers is still scant (Huws, Class assignments may become subject 2015), there is growing literature on the psychoto this policy... if the faculty member or logical cost of"
2021.naacl-main.295,P16-2096,0,0.0363695,"ata, smartphones, AI, and along with these, a plethora of complex ethical challenges. As a result, there is growing concern and discussion on ethics within the research community at large, including the NLP community. This is manifested in new ethics-focused workshops, ethics conference panels and relevant updates to peer review forms. While ethics in NLP has multiple aspects, most recent attention focuses on pressing issues related to the societal impact of NLP. These include discrimination, exclusion, over-generalization, bias, ∗ Corresponding author: shmueli@iis.sinica.edu.tw and fairness (Hovy and Spruit, 2016; Leidner and Plachouras, 2017). Other works are concerned with the ethical implications of NLP shared tasks (Parra Escartín et al., 2017), and introducing ethics into the NLP curriculum (Bender et al., 2020). A substantial amount of NLP research now takes advantage of crowdworkers — workers on crowdsourcing platforms such as Amazon Mechanical Turk (known also as AMT or MTurk), Figure Eight1 , Appen, Upwork, Prolific, Hybrid, Tencent Questionnaire, and Baidu Zhongbao, as well as internal crowdsourcing platforms in companies such as Microsoft and Apple. Workers are recruited to label, evaluate,"
2021.naacl-main.295,C14-1117,0,0.0240602,"commend fied crowdsourced tasks: a possibility of addiction increasing awareness of the potential ethical imcaused by dopamine release following a reward given during the gamified task. Gamification tech- plications of crowdsourced NLP tasks. As NLP niques can be added to data labeling, evaluation, researchers are now encouraged to add an “ethical and production. Indeed, some NLP work is us- considerations” section to their papers (NAACL, ing gamification, mostly for data collection (e.g., 2020), they should also be encouraged to carefully weigh potential benefits against risks related to the Kumaran et al., 2014; Ogawa et al., 2020; Öhman crowdsourced task. et al., 2018). We also propose increasing awareness by dissemMoreover, the crowdsourcing platform may add inating relevant knowledge and information. An elements of gamification over which the researcher has no control. For example, MTurk recently in- educational ethics resource created using a community effort could serve as a beneficial first step. troduced a “Daily Goals Dashboard”, where the worker can set game-like “HITs Goal” and “Re- Such a resource can include guidelines, checklists, and case studies that are specific to the ethical chalwa"
2021.naacl-main.295,2020.emnlp-main.154,0,0.0206891,"d Justice — to guide the action of researchers. We outline some of the specific risks and harms that might befall NLP task crowdworkers in light of these principles. While the list is not comprehensive, it can serve as a starting point to be used by researchers when planning their crowdsourced task, as well as by reviewers examining the ethical implications of a manuscript or research proposal. 6.1 Inducing Psychological Harms NLP researchers are increasingly cognizant that texts can potentially harm readers, as evident by trigger warnings they add to their own papers (e.g., Sap et al., 2020; Nangia et al., 2020; Sharma et al., 2020; Han and Tsvetkov, 2020). Moreover, researchers have long known that annotation work may be psychologically harmful. The Linguistic Data Consortium (LDC), for example, arranged stress-relieving activities for annotators of broadcast news data, following reports of “negative psychological impact” such as intense irritation, overwhelmed feelings, and task-related nightmares (Strassel et al., 2000). Although literature on the emotional toll on crowdworkers is still scant (Huws, Class assignments may become subject 2015), there is growing literature on the psychoto this polic"
2021.naacl-main.295,N16-1070,0,0.0271741,"ften ask workers to generate data, which in turn is used to train a model; they then use workers to evaluate the model’s performance. 2.2 Surveys and Gamification Although not common, some papers also collect personal information from workers. For example, Yang et al. (2015) and Ding and Pan (2016) conduct personality surveys among its crowdworkers. Pérez-Rosas and Mihalcea (2015) collect demographic data from the workers which included “their gender, age, country of origin, and education level”. Finally, we also found a few papers that add elements of gaming to their crowdsourced tasks, e.g. Niculae and Danescu-Niculescu-Mizil (2016) and Urbanek et al. (2019). 3 The Rules and Institutions of Research Ethics Given the increasing use of crowdsourced NLP tasks, how can researchers ensure ethical concerns are reasonably addressed? Should a researcher make a judgement call and decide which tasks pose a risk of harm to the worker, and which are benign? To answer such questions, we will first explore the existing ethical framework used by researchers in the biomedical, social, and behavioral sciences. 3.1 The Genesis of Modern Research Ethics The roots of contemporary research ethics originate in the 19th century, when researche"
2021.naacl-main.295,2020.lrec-1.876,0,0.0395648,"ced tasks: a possibility of addiction increasing awareness of the potential ethical imcaused by dopamine release following a reward given during the gamified task. Gamification tech- plications of crowdsourced NLP tasks. As NLP niques can be added to data labeling, evaluation, researchers are now encouraged to add an “ethical and production. Indeed, some NLP work is us- considerations” section to their papers (NAACL, ing gamification, mostly for data collection (e.g., 2020), they should also be encouraged to carefully weigh potential benefits against risks related to the Kumaran et al., 2014; Ogawa et al., 2020; Öhman crowdsourced task. et al., 2018). We also propose increasing awareness by dissemMoreover, the crowdsourcing platform may add inating relevant knowledge and information. An elements of gamification over which the researcher has no control. For example, MTurk recently in- educational ethics resource created using a community effort could serve as a beneficial first step. troduced a “Daily Goals Dashboard”, where the worker can set game-like “HITs Goal” and “Re- Such a resource can include guidelines, checklists, and case studies that are specific to the ethical chalward Goal”, as shown i"
2021.naacl-main.295,W18-6205,0,0.0540902,"Missing"
2021.naacl-main.295,W17-1604,0,0.0678956,"nd along with these, a plethora of complex ethical challenges. As a result, there is growing concern and discussion on ethics within the research community at large, including the NLP community. This is manifested in new ethics-focused workshops, ethics conference panels and relevant updates to peer review forms. While ethics in NLP has multiple aspects, most recent attention focuses on pressing issues related to the societal impact of NLP. These include discrimination, exclusion, over-generalization, bias, ∗ Corresponding author: shmueli@iis.sinica.edu.tw and fairness (Hovy and Spruit, 2016; Leidner and Plachouras, 2017). Other works are concerned with the ethical implications of NLP shared tasks (Parra Escartín et al., 2017), and introducing ethics into the NLP curriculum (Bender et al., 2020). A substantial amount of NLP research now takes advantage of crowdworkers — workers on crowdsourcing platforms such as Amazon Mechanical Turk (known also as AMT or MTurk), Figure Eight1 , Appen, Upwork, Prolific, Hybrid, Tencent Questionnaire, and Baidu Zhongbao, as well as internal crowdsourcing platforms in companies such as Microsoft and Apple. Workers are recruited to label, evaluate, and produce data. In the pre-i"
2021.naacl-main.295,W17-1608,0,0.0265211,"n on ethics within the research community at large, including the NLP community. This is manifested in new ethics-focused workshops, ethics conference panels and relevant updates to peer review forms. While ethics in NLP has multiple aspects, most recent attention focuses on pressing issues related to the societal impact of NLP. These include discrimination, exclusion, over-generalization, bias, ∗ Corresponding author: shmueli@iis.sinica.edu.tw and fairness (Hovy and Spruit, 2016; Leidner and Plachouras, 2017). Other works are concerned with the ethical implications of NLP shared tasks (Parra Escartín et al., 2017), and introducing ethics into the NLP curriculum (Bender et al., 2020). A substantial amount of NLP research now takes advantage of crowdworkers — workers on crowdsourcing platforms such as Amazon Mechanical Turk (known also as AMT or MTurk), Figure Eight1 , Appen, Upwork, Prolific, Hybrid, Tencent Questionnaire, and Baidu Zhongbao, as well as internal crowdsourcing platforms in companies such as Microsoft and Apple. Workers are recruited to label, evaluate, and produce data. In the pre-internet era, such tasks (e.g. part-of-speech (POS) tagging) were done by hiring expert annotators or lingui"
2021.naacl-main.295,D15-1133,0,0.0378142,"Missing"
2021.naacl-main.295,2020.acl-main.486,0,0.0141414,"s, Beneficence, and Justice — to guide the action of researchers. We outline some of the specific risks and harms that might befall NLP task crowdworkers in light of these principles. While the list is not comprehensive, it can serve as a starting point to be used by researchers when planning their crowdsourced task, as well as by reviewers examining the ethical implications of a manuscript or research proposal. 6.1 Inducing Psychological Harms NLP researchers are increasingly cognizant that texts can potentially harm readers, as evident by trigger warnings they add to their own papers (e.g., Sap et al., 2020; Nangia et al., 2020; Sharma et al., 2020; Han and Tsvetkov, 2020). Moreover, researchers have long known that annotation work may be psychologically harmful. The Linguistic Data Consortium (LDC), for example, arranged stress-relieving activities for annotators of broadcast news data, following reports of “negative psychological impact” such as intense irritation, overwhelmed feelings, and task-related nightmares (Strassel et al., 2000). Although literature on the emotional toll on crowdworkers is still scant (Huws, Class assignments may become subject 2015), there is growing literature on th"
2021.naacl-main.295,2020.acl-main.419,0,0.0151972,"ted to emotion classification (which might reveal a worker’s personality or mood), composing questions and answers (which point to areas of interest and cultural background), or identifying hate speech (which can indicate political orientation). Second, platforms might automatically provide information that can be considered by some to be about the individual. Even in the most benign and “objective” tasks such as POS tagging, MTurk supplies researchers with information on the amount of time taken to complete each task. This information is sometimes collected and used by NLP researchers (e.g., Sen et al., 2020). In summary, we have shown that in an academic context, NLP crowdsourcing tasks are research, but that the categorization of crowdworkers as human subjects can, in some cases, be a gray area that is open to interpretation. The Final Rule was designed to address ethical issues in medical research, and later in behavioral sciences; lawmakers and experts involved did not anticipate its use in new domains such as crowdsourcing. Therefore, its application to online data collection, and crowdsourcing in particular, can be ambiguous and unsatisfactory.8 Thus, while in some cases the protections and"
2021.naacl-main.295,2020.emnlp-main.425,0,0.0138893,"the action of researchers. We outline some of the specific risks and harms that might befall NLP task crowdworkers in light of these principles. While the list is not comprehensive, it can serve as a starting point to be used by researchers when planning their crowdsourced task, as well as by reviewers examining the ethical implications of a manuscript or research proposal. 6.1 Inducing Psychological Harms NLP researchers are increasingly cognizant that texts can potentially harm readers, as evident by trigger warnings they add to their own papers (e.g., Sap et al., 2020; Nangia et al., 2020; Sharma et al., 2020; Han and Tsvetkov, 2020). Moreover, researchers have long known that annotation work may be psychologically harmful. The Linguistic Data Consortium (LDC), for example, arranged stress-relieving activities for annotators of broadcast news data, following reports of “negative psychological impact” such as intense irritation, overwhelmed feelings, and task-related nightmares (Strassel et al., 2000). Although literature on the emotional toll on crowdworkers is still scant (Huws, Class assignments may become subject 2015), there is growing literature on the psychoto this policy... if the faculty m"
2021.naacl-main.295,strassel-etal-2000-quality,0,0.271957,"logical Harms NLP researchers are increasingly cognizant that texts can potentially harm readers, as evident by trigger warnings they add to their own papers (e.g., Sap et al., 2020; Nangia et al., 2020; Sharma et al., 2020; Han and Tsvetkov, 2020). Moreover, researchers have long known that annotation work may be psychologically harmful. The Linguistic Data Consortium (LDC), for example, arranged stress-relieving activities for annotators of broadcast news data, following reports of “negative psychological impact” such as intense irritation, overwhelmed feelings, and task-related nightmares (Strassel et al., 2000). Although literature on the emotional toll on crowdworkers is still scant (Huws, Class assignments may become subject 2015), there is growing literature on the psychoto this policy... if the faculty member or logical cost of work done by commercial content the students change their plans... applicamoderators (Steiger et al., 2021). Crowdworkers tion to the IRB for permission to use the deserve similar consideration: while NLP tasks data is required. (University of Michigan, can be as benign as the POS tagging of a children’s Research Ethics and Compliance, 2021) poem, they may also involve ex"
2021.naacl-main.295,D19-1062,0,0.0287615,"n is used to train a model; they then use workers to evaluate the model’s performance. 2.2 Surveys and Gamification Although not common, some papers also collect personal information from workers. For example, Yang et al. (2015) and Ding and Pan (2016) conduct personality surveys among its crowdworkers. Pérez-Rosas and Mihalcea (2015) collect demographic data from the workers which included “their gender, age, country of origin, and education level”. Finally, we also found a few papers that add elements of gaming to their crowdsourced tasks, e.g. Niculae and Danescu-Niculescu-Mizil (2016) and Urbanek et al. (2019). 3 The Rules and Institutions of Research Ethics Given the increasing use of crowdsourced NLP tasks, how can researchers ensure ethical concerns are reasonably addressed? Should a researcher make a judgement call and decide which tasks pose a risk of harm to the worker, and which are benign? To answer such questions, we will first explore the existing ethical framework used by researchers in the biomedical, social, and behavioral sciences. 3.1 The Genesis of Modern Research Ethics The roots of contemporary research ethics originate in the 19th century, when researchers made unparalleled disco"
2021.naacl-main.295,D15-1009,0,0.0312324,"Missing"
C16-1154,C08-2004,0,0.0745756,"Missing"
C16-1154,P11-1151,0,0.0213771,"Missing"
C16-1154,P14-2009,0,0.00556505,"Missing"
C16-1154,P13-2142,0,0.219994,"e forum5 discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table 1. We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng’s work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds (Hasan and Ng, 2013b; Hasan and Ng, 2014). The FBFans dataset has more integrated functions than the CreateDebate dataset; thus our model can utilize all linguistic and extra-linguistic features. For the CreateDebate dataset, on the other hand, the like and comment features are not available (as there is a stance label for each reply, replies are evaluated as posts as other previous work) but we still implemented our model using the content, author, and topic information. 4.2 Settings In the UTCNN training process, cross-entropy was used as the loss function and AdaGrad as the optimizer. For FBFans dataset, we l"
C16-1154,I13-1191,0,0.521467,"e forum5 discussing four topics: abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR). The posts are annotated as for (F) and against (A). Replies to posts in this dataset are also labeled with stance and hence use the same data format as posts. The labeling results are shown in the right-hand side of Table 1. We observe that the dataset is more balanced than the FBFans dataset. In addition, there are 977 unique users in the dataset. To compare with Hasan and Ng’s work, we conducted five-fold cross-validation and present the annotation results as the average number of all folds (Hasan and Ng, 2013b; Hasan and Ng, 2014). The FBFans dataset has more integrated functions than the CreateDebate dataset; thus our model can utilize all linguistic and extra-linguistic features. For the CreateDebate dataset, on the other hand, the like and comment features are not available (as there is a stance label for each reply, replies are evaluated as posts as other previous work) but we still implemented our model using the content, author, and topic information. 4.2 Settings In the UTCNN training process, cross-entropy was used as the loss function and AdaGrad as the optimizer. For FBFans dataset, we l"
C16-1154,D14-1083,0,0.174559,"opic, a user’s likes and dislikes provide clues for stance labeling. From a user point of view, users with positive attitudes toward the issue leave positive comments on the posts with praise or even just the post’s content; from a post point of view, positive posts attract users who hold positive stances. We also investigate the influence of topics: different topics are associated with different stance labeling tendencies and word usage. For example we discuss women’s rights and unwanted babies on the topic of abortion, but we criticize medicine usage or crime when on the topic of marijuana (Hasan and Ng, 2014). Even for posts on a specific topic like nuclear power, a variety of arguments are raised: green energy, radiation, air pollution, and so on. As for comments, we treat them as additional text information. The arguments in the comments and the commenters (the users who leave the comments) provide hints on the post’s content and further facilitate stance classification. In this paper, we propose the user-topic-comment neural network (UTCNN), a deep learning model that utilizes user, topic, and comment information. We attempt to learn user and topic representations which encode user interactions"
C16-1154,N15-1011,0,0.00543691,"Missing"
C16-1154,P14-1062,0,0.0147286,"Missing"
C16-1154,D14-1181,0,0.174236,"itigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform. 1 Introduction Deep neural networks have been widely used in text classification and have achieved promising results (Lai et al., 2015; Ren et al., 2016; Huang et al., 2016). Most focus on content information and use models such as convolutional neural networks (CNN) (Kim, 2014) or recursive neural networks (Socher et al., 2013). However, for user-generated posts on social media like Facebook or Twitter, there is more information that should not be ignored. On social media platforms, a user can act either as the author of a post or as a reader who expresses his or her comments about the post. In this paper, we classify posts taking into account post authorship, likes, topics, and comments. In particular, users and their “likes” hold strong potential for text mining. For example, given a set of posts that are related to a specific topic, a user’s likes and dislikes pr"
C16-1154,D14-1162,0,0.117024,"the CreateDebate dataset; thus our model can utilize all linguistic and extra-linguistic features. For the CreateDebate dataset, on the other hand, the like and comment features are not available (as there is a stance label for each reply, replies are evaluated as posts as other previous work) but we still implemented our model using the content, author, and topic information. 4.2 Settings In the UTCNN training process, cross-entropy was used as the loss function and AdaGrad as the optimizer. For FBFans dataset, we learned the 50-dimensional word embeddings on the whole dataset using GloVe6 (Pennington et al., 2014) to capture the word semantics; for CreateDebate dataset we used the publicly available English 50-dimensional word embeddings, pre-trained also using GloVe. These word embeddings were fixed in the training process. The learning rate was set to 0.03. All user and topic embeddings were randomly initialized in the range of [-0.1 0.1]. Matrix embeddings for users and topics were sized at 250 (5 × 50); vector embeddings for users and topics were set to length 10. We applied the LDA topic model (Blei et al., 2003) on the FBFans dataset to determine the latent topics with which to build topic embedd"
C16-1154,D12-1110,0,0.115091,"Missing"
C16-1154,D13-1170,0,0.00388535,"lass without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform. 1 Introduction Deep neural networks have been widely used in text classification and have achieved promising results (Lai et al., 2015; Ren et al., 2016; Huang et al., 2016). Most focus on content information and use models such as convolutional neural networks (CNN) (Kim, 2014) or recursive neural networks (Socher et al., 2013). However, for user-generated posts on social media like Facebook or Twitter, there is more information that should not be ignored. On social media platforms, a user can act either as the author of a post or as a reader who expresses his or her comments about the post. In this paper, we classify posts taking into account post authorship, likes, topics, and comments. In particular, users and their “likes” hold strong potential for text mining. For example, given a set of posts that are related to a specific topic, a user’s likes and dislikes provide clues for stance labeling. From a user point"
C16-1154,P15-1012,0,0.698107,"nservative and thus are likely to oppose gay rights. For work focusing on online forum text, since posts are linked through user replies, sequential labeling methods have been used to model relationships between posts. For example, Hasan and Ng use hidden Markov models (HMMs) to model dependent relationships to the preceding post (Hasan and Ng, 2013b); Burfoot et al. use iterative classification to repeatedly generate new estimates based on the current state of knowledge (Burfoot et al., 2011); Sridhar et al. use probabilistic soft logic (PSL) to model reply links via collaborative filtering (Sridhar et al., 2015). In the Facebook dataset we study, we use comments instead of reply links. However, as the ultimate goal in this paper is predicting not comment stance but post stance, we treat comments as extra information for use in predicting post stance. 2.2 Deep Learning on Extra-Linguistic Features In recent years neural network models have been applied to document sentiment classification (Socher et al., 2012; Socher et al., 2013; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Huang et al., 2016). Text features can be used in deep networks to capture text semantics or sentiment. For example, Dong"
C16-1154,P15-1098,0,0.0281929,"Missing"
C16-1154,W06-1639,0,0.321783,"Missing"
C16-1154,N12-1072,0,0.175125,"Missing"
C16-2030,D09-1131,1,0.778468,"Push. in LJ40k. Forty binary-class classifiers (one emotion each) were developed by using LibSVM (Fan et al., 2008) with a radial basis function kernel. We chose to develop 40 classifiers instead of one 40-class classifier to a) better compare our results to Yang and Liu (2013), b) achieve better performance, and c) extend EmotionPush to a multi-labeling system in the future. To form a balanced training set for each emotion, we randomly selected 800 posts from LJ40K as positive examples and 800 posts of the other 39 emotions as negative examples. Aware of various features proposed for affect (Ku et al., 2009; Ku et al., 2011; Balahur et al., 2014; Poria et al., 2014; Tang et al., 2014; Xu et al., 2015), we used a 300-dimension word vectors trained on Google News (Mikolov et al., 2013) (https://code.google.com/archive/p/word2vec/) to represent each post. The model’s parameters were tuned via a 10-fold cross-validation process. The evaluations were performed on the held-out testing set that contains 8,000 posts (200 posts for each emotion). The AUC, the area under the receiver operating characteristic curve, was calculated. For the 40 emotions, our classifiers achieved an average AUC of 0.6788, whi"
C16-2030,I11-1039,1,0.85284,"orty binary-class classifiers (one emotion each) were developed by using LibSVM (Fan et al., 2008) with a radial basis function kernel. We chose to develop 40 classifiers instead of one 40-class classifier to a) better compare our results to Yang and Liu (2013), b) achieve better performance, and c) extend EmotionPush to a multi-labeling system in the future. To form a balanced training set for each emotion, we randomly selected 800 posts from LJ40K as positive examples and 800 posts of the other 39 emotions as negative examples. Aware of various features proposed for affect (Ku et al., 2009; Ku et al., 2011; Balahur et al., 2014; Poria et al., 2014; Tang et al., 2014; Xu et al., 2015), we used a 300-dimension word vectors trained on Google News (Mikolov et al., 2013) (https://code.google.com/archive/p/word2vec/) to represent each post. The model’s parameters were tuned via a 10-fold cross-validation process. The evaluations were performed on the held-out testing set that contains 8,000 posts (200 posts for each emotion). The AUC, the area under the receiver operating characteristic curve, was calculated. For the 40 emotions, our classifiers achieved an average AUC of 0.6788, which was comparable"
C16-2030,S16-1001,0,0.0166799,"cation with a colored icon on his/her mobile device. We developed the EmotionPush client as an Android application1 , specifically for Facebook’s Messenger (https://www.messenger.com/). The screen shot and user scenario of the app are shown in Figure 1. The EmotionPush server was implemented as a stand-alone web server powered by pre-trained emotion classification models. Visualizing Emotions EmotionPush uses 7 colors to represent 7 emotions, as shown in Figure 2. This schema was designed as follows: First, we focused on emotions commonly connected with life events, unlike benchmarks such as (Nakov et al., 2016) which typically focus on general social media data. To simplify the mapping between emotions and text, we also decided to apply a categorical representation (e.g. Anger, Joy, etc.) (Klein et al., 2002) of emotions instead of a dimensional representation (valence, arousal) (S´anchez et al., 2006). Second, we utilized the emotion categories and data provided in LiveJournal (http://www.livejournal.com/). LiveJournal is a website where users post what they feel and tag each post with a corresponding emotion. The LJ40k corpus (Leshed and Kaye, 2006), a dataset that contains 1,000 blog posts for ea"
C16-2030,P14-1146,0,0.0152341,"oped by using LibSVM (Fan et al., 2008) with a radial basis function kernel. We chose to develop 40 classifiers instead of one 40-class classifier to a) better compare our results to Yang and Liu (2013), b) achieve better performance, and c) extend EmotionPush to a multi-labeling system in the future. To form a balanced training set for each emotion, we randomly selected 800 posts from LJ40K as positive examples and 800 posts of the other 39 emotions as negative examples. Aware of various features proposed for affect (Ku et al., 2009; Ku et al., 2011; Balahur et al., 2014; Poria et al., 2014; Tang et al., 2014; Xu et al., 2015), we used a 300-dimension word vectors trained on Google News (Mikolov et al., 2013) (https://code.google.com/archive/p/word2vec/) to represent each post. The model’s parameters were tuned via a 10-fold cross-validation process. The evaluations were performed on the held-out testing set that contains 8,000 posts (200 posts for each emotion). The AUC, the area under the receiver operating characteristic curve, was calculated. For the 40 emotions, our classifiers achieved an average AUC of 0.6788, which was comparable to the state-of-the-art performance, 0.6851, reported by (Ya"
C16-2057,C16-1154,1,0.908906,"tic word cloud based on word embedding visualizes the word usage in product reviews (Xu et al., 2016). All these show the distance between word embeddings reveals semantic relations. In a time that social media becomes part of our life, we attempt to observe the user-dependent word embeddings in a debate to analyze user-dependent semantics. In the past, incorporating meta data to train neural network models for sentiment analysis on product reviews and social media texts has been shown to be effective. For example, our UTCNN integrates users, topics and comments information in Facebook posts (Chen and Ku, 2016); Dong et al. consider topics and add an adaptive layer in their recursive neural network for target-dependent Twitter sentiment (Dong et al., 2014); Tang et al.’s UPNN incorporates users and products (Tang et al., 2015). In this paper, to see how this kind of word embeddings can be further utilized, we consider users who posted or liked the post in the process of training word embeddings in addition to a pure text-based neural network models (Kim, 2014). Such learned word embeddings for the same word would differ among posts when the engaged users are different. This work is licenced under a"
C16-2057,D14-1070,0,0.0459586,"Missing"
C16-2057,D14-1181,0,0.00571198,"dia texts has been shown to be effective. For example, our UTCNN integrates users, topics and comments information in Facebook posts (Chen and Ku, 2016); Dong et al. consider topics and add an adaptive layer in their recursive neural network for target-dependent Twitter sentiment (Dong et al., 2014); Tang et al.’s UPNN incorporates users and products (Tang et al., 2015). In this paper, to see how this kind of word embeddings can be further utilized, we consider users who posted or liked the post in the process of training word embeddings in addition to a pure text-based neural network models (Kim, 2014). Such learned word embeddings for the same word would differ among posts when the engaged users are different. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://cs.stanford.edu/people/karpathy/scholaroctopus/ 2 http://cs.stanford.edu/people/karpathy/tsnejs/wordvecs.html License details: http:// 273 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 273–277, Osaka, Japan, December 11-17 2016. Figure 1: The WordForce interface showing the result"
C16-2057,W15-1501,0,0.0183786,"rent types of controversial words ,i.e., aspects or events that people of different stance are arguing about. From Controversial Word Visualization to Suggestion After training, we gather all the word embeddings from the user-dependent transformation. For each corpus word, we collect their transformed word embeddings x0w and project them into a two-dimensional space via t-SNE (Maaten and Hinton, 2008). The two dimensions of the t-SNE plot implicitly present latent sentiment or semantic so that similar words would have similar vector representations as in many related work (Iyyer et al., 2014; Melamud et al., 2015). Now with the positions of embeddings of one word, WordForce can further calculate their intra- and inter-group distance. The intra-group distance (dispersion) of group g is defined as the average Euclidean distance to the group mean shown in equation 2. Dispersion (g) = 1 X kvn,g − µg k Ng n (2) where Ng is the size (number of dots) of this group, vn,g is the n-th vector, and µg is the mean of the group g, respectively. The inter-group distance (distance) is the average link between two groups as in equation 3. X 1 Distance (gi , gj ) = kvn − vm k (3) Ngi · Ngj v ∈g ,v ∈g n i m j where Ngi a"
C16-2057,D14-1162,0,0.0981856,"o the rise of electric charge in the nuclear power debate, has different semantics between the supportive and unsupportive posts as we expect. The right-hand side shows distribution statistics. Further clicking on any dot will show the original post content below the plot, e.g., after clicking a red dot, the unsupportive post arguing that the abandon of nuclear power will rise the electricity rate shows below. 2 Learning User-Dependent Word Embeddings To learn the user-dependent word embeddings for stance classification and visualization, we train the 50-dimensional word embeddings via GloVe (Pennington et al., 2014). These embeddings are then transformed via a user-dependent matrix embedding Uk as in equation 1. x0w = Uk · xw (1) where xw and x0w are the word embeddings of word w trained by GloVe and the transformed word embeddings, respectively. The user-dependent matrix embedding models the user’s preference for reading certain semantics where the “user” denotes a pseudo user on behalf of all likers and authors in a given post. Then the transformed word embeddings x0w are used as the input of a convolutional neural network and fed into a fully connected network to yield the final post stance. The detai"
C16-2057,P15-1012,0,0.0313493,"ce of all posts as supportive, neutral, or unsupportive. The annotation results are shown in the first row of Table 1. On average, 161.1 users are engaged to one post. The maximum is 23,297 and the minimum is one (the author). Experimental results show that the proposed model achieves good results on the Chinese Facebook fans group material as shown in the second row of Table 1 (Chen and Ku, 2016). For comparison, this model is also tested on the English open benchmark CreateDebate for stance classification and it outperforms the state of the art by achieving the accuracy 0.842 against 0.735 (Sridhar et al., 2015; Chen and Ku, 2016). 3 WordForce On top of the word embeddings obtained from the state of the art neural network model for stance classification, WordForce visualize these embeddings for debatable issues to provide useful information for research surveys or industrial applications. WordForce can illustrate each corpus word by displaying a two-dimensional word embedding distribution plot as well as the inter- and intra-group distances (dispersion and distance, respectively), where a “group” is a set of word embeddings from posts of the same stance label. Furthermore, with these statistics, Wor"
C16-2057,P15-1098,0,0.0148327,"art of our life, we attempt to observe the user-dependent word embeddings in a debate to analyze user-dependent semantics. In the past, incorporating meta data to train neural network models for sentiment analysis on product reviews and social media texts has been shown to be effective. For example, our UTCNN integrates users, topics and comments information in Facebook posts (Chen and Ku, 2016); Dong et al. consider topics and add an adaptive layer in their recursive neural network for target-dependent Twitter sentiment (Dong et al., 2014); Tang et al.’s UPNN incorporates users and products (Tang et al., 2015). In this paper, to see how this kind of word embeddings can be further utilized, we consider users who posted or liked the post in the process of training word embeddings in addition to a pure text-based neural network models (Kim, 2014). Such learned word embeddings for the same word would differ among posts when the engaged users are different. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 http://cs.stanford.edu/people/karpathy/scholaroctopus/ 2 http://cs.stanford.edu/people/karpathy/tsnejs/wordvecs.html License"
C16-2063,P97-1067,0,0.160723,"Missing"
C16-2063,D14-1162,0,0.0807747,"models. We introduce two word usage models, the Gaussian Mixture Model (GMM) (Xu and Jordan, 1996) with contextual feature and the Bi-directional Long Short Term Memory neural network (BiLSTM) (Graves et al., 2013; Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997). To build the GMM model, for each sentence s = w1 · · · wt−k · · · wt · · · wt+k · · · wn , where wt is the target word and k is the window size, we take the k words preceding and following the target word and represent them as well as their adjacent combinations in sequence using the summation of their word embeddings (Pennington et al., 2014). For example, the feature extracted by the windows size k = 2 is {ewi−2 ewi−1 ewi−2,i−1 ewi+1 ewi+2 ewi+1,i+2 }, where ew denotes the summation of word embeddings of word sequence w. Next, GMM applies Expectation–Maximization algorithm to estimate its parameters and approximate to the data distribution. Empirically, we find that the GMM model with k = 2 and number (of mixture) = 50 achieves the best performance. To train the GMM model for the target word wt , a total of 5,000 corresponding sentences are used as the training samples. To build the Bi-LSTM model, following the same idea of using"
C16-2063,W14-1821,0,0.0656421,"Missing"
C16-3002,C16-2057,1,0.843352,"Missing"
C16-3002,P07-2023,1,0.804948,"Missing"
C16-3002,D09-1131,1,0.874823,"Missing"
C16-3002,ku-etal-2010-construction,1,0.835292,"Missing"
C16-3002,I11-1039,1,0.89822,"Missing"
C16-3002,L16-1428,1,0.889018,"Missing"
C16-3002,C16-1154,1,\N,Missing
D09-1131,D08-1014,0,0.0248716,"entences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, some researchers managed to generate annotated materials and gold standards under many constraints. Ku set a standard for generating final answers from annotations of multiple annotators (Ku et al., 2007), and Somasundaran annotated discourse information from meeting dialogs to train a sentiment model (Somasundaran et al., 2007). For multilingual issues, researchers concerned mainly about the applicability of corpus and algorithms from the native language to foreign languages (Banea et al., 2008; Bautin et al., 2008). Several opinion analysis systems have been developed so far. OASYS (Cesarano et al., 2007) and CopeOpi (Ku et al., 2007) allow users input their queries and select preferred data sources, 1260 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260–1269, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP and then track opinions in a time zone. For both systems, extracting opinions is the main focus, while holders and targets are identified implicitly when retrieving relevant documents. Carenini’s team proposed a graphical user in"
D09-1131,C04-1200,0,0.307965,"lling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence seg"
D09-1131,W02-1011,0,0.0124422,"identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence segments are also defined by linguistics in the Chinese language. Th"
D09-1131,W03-1014,0,0.0262565,"variety of fields, including product recommendation, review summarization, public polling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphologi"
D09-1131,2007.sigdial-1.5,0,0.0302449,"n sentence segments are also defined by linguistics in the Chinese language. These are similar to morphological structures between Chinese characters. Based on parsing trees of sentences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, some researchers managed to generate annotated materials and gold standards under many constraints. Ku set a standard for generating final answers from annotations of multiple annotators (Ku et al., 2007), and Somasundaran annotated discourse information from meeting dialogs to train a sentiment model (Somasundaran et al., 2007). For multilingual issues, researchers concerned mainly about the applicability of corpus and algorithms from the native language to foreign languages (Banea et al., 2008; Bautin et al., 2008). Several opinion analysis systems have been developed so far. OASYS (Cesarano et al., 2007) and CopeOpi (Ku et al., 2007) allow users input their queries and select preferred data sources, 1260 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260–1269, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP and then track opinions in a time zone. For both systems, e"
D09-1131,P05-1017,0,0.0274812,"tence extraction, and 0.54 for opinion sentence polarity detection. 1 Introduction Sentiment analysis has attracted much attention in recent years because a large scale of subjective information is disseminated through various platforms on the web. Sentiment information can be applied to a wide variety of fields, including product recommendation, review summarization, public polling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanin"
D09-1131,W02-1811,0,0.174401,"ds. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence segments are also defined by linguistics in the Chinese language. These are similar to morphological structures between Chinese characters. Based on parsing trees of sentences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, s"
D17-2013,D16-1127,0,0.0421929,"I&apos;m doing fine. I&apos;m upset, but I&apos;m fine. (no suggestion) Oh, I&apos;m fine, I&apos;m widw awake. What&apos;s up? I&apos;m working. I&apos;m fine. Figure 2: Mapping between colors and emotions, and example suggestion texts for “I am fine.” mated support, helping users express their emotions in text, and therefore supplies the missing piece for an emotion-sensitive text-based communication environment. Finally, MoodSwipe introduces a new interaction paradigm, in which users explicitly provide feedback to systems about why they select this suggested response. Classic response suggestion tasks such as dialog generation (Li et al., 2016) or automated email reply (Kannan et al., 2016) assume that the in-themoment context of each user (e.g., the current emotion) is unknown. MoodSwipe opens up possibilities for users to explicitly and actively provide context on the fly, which the automated models can use to provide better suggestions. The contributions of this work are three-fold. First, we address the long-standing challenge of collecting self-reported emotion labels for dialog messages. Unlike posts on social media, where users often spontaneously tag their own emotions, self-reported emotion labels for dialog messages are ex"
D17-2013,C16-2030,1,0.844375,"India. vallari357@gmail.com Abstract assistance to rephrase their angry messages into neutral descriptions before sending them. Similarly, people may have trouble finding the perfect words to show how much they appreciate a friend’s help. Or people may want to deliberately express anger to extract concessions in negotiations, or to make a joke, such as with the “Obama’s Anger Translator” skit, in which the comedian “translates” the U.S. President’s calm statements into emotional tirades. While emotion classification has been used in helping users to better understand other people’s emotions (Wang et al., 2016; Huang et al., 2017), these technologies have rarely been used to support user needs in expressing emotions. Most prior work focuses on interface design, for instance using kinetic typography or dynamic text (Bodine and Pignol, 2003; Forlizzi et al., 2003; Lee et al., 2006) , affective buttons (Broekens and Brinkman, 2009), or text color and emoticons (S´anchez et al., 2006) to enable emotion expression in instant messengers. Other work explores the relations between user typing patterns and their emotions (Zimmermann et al., 2003; Alepis et al., 2006; Tech, 2016). However, the text itself is"
D17-2013,D14-1181,0,0.00519061,"Missing"
huang-etal-2010-predicting,D09-1131,1,\N,Missing
huang-etal-2010-predicting,W02-1811,0,\N,Missing
huang-etal-2010-predicting,I08-4008,0,\N,Missing
huang-etal-2010-predicting,I05-3005,0,\N,Missing
I11-1039,P08-2037,0,0.0162281,"e action, but the polarity is the multiplication of the signs of opinion scores of oleft and oright. if ( S (oleft )  0 and S (oright )  0) then S (oleft oright )  S (oleft )  SIGN ( S (oleft ))  SIGN ( S (oright )) (3) else S (oleft oright )  S (oleft )  S (oright )  Verb-Complement Type: The scoring function for trios of this type is defined the same as that of a Subjective-Predicate type in Formula (2). The complement node is the deciding factor of the opinion score. 3.3.2 Using opinion dependency relations The usages of opinion dependency relations were seen in several researches (Bikel and Castelli, 2008). In these researches, rules for a small number of major dependency relations were proposed in different papers but they were not listed together for a better utilization. Some rules were not ever mentioned in pervious researches. Instead, all relations are analyzed in this paper. For each relation r of which gop(r) equals true (when gold opinion relations are used for opinion analysis) or op(r) equals true (when predicted opinion relations are used for opinion analysis), we calculate its opinion score ops(r). Let RM(w) be a function to return the dependency relations of word w’s modifiers one"
I11-1039,W09-2307,0,0.100632,"corresponding dependency trees generated by the same parser from the same sentence as the gold standard for training the automatic annotator of the opinion dependency relations. We conduct experiments on the annotated opinion syntactic structures in parsing trees, and on the opinion dependency relations corresponding to them. The proposed process demonstrates a feasible direction toward the development of an opinion dependency parser. 2 Problem Definition Given a set of non-collapsed dependencies parsed from a specific sentence by the Stanford dependency parser (de Marneffe and Manning, 2008; Chang et al., 2009), each associated with a dependency relation between two words in this sentence, our goal is to identify which of them are with sentiment, i.e., those which reveal a part of opinions or the aroused emotions. For example, in the sentence “活动 取得 了 圆满 成功 (Activities scored le perfect success)”, the Stanford dependency parser gives three relations: nmod(成功 &lt;success&gt;, 圆 满 &lt;perfect&gt;), nsubj( 取 得 &lt;scored&gt;, 活 动 &lt;activities&gt;), dobj(取得 &lt;scored&gt;, 成功&lt;success&gt;), and asp(取得 &lt;scored&gt;, 了&lt;le&gt;). The goal is to identify the former three may bear sentiment or opinions. The corresponding dependency tree is shown i"
I11-1039,P08-1019,0,0.0215935,"could find the opinion passages if they can understand the whole sentence, i.e. from parsing trees. However, when the linguistic background is needed, it could be difficult for most people to reconstruct the whole sentence from the dependency trees in order to find the opinion passage. Therefore, if we want to find annotators to build a corpus which could be used to train an opinion relation recognizer, parsing trees are the better materials compared to dependency trees. However, compared to relations between words, complicated tree structures are more challenge to be utilized by algorithms (Doan et al., 2008). 345 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 345–353, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP This paper focuses on extracting opinionated dependency relations from relations generated by the Stanford parser. We design an annotation mechanism on the syntactic structures on the sentence from Chinese Treebank to create an annotation environment with a lower entry barrier so that sufficient annotations can be labeled. Then these annotations are aligned to the relations in the corresponding dependency trees generated by the sam"
I11-1039,D09-1131,1,0.850598,"Missing"
I11-1039,W02-1011,0,0.0116262,"Missing"
I11-1039,J11-1002,0,0.190669,"edu.tw formation (Qiu et al., 2008). Their researches showed that linguistic knowledge is helpful in determining opinions. For various applications related to opinions, syntactic structures have become powerful tools for extracting useful clues. To find opinions in product reviews, modification relations were used to identify the product and their features (Lu et al., 2009), e.g., a good price (feature) of this camera (product). To find opinion holders and targets, templates and linguistic rules were adopted (Breck et al., 2007). To find more opinion words, dependency relations were utilized (Qiu et al., 2011). Even when applying the basic negation rule that flips opinion polarity over, we need to find its modified word first by syntactic clues. However, we will show that syntactic relations do not directly suggest opinions. Syntactic relations are obtained usually from all kinds of syntax trees. Parsing trees (phrase structured) and dependency trees (grammatical) are the most commonly seen ones. Parsing trees are in-order trees which keep the order of words in sentences, so they are more readable for people. Instead, nodes in dependency trees are displayed by the head-modifier relations, in which"
I11-1039,2007.sigdial-1.5,0,0.0649538,"Missing"
I11-1039,I08-2079,0,0.0220489,"ns. These results clearly indicate that the syntactic information benefit opinion analysis. Because of 351 the possible information loss in the automatic alignment process, that the performance of using trios is a little better than using dependency relations matches our expectation. Setting C+W+N C+W+N+goldTrio C+W+N+Trio C+W+N+goldDep C+W+N+Dep f-Score 0.7162 0.7922 0.7993 0.7784 0.7782 Table 7. Performance of using syntactic information for opinion analysis. 5 tic structures, and it caused the lack of analysis of opinion syntactic structures. Researchers have acquired syntactic structures (Zhou, 2008), but few of them have tried to associate syntactic structures with opinions. The most similar previous work to ours was proposed by Ku et al. (2009). Compared to it, the proposed process made the development of opinion dependency parser feasible. As dependency relations and the predicted opinion dependency relations are of the same form, no extra knowledge or integration is needed for the use of them. Related Work For all we know, no previous work has annotated opinion information on all dependency relations, or mapped annotated opinionated structures to dependency relations on a large quanti"
I11-1039,P07-2023,1,0.784744,"es of opinion sentences when bearing opinions (gop(r) equals true), i.e., the value in F column, is taken as the support value. The support value indicates that in what degree this relation bears opinions. If the support value is high, it is confident to say that the relation is opinionated; otherwise, considering the content words is necessary. This idea conforms to the previous observation in Section 4.2: some of the opinions are structural, but not all of them. According to the support value, dependency relations were divided into four categories. The Chinese opinion word dictionary NTUSD (Ku et al., 2007) is involved to help identify opinion dependency relations when the support value is not high. The selecting criteria are listed as follows.  Very supportive: with the support value above 0.8, e.g., dvpmod. Relations in this category are viewed as opinionated and their gop(r) are automatically set to true.  Supportive: with the support value above 0.35 but lower than 0.8, e.g., pass, dobj, npsubj, ba, top, nsubj, neg, amod, rcmod. RH(w) is a function to return the word w’s 350 head in other relations of the same sentences, and RM(w) returns w’s modifier. For each r  {rel , wh ,wm } in this"
I13-1117,D10-1036,0,0.469533,"d discounted cumulative gain nDCG (Jarvelin and Kekalainen, 2002), precision (i.e., P), and mean reciprocal rank (i.e., MRR). We first examine the effectiveness of our semantic feature regarding content words in interest predictions. Table 1 suggests that while slight (slg) content word propagation is helpful, moderate (mod) and aggressive (agr) are not. Inflating content words’ statistics is simply sufficient. In addition, we found that smaller window size (WS=3) fit more to our context of mixed-code blogs, while suitable window sizes were much larger in news articles and research abstracts (Liu et al., 2010). Table 2 summarizes the interest prediction quality of two baselines, entropy and tfidf, and PageRank (PR) with different interest preference estimations on test set. In Table 2, entropy and tfidf, taking local (the article) and global (whole article collection) information into account, outperform PageRank using solely local information (PR+tf). Among all, PR+tfidf achieves the best performance. Compared to PR+Pr’s, entropy in PR+PrEntropy’s does help to discern topical interest words. Moreover, the benefit of entropy is more evident when better estimation strategy Pr(tp|w) is applied: commo"
I13-1117,J00-2011,0,0.912129,"building the word graph. Subsequently, word interest preferences and PageRank were utilized to find interest terms. Weightings concerning syntactic and semantic features are utilized in PageRank. Moreover, content-source and content-word weighted PageRank were exploited to return words for interest evaluation. The predicted interests can further be used as candidates for social tagging or article recommendation. 2 Related Work The state-of-the-art keyword extraction methods have been applied to a myriad of natural language processing tasks including document categorization and summarization (Manning and Schutze, 2000; Litvak and Last, 2008), indexing (Li et al., 2004), information retrieval (Turney, 2000), and text mining on social networking or micro-blogging services (Li et al., 2010; Zhao et al., 2011;Wu et al., 2010). Here we extract keywords related to readers’ interests. Recently, collaborative tagging or social tagging has grown in popularity among Web services and received much attention (Golder and Huberman 2006; Halpin et al., 2007). Instead of analyzing user (tagging) activity or tag frequencies, we analyze articles and their social interaction content to predict reader interests. Researches ha"
I13-1117,P02-1040,0,0.0953849,"y, called quality reader responses hereafter, is as follows. (1) ngramsart=generateNgram(ART) (2) Focused=findFocused(IntPrefs) (3) selectedSt=NULL for each sentence st in FB (4a)ngramsst = generateNgram(st) (4b)informativityco = Coverage-evaluate (ngramsst , ngramsart) (4c)informativityfo = Focus-evaluate (ngramsst , Focused) (4d)append st into selectedSt if conditions hold return selectedSt Each response is evaluated at sentence level concerning informativity checked in two aspects. The first concerns the topic cohesion between reader response sentence st and article ART. Similar to BLEU’s (Papineni et al., 2002) weighted ngram precision in machine translation, we compute the weighted ngram coverage of st (Step (4b)) on ART and favor the coverage of longer grams. Larger ngram coverage indicates higher topic correlation between the two. The second considers the topic distributions of words in st. We first rank and identify the words expected to have low topic uncertainty. Entropy estimation in 913 aggressively make a non-content word’s score flow to its content word partners by setting m in Step (4a) and 1/m in Step (4b) where m&gt;1, and, circulate more wi‘s score to content words if wi is a content word"
I13-1117,N10-1101,0,0.373801,"urce and content-word weighted PageRank were exploited to return words for interest evaluation. The predicted interests can further be used as candidates for social tagging or article recommendation. 2 Related Work The state-of-the-art keyword extraction methods have been applied to a myriad of natural language processing tasks including document categorization and summarization (Manning and Schutze, 2000; Litvak and Last, 2008), indexing (Li et al., 2004), information retrieval (Turney, 2000), and text mining on social networking or micro-blogging services (Li et al., 2010; Zhao et al., 2011;Wu et al., 2010). Here we extract keywords related to readers’ interests. Recently, collaborative tagging or social tagging has grown in popularity among Web services and received much attention (Golder and Huberman 2006; Halpin et al., 2007). Instead of analyzing user (tagging) activity or tag frequencies, we analyze articles and their social interaction content to predict reader interests. Researches have been done on reader profiling for content recommendation. White et al. (2009) examined five types of contextual information in website recommendation while Ye et al. (2012) further explored social influenc"
I13-1117,W04-3252,0,\N,Missing
I17-1012,P10-2045,0,0.0536043,"Missing"
I17-1012,E12-1004,0,0.0247338,"educe Chinese segmentation error. For each Word v, we retrieve all the words that share at least one feature with w and call them candidate words. Drop the candidate word if it shares less than 1 percent features, counted by frequency, with word w. We then calculate the distributional similarity score between w and its candidate words. Balanced-inclusion (BInc, (Szpektor and Dagan, 2008a)) is a well-known scoring function for 1 f ∈Fl ∩Fr 3.1.4 Word Embeddings Previous work has shown that word embeddings work well on entailment relation recognition of noun-noun pairs and (adj+noun)-noun pairs (Baroni et al., 2012; Roller et al., 2014). We choose glove (Pennington et al., 2014) to train embeddings of each word, and concatenate the embedding of two words to create the embedding for each word pair. This embedding then serves as the feature in the rbf-kernel SVM classifier to predict the entailment relation of the corresponding word pair. 3.2 Probabilistic Soft Logic (PSL) We use the PSL model to find the latent inference relations by enabling the transitivity of lexCKIP parser : http://parser.iis.sinica.edu.tw/ 113 ical relations. The lexical relations include features described in Section 3.1, and the k"
I17-1012,O97-1002,0,0.186428,"Missing"
I17-1012,P14-1114,0,0.051752,"Missing"
I17-1012,P15-2020,0,0.026146,"Missing"
I17-1012,P10-1124,0,0.0145538,"are rather flat compared to nouns, which brings problems when directly adopting approaches utilizing WordNet to detect the inference between verbs. An unsupervised concept, distributional similarity, for measuring relations between words was proposed to overcome the coverage problem. Distributional similarity related algorithms utilized a large, unstructured corpus to learn lexical entailment relations by assuming that semantically similar lexicons appear with similar context (Harris, 1954). Various implementations were proposed to assess contextual similarity between two lexicons, including (Berant et al., 2010; Lin and Pantel, 2001; Weeds et al., 2004). Lin Similarity, or known as DIRT, is one commonly adopted method to measure the lexical context similarity (Lin and Pantel, 2001). Instead of applying the Distributional Hypothesis to verbs, Lin applied this hypothesis to the paths in dependency trees. They hypothesize that the meaning of two phrases is similar, if their paths tend to link the same sets of words in a dependency tree. Later, Weeds and Weir (2004) proposed a general framework for directional similarity measurement. The measurement examined the coverage of word wl ’s features against t"
I17-1012,P11-1062,0,0.020647,"edicates in the same frame. However, no matter WordNet or FrameNet was used, the covRecent research is exploiting the effect of transitivity during model training. The intuition is that some implicit entailment relation is difficult to be identified when there is no direct features supporting it. Sometimes previous work could find the 111 3 entailment pairs w1 → w2 and w2 → w3 , but failed to answer distant entailment relation like w1 → w3 . Skeptor and Dagan (2009) first applied transitive chaining in the knowledge provided by the lexical ontology Wordnet (Miller, 1995) in the feature layer. Berant et al. (2011) built a lexical entailment knowledge graph given the predicted results from the base classifier. They used integer linear programming (ILP) to find the latent entailment in the prediction cascade, which transits in the prediction layer. Kloetzer et al. (2015), whose system outperformed Berant et al.’s on their own corpus, further use cascade entailment inference in the feature layer. They applied short transitivity optimization by a two-layered SVM classifier (Kloetzer et al., 2015). A set of candidate transitivity paths were created by concatenating two identified inference pairs from the fi"
I17-1012,D07-1017,0,0.0911354,"Missing"
I17-1012,D15-1190,0,0.0780719,"re is no direct features supporting it. Sometimes previous work could find the 111 3 entailment pairs w1 → w2 and w2 → w3 , but failed to answer distant entailment relation like w1 → w3 . Skeptor and Dagan (2009) first applied transitive chaining in the knowledge provided by the lexical ontology Wordnet (Miller, 1995) in the feature layer. Berant et al. (2011) built a lexical entailment knowledge graph given the predicted results from the base classifier. They used integer linear programming (ILP) to find the latent entailment in the prediction cascade, which transits in the prediction layer. Kloetzer et al. (2015), whose system outperformed Berant et al.’s on their own corpus, further use cascade entailment inference in the feature layer. They applied short transitivity optimization by a two-layered SVM classifier (Kloetzer et al., 2015). A set of candidate transitivity paths were created by concatenating two identified inference pairs from the first SVM classifier, e.g., w1 → w2 and w2 → w3 result in a candidate path w1 → w2 → w3 . Then the two-layered SVM classifier re-predicted whether there was an inference relation for the lexical pair w1 → w3 . However, none of these models takes into account tra"
I17-1012,I13-1095,0,0.0222899,"Missing"
I17-1012,W11-0112,0,0.0585083,"Missing"
I17-1012,W13-0904,0,0.031668,"Missing"
I17-1012,D14-1162,0,0.0821099,"ve all the words that share at least one feature with w and call them candidate words. Drop the candidate word if it shares less than 1 percent features, counted by frequency, with word w. We then calculate the distributional similarity score between w and its candidate words. Balanced-inclusion (BInc, (Szpektor and Dagan, 2008a)) is a well-known scoring function for 1 f ∈Fl ∩Fr 3.1.4 Word Embeddings Previous work has shown that word embeddings work well on entailment relation recognition of noun-noun pairs and (adj+noun)-noun pairs (Baroni et al., 2012; Roller et al., 2014). We choose glove (Pennington et al., 2014) to train embeddings of each word, and concatenate the embedding of two words to create the embedding for each word pair. This embedding then serves as the feature in the rbf-kernel SVM classifier to predict the entailment relation of the corresponding word pair. 3.2 Probabilistic Soft Logic (PSL) We use the PSL model to find the latent inference relations by enabling the transitivity of lexCKIP parser : http://parser.iis.sinica.edu.tw/ 113 ical relations. The lexical relations include features described in Section 3.1, and the known inference relations in the observed layer. In PSL, each rela"
I17-1012,C14-1097,0,0.0248601,"Missing"
I17-1012,P16-1226,0,0.01272,"ases is similar, if their paths tend to link the same sets of words in a dependency tree. Later, Weeds and Weir (2004) proposed a general framework for directional similarity measurement. The measurement examined the coverage of word wl ’s features against those of wr ’s, and more coverage indicated more similarity. Figure 1: Three-layer lexical inference system. Points of the same shape in each layer are the same verbs; the solid arrow indicates the known inference relation; the dotted arrow indicates the hidden inference relation which can be inferred by the known inference relations. 2015; Shwartz et al., 2016), which makes them limited or not capable for these newly proposed research problems. In this paper, we adopt the probabilistic soft logic (PSL) model to find lexical inference on Chinese verbs toward the math word problem solver. The contributions of this paper are listed as follows: (1) We build the largest Chinese verb lexical inference dataset with four types of inference relations as a potential testbed in the future. (2) We show that in the proposed PSL model the transitivity is easy to enabled and can benefit the lexical inference on Chinese verbs. (3) We implement and discuss the trans"
I17-1012,W14-2715,0,0.0991972,"Missing"
I17-1012,C08-1107,0,0.140185,"rbs Using Probabilistic Soft Logic Wei-Chung Wang, Lun-Wei Ku Academia Sinica 128 Academia Road, Section2 Nankang, Taipei 11529, Taiwan {anthonywang, lwku}@iis.sinica.edu.tw Abstract ples” from the ground truth “Mom buys apples”to answer the question “Who has apples?” without explicitly mentioning it. An intuitive solution to this problem is to first represent the sense of words in the lexicon to calculate the confidence of inferences from one sense to another, or to build a classifier to distinguish inference relations from other relations. Most related research is of one of these two types (Szpektor and Dagan, 2008a; Kiela et al., 2015). However, for this problem it is difficult for these models to take into account transitivity. In the framework of a lexical inference system, transitivity can be included in three layers: the observed layer, the feature layer, and the prediction layer. Figure 1 illustrates these layers and the corresponding transitives. The observed layer includes inference relations we already know, e.g., true inferences from the gold labels or ontologies; the feature layer includes the observed features for all lexicon pairs to be predicted,i.e.,features for the testing data, and the"
I17-1012,W09-2504,0,0.05868,"Missing"
I17-1012,C04-1146,0,0.0783558,"Missing"
I17-4017,W02-1011,0,0.0313478,"Missing"
I17-4017,S17-2088,0,0.0258765,"r Chinese Phrases shared task. Our model incorporates word embedding as well as image features, attempting to simulate human’s imaging behavior toward sentiment analysis. Though the performance is not comparable to others in the end, we conduct several experiments with possible reasons discussed, and analyze the drawbacks of our model. 1 Introduction Dimensional Sentiment prediction is a subcategory of sentiment analysis. Traditionally, the goal of the sentiment classification task is either binary, mostly positive and negative, or categorical, such as happy, angry, and sad(Pang et al., 2002; Rosenthal et al., 2017). Instead of categorizing different emotions to a fixed number of classes, the dimensional approach projects each emotion to valencearousal (VA) space. Valence indicates the level of pleasant and unpleasant, while arousal shows the level of excitement and calm. This methodology has drawn more attention recently since the valence-arousal space is continuous comparing to the discrete classes used previously, so it implies the better capability to describe the emotion, while better benefiting downstream model for further application. 105 Proceedings of the 8th International Joint Conference on Na"
I17-4017,P16-2037,0,0.0183147,"each single word. The method proposed is a weighted graph model, each vertex is a vector representation of a word, and the edge is weighted by the cosine similarity between two words. The unseen words use the neighboring seed words’ labeled scores, which weighted by the similarity, to update themselves’ score. This can be considered an iterative process and will loop until converge, and since the similarity is calculated based on word embedding, it is critical that the word embedding used in training can indeed represent the relationship between word with respect to sentiment. In the work of (Wang et al., 2016a), they try to deal with the task by purposing a community-based method. They define a Modularity term which can be viewed as the difference between the sum of all similarities between words within a community C, and the Related Work There are several different methods toward the VA prediction task, some researches about the valence arousal prediction on paragraphs are conducted (Wang et al., 2016b; Nicolaou et al., 2011), while 106 sum of all similarities between words in C and all the other words that not belongs to C. The training step is to maximize the modularity term with respect to all"
I17-4017,P15-2129,0,0.0296008,"ion 7 concludes the paper with future works. 2 modal 也許(maybe) 本來(originally) 應該(perhaps) Table 2: Examples of degree, negation, modifier in phrase modifier’s type phrase deg 異常驚人 neg deg 沒有太驚訝 mod neg 本來不支持 Figure 1: The example of scene that people may imagine with the word ”欣 喜 若 狂”(ecstatic). These images are directly downloaded from the Internet. Word Word Valence 成功(success) 8.2 暗殺(assassinate) 1.8 狂喜(ecstatic) 8.6 Phrase Phrase Valence 最為出色most excellent) 8.056 極度失望(very disappoint) 1.63 略為放鬆(little relaxing) 6.2 negation 不能(cannot) 沒(no) 沒有(no) 2.1 Graph-Based Approach In the work of (Yu et al., 2015), the objective is to predict the V and A score for each single word. The method proposed is a weighted graph model, each vertex is a vector representation of a word, and the edge is weighted by the cosine similarity between two words. The unseen words use the neighboring seed words’ labeled scores, which weighted by the similarity, to update themselves’ score. This can be considered an iterative process and will loop until converge, and since the similarity is calculated based on word embedding, it is critical that the word embedding used in training can indeed represent the relationship betw"
ku-etal-2006-tagging,W03-1014,0,\N,Missing
ku-etal-2006-tagging,C04-1200,0,\N,Missing
ku-etal-2006-tagging,W02-1011,0,\N,Missing
ku-etal-2006-tagging,P05-1017,0,\N,Missing
ku-etal-2010-construction,D09-1131,1,\N,Missing
ku-etal-2010-construction,W03-0404,0,\N,Missing
ku-etal-2010-construction,P07-2023,1,\N,Missing
L16-1428,huang-etal-2010-predicting,1,0.914461,"c information to the dictionary entries. In the last part of this paper, we show that using ANTUSD in sentiment word detection and word polarity classification both achieve good results. 2. Related Materials ANTUSD was built mainly by collecting manual annotation of words in the process of building several sentiment corpora in a long period of time from year 2006 to year 2010. Therefore, before we describe the construction of ANTUSD, materials involved in this process are first introduced: NTUSD, NTCIR MOAT task dataset, Chinese Opinion Treebank (L.-W. Ku, Huang, & Chen, 2010), AciBiMA (T.-H. Huang, Ku, & Chen, 2010; T.-H. K. Huang, Chen, & Kong, 2015), CopeOpi and E1 HowNet, where NTUSD is a sentiment dictionary, NTCIR MOAT task dataset and Chinese opinion Treebank are two manual labeled opinion sentence datasets, CopeOpi is a Chinese opinion scoring system, and E-HowNet is a Chinese knowledge ontology, described as follows: NTUSD NTUSD (L.-W. Ku, Liang, & Chen, 2006) is the prototype sentiment dictionary of ANTUSD. It provides a total of 11,088 sentiment words containing 2,812 positive words and 8,276 negative words. NTUSD was published in year 2006 and has been downloaded more than 300 times and widel"
L16-1428,W15-3105,0,0.0293917,"Missing"
L16-1428,D09-1131,1,0.84626,"ated number of labels collected in the annotation process is related to the word frequency, as annotators will only label sentiment words when they read them in documents or sentences. ACIBiMA ACIBiMA is a Chinese word morphological structure corpus developed by Huang (T.-H. K. Huang et al., 2015). ACIBiMA now contains more than 10,000 Chinese words and their morphological types. It was a continuous work of building the Chinese Morphology Dataset (T.-H. Huang et al., 2010) and has been utilized to test the connection between the Chinese morphological structure and the Chinese sentiment (L.-W. Ku, Huang, & Chen, 2009). The Chinese word sentiment in this dataset were labeled by the annotators as positive, neutral, negative, non-opinionated, and not-a-word. As the words for labeling were selected randomly from a large automatically Chinese-word-segmented dataset, the label not-a-word is used to note those segmented incorrectly by the word segmentation system. Though the label not-a-word is not related to sentiment, it is included in ANTUSD as most Chinese text analysis tasks involve word segmentation and including words of the not-a-word type may provide some additional clues to ignore the word candidates f"
L18-1252,P05-1045,0,0.00573673,"I spilled sauce all over the front of my shirt. You got an extra one? neutral utterance emotion speaker utterance emotion Rachel Yeah, sure. Umm... here. neutral Table 3: Data format of EmotionLines Figure 1: Worker interface on Amazon Mechanical Turk 3.3. De-identification The EmotionPush chat logs are from private conversations. Therefore, the logs may contain personal information such as names of real people, locations, organizations, and email addresses. In order to protect the privacy of EmotionPush users, we performed a two-step masking process. First, Stanford Named Entity Recognizer (Finkel et al., 2005) was adopted to detect named entities mentioned by each utterance, which were later replaced with their entity types. After this step, however, we still found named entities like lowercase or foreign names and emails. Therefore, we manually checked and cleaned utterances in the first step again to prevent the accidental reveal of personal data. Since the conversations collected in the EmotionPush chat logs involve not only our participants, we want to carefully protect their identity. Therefore, we hired a native speaker whose occupation is an editor to rewrite all the messages of participants"
L18-1252,D14-1181,0,0.0077061,"dies on emotion recognition research over 6 basic emotions: anger, disgust, fear, happiness, sadness, and surprise. His study shows it is possible to detect emotions given enough features (Ekman et al., 1987). Later studies on text-based emotion recognition are mainly divided into three categories: keyword-based, learning-based, and hybrid recommendation approaches (Kao et al., 2009). Recently, emotion recognition researches on text focus on the learning-based methods. Kim proposed CNN(Convolutional neural network) text classification, which is widely used for extracting sentence information (Kim, 2014). However, single sentence emotion recognition is lack of contextual emotion flow within a dialogue. Therefore, contextual LSTM(Long short-term memory) architecture is proposed to measure the inter-dependency of utterances in the dialogue (Poria et al., 3.1.1. Friends TV Scripts The crawled scripts are separated as episodes, and we viewed each scene in every episode as a dialogue. Then, the collected dialogues were categorized according to their dialogue length, i.e. the number of utterances in a dialogue, into four classes of which bucket length ranges are [5, 9], [10, 14], [15, 19], and [20,"
L18-1252,W17-5205,0,0.0478182,"Missing"
L18-1252,D14-1162,0,0.0799127,"Missing"
L18-1252,P17-1081,0,0.0587445,"Missing"
L18-1252,L16-1499,0,0.0268997,"mart” dialogue system by enhancing dialogue breadth (coverage), dialogue depth (complexity) or both. Those who want to increase dialogue breadth try to transfer dialogue acts across domains (Chen et al., 2016) to establish multi-domain or even open domain dialogue system, and those who want to deepen dialogue complexity pay their attention to transform a knowledge-based systems to common sense or even empathetic systems that can recognize emotion features, generate emotion-aware responses (Fung et al., 2016), or learn how to plan the dialogues while users interact via high-level descriptions (Sun et al., 2016). No matter what kind of dialogue system we want to build, a useful and large dialogue dataset is indispensable. When building a task-oriented dialogue system, dialogue corpora with dialogue act information is accessible and hence are commonly utilized. However when building a chit-chat conversational bot, though the importance of emotion detection has been noticed, pure conversation content such as movie, TV scripts or chat logs without emotion labels are more available: no emotion labels on utterances can be used for learning. Moreover, when we turn to other datasets with annotated emotion i"
L18-1252,C16-2030,1,0.895655,"Missing"
N19-1031,P17-1021,0,0.112626,"16; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017), whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and it"
N19-1031,D13-1160,0,0.178715,"defined maximum hop number is required in UHop, as it enables models within the framework to halt; (2) UHop reduces the search space complexity from exponential to polynomial while maintaining comparable results; (3) UHop facilitates the use of different models, including state-of-the-art models. 2 Related Work State-of-the-art KBQA methods are in general based on either semantic parsing, or on embedding (Zhou et al., 2018). Semantic parsing methods learn semantic parsers which parse natural language input queries into logical forms, and then use the logical forms to query the KG for answers (Berant et al., 2013; Yih et al., 2015, 2016; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast"
N19-1031,P17-1167,0,0.0387173,"Missing"
N19-1031,I17-2002,0,0.0136982,"tract the next relation: if the extracted relation has the highest score than all the outbound relations then the process is terminated (step 4), otherwise, continued (step 2). Here we use solid arrows and dash arrows to respectively represent positive/negative candidates. and the original question Q, G generates the new question representation as Q0 = G(Q, P ). Our assumption is that since the current relation has been selected, its related information in the question loses importance when extracting the next relation. Inspired by both supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017), which is lacking in our datasets, and the coverage loss design for summarization (See et al., 2017), we de-focus the selected relation by manipulating weights in the question representation. We propose two ways of updating the question representation, taking into account the existence of the attention layer in the model’s architecture. For attentive models, we directly utilize the attention weight as part of our dynamic question representation generation function by Loss is defined depending on the flag stop. If the process should continue, i.e., stop is false, loss is defined as 0 LT D = ma"
N19-1031,N18-1165,0,0.0730203,"wo entities are nodes and their relation is the edge connecting them in the knowledge graph. Given a natural language question, a KBQA system returns its answer if it is included in the knowledge graph; the process of answering a question can be transformed into a traversal that starts from the question (topic) entity and searches for the appropriate path to the answer entity. 345 Proceedings of NAACL-HLT 2019, pages 345–356 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ing logical rules, a variational auto-encoder view of the knowledge graph (Chen et al., 2018b; Zhang et al., 2018b), and reward shaping technique (Lin et al., 2018) for further improvement. The major difference between UHop and these methods is that they do not utilize annotated relations and hence require REINFORCE training (Williams, 1992) for optimization. As some datasets are already annotated with relations and paths, direct learning using an intermediate reward is more reasonable. Hence UHop adopts a novel comparative termination decision module to control the search process of the relation path. The most related approach is the IRN model (Zhou et al., 2018), composed of an inp"
N19-1031,D17-1160,0,0.0139654,"p, as it enables models within the framework to halt; (2) UHop reduces the search space complexity from exponential to polynomial while maintaining comparable results; (3) UHop facilitates the use of different models, including state-of-the-art models. 2 Related Work State-of-the-art KBQA methods are in general based on either semantic parsing, or on embedding (Zhou et al., 2018). Semantic parsing methods learn semantic parsers which parse natural language input queries into logical forms, and then use the logical forms to query the KG for answers (Berant et al., 2013; Yih et al., 2015, 2016; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al.,"
N19-1031,D11-1049,0,0.0494753,", 2018; Lukovnikov et al., 2017), whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and its variants. (Lao et al., 2011; Gardner et al., 2014, 2013; Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input. DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017) tackle this issue by framing the multi-hop reasoning problem as a Markov decision process, efficiently searching for paths using reinforcement learning; others propose an algorithm (Yang et al., 2017) for learn3 UHop Relation Extraction With UHop, we aim to handle unrestricted relation hops and to be compatible with existing relation extraction models. UHop breaks down unrestricted-hop re"
N19-1031,P15-1026,0,0.11112,"rs (Berant et al., 2013; Yih et al., 2015, 2016; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017), whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early metho"
N19-1031,D18-1362,0,0.0152183,"the knowledge graph. Given a natural language question, a KBQA system returns its answer if it is included in the knowledge graph; the process of answering a question can be transformed into a traversal that starts from the question (topic) entity and searches for the appropriate path to the answer entity. 345 Proceedings of NAACL-HLT 2019, pages 345–356 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ing logical rules, a variational auto-encoder view of the knowledge graph (Chen et al., 2018b; Zhang et al., 2018b), and reward shaping technique (Lin et al., 2018) for further improvement. The major difference between UHop and these methods is that they do not utilize annotated relations and hence require REINFORCE training (Williams, 1992) for optimization. As some datasets are already annotated with relations and paths, direct learning using an intermediate reward is more reasonable. Hence UHop adopts a novel comparative termination decision module to control the search process of the relation path. The most related approach is the IRN model (Zhou et al., 2018), composed of an input module, a memory-based reasoning module, and an answer module. At eac"
N19-1031,C16-1291,0,0.0154696,"terminate or to extract the next relation: if the extracted relation has the highest score than all the outbound relations then the process is terminated (step 4), otherwise, continued (step 2). Here we use solid arrows and dash arrows to respectively represent positive/negative candidates. and the original question Q, G generates the new question representation as Q0 = G(Q, P ). Our assumption is that since the current relation has been selected, its related information in the question loses importance when extracting the next relation. Inspired by both supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017), which is lacking in our datasets, and the coverage loss design for summarization (See et al., 2017), we de-focus the selected relation by manipulating weights in the question representation. We propose two ways of updating the question representation, taking into account the existence of the attention layer in the model’s architecture. For attentive models, we directly utilize the attention weight as part of our dynamic question representation generation function by Loss is defined depending on the flag stop. If the process should continue, i.e., stop is false, loss"
N19-1031,D14-1044,0,0.0233884,"et al., 2017), whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and its variants. (Lao et al., 2011; Gardner et al., 2014, 2013; Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input. DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017) tackle this issue by framing the multi-hop reasoning problem as a Markov decision process, efficiently searching for paths using reinforcement learning; others propose an algorithm (Yang et al., 2017) for learn3 UHop Relation Extraction With UHop, we aim to handle unrestricted relation hops and to be compatible with existing relation extraction models. UHop breaks down unrestricted-hop relation extraction into"
N19-1031,D13-1080,0,0.0650814,"Missing"
N19-1031,D16-1249,0,0.0204458,"ons to decide to terminate or to extract the next relation: if the extracted relation has the highest score than all the outbound relations then the process is terminated (step 4), otherwise, continued (step 2). Here we use solid arrows and dash arrows to respectively represent positive/negative candidates. and the original question Q, G generates the new question representation as Q0 = G(Q, P ). Our assumption is that since the current relation has been selected, its related information in the question loses importance when extracting the next relation. Inspired by both supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017), which is lacking in our datasets, and the coverage loss design for summarization (See et al., 2017), we de-focus the selected relation by manipulating weights in the question representation. We propose two ways of updating the question representation, taking into account the existence of the attention layer in the model’s architecture. For attentive models, we directly utilize the attention weight as part of our dynamic question representation generation function by Loss is defined depending on the flag stop. If the process should continue, i.e., st"
N19-1031,P15-1049,0,0.0238758,"ess, efficiently searching for paths using reinforcement learning; others propose an algorithm (Yang et al., 2017) for learn3 UHop Relation Extraction With UHop, we aim to handle unrestricted relation hops and to be compatible with existing relation extraction models. UHop breaks down unrestricted-hop relation extraction into two major subtasks: single-hop relation extraction and comparative termination decision. Algorithm 1 illustrates how we perform these two tasks in the UHop framework. Given a question Q and the topic entity e extracted by an existing entity linking method such as S-MART (Yang and Chang, 2015), we first query the knowledge graph for the candidate outbound relations R that are connected to e. For all relations R, we extract single-hop relations in order to choose one relation to transit to the next entity e0 . After transition (e ← e0 ), we decide whether to terminate, that is, we determine whether the process should proceed 346 3.1 Algorithm 1: Unrestricted-hop relation extraction. e denotes the extracted topic entity, ‘:’ is the concatenation operation, and the termination decision returns True if the framework decides to stop. 1 2 3 4 5 6 7 8 9 10 11 12 Single Hop Relation Extrac"
N19-1031,D17-1252,0,0.0127868,"halt; (2) UHop reduces the search space complexity from exponential to polynomial while maintaining comparable results; (3) UHop facilitates the use of different models, including state-of-the-art models. 2 Related Work State-of-the-art KBQA methods are in general based on either semantic parsing, or on embedding (Zhou et al., 2018). Semantic parsing methods learn semantic parsers which parse natural language input queries into logical forms, and then use the logical forms to query the KG for answers (Berant et al., 2013; Yih et al., 2015, 2016; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017), wherea"
N19-1031,D14-1162,0,0.0809185,"Missing"
N19-1031,P15-1128,0,0.853531,"ss.edu, jijnasa23@gmail.com Abstract In the literature (Yu et al., 2017; Yin et al., 2016; Yih et al., 2015) KBQA is decomposed into topic entity linking, which determines the starting entity corresponding to the question, and relation extraction, which finds the path to the answer node(s). Theoretically, relation extraction finds paths of any length, that is, paths that contain any number of relation links, or hops (between two nodes), as long as it reaches the answer node. In previous work, models consider all relation paths starting from the topic entity (Yu et al., 2017; Yin et al., 2016; Yih et al., 2015); we call these relation-chain-based methods. Two main difficulties for these methods are that processing through all relations in a KG is not practical as the combination of these relations is nearly infinite, and that the number of candidate paths grows exponentially with the path length and quickly becomes intractable for large knowledge graphs. As a result, current relation-chain-based methods set the maximum length of candidate paths to 1, 2 or 3. However, under this framework we cannot find answer entities for indirect or complicated questions. Most importantly, even given a larger maxim"
N19-1031,P16-2033,0,0.0552113,"Missing"
N19-1031,P17-1099,0,0.0124073,"en the process is terminated (step 4), otherwise, continued (step 2). Here we use solid arrows and dash arrows to respectively represent positive/negative candidates. and the original question Q, G generates the new question representation as Q0 = G(Q, P ). Our assumption is that since the current relation has been selected, its related information in the question loses importance when extracting the next relation. Inspired by both supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017), which is lacking in our datasets, and the coverage loss design for summarization (See et al., 2017), we de-focus the selected relation by manipulating weights in the question representation. We propose two ways of updating the question representation, taking into account the existence of the attention layer in the model’s architecture. For attentive models, we directly utilize the attention weight as part of our dynamic question representation generation function by Loss is defined depending on the flag stop. If the process should continue, i.e., stop is false, loss is defined as 0 LT D = max(0, −(sr − srˆ) + margin), (4) 0 where score sr is the score of the question paired with the gold re"
N19-1031,C16-1164,0,0.421073,"Missing"
N19-1031,C18-1280,0,0.0114631,"ces the search space complexity from exponential to polynomial while maintaining comparable results; (3) UHop facilitates the use of different models, including state-of-the-art models. 2 Related Work State-of-the-art KBQA methods are in general based on either semantic parsing, or on embedding (Zhou et al., 2018). Semantic parsing methods learn semantic parsers which parse natural language input queries into logical forms, and then use the logical forms to query the KG for answers (Berant et al., 2013; Yih et al., 2015, 2016; Krishnamurthy et al., 2017; Iyyer et al., 2017; Peng et al., 2017; Sorokin and Gurevych, 2018). These systems are effective and provide deep interpretation of the question, but require expensive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017), whereas others focus on extracting"
N19-1031,D15-1174,0,0.0203959,"rs focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and its variants. (Lao et al., 2011; Gardner et al., 2014, 2013; Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input. DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017) tackle this issue by framing the multi-hop reasoning problem as a Markov decision process, efficiently searching for paths using reinforcement learning; others propose an algorithm (Yang et al., 2017) for learn3 UHop Relation Extraction With UHop, we aim to handle unrestricted relation hops and to be compatible with existing relation extraction models. UHop breaks down unrestricted-hop relation extraction into two major subtasks: single-hop"
N19-1031,D17-1060,0,0.0332057,", 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and its variants. (Lao et al., 2011; Gardner et al., 2014, 2013; Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input. DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017) tackle this issue by framing the multi-hop reasoning problem as a Markov decision process, efficiently searching for paths using reinforcement learning; others propose an algorithm (Yang et al., 2017) for learn3 UHop Relation Extraction With UHop, we aim to handle unrestricted relation hops and to be compatible with existing relation extraction models. UHop breaks down unrestricted-hop relation extraction into two major subtasks: single-hop relation extraction and comparative termination decision. Algorithm 1 illustrates how we perform these two tasks in the UHo"
N19-1031,P16-1220,0,0.0686958,"sive data annotation, or require training using reinforcement learning. Embedding-based methods first allocate candidates from the knowledge graph, represent these candidates as distributed embedding vectors, and choose or rank these vectors. Here the candidates can be either entities or relations. Some use embedding-based models to predict answers directly (Dong et al., 2015; Bast and Haussmann, 2015; Hao et al., 2017; Zhou et al., 2018; Lukovnikov et al., 2017), whereas others focus on extracting relation paths and require further procedures to select the answer entity (Bordes et al., 2015; Xu et al., 2016; Yin et al., 2016; Yu et al., 2017; Zhang et al., 2018a; Yu et al., 2018; Chen et al., 2018a; Shen et al., 2018). Our work follows the latter methods in focusing on predicting relation paths, but we seek to eliminate the need to assume in advance a maximum number of hops. For the solution, we turn to the field of multihop knowledge based reasoning. Early methods include the Path-Ranking Algorithm and its variants. (Lao et al., 2011; Gardner et al., 2014, 2013; Toutanova et al., 2015) The drawback of these methods is that they use random walks independent of the type of input. DeepPath (Xiong"
N19-1031,C18-1171,0,0.378655,"the knowledge graph (Chen et al., 2018b; Zhang et al., 2018b), and reward shaping technique (Lin et al., 2018) for further improvement. The major difference between UHop and these methods is that they do not utilize annotated relations and hence require REINFORCE training (Williams, 1992) for optimization. As some datasets are already annotated with relations and paths, direct learning using an intermediate reward is more reasonable. Hence UHop adopts a novel comparative termination decision module to control the search process of the relation path. The most related approach is the IRN model (Zhou et al., 2018), composed of an input module, a memory-based reasoning module, and an answer module. At each hop, it predicts a relation path using the reasoning module, and also optimizes it using intermediate results. However, UHop has demonstrated the ability to process large-scale knowledge graphs in experiments conducted on Freebase (Bordes et al., 2015). In contrast, IRN consumes memory linearly to the size of the knowledge graph, resulting in a limited workspace, e.g., they use a subset of Freebase in their experiments. Also, IRN still uses a constraint for the number of maximum hops in the experiment"
O07-1013,W02-1011,0,0.0113611,"Missing"
O07-1013,W03-0404,0,0.0259786,"nions have long as well as complex answers which tend to scatter across different documents. Traditional QA approaches are not effective enough to retrieve answers for opinion questions as they have been for factual questions (Stoyanov et al., 2005). Hence, an opinion QA system is essential and urgent. Most of the research on QA systems has been developed for factual questions, and the association of subjective information with question answering has not yet been much studied. As for subjective information, Wiebe (2000) proposed a method to identify strong clues of subjectivity on adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective expressions. Kim and Hovy (2004) presented a system to determine word sentiments and combined sentiments within a sentence. Pang, Lee, and Vaithyanathan (2002) classified documents not by the topic, but by the overall sentiment, and then determined the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a"
O07-1013,H05-1116,0,0.290007,"ames Dean born?” and “Who won the Nobel Peace Prize in 1991?”. In addition to facts, people would also like to know about others’ opinions, thoughts, and feelings toward some specific topics, groups, and events. Opinion questions (e.g. “How do Americans consider the US-Iraq war?” and “What are the public’s opinions on human cloning?”) revealing answers about people’s opinions have long as well as complex answers which tend to scatter across different documents. Traditional QA approaches are not effective enough to retrieve answers for opinion questions as they have been for factual questions (Stoyanov et al., 2005). Hence, an opinion QA system is essential and urgent. Most of the research on QA systems has been developed for factual questions, and the association of subjective information with question answering has not yet been much studied. As for subjective information, Wiebe (2000) proposed a method to identify strong clues of subjectivity on adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective ex"
O07-1013,H05-1044,0,0.0403167,"adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective expressions. Kim and Hovy (2004) presented a system to determine word sentiments and combined sentiments within a sentence. Pang, Lee, and Vaithyanathan (2002) classified documents not by the topic, but by the overall sentiment, and then determined the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a phrase-level sentiment analysis to automatically identify the contextual polarity. Ku et al. (2006) proposed a method to automatically mine and organize opinions from heterogeneous information sources. Some research has gone from opinion analysis in texts toward that in QA systems. Cardie et al. (2003) took advantage of opinion summarization to support Multi-Perspective Question Answering (MPQA) system which aims to extract opinion-oriented information of a question. Yu and Hatzivassiloglou (2003) separated opinions from facts, at both the document and sentence levels. They intend"
O07-1013,W03-1017,0,0.291137,"ned the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a phrase-level sentiment analysis to automatically identify the contextual polarity. Ku et al. (2006) proposed a method to automatically mine and organize opinions from heterogeneous information sources. Some research has gone from opinion analysis in texts toward that in QA systems. Cardie et al. (2003) took advantage of opinion summarization to support Multi-Perspective Question Answering (MPQA) system which aims to extract opinion-oriented information of a question. Yu and Hatzivassiloglou (2003) separated opinions from facts, at both the document and sentence levels. They intended to cluster opinion sentences from the same perspective together and summarize them as answers to opinion questions. Kim and Hovy (2005) identified opinion holders, which are frequently asked in opinion questions. This paper deals with two major problems in opinion QA systems: question analysis and answer passage retrieval. Several issues, including how to separate opinion questions from factual ones, how to define question types for opinion questions, how to correctly classify opinion questions into corresp"
O08-5003,C04-1200,0,0.109551,"Missing"
O08-5003,W02-1011,0,0.0107734,"Missing"
O08-5003,W03-1014,0,0.0736729,"Missing"
O08-5003,W03-0404,0,0.0451141,"Missing"
O08-5003,H05-1116,0,0.0355929,"Missing"
O08-5003,W03-1017,0,0.147762,"Missing"
O08-5003,H05-1044,0,\N,Missing
O09-1008,C04-1200,0,0.155952,"Missing"
O09-1008,W06-0301,0,0.0471135,"Missing"
O09-1008,H05-1045,0,0.0476198,"Missing"
O09-6003,H05-1045,0,0.0951735,"Missing"
O09-6003,C04-1200,0,0.0682816,"Missing"
O09-6003,W06-0301,0,0.0450974,"Missing"
O15-3001,Q13-1005,0,0.0366724,"Missing"
O15-3001,J80-2001,0,0.282248,"Missing"
O15-3001,I05-7001,0,0.0671535,"Missing"
O15-3001,C02-1049,1,0.531032,"Missing"
O15-3001,J14-1002,0,0.0592436,"Missing"
O15-3001,D09-1122,0,0.0349179,"Missing"
O15-3001,D14-1058,0,0.0703962,"Missing"
O15-3001,D14-1100,0,0.0359364,"Missing"
O15-3001,O07-4005,0,0.0225363,"Missing"
O15-3001,O15-3002,1,0.871096,"Missing"
O15-3001,P14-1026,0,0.0768712,"Missing"
O15-3001,W03-1726,1,0.635445,"Missing"
O15-3001,P01-1052,0,0.14657,"Missing"
O15-3001,strassel-etal-2010-darpa,0,0.425543,"Missing"
O15-3001,O04-2005,0,0.102625,"Missing"
O15-3001,W02-1811,0,0.0479075,"Missing"
O15-3001,D09-1082,0,0.0271275,"Missing"
O15-3001,Q15-1001,0,\N,Missing
O15-3001,A83-1031,0,\N,Missing
O15-3001,D08-1097,0,\N,Missing
O15-3001,W14-2401,0,\N,Missing
O15-3001,D13-1160,0,\N,Missing
O15-3001,O14-3003,0,\N,Missing
O15-3001,O15-1007,1,\N,Missing
P07-2023,W02-1011,0,0.014459,"Missing"
P12-3017,W10-0201,0,0.0224344,"to use sensors to watch the heart beat and the body temperature of residents to know their current emotion for further applications, but the problem was that users had to wear sensors and it was inconvenient. Instead of watching body signals, we postulate that the communications among people is one of the important factors to influence their emotions. Therefore, we tried to find clues from the textual conversations of the residents in order to detect their psychological state. There are many ways to categorize emotions. Different emotion states were used for experiments in previous research (Bellegarda, 2010). To find suitable categories of emotions, we adopted the three-layered emotion hierarchy proposed by Parrott (2001)1. Six emotions are in the first layer, including love, joy, surprise, anger, sadness and fear. The second layer includes 25 emotions, and the third layer includes 135 emotions. Using this hierarchical classification benefits the system. We can categorize emotions from rough to fine granularities and degrade to the upper level when the experimental materials are insufficient. How to map categories in other researches to ours becomes clearer, and annotators have more information w"
P14-5001,D10-1036,0,0.108076,"al leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; … (Chinese) 義肢: 0.41; 物理治療師: 0.15; 康復:0.10; 阿富汗: 0.08, … English Keywords from Bilingual Perspectives: prosthesis, artificial, leg, rehabilitation, orthopedic, … Figure 1. An example BiKEA keyword analysis for an article. accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by automated systems, and that our PageRank graphs are built and connected bilingually. In contrast to the previous research in keyword extraction, we present a system that automatically learns topical keyword preferences and constructs and inter-connects PageRank graphs in bilingual context, expected to yield better and more accurate keyword lists for articles. To the b"
P14-5001,W03-1726,0,0.0163731,"Missing"
P14-5001,I13-1117,1,0.809982,"我們只是給他們 提供義肢。 花了很多年的程序 才讓這計劃成為現在的模樣。… Word Alignment Information: physical (物理), therapist (治療師), social (社會), reintegration (重返), physical (身體), rehabilitation (康 復), prosthesis (義肢), … Scores of Topical Keyword Preferences for Words: (English) prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; … (Chinese) 義肢: 0.41; 物理治療師: 0.15; 康復:0.10; 阿富汗: 0.08, … English Keywords from Bilingual Perspectives: prosthesis, artificial, leg, rehabilitation, orthopedic, … Figure 1. An example BiKEA keyword analysis for an article. accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by automated systems, and that our PageRank graphs are built and connected bilingually. In contrast to"
P14-5001,J00-2011,0,0.145924,"idging is to take language divergence into account and to allow for language-wise interaction over word statistics. BiKEA, then in bilingual context, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into"
P14-5001,J03-1002,0,0.00588597,"to [1/v,1/ v, …,1/v] repeat Figure 4. Constructing PageRank word graph. Step (3) in Figure 3 linearly combines word graphs EWe and EWc using α. We use α to balance language properties or statistics, and BiKEA backs off to monolingual KEA if α is one. In Step (4) of Figure 3 for each word alignment (wic, wje), we construct a link between the word nodes with the weight BiWeight. The inter-language link is to reinforce language similarities and respect language divergence while the weight aims to elevate the crosslanguage statistics interaction. Word alignments are derived using IBM models 1-5 (Och and Ney, 2003). The inter-language link is directed from wic to wje, basically from language c to e based on the directional word-aligning entry (wic, wje). The bridging is expected to help keyword extraction in language e with the statistics in language c. Although alternative approach can be used for bridging, our approach is intuitive, and most importantly in compliance with the directional spirit of PageRank. Step (6) sets KP of keyword preference model using topical preference scores learned from Section 3.2, while Step (7) initializes KN of PageRank scores or, in our case, word keyness scores. Then we"
P14-5001,N03-1017,0,0.00743078,"Missing"
P14-5001,H05-1059,0,0.0146916,"Missing"
P14-5001,N10-1101,0,0.0307704,"text, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to 3 The BiKEA System Submitting natural language articles t"
P14-5001,W08-1404,0,0.0608117,"Missing"
P15-4009,W09-1206,0,0.067147,"Missing"
P15-4009,W09-1207,0,0.058007,"Missing"
P15-4009,W05-0632,0,0.0321345,"est in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attempting different ways to handle data sparseness Introduction Semantic roles are utilized to find concepts automatically and assure their meaningfulness. Semantic role labeling is a research problem which finds in a given sentence the predicates and their arguments (identification), and further labels the semantic relationship between predicates and arguments, that is, their semantic roles (classification). There are several labeling sets. Researchers have widely adopted the semantic role labels defined in Propbank (Bonial et al., 2010) like predicate (PRED), numbered arguments 0 t"
P15-4009,J02-3001,0,0.859005,"serve as features in other NLP tasks to provide semantically related cues and potentially help in related research problems. We also show that it is easy to generate a different language version of this system by actually building an English system which performs satisfactory. 1 2 Related Work Previous studies related to this work can be divided into two groups: semantic role labeling and concept extraction. Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attempting different ways to handle data sparseness Introduction Semantic roles are utilized to find concepts automatically and assure"
P15-4009,N04-1030,0,0.0468889,"as sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attempting different ways to handle data sparseness Introduction Semantic roles are utilized to find concepts automatically and assure their meaningfulness. Semantic role labeling is a research problem which finds in a given sentence the predicates and their arguments (identification), and further labels the semantic relationship between predicates and arguments, that is, their semantic roles (classification). There are several labeling sets. Researchers have widely adopted the semantic role labels defined in Propbank (Bonial et al., 2010) like predicate (PRED), nu"
P15-4009,D09-1099,0,0.0191676,"a knowledge base, its construction process is quite different from the work described here of automatically extracting concepts from sentences. (Zapirain, Agirre, and Màrquez 2007). Moreover, some researchers have tried to extend it in novel ways. For example, Ding and Chang (2008) used a hierarchical feature selection strategy, while Jiang, Li, and Ng (2005) proposed exploiting argument interdependence, that is, the fact that the semantic role of one argument can depend on the semantic roles of other arguments. Many researchers have tried to extract concepts from texts (Gelfand et al., 1998; Hovy et al., 2009; Villalon and Calvo, 2009; Dinh and Tamine, 2011; Torii et al., 2011). Hovy narrowed the domain of interest into concepts “below” a given seed term. Villalon and Calvo extract concepts from student essays for concept map mining, which generates a directed relational graph of the extracted concepts in an essay. For specific domains, biological or medical concepts are of greatest interest to researchers (Jonnalagadda et al., 2011). Two relatively new and related approaches are the Concept parser (Rajagopal et al. 2013), a part of the SenticNet project (Cambria, Olsher, and Rajagopal 2014) and C"
P15-4009,W00-1205,0,0.0793549,"tically and assure their meaningfulness. Semantic role labeling is a research problem which finds in a given sentence the predicates and their arguments (identification), and further labels the semantic relationship between predicates and arguments, that is, their semantic roles (classification). There are several labeling sets. Researchers have widely adopted the semantic role labels defined in Propbank (Bonial et al., 2010) like predicate (PRED), numbered arguments 0 to 5 (ARG0, ARG1, ARG2, ARG3, ARG4, ARG5), or modifier arguments (ARGM-X); finer labels are those defined in Sinica Treebank (Huang et al., 2000) like agent, theme, target, which are labeled on each 49 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 49–54, c Beijing, China, July 26-31, 2015. 2015 ACL and AFNLP tic knowledge base is used to express a concept in all its different forms and their concept-parser does not use any semantic knowledge during decomposition. The latter is a semantic network based on the Open Mind Common Sense (OMCS) knowledge base. As it is a knowledge base, its construction process is quite different from the work described here of automatically extracting concepts from sentences. (Zapirain, Agirre,"
P15-4009,D07-1002,0,0.0405741,"ature engineering process. Concepts are further extracted according to templates formulated by the labeled semantic roles to serve as features in other NLP tasks to provide semantically related cues and potentially help in related research problems. We also show that it is easy to generate a different language version of this system by actually building an English system which performs satisfactory. 1 2 Related Work Previous studies related to this work can be divided into two groups: semantic role labeling and concept extraction. Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attemp"
P15-4009,C08-1050,0,0.171584,"he current node. This also conforms to the rule that when a role is taken by the other argument, it is less likely that the current argument is of the same role. We implement this idea using the dual-layer classification framework shown in Figure 3. 3 System The proposed system includes three major components: a syntactic parser, a semantic role labeler, and a concept formulation component. The framework is shown in Figure 1. The input sentence is first transformed into a syntactic parse tree through a syntactical analysis step that almost all automatic semantic role labeling systems require (Johansson and Nugues 2008). Here the Stanford parser (Klein and Manning 2003) is utilized. Figure 2 shows the system interface. The left part is the English system and the right part is the Chinese system. After users input a sentence, the system will automatically parse, label semantic roles and report the related concepts for it. 3.1 Semantic Role Labeling To develop a SRL system, a total of 33 features including features related to the head word related features, target word related features, grammar related features, and semantic type related features, are collected from related work (Xue, 2008; Ding and Chang, 200"
P15-4009,W09-1216,0,0.0423565,"Missing"
P15-4009,D08-1008,0,0.146367,"he current node. This also conforms to the rule that when a role is taken by the other argument, it is less likely that the current argument is of the same role. We implement this idea using the dual-layer classification framework shown in Figure 3. 3 System The proposed system includes three major components: a syntactic parser, a semantic role labeler, and a concept formulation component. The framework is shown in Figure 1. The input sentence is first transformed into a syntactic parse tree through a syntactical analysis step that almost all automatic semantic role labeling systems require (Johansson and Nugues 2008). Here the Stanford parser (Klein and Manning 2003) is utilized. Figure 2 shows the system interface. The left part is the English system and the right part is the Chinese system. After users input a sentence, the system will automatically parse, label semantic roles and report the related concepts for it. 3.1 Semantic Role Labeling To develop a SRL system, a total of 33 features including features related to the head word related features, target word related features, grammar related features, and semantic type related features, are collected from related work (Xue, 2008; Ding and Chang, 200"
P15-4009,P03-1054,0,0.00481494,"hen a role is taken by the other argument, it is less likely that the current argument is of the same role. We implement this idea using the dual-layer classification framework shown in Figure 3. 3 System The proposed system includes three major components: a syntactic parser, a semantic role labeler, and a concept formulation component. The framework is shown in Figure 1. The input sentence is first transformed into a syntactic parse tree through a syntactical analysis step that almost all automatic semantic role labeling systems require (Johansson and Nugues 2008). Here the Stanford parser (Klein and Manning 2003) is utilized. Figure 2 shows the system interface. The left part is the English system and the right part is the Chinese system. After users input a sentence, the system will automatically parse, label semantic roles and report the related concepts for it. 3.1 Semantic Role Labeling To develop a SRL system, a total of 33 features including features related to the head word related features, target word related features, grammar related features, and semantic type related features, are collected from related work (Xue, 2008; Ding and Chang, 2008; Sun and Jurafsky 2004; Gildea and Jurafsky 2002)"
P15-4009,W04-3212,0,0.0537736,"role labeling and concept extraction. Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attempting different ways to handle data sparseness Introduction Semantic roles are utilized to find concepts automatically and assure their meaningfulness. Semantic role labeling is a research problem which finds in a given sentence the predicates and their arguments (identification), and further labels the semantic relationship between predicates and arguments, that is, their semantic roles (classification). There are several labeling sets. Researchers have widely adopted the semantic r"
P15-4009,C10-1081,0,0.0242651,"ss. Concepts are further extracted according to templates formulated by the labeled semantic roles to serve as features in other NLP tasks to provide semantically related cues and potentially help in related research problems. We also show that it is easy to generate a different language version of this system by actually building an English system which performs satisfactory. 1 2 Related Work Previous studies related to this work can be divided into two groups: semantic role labeling and concept extraction. Semantic role labeling (SRL) has sparked much interest in NLP (Shen and Lapata, 2007; Liu and Gildea, 2010). The first automatic SRL systems were reported by Gildea and Jurafsky in 2002 (Gildea and Jurafsky 2002); since then, their ideas have dominated the field. In their approach, they emphasize the selection of appropriate lexical and syntactical features for SRL, the use of statistical classifiers and their combinations, and ways to handle data sparseness. Many researchers have tried to build on their work by augmenting and/or altering the feature set (Xue 2004), by experimenting with various classification approaches (Pradhan et al. 2004; Park and Rim 2005), and by attempting different ways to"
P15-4009,J08-2004,0,0.0751377,"Missing"
P15-4009,S07-1077,0,0.0605293,"Missing"
P15-4009,W09-1213,0,0.0631047,"Missing"
W14-5905,P10-2050,0,0.14546,"y. Aspect extraction can be seen as a general information extraction problem, for which techniques based on sequential labeling are generally used. The most popular methods in this context, in particular, are Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Jin and Ho (Jin and Ho, 2009) used a lexicalized HMM for joint extraction of opinions along with their explicit aspects. Niklas and Gurevych (Niklas and Gurevych, 2010) used CRF to extract explicit aspects in a custom corpus with data of different domains. Li et al. (Li et al., 2010), Choi and Cardie (Choi and Cardie, 2010) and Huang et al. (Huang et al., 2012) also used CRF for extraction of explicit aspects. As to the implicit aspects, the OPINE extraction system developed by Popescu and Etzioni (Popescu and Etzioni, 2005) was the first that leveraged on the extraction of this type of aspects to improve polarity classification. However, their system is not described in detail and is not publicly available. To the best of our knowledge, all existing methods for implicit aspect extraction are based on the use, in one or another way, of what we term IAC. Su (Su et al., 2008) proposed a clustering method to map IA"
W14-5905,C10-1074,0,0.0162805,"cted by their method were very noisy. Aspect extraction can be seen as a general information extraction problem, for which techniques based on sequential labeling are generally used. The most popular methods in this context, in particular, are Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Jin and Ho (Jin and Ho, 2009) used a lexicalized HMM for joint extraction of opinions along with their explicit aspects. Niklas and Gurevych (Niklas and Gurevych, 2010) used CRF to extract explicit aspects in a custom corpus with data of different domains. Li et al. (Li et al., 2010), Choi and Cardie (Choi and Cardie, 2010) and Huang et al. (Huang et al., 2012) also used CRF for extraction of explicit aspects. As to the implicit aspects, the OPINE extraction system developed by Popescu and Etzioni (Popescu and Etzioni, 2005) was the first that leveraged on the extraction of this type of aspects to improve polarity classification. However, their system is not described in detail and is not publicly available. To the best of our knowledge, all existing methods for implicit aspect extraction are based on the use, in one or another way, of what we term IAC. Su (Su et al., 200"
W14-5905,D10-1101,0,0.16677,"natural language text. However, their method seems to be very inaccurate in terms of precision as the retrieved aspects extracted by their method were very noisy. Aspect extraction can be seen as a general information extraction problem, for which techniques based on sequential labeling are generally used. The most popular methods in this context, in particular, are Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Jin and Ho (Jin and Ho, 2009) used a lexicalized HMM for joint extraction of opinions along with their explicit aspects. Niklas and Gurevych (Niklas and Gurevych, 2010) used CRF to extract explicit aspects in a custom corpus with data of different domains. Li et al. (Li et al., 2010), Choi and Cardie (Choi and Cardie, 2010) and Huang et al. (Huang et al., 2012) also used CRF for extraction of explicit aspects. As to the implicit aspects, the OPINE extraction system developed by Popescu and Etzioni (Popescu and Etzioni, 2005) was the first that leveraged on the extraction of this type of aspects to improve polarity classification. However, their system is not described in detail and is not publicly available. To the best of our knowledge, all existing methods"
W14-5905,H05-1043,0,0.347128,"xplains the features used for the labeler; Section 4 discusses novelty of the proposed methodology; Section 5 describes in detail the aspect extraction approach and results of the experimental evaluation; finally, Section 6 concludes the paper. 2 Related Work Aspect extraction from opinionated text was first studied by Hu and Liu (Hu and Liu, 2004), who also introduced the distinction between explicit and implicit aspects. However, the authors only dealt with explicit aspects by adopting a set of rules based on statistical observations. Hu and Liu’s method was improved by Popescu and Etzioni (Popescu and Etzioni, 2005) and by Blair-Goldensonh (Blair-Goldensohn et al., 2008). Popescu and Etzioni assumed the product class to be known as priori. Their algorithm detects whether a noun or noun phrase is a product feature or not by computing PMI between the noun phrase and the product class. Scaffidi et al. (Scaffidi et al., 2007) presented a method that uses a language model to identify product features. They assumed that product features are more frequent in product reviews than in general natural language text. However, their method seems to be very inaccurate in terms of precision as the retrieved aspects ext"
W14-5905,J11-1002,0,0.460389,"aspect and t has noun compound modifier h, then the aspect h-t is extracted and t is removed from the aspect list. In (12), as chicken and casserole are in noun compound modifier relation, only chicken casserole is extracted as an aspect. (12) We ordered the chicken casserole, but what we got were a few small pieces of chicken, all dark meat and on the bone. 33 4 Novelty of the proposed work First of all, the proposed method is fully unsupervised and depends on the accuracy of the dependency parser and the opinion lexicon, rather then a training corpus and supervised learning accuracy. Only (Qiu et al., 2011) follow an unsupervised learning approach but the proposed method uses an enhanced set of rules and opinion lexicon. The proposed method also outperforms (Qiu et al., 2011) on the same dataset they used. Implicit aspects extracted through the proposed method differ from implicit aspect expressions defined by Liu (Liu, 2012) as “aspect expressions that are not nouns or noun phrases” in that implicit aspects extracted by the proposed algorithm semantically refer to the values of the pre-defined aspects, irrespective of their own surface POS. Below are listed some examples where the implicit aspe"
W14-5905,H05-2017,0,\N,Missing
W15-0617,P05-1074,0,0.055453,"otion classification, word suggestion and writing assessment. The aim of paraphrasing research is how to express the same information in various ways. Such alternative expressions of the same information rely on paraphrase pairs which map an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without changing the sentence’s original meaning is the main concern. For RESOLVE, in contr"
W15-0617,N03-1003,0,0.0502248,"Missing"
W15-0617,P01-1008,0,0.0436142,"s studies related to RESOLVE can be divided into four groups: paraphrasing, emotion classification, word suggestion and writing assessment. The aim of paraphrasing research is how to express the same information in various ways. Such alternative expressions of the same information rely on paraphrase pairs which map an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without"
W15-0617,D08-1021,0,0.0114439,"ion and writing assessment. The aim of paraphrasing research is how to express the same information in various ways. Such alternative expressions of the same information rely on paraphrase pairs which map an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without changing the sentence’s original meaning is the main concern. For RESOLVE, in contrast, rather than attem"
W15-0617,W12-2009,1,0.864155,"ment. The aim of paraphrasing research is how to express the same information in various ways. Such alternative expressions of the same information rely on paraphrase pairs which map an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without changing the sentence’s original meaning is the main concern. For RESOLVE, in contrast, rather than attempting preservation,"
W15-0617,J90-1003,0,0.544789,"Missing"
W15-0617,de-marneffe-etal-2006-generating,0,0.0104659,"Missing"
W15-0617,C10-2029,0,0.0285243,"ap an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without changing the sentence’s original meaning is the main concern. For RESOLVE, in contrast, rather than attempting preservation, the focus is on appropriate in-context word substitution. There are several online paraphrasing tools. PREFER1 (Chen et al., 2012) is an online paraphrase reference interface that gene"
W15-0617,W12-3711,0,0.0212979,"entences, aiming at reducing wordiness and clarifying vague or imprecise terms. In short, rather than offering better suggestions, paraphrasing tools provide equivalent expressions. Emotion classification concerns approaches to detect the underlying emotion of a text. Related work typically attempts this using classifiers. These classifiers are trained with features such as n-grams (Tokuhisa et al., 2008), word-level pointwise mutual information (PMI) values (Agrawal et al., 2012; Bullinaria et al., 2007; and Church et al., 1990) or a combination of word POS and sentence dependency relations (Ghazi et al., 2012). The remained works of emotion classification in above mentioned research to deal with emotions aroused by events inspires us to relate events to emotion words in RESOLVE. In addition, in terms of emotion classification, RESOLVE classifies texts into fine-grained classes where each emotion word can be viewed as a single class; in contrast, most emotion classification work focuses only on coarse-grained (6 to 10 classes) emotion labeling. It is a challenging work. Word suggestion involves guessing a possible replacement for a given word in a sentence, or finding word collocations. A representa"
W15-0617,W10-0205,0,0.0179901,"can be divided into four groups: paraphrasing, emotion classification, word suggestion and writing assessment. The aim of paraphrasing research is how to express the same information in various ways. Such alternative expressions of the same information rely on paraphrase pairs which map an expression to a previously learned counterpart, or inference rules that re-structure the original sentences. Most work uses machine translation techniques such as statistical machine translation or multiple-sequence alignment to extract paraphrase pairs from monolingual corpora (Barzilay and McKeown, 2001; Keshtkar and Inkpen, 2010), or bilingual corpora (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Chen et al., 2012). Approaches based on inference rules, on the other hand, derive these rules by analyzing the dependency relations of paraphrase sentences (Lin and Pantel, 2001; Dinu and Lapata, 2010). Alternative expressions can be achieved by applying inference rules to rephrase the original sentence. In general, the focus of paraphrasing is sentence variation, which involves sentence re-structuring, phrase alternation and word substitution. Generating an alternative sentence without changing the sentence’s ori"
W15-0617,P03-1054,0,0.00536331,"y, dislike, distaste, hatred and loathing. Figure 1: RESOLVE framework. 3.1 Stage One: Learning Corpus Patterns for All Emotion Words In this stage, we learn patterns and their relations to emotion words from the corpus. Sentences are first pre-processed, after which patterns are extracted from the corpus and their emotion event scores calculated. Pre-processing. As compound sentences can be associated with more than one emotion event, they must be pre-processed before we extract patterns. Compound sentences are first broken into clauses according to the Stanford phrase structure tree output (Klein and Manning, 2003). In the experiments, these clauses are treated as sentences. Pattern Extraction. Emotion events are characterized by verb-phrase patterns, derived from the output of the Stanford dependency parser (De Marneffe et al., 2006). This parser generates the grammatical relations of word pairs and determines the ROOT, which is often the verb, after parsing each sentence. We describe the extraction steps given the sentence “We gave the poor girl a new book.”. A total of 746,919 patterns were extracted in this process. Step1: Identify the ROOT (gave) and all its dependents based on the parsing result."
W15-0617,strapparava-valitutti-2004-wordnet,0,0.0766756,"nce to these patterns, CTP estimates how often a pattern transits among emotion categories and adjusts its score accordingly in equation (11), where m is the number of categories in each level; c is the emotion category. High-CTP patterns appear in more emotion categories or are evenly distributed among emotion categories and are hence less representative. Note that categories in lower levels (for instance level 1) are less similar, and transitions among these make patterns less powerful. l p 4 4.1 c l p Experiment Emotion Words and Corpus The corpora used in this study include WordNetAffect (Strapparava and Valitutti, 2004), WordNetSynset (Fellbaum, 1999), Merriam Webster Dictionary, and Vocabulary.com. The WordNetAffect emotion list contains 1,113 emotion terms categorized into six major emotion categories: anger, disgust, fear, joy, sadness, and surprise (Ekman, 1993). 113 of the 1,113 terms were excluded because they were emotion phrases as opposed to words; thus a total of 1,000 emotion words were collected. Then, to increase coverage, synonyms of these 1,000 emotion words from WordNetSynset and Merriam Webster Dictionary were included. Thus we compiled a corpus with 3,785 emotion words. For each of these 3,"
W15-0617,C08-1111,0,0.0194557,"er, we know of no study that evaluates learning effectiveness when using MCT. Finally, SPIDER (Barreiro, 2011) targets document-level editing; it relies on derivations from dictionaries and grammars to paraphrase sentences, aiming at reducing wordiness and clarifying vague or imprecise terms. In short, rather than offering better suggestions, paraphrasing tools provide equivalent expressions. Emotion classification concerns approaches to detect the underlying emotion of a text. Related work typically attempts this using classifiers. These classifiers are trained with features such as n-grams (Tokuhisa et al., 2008), word-level pointwise mutual information (PMI) values (Agrawal et al., 2012; Bullinaria et al., 2007; and Church et al., 1990) or a combination of word POS and sentence dependency relations (Ghazi et al., 2012). The remained works of emotion classification in above mentioned research to deal with emotions aroused by events inspires us to relate events to emotion words in RESOLVE. In addition, in terms of emotion classification, RESOLVE classifies texts into fine-grained classes where each emotion word can be viewed as a single class; in contrast, most emotion classification work focuses only"
W15-0617,S07-1036,0,0.0135866,"ddition, in terms of emotion classification, RESOLVE classifies texts into fine-grained classes where each emotion word can be viewed as a single class; in contrast, most emotion classification work focuses only on coarse-grained (6 to 10 classes) emotion labeling. It is a challenging work. Word suggestion involves guessing a possible replacement for a given word in a sentence, or finding word collocations. A representative research task for word suggestion is the SemEval 2007 English Lexical Substitution task: the problem is to find a word substitute for the designated word given a sentence. Zhao et al. (2007) first uses rules to find possible candidates from WordNet and verifies the sentence after substitution using Web search results; Dahl et al. (2007) utilizes a more traditional n-gram model but uses statistics from web 5-grams. Although closely related to our work, this task is different in several ways. First, the word for which a substitute is required is already an appropriate word, as it appears in a sentence from a well-written English corpus, the Internet Corpus of English3. However, the goal of our work is to determine whether a word selected by ESL learners is appropriate, and if neces"
W15-0617,S07-1066,0,\N,Missing
W16-3910,D12-1032,0,0.0585341,"Missing"
W16-3910,W15-4319,0,0.0315765,"Missing"
W16-3910,N03-1003,0,0.024281,"on encountering the many non-standard words, found actually to be unknown named entities, they were obliged to manually eliminate these while normalizing the dataset. Thus, we believe that named entity matching (NEM) is an important problem in noisy text analysis. NEM is the task of matching the different alias names for entities back to their respective formal names. Many applications benefit from this technique, including name transliteration (Knight and Graehl, 1998), entity linking (Rao et al., 2013), entity clustering (Green et al., 2012), and entity identification for paraphrase mining (Barzilay and Lee, 2003). This task is different from most in noisy text normalization. While most work on normalization includes two parts—informal word identification and word recovery—this work is different in two ways. First, we focus on the recovery of informal names of named entities, in contrast to the general typo and abbreviation recovery. Second, the identification of these aliases is a named entity recognition (NER) problem, while conventional normalization works search for every OOV words. The challenges of NEM lie in variation, which can be attributed to many factors: nicknames, acronyms, and differences"
W16-3910,N12-1007,0,0.175466,"LP) tasks (Sproat et al., 2001). According to (Baldwin et al., 2015), on encountering the many non-standard words, found actually to be unknown named entities, they were obliged to manually eliminate these while normalizing the dataset. Thus, we believe that named entity matching (NEM) is an important problem in noisy text analysis. NEM is the task of matching the different alias names for entities back to their respective formal names. Many applications benefit from this technique, including name transliteration (Knight and Graehl, 1998), entity linking (Rao et al., 2013), entity clustering (Green et al., 2012), and entity identification for paraphrase mining (Barzilay and Lee, 2003). This task is different from most in noisy text normalization. While most work on normalization includes two parts—informal word identification and word recovery—this work is different in two ways. First, we focus on the recovery of informal names of named entities, in contrast to the general typo and abbreviation recovery. Second, the identification of these aliases is a named entity recognition (NER) problem, while conventional normalization works search for every OOV words. The challenges of NEM lie in variation, whi"
W16-3910,P11-1038,0,0.0214779,", most of those canonical technologies fail to provide accurate information in usergenerated text. Normalization on noisy text is also a highly debated topic. Most words must be normalized in this task; this includes abbreviations, non-standard spellings, and phrases prevalent in social media. Rather than considering all parts of speech, our task focuses on aliases of name entities, especially person entities. Among those different tasks, some useful metrics can be applied to NEM given appropriate refinements. Phonetic similarity is frequently applied in normalization tasks (Li and Liu, 2012; Han and Baldwin, 2011; Xu et al., 2015). Choudhury et al. (2007) represented both non-formal words and formal words in phonemics and word pairs are evaluated on the phonetic level using left-to-right HMM model. Peng et al. (2015) propose a name matching system with a linear SVM model featuring string match methods. They apply string matching at both the character and phonemic levels. Their experiments show that adding phonetic features does improve performance. Another assumption is that aliases contain strong contextual similarity with normal words. Liu et al. (2012) measures the cosine similarity of the word TF-"
W16-3910,1985.tmi-1.17,0,0.232592,"cess Given λ0j , we use the Viterbi algorithm (Rabiner, 1989) to find the most likely sequences of states through the HMM for each observation (i.e., aliases in the training set) vij for wj . We keep count of every transition and emission in the HMM, which is increased by fij (the frequency of the variant vij ) if and only if the particular transition or emission is used in the Viterbi path of vij . Once the process is completed for all the variations of wj , we re-estimate the model parameters of the HMM based on the final counts associated with each transition and emission. Add-α smoothing (Harris, 1985) is used to handle zero counts. We denote this re-estimated model for wj by αj∗ . It is possible that we lack the training data to train word models for some politician names; for these politicians, we use the initial parameters to predict the probability. 5 Fully Connected HMM The main disadvantage of the left-to-right HMM is that it fails on prediction when there is no sequence of similar character matches between the alias and formal name; this phenomenon is more common in this task than in canonical normalization. For example, “菊姐” [Ju2 Jie3] is the alias of “陳菊” [Chen2 Ju2]. However, for"
W16-3910,P14-3012,0,0.0171767,"). Choudhury et al. (2007) represented both non-formal words and formal words in phonemics and word pairs are evaluated on the phonetic level using left-to-right HMM model. Peng et al. (2015) propose a name matching system with a linear SVM model featuring string match methods. They apply string matching at both the character and phonemic levels. Their experiments show that adding phonetic features does improve performance. Another assumption is that aliases contain strong contextual similarity with normal words. Liu et al. (2012) measures the cosine similarity of the word TF-IDFs in context. Li and Liu (2014) utilize word embeddings in their work. 3 Phonetic Similarity Most Chinese characters can serve as meaningful words. Each Chinese character contains only one syllable, and each syllable is composed of an initial, a final and a tone. Initial refers to the first part of the Chinese syllable, which is a consonant. Final refers to the second part of the syllable, which is a vowel with an optional nasal sound. While some Chinese syllables have a null initial which is denoted as ∅, each syllable has a final. Mandarin Chinese is a tonal language. There are four main tones and one neutral tone, each o"
W16-3910,P11-2013,0,0.174632,"antic relations to the alias. In this work we also introduce two new datasets consisting of 167 phonemic pairs and 279 mixed pairs of aliases and formal names. Experimental results show that the proposed approach models both phonemic and semantic information and outperforms previous work on both the phonemic and mixed datasets with the best top-1 accuracies of 0.78 and 0.59 respectively. 1 Introduction Due to the casual nature of social media, there exist a large number of non-standard words in text expressions which make it substantially different from formal written text. It is reported in (Liu et al., 2011b) that more than four million distinct out-of-vocabulary (OOV) tokens occur in the Edinburgh Twitter corpus (Petrovic et al., 2010). This variation poses challenges for natural language processing (NLP) tasks (Sproat et al., 2001). According to (Baldwin et al., 2015), on encountering the many non-standard words, found actually to be unknown named entities, they were obliged to manually eliminate these while normalizing the dataset. Thus, we believe that named entity matching (NEM) is an important problem in noisy text analysis. NEM is the task of matching the different alias names for entitie"
W16-3910,P12-1109,0,0.0201073,"in normalization tasks (Li and Liu, 2012; Han and Baldwin, 2011; Xu et al., 2015). Choudhury et al. (2007) represented both non-formal words and formal words in phonemics and word pairs are evaluated on the phonetic level using left-to-right HMM model. Peng et al. (2015) propose a name matching system with a linear SVM model featuring string match methods. They apply string matching at both the character and phonemic levels. Their experiments show that adding phonetic features does improve performance. Another assumption is that aliases contain strong contextual similarity with normal words. Liu et al. (2012) measures the cosine similarity of the word TF-IDFs in context. Li and Liu (2014) utilize word embeddings in their work. 3 Phonetic Similarity Most Chinese characters can serve as meaningful words. Each Chinese character contains only one syllable, and each syllable is composed of an initial, a final and a tone. Initial refers to the first part of the Chinese syllable, which is a consonant. Final refers to the second part of the syllable, which is a vowel with an optional nasal sound. While some Chinese syllables have a null initial which is denoted as ∅, each syllable has a final. Mandarin Ch"
W16-3910,C14-1061,0,0.016285,"aliases and formal names which contain no graphemic or phonetic similarity. Third, in our experiment, our method outperforms other baselines on both datasets. Below, we describe in Section 2 the related work for the task and in Section 3 the phonetic similarity formula. The implementation of the word model and the proposed refined version are described in Sections 4 and 5. The dataset collection, evaluation, and error analysis are in Section 6. 2 Related Work Entity disambiguation is not an uncommon task. Related work such as entity coreferencing (Soon et al., 2001; Ponzetto and Strube, 2006; Moosavi and Strube, 2014) identifies entities which are mentioned more than once in formal documents. The literature contains extensive discussion on the effects of using segmentation, POS taggers, and semantic role labeling, which have achieved considerably high resolution. However, most of those canonical technologies fail to provide accurate information in usergenerated text. Normalization on noisy text is also a highly debated topic. Most words must be normalized in this task; this includes abbreviations, non-standard spellings, and phrases prevalent in social media. Rather than considering all parts of speech, ou"
W16-3910,P15-2062,0,0.145988,"includes two parts—informal word identification and word recovery—this work is different in two ways. First, we focus on the recovery of informal names of named entities, in contrast to the general typo and abbreviation recovery. Second, the identification of these aliases is a named entity recognition (NER) problem, while conventional normalization works search for every OOV words. The challenges of NEM lie in variation, which can be attributed to many factors: nicknames, acronyms, and differences in transliteration. As a result, exact string matching can yield poor results. It is reported (Peng et al., 2015) that This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 61 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 61–69, Osaka, Japan, December 11 2016. License details: http:// most tokenization errors are caused by named entities in documents. It is difficult to utilize analysis tools which depend on tokenized text. While most normalization can be achieved using features within words, for example, (‘u’, ‘you’) and (‘2mr’, ‘tomorrow’), aliases and formal names may have no graphemic or phonemic connections, f"
W16-3910,W10-0513,0,0.0191621,"airs of aliases and formal names. Experimental results show that the proposed approach models both phonemic and semantic information and outperforms previous work on both the phonemic and mixed datasets with the best top-1 accuracies of 0.78 and 0.59 respectively. 1 Introduction Due to the casual nature of social media, there exist a large number of non-standard words in text expressions which make it substantially different from formal written text. It is reported in (Liu et al., 2011b) that more than four million distinct out-of-vocabulary (OOV) tokens occur in the Edinburgh Twitter corpus (Petrovic et al., 2010). This variation poses challenges for natural language processing (NLP) tasks (Sproat et al., 2001). According to (Baldwin et al., 2015), on encountering the many non-standard words, found actually to be unknown named entities, they were obliged to manually eliminate these while normalizing the dataset. Thus, we believe that named entity matching (NEM) is an important problem in noisy text analysis. NEM is the task of matching the different alias names for entities back to their respective formal names. Many applications benefit from this technique, including name transliteration (Knight and G"
W16-3910,N06-1025,0,0.0415864,"a new method for pairs of aliases and formal names which contain no graphemic or phonetic similarity. Third, in our experiment, our method outperforms other baselines on both datasets. Below, we describe in Section 2 the related work for the task and in Section 3 the phonetic similarity formula. The implementation of the word model and the proposed refined version are described in Sections 4 and 5. The dataset collection, evaluation, and error analysis are in Section 6. 2 Related Work Entity disambiguation is not an uncommon task. Related work such as entity coreferencing (Soon et al., 2001; Ponzetto and Strube, 2006; Moosavi and Strube, 2014) identifies entities which are mentioned more than once in formal documents. The literature contains extensive discussion on the effects of using segmentation, POS taggers, and semantic role labeling, which have achieved considerably high resolution. However, most of those canonical technologies fail to provide accurate information in usergenerated text. Normalization on noisy text is also a highly debated topic. Most words must be normalized in this task; this includes abbreviations, non-standard spellings, and phrases prevalent in social media. Rather than consider"
W16-3910,J01-4004,0,0.0397347,"Second, we propose a new method for pairs of aliases and formal names which contain no graphemic or phonetic similarity. Third, in our experiment, our method outperforms other baselines on both datasets. Below, we describe in Section 2 the related work for the task and in Section 3 the phonetic similarity formula. The implementation of the word model and the proposed refined version are described in Sections 4 and 5. The dataset collection, evaluation, and error analysis are in Section 6. 2 Related Work Entity disambiguation is not an uncommon task. Related work such as entity coreferencing (Soon et al., 2001; Ponzetto and Strube, 2006; Moosavi and Strube, 2014) identifies entities which are mentioned more than once in formal documents. The literature contains extensive discussion on the effects of using segmentation, POS taggers, and semantic role labeling, which have achieved considerably high resolution. However, most of those canonical technologies fail to provide accurate information in usergenerated text. Normalization on noisy text is also a highly debated topic. Most words must be normalized in this task; this includes abbreviations, non-standard spellings, and phrases prevalent in social"
W16-3910,P15-1089,0,0.0130419,"al technologies fail to provide accurate information in usergenerated text. Normalization on noisy text is also a highly debated topic. Most words must be normalized in this task; this includes abbreviations, non-standard spellings, and phrases prevalent in social media. Rather than considering all parts of speech, our task focuses on aliases of name entities, especially person entities. Among those different tasks, some useful metrics can be applied to NEM given appropriate refinements. Phonetic similarity is frequently applied in normalization tasks (Li and Liu, 2012; Han and Baldwin, 2011; Xu et al., 2015). Choudhury et al. (2007) represented both non-formal words and formal words in phonemics and word pairs are evaluated on the phonetic level using left-to-right HMM model. Peng et al. (2015) propose a name matching system with a linear SVM model featuring string match methods. They apply string matching at both the character and phonemic levels. Their experiments show that adding phonetic features does improve performance. Another assumption is that aliases contain strong contextual similarity with normal words. Liu et al. (2012) measures the cosine similarity of the word TF-IDFs in context. L"
W16-3910,P97-1017,0,\N,Missing
W18-3505,L18-1252,1,0.851537,"Missing"
W18-3505,P14-5010,0,0.00438238,"Missing"
W18-3505,D14-1162,0,0.081645,"cessfully reached a similar performance on four emotions on both datasets. 6 6.1 6.4 Data imbalance directly harm the UWA performance. In Table 3, accuracy of minority emotions like anger and sadness are relatively low for SmartDubai and JTML, leading to low UWA performance. In contrast, AR is the only team considering data imbalance in the training process. They achieve balance accuracy on each emotion by applying weighed loss in the loss function , and ultimately obtain the best performance in the EmotionX challenge. Discussion Word Embedding All teams used pre-trained word embedding: GloVe(Pennington et al., 2014) for four teams and fastText(Joulin et al., 2016) for one team. Area66 used GloVe-Tweet which is more related to informal language and the other teams did not mention the pre-trained data in their papers. Using pretrained word embedding can reduce the unseen word issue in the testing phase especially for the relatively small dataset (Friends and EmotionPush only contain ∼ 14,000 utterances, which is small compared to the commonly used datasets for pretraining the embedding.) 6.2 7 Conclusion We have a succesfull dialogue emotion recognition challenge, EmotionX, in SocialNLP 2018. Many research"
W18-3505,D14-1127,0,0.0310202,"(Neutral) Okay! (Joy) Okay! Come on! Let’s go! All right! (Joy) Oh okay, I’ll ﬁx that to. What’s her e-mail address? (Neutral) Rachel! (Anger) All right, I promise. I’ll ﬁx this. I swear. I’ll-I’ll- I’ll-I’ll talk to her. (Non-neutral) Okay! (Anger) Okay. (Neutral) Table 1: “Okay!” of different emotions from Emotionlines dataset. However, with the progress of social media and dialogue systems, especially the online customer services, textual emotion recognition has attracted more attention. In the social media, the hashtag and emoji are widely used and could provide substantial emotion clues(Qadir and Riloff, 2014; Kralj Novak et al., 2015). For the dialogue systems, instant emotion detection could help costumer service notice dissatisfaction of clients. Still, textual emotion recognition needs further exploration in dialogue systems for many reasons. For instance, a text segment can express various emotions given different context. Take the dialogue from Hsu et al.(2018) in Table 1 as an example, Okay! could be joy or anger in different scenarios. One more reason is that informal language and short sentence are everywhere in daily conversation. For instance, lol actually means laugh out loud. Therefor"
W19-4447,C92-1019,0,0.707455,"ntences, they did try to translate in other words in the post-test, which results in the unstable scores of global grammar that are less relevant to the near-synonym recognition but to the translation instead. 466 lel Text (LDC2005T10) and Hong Kong Parallel Text (LDC2004T08), that is, a total of 2,682,129 English-Chinese sentence pairs – are utilized to learn the word alignment between L1 and L2 parallel sentences. To align example sentences from Vocabulary, first they were all translated into Traditional Chinese using Googletrans4 . Then we used NLTK5 to tokenize English sentences and CKIP (Chen and Liu, 1992) to segment Chinese sentences respectively. After that, the word alignment model GIZA++ (Och and Ney, 2003), a toolkit that implements several statistical word alignment models, was adopted to align English words to their corresponding Chinese words. After alignment, the L1 translations of confusing words were recognized, after which the sentences in the example sentence pool of the confusing words in the same set were clustered with respect to their L1 translation. There were 12 confusing word sets with more than one common L1 translation. Only words in three confusing word sets (possibility"
W19-4447,D16-1115,0,0.0208192,"es using confusing words requires a better understanding of the words: with this task we hope to discover how to better assist language learners to learn to differentiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detectio"
W19-4447,W17-5039,0,0.0252816,"Missing"
W19-4447,W14-1701,0,0.0177251,"words, we intentionally move from a receptive to a productive learning task. Generating sentences using confusing words requires a better understanding of the words: with this task we hope to discover how to better assist language learners to learn to differentiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language lear"
W19-4447,J03-1002,0,0.00853928,"global grammar that are less relevant to the near-synonym recognition but to the translation instead. 466 lel Text (LDC2005T10) and Hong Kong Parallel Text (LDC2004T08), that is, a total of 2,682,129 English-Chinese sentence pairs – are utilized to learn the word alignment between L1 and L2 parallel sentences. To align example sentences from Vocabulary, first they were all translated into Traditional Chinese using Googletrans4 . Then we used NLTK5 to tokenize English sentences and CKIP (Chen and Liu, 1992) to segment Chinese sentences respectively. After that, the word alignment model GIZA++ (Och and Ney, 2003), a toolkit that implements several statistical word alignment models, was adopted to align English words to their corresponding Chinese words. After alignment, the L1 translations of confusing words were recognized, after which the sentences in the example sentence pool of the confusing words in the same set were clustered with respect to their L1 translation. There were 12 confusing word sets with more than one common L1 translation. Only words in three confusing word sets (possibility vs. opportunity, social vs. sociable, and unusual vs. strange) had all different L1 translations. When a co"
W19-4447,D14-1162,0,0.0861901,"introverts and extraverts are two tips that we provide, as they are more difficult but not directly related to social and sociable. 3.2 To recommend sentences, we first collected sentences from Vocabulary3 , an online dictionary. The example sentences in Vocabulary mainly come from formally-written news articles. We collected 5,000 sentences for each word and used all of them to train the GMM and BiLSTM word usage models. When recommending example sentences, we used only the qualified sentences which were filtered by the dictionary-like sentence classifier. The pretrained 300-dimension GloVe (Pennington et al., 2014) embeddings were used in both GMM and BiLSTM. We selected the last five sentences from Vocabulary as a baseline setting. tive. GiveMeExample collects sentences from the COBUILD English Usage Dictionary (Sinclair, 1992) to train the dictionary-like sentence classifier with syntactic features (Pil´an et al., 2014) and a logistic regression model (Walker and Duncan, 1967). Hence, it tends to select sentences similar to those in the COBUILD dictionary. 3 Deployment: Sentence Translation The sentence translation experiment was separated into a pre-test and a post-test. In both of the tests, partici"
W19-4447,W14-1821,0,0.0741538,"Missing"
W19-4447,N16-3020,0,0.015338,"ve also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detection and correction, on the other hand, is an inevitable assistance for ESL learners since, as we know, collocation error is one of the most common lexical misuse problem. However, as interpretation is still challenging for AI models, especially deep learning models (Ribeiro et al., 2016; Doshi-Velez and Kim, 2017), there are fewer applications for tasks involving comparisons and explanations, which is the key to learning confusing words. 2 Automatic Example Sentence Selection In this study, we seek to use the GiveMeExample system (Huang et al., 2017) as a basis to improve the automatic example sentence selection task which aims to select sentences that clarify the differences between confusing words. GiveMeExample proposes a clarification score to represent the ability of a sentence to clear up confusion between the given words. In this section, we describe the three main st"
W19-4447,S07-1009,0,0.0578701,"ow to better assist language learners to learn to differentiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detection and correction, on the other hand, is an inevitable assistance for ESL learners since, as we know, collocation"
W19-4447,W15-1501,0,0.0134925,"ntiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detection and correction, on the other hand, is an inevitable assistance for ESL learners since, as we know, collocation error is one of the most common lexical misuse probl"
W19-4447,S10-1002,0,0.028397,"ers to learn to differentiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detection and correction, on the other hand, is an inevitable assistance for ESL learners since, as we know, collocation error is one of the most comm"
W19-4447,W18-0549,0,0.0127881,"ds requires a better understanding of the words: with this task we hope to discover how to better assist language learners to learn to differentiate confusing words. information these reference tools provide is appropriate and instructive, the contents – especially example sentences – are neither rich nor constantly updated. In view of this, artificial intelligence techniques recently have been widely applied to assist language learning. Applications such as grammar correction (Ng et al., 2014; Napoles and CallisonBurch, 2017) and essay scoring (Alikaniotis et al., 2016; Dong and Zhang, 2016; Zhang and Litman, 2018) are relatively mature. Research on the lexical substitution (McCarthy and Navigli, 2007, 2009; Mihalcea et al., 2010; Melamud et al., 2015) and the detection and correction of collocation errors (Futagi, 2010; Alonso Ramos et al., 2014) have also shown the potential of helping ESL learn similar words, near-synonyms or synonyms. Lexical substitution task try to determine a substitute for a word in a context and preserving its meaning and is possible to help language learners understand the correct meaning of a target word by selecting a lexical substitute. The detection and correction, on the"
W19-4447,P16-1068,0,\N,Missing
