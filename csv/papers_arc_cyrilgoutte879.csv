2021.vardial-1.15,N-gram and Neural Models for Uralic Language Identification: {NRC} at {V}ar{D}ial 2021,2021,-1,-1,3,0.64122,651,gabriel berniercolborne,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"We describe the systems developed by the National Research Council Canada for the Uralic language identification shared task at the 2021 VarDial evaluation campaign. We evaluated two different approaches to this task: a probabilistic classifier exploiting only character 5-grams as features, and a character-based neural network pre-trained through self-supervision, then fine-tuned on the language identification task. The former method turned out to perform better, which casts doubt on the usefulness of deep learning methods for language identification, where they have yet to convincingly and consistently outperform simpler and less costly classification algorithms exploiting n-gram features."
2020.vardial-1.26,Challenges in Neural Language Identification: {NRC} at {V}ar{D}ial 2020,2020,-1,-1,2,0.64122,651,gabriel berniercolborne,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"We describe the systems developed by the National Research Council Canada for the Uralic language identification shared task at the 2020 VarDial evaluation campaign. Although our official results were well below the baseline, we show in this paper that this was not due to the neural approach to language identification in general, but to a flaw in the function we used to sample data for training and evaluation purposes. Preliminary experiments conducted after the evaluation period suggest that our neural approach to language identification can achieve state-of-the-art results on this task, although further experimentation is required."
2020.coling-main.576,Human or Neural Translation?,2020,-1,-1,5,0,206,shivendra bhardwaj,Proceedings of the 28th International Conference on Computational Linguistics,0,"Deep neural models tremendously improved machine translation. In this context, we investigate whether distinguishing machine from human translations is still feasible. We trained and applied 18 classifiers under two settings: a monolingual task, in which the classifier only looks at the translation; and a bilingual task, in which the source text is also taken into consideration. We report on extensive experiments involving 4 neural MT systems (Google Translate, DeepL, as well as two systems we trained) and varying the domain of texts. We show that the bilingual task is the easiest one and that transfer-based deep-learning classifiers perform best, with mean accuracies around 85{\%} in-domain and 75{\%} out-of-domain ."
W19-1402,Improving Cuneiform Language Identification with {BERT},2019,-1,-1,2,0.64122,651,gabriel berniercolborne,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"We describe the systems developed by the National Research Council Canada for the Cuneiform Language Identification (CLI) shared task at the 2019 VarDial evaluation campaign. We compare a state-of-the-art baseline relying on character n-grams and a traditional statistical classifier, a voting ensemble of classifiers, and a deep learning approach using a Transformer network. We describe how these systems were trained, and analyze the impact of some preprocessing and model estimation decisions. The deep neural network achieved 77{\%} accuracy on the test data, which turned out to be the best performance at the CLI evaluation, establishing a new state-of-the-art for cuneiform language identification."
W18-6480,Measuring sentence parallelism using Mahalanobis distances: The {NRC} unsupervised submissions to the {WMT}18 Parallel Corpus Filtering shared task,2018,0,1,5,0,12351,patrick littell,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to score sentence pairs from a large high-recall, low-precision web-scraped parallel corpus (Koehn et al., 2018a). Participants could use existing sample corpora (e.g. past WMT data) as a supervisory signal to learn what a {``}clean{''} corpus looks like. However, in lower-resource situations it often happens that the target corpus of the language is the \textit{only} sample of parallel text in that language. We therefore made several unsupervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task{---}translating the European Medicines Agency corpus (Tiedemann, 2009){---}scored among the best systems even in the 10M-word conditions."
W18-6481,Accurate semantic textual similarity for cleaning noisy parallel corpora using semantic machine translation evaluation metric: The {NRC} supervised submissions to the Parallel Corpus Filtering task,2018,0,3,5,0.173114,13775,chikiu lo,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"We present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSi{---}a novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-million-word evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing system{---}NRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features."
L18-1277,{E}uro{G}ames16: Evaluating Change Detection in Online Conversation,2018,0,2,1,1,653,cyril goutte,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1212,Real-time Change Point Detection using On-line Topic Models,2018,0,1,2,1,2469,yunli wang,Proceedings of the 27th International Conference on Computational Linguistics,0,"Detecting changes within an unfolding event in real time from news articles or social media enables to react promptly to serious issues in public safety, public health or natural disasters. In this study, we use on-line Latent Dirichlet Allocation (LDA) to model shifts in topics, and apply on-line change point detection (CPD) algorithms to detect when significant changes happen. We describe an on-line Bayesian change point detection algorithm that we use to detect topic changes from on-line LDA output. Extensive experiments on social media data and news articles show the benefits of on-line LDA versus standard LDA, and of on-line change point detection compared to off-line algorithms. This yields F-scores up to 52{\%} on the detection of significant real-life changes from these document streams."
W17-5041,Exploring Optimal Voting in Native Language Identification,2017,0,3,1,1,653,cyril goutte,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We describe the submissions entered by the National Research Council Canada in the NLI-2017 evaluation. We mainly explored the use of voting, and various ways to optimize the choice and number of voting systems. We also explored the use of features that rely on no linguistic preprocessing. Long ngrams of characters obtained from raw text turned out to yield the best performance on all textual input (written essays and speech transcripts). Voting ensembles turned out to produce small performance gains, with little difference between the various optimization strategies we tried. Our top systems achieved accuracies of 87{\%} on the essay track, 84{\%} on the speech track, and close to 92{\%} by combining essays, speech and i-vectors in the fusion track."
W17-2702,Detecting Changes in {T}witter Streams using Temporal Clusters of Hashtags,2017,6,0,2,1,2469,yunli wang,Proceedings of the Events and Stories in the News Workshop,0,"Detecting events from social media data has important applications in public security, political issues, and public health. Many studies have focused on detecting specific or unspecific events from Twitter streams. However, not much attention has been paid to detecting changes, and their impact, in online conversations related to an event. We propose methods for detecting such changes, using clustering of temporal profiles of hashtags, and three change point detection algorithms. The methods were tested on two Twitter datasets: one covering the 2014 Ottawa shooting event, and one covering the Sochi winter Olympics. We compare our approach to a baseline consisting of detecting change from raw counts in the conversation. We show that our method produces large gains in change detection accuracy on both datasets."
W16-4823,Advances in Ngram-based Discrimination of Similar Languages,2016,9,1,1,1,653,cyril goutte,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"We describe the systems entered by the National Research Council in the 2016 shared task on discriminating similar languages. Like previous years, we relied on character ngram features, and a mixture of discriminative and generative statistical classifiers. We mostly investigated the influence of the amount of data on the performance, in the open task, and compared the two-stage approach (predicting language/group, then variant) to a flat approach. Results suggest that ngrams are still state-of-the-art for language and variant identification, and that additional data has a small but decisive impact."
S16-1102,{CNRC} at {S}em{E}val-2016 Task 1: Experiments in Crosslingual Semantic Textual Similarity,2016,7,2,2,0.173114,13775,chikiu lo,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1284,Discriminating Similar Languages: Evaluations and Explorations,2016,0,1,1,1,653,cyril goutte,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation"
C16-1089,Extracting Discriminative Keyphrases with Learned Semantic Hierarchies,2016,19,3,4,1,2469,yunli wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The goal of keyphrase extraction is to automatically identify the most salient phrases from documents. The technique has a wide range of applications such as rendering a quick glimpse of a document, or extracting key content for further use. While previous work often assumes keyphrases are a static property of a given documents, in many applications, the appropriate set of keyphrases that should be extracted depends on the set of documents that are being considered together. In particular, good keyphrases should not only accurately describe the content of a document, but also reveal what discriminates it from the other documents. In this paper, we study this problem of extracting discriminative keyphrases. In particularly, we propose to use the hierarchical semantic structure between candidate keyphrases to promote keyphrases that have the right level of specificity to clearly distinguish the target document from others. We show that such knowledge can be used to construct better discriminative keyphrase extraction systems that do not assume a static, fixed set of keyphrases for a document. We show how this helps identify key expertise of authors from their papers, as well as competencies covered by online courses within different domains."
W15-5413,Experiments in Discriminating Similar Languages,2015,-1,-1,1,1,653,cyril goutte,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,None
W15-0609,Towards Automatic Description of Knowledge Components,2015,17,2,1,1,653,cyril goutte,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"A key aspect of cognitive diagnostic models is the specification of the Q-matrix associating the items and some underlying student attributes. In many data-driven approaches, test items are mapped to the underlying, latent knowledge components (KC) based on observed student performance, and with little or no input from human experts. As a result, these latent skills typically focus on modeling the data accurately, but may be hard to describe and interpret. In this paper, we focus on the problem of describing these knowledge components. Using a simple probabilistic model, we extract, from the text of the test items, some keywords that are most relevant to each KC. On a small dataset from the PSLC datashop, we show that this is surprisingly effective, retrieving unknown skill labels in close to 50% of cases. We also show that our method clearly outperforms typical baselines in specificity and diversity."
W14-5316,The {NRC} System for Discriminating Similar Languages,2014,13,20,1,1,653,cyril goutte,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"We describe the system built by the National Research Council Canada for the xe2x80x9dDiscriminating between similar languagesxe2x80x9d (DSL) shared task. Our system uses various statistical classifiers and makes predictions based on a two-stage process: we first predict the language group, then discriminate between languages or variants within the group. Language groups are predicted using a generative classifier with 99.99% accuracy on the five target groups. Within each group (except English), we use a voting combination of discriminative classifiers trained on a variety of feature spaces, achieving an average accuracy of 95.71%, with per-group accuracy between 90.95% and 100% depending on the group. This approach turns out to reach the best performance among all systems submitted to the open and closed tasks."
W14-3363,Linear Mixture Models for Robust Machine Translation,2014,18,6,2,0,6058,marine carpuat,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains. However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions. In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks."
S14-2030,{CNRC}-{TMT}: Second Language Writing Assistant System Description,2014,14,1,1,1,653,cyril goutte,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We describe the system entered by the National Research Council Canada in the SemEval-2014 L2 writing assistant task. Our system relies on a standard Phrase-Based Statistical Machine Translation trained on generic, publicly available data. Translations are produced by taking the already translated part of the sentence as fixed context. We show that translation systems can address the L2 writing assistant task, reaching out-of-five word-based accuracy above 80 percent for 3 out of 4 language pairs. We also present a brief analysis of remaining errors."
W13-1712,Feature Space Selection and Combination for Native Language Identification,2013,10,6,1,1,653,cyril goutte,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We decribe the submissions made by the National Research Council Canada to the Native Language Identification (NLI) shared task. Our submissions rely on a Support Vector Machine classifier, various feature spaces using a variety of lexical, spelling, and syntactic features, and on a simple model combination strategy relying on a majority vote between classifiers. Somewhat surprisingly, a classifier relying on purely lexical features performed very well and proved difficult to outperform significantly using various combinations of feature spaces. However, the combination of multiple predictors allowed to exploit their different strengths and provided a significant boost in performance."
2012.eamt-1.65,Learning Machine Translation from In-domain and Out-of-domain Data,2012,23,3,2,0,5084,marco turchi,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"The performance of Phrase-Based Statistical Machine Translation (PBSMT) systems mostly depends on training data. Many papers have investigated how to create new resources in order to increase the size of the training corpus in an attempt to improve PBSMT performance. In this work, we analyse and characterize the way in which the in-domain and outof-domain performance of PBSMT is impacted when the amount of training data increases. Two different PBSMT systems, Moses and Portage, two of the largest parallel corpora, Giga (French-English) and UN (Chinese-English) datasets and several in- and out-of-domain test sets were used to build high quality learning curves showing consistent logarithmic growth in performance. These results are stable across language pairs, PBSMT systems and domains. We also analyse the respective impact of additional training data for estimating the language and translation models. Our proposed model approximates learning curves very well and indicates the translation model contributes about 30% more to the performance gain than the language model."
2012.amta-papers.7,The Impact of Sentence Alignment Errors on Phrase-Based Machine Translation Performance,2012,-1,-1,1,1,653,cyril goutte,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"When parallel or comparable corpora are harvested from the web, there is typically a tradeoff between the size and quality of the data. In order to improve quality, corpus collection efforts often attempt to fix or remove misaligned sentence pairs. But, at the same time, Statistical Machine Translation (SMT) systems are widely assumed to be relatively robust to sentence alignment errors. However, there is little empirical evidence to support and characterize this robustness. This contribution investigates the impact of sentence alignment errors on a typical phrase-based SMT system. We confirm that SMT systems are highly tolerant to noise, and that performance only degrades seriously at very high noise levels. Our findings suggest that when collecting larger, noisy parallel data for training phrase-based SMT, cleaning up by trying to detect and remove incorrect alignments can actually degrade performance. Although fixing errors, when applicable, is a preferable strategy to removal, its benefits only become apparent for fairly high misalignment rates. We provide several explanations to support these findings."
D10-1044,Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation,2010,24,170,2,0,3518,george foster,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure. We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines."
2009.mtsummit-papers.9,Automatic Detection of Translated Text and its Impact on Machine Translation,2009,-1,-1,2,0,47487,david kurokawa,Proceedings of Machine Translation Summit XII: Papers,0,None
2009.eamt-smart.7,Improving {SMT} by learning translation direction,2009,-1,-1,1,1,653,cyril goutte,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,None
N07-1064,Statistical Phrase-Based Post-Editing,2007,9,114,2,0.424187,5047,michel simard,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We propose to use a statistical phrasebased machine translation system in a post-editing task: the system takes as input raw machine translation output (from a commercial rule-based MT system), and produces post-edited target-language text. We report on experiments that were performed on data collected in precisely such a setting: pairs of raw MT output and their manually post-edited versions. In our evaluation, the output of our automatic post-editing (APE) system is not only better quality than the rule-based MT (both in terms of the BLEU and TER metrics), it is also better than the output of a stateof-the-art phrase-based MT system used in standalone translation mode. These results indicate that automatic post-editing constitutes a simple and efcient way of combining rule-based and statistical MT technologies."
2007.mtsummit-papers.34,Domain adaptation of {MT} systems through automatic post-editing,2007,-1,-1,2,0,33186,pierre isabelle,Proceedings of Machine Translation Summit XI: Papers,0,None
H05-1095,Translating with Non-contiguous Phrases,2005,19,80,6,0.424187,5047,michel simard,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data."
2005.jeptalnrecital-long.24,Une approche {\\`a} la traduction automatique statistique par segments discontinus,2005,-1,-1,6,0.424187,5047,michel simard,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une m{\'e}thode de traduction automatique statistique bas{\'e}e sur des segments non-continus, c{'}est-{\`a}-dire des segments form{\'e}s de mots qui ne se pr{\'e}sentent pas n{\'e}c{\'e}ssairement de fa{\c{c}}on contigu{\""e} dans le texte. On propose une m{\'e}thode pour produire de tels segments {\`a} partir de corpus align{\'e}s au niveau des mots. On pr{\'e}sente {\'e}galement un mod{\`e}le de traduction statistique capable de tenir compte de tels segments, de m{\^e}me qu{'}une m{\'e}thode d{'}apprentissage des param{\`e}tres du mod{\`e}le visant {\`a} maximiser l{'}exactitude des traductions produites, telle que mesur{\'e}e avec la m{\'e}trique NIST. Les traductions optimales sont produites par le biais d{'}une recherche en faisceau. On pr{\'e}sente finalement des r{\'e}sultats exp{\'e}rimentaux, qui d{\'e}montrent comment la m{\'e}thode propos{\'e}e permet une meilleure g{\'e}n{\'e}ralisation {\`a} partir des donn{\'e}es d{'}entra{\^\i}nement."
P04-1064,Aligning words using matrix factorisation,2004,17,31,1,1,653,cyril goutte,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"Aligning words from sentences which are mutual translations is an important problem in different settings, such as bilingual terminology extraction, Machine Translation, or projection of linguistic features. Here, we view word alignment as matrix factorisation. In order to produce proper alignments, we show that factors must satisfy a number of constraints such as orthogonality. We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment. This is illustrated on a French-English alignment task from the Hansard."
C04-1046,Confidence Estimation for Machine Translation,2004,35,236,5,0,52360,john blatz,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present a detailed study of confidence estimation for machine translation. Various methods for determining whether MT output is correct are investigated, for both whole sentences and words. Since the notion of correctness is not intuitively clear in this context, different ways of defining it are proposed. We present results on data from the NIST 2003 Chinese-to-English MT evaluation."
W03-0305,Reducing Parameter Space for Word Alignment,2003,6,29,3,0,18523,herve dejean,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm. We use IBM Model 4 as a baseline. In order to reduce the parameter space, we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm. Using these additional components, we obtained an improvement in the alignment error rate."
W02-2011,Combining Labelled and Unlabelled Data: A Case Study on Fisher Kernels and Transductive Inference for Biological Entity Recognition,2002,14,12,1,1,653,cyril goutte,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"We address the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities. Our approach relies on a combination of probabilistic models, which we use to model the generation of entities and their context, and kernel machines, which implement powerful categorisers based on a similarity measure and some labelled data. This combination takes the form of the so-called Fisher kernels which implement a similarity based on an underlying probabilistic model. Such kernels are compared with transductive inference, an alternative approach to combining labelled and unlabelled data, again coupled with Support Vector Machines. Experiments are performed on a database of abstracts extracted from Medline."
