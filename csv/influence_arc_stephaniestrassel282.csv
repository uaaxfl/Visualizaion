2008.amta-govandcom.10,clifford-etal-2004-effect,0,0.22655,"ght areas for improvement on both ends and will inform the production of linguistic resources to support MT. Moreover, if a common set of challenges for HT and MT can be defined, source data in the human translation pipeline can be analyzed to better predict MT performance, thus enabling more informed data selection for system training and evaluation. Vanni (2000) argued that the metric used for evaluation of competence in human language learners may be applicable to MT evaluation; we apply similar thinking to improve the prediction of MT performance, which is currently unreliable. Similarly, Clifford (2004) reported that while Interagency Language Roundtable Scale (ILR) difficulty ratings are moderately useful, they are not consistently reliable in predicting MT performance. In the current paper we explore an alternate model based upon a set of genre-defining features that prove to be consistently challenging for both humans and MT systems. Abstract The dramatic improvements shown by statistical machine translation systems in recent years clearly demonstrate the benefits of having large quantities of manually translated parallel text for system training and development. And while many competing"
2008.amta-govandcom.10,C04-1059,0,0.0191589,"Missing"
2020.lrec-1.423,W15-1409,0,0.0195902,"Missing"
2020.lrec-1.423,K19-2001,0,0.0577783,"Missing"
2020.lrec-1.423,strassel-etal-2010-darpa,1,0.705151,"though the types have been diversifying over time. Annotation often includes entities, events, relations and coreference and may be normalized or linked within or across documents, sometimes across languages and increasingly to knowledgebases. • Committed Belief Annotation of discussion forums in Chinese (LDC2019T03), English (LDC2019T16) and Spanish (LDC2019T09) developed by LDC for the DARPA DEFT program described in §4.5 • A collection (LDC2019T14) of 110 source documents from English newswire manually annotated for instances of categories defined with respect to the NFL Scoring ontology (Strassel et al. 2010) to support the DARPA Machine Reading program 7 8 3451 https://dirha.fbk.eu https://librivox.org • • 3.7 Multiple datasets from the NIST TAC KBP task including: source data for 2009-2014 (LDC2018T03) and 2016-2017 (LDC2019T12); training and evaluation data for the Slot Filling task in English for 2009-2014 (LDC2018T22) and in Chinese for 2014 (LDC2019T08); evaluation data for the 2012-2017 Cold Start task (LDC2019T17); training and evaluation data for the Entity Discovery and Linking task for 20092013 (LDC2018T16), 2014-2015, (LDC2019T02) and 2016-2017 (LDC2019T19), the Relation Extraction tas"
2020.lrec-1.423,wright-etal-2012-annotation,1,0.779908,"o supplement current approaches that use paid experts and paid crowd workers. They demonstrate the nearly boundless power of organizing willing contributors and offering as incentives: challenge, entertainment, competition, opportunities to contribute to technological advancement and by extension the betterment of one’s own community and the broader society. For example, Zooniverse has recruited nearly two million citizen scientists who have contributed more than 250 million judgements to researchers in astronomy, biology and other fields. LDC&apos;s internal web based annotation platform, webann (Wright et al. 2012), continues to support the majority of our workload, adding support for video and image annotation as these media gained importance in LDC projects. With the support of the NSF NIEUW project, we have created a new platform, Universal Annotator (UA), a reboot of the webann code base with portability as an explicit design goal. NIEUW has used UA to build the NameThatLanguage 14 language identification game and the Citizen Linguist portal LanguageARC15 (see §7) along with the 9 projects currently deployed on that portal and the LanguageARC, built upon LDC’s UA framework (see §6), hosts LR develop"
2020.lrec-1.493,W10-2211,0,0.0911768,"Missing"
2020.lrec-1.493,J93-2004,1,0.0968492,"Missing"
2020.lrec-1.493,N15-1186,0,0.0615565,"xist, such as Morfessor CatMAP (Creutz and Lagus, 2007), Morfessor FlatCat (Grönroos et al., 2014), etc. As discussed above, most of these systems are only designed for identifying morpheme boundaries. The annotation discussed here is intended primarily to move this stream of work forward. Some more recent systems focus on identifying morphologically related word pairs, such as (stop, stopped), and then transform the output to morpheme segmentations so they are compatible with the available segmentation based evaluation. Such work includes Schone and Jurafsky (2001); Narasimhan et al. (2015); Soricut and Och (2015); Luo et al. (2017) and Xu et al. (2018). These systems have the advantage of finding morphologically related word pairs, which is equivalent to finding roots for complex words. Another stream of completely unsupervised work primarily aims at discovering morphological paradigms (Parkes et al., 1998; Goldsmith, 2001; Chan, 2006). Xu et al. (2018) discover such paradigms, but primarily use them for improving a probabilistic segmentation model. The annotation we report on here does not attempt to determine paradigm information for a wide set of word roots, which is a somewhat different task. Whil"
2020.lrec-1.493,L16-1521,1,0.86281,"tion and so include some related higher-resource languages. Representative Language Packs have been created for Akan, Amharic, Arabic, Bengali, Farsi, Hindi, Hungarian, Indonesian, Mandarin, Russian, Somali, Spanish, Swahili, Tagalog, Tamil, Thai, Ukrainian, Vietnamese, Wolof, Yoruba, Zulu, plus partial resource packs for Hausa, Turkish, Uzbek and English. Incident Language Packs contain manually labeled evaluation data designed to test system performance on tasks related to situational awareness for one or more surprise languages that remain unknown until the start of each annual evaluation (Strassel and Tracey, 2016). Incident Language Packs have been created for Ilocano, Kinyarwanda, Odia, Oromo, Sinhala, Tigrinya, and Uyghur. 1.2 Overall Research Goals Morphology analysis is useful as an underlying task for various natural language processing applications. Supervised methods for such analysis require significant training data and suffer from difficulty in transferring tools from one language to another. Unsupervised methods do not suffer from these problems and are therefore better suited for the LORELEI program’s research goals of rapidly functioning in a new unanticipated language. In general, unsuper"
2020.lrec-1.493,C18-1005,1,0.744915,"he nine languages in our corpus cover five primary language families (Austronesian: Indonesian, Tagalog; Dravidian: Tamil; Indo-European: Hindi, Russian, Spanish; Niger-Congo: Akan (Twi), Swahili; Uralic: Hungarian), and cover a range of morphological phenomena including suffixation, prefixation, infixation, circumfixation, full and partial reduplication, and vowel harmony. Further, seven of the nine languages are annotated with root information as well, which can be used to test existing systems that are designed for identifying roots, such as Narasimhan et al. (2015), Luo et al. (2017), and Xu et al. (2018). Although this data set does not include languages in which templatic morphology plays a sizeable role (primarily of Semitic and Afro-Asiatic language families), it is sufficient to provide a real challenge for the current state-of-the-art in unsupervised morphology. 2. 2.1 Related Work Related Morphological Annotation The Penn Treebank developed a part-of-speech tagset for English that encoded some morphological distinctions but that did not provide information on morphemes or morphological segments (Marcus et al., 1993). The Penn Arabic Treebank moved to more complete morphological annotati"
2020.lrec-1.816,masmoudi-etal-2014-corpus,0,0.0295274,"oken data resources. Since French is commonly spoken in Tunisia, and Modern Standard Arabic is the country’s official language, codeswitching is a common characteristic of everyday speech, presenting challenges for a corpus required to be unambiguously Tunisian Arabic. For this reason, careful management of recruited speakers was a necessity, as was careful auditing of speech segments to confirm language and dialect. The CMN2 collection differs from prior Tunisian speech datasets such as the Spoken Tunisian Arabic Corpus (Zribi et al., 2015) and the Tunisian Arabic Railway Interactive Corpus (Masmoudi et al., 2014) in that it consists of spontaneous conversational telephone speech on open topics between speakers who know each other. 3. In-country Collection Speaker recruitment and collection was conducted entirely within Tunisia, utilizing a recording platform built and hosted by a collection partner in Tunis working under LDC direction. Hosting recording platforms at remote locations necessitates measures to minimize the risk of both unexpected platform behavior and vendor misinterpretation of collection requirements. 3.1 Collection Platform and Speaker CoLocation In the first Call My Net collection (2"
2020.sltu-1.39,L16-1405,0,0.0133405,"template was used for all languages so that each sketch includes basic information about the language (e.g., classification, ISO code, word order), orthography, encoding (Unicode chart, etc.), morphology, syntax, and specialized sections for personal names, locations, numbers, and variation, as well as references to in-depth grammars. In the three languages for which resources were developed prior to the start of LORELEI (Turkish, Uzbek, and Hausa), morphological analysis and alignment were also performed, tightly coupled with the creation morphological analyzers for each of these languages (Kulick and Bies, 2016). In this task, annotators were presented with a list of possible solutions from the analyzer and were required to select the best one from the list, or choose &quot;unanalyzable&quot; if no correct solution for the token was present. Part-of-speech labels were not directly annotated, but were instead derived from the morphological annotation. For Turkish and Uzbek, morpheme alignment between the Turkish/Uzbek text and an English translation was also performed to identify translational correspondence using the same general For ILs, no LORELEI-specific lexicon or grammatical sketch was produced, but poin"
2020.sltu-1.39,ma-2006-champollion,0,0.0227468,"to make up the remaining amount. Therefore, the RL packs differ considerably in the proportion of professional, crowd and found translation. 3.2.1 Found Parallel Text Parallel text was harvested from the web using a combination of approaches. When possible, native speaker annotators provided information about sites containing parallel text in their language. We also used Bilingual Internet Text Search (BITS) (Ma and Liberman, 1999) to locate additional data by scanning potential parallel sites and using a translation lexicon to identify pairs of documents that are translations4. Champollion (Ma, 2006) was then used to align the documents at the sentence level. In some cases, the amount of parallel text harvested far exceeded the total parallel text target. In addition to collecting individual documents identified by native speakers, wherever possible other documents from the same website were also harvested to provide a pool of background data that was not specifically about LORELEI domain topics. All collected documents were then processed into a uniform tokenized and sentencesegmented xml format to support downstream annotation and evaluation pipelines. LDC acquired rights to use and dis"
2020.sltu-1.39,1999.mtsummit-1.79,0,0.0680194,"ers for the language. The goal for RLs was to first maximize the amount of found translations, then use crowdsourcing if viable, and only use professional translation as much as required to make up the remaining amount. Therefore, the RL packs differ considerably in the proportion of professional, crowd and found translation. 3.2.1 Found Parallel Text Parallel text was harvested from the web using a combination of approaches. When possible, native speaker annotators provided information about sites containing parallel text in their language. We also used Bilingual Internet Text Search (BITS) (Ma and Liberman, 1999) to locate additional data by scanning potential parallel sites and using a translation lexicon to identify pairs of documents that are translations4. Champollion (Ma, 2006) was then used to align the documents at the sentence level. In some cases, the amount of parallel text harvested far exceeded the total parallel text target. In addition to collecting individual documents identified by native speakers, wherever possible other documents from the same website were also harvested to provide a pool of background data that was not specifically about LORELEI domain topics. All collected document"
2020.sltu-1.39,2020.lrec-1.493,1,0.757963,"ersions of the task included a binary judgment of whether or not a situation was urgent, but poor inter-annotator agreement on this decision led to a change in which urgency was decomposed into scalar judgments about the scope (size of affected region/population) and severity (low = inconvenience; high = death) of the situation. Table 2 shows the elements of the SF annotation task over the four years of the LORELEI program. However, in collaboration with other LORELEI researchers, some additional morphological segmentation annotation was performed on 9 languages toward the end of the program (Mott et al., 2020). The languages selected for this task include a variety of morphological features of interest such as case marking and noun class systems, infixes, circumfixes, etc. For Akan, Hindi, Hungarian, Indonesian, Russian, Spanish, Swahili, Tagalog, and Tamil, 2000 tokens per language were segmented at morpheme boundaries, and markup was added to indicate substitution (as in come/came in English). Due to the difficulty of this type of task for non-expert annotators, the segmentation was performed by a trained linguist working in tandem with a native speaker annotator. Situation Frame was defined as a"
2020.sltu-1.39,W13-2322,0,0.0177278,"addition to named mentions, all nominal and pronominal mentions are tagged as well. There is also an additional category of title, which is used to capture professional or honorific titles of persons. All annotations were grounded in text extents. FE also includes within document coreference of entity mentions. Simple Semantic Annotation In Simple Semantic Annotation (SSA), documents were labeled for predicates (Acts and States) and their basic arguments (Agents, Patients, and Locations). Existing predicate-argument based semantic annotation protocols such as Abstract Meaning Representation (Banarescu et al., 2013) or PropBank (Palmer et al., 2005) are fairly 5 All RLs hit the target volume for SNE and FE annotation except for Hausa, which had the full amount of SNE but only 13,000 words of FE due to limited availability of annotators. All ILs met the target volume for SNE annotation. In the year 3 evaluation, EDL was performed on a set of English documents labeled for Full Entity, such that both named and nominal entity mentions were linked to the KB. In all other data sets, EDL used only name mentions from SNE. 6 www.geonames.org 7 www.cia.gov/library/publications/world-leaders-1/ 8 www.cia.gov/librar"
2020.sltu-1.39,J05-1004,0,0.0297088,"nal and pronominal mentions are tagged as well. There is also an additional category of title, which is used to capture professional or honorific titles of persons. All annotations were grounded in text extents. FE also includes within document coreference of entity mentions. Simple Semantic Annotation In Simple Semantic Annotation (SSA), documents were labeled for predicates (Acts and States) and their basic arguments (Agents, Patients, and Locations). Existing predicate-argument based semantic annotation protocols such as Abstract Meaning Representation (Banarescu et al., 2013) or PropBank (Palmer et al., 2005) are fairly 5 All RLs hit the target volume for SNE and FE annotation except for Hausa, which had the full amount of SNE but only 13,000 words of FE due to limited availability of annotators. All ILs met the target volume for SNE annotation. In the year 3 evaluation, EDL was performed on a set of English documents labeled for Full Entity, such that both named and nominal entity mentions were linked to the KB. In all other data sets, EDL used only name mentions from SNE. 6 www.geonames.org 7 www.cia.gov/library/publications/world-leaders-1/ 8 www.cia.gov/library/publications/the-worldfactbook/a"
2020.sltu-1.39,L18-1293,0,0.0239874,"tries. Manual effort focused on adding high frequency tokens that were missing from the found resources, and the amount of manual annotation required varied significantly by language. Lexicons were augmented for several languages by integrating the detailed morphological analysis information in a separate word forms table that is indexed to the entries in the main lexicon. For Arabic, this information was extracted from the Penn Arabic TreeBank (Kulick, et al., 2010); for Amharic, Farsi, Hungarian, Russian, Somali, Spanish and Yoruba, morphological information comes from the Unimorph Project (Kirov, et al., 2018). Morphosyntactic Annotations Several types of morphological and syntactic annotations were performed on subsets of the RLs. These annotations were intended to serve as resources for system development and were not directly evaluated, so they only appear in RLs. In response to input from DARPA and LORELEI performers, most morphosyntactic annotation was dropped after the first set of language packs in favor of greater effort elsewhere (e.g. Situation Frame), though one new morphological annotation task was added in subsequent phases of the program. The first type of morphosyntactic annotation w"
2020.sltu-1.39,L16-1521,1,0.929628,"Missing"
2020.sltu-1.39,J06-4003,0,0.123573,"Missing"
2020.sltu-1.39,kulick-etal-2010-consistent,0,0.0264545,"lexicon was created using a combination of found resources such as existing online dictionaries, and manual effort by native speakers to create new entries. Manual effort focused on adding high frequency tokens that were missing from the found resources, and the amount of manual annotation required varied significantly by language. Lexicons were augmented for several languages by integrating the detailed morphological analysis information in a separate word forms table that is indexed to the entries in the main lexicon. For Arabic, this information was extracted from the Penn Arabic TreeBank (Kulick, et al., 2010); for Amharic, Farsi, Hungarian, Russian, Somali, Spanish and Yoruba, morphological information comes from the Unimorph Project (Kirov, et al., 2018). Morphosyntactic Annotations Several types of morphological and syntactic annotations were performed on subsets of the RLs. These annotations were intended to serve as resources for system development and were not directly evaluated, so they only appear in RLs. In response to input from DARPA and LORELEI performers, most morphosyntactic annotation was dropped after the first set of language packs in favor of greater effort elsewhere (e.g. Situati"
bendahman-etal-2008-quick,choukri-etal-2004-network,1,\N,Missing
bendahman-etal-2008-quick,vandecatseye-etal-2004-cost278,0,\N,Missing
bies-etal-2006-linguistic,A00-2018,0,\N,Missing
bies-etal-2006-linguistic,graff-bird-2000-many,0,\N,Missing
bies-etal-2006-linguistic,N01-1016,0,\N,Missing
bies-etal-2006-linguistic,W02-1007,0,\N,Missing
bies-etal-2006-linguistic,N04-4032,0,\N,Missing
bies-etal-2006-linguistic,J03-4003,0,\N,Missing
bies-etal-2006-linguistic,N06-1024,1,\N,Missing
bies-etal-2006-linguistic,P04-1005,0,\N,Missing
bies-etal-2006-linguistic,cieri-etal-2004-fisher,0,\N,Missing
cieri-etal-2000-large,wayne-2000-multilingual,0,\N,Missing
cieri-etal-2000-large,strassel-etal-2000-quality,1,\N,Missing
cieri-etal-2000-large,graff-bird-2000-many,1,\N,Missing
cieri-etal-2008-bridging,maeda-strassel-2004-annotation,1,\N,Missing
cieri-etal-2008-bridging,cieri-etal-2006-mixer,1,\N,Missing
cieri-etal-2008-bridging,cieri-etal-2004-fisher,1,\N,Missing
cieri-etal-2014-new,cieri-etal-2008-bridging,1,\N,Missing
cieri-etal-2014-new,P11-2122,0,\N,Missing
cieri-etal-2014-new,ide-etal-2014-language,1,\N,Missing
cieri-etal-2014-new,wright-2014-restful,1,\N,Missing
glenn-etal-2010-transcription,cieri-etal-2008-bridging,1,\N,Missing
glenn-etal-2010-transcription,fiscus-etal-2006-multiple,0,\N,Missing
L16-1145,W13-2322,1,0.860349,"Missing"
L16-1145,bies-etal-2014-incorporating,1,0.844507,"ish Treebank within BOLT improved annotation quality by adding several rounds of Quality Control (QC) to the annotation process. The first QC process consists of a series of specific searches for approximately 200 types of potential inconsistencies and parser or annotation errors. Any errors found in these searches were hand corrected. An additional QC process then identifies repeated text and structures, and flags non-matching annotations. Identified annotation errors are also manually corrected. A special effort for English translation Treebank is the annotation of alternative translations (Bies et al. 2014). Both literal and fluent translation alternates are annotated for word-level tokenization and part-of-speech, whereas only the fluent translation alternates are annotated as part of the syntactic structure of the tree. 3.1.2 Arabic Treebank Arabic Treebank started with Penn Arabic Treebank guidelines, enhanced first with the GALE (Global Autonomous Language Exploitation) project and now with the BOLT project (Maamouri et al. 2011). The two efforts of Arabic Treebank within the GALE project are enhancement of Penn style Treebank annotation (Maamouri & Bies, 2010) and the creation of CATiB (Col"
L16-1145,bonial-etal-2014-propbank,1,0.859958,"or challenge for Arabic was with special features of the Egyptian dialect. It was sometimes very difficult to decide if an MSA word form in the dialect had an equivalent meaning or a slightly different meaning. Additionally, as a pro-drop language, Arabic poses special issues for coreference. Co-reference chains creation problems were initially solved by manual annotation but are now able to be created during post-processing. The English PropBank has focused on expanding predicate annotation beyond the verb and is now annotating verbs, eventive nouns, adjectives, and light verb constructions (Bonial et al. 2014). In English, light verbs are semantically bleached verbs. The argument structure for light verb and non-light verb instances are different. In light verb constructions, the real predicate is generally the nominalized predicate that the light verb supports. A major focus for English PropBank has been to unify FrameFiles across these different parts of speech. This means that the frame used for 'bathe' is always identical to that used for 'bath'. The goal of this expansion is to provide event semantic representations for the entire sentence, specifically pieces most often missed when looking so"
L16-1145,cotterell-callison-burch-2014-multi,0,0.0685218,"Missing"
L16-1145,W10-0702,0,0.0308468,"nd facilitating linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2010-enriching,1,0.823206,"ting linguistic analysis of languages. With the rapid growth of the internet, NLP technologies are challenged with an avalanche of unstructured user-generated data. Off-the-shelf tools, trained on venerable formal data, perform worse when applied to new social media data. Various research and development efforts have been invested in this new domain -- massive informal and unstructured data. In this line, Ryan Cotterell and Chris Callison-Burch (2014) created a multidialect and multi-genre corpus of informal text via Amazon’s Mechanical Turk services. With a crowdsourcing approach, Jha et al. (2010) built a prepositional phrase attachment corpus of informal and noisy blog text. Owoputi et al. (2013) created part-ofspeech tagged data for informal and online conversational Twitter text. The OntoNotes corpus (Weischedel et al. 2013) is a collaborative effort between BBN Technologies, Brandeis University, the University of Colorado, the University of Pennsylvania, and the University of Southern California's Information Sciences. The OntoNotes corpus comprises integrated annotation of multiple levels in various genres and in three languages (English, Chinese, and MSA Arabic), providing struct"
L16-1145,li-etal-2012-parallel,1,0.828457,"-9 (-NONE- *T*))))))) (. .)) ) POS annotation is stored in .pos files, where POS tags are attached to tokenized/segmented word units, each sentence per line, as shown in the following example. 好_VA 的_SP 呗_SP ，_PU 来_VV 的话_SP 打_VV 我 _PN 电话_NN 就_AD 可以_VV ，_PU 或者_CC 报_VV 我_PN 名字_NN ，_PU 我_PN 定_VV 了_AS 包厢_NN 3.2 Word Alignment 3.2.1 Alignment Approach Word alignment guidelines are developed based on guidelines for the Blinker and ARCADE projects, enriched during GALE by adding tagging guidelines (Li et al. 2010), and further enhanced to tackle new genres and language features for the BOLT project (Li et al. 2012). The alignment annotation involves a process of 2-pass annotation plus one round of cross-file checking. The initial alignment by junior annotators goes through quality control by senior annotators, and is then followed by a cross-file check by lead annotators for consistency. The word alignment tool is developed by LDC (Figure 3), allowing annotators to align source and translation words as well as to label both alignment links and individual words. Alignment is performed on two pairs of languages: Chinese-English and Egyptian-English. Chinese alignment is performed at two levels: character-"
L16-1145,maamouri-etal-2014-developing,1,0.87281,"Missing"
L16-1145,N13-1039,0,0.101377,"Missing"
L16-1467,W15-4109,0,0.0650733,"Missing"
L16-1467,li-etal-2010-enriching,1,0.810982,"Missing"
L16-1467,C10-1084,0,0.0193722,"n highly inflected languages exacerbates this problem. Morphemebased alignment is useful in the translation process of highly inflected languages. Morphological inflections indicate tense, gender or number which are normally expressed as separate words in uninflected languages. Capturing such sub-word alignments can yield better word alignments. Recent SMT research has found that utilizing information from morphology improves the quality of word alignments. Eyig¨oz et al. (2013) developed a MT model using a two-level Turkish-English alignment, achieving significant improvement of BLEU scores. Luong and Kan (2010) proposed a morphologically sensitive approach to word alignment for language pairs involving a highly inflected language, addressing morpheme alignment issues which are peculiar to highly inflected languages. Toutanova et al. (2008) improved the quality of SMT by applying inflection generation models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages, Russian and Arabic. Their model improves the quality of SMT over both phrasal and syntax-based approaches. Minkov et al. (2007) adopted a novel method for pr"
L16-1467,P07-1017,0,\N,Missing
L16-1521,L16-1405,0,0.0476256,"to mark. Unlike Entity annotation tasks, names within larger NPs are extracted/labeled as their own NPs when syntactic structure dictates, as in: [[University] of [Pennsylvania]]. 2.3.4. Morph and POS Annotation The final and most challenging annotation task for the RL packs is part-of-speech and morphological annotation. The same 10Kw labeled for NP Chunking is also subject to morph/POS annotation. In the three LORELEI languages produced under BOLT (Uzbek, Hausa and Turkish), our approach to morphological annotation was tightly integrated with creation of language-specific analyzers at LDC (Kulick and Bies, 2016). For subsequent LORELEI RL packs we will utilize a universal analyzer being developed by LORELEI performers, rather than creating custom analyzers for each language. The universal analyzer is expected to produce multiple (possibly ranked) analyses for each token, consisting of a lemma plus features (possibly with individually segmented, labeled morphemes). Annotators select the best solution from the list, or choose ""unanalyzable"" if the analyzer has no correct solution for the token. Part-of-speech labels are not directly annotated, but are instead derived from the morph annotation. In cases"
L16-1521,ma-2006-champollion,0,0.0282861,"Missing"
L16-1589,bies-etal-2014-incorporating,1,0.837176,"Table 4: Light and Rich ERE tagged items 3720 5. Annotation Decisions for Translation Artifacts As mentioned in Section 3 above, ERE data used in this corpus was collected and translated under DARPA’s BOLT program, which focused on machine translation. The emphasis on maximizing meaning fidelity over fluency in the translations resulted in some features that ERE had to accommodate by developing specific new annotation policies. 5.1 Alternate Translations The English translations of this data have 274 instances where both fluent and literal translations of some Chinese expressions are present (Bies et al., 2014). The inclusion of the alternates was intended to assist machine translation system development. Below, the Chinese phrase 老毛子 lǎo máozi is rendered with a fluent and literal translation: 为了目前我们既得利益，老 毛 子 的事可以先不 考虑。 For the sake of our current vested interests, we can disregard the matter of [Russians |Old Hairy] for the time being. In Light ERE, only the fluent translations are tagged. In Rich ERE, however, both translation alternates are annotated and coreferenced when appropriate. In some cases, the fluent translations do not match the literal translations exactly in terms the entities, rel"
L16-1589,R11-1017,0,0.0589143,"Missing"
L16-1589,W14-2904,1,0.867817,"Missing"
L16-1589,W15-0812,1,0.916562,"). Given the large number and variety of approaches to algorithm development within DEFT, we set out to define an annotation task that would be supportive of multiple research directions and technology evaluations, and that would provide a useful foundation for follow-on DEFT annotation tasks like entailment, inference and belief/sentiment. The resulting Entities, Relations and Events annotation task has evolved over the course of the DEFT program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program (Song et al. 2015). ERE corpora are used by DEFT performers as a general resource, and also serve as training data for several tracks within the Text Analysis Conference Knowledge Base Population (TAC KBP) evaluation series, which is open to non-DEFT participants conducted by the National Institute of Standards and Technology (NIST). TAC KBP aims to develop and evaluate technologies for building and populating knowledge bases from unstructured texts (NIST 2015). The ERE corpora provide a training resource for component evaluation tasks such as Entity Detection and Linking and Event Argument Linking. In keeping"
L16-1589,H01-1035,0,0.146667,"Missing"
L16-1589,P15-2064,0,0.0399635,"Missing"
L16-1593,medero-etal-2006-efficient,1,0.735439,"Missing"
L16-1720,elmahdy-etal-2014-development,0,0.0345854,"Missing"
L16-1720,W06-0605,1,0.806823,"Missing"
L18-1245,li-etal-2012-linguistic,1,0.788931,"e version for use during annotation and assessment. 3. 3.1 Evolution of Data for KBP Evaluation Tracks: 2009-2017 Entity Linking Entity Linking (EL) requires systems to link named mentions of person, organization, and geopolitical entities in text to entries in a knowledge base (KB), report if no matching entries exist, and group mentions without entries according to identity coreference. Entity Linking began in 2009 in English (Simpson et al., 2010), added a Chinese cross-lingual version of the task in 2011, and further expanded with a Spanish version of the task in 2012 (Ellis et al., 2012; Li et al., 2012). The 2013 EL task remained largely unchanged from the tri-lingual version established in 2012. In 2014, only cross-lingual Chinese and Spanish EL evaluations were held, as English was replaced by Entity Discovery & Linking. Although the languages in Entity Linking changed over the track's six years, the goals of query selection for Entity Source Documents ENG CMN SPA 3688 3684 2231 4329 2016 2271 3772 1820 2143 1832 2860 2207 Queries ENG CMN 3904 3750 2250 4347 2226 2280 2190 2155 3253 SPA 3890 2117 2596 Table 2: Entity Linking resources 3.2 Entity Discovery & Linking A new variant of Entity"
L18-1245,W15-0809,1,0.82744,"event. The following table summarizes Event Argument Linking resources developed by LDC. Year Task Lang. 2014 2014 2015 2015 2016 pilot eval training eval gold standard eval x-doc pilot x-doc eval gold standard eval ENG ENG ENG ENG ENG, CMN, SPA ENG 2016 2016 2017 Source Docs 60 528 55 500 505 LDC Responses 5947 5207 17809 Assessed Responses 32054 57599 45391 - 2092 98 2689 ENG 30000 700 7697 ENG, CMN, SPA 500 27109 - Table 8: Event Argument Linking resources 3.5 Event Nugget The Event Nugget track seeks to evaluate system performance in detection and coreference of event references in text (Mitamura et al., 2015). An event ‘nugget’, as defined by the task, includes a text extent, a classification of event type and subtypes, and an indication of whether realis mood was used to describe the event (Ellis et al., 2015). Event Nugget started as a pilot evaluation within the DEFT program in 2014. In 2015, event nuggets were redefined to align with the treatment of events in DEFT Rich ERE (Song et al., 2015). Also in 2015, coreference of event nuggets was added, using the definition of event hoppers developed in Rich ERE. In 2016 and 2017, there was no separate annotation task conducted solely to support the"
L18-1245,mcnamee-etal-2010-evaluation,1,0.681623,"e 2009, developing, maintaining, and distributing linguistic resources in three languages for seven distinct evaluation tracks. This paper describes LDC's resource creation efforts for the various KBP tracks, and highlights changes made over the years to support evolving evaluation requirements. Keywords: knowledge base population, information extraction, linguistic resources 1. Introduction Text Analysis Conference (TAC) an annual series of open technology evaluations organized by the National Institute of Standards and Technology (NIST). The Knowledge Base Population (KBP) evaluation track (McNamee et al. 2010) encourages the development of systems that can extract information from unstructured multilingual text and in order to populate an existing or emergent knowledge base. Since the start of TAC KBP in 2009 Linguistic Data Consortium (LDC) has provided linguistic resources by building labeled training and test sets and by assessing system results. challenging tasks led to the use of a single set of test data shared by all tasks. This put additional demands on data selection given the need for a single corpus to support multiple, sometimes mutually contradictory, requirements. For instance, the li"
L18-1245,simpson-etal-2010-wikipedia,1,0.806472,"ented in RDF (Linguistic Data Consortium, 2015). Because of the additional complexity of BaseKB compared to the Wikipedia-based KB, LDC prepared a custom human-readable version for use during annotation and assessment. 3. 3.1 Evolution of Data for KBP Evaluation Tracks: 2009-2017 Entity Linking Entity Linking (EL) requires systems to link named mentions of person, organization, and geopolitical entities in text to entries in a knowledge base (KB), report if no matching entries exist, and group mentions without entries according to identity coreference. Entity Linking began in 2009 in English (Simpson et al., 2010), added a Chinese cross-lingual version of the task in 2011, and further expanded with a Spanish version of the task in 2012 (Ellis et al., 2012; Li et al., 2012). The 2013 EL task remained largely unchanged from the tri-lingual version established in 2012. In 2014, only cross-lingual Chinese and Spanish EL evaluations were held, as English was replaced by Entity Discovery & Linking. Although the languages in Entity Linking changed over the track's six years, the goals of query selection for Entity Source Documents ENG CMN SPA 3688 3684 2231 4329 2016 2271 3772 1820 2143 1832 2860 2207 Queries"
L18-1245,W15-0812,1,0.947534,"all valid mentions of the targeted entity types within the source corpus (Ellis et al., 2015). Titles were also annotated in 2015 to help systems distinguish between titles and nominal mentions of persons (e.g. “president”). In 2016-2017, rather than starting with a blank slate, annotators worked with entity mentions from Entities, Relations, and Events (ERE), an annotation task developed by LDC for DARPA’s Deep Exploration and Filtering of Text program (DEFT) (DARPA, 2012). ERE exhaustively labels entities, relations and events, along with their attributes, according to specified taxonomies (Song et al., 2015). In EDL, ERE entity annotations were displayed in the context of their source documents, so annotators could check for errors and misses, as well as ERE annotations at variance with EDL guidelines (though correct for ERE). In all years, 1553 labeled entities were then linked to the KB or marked as NIL (not in the KB) or Unknown (insufficient information in the source data to know if an entity was in the KB). After document-level KB linking, senior annotators then performed cross-document, cross-language NIL clustering, aided by English descriptions of non-English entities. The following table"
L18-1245,W16-1005,1,0.879699,"Missing"
L18-1265,P98-1013,0,0.752656,"ns, their participants, and their locations in text data. Given 1672 LORELEI’s low resource language setting combined with the need to simultaneously create resources for dozens of languages, the SSA task was designed with a naïve annotator in mind (i.e. without formal linguistic training or prior annotation experience). In this way it contrasts with more complex predicate-argument focused semantic representation schemes like Abstract Meaning Representation (AMR) (Banarescu, et al., 2013) Automatic Content Extraction (ACE) (Doddington, et al., 2004), PropBank (Palmer, et al., 2005), FrameNet (Baker, et al. 1998), Richer Event Description (RED) (Ikuta, et al., 2014), and Universal Decompositional Semantics (UDS) (White, et al., 2016), which require a background in linguistics and/or a long training period. In order to make SSA feasible for non-experts to master quickly, we annotate a small number of broad, underspecified predicate and argument categories that do not require fine-grained semantic distinctions. We generally select names, pronouns, or heads of nominal phrases as annotation extents, but annotators are allowed to select “intuitive extents” if needed (e.g. for multiword expressions), meanin"
L18-1265,W13-2322,1,0.919361,"Missing"
L18-1265,W14-2903,0,0.0418391,"Missing"
L18-1265,J05-1004,0,0.567902,"ts and disaster-relevant situations, their participants, and their locations in text data. Given 1672 LORELEI’s low resource language setting combined with the need to simultaneously create resources for dozens of languages, the SSA task was designed with a naïve annotator in mind (i.e. without formal linguistic training or prior annotation experience). In this way it contrasts with more complex predicate-argument focused semantic representation schemes like Abstract Meaning Representation (AMR) (Banarescu, et al., 2013) Automatic Content Extraction (ACE) (Doddington, et al., 2004), PropBank (Palmer, et al., 2005), FrameNet (Baker, et al. 1998), Richer Event Description (RED) (Ikuta, et al., 2014), and Universal Decompositional Semantics (UDS) (White, et al., 2016), which require a background in linguistics and/or a long training period. In order to make SSA feasible for non-experts to master quickly, we annotate a small number of broad, underspecified predicate and argument categories that do not require fine-grained semantic distinctions. We generally select names, pronouns, or heads of nominal phrases as annotation extents, but annotators are allowed to select “intuitive extents” if needed (e.g. for"
L18-1265,W15-0812,1,0.88758,"Missing"
L18-1265,L16-1521,1,0.774453,"Representative Language Packs – consisting of large volumes of formal and informal monolingual and parallel (with English) text with a variety of manual annotations to support situational awareness, plus a lexicon, grammatical sketch and basic processing tools – are designed to enable research into language universals and cross-language projection. Incident Language Packs contain manually labeled evaluation data designed to test system performance on tasks related to situational awareness for one or more surprise languages per year that remain unknown until the start of the annual evaluation (Strassel and Tracey, 2016). Akan (Twi) Amharic Arabic Bengali English Farsi Hausa Hindi Hungarian Indonesian Mandarin Oromo Russian Somali Spanish Swahili Tagalog Tamil Thai Tigrinya Turkish Ukrainian Uyghur Uzbek Vietnamese Wolof Yoruba Zulu Table 1: LORELEI Representative and Incident Languages This paper focuses on two semantic annotation tasks developed by LDC to support LORELEI research and evaluation: Simple Semantic Annotation (SSA) and Situation Frame (SF). Both SSA and SF label basic information relevant to humanitarian aid and disaster relief (HADR) scenarios. Situation Frame annotation directly corresponds t"
L18-1265,D16-1177,0,0.0476168,"Missing"
L18-1516,W01-0513,0,0.175699,"countries with common research infrastructure, especially HLTs, with additional support in the form of training, travel grants, workshops, help-desks and other outreach. At the same time, clinical groups and the medical field are beginning to recognize the promise of LRs and HLTs not only in the mining and management of vast stores of text and speech data but also in the diagnosis and tracking of disorders and therapies. These shifts in emphasis should not, and generally are not, seen as proof that our communities have fully solved traditional problems such as multiword expression extraction (Schone & Jurafsky, 2001), language identification from text (da Silva & Lopes, 2006), speech synthesis (Sproat, 2010, p. 206) or speech recognition (Schwartz et al., 2011, p. 399) even in well-studied English. However, in a world with limited resources, they do suggest that sponsors recognize the growing need to address a wider range of new languages and technologies and the growing power of the commercial sector to improve performance in market products. 2. Data Centers as Global Networks LDC is a consortium of educational, research and technology development groups in the academic, government and commercial sectors"
L18-1516,L16-1674,1,0.89746,"Missing"
L18-1558,W14-2907,1,0.899127,"Missing"
L18-1558,araki-etal-2014-detecting,0,0.0892141,"to work that has been done for Entity Detection and Linking (EDL) (Ji et al., 2010), linking to such a KB of events would reduce the need to compare every relevant document-level event hopper to every potentially coreferent hopper, since many document-level hoppers could be linked directly to the event KB. The remaining document-level event hoppers that are not found in the KB would still need to be coreferenced via a pairwise comparison as in this paper (as NIL clusters are created for EDL entities). This direction, however, would require building such a KB before annotation. Giraldi et al. (2014) adopted this approach and demonstrated feasibility, but the event and coreference 7. Conclusion We created a small corpus annotated for cross-document and cross-lingual event coreference in 505 documents in three languages. Although we leveraged existing ERE annotation as input, this task required the development of new annotation guidelines, new data processes and user interfaces, and the creation of new cross-document and cross-lingual annotation. The more intuitive, coarser grained event hopper concept that was originally developed as part of within-document Rich ERE annotation (Song et al"
L18-1558,bejan-harabagiu-2008-linguistic,0,0.0363816,"ation extracted from multiple languages, sources and genres. In the sections that follow we present the results of our effort to define an approach to crossdocument, cross-lingual event coreference using event hoppers as part of the Rich ERE annotation task in DEFT. 2. Related Work There have been other efforts that have captured some variety of cross-document event-event coreference, which have informed our design of the cross-document event coreference annotation task. These include topic-clustering of documents and pair-wise comparison of event mentions. The EventCorefBank (ECB) (Bejan and Harabagiu, 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi"
L18-1558,W16-1004,1,0.885851,"Missing"
L18-1558,cybulska-vossen-2014-using,0,0.0798195,"Missing"
L18-1558,L18-1245,1,0.834948,"Consortium (LDC) to support multiple research directions and evaluations in DEFT. ERE builds on the approach to labeling entities, relations, events and their attributes under a pre-defined taxonomy, following the approach used in Automatic Content Extraction (ACE) (LDC, 2005; Walker et al., 2006; Song et al., 2015; Mott et al., 2016). One important DEFT use case is automatically building a structured Knowledge Base (KB) from scratch. This task, known as Cold Start, is one of several tasks relevant to DEFT that are evaluated in the NIST Knowledge Base Population evaluation series (NIST 2017; Getman et al, 2018). Given DEFT’s focus on whole-corpus understanding culminating in the Cold Start task, Rich ERE annotation has evolved over the course of the program to emphasize cross-document and cross-lingual approaches. Rich ERE event annotation includes 9 event types and 38 subtypes (e.g. Conflict.Attack, Contact.Meet, Movement.TransportPerson). For each event mention annotators label the most salient word evoking the event (the “event trigger”), the event type and subtype, the realis status (Actual, Other or Generic), all of the event’s arguments (e.g. agent, instrument) and several attributes like temp"
L18-1558,girardi-etal-2014-cromer,0,0.0176587,", 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi et al., 2014). Event coreference in ECB+ required matching time, place, and participants (Cybluska and Vossen, 2014b). A richer event-event relation annotation scheme (Hong et al., 2016) was developed to capture cross-document eventevent relations, including coreference. This data was constructed using ACE 2005 data and supplemented with data collected by researchers; event relations including coreference were annotated by pairwise comparison of events from documents within a given topic. Event coreference required event arguments in the pair to match. To support the cross-document component of Event Argum"
L18-1558,W16-1701,0,0.0905076,"Missing"
L18-1558,D12-1045,0,0.0197889,"g event hoppers as part of the Rich ERE annotation task in DEFT. 2. Related Work There have been other efforts that have captured some variety of cross-document event-event coreference, which have informed our design of the cross-document event coreference annotation task. These include topic-clustering of documents and pair-wise comparison of event mentions. The EventCorefBank (ECB) (Bejan and Harabagiu, 2008) contains 482 documents clustered into 43 topics that were annotated for within-document and cross-document coreference according to the TimeML specification (Pustejovsky et al., 2013). Lee et al. (2012) extended ECB annotation, following the OntoNotes guidelines (Pradhan et al., 2007). These studies required both matching predicates and matching arguments for event coreference. The ECB+ corpus (Cybluska and Vossen, 2014a) is an extension of ECB with the addition of source documents as well as event components, using the CROMER (CROssdocument Main Events and entities Recognition) tool (Girardi et al., 2014). Event coreference in ECB+ required matching time, place, and participants (Cybluska and Vossen, 2014b). A richer event-event relation annotation scheme (Hong et al., 2016) was developed t"
L18-1558,W15-0812,1,0.87345,"task first defined as part of DARPA’s Deep Exploration and Filtering of Text (DEFT) program (DARPA, 2012). The goal of the DEFT program is to develop technologies capable of extracting knowledge from unstructured text in multiple languages and genres. ERE annotation was developed at Linguistic Data Consortium (LDC) to support multiple research directions and evaluations in DEFT. ERE builds on the approach to labeling entities, relations, events and their attributes under a pre-defined taxonomy, following the approach used in Automatic Content Extraction (ACE) (LDC, 2005; Walker et al., 2006; Song et al., 2015; Mott et al., 2016). One important DEFT use case is automatically building a structured Knowledge Base (KB) from scratch. This task, known as Cold Start, is one of several tasks relevant to DEFT that are evaluated in the NIST Knowledge Base Population evaluation series (NIST 2017; Getman et al, 2018). Given DEFT’s focus on whole-corpus understanding culminating in the Cold Start task, Rich ERE annotation has evolved over the course of the program to emphasize cross-document and cross-lingual approaches. Rich ERE event annotation includes 9 event types and 38 subtypes (e.g. Conflict.Attack, Co"
L18-1682,bendahman-etal-2008-quick,1,0.747611,"ndividual speaker within this subset of data. Diarization was performed on speech segments only (i.e., voices of singers in music segments were not included), and the speaker sex and light LID labels from the SAD task were not applied to the diarized segments. 3.3 Transcription Sub-corpus A portion of the main corpus was selected for transcription in each of the primary languages The Transcription sub-corpus includes 30 hours of English, 40 hours of Iraqi Arabic, 50 hours of Egyptian Arabic, and 29 hours of Mandarin Chinese, transcribed in a Quick Rich transcription style (Glenn et al., 2010; Bendahman et al., 2008). Selection of videos for transcription favored data with a high ratio of speech to non-speech. We also tried to maximize the number of distinct speakers in this sub-corpus. The English files for transcription were selected from the Speaker Diarization sub-corpus described above. For the Egyptian and Iraqi transcription efforts, guidelines for standardized spelling of the dialectal varieties were taken from previous transcription projects involving these dialects undertaken at LDC (Maamouri et al., 2004; Habash et al., 2012). The transcription was carried out in at least two passes, with the f"
L18-1682,glenn-etal-2010-transcription,1,0.767056,"segments for each individual speaker within this subset of data. Diarization was performed on speech segments only (i.e., voices of singers in music segments were not included), and the speaker sex and light LID labels from the SAD task were not applied to the diarized segments. 3.3 Transcription Sub-corpus A portion of the main corpus was selected for transcription in each of the primary languages The Transcription sub-corpus includes 30 hours of English, 40 hours of Iraqi Arabic, 50 hours of Egyptian Arabic, and 29 hours of Mandarin Chinese, transcribed in a Quick Rich transcription style (Glenn et al., 2010; Bendahman et al., 2008). Selection of videos for transcription favored data with a high ratio of speech to non-speech. We also tried to maximize the number of distinct speakers in this sub-corpus. The English files for transcription were selected from the Speaker Diarization sub-corpus described above. For the Egyptian and Iraqi transcription efforts, guidelines for standardized spelling of the dialectal varieties were taken from previous transcription projects involving these dialects undertaken at LDC (Maamouri et al., 2004; Habash et al., 2012). The transcription was carried out in at lea"
li-etal-2010-enriching,E03-1026,0,\N,Missing
li-etal-2010-enriching,J93-2003,0,\N,Missing
li-etal-2010-enriching,H05-1012,0,\N,Missing
li-etal-2010-enriching,J00-2004,0,\N,Missing
li-etal-2010-enriching,J97-2004,0,\N,Missing
li-etal-2010-enriching,P02-1038,0,\N,Missing
li-etal-2012-linguistic,W11-2213,0,\N,Missing
li-etal-2012-linguistic,mcnamee-etal-2010-evaluation,1,\N,Missing
li-etal-2012-linguistic,W06-0206,1,\N,Missing
li-etal-2012-linguistic,simpson-etal-2010-wikipedia,1,\N,Missing
li-etal-2012-parallel,maamouri-etal-2008-enhancing,1,\N,Missing
li-etal-2012-parallel,D11-1018,0,\N,Missing
li-etal-2012-parallel,W04-2208,0,\N,Missing
li-etal-2012-parallel,W06-2717,0,\N,Missing
li-etal-2012-parallel,W04-1602,1,\N,Missing
li-etal-2012-parallel,H05-1012,0,\N,Missing
li-etal-2012-parallel,J03-1002,0,\N,Missing
li-etal-2012-parallel,grimes-etal-2012-automatic,1,\N,Missing
li-etal-2012-parallel,li-etal-2010-enriching,1,\N,Missing
li-etal-2012-parallel,W11-4305,0,\N,Missing
li-etal-2012-parallel,N06-1014,0,\N,Missing
maeda-etal-2006-new,maeda-strassel-2004-annotation,1,\N,Missing
maeda-etal-2006-new,medero-etal-2006-efficient,1,\N,Missing
maeda-etal-2008-annotation,maeda-strassel-2004-annotation,1,\N,Missing
maeda-etal-2008-annotation,maeda-etal-2006-new,1,\N,Missing
maeda-etal-2008-annotation,strassel-etal-2006-integrated,1,\N,Missing
maeda-etal-2008-creating,J93-2003,0,\N,Missing
maeda-etal-2008-creating,J05-4003,0,\N,Missing
maeda-etal-2008-creating,ma-2006-champollion,1,\N,Missing
maeda-etal-2008-creating,ma-cieri-2006-corpus,1,\N,Missing
maeda-etal-2008-creating,1999.mtsummit-1.79,1,\N,Missing
mcnamee-etal-2010-evaluation,E06-1002,0,\N,Missing
mcnamee-etal-2010-evaluation,strassel-etal-2008-linguistic,1,\N,Missing
mcnamee-etal-2010-evaluation,D07-1074,0,\N,Missing
medero-etal-2006-efficient,maeda-strassel-2004-annotation,1,\N,Missing
medero-etal-2006-efficient,maeda-etal-2006-new,1,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
saggion-etal-2002-developing,J93-1004,0,\N,Missing
saggion-etal-2002-developing,W97-0703,0,\N,Missing
saggion-etal-2002-developing,A00-2035,0,\N,Missing
saggion-etal-2002-developing,W00-0408,0,\N,Missing
saggion-etal-2002-developing,E99-1011,0,\N,Missing
saggion-etal-2002-developing,W97-0704,0,\N,Missing
saggion-etal-2002-developing,W00-0403,1,\N,Missing
saggion-etal-2002-developing,grover-etal-2000-lt,0,\N,Missing
saggion-etal-2002-developing,J96-2004,0,\N,Missing
saggion-etal-2002-developing,I05-2047,0,\N,Missing
saggion-etal-2002-developing,W01-0100,0,\N,Missing
simpson-etal-2010-wikipedia,mcnamee-etal-2010-evaluation,1,\N,Missing
simpson-etal-2010-wikipedia,strassel-etal-2008-linguistic,1,\N,Missing
song-etal-2010-enhanced,song-strassel-2008-entity,1,\N,Missing
song-etal-2010-enhanced,ma-2006-champollion,0,\N,Missing
song-etal-2010-enhanced,ma-cieri-2006-corpus,0,\N,Missing
song-etal-2010-enhanced,strassel-etal-2006-integrated,1,\N,Missing
song-etal-2010-enhanced,maeda-etal-2008-creating,1,\N,Missing
song-etal-2010-enhanced,1999.mtsummit-1.79,0,\N,Missing
song-etal-2010-enhanced,W09-3408,1,\N,Missing
song-etal-2012-linguistic,song-etal-2010-enhanced,1,\N,Missing
song-etal-2012-linguistic,strassel-etal-2008-new,1,\N,Missing
song-etal-2012-linguistic,li-etal-2010-enriching,1,\N,Missing
song-strassel-2008-entity,W03-2201,0,\N,Missing
strassel-2004-linguistic,maeda-strassel-2004-annotation,1,\N,Missing
strassel-etal-2000-quality,wayne-2000-multilingual,0,\N,Missing
strassel-etal-2000-quality,cieri-etal-2000-large,1,\N,Missing
strassel-etal-2006-integrated,W04-1602,1,\N,Missing
strassel-etal-2006-integrated,maeda-etal-2006-new,1,\N,Missing
strassel-etal-2008-linguistic,day-etal-2004-callisto,0,\N,Missing
strassel-etal-2008-new,przybocki-etal-2006-edit,0,\N,Missing
strassel-etal-2008-new,ma-2006-champollion,0,\N,Missing
strassel-etal-2008-new,strassel-etal-2006-integrated,1,\N,Missing
strassel-etal-2008-new,1999.mtsummit-1.79,0,\N,Missing
strassel-etal-2010-darpa,D08-1020,0,\N,Missing
W03-1507,cieri-liberman-2002-tides,0,0.0277191,"New approaches to research require not tens but hundreds and thousands of hours of speech data, and millions of words of text. The availability of high quality language resources remains a central issue for the many communities involved in basic research, technology development and education Shudong Huang Linguistic Data Consortium 3600 Market St., Ste 810 Philadelphia, PA 19104 shudong@ldc.upenn.edu technology development and education related to language. The role of international data centers continues to evolve to accommodate emerging needs in the speech and language technology community (Liberman and Cieri 2002). The Linguistic Data Consortium (LDC) was founded in 1992 at the University of Pennsylvania, with seed money from DARPA, specifically to address the need for shared language resources. Since then, LDC has created and published more than 241 linguistic databases and has accumulated considerable experience and skill in managing large-scale, multilingual data collection and annotation projects. LDC has established itself as a center for research into standards and best practices in linguistic resource development, while participating actively in ongoing HLT research. LDC has had a major role in"
W10-0707,D08-1027,0,\N,Missing
W10-0707,P09-2078,0,\N,Missing
W10-0707,D09-1030,0,\N,Missing
W10-0707,kaisser-lowe-2008-creating,0,\N,Missing
W10-0707,D09-1006,0,\N,Missing
W14-2907,W04-0803,0,0.0335914,"result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based"
W14-2907,P98-1013,0,0.0831239,"82). Frames are descriptions of event (or state) types and contain information about event participants (frame elements), information as to how event types relate to each other (frame relations), and information about which words or multi-word expressions can trigger a given frame (lexical units). FrameNet is designed with text annotation in mind, but unlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and att"
W14-2907,S07-1018,0,0.00934785,"ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs"
W14-2907,mcnamee-etal-2010-evaluation,1,0.552655,"Event). Additionally, both annotation guidelines agree on the following: • ERE is limited to pre-specified arguments for each event and relation subtype. The possible arguments for ACE are: Event participants (limited to pre-specified roles for each event type); Event-specific attributes that are associated with a particular event type (e.g., the victim of an attack); and General event attributes that can apply to most or all event types (e.g., time, place). • Tagging of Resultative Events (states that result from taggable Events) 49 information to entity profiles that is missing from the KB (McNamee et al., 2010). Due to its generous license and large scale, a snapshot of English Wikipedia from late 2008 has been used as the reference KB in the TAC-KBP evaluations. • ACE tags arguments regardless of modal certainty of their involvement in the event. ERE only tags asserted participants in the event. • The full noun phrase is marked in both ERE and ACE arguments, but the head is only specified in ACE. This is because ACE handles entity annotation slightly differently than ERE does; ACE marks the full noun phrase with a head word for entity mention, and ERE treats mentions differently based on their synt"
W14-2907,N10-1138,0,0.0125127,"re more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting in 2012 TAC-KBP introduced the “Cold Start” task, which is to literally produce a KB based on the Slot Filling schema. To date, Cold Start KBs have been built fro"
W14-2907,doddington-etal-2004-automatic,1,0.823282,"Missing"
W14-2907,J02-3001,0,0.0107376,"nlike ACE/ERE it prioritizes lexicographic and linguistic completeness over ease of annotation. As a result Frames tend to be much finer grained than ACE/ERE events, and are more numerous by an order of magnitude. The Berkeley FrameNet Project (Baker et al., 1998) was developed as a machine readable database of distinct frames and lexical units (words and multi-word constructions) that were known to trigger specific frames.1 FrameNet 1.5 includes 1020 identified frames and 11830 lexical units. One of the most widespread uses of FrameNet has been as a resource for Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002). FrameNet related SRL was promoted as a task by the SENSEVAL-3 workshop (Litkowski, 2004), and the SemEval-2007 workshop (Baker et al., 2007). (Das et al., 2010) is a current system for automatic FrameNet annotation. The relation and attribute types of TAC-KBP and the relation and event types in the ACE/ERE standards can be mapped to FrameNet frames. The mapping is complicated by two factors. The first is that FrameNet frames are generally more fine-grained than the ACE/ERE categories. As a result the mapping is sometimes one-to-many. For example, the ERE relation AfAdditional tasks Starting"
W14-2907,C98-1013,0,\N,Missing
W14-3612,W14-1604,1,0.873348,"buṣṣ/ “look” is  أنظر/’unZur/ in MSA. 3 per as part of the automatic transliteration step because they target the same conventional orthography of dialectal Arabic (CODA) (Habash et al., 2012a, 2012b), which we also target. There are several commercial products that convert Arabizi to Arabic script, namely: Microsoft Maren, 2 Google Ta3reeb, 3 Basis Arabic chat translator4 and Yamli.5 Since these products are for commercial purposes, there is little information available about their approaches, and whatever resources they use are not publicly available for research purposes. Furthermore, as Al-Badrashiny et al. (2014) point out, Maren, Ta3reeb and Yamli are primarily intended as input method support, not full text transliteration. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arab"
W14-3612,W12-4808,0,0.117255,"(MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-trans"
W14-3612,R13-1026,0,0.0169932,"d topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2"
W14-3612,W14-3901,1,0.757221,"Missing"
W14-3612,N13-1066,1,0.386912,"&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2, LDC collected large volumes of naturally occurring informal text (SMS) and chat messages from individual users in English, Chinese and Egyptian Arabic (Song et al., 2014). Altogether we recruited 46 Egyptian Arabic participants, and of those 26 contributed data. To protect privacy, participation was"
W14-3612,P11-2008,0,0.0502639,"Missing"
W14-3612,W11-0704,0,0.0923825,"mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level"
W14-3612,habash-etal-2012-conventional,1,0.702911,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,W12-2301,1,0.860943,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,P97-1017,0,0.0276316,"tion. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arabizi (Al-Badrashiny et al., 2014). Name Transliteration There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter"
W14-3612,maamouri-etal-2014-developing,1,0.720718,"ect Arabizi is used to write in multiple dialects of Arabic, and differences between the dialects themselves have an effect on the spellings chosen by individual writers using Arabizi. Because Egyptian Arabic is the dialect of the corpus cre1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op erational_Language_Translation_%28BOLT%29.aspx 94 ated for this project, we will briefly discuss some of the most relevant features of Egyptian Arabic with respect to Arabizi transliteration. For a more extended discussion of the differences between MSA and Egyptian Arabic, see Habash et al. (2012a) and Maamouri et al. (2014). Phonologically, Egyptian Arabic is characterized by the following features, compared with MSA: (a) The loss of the interdentals /ð/ and /θ/ which are replaced by /d/ or /z/ and /t/ or /s/ respectively, thus giving those two original consonants a heavier load. Examples include  ذكر/zakar/ “to mention”,  ذبح/dabaħ/ “to slaughter”,  ثلج/talg/ “ice”,  ثمن/taman/ “price”, and  ثبت/sibit/ “to stay in place, become immobile”. (b) The exclusion of /q/ and /ǰ/ from the consonantal system, being replaced by the /ʔ/ and /g/, e.g.,  قطن/ʔuṭn/ “cotton”, and جمل /gamal/ “camel”. At the level"
W14-3612,pasha-etal-2014-madamira,1,0.89422,"Missing"
W14-3612,D11-1141,0,0.0398961,"is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communic"
W14-3612,W11-2602,1,0.567837,"013). Social media communication in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary i"
W14-3612,voss-etal-2014-finding,0,0.0375887,"then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-translatortransforms-social-media-analysis/ 5 http://www.yamli.com/ 3 95 multilingual documents using a generative mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic y"
W14-3612,P11-2007,0,0.0118384,"ere has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al."
W14-3612,Q14-1003,0,0.0204432,"een these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here"
W14-3612,W02-0505,0,\N,Missing
W14-3612,W14-3629,0,\N,Missing
W14-3612,song-etal-2014-collecting,1,\N,Missing
W14-3612,N06-1060,0,\N,Missing
W15-0809,doddington-etal-2004-automatic,1,0.296897,"nre NW DF NW DF Documents 77 74 101 99 Tokens 44,962 70,427 50,997 169,740 Table 1. Event Nugget Data Profile While the Light ERE and KBP Event Argument tasks rely on character offsets for annotation and scoring, the Event Nugget Tuple Scorer 2 (Liu, Mitamura & Hovy, 2015) requires tokenized data. Therefore, prior to annotation, all selected documents were automatically tokenized in the Penn English Treebank style. No manual correction was performed on the tokenization due to time constraints. 8 8.1 Corpus and Consistency Analysis Corpus Experience with event annotation for Light ERE and ACE (Doddington et al., 2004) and related tasks suggests that a major challenge for annotation consistency is poor recall – human annotators are not highly consistent in recognizing that a mention has occurred. To reduce the impact of this known issue for the Event Nugget task, two anno2 Event Nugget Tuple refers to the tuple made up of the nugget, event type/subtype, and realis. tators independently labeled each document (two first pass annotation passes, referred to as FP1 and FP2 below); a senior annotator then adjudicated discrepancies to create a gold standard. The team consisted of four first pass annotators, two of"
W15-0809,W15-0807,1,0.678904,"ed improvement in annotation quality in the workflow by comparing the adjudicated (ADJ) and first (FP1 and FP2) passes, shown in the columns ADJ vs. FP1 and FP2 in Table 3. The noticeable improvement in score shows the advantage of including adjudication as part of the annotation process. (For IAA purposes, there is obviously no gold or system, but in order to use the scorer we arbitrarily treated one file as the “gold”.) FP1 vs. FP2 Consistency Analysis We examined annotation consistency and quality by comparing different passes of the eval set annotation using the Event Nugget Tuple Scorer (Liu, Mitamura, & Hovy, 2015) developed for the event nugget evaluation task. This scorer treats one file as “gold” and the other as “system”, and matches each nugget in the gold file to one or more nuggets in the system file. This mapping is based on the overlap of the nugget spans. By nugget span, we 3 16 event nuggets in the training set did not receive a realis attribute, due to annotation error. 72 ADJ vs. FP1 78.2 71.7 63.2 ADJ vs. FP2 Span 69.0 89.3 Type 68.2 84.3 Realis 60.0 85.7 Table 3. Scores for Event Nugget Eval Set Annotation To gain some further insight into these numbers we expanded the analysis in two di"
W15-0809,E12-2021,0,\N,Missing
W15-0812,bies-etal-2014-incorporating,1,0.734052,"r Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST) that was developed to encourage research in natural language p"
W15-0812,doddington-etal-2004-automatic,1,0.813222,"nships among them. Given the variety of approaches and evaluations within DEFT, we set out to define an annotation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased a"
W15-0812,W14-2904,1,0.767479,"to be met by the end of this year. 4.1 Smart Data Selection In an attempt to minimize annotator effort on documents with insufficient content, documents were fed into the annotation pipeline in descending order of event trigger density, defined as the number of event triggers per 1,000 tokens. Triggers were automatically tagged using a deep neural network based tagger trained on the ACE 2005 annotations (Walker et al., 2006) with orthographic and word 96 Rich ERE Challenges and Next Steps Inter-Annotator Agreement Work on inter-annotator agreement (IAA) will be based on the method outlined in Kulick et al. (2014), which described a matching algorithm used at each level of the annotation hierarchy, from entity mentions to events. This work focused on the evaluation for entity, relation, and event mentions, as well as for entities overall. The algorithm for entity mention mapping is based on the span for an entity mention, while the mapping for relation and event mentions is more complex, based on the mapping of the arguments, which in turn depends on the entity mention mapping. IAA work will be conducted on dual annotation for Rich ERE. Analysis will be reported in the future. 5 Conclusion Rich ERE ann"
W15-0812,W15-0809,1,0.3802,"collection and add it to a new or existing knowledge base. In 2014, TAC KBP moved into the events domain with the addition of the Event Argument Extraction (EAE) evaluation, in which systems were required to extract mentions of entities from unstructured text and indicate the roles they played in events as supported by text (Ellis et al., 2014). Additionally, TAC KBP 2014 also conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute (Mitamura et al., 2015). TAC KBP 2015 EAE and END evaluations both plan to expand the tasks such that event tuples would be grouped together or linked to one another to show event identity, either by linking event arguments that participate in the same event (EAE) or by grouping event nuggets that refer to the same event (END). Such expansion in both evaluations would require identification of event coreference, which is a challenging issue in both ACE and Light ERE. The transition from Light ERE to Rich ERE tackles this challenge with the addition of event hoppers. 3 Transition from Light ERE to Rich ERE The simpli"
W15-0812,J14-4007,0,0.017686,"ons, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over time. In the first phase of the program, we defined Light ERE as a simplified form"
W15-0812,W11-0419,0,0.0236293,"election process have been very encouraging, with annotators reporting much richer documents on average, compared to the prior approach in which no ranking was imposed. 4.2 One of the challenges in event annotation is to determine the level of granularity that will be distinguished as sub-event vs. event hopper. We observed this issue in our pilot Rich ERE annotation, and the goal is to have sub-event annotation be a relationship between event hoppers in the future. In order to represent the relations between event hoppers, we are planning the addition of a notion such as Narrative Container (Pustejovsky and Stubbs, 2011) to capture non-identity eventevent relations such as causality, part-whole, precedence, enablement, etc. Event hoppers will serve as a level between individual event mentions and Narrative Containers. Event hoppers will be grouped into Narrative Containers, and so relations will be between event hoppers, instead of between individual event mentions. More specific relations between individual event mentions can then be derived from the event-event relations between the event hoppers within narrative containers or from relations between narrative containers. 4.3 The overall target for this phas"
W15-0812,strassel-etal-2010-darpa,1,0.908256,"notation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over ti"
W15-0812,wright-etal-2012-annotation,1,0.855178,"ent hopper in Rich ERE, and all tagged event mentions that refer to the same event occurrence will be grouped into the same event hopper. Event hoppers will allow annotators to group together more event mentions and therefore also label more event arguments in Rich ERE. This richer annotation will lead to a more complete knowledge base and better support for the Event Argument Linking and END evaluations in 2015, when one of the goals is to evaluate event identity. 3.2 Development of an Annotation GUI for Rich ERE The Rich ERE annotation tool was developed following the framework described in Wright et al. (2012), allowing for rapid development of a new interface for Rich ERE. Numerous features were included “for free” in that they were developed for previous interfaces, and therefore required no additional development time. One important example of this is the representation of annotated text extents with underlines that can overlap arbitrarily, be color coded based on other annotations (e.g., entity type), and allow the user to click to navigate among the annotations. An important feature developed specifically for the Rich ERE tool is a “reference annotation”, which is essentially one widget pointi"
W15-0812,W12-6306,0,0.0230474,"ACE, only attested actual events are annotated (no irrealis events or arguments). Our Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards a"
W15-0812,W14-2907,1,\N,Missing
W16-1004,W13-2322,1,0.735417,"me. RED also labels a causal and temporal relation between the two events, &quot;BEFORE/PRECONDITIONS&quot;, showing that the quitting event leads to, but does not directly cause, the replacement, and a temporal CONTAINS relation linking quit to Wednesday.     Event 1: quit - BEFORE DOCTIME, Actual Modality Event 2: replace - AFTER DOCTIME, Actual Modality Relation 1: quit BEFORE/ PRECONDITIONS replace Relation 2: Wednesday CONTAINS quit Although RED does not annotate the arguments of events, it is intended to be combined with semantic role annotations such as PropBank (Bonial et al., 2014) or AMR (Banarescu et al., 2013), which would provide the argument information. For this example, the quit and replace events would also be given the predicate argument structures below: quit.01 Arg0: Media Tycoon Barry Diller Arg1: as chief of Vivendi Universal Entertainment ArgM-TMP: on Wednesday replace.01 Arg2: Parent company chairman JeanRene Fourtou Arg1: Diller ArgM-MOD: will ArgM-PRD: as chief executive of US unit. EER: The following events are connected by Condition and Temporality relations:   Event 1 (Personnel.EndPosition): quit Event 2 (Personnel.StartPosition): replace 34 A preliminary analysis of the Rich ER"
W16-1004,W14-2903,1,0.927744,"Missing"
W16-1004,D12-1045,0,0.048737,"y – along with 21 sense-based subtypes (or relation senses), as shown in Table 1. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency_Causality will play Cause and Result roles respectively. Figure 1 shows more information about types and roles. Figure 1: Roles and examples specific to fine-grained event-event relation subtypes. 2.5 Richer Event Descriptions (RED) 3 RED annotation (Ikuta et al., 2014) marks all events in a document, as well as certain relations between those events. RED combines coreference (Pradhan et al., 2007; Lee et al., 2012) and THYME Temporal Relations annotation (Styler et al., 2014) to provide a thorough representation of entities, events and their relations. The RED schema also goes beyond prior annotations of coreference or temporal relations by also annotating subevent structure, cause-effect relations and reporting relations. Guidelines for RED annotation can be found at https://github.com/timjogorman/RicherEventDescr iption/blob/master/guidelines.md. 31 Annotation Data Annotated Features and The representation of events and the scope of annotation vary across the different annotation approaches. Table 2 c"
W16-1004,W15-0809,1,0.776814,"verlapping data set could be used to explore how the differences in annotation procedure lead to differences in decisions about event granularity. 2.3 Event Nugget (EN) An Event Nugget is a tuple of an event trigger, classification of event type and subtype, and realis attribute. It is similar to an event mention in ERE, but arguments are not labelled. EN annotation in 2014 focused on event nuggets (expanded triggers) only, and followed the same taxonomy of 33 event types and subtypes as Light ERE. However, instead of tagging minimal extent as the trigger, EN allowed multi-word event nuggets (Mitamura et al., 2015). Multi-word event nuggets can be either continuous or discontinuous, and are based on the goal of marking the maximal extent of a semantically meaningful unit to express the event in a sentence. EN also added a realis attribute for each event mention. The realis attribute labels each event as Actual, Generic, or Other. TAC KBP 2014 conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute. 30 Table 1: EER event relation types. In 2015,"
W16-1004,W15-0812,1,0.92996,"e taggable, and entity subtypes are not labeled). The event ontology of Light ERE is similar to ACE, with slight modification and reduction, and there is strict coreference of events within documents (Aguilar et al., 2014). As in ACE, the annotation of each event mention includes the identification of a trigger, the labeling of the event type, subtype, and participating event argument entities and time expressions. Simplifying from ACE, only attested actual events are annotated (no irrealis events or arguments). Rich ERE annotation expands on both the inventories and taggability of Light ERE (Song et al., 2015). Rich ERE Entity annotation adds nonspecific entities and nominal head marking, in addition to adding a distinction between Location and Facility entity types. Rich ERE Relation annotation doubles the Light ERE ontology to twenty relation subtypes, and also adds future, hypothetical, and 28 conditional relations. A new category of argument fillers was added for Rich ERE, to allow arguments that are not taggable as entities to be used as fillers for specific relation and event subtypes. For each event mention, Rich ERE labels the event type and subtype, its realis attribute, any of its argumen"
W16-1004,W16-1005,1,0.820435,"event relation types. In 2015, TAC KBP ran an open evaluation on EN that was expanded to three evaluation tasks: Event Nugget Detection, Event Nugget Detection and Coreference, and Event Nugget Coreference. Full Event Nugget Coreference is identified when two or more Event Nuggets refer to the same event. EN annotation in 2015 followed the Rich ERE event taxonomy, which added 5 event types and subtypes to make a total of 38 event types and subtypes, and also followed the Rich ERE guidelines on trigger extents, which adopted the minimal extent rule and disallowed discontinuous event triggers (Song et al., 2016). Annotation of Event Nugget Coreference adopted the concept of Event Hopper as in Rich ERE. 2.4 Event-Event Relations (EER) EER annotation focuses on relations between events in the ERE/ACE taxonomy, both within document and cross-document (Hong et al., 2016). Our general goal is to construct event-centric knowledge networks, where each node is an event and the edges effectively capture the relations between any two events. EER includes five main types of event relations – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 sense-based subtypes (or relation senses)"
W16-1005,babko-malaya-etal-2012-identifying,0,0.0482332,"Missing"
W16-1005,doddington-etal-2004-automatic,1,0.709108,"nault chief George Besse in 1986 and the head of government arms sales Rene Audran a year earlier. In 2015 EN annotation, the word “murder” would be the trigger for two Life.Die events, one with the victim “George Besse” and the other with “Rene Audran” as well as two Conflict.Attack events, one occurring in 1986 and the other in 1985. In 2014 EN annotation, the word “murder” would be the trigger for only one Conflict.Attack event. 3.2 Event Taxonomy EN annotation and evaluation focus on a limited inventory of event types and subtypes, as defined in ERE, based on Automatic Content Extraction (Doddington et al., 2004; Walker et al., 2006; Aguilar et 39 al., 2014; Song et al., 2015). The 2014 EN evaluation covered the inventory of event types and subtypes from Light ERE, including 8 event types and 33 subtypes. The 2015 evaluation added a new event type (Manufacture) and four new subtypes – Movement.TransportArtifact, Contact.Broadcast, Contact.Contact, Transaction.Transaction – which aligned the EN event ontology with that of Rich ERE in order to take advantage of the existing Rich ERE annotated data as training data. The EN annotation task also adopted a new approach for applying the Contact event subtyp"
W16-1005,W15-0807,1,0.796291,"Missing"
W16-1005,W15-0809,1,0.896858,"these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that focused on the text extents establishing valid references to events, clarifications on transaction event types, and the additional annotation of event realis attributes, which indicated whether each event mention was asserted (Actual), generic or habitual (Generic), or some other category, such as future, hypothetical, negated, or uncertain (Other) (Mitamura, et al., 2015) . In 2015, EN annotation followed the Rich ERE Event annotation guidelines (except for the annotation of event arguments). As compared to EN annotation in 2014, Rich ERE Event annotation and 2015 EN annotation include increased taggability in several areas: slightly expanded event ontology, additional attributes for contact and transaction events, and double tagging of event mentions for multiple types/subtypes and for certain types of coordination, in addition to event coreference.General Instructions 3 Event Nugget Annotation In this section, we describe the EN annotation as well as the maj"
W16-1005,N04-1019,0,0.189396,"Missing"
W16-1005,W15-0812,1,0.922361,"icipating systems must identify full event coreference links, given the annotated event nuggets in the text. ERE was developed as an annotation task that would be supportive of multiple research directions and evaluations in the DEFT program, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting ERE annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text (Light ERE), to a richer representation of phenomena of interest to the program (Rich ERE) (Song, et al., 2015). In ERE Event annotation, each event mention has annotation of event type and subtype, its realis attribute, any of its arguments or participants that are present, and a required “trigger” string in the text; furthermore, event mentions within the same document are coreferenced into event hoppers (Song, et al., 2015). EN annotation includes all of these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that fo"
W16-1005,W14-2907,1,\N,Missing
W19-6808,L18-1293,0,0.0456535,"Missing"
W19-6808,kulick-etal-2010-consistent,1,0.673071,"Missing"
wright-etal-2012-annotation,strassel-etal-2010-darpa,1,\N,Missing
wright-etal-2012-annotation,maeda-etal-2010-technical,1,\N,Missing
