2004.iwslt-evaluation.12,hogan-frederking-1998-evaluation,0,\N,Missing
2004.iwslt-evaluation.12,A94-1016,0,\N,Missing
2004.iwslt-evaluation.12,J93-2003,0,\N,Missing
2004.iwslt-evaluation.12,W00-0203,0,\N,Missing
2005.iwslt-1.14,J93-2003,0,0.0102317,"Missing"
2005.iwslt-1.14,J82-2005,0,0.0798235,"udied. We translate numeral phrases first by using a standard templates depository. We develop a phrase-based decoder that employs a beam search algorithm. To make the result more reasonable, we apply those words with fertility probability of zero. We improve the previously proposed tracing back algorithm to get the best path. Some experiments concerned are presented. 1 Introduction Statistical machine translation is a promising approach for large vocabulary text translation. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translatio"
2005.iwslt-1.14,P01-1067,0,0.0994031,"al machine translation is a promising approach for large vocabulary text translation. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the transla"
2005.iwslt-1.14,1983.tc-1.13,0,0.514714,"ion. In the early 90s, IBM developed Candide system. Since then, many statistical machine translation systems were proposed [2][3]. These systems apply a translation model to capture the relationship between the source and target languages, and use a language model to drive the search process. The primary IBM model was purely word-based. To get more complex structure, better lexical choice and more reliable local reordering, the phrase-based statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the translation quality largely depends on the accuracy of phrase-to-phrase translation pair"
2005.iwslt-1.14,N03-1017,0,0.0722604,"sed statistical machine translation systems were proposed. Yamada and Knight [4] used phrase translation in a syntax-based translation system; March and Wong [5] introduced a joint-probability model for phrase translation; CMU and IBM also improved their systems with phrase translation capability. Our system applies a phrase-based translation model to capture the corresponding relationship between two languages. We propose a formula to compute the phrase translation probability through word alignment. The phrase-based decoder we developed employs a beam search algorithm, similar to the one in [6]. We applied zero fertility words in the target language. Because the translation quality largely depends on the accuracy of phrase-to-phrase translation pairs extracted from bilingual corpora, we propose a different tracing back algorithm to find the best path. Four methods are studied to extract bilingual phrase pairs. We describe these methods and phrase-based translation model in Section 2. Section 3 explains the method of numeral phrase translation. Section 4 outlines the architecture of the decoder that combines the translation model, distortion model, language model to generate target s"
2005.iwslt-1.14,C96-2141,0,0.149702,"der two constraints: (1). all the cells in the expanded region accord with the evaluation function ; (2). all the cells should not be marked. d. The words in this area make up of a phrase pair. Mark all nodes between x-coordinate and y-coordinate in this matrix, then search other max points and corresponding rectangles among the rest unmarked nodes until all nodes in the MI matrix of this sentence pair are marked.[7] 2-3 Extracting Phrase Pairs From HMM Word Alignment Model A simple way to extract phrase pairs is using a word alignment model. We use the HMM-based alignment model introduced in [8]. For a source phrase that ranges from position j1 to j2 in sentence, we can get the corresponding target phrase’s beginning position and ending position to extract the phrase translation. Just like the method described in 2-1, a given factor that prevents the length of the phrase pairs differ greatly is needed. 2-4 Extracting phrase pair by Giza++ toolkit The Giza++ toolkit can be used to establish word-based alignments. There are some heuristic functions can improve the quality of alignment and extract phrase pair. In [6], the parallel corpus is aligned bidirectionally, some additional align"
2005.iwslt-1.14,W99-0604,0,0.0566577,"events the length of the phrase pairs differ greatly is needed. 2-4 Extracting phrase pair by Giza++ toolkit The Giza++ toolkit can be used to establish word-based alignments. There are some heuristic functions can improve the quality of alignment and extract phrase pair. In [6], the parallel corpus is aligned bidirectionally, some additional alignment points are added to the intersection of the two alignments. All aligned phrase pairs are connected to be consistent with the word alignment: each word corresponds strictly to another word in a legal phrase pair, not to any word outside the pair [9]. 2-5 Phrase Translation Probability CMU used the phrase translation probability formula based on the IBM1 alignment model: p( c |e ) = ∏∑ p( ci |e j ) (2-2) i j There is a drawback for this method: If only one word of source phrase has no appropriate corresponding word in target phrase, the phrase translation probability will be small. Since there are many auxiliary words and mood words in Chinese, this issue becomes more serious. To prevent this, we use the word alignment generated by the IBM model 4 to divide the whole phrase pair into several small phrase pair blocks. If one source word"
2005.iwslt-1.14,W01-1408,0,0.0408713,"the source phrase of the existing hypothesis. The hypotheses are stored in different stacks. Each of them has a sequence number. The odd stack s2 p −1 contains all hypotheses whose target phrases are not F-zerowords and in which p source words have been translated so far. (If the target phrase of the hypothesis is not F-zeroword, it stored in the stack 2p-1, p is the number of source words translated), the even stack s2 p contains all hypotheses whose target phrases are F-zerowords and in which p source words have been translated accumulatively. We recombine search hypotheses as described in [10], and prune out weak hypotheses based on the probability they incurred so far and a future score estimated as in [6]. All these reduce the number of hypotheses stored in stacks to speed up the decoder. The current probability of the new hypothesis is the probability of the original hypothesis multiplied with the translation, distortion and language probability of the added phrasal translation, the probability formula is: p( e |c ) = pT ( c |e)λt × pL ( e) λl × pD ( e, c) λd (3-1) In which pT ( c |e) is the translation model computed according to (2-3), pL ( e) is the target language model in w"
2006.iwslt-evaluation.13,2003.mtsummit-papers.53,0,0.0199386,"osed [2] [3]. These systems are based on the source channel model and apply a translation model to capture the relationship between the source language and the target language, and use a language model to drive the search process. The primary IBM model was purely word-based model and one improvement is phrase-based statistical translation model. The phrase-based SMT model is proposed to incorporate more complex structure and to get better lexical choice and more reliable local reordering. There are many researchers using the phrase-based translation method to improve their systems performance [4][5][6]. In our system, we apply a phrasebased translation model. This paper is organized as follows: section 2 describes a hybrid approach to Chinese-to-English translation system, which combines the TBMT system and phrase-based SMT system. Section 3 presents the improvements of base system for the evaluation. In section 4, we present a series of experiments of Chinese-to-English translation and the results are analyzed. Section 5 gives the conclusion. 2. System Description Our system combines two translation engines: the templatebased translation engine and phrase-based statistical translatio"
2006.iwslt-evaluation.13,P01-1067,0,0.0573686,"d [2] [3]. These systems are based on the source channel model and apply a translation model to capture the relationship between the source language and the target language, and use a language model to drive the search process. The primary IBM model was purely word-based model and one improvement is phrase-based statistical translation model. The phrase-based SMT model is proposed to incorporate more complex structure and to get better lexical choice and more reliable local reordering. There are many researchers using the phrase-based translation method to improve their systems performance [4][5][6]. In our system, we apply a phrasebased translation model. This paper is organized as follows: section 2 describes a hybrid approach to Chinese-to-English translation system, which combines the TBMT system and phrase-based SMT system. Section 3 presents the improvements of base system for the evaluation. In section 4, we present a series of experiments of Chinese-to-English translation and the results are analyzed. Section 5 gives the conclusion. 2. System Description Our system combines two translation engines: the templatebased translation engine and phrase-based statistical translation e"
2006.iwslt-evaluation.13,W02-1018,0,0.0144003,"2] [3]. These systems are based on the source channel model and apply a translation model to capture the relationship between the source language and the target language, and use a language model to drive the search process. The primary IBM model was purely word-based model and one improvement is phrase-based statistical translation model. The phrase-based SMT model is proposed to incorporate more complex structure and to get better lexical choice and more reliable local reordering. There are many researchers using the phrase-based translation method to improve their systems performance [4][5][6]. In our system, we apply a phrasebased translation model. This paper is organized as follows: section 2 describes a hybrid approach to Chinese-to-English translation system, which combines the TBMT system and phrase-based SMT system. Section 3 presents the improvements of base system for the evaluation. In section 4, we present a series of experiments of Chinese-to-English translation and the results are analyzed. Section 5 gives the conclusion. 2. System Description Our system combines two translation engines: the templatebased translation engine and phrase-based statistical translation engi"
2006.iwslt-evaluation.13,P02-1038,0,0.0621246,"ample 2: #QP 个 带|有 N1 的 N2 ＝&gt; !QP !N2+s with !N1. With the template, the input Chinese ‘一个带空调的单人 间 ’ will be translated into ‘One single room with air condition.’, and the input Chinese ‘两个有电话的双人间’ will be translated into ‘Two double rooms with telephone’. In our system, four hundred and sixty five templates are summarized from the training data. For more details of the template-based translation system, please refer to the reference [1]. 2.2. Phrase-based translation model 2.2.1. Phrase-based translation model In our system, the phrase-based translation model is based on the log-linear model [7] and the phrase we mention here is composed of a series of words that perhaps possess no syntax or semantic meanings. In the log-linear model, given the sentence f (source language), the translating process is searching the translation e (target language) with the highest probability: M e* = arg max ∑ λm hm (e, f ) e (2) m =1 where hm(e,f) is a feature function and λm is the model parameter. 2.2.2. Phrase extraction Word alignments are first obtained by using the GIZA++ toolkit in both translation directions and then summarizing the two alignments. We use a number of heuristics, which belongs"
2006.iwslt-evaluation.13,koen-2004-pharaoh,0,0.058438,"q ( e k |f k ) are the probabilities of phrase frequency. λ i (i=1 to 4) is the parameter of probability. z Distortion model: in our system, we use a simple distortion model: (4) Pd ( a − b ) = λ |a k − b k − 1 − 1 | k k −1 Where a k denotes the start position of the source phrase that was translated into the kth English phrase, and b k − 1 denotes the end position of the source phrase translated into the (k1)th English phrase. 2.2.4. Decoding strategy In the phrase-based statistical machine translation system, the decoder employs a beam search algorithm that is similar to the Pharaoh decoder [9]. Considering the different expression habits between Chinese and English, some words must be complemented when translating Chinese sentences into English. For example, some frequent words, such as “a, an, of, the”, are difficult to extract because those words have zero fertility and correspond to NULL in IBM model 4. We call them F-zerowords. When decoding, the F-zerowords can be added after each new hypothesis, which means, a NULL is added after each phrase in the source sentences. At the same time, in Chinese sentence there are many auxiliary words and mood words which correspond to NULL in"
2006.iwslt-evaluation.13,2003.mtsummit-papers.36,0,0.0463937,"score: (5) S b e s t = a r g m a x { Ps } where Ps is the accumulative probability of the hypothesis S. 2.3. Combine the two engines The two translation systems have their own advantages. The TBMT system can generate more accurate results than the SMT system but the number of templates is limited while there are a lot of sentences that can not find proper templates The SMT system can translate more sentences than the TBMT system. So we combine the TBMT system with the phrase-based SMT system. There are some different methods to select the best results generated by different translation system [10] [11]. The selectors in [10] [11] work after all the translation system separately have one or more results. In our system, the candidate selector is very simple. Given a source sentence, the processes of translation are given as follows: First, the sentence is inputted into the TBMT system. If the TBMT can translate the sentence (there is suit template for the sentence), the translation is generated and the translation process is ended. Otherwise the sentence is sent to the SMT system and the SMT translate the sentence. The translation process is shown in Figure 1. 3. Improvements of the base"
2006.iwslt-evaluation.13,2003.mtsummit-papers.15,0,0.0178512,": (5) S b e s t = a r g m a x { Ps } where Ps is the accumulative probability of the hypothesis S. 2.3. Combine the two engines The two translation systems have their own advantages. The TBMT system can generate more accurate results than the SMT system but the number of templates is limited while there are a lot of sentences that can not find proper templates The SMT system can translate more sentences than the TBMT system. So we combine the TBMT system with the phrase-based SMT system. There are some different methods to select the best results generated by different translation system [10] [11]. The selectors in [10] [11] work after all the translation system separately have one or more results. In our system, the candidate selector is very simple. Given a source sentence, the processes of translation are given as follows: First, the sentence is inputted into the TBMT system. If the TBMT can translate the sentence (there is suit template for the sentence), the translation is generated and the translation process is ended. Otherwise the sentence is sent to the SMT system and the SMT translate the sentence. The translation process is shown in Figure 1. 3. Improvements of the baseline"
2006.iwslt-evaluation.13,2006.iwslt-plenaries.1,0,\N,Missing
2008.amta-papers.10,N04-1035,0,0.0214406,"given in Section 6. 2 Related work In order to overcome the weakness of existing phrase-based models, syntax-based models have been proposed and become a hot topic in SMT research. Generally, the syntax-based approaches can be classified into two categories according to the syntactic knowledge source: linguistically syntax-based approaches and formally syntaxbased approaches. The linguistically syntax-based approaches employ syntactic structures informed by syntactic theory. Their syntactic trees are either from the phrase-structured parsers or the dependency parsers (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005). All these linguistically syntactic approaches use syntactic structured information to enhance their reordering capability and use some non-contiguous phrases to obtain some generalization. However, these models are highly dependent on the syntactic parsers, and their performances are restricted by the accuracy of syntactic parsers. The formally syntax-based models are a simple and powerful mechanism to improve the phrase-based approaches, which use synchronous context-f"
2008.amta-papers.10,P06-1121,0,0.0491056,"Missing"
2008.amta-papers.10,koen-2004-pharaoh,0,0.0427116,"e in the same cell is discarded. b= < ci13 ◊ci42 +1 , e j13 ◊e j42 +1 > and i −1 Collocation features: the combination of lexical features. Then we use these features to train maximum entropy reordering model. j1 ≤ j3 − 1 < j4 + 1 ≤ j2 and i1 ≤ i3 − 1 < i4 + 1 ≤ i2 ) 10: Maximum Entropy-Based Reordering Model j b ∪ PPSet_4 Figure 2: Extraction Algorithm of Contiguous and Non-contiguous Phrase Pairs. choose the intersection of the two directions in order to increase the precise. After extraction of phrase pairs, the features of phrase translation are computed as phrasebased translation models (Koehn, 2004). In our training approach we only look the gap ◊ of noncontiguous phrases as a common word and each phrase pair has four probabilities: two translation probabilities in both directions and two lexical translation probabilities in both directions. We give a count of one to each extracted phrase pair occurrence, and then distribute its weight equally among the contiguous and non-contiguous phrases. Then treating distribution as our observed data to estimate the relative-frequency, we get the phrase translation probabilities in both directions. The lexical translation probabilities are calculate"
2008.amta-papers.10,J97-3002,0,0.352838,"Missing"
2008.amta-papers.10,P06-1066,0,0.477028,"0190, China yqhe@nlpr.ia.ac.cn cqzong@nlpr.ia.ac.cn In order to overcome the weakness of existing phrase-based models, two problems must be solved. The first problem is the type of phrases, which may not only involve the contiguous strings but also some non-contiguous strings. The second problem is the reordering of phrases. Bracket transduction grammar (BTG) (Wu, 1995) can reorder two contiguous translations of any two contiguous source strings in straight or inverted directions. BTG is used widely in SMT because of its good tradeoff between efficiency and expressiveness (Zens et al., 2004). Xiong et al. (2006) proposed an enhanced BTG with a maximum entropy-based reordering model (MEBTG). However, in BTG or MEBTG the phrases are only contiguous strings. Such a phrase has no generalization capability. Simard et al. (2005) introduced multi-word expressions into SMT that need not be contiguous in either or both the source and the target side. But in that approach the gap in non-contiguous phrases only stands for a single word. In this paper, we propose a generalized reordering model for phrase-based statistical machine translation (GREM), which not only properly deals with the non-contiguous phrases b"
2008.amta-papers.10,W02-1018,0,0.0483683,"Missing"
2008.amta-papers.10,P01-1067,0,0.0638162,"e concluding remarks are given in Section 6. 2 Related work In order to overcome the weakness of existing phrase-based models, syntax-based models have been proposed and become a hot topic in SMT research. Generally, the syntax-based approaches can be classified into two categories according to the syntactic knowledge source: linguistically syntax-based approaches and formally syntaxbased approaches. The linguistically syntax-based approaches employ syntactic structures informed by syntactic theory. Their syntactic trees are either from the phrase-structured parsers or the dependency parsers (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005). All these linguistically syntactic approaches use syntactic structured information to enhance their reordering capability and use some non-contiguous phrases to obtain some generalization. However, these models are highly dependent on the syntactic parsers, and their performances are restricted by the accuracy of syntactic parsers. The formally syntax-based models are a simple and powerful mechanism to improve the phrase-based approaches, which use"
2008.amta-papers.10,W06-1606,0,0.0238237,"ed work In order to overcome the weakness of existing phrase-based models, syntax-based models have been proposed and become a hot topic in SMT research. Generally, the syntax-based approaches can be classified into two categories according to the syntactic knowledge source: linguistically syntax-based approaches and formally syntaxbased approaches. The linguistically syntax-based approaches employ syntactic structures informed by syntactic theory. Their syntactic trees are either from the phrase-structured parsers or the dependency parsers (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005). All these linguistically syntactic approaches use syntactic structured information to enhance their reordering capability and use some non-contiguous phrases to obtain some generalization. However, these models are highly dependent on the syntactic parsers, and their performances are restricted by the accuracy of syntactic parsers. The formally syntax-based models are a simple and powerful mechanism to improve the phrase-based approaches, which use synchronous context-free grammar (SCFG) but ind"
2008.amta-papers.10,C04-1030,0,0.0519925,"Missing"
2008.amta-papers.10,P04-1083,0,0.0379058,"Missing"
2008.amta-papers.10,J04-4002,0,0.0377057,"or e is any contiguous string in sentence c or e. c1◊c2 is a non-contiguous Chinese string, e1◊e2 is a non-contiguous English string. A Chinese phrase c is either a contiguous Chinese string c or a non-contiguous Chinese string c1◊c2 , namely c ∈ {c1 , c1◊c2 } . An English phrase e is either a contiguous English string e or a non-contiguous English string e1◊e2 , namely e ∈ {e , e1◊e2 } .Then < c, e > is a phrase pair iff: (1) ∀ci ∈ c : (i, j ) ∈ A → e j ∈ e ; 6 ∆ rLM = LM ( x2r x1l ) − LM ( x2r ) − LM ( x1l ) The phrase-based models often obtain phrase pairs satisfying Definition 1 (Och and Ney, 2004; Koehn et al., 2003): Definition 1. Given a word-aligned sentence pair < c, e, A > , c or e is any contiguous string in sentence c or e. < c , e > is a phrase pair iff: (1) ∀ci ∈ c : (i, j ) ∈ A → e j ∈ e ; (2) (4) l 1 When we use n-gram language model, x and x1r respectively denote the leftmost and the rightmost n-1 words of contiguous string x1 , and the corresponding notation of other contiguous string is the same. LM (i) is the log probability of language model of a string i . For other rules, we use the following features: • Translation probabilities in both directions; • Lexical transla"
2008.amta-papers.10,P03-1041,0,0.0522237,"Missing"
2008.amta-papers.10,J93-2003,0,\N,Missing
2008.amta-papers.10,C90-3045,0,\N,Missing
2008.amta-papers.10,2007.iwslt-1.1,0,\N,Missing
2008.amta-papers.10,P06-1077,0,\N,Missing
2008.amta-papers.10,P05-1067,0,\N,Missing
2008.amta-papers.10,P03-2041,0,\N,Missing
2008.amta-papers.10,N03-1017,0,\N,Missing
2008.amta-papers.10,P05-1034,0,\N,Missing
2008.amta-papers.10,J07-2003,0,\N,Missing
2008.amta-papers.10,W90-0102,0,\N,Missing
2008.iwslt-evaluation.12,P07-2045,0,0.0131546,"d the evaluation results. 1. Introduction This paper describes the statistical machine translation system of CASIA (Institute of Automation, Chinese Academy of Sciences), which is used for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our s"
2008.iwslt-evaluation.12,W06-3119,0,0.027318,"nese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are preprocessed, they are passed into multiple translation systems respectively to produce an N-Best translation list, and then all the N-Best translations in the list a"
2008.iwslt-evaluation.12,J03-1002,0,0.00456062,"rs into Chinese words using the free software toolkit ICTCLAS3.01;  Transforming the SBC case into DBC case; For the English part of the training data and development data and test data, also two types of preprocessing are performed:  Tokenization of the English words: which separates the punctuations with the English words;  Transforming the uppercase into lowercase. 1 - 85 - http://www.nlp.org.cn Proceedings of IWSLT 2008, Hawaii - U.S.A. 2.2. Multiple translation systems 2.2.1. Three phrase-based SMT systems Phrase-based translation systems are usually modeled through a log-linear model [7]. In the log-linear model, given the sentence f (source language), the translating process is searching the translation e (target language) with the highest probability. The translation probability and the decision rule are given as Formula (1). M e* = arg max ∑ λm hm (e, f ) e (1) m =1 Where hm(e,f) is a feature function and λm Figure 2: Architecture of Bandore. is the weight of the feature. The entire λm are obtained by the minimum error rate training [8]. We use three phrase-based machine translation systems, Moses system (MOSES) [1], an in-home phrase-based system (PB) [2] and a sentence t"
2008.iwslt-evaluation.12,2005.eamt-1.36,0,0.242798,"translation systems 2.2.1. Three phrase-based SMT systems Phrase-based translation systems are usually modeled through a log-linear model [7]. In the log-linear model, given the sentence f (source language), the translating process is searching the translation e (target language) with the highest probability. The translation probability and the decision rule are given as Formula (1). M e* = arg max ∑ λm hm (e, f ) e (1) m =1 Where hm(e,f) is a feature function and λm Figure 2: Architecture of Bandore. is the weight of the feature. The entire λm are obtained by the minimum error rate training [8]. We use three phrase-based machine translation systems, Moses system (MOSES) [1], an in-home phrase-based system (PB) [2] and a sentence type-based reordering model (Bandore) [3]. The Moses decoder provided in the open source Moses package1 is run by the default parameters. We only train 3gram language model and extract phrase pairs no more than 10 words. Our in-home PB system’s word alignment is based on the training results of the GIZA++ 2 toolkit under the default parameters. We obtain word alignment by the method of grow-diag-final on the bi-directional word alignments of GIZA++. PB’s phr"
2008.iwslt-evaluation.12,I08-1066,0,0.0698379,"rted } , hi (O, A) is a feature, and λi is the weight of the feature. After reordering the Chinese sentences of training set and test set, we pass the reordered sentences into a phrase-based decoder such as Moses or PB decoder to get the final translation results. In our experiments Bandore uses Moses as its decoder. 2.2.2. Two formal syntax-based translation models Here we use two formal syntax-based translation models, a maximum entropy-based reordering model (MEBTG) [5] and a hierarchical phrase-based translation model (HPB) [4]. The system of MEBTG is realized in home according to [5] and [11]. In this model the prediction of relative orders of any two adjacent blocks is considered as a problem of classification. We extract reordering examples from the wordaligned training corpus and extract the following features from every two consecutive phrase pairs:  Lexical features: the last word of two source phrases or target phrases;  Collocation features: the combination of lexical features. With these features we train a MaxEnt classifier1. We extract phrase pairs using Och&apos;s algorithm [12]. The maximum length of source phrase is limited in 10 words. We use a CKY style decoder which l"
2008.iwslt-evaluation.12,J04-4002,0,0.032787,"based translation model (HPB) [4]. The system of MEBTG is realized in home according to [5] and [11]. In this model the prediction of relative orders of any two adjacent blocks is considered as a problem of classification. We extract reordering examples from the wordaligned training corpus and extract the following features from every two consecutive phrase pairs:  Lexical features: the last word of two source phrases or target phrases;  Collocation features: the combination of lexical features. With these features we train a MaxEnt classifier1. We extract phrase pairs using Och&apos;s algorithm [12]. The maximum length of source phrase is limited in 10 words. We use a CKY style decoder which limits the phrase table within 40 and the partial hypotheses is within 200. HPB translation engine is a re-implementation of David Chiang&apos;s hierarchical phrase translation model. Based on the union of the bidirectional alignment results of GIZA++, initial rules consistent with the alignment are extracted using Och&apos;s algorithm [12] and then rule subtraction is performed to obtain rules with no more than two non-terminals. Nullaligned words are allowed at the boundary of phrases. We set a limitation th"
2008.iwslt-evaluation.12,N04-1022,0,0.0689939,"et a limitation that initial rules are of no more than 10 words and other rules should have no more than 5 terminals and nonterminals. The decoder is CYK-style chart parser that maximizes the derivation probability. A 3-gram language model generated by SRILM is used in the cube-pruning process. The search space is pruned with a chart cell size limit 1 http://maxent.sourceforge.net/ Figure 3: System combination architecture. We collect the N-Best list translation hypotheses from each translation system in Section 2.2, and find a hypothesis as the alignment reference with the minimum Bayes risk [13]. We exploit word reordering alignment approaches to align all the hypotheses against the alignment reference and form a consensus alignment. Given N (N=3) translation hypotheses: please show me on this map . please on the map for me . show me on the map , please . when the first translation hypothesis is chosen as the alignment reference, the result of consensus alignment may look something like Figure 4, where “null” strings are used to accommodate insertions and deletions. 2 - 87 - http://www.cs.cmu.edu/~zollmann/samt Proceedings of IWSLT 2008, Hawaii - U.S.A. null null , please please plea"
2008.iwslt-evaluation.12,2007.iwslt-1.8,0,0.0239539,"Missing"
2008.iwslt-evaluation.12,P07-1002,0,0.0398822,"Missing"
2008.iwslt-evaluation.12,W06-3110,0,0.0625255,"Missing"
2008.iwslt-evaluation.12,C08-1137,1,0.735422,"ystem of CASIA (Institute of Automation, Chinese Academy of Sciences), which is used for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclus"
2008.iwslt-evaluation.12,J07-2003,0,0.651264,"e evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are"
2008.iwslt-evaluation.12,P06-1066,0,0.425706,"poken Language Translation (IWSLT) 2008. We participated in challenge task for Chinese-English and English-Chinese, BTEC task for Chinese-English. Our system combines the output results of multiple machine translation systems. These systems are listed as follows:  Three phrase-based statistical machine translation (SMT) models: Moses decoder (MOSES) [1], an inhome phrase-based decoder (PB) [2] and a sentence type-based reordering decoder (Bandore) [3];  Two formal syntax-based translation models: a hierarchical phrase-based model (HPB) [4] and a maximum entropy-based reordering model (MEBTG)[5];  A linguistically syntax-based translation model: a syntax-augmented machine translation (SAMT) decoder [6]. Then by using some global features we rescore the combination results to get our final translation outputs. This paper is structured as follows: Section 2 presents the overview of CASIA system. In Section 3, the experimental results of our system are reported and the details on analyses of the results are given. Section 4 gives the conclusions. 2. System Overview Figure 1 depicts our system architecture. After the test data are preprocessed, they are passed into multiple translation"
2008.iwslt-evaluation.12,2005.iwslt-1.16,0,\N,Missing
2008.iwslt-evaluation.12,2006.iwslt-evaluation.22,0,\N,Missing
2009.iwslt-evaluation.13,P07-2045,0,0.00656902,"Missing"
2009.iwslt-evaluation.13,W09-0424,0,0.0350416,"Missing"
2009.iwslt-evaluation.13,P06-1066,0,0.073844,"Missing"
2009.iwslt-evaluation.13,2008.amta-papers.10,1,0.900475,"Missing"
2009.iwslt-evaluation.13,C08-1137,1,0.888149,"Missing"
2009.iwslt-evaluation.13,P07-1040,0,0.133371,"Missing"
2009.iwslt-evaluation.13,2006.amta-papers.25,0,0.0477907,"Missing"
2009.iwslt-evaluation.13,P02-1040,0,0.085985,"Missing"
2009.iwslt-evaluation.13,J97-3002,0,0.020293,"Missing"
2009.iwslt-evaluation.13,N04-1022,0,0.192674,"Missing"
2009.iwslt-evaluation.13,P07-1003,0,0.0477973,"Missing"
2009.iwslt-evaluation.13,2008.iwslt-evaluation.18,0,\N,Missing
2009.iwslt-evaluation.13,2008.iwslt-evaluation.12,1,\N,Missing
2009.iwslt-evaluation.13,2006.iwslt-evaluation.4,0,\N,Missing
2009.iwslt-evaluation.13,P02-1038,0,\N,Missing
2009.iwslt-evaluation.13,2008.iwslt-evaluation.13,0,\N,Missing
2011.mtsummit-papers.29,J07-2003,0,0.477581,"Missing"
2011.mtsummit-papers.29,N09-1025,0,0.0737627,"Missing"
2011.mtsummit-papers.29,P10-1146,0,0.0331773,"to-tree rule. The tree-to-tree style SPMT algorithm used in our experiments can be described as follows: for each phrase pair, traverse the source and target parsing tree bottom up until it finds a node that subsumes the corresponding phrase respectively, then we can extract a rule whose roots are the nodes just found and the leaf nodes are the phrases. However, even with GHKM and SPMT rules, the rule coverage is still very low since tree-to-tree model requires that both source side and target side of its rule must be a subtree of the parsing tree. With this hard constraint (Liu et al., 2009; Chiang, 2010), the model would lose a large amount of bilingual phrases which are very useful to the translation process (DeNeefe et al., 2007). Eng Chn tree non-tree total Figure 1. An example of Chinese-English tree pair. 2.1 Limitations on Tree-to-tree Rule Extraction To extract all valid tree-to-tree rules, (Liu et al., 2009) extends the famous tree-to-string rule extraction algorithm GHKM (Galley et al., 2004) to their forest-based tree-to-tree model. However, only with GHKM rules, the rule coverage is very low1. As SPMT rules (Marcu et al., 2006) have proven to be a good complement to GHKM (DeNeefe e"
2011.mtsummit-papers.29,D07-1079,0,0.0557755,"air, traverse the source and target parsing tree bottom up until it finds a node that subsumes the corresponding phrase respectively, then we can extract a rule whose roots are the nodes just found and the leaf nodes are the phrases. However, even with GHKM and SPMT rules, the rule coverage is still very low since tree-to-tree model requires that both source side and target side of its rule must be a subtree of the parsing tree. With this hard constraint (Liu et al., 2009; Chiang, 2010), the model would lose a large amount of bilingual phrases which are very useful to the translation process (DeNeefe et al., 2007). Eng Chn tree non-tree total Figure 1. An example of Chinese-English tree pair. 2.1 Limitations on Tree-to-tree Rule Extraction To extract all valid tree-to-tree rules, (Liu et al., 2009) extends the famous tree-to-string rule extraction algorithm GHKM (Galley et al., 2004) to their forest-based tree-to-tree model. However, only with GHKM rules, the rule coverage is very low1. As SPMT rules (Marcu et al., 2006) have proven to be a good complement to GHKM (DeNeefe et al., 2007), we also extract full lexicalized SPMT rules to improve the rule coverage. 1 (Liu et al., 2009) investigate how many"
2011.mtsummit-papers.29,P05-1067,0,0.0942653,"ize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in our training corpus th"
2011.mtsummit-papers.29,P03-2041,0,0.244471,"ation to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in"
2011.mtsummit-papers.29,N04-1035,0,0.203798,"Missing"
2011.mtsummit-papers.29,P06-1121,0,0.104779,"oding. In this paper we propose two simple but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons w"
2011.mtsummit-papers.29,P07-1019,0,0.10325,"Missing"
2011.mtsummit-papers.29,W06-3601,0,0.356544,"ting to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure"
2011.mtsummit-papers.29,D10-1014,0,0.0114776,"4. Rule(c) is an example of source phrase after transformation. When translating the tree structure, match the rule’s category with the head node and match the rule’s words with the terminal nodes of the structure. In the figure, if we do exact match between categories, rule(c) cannot be used yet. Normally, if we do exact match, rule(c) in Fig.4 will not be employed due to the mismatch between categories of rule and tree structure. Hence, to maximize the capacities of the source phrases, we utilize fuzzy matching method which has been successfully employed in hierarchical phrase-based model (Huang et al., 2010) and string-to-tree model (Zhang et al., 2011b) to match categories. With fuzzy matching method, we represent each SAMT-style syntactic category with a real-valued G vector F ( c ) using latent syntactic distribution. Due to the space limitation, here we ignore the details as we just follow the work of (Huang et al., 2010; Zhang et al., 2011b). Then the degree of syntactic similarity between two categories can be simply computed by dot-product: JG JG F ( c ) ⋅ F ( c &apos;) = ¦ f i ( c ) fi ( c &apos;) (2) 1≤ i ≤ n which yields a similarity score ranging from 0 (totally syntactic different) to 1 (totall"
2011.mtsummit-papers.29,P07-2045,0,0.00738743,"number of unmatchable nodes is reduced from 2990 to 2422. This is the contribution of fuzzy matching method of source phrase rules. With tree binarization, many unmatchable nodes are eliminated, as we can see, from 2990 to 2487, among which only 49 nodes are created by binarization. When we combine the two approaches, the number of unmatchable nodes decreases further (1957 unmatchable nodes), indicating that both bilingual phrases and binarization can help to alleviate the exact matching constraint and enlarge the search space. 4.3 Tree-to-tree vs. State-of-the-art Systems We also ran Moses (Koehn et al., 2007) with its default settings using the same data and obtained BLEU score of 32.35 and 30.03 on MT04 and MT05 respectively. Our best results are 34.50 and 31.37 on the two test sets which are significant better than Moses. 5 Conclusion and Future Work To overcome the limitations in rule extraction and decoding procedure of tree-to-tree model, this paper proposed two simple but effective approaches to integrate bilingual phrases and binarize the bilingual parsing trees. The experiments have shown that the approaches yield dramatic improvements over conventional tree-to-tree systems. Furthermore, o"
2011.mtsummit-papers.29,W04-3250,0,0.461991,"Missing"
2011.mtsummit-papers.29,P06-1077,0,0.767189,"d category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction an"
2011.mtsummit-papers.29,W06-1606,0,0.101091,"we propose two simple but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree mode"
2011.mtsummit-papers.29,P03-1021,0,0.0344325,"Missing"
2011.mtsummit-papers.29,P02-1040,0,0.0821005,"Missing"
2011.mtsummit-papers.29,P06-1055,0,0.360707,"Missing"
2011.mtsummit-papers.29,P05-1034,0,0.177764,"Missing"
2011.mtsummit-papers.29,P08-1066,0,0.0400797,"e but effective approaches to overcome the constraints: utilizing fuzzy matching and category translating to integrate bilingual phrases and using head-out binarization to binarize the bilingual parsing trees. Our experiments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfact"
2011.mtsummit-papers.29,D07-1078,0,0.337553,"Missing"
2011.mtsummit-papers.29,J10-2004,0,0.0977337,"Missing"
2011.mtsummit-papers.29,N10-1016,0,0.111775,"Missing"
2011.mtsummit-papers.29,P11-1084,0,0.188253,"Missing"
2011.mtsummit-papers.29,D09-1108,0,0.0237871,"Missing"
2011.mtsummit-papers.29,D11-1019,1,0.661239,"Missing"
2011.mtsummit-papers.29,2007.mtsummit-papers.71,0,0.141741,"Missing"
2011.mtsummit-papers.29,P08-1064,0,0.626595,"ments show that the proposed approaches can significantly improve the performance of tree-to-tree system and outperform the state-of-the-art phrase-based system Moses. 1 Introduction In recent years, syntax-based translation models have shown promising progress in improving translation quality. These models include string-totree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009), tree-tostring models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al.,2008), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). With the ability to incorporate both source and target syntactic information, tree-to-tree models are believed to be much potential to achieve promising translation quality. However, the con261 ventional tree-to-tree based translation systems haven’t shown superiority in empirical evaluations. To explore the reasons why tree-to-tree model is so unsatisfactory, this paper makes a deep analysis of the limitations on its rule extraction and decoding procedure respectively. Towards rule extraction, we found that in our training corpus the bilingual phrases that tree-totree mod"
2011.mtsummit-papers.29,W06-3119,0,0.0851968,"ntax structure like treesequence based model. However, our analysis has shown that exact match would do harm to the translation quality. Thus instead of syntax structures, we decorate the source phrases with proper syntactic categories which have been proven to be necessary and effective for translation (Zhang et al., 2011b). When decoding with these source phrases, we ignore the internal structure of the subtree for translation and only match the rule’s category with root node of the subtree along with the matching between leaf nodes, just as shown in Fig.4. Here we utilize the SAMT grammar (Zollmann and Venugopal, 2006), with which each phrase can be associated with a corresponding syntactic category. For example, in Fig.1 the source span “ Ҫ Ӏ 䅼䆎 ℸџ” does not correspond to a subtree, 264 Figure 4. Rule(c) is an example of source phrase after transformation. When translating the tree structure, match the rule’s category with the head node and match the rule’s words with the terminal nodes of the structure. In the figure, if we do exact match between categories, rule(c) cannot be used yet. Normally, if we do exact match, rule(c) in Fig.4 will not be employed due to the mismatch between categories of rule and"
2011.mtsummit-papers.29,N06-1033,0,\N,Missing
2011.mtsummit-papers.29,P08-1023,0,\N,Missing
2011.mtsummit-papers.29,P09-1063,0,\N,Missing
2011.mtsummit-papers.29,W06-1628,0,\N,Missing
2012.iwslt-papers.9,N03-1017,0,0.00983087,"s, we can extend rules to various language-pairs without re-coding, which contributes a lot to the efficient development of an SMT system with good portability. We classify numbers and time expressions into seven types, which are Arabic number, cardinal numbers, ordinal numbers, date, time of day, day of week and figures. A greedy algorithm is developed to deal with rule conflicts. Experiments have shown that our approach can significantly improve the translation performance. 1. Introduction Recently, statistical machine translation (SMT) models, especially the phrase based translation models [1], have been widely used and have achieved great improvements. However, there are still some hard problems. One of them is how to translate OOV words. Among all OOV words, the numerical and time expressions (we generally call numbers hereafter) are typically and widely distributed in some corpora. According to our rough statistics in a corpus of travelling domain, there are about 15 percent sentences containing numbers in all 5000 sentences. Theoretically, numbers are innumerable and the forms of numbers vary greatly from universal Arabic numbers to language-dependent number words. For example,"
2012.iwslt-papers.9,P02-1040,0,0.103835,"orpus. In the table, the precision is the ratio of correctly captured numbers’ counts to all captured ones. The recall is the ratio of correctly captured numbers’ counts to the manually marked ones. In fact the performance largely relies on the rule-makers. The more numerical and time expressions they discover the better the performance will be. Table 8: Performance of automatic recognition by RTN Corpus Precision Recall F-score 0.94 BTEC 0.98 0.90 CT-CE 0.96 0.94 0.93 CT-EC 0.91 0.84 0.88 4.2.1. Results on development and testing set Table 9 shows how the translation quality measured by BLEU [9] on the original testing set changes score when we add the additional transferred translation table generated by the RTN 1 module into the phrase-based translation table. The C-E evaluation is based on the case-insensitive BLEU-4 score, and the E-C evaluation is based on the BLEU-4 score of words. Table 9: BLEU scores of development and testing set BTEC CT-CE CT-EC Dev. Test Baseline Ori + RTN Baseline Ori + RTN 41.25 41.37 37.67 37.79 33.64 33.71 32.56 32.99 34.54 34.84 33.38 33.71 Table 10: BLEU scores of with-number sentence set BTEC CT-CE CT-EC With-number Dev. With-number Test Baseline Or"
2012.iwslt-papers.9,2007.mtsummit-wpt.4,0,0.0371935,"onstruction of number names. The translation from a figure expression into a word expression is also done. [11] did part work of of [2], which only discussed about how to translate number names to Dutch. But they did not separate rules from source codes, which handicaps the scalability of rules. And the number representation they cared about is limited to cardinals with well formats, which is not enough for processing complicated and not so standardized text. [12] proposed an embedded-incode-rule based framework to do the number recognition and translation task on Chinese-English translation. [13] combined a rule based machine translation system with a statistic based post editor, which showed helpful for a more accurate performance than only using statistical machine translation. However, although the former works on RTN have achieved successes in different ways, as far as we see, works on language-independent and code-separate rule-based number recognition and translation are very rare. Our work may be a big step in the work of number recognition and translation. 6. Conclusions In this paper, we design a language-independent and rulebased method to translate numbers more precisely. T"
2020.aacl-main.1,W16-3415,0,0.0144565,"ed approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing repeat mistakes by memorizing revision actions. Hokamp and Liu (2017) propose grid beam search to incorporate lexical constraints like words and phrases provided by human translators and force the constraints to appear in hypothesis. Recently, some researchers resort to more flexible interactions, which only require mouse click or touch actions. For example, Marie and Max (2015); Domingo et al. (2016) propose interactive translation methods which ask user to select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polis"
2020.aacl-main.1,J09-1002,0,0.748515,"Missing"
2020.aacl-main.1,2012.amta-papers.22,0,0.0154104,"tic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masselot, 2010; Federico et al., 2012). Many notable works introduce different levels of human-machine interactions in post-editing. Barrachina et al. (2009) propose a prefix-based interactive method which enable users to correct the first translation error from left to right in each iteration. 6 Conclusion In this paper, we propose Touch Editing, a flexible and effective interaction approach which allows human translators to revise machine translation results via touch actions. The actions we introduce can be provided with gestures like tapping, panning, swiping or long pressing on touch screens to represent human editing intenti"
2020.aacl-main.1,D18-1048,0,0.0181626,"o select correct or incorrect segments of a translation with mouse only. Similar to our work, Grangier and Auli (2018) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate refined translations. Herbig et al. (2019, 2020) propose a multi-modal interface for post-editors which takes pen, touch, and speech modalities into consideration. The protocol that given an initial translation to generate a refined translation, is also used in polishing mechanism in machine translation (Xia et al., 2017; Geng et al., 2018) and automatic post-editing (APE) task (Lagarda et al., 2009; Pal et al., 2016). The idea of multi-source encoder is also widely used in the field of APE research (Chatterjee et al., 2018, 2019). In human-machine interaction scenarios, the human feedback is used as extra information in polishing process. Related Work Post-editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations (Simard et al., 2007). Comparing to purely manual translation, it achieves higher productivity while maintaining the human translation quality (Plitt and Masse"
2020.aacl-main.1,K16-1020,0,0.0429466,"Missing"
2020.aacl-main.1,N18-1025,0,0.191393,"ssing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We present a dual-encoder model to handle the actions and generate refined translations. To mimic the user feedback, we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions"
2020.aacl-main.1,2014.iwslt-evaluation.1,0,0.0223119,"Missing"
2020.aacl-main.1,D14-1130,0,0.186457,"We observe that the users with Touch Editing tends to correct an error for multiple times when the system cannot predict a word they want, while the users with keyboard input tends to modify more content of initial translation and spend more time on choosing words. We then conduct an unstructured interview on the usability of our method. The result of the interview shows that Touch Editing is convenient and intuitive but lack of ability of generating final accurate translation. It can be treated as a light-weight proofreading method, and suitable for Pre-Post-Editing (Marie and Max, 2015). 5 Green et al. (2014) implement a prefix-based interactive translation system and Huang et al. (2015) adopt the prefix constrained translation candidates into a novel input method for translators. Peris et al. (2017) further extend this idea to neural machine translation. The prefix-based protocol is inflexible since users have to follow the left-to-right order. To overcome the weakness of prefix-based approach, Gonz´alez-Rubio et al. (2016); Cheng et al. (2016) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis, while Weng et al. (2019) also preventing r"
2020.aacl-main.1,W19-5402,0,0.0371464,"Missing"
2020.aacl-main.1,2020.acl-main.155,0,0.324106,"Missing"
2020.aacl-main.1,P16-2046,0,0.0266837,"Missing"
2020.aacl-main.1,P17-1141,0,0.285801,"to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020. 2020 Association for C"
2020.aacl-main.1,P18-2052,0,0.0167696,"our method significantly improves original translation of Transformer (up to 25.31 BLEU) and outperforms existing interactive translation methods (up to 16.64 BLEU). We also conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Confe"
2020.aacl-main.1,D10-1092,0,0.0679881,"Missing"
2020.aacl-main.1,2016.amta-researchers.9,0,0.152262,"on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–11 c December 4 - 7, 2020."
2020.aacl-main.1,N09-2055,0,0.0869955,"Missing"
2020.aacl-main.1,2006.amta-papers.25,0,0.553195,"d by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translations: for instance, INSERTION indicates a word is missing at a particular position, and our method is expected to insert the correct word. To this end, we present a neural network model by augmenting Transformer (Vaswani et al., 2017) with an extra encoder for a hypothesis and its actions. Since it is impractical to manually annotate large-scale action dataset to train the model, we thereby adopt the algorithm of TER (Snover et al., 2006) to automatically extract actions from a draft hypothesis and its reference. To evaluate our method, we conduct simulated experiments on translation datasets the same as in other works (Denkowski et al., 2014; Grangier and Auli, 2018), The results demonstrate that our method can address the well-known challenging issues in machine translation including overWe propose a touch-based editing method for translation, which is more flexible than traditional keyboard-mouse-based translation postediting. This approach relies on touch actions that users perform to indicate translation errors. We presen"
2020.aacl-main.1,D15-1166,0,0.0602627,"9.47 56.47 0.48 0.39 0.37 0.24 32.53 41.20 38.96 57.84 0.55 0.43 0.42 0.28 21.89 29.78 29.17 45.67 0.61 0.51 0.51 0.33 Model WMT’17 EN-ZH ZH-EN BLEU TER BLEU TER Table 1: Results of different systems measured in BLEU and TER. † denotes the results from Quick Edit. QuickEdit‡ is our reimplementation based on Transformer. Touch baseline is the result modified from initial hypothesis by deleting and reordering words. Touch Editing is our model trained with all actions described in Section 2.1. tains 4698 sentence pairs. For WMT’14 EnglishGerman dataset, we use the same data and preprocessing as (Luong et al., 2015). The dataset consists of 4.5M sentence pairs for training1 . We take newstest2013 for validation and newstest2014 for testing. For Chinese to English dataset, we use CWMT portion which is a subset of WMT’17 training data containing 9M sentence pairs. We validate on newsdev2017 and test on newstest2017. As for vocabulary, the English and German datasets are encoded using byte-pair encoding (Sennrich et al., 2015) with a shared vocabulary of 8k tokens for IWSLT’14 and 32k tokens for WMT’14. For Chinese to English dataset, the English vocabulary is set to 30k subwords, while the Chinese data is"
2020.aacl-main.1,D18-1458,0,0.0281639,"Missing"
2020.aacl-main.1,D15-1120,0,0.124853,"or TED talks (IWSLT). However in real world, data may be from any other domains. For model inconsistency, we use Transformer to build our training data while the translation model used 4.5 Discussion on Real Scenarios So far, the experiments we conducted are based on simulated human feedbacks, in which the actions are extracted from initial machine translation results and their corresponding references to simulate human editing actions. Thus in our simulated setting, the references are used in inference phase to simulate human behavior, as in other interaction methods (Denkowski et al., 2014; Marie and Max, 2015; Grangier and Auli, 2018). These experiments show that our method can significantly 7 improve the initial translation with similated actions. However, whether the actions are convenient to perform is a key point in real applications. To investigate the usability and applicable scenarios of our method, we implement a real mobile application on iPhone, in which the actions can be performed on multi-touch screens. For a given source sentence, the application provides an initial machine translation. The text area of translation can response to several gestures 4 : Tap indicated a missing word sho"
2020.aacl-main.1,W18-6304,0,0.0371598,"Missing"
2020.aacl-main.1,W18-6471,0,0.0192956,"- (3) Where P E∗ denote the action positional embedding matrixes in Figure 3. The learned action positional embedding is used in hypothesis encoder to replace the fixed sinusoids positional encoding in Transformer encoder. Next, the encoder adds the word embedding w and the action positional embedding p to obtain input embedding e = {w1 + p1 , · · · , wl + pl }. The following part of hypothesis encoder lies the same as Transformer encoder. P (yn |y<n , x, m(y0 ), a; θ). (2) n=1 As shown in Figure 2, the neural network model we developed is a dual encoder model based on Transformer similar to Tebbifakhr et al. (2018). Specifically, besides encoding the source sentence x with source encoder (the left part of Figure 2), our model additionally encodes A(y0 ) with an extra hypothesis encoder (the right part of Figure 2) and integrates the encoded representations into decoding network using dual multi-head attention. Decoding The output of hypothesis encoder, together with the output of source encoder, are fed into the decoder. To combine both of the encoders’ outputs, we apply dual multi-head attention in each layer of decoder: the attention sub-layer attends to both encoders’ outputs by performing multi-head"
2020.aacl-main.1,P16-1008,0,0.0205888,"ng, jjzhang, cqzong}@nlpr.ia.ac.cn, {redmondliu, donkeyhuang}@tencent.com Abstract forts, QuickEdit, in which users are asked to simply mark incorrect words in a translation hypothesis for one time in the hope that the system will change them. QuickEdit delivers appealing improvements on draft hypotheses while maintaining the flexibility of human-machine interaction. Unfortunately, only marking incorrect words is far from adequate: for example, it does not indicate the missing information beyond the original hypothesis, which is a typical issue called under-translation in machine translation (Tu et al., 2016). In this paper, we propose a novel one-time interaction method called Touch Editing, which is flexible for users and more adequate for a system to generate better translations. Inspired by human editing process, the proposed method relies on a series of touch-based actions including SUBSTITU TION , DELETION , INSERTION and REORDERING . These actions do not include lexical information and thus can be flexibly provided by users through various of gestures on touch screen devices. By using these actions, our method is able to capture the editing intention from users to generate better translatio"
2020.aacl-main.1,P16-1007,0,0.142257,"so conduct experiments on post-editing dataset to further prove the robustness and effectiveness of our method. 1 Introduction Neural machine translation (NMT) has made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017), but automatic machine translation is still far from perfect and cannot meet the strict requirements of users in real applications (Petrushkov et al., 2018). Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results (Wuebker et al., 2016; Knowles and Koehn, 2016; Hokamp and Liu, 2017). As an instance of such approaches, post-editing directly requires translators to modify outputs from machine translation (Simard et al., 2007). However, traditional post-editing requires intensive keyboard interaction, which is inconvenient on mobile devices. Grangier and Auli (2018) suggest a one-time interaction approach with lightweight editing ef1 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 1–1"
2020.acl-main.121,D16-1250,0,0.0196286,"the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2"
2020.acl-main.121,P18-1073,0,0.0188278,"on on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2 ) is the average of source-to-target and target-to-source probabilities calculated through maximum lik"
2020.acl-main.121,N16-1012,0,0.0292305,"ach the cross-lingual system. Zhu et al. (2019) propose to acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to bot"
2020.acl-main.121,P15-1166,0,0.0319216,"3,000 test pairs. Both the English-to-Chinese and Chinese-toEnglish test sets are manually corrected. 4.2 • TNCLS: It denotes the Transformer-based neural cross-lingual summarization system. The above methods only employ the cross-lingual summarization dataset, and we also compare our method with the following two methods (Zhu et al., 2019) that use extra datasets in other tasks. • CLSMS: It refers to the multi-task method, which simultaneously trains cross-lingual summarization and monolingual summarization. • CLSMT: It is the multi-task method which adopts the alternating training strategy (Dong et al., 2015) to train cross-lingual summarization and machine translation jointly. Experimental Settings We follow the setting of the vocabularies described in Zhu et al. (2019). In En2ZhSum, we surround each target sentence with tags “<t>” and “</t>”. If there is no special explanation, the limit on the number of translation candidate m in our models is set to 10. All the parameters are initialized via Xavier initialization method (Glorot and Bengio, 2010). We train our models using configuration transformer base (Vaswani et al., 2017), which contains a 6-layer encoder and a 6-layer decoder with 512-dime"
2020.acl-main.121,P19-1305,0,0.483224,"tion can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate machine translation and monolingual summarization into the training of cross-lingual summarization using multi-task learning to improve the summary quality with a quite promising performance. However, we find that there exist the following problems: (1) The multi-task methods adopt extra large-scale parallel data from other related tasks, suc"
2020.acl-main.121,N13-1073,0,0.0196664,"Since αth is a multi-head attention, we take the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next,"
2020.acl-main.121,P16-1154,0,0.0283382,"eural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates the operation of attending, translating, and summarizing. 6 Conclusion and Future Work In this paper, we present a novel method consistent with the translation pattern in the pr"
2020.acl-main.121,C18-1121,1,0.846956,"uage model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates the operation of attending, translating, and summarizing. 6 Conclusion and Future Work In this paper, we present a novel method consistent with the translation pattern in the process of obtaining"
2020.acl-main.121,W04-1013,0,0.0333936,"Missing"
2020.acl-main.121,D16-1031,0,0.0156619,"summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lin"
2020.acl-main.121,K16-1028,0,0.0293629,"translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network"
2020.acl-main.121,orasan-chiorean-2008-evaluation,0,0.266832,"me words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets."
2020.acl-main.121,N19-1204,0,0.452155,"le also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate machine translation and monolingual summarization into the training of cross-l"
2020.acl-main.121,D15-1044,0,0.042495,"the target-side counterparts. Recently, end-to-end methods have been applied to cross-lingual summarization. Due to the lack of supervised training data, Ayana et al. (2018) and Duan et al. (2019) focus on zero-shot training methods that use machine translation or monolingual summarization or both to teach the cross-lingual system. Zhu et al. (2019) propose to acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a po"
2020.acl-main.121,P17-1099,0,0.236716,"s from the translation candidates of the source text. The final distribution is obtained by the weighted sum (weighed by ptrans ) of the neural distribution PN and the translation distribution PT . Best viewed in color. method. It first attends to some source words, then obtains the translation candidates of them, and finally generates words from the translation candidates or the neural distribution. Our proposed method is a hybrid between Transformer and an additional translation layer, which is depicted in Figure 2 and described as follows. Attend. Inspired by the pointer-generator network (See et al., 2017), we employ the encoderdecoder attention distribution αth (the last layer) to help focus on some salient words in the source text. Since αth is a multi-head attention, we take the mean value over the heads as follow: 1X h αt = αt (4) h h Translate. With the attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bili"
2020.acl-main.121,P16-1162,0,0.060767,"t “subword-subword” and “word-character” segmentation granularities in Zh2En and En2Zh models, respectively. Unit pmacro trans pmicro trans rmacro rmicro Zh2En Zh2En sw-sw w-w 21.41 21.17 20.71 20.46 21.86 21.90 21.00 21.05 En2Zh w-c 14.91 14.84 14.27 14.05 Model Unit RG-1 RG-2 RG-L MVS Task TNCLS w-w sw-sw 37.70 38.85 21.15 21.93 34.05 35.05 19.43 19.07 ATS-A w-w sw-sw 39.65 40.68 23.79 24.12 36.05 36.97 22.06 22.15 Table 5: Results of models on Zh2EnSum with different segmentation granularities. Unit represents the granularity combination of text units. w and sw denote “word” and “subword” (Sennrich et al., 2016), respectively. The improvement of all ATS models over TNCLS is statistically significant (p < 0.01). more aggressive value 1. The results are presented in Table 4. In Zh2En experiment, the ATS-A (m=5) performs best while ATS-A (m=1) performs comparably with ATS-A (m=10). In En2Zh experiment, the ATS-A (m=5) performs comparably with ATSA (m=10) while the performance drops a bit when m=1. The above results illustrate that (1) A slightly larger m enables the model to learn when to search for translation candidates from the source words and which ones to choose, leading to improve the quality of"
2020.acl-main.121,D16-1112,0,0.0204318,"acquire largescale datasets via a round-trip translation strategy. They incorporate monolingual summarization or machine translation into cross-lingual summarization training using multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization"
2020.acl-main.121,P11-1155,0,0.271514,"English. Wan et al. (2010) apply the summarizethen-translate scheme to English-to-Chinese crosslingual summarization, which extracts English sentences considering both the informativeness and translation quality of sentences and automatically translates the English summary into Chinese. They also argue that summarize-then-translate is better, since it can alleviate both the computational expense of translating sentences and sentence extraction errors caused by incorrect translations. There have been some researches focusing on improving cross-lingual summarization with bilingual information. Wan (2011) translates the English document into Chinese and extracts sentences based on the original English sentences and Chinese translation. Yao et al. (2015) propose a compressive method which calculates the sentence scores based on the aligned bilingual phrases obtained by machine translation service and performs compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) introduce an abstractive method that constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures and the target-side counterparts. Recentl"
2020.acl-main.121,P10-1094,0,0.8245,"translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarization methods are pipeline-based. These methods either adopt summarize-then-translate (Orasan and Chiorean, 2008; Wan et al., 2010) or employ translate-thensummarize (Leuski et al., 2003; Ouyang et al., 2019). The pipeline-based approach is intuitive and straightforward, but it suffers from error propagation. Due to the difficulty of acquiring cross-lingual summarization dataset, some previous researches focus on zero-shot methods (Ayana et al., 2018; Duan et al., 2019), i.e., using machine translation or monolingual summarization or both to teach the cross-lingual system. Recently, Zhu et al. (2019) propose to use roundtrip translation strategy to obtain large-scale crosslingual summarization datasets. They incorporate m"
2020.acl-main.121,D15-1012,0,0.278288,"Missing"
2020.acl-main.121,P17-1179,0,0.0313971,"attention distribution on the source words, we also need to know what should each source word be translated into. To achieve that, we obtain a probabilistic bilingual lexicon P L (w1 ⇒ w2 ) from existing machine translation corpora and then acquire the translation probability PT based on P L (w1 ⇒ w2 ). Acquisition of the probabilistic bilingual lexicon. There are many different ways to get the probabilistic bilingual lexicon, such as learning from bilingual corpora (Dyer et al., 2013; Chandar A P et al., 2014; Artetxe et al., 2016) and learning from monolingual corpora (Conneau et al., 2018; Zhang et al., 2017; Artetxe et al., 2018). To facilitate access to the high-quality probabilistic bilingual lexicon, we apply the method described in Dyer et al. (2013). Specifically, we first extract word alignments L using the fast-align tool (Dyer et al., 2013) on the bilingual parallel corpus2 for machine translation in both source-to-target and target-tosource directions. To improve the quality of the word alignments, we only keep the alignments existing in both directions. Next, the lexicon translation probability P L (w1 ⇒ w2 ) is the average of source-to-target and target-to-source probabilities calcula"
2020.acl-main.121,D19-1053,0,0.0357714,"Missing"
2020.acl-main.121,P17-1101,0,0.0131077,"ng multi-task learning. Neural Abstractive Summarization. Rush et al. (2015) present the first neural abstractive summarization model, an attentive convolutional encoder and a neural network language model decoder, which learns to generate news headlines from the lead sentences of news articles. Their approach has been further improved with recurrent decoders (Chopra et al., 2016), abstractive meaning representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), hybrid strategy (Zhu et al., 2017), selective mechanism (Zhou et al., 2017), and entailment knowledge. See et al. (2017) propose a pointer-generator network, which allows copying words from the source text with the copying mechanism (Gu et al., 2016). Li et al. (2018) incorporate entailment knowledge into summarization model to improve the correctness of the generated summaries. Li et al. (2020) apply guidance signals of keywords to both the encoder and decoder in the abstractive summarization model. Inspired by the pointer-generator network and the translation pattern in obtaining cross-lingual summaries, we introduce a novel model in this paper, which integrates th"
2020.acl-main.121,D19-1302,1,0.889956,"or from the translation candidates of source words. Experimental results on Chinese-toEnglish and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art. 1 10 Input (Chinese) 10 Output (English) —Foshan —young couple —train tickets —10 yuan —detained —migrant workers Translation Table Foshan young couple was detained for charging 10 yuan for buying train tickets online for migrant workers Figure 1: An example of the translation pattern in a sample extracted from Zh2EnSum (Zhu et al., 2019) which is a Chinese-to-English cross-lingual summarization dataset. It shows that some words in the summary are translated from the source words (in the same color). The translation table also gives the corresponding relation to these words. Best viewed in color. Introduction Cross-lingual summarization is to produce a summary in a target language (e.g., English) from a document in a different source language (e.g., Chinese). Cross-lingual summarization can help people efficiently understand the gist of an article written in an unfamiliar foreign language. Traditional cross-lingual summarizati"
2020.autosimtrans-1.4,J82-2005,0,0.674208,"Missing"
2020.autosimtrans-1.4,W17-4123,0,0.30461,"slightly lower than 24 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 24–29 c July 10, 2020. 2020 Association for Computational Linguistics Softmax mechanism (Vaswani et al., 2017). Softmax N N Feed-Forward Feed-Forward Enc-NAD Cross-Attention Enc-AD Cross-Attention Feed-Forward Position Attention NAD-AD Cross-Attention Self-Attention Unmask SelfAttention Mask SelfAttention Source Embedding Copied Source Embedding Target Embedding 2.2 We initialize the non-autoregressive decoder inputs using copied source inputs from the encoder side by the fertility mechanism (Gu et al., 2017). For each layer in non-autoregressive decoder, the lowest sublayer is the unmasked multi-head self-attention network, and it also uses residual connections around each of the sublayers, followed by layer normalization. N z1l = LN(z l−1 + MHAtt(z l−1 , z l−1 , z l−1 )) (2) Figure 2: The extended Transformer translation model that exploits global information produced by NAT. We omit the residual connection and layer normalization in each sub-layer for simplicity. The second sub-layer is a positional attention. We follow (Gu et al., 2017) and use the positional encoding p as both query and key"
2020.autosimtrans-1.4,D16-1139,0,0.0595642,"Missing"
2020.autosimtrans-1.4,D18-1149,0,0.198734,"en the input and output languages: sl3 = LN(sl2 + MHAtt(sl2 , hN , hN )) e hl = LN(hl−1 + MHAtt(hl−1 , hl−1 , hl−1 )) (1) hl = LN(e hl + FFN(e hl )) sl = LN(sl3 + FFN(sl3 )) 2.4 where the superscript l indicates layer depth, hl denotes the source hidden state of l-th layer, LN is layer normalization, FFN means feed-forward networks, and MHAtt denotes the multi-head attention (7) Training and Inference Given a set of training examples {x(z) , y (z) }Z z=1 , the training algorithm aims to find the model parameters that maximize the likelihood of the training 25 # System 1 2 3 (Gu et al., 2017) (Lee et al., 2018) (Wang et al., 2019) 4 5 6 7 (Wu et al., 2016) (Gehring et al., 2017) (Vaswani et al., 2017) (Xia et al., 2017) 8 9 10 this work Architecture En⇒De Existing NAT Systems NAT 17.35 NAT-IR (adaptive) 18.91 NAT-AR 20.61 Existing AT Systems Google-NMT 24.60 ConvS2S 26.36 Transformer 27.30 Deliberate Network 27.56 Our NMT Systems Transformer 27.06 NAT 21.25 Our Model 27.65↑ En⇒Ro De⇒En 26.22 - 23.89 33.18 33.95 32.28 26.60 33.17⇑ 32.87 27.06 34.01⇑ Table 1: Comparing with existing NMT systems on WMT14 En⇒De, WMT16 En⇒Ro, and IWSLT14 De⇒En test sets. “↑/⇑” indicates statistically significant (p<0.05/"
2020.autosimtrans-1.4,D18-1336,0,0.0285778,"Missing"
2020.autosimtrans-1.4,P05-1066,0,0.366946,"Missing"
2020.autosimtrans-1.4,P02-1040,0,0.108208,"l as Vaswani et al. (2017), whose encoder and decoder both have 6 layers, 8 attention-heads, and 512 hidden sizes. We follow Gu et al. (2017) to use the same small Transformer setting for IWSLT14 because of its smaller dataset. For evaluation, we use argmax decoding for NAD, and beam search with a beam size of k=4 and length penalty α=0.6 for AD. We also re-implement and compare with deliberate network (Xia et al., 2017) based on strong Transformer, which adopts the two-pass decoding method and uses the autoregressive decoding manner for the first decoder. Experiments We use 4-gram NIST BLEU (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance. 3.1 Model Settings Datasets We conduct experiments on three widely used public machine translation corpora: WMT14 English-German2 (En⇒De), WMT16 EnglishRomanian3 (En⇒Ro), and IWSLT14 GermanEnglish4 (De⇒En), whose training sets consist of 4.5M, 600K, 153K sentence pairs, respectively. We employ 37K, 40K, and 10K shared BPE (Sennrich et al., 2016) tokens for En⇒De, En⇒Ro, and De⇒en respectively. For En⇒De, 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt16/translatio"
2020.autosimtrans-1.4,P16-1162,0,0.0917784,"Missing"
2020.autosimtrans-1.4,D18-1048,0,0.0481522,"Missing"
2020.coling-main.284,P19-1140,0,0.0719163,"Missing"
2020.coling-main.284,J13-2001,1,0.798521,"anually track emerging concepts and dynamically update the knowledge bases, making the whole process quite expensive. Fortunately, KGs of different languages are often complementary, which means that many components can be shared. To integrate this complementary knowledge, researches begin to pay attention to cross-lingual entity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally,"
2020.coling-main.284,C16-1276,0,0.0269876,"ge bases, making the whole process quite expensive. Fortunately, KGs of different languages are often complementary, which means that many components can be shared. To integrate this complementary knowledge, researches begin to pay attention to cross-lingual entity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally, a mapping function is used to align the entities from the source k"
2020.coling-main.284,D19-1274,0,0.405788,"oss-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally, a mapping function is used to align the entities from the source knowledge graph to the target one. However, due to the incompleteness of knowledge graphs and the diversity of knowledge, the structure of KGs in different languages is usually quite different. In the entity alignment task, this difference is mainly reflected in two aspects: t"
2020.coling-main.284,P15-1016,0,0.0295532,"eight of the different update layers. 4.1 Relation-aware Graph Attention Module This module is designed to mitigate the difference of non-isomorphism among counterpart entities by aggregating multi-hierarchy neighborhood information with different weights. Compared to GAT ignoring relation information, in R-GAT, we fuse relation-level information, which is vital to representation learning of entity to graph attention module. Meanwhile, to speed up the flow of information, we add inverse triples to KGs, which have been proved to be effective in knowledge graphs completion (Bordes et al., 2013; Neelakantan et al., 2015). The module R-GAT takes as input an tail entity et with its neighborhood set Net = {(e1 , r1 ), (e2 , r2 ), ..., (en , rn )}, where et ∈ {E1 , E2 } . In NAEA, the author only considers a fixed number of entities, which can lead to information loss. Here, just like the original GAT, we consider all neighbor entities. In R-GAT, we first calculate attention coefficients between entity et and neighborhood information. We use two weight matrices W1 and W2 to transform the entity and relation, respectively. Here we merge the translational assumption from TransE into attention coefficients. For each"
2020.coling-main.284,C18-1128,0,0.0174288,"y update the knowledge bases, making the whole process quite expensive. Fortunately, KGs of different languages are often complementary, which means that many components can be shared. To integrate this complementary knowledge, researches begin to pay attention to cross-lingual entity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally, a mapping function is used to align the entiti"
2020.coling-main.284,D18-1032,0,0.113197,"ntity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally, a mapping function is used to align the entities from the source knowledge graph to the target one. However, due to the incompleteness of knowledge graphs and the diversity of knowledge, the structure of KGs in different languages is usually quite different. In the entity alignment task, this difference is mainly reflected"
2020.coling-main.284,D19-1185,1,0.825069,"eighborhood information of counterpart entities and distinguish noncounterpart entities with similar structures. Finally, we treat cross-lingual entity alignment as a process of linking prediction. Experimental results on three real-world cross-lingual entity alignment datasets have shown the effectiveness of DAEA. 1 Introduction In recent years, many large-scale knowledge graphs (Bordes et al., 2013; Mahdisoltani et al., 2015; Auer et al., 2007) are built to represent and organize the explosive information over the Internet. They have been widely used in many fields, such as dialogue system (Wang et al., 2019), machine translation (Zhao et al., 2020) and medicine (Yan et al., 2020). However, the KGs are usually incomplete, so it is necessary to manually track emerging concepts and dynamically update the knowledge bases, making the whole process quite expensive. Fortunately, KGs of different languages are often complementary, which means that many components can be shared. To integrate this complementary knowledge, researches begin to pay attention to cross-lingual entity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various"
2020.coling-main.284,D19-1023,0,0.104156,"t finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Traditional approaches rely on machine translation or feature engineering (Chen et al., 2013; Mahdisoltani et al., 2015; Otani et al., 2018; Feng et al., 2016). The effectiveness of these methods depends largely on the quality of the translation and the nature of the definition. Recently, many embedding approaches based on graph neural network (GNN) are proposed for cross-lingual entity alignment (Wang et al., 2018; Li et al., 2019; Sun et al., 2020; Wu et al., 2019a). These methods first represent the entities and relations in low dimensional spaces and then utilize the powerful encoding ability of graph neural network to learn vector representations for entity or relation. Finally, a mapping function is used to align the entities from the source knowledge graph to the target one. However, due to the incompleteness of knowledge graphs and the diversity of knowledge, the structure of KGs in different languages is usually quite different. In the entity alignment task, this difference is mainly reflected in two aspects: the non-isomorphism among the neighb"
2020.coling-main.284,P19-1304,0,0.092401,"Missing"
2020.coling-main.284,2020.emnlp-main.116,1,0.693556,"part entities with similar structures. Finally, we treat cross-lingual entity alignment as a process of linking prediction. Experimental results on three real-world cross-lingual entity alignment datasets have shown the effectiveness of DAEA. 1 Introduction In recent years, many large-scale knowledge graphs (Bordes et al., 2013; Mahdisoltani et al., 2015; Auer et al., 2007) are built to represent and organize the explosive information over the Internet. They have been widely used in many fields, such as dialogue system (Wang et al., 2019), machine translation (Zhao et al., 2020) and medicine (Yan et al., 2020). However, the KGs are usually incomplete, so it is necessary to manually track emerging concepts and dynamically update the knowledge bases, making the whole process quite expensive. Fortunately, KGs of different languages are often complementary, which means that many components can be shared. To integrate this complementary knowledge, researches begin to pay attention to cross-lingual entity alignment. Cross-lingual entity alignment aims at finding entities with the same semantics in KGs of different languages. Various methods have been explored for cross-lingual entity alignment. Tradition"
2020.coling-main.318,D15-1076,0,0.0286831,"subsection but based on the real examples of the Ti . The outer-phase method is slower than the inner-phase one, but higher compression quality can be expected. 4 Experimental Setup 4.1 Tasks and Datasets Following the setting of Sun et al. (2020), we select five different tasks from decaNLP. We use Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)) for question answering task, Stanford Sentiment Treebank (SST, Radford et al. (2017)) for sentiment analysis, WikiSQL (Zhong et al. (2017)) for Semantic Parsing, English Wizard of Oz (WOZ) for task-oriented dialogue, and QA-SRL (He et al., 2015) for semantic role labeling. As shown in Table 1, the samples of these tasks are framed into the scheme of SQuAD by decaNLP. The setting of continually learning tasks of different types is challenging. To conduct a fair evaluation, we also compare models on learning tasks of the same type but from different domains in a sequence. We follow de Masson d’Autume et al. (2019)’s setting to use Zhang et al. (2015)’s collection of five text classification tasks, as briefed in Table 2. 4.2 Baselines We include the following baselines in the evaluation. All the baselines are based on GPT2 with 12 hidde"
2020.coling-main.318,D16-1264,0,0.0286672,"task Ti . In this compression phase, the distillation is not only for transferring knowledge across tasks, but also for producing a compressed copy that mimics the behavior of the initial model. So we use the distillation methods proposed in the previous subsection but based on the real examples of the Ti . The outer-phase method is slower than the inner-phase one, but higher compression quality can be expected. 4 Experimental Setup 4.1 Tasks and Datasets Following the setting of Sun et al. (2020), we select five different tasks from decaNLP. We use Stanford Question Answering Dataset (SQuAD, Rajpurkar et al. (2016)) for question answering task, Stanford Sentiment Treebank (SST, Radford et al. (2017)) for sentiment analysis, WikiSQL (Zhong et al. (2017)) for Semantic Parsing, English Wizard of Oz (WOZ) for task-oriented dialogue, and QA-SRL (He et al., 2015) for semantic role labeling. As shown in Table 1, the samples of these tasks are framed into the scheme of SQuAD by decaNLP. The setting of continually learning tasks of different types is challenging. To conduct a fair evaluation, we also compare models on learning tasks of the same type but from different domains in a sequence. We follow de Masson d"
2020.coling-main.318,D18-1173,1,0.818923,"i et al., 2019). However, the currently dominant paradigm of machine language learning is still training a model on a static dataset to achieve satisfactory performance on that particular task (Wang et al., 2018; Ostapenko et al., 2019). Most of these methods, especially the deep neural network-based ones, do not fare well in the continual learning scenarios. In continual learning, a model is required to fit on a stream of tasks where data distribution may not be uniform. For example, a network fitted on a first task tends to forget how to perform on it after trained on a sequential new task (Sun et al., 2018; Shin et al., 2017). This problem, namely catastrophic forgetting, poses a severe challenge in building a general language intelligent system with lifelong learning capacity. Efforts have been made in recent years to overcome the catastrophic forgetting of deep neural networks. There are mainly two stretches of methods, differentiated by the way of isolating and reusing the accumulated knowledge. One stretch of method works by reproducing the data distribution of witnessed tasks. Some members of this family select and store informative samples in an explicit memory module. The memorized sampl"
2020.coling-main.318,D18-1011,1,0.834186,"d effective in results, promoting the empirical application of continual language learning. We also hope that DnR could contribute to building human-level language intelligence that is no longer bothered by catastrophic forgetting. 2 Related Work Tackling new tasks without necessarily losing the knowledge learned in previous tasks is one fundamental requirement for a human-like intelligent system (Parisi et al., 2019). However, the currently dominant paradigm of machine language learning is still training a model on a static dataset to achieve satisfactory performance on that particular task (Wang et al., 2018; Ostapenko et al., 2019). Most of these methods, especially the deep neural network-based ones, do not fare well in the continual learning scenarios. In continual learning, a model is required to fit on a stream of tasks where data distribution may not be uniform. For example, a network fitted on a first task tends to forget how to perform on it after trained on a sequential new task (Sun et al., 2018; Shin et al., 2017). This problem, namely catastrophic forgetting, poses a severe challenge in building a general language intelligent system with lifelong learning capacity. Efforts have been m"
2020.coling-main.397,D16-1162,0,0.0187155,"entities in the sentences. Moussallem et al. (2019) exploit the entity linking to disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel s"
2020.coling-main.397,P16-1160,0,0.021606,"affects on PER, PLC, Reason, and Anatomy. We think there may be two reasons to cause this phenomenon: i) During translating these entities, KG is unnecessary7 . ii) The KG we utilize in this paper does not cover the useful knowledge and semantic information which can improve the translation of these entities. 6.3 Comparison on Different Granularities In this paper, we split the entities into sub-entity granularity. Actually, besides the sub-entity granularity, we also evaluate the other fine granularities: hybrid word-character granularity (Luong and Manning, 2016) and character granularity (Chung et al., 2016). The results are reported in Table 5. The results show that in both Transformer and our proposed multi-task method, the sub-entity granularity can produce better results than hybrid word-character granularity and character granularity. 7 Take the entity PER as an example, assuming that the neural model tends to translate a person’s name, while the KG always contains knowledge on his/her occupation, age or education, etc. Intuitively, this knowledge is not benefit for the translation of a person’s name. 4502 Model TransE TransH Transformer(sub-entity) Transformer(sub-entity)+MT Appeared Head E"
2020.coling-main.397,P19-1294,0,0.0660377,"dling the entities. 1 Introduction Neural machine translation (NMT) based on the encoder-decoder architecture becomes a new state-ofthe-art approach due to its distributed representation and end-to-end learning (Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). During translation, the translation quality of the entities in a sentence has a great influence on the translation quality of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies utilize KG to enhance the semantic representing of entities in a sentence (Moussallem et al., 2019; Lu et al., 2018) or extract the important semantic vectors with KG (Shi et al., 2016). Although great efforts have been made to incorporate KG into NMT, we find the existing methods have the following two problems: Knowledge Under-utilization: Given a KG (denoted by K) and a parallel se"
2020.coling-main.397,D17-1146,0,0.0175299,"l. (2019) exploit the entity linking to disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity"
2020.coling-main.397,N18-2081,0,0.0475228,"Missing"
2020.coling-main.397,P19-1581,0,0.0186415,"studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameter"
2020.coling-main.397,P19-1352,0,0.013042,"on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameters in encoder or de"
2020.coling-main.397,P16-1100,0,0.121946,"U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-13, 2020 yansuan anfei latong yansuananfeilatong yansuanyipu component yansuanbenbing paitong latong （盐酸苯丙哌酮） （盐酸依普拉酮） anfeitaiming type (安非他命) U − (K yaopin (药品) D) D −K"
2020.coling-main.397,D15-1166,0,0.29435,"cally, we first split the entities in KG and sentence pairs into sub-entity granularity by using joint BPE. Then we utilize the multi-task learning to combine the machine translation task and knowledge reasoning task. The extensive experiments on various translation tasks have demonstrated that our method significantly outperforms the baseline models in both translation quality and handling the entities. 1 Introduction Neural machine translation (NMT) based on the encoder-decoder architecture becomes a new state-ofthe-art approach due to its distributed representation and end-to-end learning (Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). During translation, the translation quality of the entities in a sentence has a great influence on the translation quality of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies"
2020.coling-main.397,P02-1040,0,0.109756,"sentence pairs on tourism as development set, and 2000 other sentence pairs as test set. The CN⇒UY translation are extracted from CCMT-19 dataset. The RO⇒EN translation are extracted from TED dataset. The statistics of training pairs and KGs are shown in Table 1. Training and Evaluation Details. We implement the NMT model based on the THUMT toolkit6 (Zhang et al., 2017). We use the “base” version parameters of the Transformer model. On all translation tasks, we use the BPE (Sennrich et al., 2016) method to merge 30K steps. We evaluate the final translation quality with case-insensitive BLEU (Papineni et al., 2002) for all translation tasks. 2 http://www.openkg.cn/dataset/cndbpedia http://www.openkg.cn/dataset/symptom-in-chinese 4 The target KG in medical and tourism KG is filtered by retaining the triples which contain the pre-defined key words. 5 http://www.openkg.cn/dataset/tourist-attraction 6 https://github.com/THUNLP-MT/THUMT 3 4499 Task CH⇒EN CH⇒UY RO⇒EN Domain General Medical Tourism General General Source KG 3.3M 0.41M 0.16M 3.3M - Target KG 2.4M 0.29M 0.28M 2.4M Pair 2.01M 0.22M 0.44M Dev/Test 919/6146 2000/2000 2000/2000 914/1678 1166/1160 Table 1: The statistics of the training data. Column"
2020.coling-main.397,P16-1162,0,0.737095,"n D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-13, 2020 yansuan anfei latong yansuananfeilatong yansuanyipu component yansuanbenbing paitong latong （盐酸苯丙哌酮） （盐酸依普拉酮） a"
2020.coling-main.397,P16-1212,0,0.0823736,"y of the whole sentence. However, translating these entities remains challenging (Moussallem et al., 2019) and various methods are proposed to improve the translation of entities (Zhang and Zong, 2016; Dinu et al., 2019; Ugawa et al., 2018; Wang et al., 2019). Among them, some methods aim at incorporating the knowledge graph (KG) to utilize their structured knowledge on entities and improve the entity translation. These studies utilize KG to enhance the semantic representing of entities in a sentence (Moussallem et al., 2019; Lu et al., 2018) or extract the important semantic vectors with KG (Shi et al., 2016). Although great efforts have been made to incorporate KG into NMT, we find the existing methods have the following two problems: Knowledge Under-utilization: Given a KG (denoted by K) and a parallel sentence pair dataset (denoted by D), the full entity set U can be divided into four subsets as shown in Fig. 1 (a): 1) K ∩ D entities, which appear in both K and D; 2) D −K entities, which only appear in D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on"
2020.coling-main.397,2020.acl-main.325,0,0.0161976,"ies. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to improve the semantic represent of sub-entity and parameters in encoder or decoder. The proposed met"
2020.coling-main.397,D17-1060,0,0.168589,"bsets as shown in Fig. 1 (a): 1) K ∩ D entities, which appear in both K and D; 2) D −K entities, which only appear in D; 3) K −D entities, which only appear in K; 4) U −(K ∪D) entities, which are neither in K nor D. While previous studies (Shi et al., 2016; Moussallem et al., 2019; Lu et al., 2018) only focus on the K ∩ D entities, the other three subsets are ignored. Consequently, much knowledge on the other three subsets in KG is wasted. Granularity Mismatch: The current KG methods, such as knowledge embedding methods (Bordes et al., 2013; Wang et al., 2014) and knowledge reasoning methods (Xiong et al., 2017), always utilize the entity as the basic granularity. While the NMT models utilize the sub-word (Sennrich et al., 2016) or hybrid word-character (Luong and Manning, 2016) as the translation granularity. This granularity mismatch between KG and NMT makes the current KG methods different to be utilized into NMT. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 4495 Proceedings of the 28th International Conference on Computational Linguistics, pages 4495–4505 Barcelona, Spain (Online), December 8-1"
2020.coling-main.397,2020.amta-research.11,0,0.0774918,"Missing"
2020.coling-main.397,D18-1036,1,0.844092,"disambiguate the entities found in a sentence. While these studies only focus on the K ∩ D entities. Recently, Zhao et al. (2020) utilize the entity alignment methods to improve the D −K entities. Different from these methods, the proposed methods utilize multi-task learning on sub-entity granularity to make full use of KG and improve the entity translation. Incorporating bilingual lexicons and Phrases into NMT. Our method is also inspired by the studies of incorporating bilingual lexicons and phrases into NMT (Arthur et al., 2016; Zhang and Zong, 2016; Feng et al., 2017; Hasler et al., 2018; Zhao et al., 2018b; Zhao et al., 2018a; Dinu et al., 2019; Huck et al., 2019; Liu et al., 2019; Susanto et al., 2020). They utilize the external bilingual lexicons and phrases to improve the lexical and phrases translation. Different from these studies, we incorporate the KG to improve the entity translation. 8 Conclusion To improve the entity translation and make1 full use of KG, in this paper we propose a KG enhanced NMT method with multi-task learning on sub-entity granularity. We first represent the entity in KG and parallel sentence pairs into sub-entity granularity. Then we utilize multi-task learning to"
2020.coling-main.496,W18-6402,0,0.0186341,"e highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not involved in the process of text encoding. Obviously, it will be easier for the decoder to generate This work is licensed under a Creative Commons Attribution 4.0 Internatio"
2020.coling-main.496,W17-4746,0,0.0458668,"Missing"
2020.coling-main.496,D17-1105,0,0.0235329,"ect-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a"
2020.coling-main.496,P17-1175,0,0.389415,"rce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely explored in machine translation (MT) (Calixto et al., 2017; Caglayan et al., 2017; Helcl et al., 2018; Gr¨onroos et al., 2018) in recent years, and the performances of their models surpass text-only models (Barrault et al., 2018). The main difference between multimodal MT and multimodal text summarization is: the model for MT is required to convert the same semantics from the input sentence and the paired image to the output, while for summarization, the model is expected to select the important information from the input. Li et al. (2018a) propose a hierarchical attention model for the multimodal sentence summarization task, while the image is not i"
2020.coling-main.496,D18-1438,0,0.0891796,"; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learning has been widely expl"
2020.coling-main.496,N16-1012,0,0.201591,"highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015)"
2020.coling-main.496,N18-2097,0,0.0120218,"e between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi"
2020.coling-main.496,D17-1095,0,0.0234216,"ctive gates including global-level, gridlevel and object-level visual gates to select salient encoding information. We also integrate the textual and visual selective gates to construct multimodal selective gates. In this figure, the summary is generated with the text-based decoder, and we also apply multimodal selective gates to the multimodal-based decoder (Li et al., 2018a) in our work. 2.2 Multimodal Seq2seq Models Libovick´y and Helcl (2017) propose multi-source seq2seq learning with hierarchical attention. Calixto and Liu (2017) use images as source words to improve translation quality. Delbrouck and Dupont (2017) adjust various attention for the visual modality. Calixto et al. (2017) propose attention mechanisms for textual and visual modalities and combine them to decode target words. Narayan et al. (2017) develop extractive summarization with side information including images and captions. Zhu et al. (2018), Chen and Zhuge (2018) and Zhu et al. (2020) propose to generate multimodal summary for multimodal news document. Li et al. (2018a) first introduce the multimodal sentence summarization task, and they propose a hierarchical attention model, which can pay different attention to image patches, word"
2020.coling-main.496,D19-1301,0,0.0122215,"ploy a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visu"
2020.coling-main.496,W18-6439,0,0.0553968,"Missing"
2020.coling-main.496,P16-1154,0,0.024788,"the important information from the source text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao"
2020.coling-main.496,P16-1014,0,0.0210833,"text. • We propose a visual-guided modality regularization module to encourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the pr"
2020.coling-main.496,W18-6441,0,0.0326239,"Missing"
2020.coling-main.496,N16-1082,0,0.0779981,"Missing"
2020.coling-main.496,D17-1222,0,0.0734802,"curately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Nar"
2020.coling-main.496,C18-1121,1,0.321043,"select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image wi"
2020.coling-main.496,P17-2031,0,0.0523496,"Missing"
2020.coling-main.496,W04-1013,0,0.0202043,"on samples. 4.2 Experimental Settings We set word embedding size to 300 and GRU hidden state size to 512. We use the full source and target vocabularies collected from the training data, which have 36,916 source words and 26,168 target words, respectively. The mini-batch size is 64, and beam search size is 10. Adam optimizer is applied with the learning rate of 0.0005, momentum parameters β1 = 0.9 and β1 = 0.999, and  = 10−8 . We use dropout (Srivastava et al., 2014) with probability of 0.2 and gradient clipping (Pascanu et al., 2013) with range [−1, 1]. During training, we test the ROUGE-2 (Lin, 2004) F1-score on the validation set for every 5,000 batches, and we halve the learning rate if the score drops for 5 consecutive testings. 5660 Methods RG-1 RG-2 RG-L Non-Selective T-Selective Global V-Selective Object V-Selective Grid V-Selective 44.53 (± 0.11) 44.81 (± 0.14) 45.31 (± 0.15) 44.83 (± 0.12) 45.11 (± 0.16) 22.67 (± 0.12) 23.13 (± 0.13) 23.39 (± 0.13) 23.28 (± 0.13) 23.33 (± 0.15) 41.91 (± 0.09) 41.88 (± 0.11) 42.48 (± 0.11) 42.03 (± 0.12) 42.21 (± 0.12) T + Global V-Selective T + Object V-Selective T + Grid V-Selective T + Grid V-Selective + MR 45.51 (± 0.13) 45.33 (± 0.14) 45.58 (±"
2020.coling-main.496,P17-2100,0,0.0165761,"advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations"
2020.coling-main.496,W04-3252,0,0.0347575,"overlapping words, except for stop-words, between the input sentence and the reference summary as the ground-truth keywords). Following Zhou et al. (2017), we use the method of Li et al. (2016) to calculate the contribution of the selective gates to the final summary. Considering that the average word count of the summaries in the validation set is eight, we take the words with top-8 contribution values as the activated keywords by the selective mechanisms. Table 3 shows the results for keyword extraction. Our models with selective encoding perform better than unsupervised TextRank algorithm (Mihalcea and Tarau, 2004) (we also take the top-8 scored words as the keywords), and Multimodal Selective show advantages over Textual Selective. We further train a BiLSTM-CRF model (Huang et al., 2015) using sentence-keyword samples of the multimodal sentence summarization dataset, and the keyword extraction result for BiLSTM-CRF is better than our model, indicating a promising prospect for further development of the selective mechanism. In the future, we will dedicate our efforts to explore whether the selective gate can benefit from supervision signals of a special keyword extractor. A feasible research direction m"
2020.coling-main.496,K16-1028,0,0.0569473,"Missing"
2020.coling-main.496,D15-1044,0,0.34564,"ary to capture the highlights embedded in the image more accurately. To verify the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015;"
2020.coling-main.496,P17-1099,0,0.210618,"ourage the model focus on the key information in the source. • The experimental results on a multimodal sentence summarization dataset demonstrate that our proposed system can take advantage of multimodal information and outperform baseline methods. 2 2.1 Related Work Abstractive Sentence Summarization Rush et al. (2015) first propose a seq2seq model to generate the summary for a sentence. Chopra et al. (2016) and Nallapati et al. (2016) further develop the seq2seq for this task. Gu et al. (2016), Zeng et al. (2016), and Gulcehre et al. (2016) incorporate a copying mechanism into the seq2seq. See et al. (2017) incorporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the inpu"
2020.coling-main.496,C18-1146,0,0.0135528,"(2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate"
2020.coling-main.496,P17-1108,0,0.0123044,"fy the generalization of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017;"
2020.coling-main.496,P19-1207,0,0.0147737,"generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ... h1 x1 h2 x2 h3 ... x3 ... → [hn;h1] hi hn MLP xn hi Global-level visual sGate MLP hi sGateo MLP hi ... ... Grid-level visual sGate sGateg Selective Gate (sGate) max-pooling sGateM hi Textual sGate hn＇ sGate2 MLP MLP hi Attention h1＇ sGate1 → yt-1 sGate1 sGate2 MLP MLP max-pooling sGateN ... ... hi MLP hi Object-level visual sGate Figure 2: The framework of our model. We design visual selective gates including global-level, grid"
2020.coling-main.496,P17-1101,0,0.258595,"ion of our model, we adopt the multimodal selective gate to the text-based decoder and multimodal-based decoder. Experimental results on a public multimodal sentence summarization dataset demonstrate the advantage of our models over baselines. Further analysis suggests that our proposed multimodal selective gate network can effectively select important information in the input sentence. 1 Introduction Text summarization is a task that condenses a long sentence to a short version. Existing researches (Rush et al., 2015; Chopra et al., 2016; Zeng et al., 2016; Li et al., 2017; Tan et al., 2017; Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018;"
2020.coling-main.496,D18-1400,0,0.0969879,"corporate the pointer-generator model with the coverage mechanism. Chen et al. (2016) propose a distraction model to focus on the different parts of the input. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding mechanism to filter secondary information. Li et al. (2017) apply a deep recurrent generative decoder to the seq2seq framework. Cao et al. (2018) and Li et al. (2018b) solve the problem of fake facts in a summary using fact descriptions of the input. Zhou et al. (2018b) extend the copying mechanism from word to sequence level. Song et al. (2018) propose structure-infused copy mechanisms to copy important words and relations from the input sentence to the summary. Cohan et al. (2018) propose a discourse-aware hierarchical attention model for abstractive summarization. Duan et al. (2019) propose a contrastive attention mechanism that attends to irrelevant parts of the input. Wang et al. (2019) present a bi-directional selective encoding model with template to softly select key information from source text. 5656 s0 yt ... st-1 sGater sGatet st h2＇ h3＇ ... ..."
2020.coling-main.496,D18-1448,1,0.820738,"Zhou et al., 2017; Zhang et al., 2018; Li et al., 2020b) produce summary only from the text. However, it has been proved that human understands the text relying on multimodal information (Waltz, 1980; Srihari, 1994; He and Deng, 2017), such as linguistic and visual signals. In this paper, we focus on the multimodal summarization task (Li et al., 2018a) that generates summary simultaneously drawing knowledge from coupled text and image, which can facilitate other applications such as image captioning (Xu et al., 2015; Vinyals et al., 2015), multimodal news summarization (Narayan et al., 2017; Zhu et al., 2018; Chen and Zhuge, 2018), and electronic commerce (e-commerce) product description generation (Chen et al., 2019; Elad et al., 2019; Zhang et al., 2019; Li et al., 2020a), etc. Intuitively, it is easier for a reader to grasp the highlight of a news event by viewing an image than by reading a long text. Hence we believe that the image will benefit text summarization system. Figure 1 illustrates this phenomenon. For a given source sentence, a paired image visualizes a set of event highlight words, which highly correlates with the reference summary. Multimodal sequence-to-sequence (seq2seq) learni"
2020.emnlp-main.116,E06-1002,0,0.0658535,"Missing"
2020.emnlp-main.116,D19-5721,0,0.0146246,"fferent standard entities, where the word “颅神经 (cranial nerve)” is omitted from the mention. Besides, features in the textual level are not detailed enough to identify the exact number of procedures in given mentions such as case 2, which is a Tri-combined procedure but with only one “+” delimiter. Hence how to identify the number of linking entities for a given mention is crucial for Chinese medical procedure normalizing. Previous studies which adopt the discriminative model to solve the problem of variation in medical entity normalization (Li et al., 2017; Luo et al., 2018; Ji et al., 2019; Deng et al., 2019) involve two basic steps: First, entity candidates are selected from all entities in KB through artificially designed rules (Li et al., 2017) or text similarity methods, 1490 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1490–1499, c November 16–20, 2020. 2020 Association for Computational Linguistics mentions 双侧脑深部电刺激植⼊术 (Bilateral deep brain stimulation implantation) case 1 输尿管镜扩张检查 置双 管术 + J (Ureteroscopy and dilatation + Double J-stent placement) case 2 standard entities 脑深部电极置⼊术 (Deep brain electrode implantation) 颅神经刺激脉冲发⽣器植⼊ (Cranial nerve"
2020.emnlp-main.116,P15-2049,0,0.0577922,"Missing"
2020.emnlp-main.116,K16-1026,0,0.0145797,"tegory-based constraint decoding and category-based model refining to avoid unrealistic results. The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures. 1 Introduction Named entity normalization (NEN), which is also known as entity linking, is one of the fundamental tasks within natural language processing (Hachey et al., 2013; D’Souza, 2015; Fang et al., 2016; Wu et al., 2018). Medical entity normalization is a typical problem of NEN in the medical domain, which aims at linking references or mentions of medical terminology to standard entities in a given medical knowledge base (KB) such as the International Statistical Classification of Diseases and Related Health Problems 9th Revision (ICD-9). Due to the nature of the domain, although different occupational or writing habits can result in stan∗ Contribution during internship at National Laboratory of Pattern Recognition, Institute of Automation, CAS the linked standard entities of a given medical"
2020.emnlp-main.116,D15-1166,0,0.101447,"Missing"
2020.emnlp-main.116,L18-1146,1,0.815298,"and derive a label prefix tree for each category to accommodate all the entities belonging to it. Then, the generative model will, in turn, decode the category label and standard entities when given the input mention. At each decoding step, we construct a constraint character set by tracing previously generated characters with corresponding label prefix trees. Finally, we integrate the constraint set into our model to restrict the generated characters belonging to its corresponding category. 2) Catebory-based refining. Entities under the same label always share common information. Inspired by Li et al. (2018), we propose a category-based refinement strategy in order to make the model parameters better fit the category of input mentions. To achieve this, for each input mention, we first adopt the general model to normalize it and obtain the category information from the output. Then we redistribute the original test dataset into several sub-test datasets based on category. Finally, for each sub-test dataset, we find sentence pairs within the same categories from the training data and use them to fine-tune the parameters of the general model. In addition, we propose a “generating and re-ranking” str"
2020.emnlp-main.116,P16-1096,0,0.158098,"og P (ti |si ; θ)} (3) ˆ θ∈N (θ) implication-number n candidates Figure 5: The overall framework of generating and reranking search in our decoder. Given an input mention, the beam search decoder with beam size k will output k results, where each result may contain one or more entities due to the “multi-implication” problem. All these entities are grouped as a candidate set of the input mention and any duplicates are removed. The calculation of semantic similarity can be implemented by various neural network structures, such as ABCNN (Yin et al., 2016), LSTM (Hochreiter and Schmidhuber, 1997; Limsopatham and Collier, 2016), and bidirectional encoder representations from Transformers (BERT) (Devlin et al., 2019; Ji et al., 2019). Considering the state-of-the-art performance of BERT in semantic similarity learning, which could also be applied to entity normalization tasks (Ji et al., 2019), we calculate the semantic similarity between input mentions with candidates by adopting a BERT-based method as introduced 1494 by (Ji et al., 2019). For the “multi-implication” mentions, we calculate the similarity score for each generated entity candidate and select the top n candidates as the final result of the linked stand"
2020.emnlp-main.116,N10-1072,0,0.0394301,"aseline model does not. 4.3 character Table 7: Performance of different granularity Table 6: Number of candidates per mention and rate of standard entity recall for the candidate set generated by different beam sizes of the proposed sequence generative model, compared against those selected by the traditional IR model BM25 Input mention: token Related work There are two areas related to our work: Entity normalization: Most of the entity normalization studies consider the domain-specific knowledge base or dictionary as the scope of standard entities. Early methods Bunescu and Pas¸ca (2006) and Zheng et al. (2010) design discriminative features, such as the TF-IDF, to compare the similarity of candidate entity with entity description and feed to the ranking framework. Popular approaches Leaman et al. (2013); Limsopatham and Collier (2016); Li et al. (2017); Ji et al. (2019) handle this as a sentence-pair classification task. Leaman et al. (2013) first proposed a pairwise learning-torank technique that adopts a vector-space model to measure the text similarity between medical entity mentions and standard entity in KB. Deep neural networks have also been proposed to normalize biomedical entities. Limsopa"
2020.emnlp-main.116,D15-1104,0,0.0225882,"omain, which aims at linking references or mentions of medical terminology to standard entities in a given medical knowledge base (KB) such as the International Statistical Classification of Diseases and Related Health Problems 9th Revision (ICD-9). Due to the nature of the domain, although different occupational or writing habits can result in stan∗ Contribution during internship at National Laboratory of Pattern Recognition, Institute of Automation, CAS the linked standard entities of a given medical mention should always be unique. Thus, unlike NEN as applied generally(Hachey et al., 2013; Luo et al., 2015; Wu et al., 2018; Aguilar et al., 2019), in the medical domain, the main challenge is not ambiguity – the same entity mention may be linked to different concepts, it is variation – the same underlying concept can be linked by different entity mentions. However, different from the normalization task of the medical entity with simple nominal structure, such as disease (Kang et al., 2012; D’Souza and Ng, 2015) or anatomical body (Wang et al., 2019), Chinese medical procedure normalization have to face the challenge of multi-implication – a mention which contains multiple procedure concepts shoul"
2020.emnlp-main.175,C18-1305,1,0.834672,"value > 0) of generating the ground truth sentence Y = {y1 , · · · , yT }, and utilize a monotone decreasing function to get the final reward bounded in 0 ∼ 1 as follows: 1 r(g) = e−g = e− T PT t=1 gt (5) A high value of the reward means that it is easy to generate the ground truth. Therefore, the selected context sentences should be encouraged. Conversely, if a reward is low, generating the ground truth with the selected context would cost a lot, so the selection is discouraged. 3.3.3 Self-Critical Training We train the whole model with the self-critical training method (Rennie et al., 2017; Bai et al., 2018). The goal of RL training is to minimize the negative expected reward. And in practice, the loss is usually approximated with a single sample u from the policy P as follows: Lrl = −Eu∼P [r(u)] ≈ −r(u), u ∼ P (6) The self-critical training introduces a baseline reward r(u0 ) to reduce the variance of the gradient, where u0 is obtained by the inference algorithm at test time. The final gradient is estimated by: ∇Lrl = (r(u) − r(u0 ))∇logP (u) (7) Specifically, we denote the trainable parameters of the context scorer and DocNMT by ω and θ, respectively. For each source sentence X, we select a set"
2020.emnlp-main.175,N18-1118,0,0.347023,"ring the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models and test them using various context settings1 . During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared wit"
2020.emnlp-main.175,P18-1063,0,0.0263756,"rrer, 2017), and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires backtranslation model at inference phrase. Maruf et al. (2019) sharpen the attention weights between the source and context sentences through the sparse2249 max function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation where context is not needed. Inspired by the extractive-abstractive summarization (Chen and Bansal, 2018), our approach is different from above DocNMT methods. Our approach can explicitly select dynamic size (that can be 0) of context sentences for the translation of different source sentences. 7 Conclusion and Future Work We propose a dynamic selection method to choose variable sizes of context sentences for documentlevel translation. The candidate context sentences are scored and selected by two proposed strategies. We train the whole model via reinforcement learning, and design a novel reward to encourage the selection of useful context sentences. When applied to existing DocNMT models, our ap"
2020.emnlp-main.175,D14-1179,0,0.016645,"Missing"
2020.emnlp-main.175,N19-1423,0,0.0148332,"ording to the selection probability in Pselect , we can obtain useful context sentences for the translation task. To select context dynamically, we add a special empty sentence “hN ON i” into the candidate context set, which stands for the situation that translates a source sentence without any context. As a result, we select those context sentences …… Candidate Context Sentences Reward DocNMT Source Sentence Sample As Figure 1 shows, we obtain the representation of context sentences for scoring. Inspired by the popular pre-training language models such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), we produce one instance by concatenating the source sentence with a context sentence, and adding a special symbol “hDCSi” at the beginning and a separator token “hSEP i” in between. The instance is fed into a stack of L1 Transformer encoder layers. We believe the special symbol “hDCSi” can encode the information of source-context sentence pairs well by the self-attention. For a candidate context sentence z ∈ Z, its hidden state of “hDCSi” after L1 layers is extracted as the input to L2 Transformer encoder layers to model the dependencies among context sentences. We denote the hidden state af"
2020.emnlp-main.175,W19-5321,0,0.0290954,"n (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train t"
2020.emnlp-main.175,D19-1294,0,0.0887846,"el neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models and test them using various context settings1 . During the test, we obtain dynamic context sentences that achieve the best BLEU scores by traversing all the context combinations for each source sentence. Compared with the fixed size context (row 1 and 2), dynamic context (row 3 and 4)"
2020.emnlp-main.175,D19-6503,0,0.0877178,"Missing"
2020.emnlp-main.175,W19-6616,0,0.0142413,", 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018; Yang et al., 2019; Voita et al., 2018; Xu et al., 2020) , or full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Kang and Zong, 2020; Zheng et al., 2020). They ignore the individualized needs for context when translating different source sentences. Some works have noticed that not all context is useful (Jean and Cho, 2019; Kim et al., 2019). Kimura et al. (2019) explore the context selection in the single-encoder framework (Tiedemann and Scherrer, 2017), and select context sentences that yield highest forced back-translation probability. However, the method cannot optimize DocNMT model at training phase, and requires backtranslation model at inference phrase. Maruf et al. (2019) sharpen the attention weights between the source and context sentences through the sparse2249 max function, and implicitly select context with high attention weights. Nevertheless, the method lacks direct supervision over context selection, and it cannot cover the situation w"
2020.emnlp-main.175,C18-1050,0,0.125614,"l machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown"
2020.emnlp-main.175,D15-1166,0,0.202437,"Missing"
2020.emnlp-main.175,2020.acl-main.321,0,0.666884,"eat progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires different context. As shown in Table 1, we train two DocNMT models a"
2020.emnlp-main.175,P18-1118,0,0.206941,"xt size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sente"
2020.emnlp-main.175,N19-1313,0,0.463049,"n strategies. The details of Transformer layers are shown in the right dotted box. 2 Document-level Machine Translation A standard DocNMT system generally translates a source sentence X = {x1 , · · · , xI } to a target sentence Y = {y1 , · · · , yT } with the aid of contextual information Z that is usually a subset of the candidate context set Z. The model is trained to minimize the negative log-likelihood as: Lmle = − T X logP (yt |y&lt;t , X, Z; θ) (1) t=1 Different granularity (word or sentence) and different sources (source-side or target-side) of contextual information Z have been explored. Maruf et al. (2019) divide the candidate context set Z into two cases: offline where the context comes from the entire document, and online that only uses the past context. In this paper, we mainly focus on a general scenario, where DocNMT translates sentences with the online source-side context sentences. 3 Dynamic Context Selection Our approach translates a source sentence X in the document in two steps. First, we select the appropriate context sentences for the translation of X via the selection module. Independent of DocNMT module, this step is conducted before the context encoding in DocNMT module. The core"
2020.emnlp-main.175,W18-6307,0,0.0444008,"Missing"
2020.emnlp-main.175,P02-1040,0,0.10748,"2019). 4.2 Models We compare our approach with the following methods: 1) S ENT N MT (Vaswani et al., 2017) is a standard sentence-level Transformer model using the “base” version parameters. 2) TDNMT (Zhang et al., 2018) introduces the contextual information by adding attention sub-layers at each encoder and decoder layer. We use 2 previous consecutive context sentences as they suggested. 3) HAN (Miculicich et al., 2018) uses 3 previous sentences as 3 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 4 http://data.statmt.org/ news-commentary/v14 Results and Analysis Main Results We use BLEU (Papineni et al., 2002) score to evaluate the translation quality. Considering the memory limitation and complex sampling space, we select dynamic context from previous six sentences. Table 10 shows the performance of models utilizing different context settings. We always keep the same setting for training and test. Comparison with Fixed Context Methods. The performance of DocNMT models with fixed context is shown in row 2∼5. Row 2 and 3 follow the context settings in the published papers. It can be found that using more context sentences indiscriminately (row 4 and 5) does not bring significant BLEU improvement. In"
2020.emnlp-main.175,P16-1162,0,0.321796,"Missing"
2020.emnlp-main.175,D19-1168,0,0.410747,"xed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attention weights lacks guidance, and they cannot handle the situation where the source sentences achieve the b"
2020.emnlp-main.175,W17-4811,0,0.123918,"ngs are consistent. Introduction Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences"
2020.emnlp-main.175,Q18-1029,0,0.547054,"on Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation of different source sentences requires differe"
2020.emnlp-main.175,P16-1008,0,0.0488321,"Missing"
2020.emnlp-main.175,D19-1081,0,0.344414,"Missing"
2020.emnlp-main.175,P19-1116,0,0.225925,"Missing"
2020.emnlp-main.175,P18-1117,0,0.25425,"Experiments indicate that only the limited context sentences are really useful, and they change with source sentences. Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the mo"
2020.emnlp-main.175,D19-1164,0,0.26443,"ey change with source sentences. Majority of existing DocNMT models set the context size or scope to be fixed. They utilize all of 1 We apply a typical DocNMT method (Zhang et al., 2018) to train models on Zh→En TED, and select 1,000 sentences to test. The BLEU of sentence-level baseline is 20.06. 2242 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2242–2254, c November 16–20, 2020. 2020 Association for Computational Linguistics the previous k context sentences (Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Voita et al., 2019b; Yang et al., 2019; Xu et al., 2020), or the full context in the entire document (Maruf and Haffari, 2018; Tan et al., 2019; Xiong et al., 2019; Zheng et al., 2020). As a result, the inadequacy or redundancy of contextual information is almost inevitable. From this viewpoint, Maruf et al. (2019) propose a selective attention approach that uses the sparsemax function (Martins and Astudillo, 2016) instead of the softmax to normalize the attention weights. The sparsemax assigns the low probability in softmax to zero so that the model can focus on the sentences with high probability. However, the learning of attent"
2020.emnlp-main.175,P17-4012,0,0.0864033,"ampled context Z. context. We adopt the “HAN encoder + HAN decoder” strategy that adds a hierarchical network on the top of the last encoder and decoder layer to model sentence-level and word-level contextual information. 4) SAN (Maruf et al., 2019) utilizes all context in the entire document by calculating the sentence-level and word-level weights. It focuses on relevant context sentences through the sparsemax function. We choose the model that integrates the online context into encoder with “sparse-soft H-Attention”. We implement our approach and baseline methods based on the toolkit THUMT (Zhang et al., 2017). The parameters are the “base” version of the original Transformer (Vaswani et al., 2017). The d1 and d2 in Eq. 2 are 512 and 256, respectively. We set the layers of L1 = 2 and L2 = 2. The effect of layer depth of context scorer and more implementation details are shown in the appendix. 5 5.1 4 4.1 Experiment Datasets We evaluate our approach on different domains of Chinese-English (Zh→En) and English-German (En→De) datasets. The corpora statistics are listed in Table 9. For TED Talks in IWSLT173 , we use dev-2010 as the development set, and tst2010∼2013 as the test set for both Zh→En and En→"
2020.emnlp-main.175,D18-1049,0,0.281493,"Missing"
2020.emnlp-main.175,D17-1301,0,0.273126,"test context settings are consistent. Introduction Although neural machine translation (NMT) has achieved great progress in recent years (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017), when fed an entire document, standard NMT systems translate sentences in isolation without considering the cross-sentence dependencies. Consequently, document-level neural machine translation (DocNMT) methods are proposed to utilize source-side or target-side intersentence contextual information to improve translation quality over sentences in a document (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Tu et al., 2018; Kuang et al., 2018; Junczys-Dowmunt, 2019; Ma et al., 2020). More recently, researchers of DocNMT mainly focus on exploring various attention-based networks to leverage the cross-sentence context efficiently, and evaluate the special discourse phenomena (Bawden et al., 2018; M¨uller et al., 2018; Voita et al., 2019b; Jwalapuram et al., 2019). However, there is still an issue that has received less attention: which context sentences should be used when translating a source sentence? We conduct an experiment to verify an intuition: the translation"
2020.emnlp-main.175,D16-1160,1,0.891947,"Missing"
2020.emnlp-main.175,P19-1117,1,0.843124,"simple way of selecting target-side context bears the risk of missing selection, the accuracy of some phenomena does not change significantly. Table 7 has shown that our approach can select useful target-side context in most cases. And the selection mechanism can make the model focus more on the useful context to resolve the discourse phenomena. 6 Related Work Standard neural machine translation methods usually focus on the sentence-level translation (Cho et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Luong et al., 2015; Tu et al., 2016; Zhang and Zong, 2016; Vaswani et al., 2017; Wang et al., 2019; Zhou et al., 2019; Zhao et al., 2020). As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality (Jean et al., 2017; Tu et al., 2018; Kuang et al., 2018) or discourse phenomena (Bawden et al., 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al.,"
2020.emnlp-main.175,D18-1397,0,0.0207271,"eline reward r(Z ∗ ) obtained by the current best policy (i.e., learned selection strategies), the method encourages model to explore more useful context (i.e., sampled context) that yields higher reward than the current best (i.e., selected context). 2245 Datasets TED Zh→En News TED En→De News Europarl Training 0.23M 0.31M 0.21M 0.33M 1.67M Dev 0.88K 2.00K 0.89K 3.00K 3.59K Test 4.68K 3.98K 4.70K 3.00K 5.14K Table 2: Dataset statistics in the number of sentences. For DocNMT module, we can combine the MLE objective (Eq. 1) and RL objective (Eq. 6) together to stabilize the training procedure (Wu et al., 2018) through a balance factor α as follows: ˆ θ) + (1 − α) ∗ Lrl (θ) L(θ) = α ∗ Lmle (Y |X, Z, (9) We introduce the RL objective into DocNMT module so that the model can make better use of the selected context. The final RL gradient of DocNMT is calculated by: ˆ − r(Z ∗ ))∇θ logPθ (Yˆ |X, Z) ˆ ∇θ Lrl (θ) = (r(Z) (10) where Yˆ is a sequence generated by current Docˆ NMT model with the sampled context Z. context. We adopt the “HAN encoder + HAN decoder” strategy that adds a hierarchical network on the top of the last encoder and decoder layer to model sentence-level and word-level contextual informa"
2020.emnlp-main.175,Q19-1006,1,0.830299,"ting target-side context bears the risk of missing selection, the accuracy of some phenomena does not change significantly. Table 7 has shown that our approach can select useful target-side context in most cases. And the selection mechanism can make the model focus more on the useful context to resolve the discourse phenomena. 6 Related Work Standard neural machine translation methods usually focus on the sentence-level translation (Cho et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Luong et al., 2015; Tu et al., 2016; Zhang and Zong, 2016; Vaswani et al., 2017; Wang et al., 2019; Zhou et al., 2019; Zhao et al., 2020). As a contrast, document-level neural machine translation methods mainly pay attention to how to utilize the cross-sentence context. Researchers propose various context-aware networks to utilize contextual information to improve the performance of DocNMT models on the translation quality (Jean et al., 2017; Tu et al., 2018; Kuang et al., 2018) or discourse phenomena (Bawden et al., 2018; Xiong et al., 2019; Voita et al., 2019b,a). However, most methods roughly leverage all context sentences in a fixed size that is tuned on development sets (Wang et al., 2017; Miculicich et"
2020.iwslt-1.15,W17-4712,0,0.0183908,"ain 6.1M and 16.4M monolingual sentences for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel."
2020.iwslt-1.15,C18-1111,0,0.0183138,"s for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is mainly source docume"
2020.iwslt-1.15,N13-1073,0,0.0374842,"ethods are used to implement escape character transformation and text normalization as pre-processing. For UNK processing, we find some of the numbers can not be well translated by model and we replace these UNKs with the numbers in source sentence. Otherwise, we remove the UNK symbols. 3.2 Parallel Data Filtering The following methods are applied to further filter the parallel sentence pairs. We remove sentences longer than 50 and select the parallel sentences where the length ratio (Ja/Zh) is between 0.53 and 2.90. We then calculate word alignment of each sentence pair by using fast align4 (Dyer et al., 2013). The percentage of aligned words and alignment perplexities are used as the metric where the thresholds are set as 0.4 and −30 respectively. Through the above filtering procedure, the number of the remaining data is reduced from 20.9M to 15.7M, as shown in Table 2. 3.3 Monolingual Data Filtering It is proven that back-translation is a simple but effective approach to enhance the translation quality 4 https://github.com/fxsjy/jieba 133 https://github.com/clab/fast_align Figure 4: The domain data processing steps, including the NMT model trained on general domain data, the NMT models fine tuned"
2020.iwslt-1.15,D16-1139,0,0.0251575,"anslation direction. Recent work (Edunov et al., 2018) has shown that different methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam search or greedy search. We adopt the back-translation script from fairseq2 and generate back-translated data with sampling for both translation directions. 2.3 Knowledge Distillation The goal of knowledge distillation is to deliver a student model that matches the accuracy of a teacher model (Kim and Rush, 2016). Prior work (Yang et al., 2018) demonstrates that student model can surpass the accuracy of the teacher model. In our experiments, we adopt sequence-level knowledge distillation method and investigate four different teacher models to boost the translation quality of student model. S2T+L2R Teacher Model: We translate the source sentences of the parallel data into target language using our source-to-target (briefly, S2T) system described in Section 2.1 with left-to-right (briefly, L2R) manner. S2T+R2L Teacher Model: We translate the source sentences of the parallel data into target language usi"
2020.iwslt-1.15,W18-6304,0,0.0143967,"performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depict"
2020.iwslt-1.15,W17-3204,0,0.0246073,"3 https://github.com/clab/fast_align Figure 4: The domain data processing steps, including the NMT model trained on general domain data, the NMT models fine tuned on specific domain, the domain classification and weighted ensemble in the decoding stage. Filtering Methods original remove illegal filter by length filter by LM Ja 941,297,925 10,078,827 8,175,157 6,128,443 Domain Wiki Spoken News Zh 928,670,666 32,644,917 30,415,964 16,374,195 3.4 Domain Data Processing Although the amount of provided training data is large enough, it is a noise set of web data built from multiple domain sources. Koehn and Knowles (2017) have demonstrated that the NMT model performs poorly when the test domain does not match 5 https://github.com/kpu/kenlm Web 4,006,232 9,534,754 2,444,884 Table 4: Statistics of domain data. Existing indicates existing parallel which is used to train the domain classifier, while Web means web crawled parallel in which the domain labels are predicted by the classifier. Table 3: The number of the remaining monolingual sentences for Japanese and Chinese after each filtering operation. as described in Section 2.2. To achieve that, we extract the high-quality monolingual sentences from the provided"
2020.iwslt-1.15,D15-1166,0,0.0604003,"em is neural machine translation system based on Transformer model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation"
2020.iwslt-1.15,N19-4009,0,0.0145701,"ty due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depicts the whole process of our submission system, in which we pre-process the provided data and train our advanced Transformer models on the bilingual data together with synthetic corpora from back-translation and knowledge distillation. With domain classification and fine tuning techniques, we obtain multiple models for ensemble strategy and post-processing. In this section, we will introduce each process step in detail. 2.1 NMT Baseline In this work, we build our model based on the powerful Transformer (Vaswani et al., 20"
2020.iwslt-1.15,P16-1009,0,0.104076,"to an output. Specifically, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We"
2020.iwslt-1.15,P16-1162,0,0.386197,"to an output. Specifically, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We"
2020.iwslt-1.15,N18-2074,0,0.169049,"al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al., 2018) and Dynamic Convolutions (Wu et al., 2019) to investigate the performances of advanced model variations. For the implementation, we extend the latest release of Fairseq1 (Ott et al., 2019). 1 https://github.com/pytorch/fairseq 2 System Description Figure 1 depicts the whole process of our submission system, in which we pre-process the provided data and train our advanced Transformer models on the bilingual data together with synthetic corpora from back-translation and knowledge distillation. With domain classification and fine tuning techniques, we obtain multiple models for ensemble strategy"
2020.iwslt-1.15,D18-1458,0,0.0285615,"Missing"
2020.iwslt-1.15,P17-2089,0,0.0184314,"onolingual sentences for Japanese and Chinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is"
2020.iwslt-1.15,D16-1160,1,0.675141,"ly, we can multiply query Qi by key Ki to obtain an attention weight matrix, which is then multiplied by value Vi for each token to obtain the self-attention token representation. As shown in Figure 3, we compute the matrix of outputs as: QK T Attention(Q, K, V ) = Softmax( √ )V dk (2) where dk is the dimension of the key. For the sake of brevity, we refer the reader to Vaswani et al. (2017) for more details. 131 2.2 Back-Translation Back-translation is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system (Sennrich et al., 2016a; Zhang and Zong, 2016). Especially for low-resource language tasks, it is indispensable to augment the training data by mixing the pseudo corpus with the parallel part. Back-translation first trains an intermediate target-to-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-to-target system. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We trained a n-gram based"
2020.iwslt-1.15,C18-1269,0,0.0176612,"hinese separately. The filtering results are presented in Table 3. The obtained monolingual sentences are fed to the trained model to generate pseudo parallel sentence pairs, which are employed to boost the performance of the model. Existing 558,531 1,290,796 21,661 the training data. Only the same or similar corpora are typically able to improve translation performance. Therefore, we apply domain adaptation methods in this task. Adaptation methods for neural machine translation have attracted much attention in the research community (Britz et al., 2017; Wang et al., 2017; Chu and Wang, 2018; Zhang and Xiong, 2018; Wang et al., 2020). They can be roughly classified into two categories, namely data selection and model adaptation. The former focuses on selecting the similar training data from out-of-domain parallel corpora, while the latter focuses on the internal model to improve model performance. Following these two categories, our domain data processing takes the following steps, as shown in Figure 4. Domain Label In this task, there are two kinds of domain labels provided: domains in existing parallel and domains in web crawled parallel. Since the later is mainly source document index for each sente"
2020.iwslt-1.15,P17-2060,1,0.83251,"r model. We augment the training data with knowledge distillation and back translation to improve the translation performance. Domain data classification and weighted domain model ensemble are introduced to generate the final translation result. We compare and analyze the performance on development data with different model settings and different data processing techniques. 1 Introduction Neural machine translation(NMT) has been introduced and made great success during the past few years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Gehring et al., 2017; Zhou et al., 2017; Vaswani et al., 2017). Among those different neural network architectures, the Transformer, which is based on self-attention mechanism, has further improved the translation quality due to the ability of feature extraction and word sense disambiguation (Tang et al., 2018a,b). In this paper, we describe our Transformer based neural machine translation system submitted to the IWSLT 2020 Chinese→Japanese and Japanese→Chinese open domain translation task (Ansari et al., 2020). Our system is built upon Transformer neural machine translation architecture. We also adopt Relative Position (Shaw et al"
2021.acl-long.184,P17-1067,0,0.0179121,"ogical perspective. The quantitative relations among basic emotions remain to be detected. In this work, emotion relations are quantitatively revealed based on our emotion representations. 2365 Emotion Datasets: Strapparava and Mihalcea (2007) introduced first emotion recognition dataset, Affective Text, in the domain of news headlines. After that, many emotion datasets that vary in domain, size and taxonomy have been developed. Wang et al. (2012) automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hashtags available in the tweets. Abdul-Mageed and Ungar (2017) introduced a fine-grained dataset with up to 24 types of emotion categories with Twitter data. Li et al. (2017) developed a multi-turn dialog dataset, DailyDialog, for detecting the emotions in ¨ the field of dialog systems. Ohman et al. (2018) presented a multi-dimensional emotion dataset with annotations in movie subtitles for the purpose of creating a robust multilingual emotion detection tool. Demszky et al. (2020) built a manually dataset with up to 27 fine-grained emotion categories on Reddit comments for emotion prediction. However, all above datasets are annotated with discrete basic"
2021.acl-long.184,C18-1081,0,0.0734803,"t vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to learn emotionenriched word embedding (EWE). However, all the above algorithms represent emotions in semantic space rather than emotion space. As shown in Table 1, each emotion category represented in semantic space reflect a piece of semantic information rather than a specific emotional state. In this work, we regard each emotion category as a specific emotional state in emotion space and represent each emotion category with a point in emotion 2364 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conf"
2021.acl-long.184,H05-1073,0,0.664308,"framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space. 1 Introduction In the past decades, a lot of tasks have been proposed in the field of text emotion analysis. The most primary one among them is emotion classification task (Alm et al., 2005). Based on emotion classification task, many new tasks have been proposed from different considerations. Lee et al. (2010) proposed the task of emotion cause extraction, which aims at predicting the reason of a given emotion in a document. Based on the emotion cause extraction task, Xia and Ding (2019) introduced the emotion-cause pair extraction task for the purpose of extracting the potential pairs of emotions and corresponding causes in a document. Jiang et al. (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To expres"
2021.acl-long.184,P11-1016,0,0.0709496,"asks have been proposed in the field of text emotion analysis. The most primary one among them is emotion classification task (Alm et al., 2005). Based on emotion classification task, many new tasks have been proposed from different considerations. Lee et al. (2010) proposed the task of emotion cause extraction, which aims at predicting the reason of a given emotion in a document. Based on the emotion cause extraction task, Xia and Ding (2019) introduced the emotion-cause pair extraction task for the purpose of extracting the potential pairs of emotions and corresponding causes in a document. Jiang et al. (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 201"
2021.acl-long.184,2020.acl-main.372,0,0.352478,". Wang et al. (2012) automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hashtags available in the tweets. Abdul-Mageed and Ungar (2017) introduced a fine-grained dataset with up to 24 types of emotion categories with Twitter data. Li et al. (2017) developed a multi-turn dialog dataset, DailyDialog, for detecting the emotions in ¨ the field of dialog systems. Ohman et al. (2018) presented a multi-dimensional emotion dataset with annotations in movie subtitles for the purpose of creating a robust multilingual emotion detection tool. Demszky et al. (2020) built a manually dataset with up to 27 fine-grained emotion categories on Reddit comments for emotion prediction. However, all above datasets are annotated with discrete basic emotion categories, which means the emotion categories are represented with one-hot vectors. Onehot representations ignore the underlying relations among emotion categories. In this work, the underlying emotion relations contained in the datasets are revealed with our emotion representations. Soft Labels: Hinton et al. (2015) observed that it is easier to train classifier using the soft targets output by trained classif"
2021.acl-long.184,N19-1423,0,0.0165807,", fear, joy, love, optimism, pessimism, sadness, surprise, and trust. Experiments In order to validate the intrinsic quality of our emotion representations, we conducted three experiGoEmotions is used to conduct the first two experiments (arrangement and mapping), and the above four datasets are used to validate our representations across corpora in last experiment. 2368 4.2 Model Settings Any model that outputs are soft labels can be employed to learn the distributed representations for emotion categories. In our experiments, TextCNN (Kim, 2014), BiLSTM (Schuster and Paliwal, 1997) and BERT (Devlin et al., 2019) are used as the training models. For comparison, experiments on word embedding learning algorithms are conducted to show emotion relations in semantic space. For a specific emotion category, we use its word embedding as its representations in semantic space. 100-dimensional GloVe (Pennington et al., 2014) is the word vectors used in TextCNN and BiLSTM. The detailed model settings are listed as follows: TextCNN: The height of convolutional kernel size is divided into three groups (3,4,5) and the width is 100, which is equal to the dimension of the word vectors. There are 32 channels in each gr"
2021.acl-long.184,D16-1170,0,0.0241208,"definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to"
2021.acl-long.184,P84-1044,0,0.631251,"Missing"
2021.acl-long.184,D14-1181,0,0.0471819,". (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use"
2021.acl-long.184,C18-1179,0,0.035546,"Missing"
2021.acl-long.184,W10-0206,0,0.0455737,"ation dataset. Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm. Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space. 1 Introduction In the past decades, a lot of tasks have been proposed in the field of text emotion analysis. The most primary one among them is emotion classification task (Alm et al., 2005). Based on emotion classification task, many new tasks have been proposed from different considerations. Lee et al. (2010) proposed the task of emotion cause extraction, which aims at predicting the reason of a given emotion in a document. Based on the emotion cause extraction task, Xia and Ding (2019) introduced the emotion-cause pair extraction task for the purpose of extracting the potential pairs of emotions and corresponding causes in a document. Jiang et al. (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emoti"
2021.acl-long.184,C18-1079,1,0.845692,"pendent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentime"
2021.acl-long.184,I17-1099,0,0.0268721,"e quantitatively revealed based on our emotion representations. 2365 Emotion Datasets: Strapparava and Mihalcea (2007) introduced first emotion recognition dataset, Affective Text, in the domain of news headlines. After that, many emotion datasets that vary in domain, size and taxonomy have been developed. Wang et al. (2012) automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hashtags available in the tweets. Abdul-Mageed and Ungar (2017) introduced a fine-grained dataset with up to 24 types of emotion categories with Twitter data. Li et al. (2017) developed a multi-turn dialog dataset, DailyDialog, for detecting the emotions in ¨ the field of dialog systems. Ohman et al. (2018) presented a multi-dimensional emotion dataset with annotations in movie subtitles for the purpose of creating a robust multilingual emotion detection tool. Demszky et al. (2020) built a manually dataset with up to 27 fine-grained emotion categories on Reddit comments for emotion prediction. However, all above datasets are annotated with discrete basic emotion categories, which means the emotion categories are represented with one-hot vectors. Onehot representati"
2021.acl-long.184,D19-1656,0,0.0316748,"Missing"
2021.acl-long.184,S12-1033,0,0.041188,"t vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) pro"
2021.acl-long.184,S17-1007,0,0.310859,"ations. Lee et al. (2010) proposed the task of emotion cause extraction, which aims at predicting the reason of a given emotion in a document. Based on the emotion cause extraction task, Xia and Ding (2019) introduced the emotion-cause pair extraction task for the purpose of extracting the potential pairs of emotions and corresponding causes in a document. Jiang et al. (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of e"
2021.acl-long.184,S18-1001,0,0.162097,"or. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emotion relations. Based on existing emotion detection tasks, many efforts have been made to achieve better performance (Danisman and Alpkocak, 2008; Xia et al., 2011; Kim, 2014; Xia et al., 2015; Li et al., 2018; Zong et al., 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) emb"
2021.acl-long.184,W18-6205,0,0.0225566,"first emotion recognition dataset, Affective Text, in the domain of news headlines. After that, many emotion datasets that vary in domain, size and taxonomy have been developed. Wang et al. (2012) automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hashtags available in the tweets. Abdul-Mageed and Ungar (2017) introduced a fine-grained dataset with up to 24 types of emotion categories with Twitter data. Li et al. (2017) developed a multi-turn dialog dataset, DailyDialog, for detecting the emotions in ¨ the field of dialog systems. Ohman et al. (2018) presented a multi-dimensional emotion dataset with annotations in movie subtitles for the purpose of creating a robust multilingual emotion detection tool. Demszky et al. (2020) built a manually dataset with up to 27 fine-grained emotion categories on Reddit comments for emotion prediction. However, all above datasets are annotated with discrete basic emotion categories, which means the emotion categories are represented with one-hot vectors. Onehot representations ignore the underlying relations among emotion categories. In this work, the underlying emotion relations contained in the dataset"
2021.acl-long.184,D14-1162,0,0.0901879,"o validate our representations across corpora in last experiment. 2368 4.2 Model Settings Any model that outputs are soft labels can be employed to learn the distributed representations for emotion categories. In our experiments, TextCNN (Kim, 2014), BiLSTM (Schuster and Paliwal, 1997) and BERT (Devlin et al., 2019) are used as the training models. For comparison, experiments on word embedding learning algorithms are conducted to show emotion relations in semantic space. For a specific emotion category, we use its word embedding as its representations in semantic space. 100-dimensional GloVe (Pennington et al., 2014) is the word vectors used in TextCNN and BiLSTM. The detailed model settings are listed as follows: TextCNN: The height of convolutional kernel size is divided into three groups (3,4,5) and the width is 100, which is equal to the dimension of the word vectors. There are 32 channels in each group. Batch size and learning rate are set to 16 and 0.001. BiLSTM: There is only one layer in this model. Batch size and learning rate are set to 16 and 0.001 separately, which are the same as for TextCNN. There are 32 neurons in the hidden layer in each direction. BERT: BERT-based model is used in this ex"
2021.acl-long.184,S07-1013,0,0.618349,"d eight distinct state emotions in their study: anger, disgust, fear, anxiety, sadness, happiness, relaxation, and desire. Similarly, Cowen and Keltner (2017) introduced a conceptual framework to analyze reported emotional states and elicited 27 distinct varieties of reported emotional experience. However, above work only gives the basic emotions of human emotional state from a psychological perspective. The quantitative relations among basic emotions remain to be detected. In this work, emotion relations are quantitatively revealed based on our emotion representations. 2365 Emotion Datasets: Strapparava and Mihalcea (2007) introduced first emotion recognition dataset, Affective Text, in the domain of news headlines. After that, many emotion datasets that vary in domain, size and taxonomy have been developed. Wang et al. (2012) automatically created a large emotion-labeled dataset (of about 2.5 million tweets) by harnessing emotion-related hashtags available in the tweets. Abdul-Mageed and Ungar (2017) introduced a fine-grained dataset with up to 24 types of emotion categories with Twitter data. Li et al. (2017) developed a multi-turn dialog dataset, DailyDialog, for detecting the emotions in ¨ the field of dial"
2021.acl-long.184,P14-1146,0,0.0489303,"st majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to learn emotionenriched word embedding (EWE). However, all the above algorithms represent emotions in semantic space rather than emotion space. As shown in Table 1, each emotion category represented in semantic space reflect a piece of semantic information rather than a specific emotional state. In this work, we regard each emotion category as a specific emotional state in emotion space and represent each emotion category"
2021.acl-long.184,P10-1040,0,0.13051,", 2019) and many datasets have been introduced to train and evaluate the corresponding models (Ghazi et al., 2015; Mohammad et al., 2018; Liu et al., 2019). The vast majority of existing emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to learn emotionenriched word embedding (EWE). However, all the above algorithms represent emotions in semantic space rather than emotion space. As shown in Table 1, each emotion category represented in semantic space reflect a piece of semantic information rather"
2021.acl-long.184,P19-1096,0,0.0194462,"the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space. 1 Introduction In the past decades, a lot of tasks have been proposed in the field of text emotion analysis. The most primary one among them is emotion classification task (Alm et al., 2005). Based on emotion classification task, many new tasks have been proposed from different considerations. Lee et al. (2010) proposed the task of emotion cause extraction, which aims at predicting the reason of a given emotion in a document. Based on the emotion cause extraction task, Xia and Ding (2019) introduced the emotion-cause pair extraction task for the purpose of extracting the potential pairs of emotions and corresponding causes in a document. Jiang et al. (2011) proposed a target-dependent emotion recognition task, which aims at predicting the sentiment with the given query. To express the intensity of ∗ Corresponding author. a specific emotion in text, Mohammad and BravoMarquez (2017) proposed the emotion intensity detection task. However, all the above tasks treat emotions as independent ones and represent emotions with one-hot vectors, which definitely ignore the underlying emot"
2021.acl-long.184,W18-6243,0,0.061986,"tation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to learn emotionenriched word embedding (EWE). However, all the above algorithms represent emotions in semantic space rather than emotion space. As shown in Table 1, each emotion category represented in semantic space reflect a piece of semantic information rather than a specific emotional state. In this work, we regard each emotion category as a specific emotional state in emotion space and represent each emotion category with a point in emotion 2364 Proce"
2021.acl-long.184,D17-1056,0,0.150307,"ting emotion annotation work assumes that the emotions are orthogonal to each other and represent the emotion categories with one-hot vectors (Mohammad, 2012; Gui et al., 2016; Klinger et al., 2018). Actually, the boundaries as well as the relations among emotion categories are not clearly distinguished and defined. Typical word embedding learning algorithms only use the contexts but ignore the sentiment of texts (Turian et al., 2010; Mikolov et al., 2013). To encode emotional information into word embedding, sentiment embedding and emotion(al) embedding have been proposed (Tang et al., 2014; Yu et al., 2017; Xu et al., 2018). Tang et al. (2015) proposed a learning algorithm dubbed sentimentspecific word embedding (SSWE). Agrawal et al. (2018) proposed a method to learn emotionenriched word embedding (EWE). However, all the above algorithms represent emotions in semantic space rather than emotion space. As shown in Table 1, each emotion category represented in semantic space reflect a piece of semantic information rather than a specific emotional state. In this work, we regard each emotion category as a specific emotional state in emotion space and represent each emotion category with a point in"
2021.acl-long.184,2020.coling-main.248,0,0.0202221,"assifier using the soft targets output by trained classifier as target values than using manual ground-truth labels. Phuong and Lampert (2019) provided their insights into the working mechanisms of distillation by studying the special case of linear and deep linear classifiers. Szegedy et al. (2016) proposed a label smoothing mechanism for the purpose of encouraging the model to be less confident by smoothing the initial one-hot labels. Imani and White (2018) investigated the reasons for the improvement of the model performance by converting hard targets to soft labels in supervised learning. Zhao et al. (2020) proposed a robust training method for machine reading comprehension by learning soft labels. In this work, soft labels output by the trained neural network model are used to generate distributed representations for emotion categories. 3 Methodology simple and effective algorithm is derived based on the soft labels from a pre-trained neural network model. After that, we extend our method to multilabel datasets. At last, detailed approaches of the algorithm are listed. 3.1 The General Framework As shown in Table 2, the four instances from dataset SemEval-2007 task 14 (Strapparava and Mihalcea,"
2021.emnlp-main.365,2020.lrec-1.58,0,0.0615961,"Missing"
2021.emnlp-main.365,P18-1063,0,0.0180648,"order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based seq2seq model using source word copy mechanism and attention coverage mechanism. Fast-RL (Chen and Bansal, 2018): This method ﬁrst extracts important sentences and then compresses them into summaries. The whole model is at last jointly trained by reinforcement learning. BERTAbs (Liu and Lapata, 2019): It uses pretrained BERT as the encoder and a transformerbased network as the decoder to summarize. TDS+SATM (Zou et al., 2021b): It is similar to Fast-RL but uses BERT and transformer structure as the extractive model and abstractive model. Besides, it also introduces a topic model to enhance summary generation. For all abstractive methods containing the extractive process, such as Fast-RL and TDS+SATM, we"
2021.emnlp-main.365,W04-3247,0,0.126744,"ng to generate different kinds of summaries separately and relax the structural requirements2 . 4.2 Summarization Models In this section, we will introduce some widely-used extractive and abstractive summarization models on dialogue summarization. We also enhance some of the models using our special annotations. The extractive methods include: LONGEST: As the longer utterances in the dialogue may contain more useful information, we sort the utterances by their lengths and extract the top k longest utterances as the summary. Number k is decided by the maximum summary length limit. LexPageRank (Erkan and Radev, 2004): This method ranks dialogue utterances by PageRank algorithm and extracts utterances in order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based se"
2021.emnlp-main.365,D19-5409,0,0.0853486,"rmance beacquire the user’s problems and previous service comes much worse when analyzing the perforprogress, ﬁguring out the solved and unsolved probmance on role-oriented summaries and topic lems. Besides, role-oriented and structural sumstructures. We hope that this study could benchmark Chinese dialogue summarization maries are also valuable for other dialogue domains and beneﬁt further studies. such as debating and court trials. Although several dialogue summarization 1 Introduction datasets have been proposed recently (McCowan Text summarization aims to compress a long in- et al., 2005; Gliwa et al., 2019; Zou et al., 2021a,b), put text and generate a condensed summary (Zong none of them adds dialogue features (i.e., different et al., 2021). It can help people capture the gist of speakers’ roles or topic structure) in summaries, a long document quickly. Traditional summariza- limiting the application of these datasets. Theretion tasks mainly focus on news reports (Nallapati fore, we aim to construct a ﬁne-grained Chinese et al., 2016; Narayan et al., 2018; Zhu et al., 2018). dataset for Customer Service domain Dialogue However, as the communication tools become con- Summarization (CSDS). venie"
2021.emnlp-main.365,2020.inlg-1.23,0,0.0243619,"Missing"
2021.emnlp-main.365,2020.findings-emnlp.335,0,0.0200117,"herent summary. First, we present automatic evaluation metric results of different models in Table 3. In general, we observe that abstractive methods perform better than extractive methods with a large margin. Among extractive methods, SummaRunner achieves the best results, indicating the effectiveness of supervised utterance index labels. As for abstractive methods, Fast-RL and Fast-RL* perform best on almost all metrics except for ROUGEL of the overall summary, where the PGN method obtains a better result. Transformer-based methods perform worse mainly because of relatively small data size (Joshi et al., 2020). It is worth noticing that enhanced methods (Fast-RL*, TDS+SATM*) are usually better than their original version on the 6 Dataset Difﬁculties overall summary and the user summary. This highlights the effect of the key utterance indexes even In this section, we want to analyze the difﬁculties just used as supervised signals, as it can reﬂect of CSDS further. According to the ﬁne-grained which utterance is more critical for summarization. features in CSDS and the challenges mentioned in By comparing with the same model in different Section 3.6, we raise the following two questions. tasks, we ﬁn"
2021.emnlp-main.365,N18-1149,0,0.0639934,"Missing"
2021.emnlp-main.365,W02-0406,0,0.315851,"gher quality. We will leave it to future work. as supervised signals only in the training process. We name them as Fast-RL* and TDS+SATM*. Besides, all extractive methods are restricted to generate summaries less than a limited length, which is 84 for the overall summary, 38 for the user summary, and 49 for the agent summary. They are set according to the average length of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level."
2021.emnlp-main.365,K16-1028,0,0.0794084,"Missing"
2021.emnlp-main.365,D18-1206,0,0.130882,"alogue summarization 1 Introduction datasets have been proposed recently (McCowan Text summarization aims to compress a long in- et al., 2005; Gliwa et al., 2019; Zou et al., 2021a,b), put text and generate a condensed summary (Zong none of them adds dialogue features (i.e., different et al., 2021). It can help people capture the gist of speakers’ roles or topic structure) in summaries, a long document quickly. Traditional summariza- limiting the application of these datasets. Theretion tasks mainly focus on news reports (Nallapati fore, we aim to construct a ﬁne-grained Chinese et al., 2016; Narayan et al., 2018; Zhu et al., 2018). dataset for Customer Service domain Dialogue However, as the communication tools become con- Summarization (CSDS). venient, enormous information is presented in a To achieve this goal, we employ Questionconversational format, such as meeting records, Answer (QA) pairs as the annotation format since ∗ Corresponding author. it is the basic granularity in a customer service di4436 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4436–4451 c November 7–11, 2021. 2021 Association for Computational Linguistics Fine-grained Annotation"
2021.emnlp-main.365,P02-1040,0,0.110507,"xtractive methods are restricted to generate summaries less than a limited length, which is 84 for the overall summary, 38 for the user summary, and 49 for the agent summary. They are set according to the average length of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level. First, we split the ground truth summaries and the summaries generated by models into different topic segments4 . Then we evaluate the summary quality for"
2021.emnlp-main.365,2020.acl-main.459,0,0.0295048,"apers (Kang et al., 2018). Different from document summarization, dialogue summarization aims to summarize a conversation into a narrative text. McCowan et al. (2005); Janin et al. (2003) provide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an ove"
2021.emnlp-main.365,P17-1099,0,0.0371487,"exPageRank (Erkan and Radev, 2004): This method ranks dialogue utterances by PageRank algorithm and extracts utterances in order until the length of the summary reaches the limit. SummaRuNNer (Nallapati et al., 2017): A supervised extractive summarization method by scoring each utterance using RNN. Here, we use the key utterance indexes as extractive labels. BERTExt (Liu and Lapata, 2019): This method scores each utterance in dialogue by ﬁnetuning on the pretrained BERT (Devlin et al., 2019) model. Extractive labels are the same as SummaRuNNer. We also implement some abstractive methods: PGN (See et al., 2017): An RNN-based seq2seq model using source word copy mechanism and attention coverage mechanism. Fast-RL (Chen and Bansal, 2018): This method ﬁrst extracts important sentences and then compresses them into summaries. The whole model is at last jointly trained by reinforcement learning. BERTAbs (Liu and Lapata, 2019): It uses pretrained BERT as the encoder and a transformerbased network as the decoder to summarize. TDS+SATM (Zou et al., 2021b): It is similar to Fast-RL but uses BERT and transformer structure as the extractive model and abstractive model. Besides, it also introduces a topic model"
2021.emnlp-main.365,2020.coling-main.63,0,0.0366529,"ovide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an overall summary from the agent’s perspective for each dialogue. Besides, their publicly available data are difﬁcult to analyze since all the sentences are given by word indexes. On t"
2021.emnlp-main.365,D19-1053,0,0.0157651,"gth of reference summaries. More experimental settings are given in Appendix E. 4.3 Evaluation Metrics We employ ﬁve widely used automatic metrics to evaluate the above methods. The automatic metrics3 include: ROUGE-based methods (Lin and Hovy, 2002): Widely used metrics by measuring the overlap of n-grams between two texts. Here we choose ROUGE-2 and ROUGE-L for comparison. BLEU (Papineni et al., 2002): Another n-gram overlap metric by considering up to 4-grams. BERTScore (Zhang et al., 2020): It measures the word overlap between two texts according to contextual BERT embeddings. MoverScore (Zhao et al., 2019): It measures the semantic distance between two texts according to pretrained embeddings. Here we use BERT embedding as well. As for human evaluation metrics, we try to evaluate the quality of summaries at a ﬁne-grained topic level. First, we split the ground truth summaries and the summaries generated by models into different topic segments4 . Then we evaluate the summary quality for each segment in the following three aspects: informativeness, non-redundancy, ﬂuency5 . These three aspects are frequently used in the summarization community (Zhu et al., 2019; Fabbri et al., 2021), and we also"
2021.emnlp-main.365,2021.naacl-main.474,0,0.0411465,"marization, dialogue summarization aims to summarize a conversation into a narrative text. McCowan et al. (2005); Janin et al. (2003) provide two datasets for dialogue summarization at the earliest, and the task of their data is to summarize a meeting transcript into a few short sentences. Zhong et al. (2021) incorporate them and propose a query-based summarization task. Gliwa et al. (2019) propose an English daily conversation summarization dataset on ﬁctitious dialogues, providing a new daily chatting scenario. Other datasets provided by Rameshkumar and Bailey (2020), Duan et al. (2019) and Zhu et al. (2021) also show the potential of dialogue summarization in other scenarios. As for Chinese dialogue summarization datasets, Song et al. (2020) construct a medical dialogue summarization dataset, where most of the summaries are extractive and relatively easy to generate. Almost all the above datasets only provide an overall summary for each dialogue without further annotations. In the customer service domain, Zou et al. (2021b) provide a related dialogue summarization dataset, which is the most similar to our work. However, their dataset only contains an overall summary from the agent’s perspective"
2021.findings-emnlp.92,W18-6402,0,0.0314119,"Missing"
2021.findings-emnlp.92,W17-4746,0,0.0426111,"Missing"
2021.findings-emnlp.92,P17-1175,0,0.0347155,"Missing"
2021.findings-emnlp.92,P19-1642,0,0.158365,"ti30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5 (0.5) 41.2 (6.3) 44.8 (0.2) word Test2016 EMMTSR EMMTSS EMMTT 37.8 (0.2) 38.0 (0.5) 36.3 (0.5) 56.1 (0.2) 56.2 (0.2) 55.0 (0.1) 30.1 (0.7) 30.3 (0.5) 28.4 (0.1) 50.3 (0.1) 50.1 (0.1) 48.6 (0.2) 27.0 (0.1) 26.1 (0.7) 25.3 (0.1) 46.4 (0.2) 45.6 (0.7) 44.3 (0.4) phrase RNN-based EMMTSR EMMTSS EMMTT 38.0 (0.1) 37.8 (0.1) 36.8 (0.1) 56.5 (0.3) 56.1 (0.2) 55.0 (0.4) 30.2 (0.8) 30.5 (0.5) 29.4 (0.2) 50.3 (0.4) 50.1"
2021.findings-emnlp.92,D17-1095,0,0.0386098,"Missing"
2021.findings-emnlp.92,W14-3348,0,0.0125568,"ms. RNN-based models: Other Settings We train our models by randomly selecting from the translation task and the reconstruction task. The parameter w is the probability of updating the translation model in the current minibatch. It is set according to the ratio of the amount of data used in the translation task and the reconstruction task. For the Multi30K dataset, we set 0.5 to keep the balance between two tasks. we report mean and standard deviation over 3 independent runs for all models. Finally, we evaluate translation quality using the metrics of BLEU4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). 1071 • NMT: It is the text-only RNN-based attentional NMT system (Luong et al., 2015) with default setting. • pRCNNs (Huang et al., 2016): Visual objects are respectively encoded with the source sentence. In the decoding phase, the decoder chooses to attend mostly to the relevant words in the sequence encoded with the relevant visual object. • DATT (Calixto et al., 2017): It is an NMT model with a doubly attentive decoder. One of the decoders attends to the relevant region of the image to help to predict a word. • Imagination (Elliott and Kádár, 2017): It is an NMT model with an auxiliary ta"
2021.findings-emnlp.92,W17-4718,0,0.0164199,"rmore, our in-depth analysis shows how vilearning approach for multi-modal machine translasual information improves translation. tion (EMMT). Different from sentence-level crossmodal semantics fusion approaches, our approach 1 Introduction aims to augment the entity representation explicitly. Multi-modal machine translation (MMT) aims at We frame the entity-level cross-modal learning apimproving the translation performance with the proach as a reconstruction task that reconstructs help of visual information such as image (Specia the original textual sentence from a degraded multiet al., 2016; Elliott et al., 2017; Barrault et al., 2018; modal input (Lewis et al., 2020). The multi-modal Zhang et al., 2020). The assumption behind this input is a mixture of a degraded sentence and reis that images consist of relatively complete infor- lated visual objects. The degraded sentence is mation compared with textual description and can generated by erasing the visually depictable entity provide complementary knowledge to guide trans- words as done by (Caglayan et al., 2019) and filling lation (Elliott et al., 2016). the erased position with corresponding visual obPrevious studies mainly focus on integrating the"
2021.findings-emnlp.92,W16-3210,0,0.0564879,"Missing"
2021.findings-emnlp.92,I17-1014,0,0.333768,"the encoding stage and the linguistic feature space in the decoding stage. The reconstruction model is trained to minimise the negative log-likelihood function: 2.3 Multi-task Framework As illustrated in Figure 2, the architecture of the reconstruction model is basically the same as the translation model. The objective function of translation model is also similar to LR (θ, ψ): LT (θ, ϕ) = − N X log p(yi |y&lt;i , X) (3) i where ϕ is the decoder parameters of the translation model. To combine the reconstruction task with the translation task, we mix their objective function with the parameter w (Elliott and Kádár, 2017): L(θ, ϕ, ψ) = wLT (θ, ϕ) + (1 − w)LR (θ, ψ) (4) LR (θ, ψ) = − N X log p(xi |x&lt;i , XM M ) (1) i where XM M is XM M w or XM M p , θ is the parameters of the shared encoder, and ψ is the parameters of the reconstruction decoder. We also consider reconstructing the target language text Y = {y0 , y1 , . . . , yM }. As shown in Figure 2, the black dash line points to decoder of the translation model. To reconstruct the target text Y , we modify the reconstruction objective function to: LR (θ, ψ) = − M X j log p(yj |y&lt;j , XM M ) where w is the probability of updating translation model parameters in"
2021.findings-emnlp.92,W16-2360,0,0.130593,"to 0. The RNN-based models are trained with the tively. We also evaluate our model in the Multi30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5 (0.5) 41.2 (6.3) 44.8 (0.2) word Test2016 EMMTSR EMMTSS EMMTT 37.8 (0.2) 38.0 (0.5) 36.3 (0.5) 56.1 (0.2) 56.2 (0.2) 55.0 (0.1) 30.1 (0.7) 30.3 (0.5) 28.4 (0.1) 50.3 (0.1) 50.1 (0.1) 48.6 (0.2) 27.0 (0.1) 26.1 (0.7) 25.3 (0.1) 46.4 (0.2) 45.6 (0.7) 44.3 (0.4) phrase RNN-based EMMTSR EMMTSS EMMTT 38.0 (0.1) 37.8 (0.1"
2021.findings-emnlp.92,P19-1653,0,0.413649,"liott and Kádár, ies leverage the visual information through in2017; Zhou et al., 2018). Some works leverage the tegrating the global image features as auxiliary spatial information in the decoding stage by attendinput or decoding by attending to relevant loing to relevant local regions of the image (Calixto cal regions of the image. However, this kind of usage of visual information makes it difet al., 2017; Caglayan et al., 2017, 2018; Libovický ficult to figure out how the visual modality and Helcl, 2017; Libovický et al., 2018; Yao and helps and why it works. Inspired by the findWan, 2020; Ive et al., 2019). ings of (Caglayan et al., 2019) that entities are However, these sentence-level approaches which most informative in the image, we propose an implicitly incorporate image features make it exexplicit entity-level cross-modal learning aptremely difficult to figure out how visual features proach that aims to augment the entity representation. Specifically, the approach is framed affect the representation of source-side sentences as a reconstruction task that reconstructs the or the decision when generating a target-side word. original textural input from multi-modal input Furthermore, results f"
2021.findings-emnlp.92,N13-1073,0,0.0213821,"xpected to obtain more increment for the entity words. It is represented Table 4: Adversarial evaluation and ablation study reas lowering the increment difference besults on Multi30K 2016 test set. The best results are bold, and the worst are underlined. tween the other words and the entity words. The measurement is based on the sentence-level As pointed out by previous studies that noise is translation results of various MMT models. To the major part of visual features in the image-to- get the word-level translation, we employ the fasttext task. It is necessary to find out whether our align (Dyer et al., 2013) toolkit which aligns tokens model can eliminate noise and learn useful infor- from source-side to target-side and concatenates mation from visual features. We suppose that our the training set and the test set to train better alignmodels benefit from the visual object information, ments. The aligned target-side words are considthe multi-task scheme, and the de-noising ability. ered to be the translation of the source-side words. To investigate the effectiveness of these compo- We take the alignment results of reference paralnents, we conduct several experiments to compare lel data as the corr"
2021.findings-emnlp.92,2020.acl-main.703,0,0.023739,"for multi-modal machine translasual information improves translation. tion (EMMT). Different from sentence-level crossmodal semantics fusion approaches, our approach 1 Introduction aims to augment the entity representation explicitly. Multi-modal machine translation (MMT) aims at We frame the entity-level cross-modal learning apimproving the translation performance with the proach as a reconstruction task that reconstructs help of visual information such as image (Specia the original textual sentence from a degraded multiet al., 2016; Elliott et al., 2017; Barrault et al., 2018; modal input (Lewis et al., 2020). The multi-modal Zhang et al., 2020). The assumption behind this input is a mixture of a degraded sentence and reis that images consist of relatively complete infor- lated visual objects. The degraded sentence is mation compared with textual description and can generated by erasing the visually depictable entity provide complementary knowledge to guide trans- words as done by (Caglayan et al., 2019) and filling lation (Elliott et al., 2016). the erased position with corresponding visual obPrevious studies mainly focus on integrating the jects. Reconstructed from this kind of input, entity vis"
2021.findings-emnlp.92,D18-1329,0,0.0689474,"s of (Caglayan et al., 2019) that entities are However, these sentence-level approaches which most informative in the image, we propose an implicitly incorporate image features make it exexplicit entity-level cross-modal learning aptremely difficult to figure out how visual features proach that aims to augment the entity representation. Specifically, the approach is framed affect the representation of source-side sentences as a reconstruction task that reconstructs the or the decision when generating a target-side word. original textural input from multi-modal input Furthermore, results from (Elliott, 2018) have in which entities are replaced with visual feashown that visual information maybe not the reatures. Then, a multi-task framework is emson why MMT models were promoted, and it is ployed to combine the translation task and the observed that irrelevant images can improve transreconstruction task to make full use of crosslation unexpectedly. modal entity representation learning. The extensive experiments demonstrate that our apInspired by the findings of (Caglayan et al., proach can achieve comparable or even better 2019) that entities are most informative in the performance than state-of-th"
2021.findings-emnlp.92,P17-2031,0,0.0469028,"Missing"
2021.findings-emnlp.92,W18-6326,0,0.0306031,"Missing"
2021.findings-emnlp.92,J82-2005,0,0.650276,"Missing"
2021.findings-emnlp.92,D15-1166,0,0.0666437,"ation task and the reconstruction task. The parameter w is the probability of updating the translation model in the current minibatch. It is set according to the ratio of the amount of data used in the translation task and the reconstruction task. For the Multi30K dataset, we set 0.5 to keep the balance between two tasks. we report mean and standard deviation over 3 independent runs for all models. Finally, we evaluate translation quality using the metrics of BLEU4 (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). 1071 • NMT: It is the text-only RNN-based attentional NMT system (Luong et al., 2015) with default setting. • pRCNNs (Huang et al., 2016): Visual objects are respectively encoded with the source sentence. In the decoding phase, the decoder chooses to attend mostly to the relevant words in the sequence encoded with the relevant visual object. • DATT (Calixto et al., 2017): It is an NMT model with a doubly attentive decoder. One of the decoders attends to the relevant region of the image to help to predict a word. • Imagination (Elliott and Kádár, 2017): It is an NMT model with an auxiliary task that imagines the image from the source sentence description. • VMMT (Calixto et al."
2021.findings-emnlp.92,P02-1040,0,0.114208,"one trans- the decoder, and the attention layer. All model parameters are initialized sampling from a uniform lated German description. Multi30k was split into distribution u(−0.1, +0.1) and bias vectors are set three parts: training, validation, and test, containing 29,000, 1,014, and 1,000 pairs of sentences respec- to 0. The RNN-based models are trained with the tively. We also evaluate our model in the Multi30k Adam optimizer with an initial learning rate of 0.002. We set the minibatch size to 40. Models are 1 In our experiments, we use “hen_sosi ” as the language selected based on BLEU4 (Papineni et al., 2002) identification token for English reconstruction decoding and “hde_sosi ” for German translation decoding. results of the translation task on the validation data. 1070 Test2017 MSCOCO BLEU METEOR BLEU METEOR BLEU METEOR NMT 35.9 (0.1) 54.9 (0.1) 28.8 (0.6) 49.5 (0.2) 25.9 (1.0) 45.7 (0.7) pRCNNs (Huang et al., 2016) DATT (Calixto et al., 2017) Imagination (Elliott and Kádár, 2017) VMMTC (Calixto et al., 2019) VMMTF (Calixto et al., 2019) 36.5 (0.8) 36.5 36.8 (0.8) 37.5 (0.3) 37.7 (0.4) 54.1 (0.7) 55.0 55.8 (0.4) 55.7 (0.1) 56.0 (0.1) 26.1 (6.6) 30.0 (0.3) 45.4 (7.3) 49.9 (0.3) 21.8 (5.6) 25.5"
2021.findings-emnlp.92,W16-2346,0,0.0462908,"Missing"
2021.findings-emnlp.92,D18-1011,1,0.816307,"focused on fusing the multi-modal information into the sentence-level semantics (Huang et al., 2016; Calixto and Liu, 2017; Calixto et al., 2017; Libovický and Helcl, 2017; Delbrouck and Dupont, 2017) in the RNN-based architecture (Bahdanau et al., 2015). Besides above approaches, Toyama et al. (2016); Calixto et al. (2019) proposed to apply latent variables as the unified semantic representations. Ive et al. (2019) proposed a translate-and-refine approach to generate a good translation from the first draft by making better use of the target language and visual context. There are also works (Wang et al., 2018a,b; Zhao et al., 2020) show that extra modality information Results of Our Multi-task Models As shown in Figure 3, neither adversarial models (“ro” is useful in a more fine-grained way. Recently, Yin et al. (2020) proposed a fineand “rw”) nor MLM models get stable lower grained method that employs a graph-based multidifferences. No evidence was found that the modal fusion encoder to fuse image and source text de-noising ability or the MLM of our multi-task scheme was a guarantee for helping the transla- in the entity level. The input sentence and image are represented as a unified graph. The"
2021.findings-emnlp.92,2020.acl-main.273,0,0.19187,"port the mean and the standard deviation over 3 independent runs. Best overall results are bold. The training procedure is halted if the model does not improve BLEU4 scores on the validation set for 10 epochs. We translate test data on the last saved model. Transformer-based Model For Transformerbased models, we set it up with a 128D word embedding layer and 256D hidden size. The embedding layer is shared between source and target vocabularies. Both the encoder and the decoder have Ld = 4 layers, and the number of heads is 4. We set the dropout to 0.2 which gets a similar baseline model with (Yin et al., 2020). Adam optimizer is applied in the same way with the original transformer model (Vaswani et al., 2017). Each training batch contained 2,000 source tokens and corresponding target sentences and images. The training was halted after 80,000 steps. All above Transformer-based settings are basically the same as the set up in the publication of (Yin et al., 2020) which we will compare with. 4 Experimental Results 4.1 Baselines We compare the proposed models against the following MMT systems. RNN-based models: Other Settings We train our models by randomly selecting from the translation task and the"
2021.findings-emnlp.92,Q14-1006,0,0.14725,"Missing"
C00-2174,W96-0411,0,\N,Missing
C02-2028,J87-1004,0,\N,Missing
C08-1125,J93-2003,0,0.0149626,"main monolingual corpora to improve the indomain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-ofdomain corpora. 1 Introduction In statistical machine translation (SMT), the translation process is modeled to obtain the translation e best of the source sentence f by maximizing the following posterior probability (Brown et al., 1993). e best = arg max e p (e |f ) = arg max e p (f |e ) pLM (e ) (1) State-of-the-art SMT systems are trained on large collections of bilingual corpora for the C 2008. Licensed under the Creative Commons Attri○ bution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. Chengqing Zong NLPR, Institute of Automation Chinese Academy of Sciences Beijing 100080, China cqzong@nlpr.ia.ac.cn translation model p (f |e ) and monolingual target language corpora for the language model (LM) pLM (e ) . The trained SMT systems are suitable for"
C08-1125,W07-0718,0,0.0184073,"Missing"
C08-1125,W07-0722,0,0.0694584,"adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated s"
C08-1125,W07-0717,0,0.0622145,"n studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the"
C08-1125,W06-3114,0,0.015664,"Missing"
C08-1125,P07-2045,0,0.00675131,"rmance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In this paper, we use an in-domain translation dictionary and/or in-domain monolingual corpora (in both source language and target language) to improve the performance of a SMT system trained on the out-of-domain corpora. Thus, our method uses these resources, instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distorti"
C08-1125,W07-0733,0,0.126911,"o et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test se"
C08-1125,J05-4003,0,0.0482352,"s indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain bilingual corpus, Munteanu and Marcu (2005) automatically extracted in-domain bilingual sentence pairs from comparable corpora. Adding the extracted bilingual corpus to the training data improved the performance of the MT system. In addition, Ueffing et al. (2007) explored transductive learning for SMT, where source language corpora are used to train the models. They repeatedly translated source sentences from the development set and test set. Then the generated translations are used to improve the performance of the SMT system. This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts. In"
C08-1125,P03-1021,0,0.00513523,"instead of an indomain bilingual corpus, to adapt a baseline system trained on the out-of-domain corpora to indomain texts. 3 Baseline MT System The phrase-based SMT system used in our experiments is Moses (Koehn et al., 2007). In Moses, phrase translation probabilities, reordering probabilities, and language model probabilities are combined in the log-linear model to obtain the best translation e best of the source sentence f : e best = arg max e p (e |f ) ≈ arg max e M ∑ λm hm (e, f) (2) m =1 The weights are set by a discriminative training method using a held-out data set as described in (Och, 2003). The models or features which are employed by the decoder are (a) one or several phrases tables, (b) one or more language models trained with SRILM toolkit (Stolcke, 2002), (c) distance-based and lexicalized distortion models, (d) word penalty, (e) phrase penalty. 994 Input Out-of-domain training data LO In-domain translation dictionary DI In-domain target language corpus TI (optional) In-domain source language corpus S I (optional) Begin Assign translation probabilities to D I If TI is available Training step: π = Estimate ( LO , DI , TI ) , where π represents the general model. Else Trainin"
C08-1125,P02-1040,0,0.105146,"ing indomain dictionary and monolingual corpora. And then we present the experimental results in sections 5. In the last section, we conclude this paper. 2 Related Work Translation model and language model adaptation are usually used in domain adaptation for SMT. Language model adaptation has been widely used in speech recognition (Bacchiani and Roark, 2003). In recent years, language model adaptation has also been studied for SMT (Bulyko et al., 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score (Papineni et al., 2002). Their experiments indicated about 0.4 BLEU score improvement. A shared task is organized as part of the Second Workshop on Statistical Machine Translation. A part of this shared task focused on domain adaptation for machine translation among European languages. Several studies investigated mixture model adaptation for both translation model and language model in SMT (Civera and Juan, 2007; Foster and Kuhn, 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT. Their experiments indicate an absolute improvement of more than 1 BLEU score. To enlarge the in-domain"
C08-1125,P07-1004,0,0.228266,"y. Moreover, if an indomain source language corpus (SLC) is available, we automatically translate it and obtain a synthetic in-domain bilingual corpus. By adding this synthetic bilingual corpus to the training data, we rebuild the translation model to improve 993 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 993–1000 Manchester, August 2008 translation quality. We can repeatedly translate the in-domain source language corpus with the improved model until no more improvement can be made. This is similar to transductive learning described in (Ueffing et al., 2007). We perform domain adaptation experiments on two tasks: one is the Chinese to English translation, using the test set released by the International Workshop on Spoken Language Translation 2006 (IWSLT 2006), and the other is the English to French translation, using the data released by the Second Workshop on Statistical Machine Translation (WMT 2007) (CallisonBurch et al., 2007). Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baseline"
C08-1125,2007.mtsummit-papers.67,1,0.744835,"8.16 BLEU score (Model 7 vs. Model 1). Comparison of Different Dictionaries We compare the effects of different dictionaries with concern to the translation quality. Besides the manually-made in-domain dictionary, we use other two dictionaries: the LDC dictionary and an automatically built dictionary, which is extracted from the BTEC corpus. This extracted dictionary only contains Chinese words and their translations. The extraction method is as follows: • • Build a phrase table with the in-domain bilingual corpus. Filter those phrase pairs whose values are below a threshold as described in (Wu and Wang, 2007). • • From the filtered phrase table, extract the Chinese words and their translations. Assign constant translation probabilities to the entries of the extracted dictionary. Table 6 shows the translation results. All of the methods use the out-of-domain corpus, the in-domain target language corpus, and the corresponding translation dictionaries with constant translation probabilities. The results indicate that using the general-domain dictionary also improves translation quality, achieving an improvement of about 2 BLEU score as compared with Model 2 in Table 5. It can also be seen that the in"
C08-1125,H93-1040,0,\N,Missing
C08-1125,1993.mtsummit-1.24,0,\N,Missing
C08-1137,C04-1073,0,0.0585709,"n et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2"
C08-1137,W06-1609,0,0.0665215,"ld be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,D07-1077,0,0.435576,"osed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P07-1091,0,0.0130776,"l was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The rewriting patterns are employed on the input source sentence to make the word order more accordant to target language. Collins et al. (2005) described an approach to reorder German in German-to-English translation. The method concentrates on the German clauses and six types of transforming rules are applied to the parsed source sentence. However, all the rules are manually built. Li et al. (2007) used a parser to get the syntactic t"
C08-1137,P03-1054,0,0.00678349,"Missing"
C08-1137,P05-1033,0,0.020392,"sting reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatic"
C08-1137,P06-1066,0,0.107323,"pproaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord (2004), Collins et al. (2005), Li et al. (2007) and Wang et al. (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewri"
C08-1137,J04-4002,0,0.0732557,"Missing"
C08-1137,N03-1017,0,0.00581239,"model. However, reordering is always a key issue in the decoding process. A number of models have been developed to deal with the problem of reordering. The existing reordering approaches could be divided into two categories: one is integrated into the decoder and the other is employed as a preprocessing module. © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Many reordering methods belong to the former category. Distortion model was first employed by Koehn et al. (2003); a lexicalized reordering model was proposed by Och et al. (2004) and Koehn et al. (2005); and the formal syntax-based reordering models were proposed by Chiang (2005) and Xiong et al. (2006). It is worthy to note that little syntactic knowledge is used in the models mentioned above. Compared to the reordering models that are integrated into the decoder, the reordering at the source side can utilize more syntactic knowledge, with the goal of adjusting the source language sentence to make its word order closer to that of the target language. The most notable models are given by Xia and McCord"
C08-1137,W07-0401,0,0.0694085,"Missing"
C08-1137,P02-1038,0,\N,Missing
C08-1137,P05-1066,0,\N,Missing
C10-1051,P05-1033,0,0.663214,"ley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xiong et al., 2006). A hierarchical phrase-based translation model (HPTM) reorganizes phrases into hierarchical ones by reducing sub-phrases to variables (Chiang 2005). Xiong et al. (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). Compared with contiguous phrasebased reordering model, Syntax-based models need to shoulder a great deal of rules and have high computational cost of time and space. The type of reordering models has a weaker ability of processing long sentences and large-scale data, which heavily restrict their application."
C10-1051,J07-2003,0,0.0763991,"g model have no segmentation tag. Because any segmentation for the input before decoding will influence the use of some rules or phrase pairs and may cause some rules or phrase pairs losses. It would be better to employ different phrase table to limit reordering models and let each decoder automatically decide reordering model for each segments of the input. Thus by controlling the phrase tables we apply different reordering models on different phrases. For each reordering model we perform the maximum BLEU training (Venugopal et al. 2005) on a development set. For HPTM the training is same as Chiang 2007. For MEBTG we use chunk phrase table and base table to obtain translation parameters. For monotone reordering model all the three phrase tables are merged to get translation weights. 4 Experiments This section gives the experiments with Chineseto-English translation task in news domain. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002). We use NIST MT 2005, NIST MT 2006 and NIST MT 2008 as our test data. Our training data is filtered from the LDC corpus4. Table 1 gives the statistics of our data. 4.1 Evaluating translation Performance We compare our PRML against two base"
C10-1051,P05-1067,0,0.0886288,"ontent-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phrase movement. Linguistically syntactic approaches (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xiong et al., 2006). A hierarchical phrase-based translation model (HPTM) reorganizes phrases into hierarchical ones by reduci"
C10-1051,N04-1035,0,0.0482446,"ith probabilities for each bilingual phrase from training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phrase movement. Linguistically syntactic approaches (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xi"
C10-1051,P06-1121,0,0.0388155,"Missing"
C10-1051,W05-1506,0,0.0441465,"Missing"
C10-1051,P02-1040,0,0.0809895,"the input. Thus by controlling the phrase tables we apply different reordering models on different phrases. For each reordering model we perform the maximum BLEU training (Venugopal et al. 2005) on a development set. For HPTM the training is same as Chiang 2007. For MEBTG we use chunk phrase table and base table to obtain translation parameters. For monotone reordering model all the three phrase tables are merged to get translation weights. 4 Experiments This section gives the experiments with Chineseto-English translation task in news domain. Our evaluation metric is case-insensitive BLEU-4 (Papineni et al. 2002). We use NIST MT 2005, NIST MT 2006 and NIST MT 2008 as our test data. Our training data is filtered from the LDC corpus4. Table 1 gives the statistics of our data. 4.1 Evaluating translation Performance We compare our PRML against two baselines: MEBTG system developed in house according to Xiong (2006, 2008) and HPTM system 5 in PYTHON based on HPTM reordering model (Chiang 2007). In MEBTG phrases of up to 10 words in length on the Chinese side are extracted and reordering examples are obtained without limiting the length of each example. Only the last word of each reordering example is used"
C10-1051,H05-1021,0,0.022202,"ments of phrase don’t involve insertions inside the other phrases. Some of these models are contentindependent, such as distortion models (Och and Ney, 2004; Koehn et al., 2003) which penalize translation according to jump distance of phrases, and flat reordering model (Wu, 1995; Zens et al., 2004)which assigns constant probabilities for monotone order and non-monotone order. These reordering models are simple and the contents of phrases have not been considered. So it’s hard to obtain a satisfactory translation performance. Some lexicalized reordering models (Och et al., 2004; Tillmann 2004, Kumar and Byrne, 2005, Koehn et al., 2005) learn local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phra"
C10-1051,P06-1077,0,0.0778138,"m training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phrase movement. Linguistically syntactic approaches (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xiong et al., 2006). A hierarchical phrase-bas"
C10-1051,W02-1018,0,0.0611198,"Missing"
C10-1051,W06-1606,0,0.0232225,"bilingual phrase from training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phrase movement. Linguistically syntactic approaches (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xiong et al., 2006). A hiera"
C10-1051,N04-4026,0,0.032072,"it and the movements of phrase don’t involve insertions inside the other phrases. Some of these models are contentindependent, such as distortion models (Och and Ney, 2004; Koehn et al., 2003) which penalize translation according to jump distance of phrases, and flat reordering model (Wu, 1995; Zens et al., 2004)which assigns constant probabilities for monotone order and non-monotone order. These reordering models are simple and the contents of phrases have not been considered. So it’s hard to obtain a satisfactory translation performance. Some lexicalized reordering models (Och et al., 2004; Tillmann 2004, Kumar and Byrne, 2005, Koehn et al., 2005) learn local orientations (monotone or non-monotone) with probabilities for each bilingual phrase from training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obta"
C10-1051,P03-1041,0,0.0607611,"Missing"
C10-1051,P06-1066,0,0.0966611,"04, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumptions (Chiang, 2005; Xiong et al., 2006). A hierarchical phrase-based translation model (HPTM) reorganizes phrases into hierarchical ones by reducing sub-phrases to variables (Chiang 2005). Xiong et al. (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). Compared with contiguous phrasebased reordering model, Syntax-based models need to shoulder a great deal of rules and have high computational cost of time and space. The type of reordering models has a weaker ability of processing long sentences and large-scale data, which heavily restrict their application. The above methods hav"
C10-1051,I08-1066,0,0.0297098,"Missing"
C10-1051,P01-1067,0,0.0418346,"notone or non-monotone) with probabilities for each bilingual phrase from training data. These models are phrase-dependent, so improvements over content-independent reordering models are obtained. However, many parameters need to be estimated. Non-contiguous phrase-based reordering models are proposed to process non-contiguous phrases and the movements of phrase involve insertion operations. This type of reordering models mainly includes all kinds of syntaxbased models where more structural information is employed to obtain a more flexible phrase movement. Linguistically syntactic approaches (Yamada and Knight, 2001; Galley et al., 2004, 2006; Marcu et al., 2006; Liu et al., 2006; Shieber et al., 1990; Eisner, 2003; Quirk et al., 2005; Ding and Palmer, 2005) employ linguistically syntactic information to enhance their reordering capability and use non-contiguous phrases to 447 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447–455, Beijing, August 2010 obtain some generalization. The formally syntax-based models use synchronous context-free grammar (SCFG) but induce a grammar from a parallel text without relying on any linguistic annotations or assumpti"
C10-1051,C04-1030,0,0.043118,"Missing"
C10-1051,C90-3045,0,\N,Missing
C10-1051,J04-4002,0,\N,Missing
C10-1051,2006.amta-papers.13,0,\N,Missing
C10-1051,P03-2041,0,\N,Missing
C10-1051,N03-1017,0,\N,Missing
C10-1051,P05-1034,0,\N,Missing
C10-1051,2005.iwslt-1.8,0,\N,Missing
C10-1051,W90-0102,0,\N,Missing
C10-1132,W02-1001,0,0.0630204,"Missing"
C10-1132,I05-3017,0,0.628415,"character-based models are much more robust on OOV words than word-based approaches do, as the vocabulary size of characters is a closed set (versus the open set of that of words). Furthermore, among those character-based approaches, the generative model and the discriminative one complement each other in handling in-vocabulary (IV) words and OOV words. Therefore, a characterbased joint model is proposed to combine them. This proposed joint approach has achieved good balance between IV word recognition and OOV word identification. The experiments of closed tests on the second SIGHAN Bakeoff (Emerson, 2005) show that the joint model significantly outperforms the baseline models of both generative and discriminative approaches. Moreover, statistical significance tests also show that the joint model is significantly better than all those state-of-the-art systems reported in the literature and achieves the best F-score in four of the five corpora tested. 2 Character-Based Models for CWS The goal of CWS is to find the corresponding word sequence for a given character sequence. Character-based model is to find out the corresponding tags for given character sequence. 2.1 Character-Based Discriminative"
C10-1132,P03-1035,0,0.0950296,"probability of the given input and its associated label sequence, while the discriminative model learns the posterior probability directly. Generative models often do not perform well because they make strong independence assumptions between features and labels. However, (Toutanova, 2006) shows that generative models can also achieve very similar or better performance than the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works;"
C10-1132,P08-1102,0,0.750786,"y have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173–1181, Beijing, August 2010 4) The character-based generative model (Wang et al., 2009), which adopts a charactertag-pair-based n-gram model and achieves comparable results with the popular characterbased discriminative model. In general, character-based models are much more robust on OOV words than word-based approa"
C10-1132,W04-3236,0,0.227271,"nce this tagging approach treats characters as basic units, the vocabulary size of those possible character-tag-pairs is limited. Therefore, this method is robust to OOV words and could possess a high recall of OOV words (ROOV). Although the dependency between adjacent tags/labels can be addressed, the dependency between adjacent characters within a word cannot be directly modeled under this framework. Lower recall of IV words (RIV) is thus usually accompanied (Wang et al., 2009). In this work, the character-based discriminative model is implemented by adopting the feature templates given by (Ng and Low, 2004), but excluding those ones that are forbidden by the closed test regulation of SIGHAN (e.g., Pu(C0): whether C0 is a punctuation). Those feature templates adopted are listed below: (a ) Cn (n = −2, −1, 0,1, 2); (b) Cn Cn +1 (n = −2, −1, 0,1); (c) C−1C1 For example, when we consider the third character “奥” in the sequence “北京奥运会”, template (a) results in the features as following: C-2=北, C-1=京, C0=奥, C1=运, C2=会, and template (b) generates the features as: C-2C-1=北京, C-1C0=京奥, C0C1=奥运, C1C2=运会, and template (c) gives the feature C-1C1=京运. 2.2 Character-Based Generative Model To incorporate the d"
C10-1132,C04-1081,0,0.0724455,"responding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173–1181, Beijing, August 2010 4) The character-based generative model (Wang et al., 2009), which adopts a charactertag-pair-based n-gram model and achieves comparable results with the popular characterbased discriminative model. In general, character-based models are much more r"
C10-1132,W06-1668,0,0.0215273,"airs is limited, the characterbased models can tolerate out-of-vocabulary (OOV) words and have become the dominant technique for CWS in recent years. On the other hand, statistical approaches can also be classified as either adopting a generative model or adopting a discriminative model. The generative model learns the joint probability of the given input and its associated label sequence, while the discriminative model learns the posterior probability directly. Generative models often do not perform well because they make strong independence assumptions between features and labels. However, (Toutanova, 2006) shows that generative models can also achieve very similar or better performance than the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model tha"
C10-1132,I05-3027,0,0.699833,"Missing"
C10-1132,O03-4002,0,0.779486,"han the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173–1181, Beijing, August 2010 4) The character-based generative model (Wang et al., 2009), which adopts a charactertag-pair-based n-gram model and achieves comparable results with the popular characterbased discriminative model. In general, character-based mod"
C10-1132,W03-1730,0,0.139802,"e given input and its associated label sequence, while the discriminative model learns the posterior probability directly. Generative models often do not perform well because they make strong independence assumptions between features and labels. However, (Toutanova, 2006) shows that generative models can also achieve very similar or better performance than the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 Proceedings of t"
C10-1132,zhang-etal-2004-interpreting,0,0.050023,"the proposed joint model among Although Table 5 has shown that the proposed those approaches that have been implemented. joint (joint-plus) model outperforms all the However, it would be interesting to know if the baselines mentioned above, we want to know joint (and joint-plus) model also outperforms if the difference is statistically significant those previous state-of-the-art systems. enough to make such a claim. Since there is The systems that performed best for at least only one testing set for each training corpus, one corpus in the second SIGHAN Bakeoff are the bootstrapping technique (Zhang et al., 2004) first selected for comparison. This category is adopted to conduct the tests: Giving an includes (Asahara et al., 2005) (denoted as 7 Statistical Significance Tests 1179 Asahara05) and (Tseng et al., 2005) 4 (Tseng05). (Asahara et al., 2005) achieves the best result in the AS corpus, and (Tseng et al., 2005) performs best in the remaining three corpora. Besides, those systems that are reported to exceed the above two systems are also selected. This category includes (Zhang et al., 2006) (Zhang06), (Zhang and Clark, 2007) (Z&C07) and (Jiang et al., 2008) (Jiang08). They are briefly summarized"
C10-1132,P07-1106,0,0.62793,"ften do not perform well because they make strong independence assumptions between features and labels. However, (Toutanova, 2006) shows that generative models can also achieve very similar or better performance than the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions. In terms of the above dimensions, methods for CWS can be classified as: 1) The word-based generative model (Gao et al., 2003; Zhang et al., 2003), which is a wellknown approach and has been used in many successful applications; 2) The word-based discriminative model (Zhang and Clark, 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach； 3) The character-based discriminative model (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Jiang et al., 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features, and it has been adopted in many previous works; 1173 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173–1181, Beijing, August 2010 4) The character-based generat"
C10-1132,P06-2123,0,\N,Missing
C10-1132,Y09-2047,1,\N,Missing
C10-1153,P06-2013,0,0.021533,"SRL can benefit from combination. And, as we will show, existing combination strategies can still be improved. 3 Individual SRL Systems 3.1 Full Parsing Based System The full parsing based system utilize full syntactic analysis to perform semantic role labeling. The shallow parsing based system utilize shallow syntactic information at the level of phrase chunks to perform semantic role labeling. Sun et al. (2009) proposed such a system on Chinese SRL and reported encouraging results. The system used in this paper is based on their approach. For Chinese chunking, we adopted the method used in (Chen et al., 2006), in which chunking is regarded as a sequence labeling task with IBO2 representation. The features used for chunking are the uni-gram and bi-gram word/POS tags with a window of size 2. The SRL task is also regarded as a sequence labeling problem. For an argument with label ARG*, we assign the label B-ARG* to its first chunk, and the label I-ARG* to its rest chunks. The chunks outside of any argument are assigned the label O. The features used for SRL are the same with those used in the onestage method in (Sun et al., 2009). In this paper, we employ Tiny SVM along with Yamcha (Kudo and Matsumot"
C10-1153,D08-1034,0,0.0205539,"difficulty, we perform multiple searches using different start points, and then choose the best solution found. 7 development Experiments 7.1 Experimental setup test We use Chinese Proposition Bank (CPB) 1.0 and Chinese Tree Bank (CTB) 5.0 of Linguistic Data Consortium corpus in our experiments. The training set is comprised of 648 files(chtb 081.fid to chtb 885.fid). The development set is comprised of 40 files(chtb 041.fid to chtb 080.fid). The test set is comprised of 72 files(chtb 001.fid to chtb 040.fid and chtb 900.fid to chtb 931.fid). The same data setting has been used in (Xue, 2008; Ding and Chang, 2008; Sun et al., 2009). Sun et al. (2009) used sentences with golden segmentation and POS tags as input to their SRL system. However, we use sentences with only golden segmentation as input. Then we perform automatic POS tagging using Stanford POS tagger (Toutanova et al., 2003). In (Xue, 2008), the parser used by the SRL system is trained on the training and development set plus 275K words of broadcast news. In this paper, all parsers used by the full parsing based system are trained on the training set plus the broadcast news portion of CTB6.0. And the chunker used in the shallow parsing based"
C10-1153,P03-1054,0,0.00537298,"p). For example, the ARG0 in Figure 1 is ((0, 2), ARG0, 0.94). Because the outputs of baseline systems are to be combined, we call such triple a candidate for combination. 4 Approach Overview As illustrated in Figure 2, the architecture of our system consists of a candidates generation stage, a weighted merging stage, and a combination stage. In the candidates generation stage, the baseline systems are run individually and their outputs are collected. We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. The second best parse tree of Berkeley parser is used here for its good quality. So together we have four different outputs from the full parsing based system. From the shallow parsing based system, we have only one output. Figure 2: The overall architecture of our system. In the weighted merging stage, each system output is assigned a weight according to our prior knowledge obtained on the development set. Details about how to obtain appropriate weights will be explained in Section 6. Then all candidates with the same loc and l are merged to one by"
C10-1153,W05-0625,0,0.40537,"ng National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences {tzhuang, cqzong}@nlpr.ia.ac.cn Abstract al., 2009). As pointed out by Xue (Xue, 2008), the SRL errors are mainly caused by the errors in automatic syntactic analysis. In fact, Chinese SRL suffers from parsing errors even more than English SRL, because the state-of-the-art parser for Chinese is still not as good as that for English. And previous research on English SRL shows that combination is a robust and effective method to alleviate SRL’s dependency on parsing results (M`arquez et al., 2005; Koomen et al., 2005; Pradhan et al., 2005; Surdeanu et al., 2007; Toutanova et al., 2008). However, the effect of combination for Chinese SRL task is still unknown. This raises two questions at least: (1) How much can Chinese SRL benefit from combination? (2) Can existing combination strategies be improved? All existing combination strategies trust each individual system’s output with the same confidence when putting them into a pool of candidates. But according to our intuition, different systems have different performance. And the system that have better performance should be trusted with more confidence. We c"
C10-1153,C04-1197,0,0.704124,"ence. In the argument classification stage, each location identified in the first stage is assigned a semantic role label. The features used in this paper are the same with those used in (Xue, 2008). Maximum entropy classifier is employed for both the argument identification and classification tasks. And Zhang Le’s MaxEnt toolkit1 is used for implementation. 2 3.2 Shallow Parsing Based System Related work A lot of research has been done on SRL combination. Most of them focused on English SRL task. But the combination methods are general. And they are closely related to the work in this paper. Punyakanok et al. (2004) formulated an Integer Linear Programming (ILP) model for SRL. Based on that work, Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) proposed a combination strategy that does not require the individual system to give a score for each argument. They used a binary classifier to filter different systems’ outputs. Then they used a greedy method to combine the candidates that pass the filtering process. Pradhan et al. (2005) combined systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing. They also used greedy method when"
C10-1153,N04-1032,0,0.0457419,"Missing"
C10-1153,D09-1153,0,0.470812,"esearch on a variety of combination strategies. All these research shows that combination can improve English SRL performance by 2∼5 points on F1 score. However, little is known about how much Chinese SRL can benefit from combination. And, as we will show, existing combination strategies can still be improved. 3 Individual SRL Systems 3.1 Full Parsing Based System The full parsing based system utilize full syntactic analysis to perform semantic role labeling. The shallow parsing based system utilize shallow syntactic information at the level of phrase chunks to perform semantic role labeling. Sun et al. (2009) proposed such a system on Chinese SRL and reported encouraging results. The system used in this paper is based on their approach. For Chinese chunking, we adopted the method used in (Chen et al., 2006), in which chunking is regarded as a sequence labeling task with IBO2 representation. The features used for chunking are the uni-gram and bi-gram word/POS tags with a window of size 2. The SRL task is also regarded as a sequence labeling problem. For an argument with label ARG*, we assign the label B-ARG* to its first chunk, and the label I-ARG* to its rest chunks. The chunks outside of any argu"
C10-1153,J08-2002,0,0.124274,"Missing"
C10-1153,N03-1033,0,0.0151548,"Missing"
C10-1153,J08-2004,0,0.179389,"the second difficulty, we perform multiple searches using different start points, and then choose the best solution found. 7 development Experiments 7.1 Experimental setup test We use Chinese Proposition Bank (CPB) 1.0 and Chinese Tree Bank (CTB) 5.0 of Linguistic Data Consortium corpus in our experiments. The training set is comprised of 648 files(chtb 081.fid to chtb 885.fid). The development set is comprised of 40 files(chtb 041.fid to chtb 080.fid). The test set is comprised of 72 files(chtb 001.fid to chtb 040.fid and chtb 900.fid to chtb 931.fid). The same data setting has been used in (Xue, 2008; Ding and Chang, 2008; Sun et al., 2009). Sun et al. (2009) used sentences with golden segmentation and POS tags as input to their SRL system. However, we use sentences with only golden segmentation as input. Then we perform automatic POS tagging using Stanford POS tagger (Toutanova et al., 2003). In (Xue, 2008), the parser used by the SRL system is trained on the training and development set plus 275K words of broadcast news. In this paper, all parsers used by the full parsing based system are trained on the training set plus the broadcast news portion of CTB6.0. And the chunker used in the"
C10-1153,N01-1025,0,0.0558851,"hen et al., 2006), in which chunking is regarded as a sequence labeling task with IBO2 representation. The features used for chunking are the uni-gram and bi-gram word/POS tags with a window of size 2. The SRL task is also regarded as a sequence labeling problem. For an argument with label ARG*, we assign the label B-ARG* to its first chunk, and the label I-ARG* to its rest chunks. The chunks outside of any argument are assigned the label O. The features used for SRL are the same with those used in the onestage method in (Sun et al., 2009). In this paper, we employ Tiny SVM along with Yamcha (Kudo and Matsumoto, 2001) for Chinese chunking, and CRF++2 for SRL. 3.3 Individual systems’ outputs The maximum entropy classifier used in full parsing based system and the CRF model used in shallow paring based system can both output classification probabilities. For the full parsing based system, the classification probability of the ar1 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html 2 http://crfpp.sourceforge.net/ 1363 gument classification stage is used as the argument’s probability. Whereas for the shallow parsing based system, an argument is usually comprised of multiple chunks. For example, an argu"
C10-1153,H05-1081,0,0.0592056,"Missing"
C10-1153,N07-1051,0,0.0136834,"tion loc, label l, and probability p. So each argument outputted by a system is a triple (loc, l, p). For example, the ARG0 in Figure 1 is ((0, 2), ARG0, 0.94). Because the outputs of baseline systems are to be combined, we call such triple a candidate for combination. 4 Approach Overview As illustrated in Figure 2, the architecture of our system consists of a candidates generation stage, a weighted merging stage, and a combination stage. In the candidates generation stage, the baseline systems are run individually and their outputs are collected. We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. The second best parse tree of Berkeley parser is used here for its good quality. So together we have four different outputs from the full parsing based system. From the shallow parsing based system, we have only one output. Figure 2: The overall architecture of our system. In the weighted merging stage, each system output is assigned a weight according to our prior knowledge obtained on the development set. Details about how to obtain appropriate weight"
C10-1153,P05-1072,0,0.596046,"y of Pattern Recognition Institute of Automation, Chinese Academy of Sciences {tzhuang, cqzong}@nlpr.ia.ac.cn Abstract al., 2009). As pointed out by Xue (Xue, 2008), the SRL errors are mainly caused by the errors in automatic syntactic analysis. In fact, Chinese SRL suffers from parsing errors even more than English SRL, because the state-of-the-art parser for Chinese is still not as good as that for English. And previous research on English SRL shows that combination is a robust and effective method to alleviate SRL’s dependency on parsing results (M`arquez et al., 2005; Koomen et al., 2005; Pradhan et al., 2005; Surdeanu et al., 2007; Toutanova et al., 2008). However, the effect of combination for Chinese SRL task is still unknown. This raises two questions at least: (1) How much can Chinese SRL benefit from combination? (2) Can existing combination strategies be improved? All existing combination strategies trust each individual system’s output with the same confidence when putting them into a pool of candidates. But according to our intuition, different systems have different performance. And the system that have better performance should be trusted with more confidence. We can use our prior knowl"
C10-1153,J04-4004,0,\N,Missing
C10-1153,D09-1133,0,\N,Missing
C10-2148,P98-1115,0,0.0510352,"earchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head words information. Both of the two PCFG based approaches have improved the basic PCFG based parsers significantly. However, neither of them has been guided by enough linguistic priori knowledge. Their parsing procedures are too mechanical. Because of this, the efficiency is always worse, and much more artificial ambiguities, which are different from linguistic ambiguities (Krotov et al., 1998; Johnson, 1998), are generated. We believe parsing procedure guided by more linguistic priori knowledge will help to overcome the drawbacks in some extent. From our intuition, dependency structure, another type of syntactic structure with much linguistic knowledge, will be a good candidate to guide phrase parsing procedure. In this paper we present a novel approach to using dependency structure to guide phrase parsing. This novel approach has its virtues from multiple angles. First, dependency structure offers a good compromise between the conflicting demands of analysis depth, which makes it"
C10-2148,P03-1054,0,0.13932,"w years, several high-precision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so"
C10-2148,W00-1201,0,0.0607234,"Missing"
C10-2148,C02-1126,0,0.0645014,"Missing"
C10-2148,W07-2416,0,0.0635481,", the dependency structures are often projective trees. In this paper, we only consider English parsing based on Penn Treebank (PTB) and Chinese parsing based on Penn Chinese Treebank (PCTB), so we just research the consistency between phrase structure and projective dependency structure through PTB/PCTB. Information carried by the two structures isn’t equal. Phrase structure is more flexible, carries more information, and even contains all the information of dependency structure. So the task to convert a phrase structure to dependency structure is more straight, e.g. Nivre and Scholz (2004), Johansson and Nugues (2007). However, the reverse procedure is much more difficult, because dependency structure lacks the syntactic symbols, which are indispensable in phrase structure. Although the two structures are completely different, they have consistency in some deep level. In this paper we analyze the consistency from a practical perspective in order to do phrase parsing with the help of dependency structure. Having investigated the two kinds of trees with dependency structure and phrase structure, we find a consistency 1 that each subtree in dependency structure must correspond to a sub-tree in phrase structur"
C10-2148,N06-1020,0,0.0907076,"Missing"
C10-2148,P03-1056,0,0.123951,"cision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nontermina"
C10-2148,E06-1011,0,0.0711487,"Missing"
C10-2148,H01-1014,0,0.859493,"and Johnson, 2005) just bring a little linguistic priori knowledge (head word information) into learning phase. In inference phase, both of the unlexicalized PCFG based approach and lexicalized PCFG based approach are using the pure searching algorithms, which try to parse a sentence monotonously, either from left to right or from right to left. From these states, we can find that manners of current parsers are too mechanical. Because of this, the efficiency of phrase parsers is always worse, and much more artificial ambiguities are generated. There have been some work (Collins et al., 1999b; Xia and Palmer, 2001) about converting dependency structures to phrase structures. Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)’s model. Results showed the accuracy of dependency parsing for Czech was improved largely. Xia and Palmer (2001) proposed a more generalized algorithm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the ones in Penn Treebank. How"
C10-2148,W03-3023,0,0.451785,"Missing"
C10-2148,N07-1051,0,0.060902,"sing procedure with the help of dependency structure, and make the parsing procedure flexibly. Matsuzaki et al. (2005) defined a generative model called PCFG with latent annotations (PCFG-LA). Using EM-algorithm each nonterminal symbols was annotated with a latent variable, and a fine-grained model can be got. In order to get a more compact PCFG-LA model, Petrov et al. (2006) presented a split-and-merge method which can get PCFG-LA model hierarchically, and their final result outperformed state-of-the-art phrase parsers. To make the parsing process of hierarchical PCFGLA model more efficient, Petrov and Klein (2007) presented a coarse-to-fine inference algorithm. In Section 4 of this paper, we try to combine the hierarchical PCFG-LA model in learning phase and coarse-to-fine method in inference phase into our parser in order to get an accurate and efficient parser. 1293 join Vinken will (1) board as the 29 director a Nov (3) nonexecutive (2) (a) Dependency structure S NP VP MD VP NNP will VB NP PP NP Vinken DT (1) NN NP IN NNP CD Nov 29 join the board as DT a JJ NN director (3) nonexecutive (b) Phrase structure (2) Figure 1. The consistency between phrase structure and dependency structure 3 Our framewor"
C10-2148,P06-1055,0,0.651111,"ost of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head words information. Bot"
C10-2148,P05-1010,0,0.352437,"ve been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical items (Charniak, 1997), so they annotate nonterminal symbols with the head"
C10-2148,I05-2002,1,0.807382,"hm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the ones in Penn Treebank. However, we have to point out that they only computed the unlabeled performance but lost all the exact syntactic symbols. Different from treetransformed PCFG based approach and lexicalized PCFG based approach, both of Collins et al. (1999b) and Xia and Palmer (2001) attempted to build some heuristic rules through linguistic theory, but didn’t try to learn anything from Treebank. Li and Zong (2005) presented a hierarchical parsing algorithm for long complex Chinese sentences with the help of punctuations. They first divided a long sentence into short ones according to punctuation marks, then parsed the short ones into sub-trees individually, and at last combined all the sub-trees into a whole tree. Experimental results showed the parsing time was reduced largely, and performance was improved too. Although the procedure of their parser is more close to human beings’ manner, it appears a little shallow just using the punctuation marks. In this paper our motivations are to bring more lingu"
C10-2148,D09-1087,0,0.066032,"Missing"
C10-2148,P08-1067,0,0.137056,"Missing"
C10-2148,J98-4004,0,0.386337,"ver the past few years, several high-precision phrase parsers have been presented, and most of them are employing probabilistic context-free grammar (PCFG). As we all know, the basic PCFG has the problems that the independence assumption is too strong and lacks of lexical conditioning (Jurafsky and Martin, 2007). Although researchers have proposed various models and inference algorithms aiming to solve these problems, the performance of existing phrase parsers is still remained to further improve. Most of the existing approaches can be classified into two categories: unlexicalized PCFG based (Johnson, 1998; Klein and Manning, 2003; Levy and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006) and lexicalized PCFG based (Collins, 1999a; Charniak, 1997; Bikel, 2004; Charniak and Johnson, 2005). Unlexicalized PCFG based approach attempts to weaken the independence assumption by annotating non-terminal symbols with labels of ancestor, siblings and even the latent annotations encoded by local information. In lexicalized PCFG based approach, researchers believe that the forms of a constituent and its subconstituents are determined more by the constituent’s head than any other of its lexical it"
C10-2148,P99-1065,0,0.622724,"Bikel, 2004; Charniak and Johnson, 2005) just bring a little linguistic priori knowledge (head word information) into learning phase. In inference phase, both of the unlexicalized PCFG based approach and lexicalized PCFG based approach are using the pure searching algorithms, which try to parse a sentence monotonously, either from left to right or from right to left. From these states, we can find that manners of current parsers are too mechanical. Because of this, the efficiency of phrase parsers is always worse, and much more artificial ambiguities are generated. There have been some work (Collins et al., 1999b; Xia and Palmer, 2001) about converting dependency structures to phrase structures. Collins et al. (1999b) proposed an algorithm to convert the Czech dependency Treebank into a phrase structure Treebank and do dependency parsing through Collins (1999a)’s model. Results showed the accuracy of dependency parsing for Czech was improved largely. Xia and Palmer (2001) proposed a more generalized algorithm according to X-bar theory and Collins et al. (1999b), and they did some experiments on Penn Treebank. The results showed their algorithm produced phrase structures that were very close to the on"
C10-2148,A00-2018,0,\N,Missing
C10-2148,J04-4004,0,\N,Missing
C10-2148,W08-2102,0,\N,Missing
C10-2148,J03-4003,0,\N,Missing
C10-2148,C98-1111,0,\N,Missing
C10-2148,P05-1022,0,\N,Missing
C10-2148,C04-1010,0,\N,Missing
C10-2148,P08-1006,0,\N,Missing
C10-2153,C04-1121,0,0.295828,"ation is known as the bag-ofwords (BOW) model. Although BOW is quite simple and efficient, a great deal of the information from original text is discarded, word order is disrupted and syntactic structures are broken. Therefore, more sophisticated features with a deeper understanding of the text are required for sentiment classification tasks. With the attempt to capture the word relation information behind the text, word relation (WR) features, such as higher-order n-grams and word dependency relations, have been employed in text representation for sentiment classification (Dave et al., 2003; Gamon, 2004; Joshi and Penstein-Rosé, 2009). However, in most of the literature, the performance of individual WR feature set was poor, even inferior to the traditional unigrams. For this reason, WR features were commonly used as additional features to supplement unigrams, to encode more word order and word relation information. Even so, the performance of joint features was still far from satisfactory (Dave et al., 2003; Gamon, 2004; Joshi and PensteinRosé, 2009). We speculate that the poor performance is possibly due to the following two reasons: 1) in WR features, the data are sparse and the features"
C10-2153,P06-2079,0,0.203182,"employ the average perceptron (AvgP) (Freund and Schapire, 1999), a variation of perceptron model that averages the weights of all iteration loops, to improve the generalization performance. 4 Feature Selection for WR Features In the past decade, feature selection (FS) studies mainly focus on topical text classification. (Yang and Pedersen, 1997) investigate five FS metrics and reported that good FS methods (such as IG and CHI) can improve the categorization accuracy with an aggressive feature removal. In sentiment classification tasks, traditional FS methods were also proven to be effective (Ng et al., 2006; Li et al., 2009). With regard to WR features, since the dimension of feature space has sharply increased, the amount of computation is considerably large when employing traditional FS methods. 4.1 (8) If w and s are independent, they are condi(6) tionally independent. Thus we have i 1 i P ( ws, ck ) . P ( ws ) P (ck ) Fast MI and Fast IG In order to address this problem, we propose a fast feature selection method that is specially designed for GWR features. In our method, the (9) Formula (9) indicates that under the assumption that two component parts w and s of a relation feature ws are mu"
C10-2153,P04-1035,0,0.00463746,"ature selection is performed (traditional IG on unigrams, FIG on GWR features). Note that when performing FIG on individual GWR feature sets, the computation of nonback-off word G ( w) , is taken care of by having already computed IG on unigrams. Thus, we only need to compute the score of 25 POS pairs. From this point of view, FIG (FMI) is quite suitable for the ensemble model. 5 Experiments We first present the performance of system performance, and then demonstrate the effectiveness of fast feature selection. 5.1 Experimental Setup Datasets: The Cornell movie-review dataset 1 introduced by (Pang and Lee, 2004) is used in our experiments. It is a document-level polarity dataset that contains 1,000 positive and 1,000 negative processed reviews. We also use the dataset 2 introduced in (Joshi and Penstein-Rosé, 2009) for comparison. It is a subset (200 sentences each for 11 different products) of the product review dataset released by (Hu and Liu, 2004). We will refer to it Eproduct dataset. The Movie dataset is a domain-specific document-level dataset and the E-product dataset is at sentence-level and cross-domain. We conduct experiments on both of them to evaluate our approach in a wide range of task"
C10-2153,W02-1011,0,0.0206028,"ependency relation pairs to its POS tag (e.g., great-movie to great-noun), which are supposed to be more generalized than word pairs. Based on Joshi and Rosé’s method, we back off the word in each word relation pairs to its corresponding POS cluster, making the feature space smarter and more effective. On the other hand, we find that from unigrams to WR features, relevance between features is reduced and the independence is in1336 Coling 2010: Poster Volume, pages 1336–1344, Beijing, August 2010 creased. Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al., 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case. Based on this finding, we therefore intuitively seek, instead of jointly using unigrams and GWR features, to efficiently integrate them to synthesize a more accurate classification procedure. We use the ensemble model to fuse different types of features under distinct classification models, with an attempt to overcome individual drawbacks and benefit from each other’s merit, and"
C10-2153,P09-2079,0,0.361336,"n as the bag-ofwords (BOW) model. Although BOW is quite simple and efficient, a great deal of the information from original text is discarded, word order is disrupted and syntactic structures are broken. Therefore, more sophisticated features with a deeper understanding of the text are required for sentiment classification tasks. With the attempt to capture the word relation information behind the text, word relation (WR) features, such as higher-order n-grams and word dependency relations, have been employed in text representation for sentiment classification (Dave et al., 2003; Gamon, 2004; Joshi and Penstein-Rosé, 2009). However, in most of the literature, the performance of individual WR feature set was poor, even inferior to the traditional unigrams. For this reason, WR features were commonly used as additional features to supplement unigrams, to encode more word order and word relation information. Even so, the performance of joint features was still far from satisfactory (Dave et al., 2003; Gamon, 2004; Joshi and PensteinRosé, 2009). We speculate that the poor performance is possibly due to the following two reasons: 1) in WR features, the data are sparse and the features lack generalization capability;"
C10-2153,P09-1078,1,\N,Missing
C12-1101,P96-1041,0,0.0862461,"Missing"
C12-1101,I05-3017,0,0.294898,"Missing"
C12-1101,P03-1035,0,0.0745001,"Missing"
C12-1101,W10-4127,0,0.0542028,"Missing"
C12-1101,W10-4128,0,0.0243565,"Missing"
C12-1101,W10-4141,0,0.0240683,"Missing"
C12-1101,P08-1102,0,0.144855,"e with prefix/suffix. For example, “造影术” (radiography) is an OOV word with suffix “术” (technique), while the word “造影” (radiograph) is contained in the dictionary. However, it is wrongly split into “造影” and “术”, since the longest word in the dictionary is preferred. This problem will be our future work. 6 Related work The word-based generative model (Gao et al., 2003; Zhang et al., 2003) is a classical approach for CWS. However, this approach needs an additional module to recognize OOV words. Therefore, the character-based discriminative model (Xue, 2003; Low et al., 2005; Zhang et al., 2006; Jiang et al., 2008; Zhao et al., 2010) has become the main stream due to its capability in handling OOV words. However, the character-based discriminative model cannot give satisfactory performance for IV words. Wang et al. (2010) thus proposed a generative model to fix this problem. Afterwards, they 1665 further proposed an integrated model to integrate generative and discriminative approaches, as these two approaches complement each other. On the other hand, dictionary information has been utilized in the discriminative approach in the previous works of (Low et al., 2005; Zhao et al., 2010). However, they foc"
C12-1101,W04-3250,0,0.0766166,"Missing"
C12-1101,I05-3025,0,0.642104,". 1 2 1654 Several factors that might affect the performance of the new model are studied in this paper: including the context information, the OOV coverage rate of the dictionary, and the weight of the new factor in the model. We evaluated our final system on the CIPS-SIGHAN-2010 Bakeoff data. The obtained results not only convincingly demonstrate the effectiveness of the proposed model for cross-domain CWS, but also achieve the best performance on 3 out of 4 domains in the open test. Afterwards, the proposed enhanced generative model is integrated with another enhanced discriminative model (Low et al., 2005) to further improve the performance, and achieves the best performance on all the tested corpora. The remainder of this paper is organized as: Section 2 discusses how to incorporate dictionary information and section 3 describes the proposed models. Empirical results and error analysis are presented in section 4 and 5. Section 6 reviews the related work. 2 Dictionary related features 2.1 Word-ID or Word-Matching-Indicator? Given a dictionary, there are two kinds of features that can be utilized: word-ID, which are binary features that fire only when the word matches one specific word entry, an"
C12-1101,W04-3236,0,0.0593811,"Missing"
C12-1101,C10-2139,0,0.490364,"lated work. 2 Dictionary related features 2.1 Word-ID or Word-Matching-Indicator? Given a dictionary, there are two kinds of features that can be utilized: word-ID, which are binary features that fire only when the word matches one specific word entry, and Word-MatchingIndicator (e.g. TM defined in Section 2.3), which checks the relationship between the assigned position tag of the current character and the dictionary words within local context. Since the statistics of OOV words can never be learnt from the training corpus, the approaches that adopt word-ID as features (Zhang and Clark, 2007; Sun, 2010; Zhang and Clark, 2011) cannot really utilize the information of the OOV words kept in the dictionary. On the contrary, the wordmatching-indicator is applicable for both IV and OOV words kept in the dictionary. This feature thus provides valuable information for those OOV words covered by the dictionary. Therefore, based on the positions of those dictionary matching words, two dictionary-related features (i.e., Dictionary Coverage Status and Tag Matching Status, to be specified later) are proposed in this paper, and they will be incorporated into the character-based generative model. 2.2 Dict"
C12-1101,D11-1090,0,0.0749426,"generative and discriminative approaches, as these two approaches complement each other. On the other hand, dictionary information has been utilized in the discriminative approach in the previous works of (Low et al., 2005; Zhao et al., 2010). However, they focus on improving the in-domain word segmentation accuracy, while we investigate how the domain invariant feature (based on dictionary information) helps for cross-domain tasks. Besides, the effect of varying OOV words coverage rates is studied in this paper for the first time. In addition to dictionary feature, Zhao and Kit (2007; 2008), Sun and Xu (2011) too, also adopted the accessor variety feature to gain better generalization ability. Since this feature can be extracted from unlabelled corpora, it is suitable to be adopted for domain adaptation. Again, all their works focus on in-domain performance. Other works that focus on in-domain performance also include (Zhang and Clark, 2007), (Fu et al., 2008), (Jiang et al., 2008), (Lin, 2009), (Xiong et al., 2009), and (Zhang and Clark, 2011). Last, (Ben-David et al., 2007) pointed out that a good feature representation for domain adaptation should minimize the difference between its distributio"
C12-1101,P12-1027,0,0.20558,"Missing"
C12-1101,Y09-2047,1,0.845539,"me that the dictionary matching words are {“大学”, “大学生”}: if the tag assigned to „学‟ is “M”, then TM 2 will be “Following-Longest-Word”; if it is “E”, then TM 2 will be “Only-FollowingShorter-Word”; if it is “B” or “S”, TM 2 would be “Not-Following-Any-Word”. Therefore, this candidate-feature is associated with each candidate of the position-tag. However, if no dictionary word covers this character, then TM 2 will be set to “Inapplicable” regardless of which tag is assigned to „学‟ (i.e., we do not want to disturb the original model in this case). 3 3.1 Proposed models Enhanced generative model Wang et al. (2009) proposed a character-based generative model for CWS, which is able to handle the dependency of character-bigrams within words and thus give a good balance for the performance of IV words and OOV words. Their approach adopts the character-tag-pair trigram model, and obtains the desired position-tag sequence t1 n as follows: 1657 t1n  arg max  P ([c, t ]i |[c, t ]ii 12 ) n t1n (1) i 1 where [c, t ]1n is the associated character-tag-pair sequence for the given character sequence c1n . To alleviate the data sparseness problem, we pre-convert the given character string into its corresponding"
C12-1101,C10-1132,1,0.914569,"ch are features (d) and (e) in the following list) to the widely adopted primitive templates described in (Ng and Low, 2004), and used them to enhance the original discriminative model (Xue, 2003): (a ) Cn (n  2, 1, 0,1, 2); (b) Cn Cn 1 (n  2, 1, 0,1); (d ) MWL0 , t &apos;0 ; (e) Cn t &apos;0 (n  1, 0,1). (c ) C1C1 ; Let W denote the longest dictionary word that covers c0, then MWL0 denotes the length of W, and t &apos;0 denotes the corresponding tag of c0 in W. Since the enhanced generative model cannot utilize the features from future context, which is a common drawback of generative approaches (Wang et al., 2010), following the approach of (Wang et al., 2011), we further integrate the enhanced generative model with the above enhanced discriminative model via log-linear interpolation, shown as follows: Score(ti )    [  log P ([u , t ]i |[u , t ]ii 12 )  (1   ) log P (TM i |MWLi , DCi , uii 2 )]  (1   )  log( P (ti |uii22 , MWLi , t &apos;i ) (5) Where  and  are two weighting coefficients to be decided from the development set, and 0   ,   1.0 . 4 Experiments All experiments are conducted on the corpora provided by SIGHAN-2005 (Emerson, 2005) and CIPS-SIGHAN-20106 (Zhao and Liu, 2010)."
C12-1101,O03-4002,0,0.405225,"Missing"
C12-1101,W03-1730,0,0.54596,"Missing"
C12-1101,P06-2123,0,0.389657,"e uncovered terms are with prefix/suffix. For example, “造影术” (radiography) is an OOV word with suffix “术” (technique), while the word “造影” (radiograph) is contained in the dictionary. However, it is wrongly split into “造影” and “术”, since the longest word in the dictionary is preferred. This problem will be our future work. 6 Related work The word-based generative model (Gao et al., 2003; Zhang et al., 2003) is a classical approach for CWS. However, this approach needs an additional module to recognize OOV words. Therefore, the character-based discriminative model (Xue, 2003; Low et al., 2005; Zhang et al., 2006; Jiang et al., 2008; Zhao et al., 2010) has become the main stream due to its capability in handling OOV words. However, the character-based discriminative model cannot give satisfactory performance for IV words. Wang et al. (2010) thus proposed a generative model to fix this problem. Afterwards, they 1665 further proposed an integrated model to integrate generative and discriminative approaches, as these two approaches complement each other. On the other hand, dictionary information has been utilized in the discriminative approach in the previous works of (Low et al., 2005; Zhao et al., 2010"
C12-1101,P07-1106,0,0.591488,"ection 6 reviews the related work. 2 Dictionary related features 2.1 Word-ID or Word-Matching-Indicator? Given a dictionary, there are two kinds of features that can be utilized: word-ID, which are binary features that fire only when the word matches one specific word entry, and Word-MatchingIndicator (e.g. TM defined in Section 2.3), which checks the relationship between the assigned position tag of the current character and the dictionary words within local context. Since the statistics of OOV words can never be learnt from the training corpus, the approaches that adopt word-ID as features (Zhang and Clark, 2007; Sun, 2010; Zhang and Clark, 2011) cannot really utilize the information of the OOV words kept in the dictionary. On the contrary, the wordmatching-indicator is applicable for both IV and OOV words kept in the dictionary. This feature thus provides valuable information for those OOV words covered by the dictionary. Therefore, based on the positions of those dictionary matching words, two dictionary-related features (i.e., Dictionary Coverage Status and Tag Matching Status, to be specified later) are proposed in this paper, and they will be incorporated into the character-based generative mode"
C12-1101,J11-1005,0,0.0410802,"2 Dictionary related features 2.1 Word-ID or Word-Matching-Indicator? Given a dictionary, there are two kinds of features that can be utilized: word-ID, which are binary features that fire only when the word matches one specific word entry, and Word-MatchingIndicator (e.g. TM defined in Section 2.3), which checks the relationship between the assigned position tag of the current character and the dictionary words within local context. Since the statistics of OOV words can never be learnt from the training corpus, the approaches that adopt word-ID as features (Zhang and Clark, 2007; Sun, 2010; Zhang and Clark, 2011) cannot really utilize the information of the OOV words kept in the dictionary. On the contrary, the wordmatching-indicator is applicable for both IV and OOV words kept in the dictionary. This feature thus provides valuable information for those OOV words covered by the dictionary. Therefore, based on the positions of those dictionary matching words, two dictionary-related features (i.e., Dictionary Coverage Status and Tag Matching Status, to be specified later) are proposed in this paper, and they will be incorporated into the character-based generative model. 2.2 Dictionary Coverage Status L"
C12-1101,zhang-etal-2004-interpreting,0,0.0599932,"Missing"
C12-1101,I08-4017,0,0.262701,"Missing"
C12-1101,W10-4126,0,\N,Missing
C12-1185,W11-2136,0,0.0914387,"them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3) we design a CKY algorithm to translate the entire PAS according to the target-side-like PAS. The algorithm can be easily inte"
C12-1185,J04-4004,0,0.0311547,"ture is used to evaluate which TP is more appropriate for the specific SP 6 . The two features indicate the distribution of bilingual PASs from two different angles, which will be helpful for the decoder to choose effective PAS transformation rules. 3 PAS-based Translation Framework In the PAS acquisition step, we perform SRL on each test sentence with a monolingual SRL system. To alleviate the negative impact of SRL errors, we use multiple SRL results. We provide the monolingual SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). FIGURE 4(a) shows an example of multiple SRL results. In the transformation step, we match the multiple SRL results with PAS transformation rules and convert them to target-side-like PASs. Then in the translation step, we decode the PAS based on these target-side-like PASs. FIGURE 4 – Multiple SRL results and the final mathcing result of the example sentence. 3.1 PAS Transformation In this section, we describe how to match the multiple SRL results with PAS transformation rules and transform them to target-side-like PASs. We design Algorithm 1 to"
C12-1185,J93-2003,0,0.02612,"transformation rules; (3) Translation: first translate the predicate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a ne"
C12-1185,J07-2003,0,0.0825815,"decision by voting with the abovementioned five parse trees. After attachment, some PASs may be identical to each other, such as the matching result M2 and M3 of FIGURE 4(b). We only retain the one whose matching score is larger. FIGURE 5 – An example of gap word attachment using parse tree. 3.3 PAS Translation In the translation step, we translate each source element by a traditional translation method. Then we combine these candidates to translate the entire PAS based on the target-side-like PAS, just as FIGURE 1 shows. Intuitively, the combination can be operated directly by cube pruning (Chiang, 2007). However, since the source elements are translated independently and many source elements’ spans are very short, numerous phrase translation rules are ignored during translation. This fact leads to a narrow decoding space and poor translation accuracy. To alleviate this problem, we design a CKY-style decoding algorithm for each target-side-like PAS. FIGURE 6 – An example of our CKY-style decoding algorithm for target-side-like PAS. In this example, only one path is generated for the final span 3-12. In practice, there can be many paths. In the CKY-style decoding algorithm, we organize the sou"
C12-1185,P03-2041,0,0.116531,"hat depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are"
C12-1185,2007.tmi-papers.10,0,0.440117,"Missing"
C12-1185,P06-1121,0,0.0316988,"KY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-"
C12-1185,W11-1012,0,0.183153,"ignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3) we design a CKY algorithm to translate the entire PAS according to the target-side-like PAS. The algorithm can be easily integrated with any CKY-based decoder to generate better translation hypotheses. Conclusion and Pers"
C12-1185,P03-1054,0,0.00598437,"is more appropriate for the specific SP 6 . The two features indicate the distribution of bilingual PASs from two different angles, which will be helpful for the decoder to choose effective PAS transformation rules. 3 PAS-based Translation Framework In the PAS acquisition step, we perform SRL on each test sentence with a monolingual SRL system. To alleviate the negative impact of SRL errors, we use multiple SRL results. We provide the monolingual SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). FIGURE 4(a) shows an example of multiple SRL results. In the transformation step, we match the multiple SRL results with PAS transformation rules and convert them to target-side-like PASs. Then in the translation step, we decode the PAS based on these target-side-like PASs. FIGURE 4 – Multiple SRL results and the final mathcing result of the example sentence. 3.1 PAS Transformation In this section, we describe how to match the multiple SRL results with PAS transformation rules and transform them to target-side-like PASs. We design Algorithm 1 to achieve our purpose. First, we look for the pr"
C12-1185,N03-1017,0,0.0174728,"translate the predicate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the s"
C12-1185,W04-3250,0,0.0513644,"Missing"
C12-1185,P07-2045,0,0.00708231,"Missing"
C12-1185,2006.iwslt-evaluation.11,0,0.0653863,"TG) system segments the sentence based on its PAS. Since a correct PAS denotes the skeleton structure of the sentence, it performs both reasonable sentence segmentation and better global phrase reordering for translation. Furthermore, in the second example, our PASbased method successfully recognizes the [AM-TMP] argument “2005年” and move it to the end of sentence. However, the BTG system only performs translation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted s"
C12-1185,W08-0308,0,0.217448,"ed semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work is different from the existing work in the following aspects: (1) we induce PAS transformation rules to model the interrelation between source-side PAS and its target counterpart; (2) we utilize multiple SRL results to alleviate the negative impact of bad PASs; (3"
C12-1185,C10-1081,0,0.397582,"nslation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles a"
C12-1185,P06-1077,0,0.138834,"orithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure"
C12-1185,W06-1606,0,0.187065,"e the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure"
C12-1185,P03-1021,0,0.022423,"Missing"
C12-1185,J04-4002,0,0.143747,"cate and arguments of PAS and then adopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance. KEYWORDS: Predicate-argument structure; Semantic role labeling; PAS transformation; PASbased translation Proceedings of COLING 2012: Technical Papers, pages 3019–3036, COLING 2012, Mumbai, December 2012. 3019 1 Introduction Statistical machine translation (SMT) has made significant progress from word-based models (Brown et al., 1993) to phrase-based models (Koehn et al., 2003; Och and Ney, 2004) and syntax-based models (Galley et al., 2006; Liu et al., 2006; Marcu et al., 2006) over the past decades. However, the existing SMT models are always criticized for not modeling the semantics of languages. Furthermore, reordering is always one of the most difficult and important research problems in SMT. However, although current translation models are much good at local reordering 1 , most of them are weak to cope with global reordering 2 . The two weaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of language"
C12-1185,P02-1040,0,0.0851425,"Missing"
C12-1185,P06-1055,0,0.0205813,"Missing"
C12-1185,D07-1078,0,0.0166676,"respectively. We compare these two ancestor nodes and attach the gap word to the element whose corresponding ancestor node is lower in the parse tree. For example in FIGURE 5, the common ancestor node of word “减税” and [A1] is node NP11,12, while it is node VP10,12 for [Pred]. Hence, we attach word “减税” to [A1] and transform the PAS1 to PAS2 in FIGURE 5. In practice, it is common that the neighboring left and right elements get the same ancestor node. This is because a father node can dominate many children nodes in parse trees. To address this problem, we employ the head binarization method (Wang et al., 2007) to binarize the parse trees. We average the five probabilities given by the parse trees as this probability. 3025 We make the final attachment decision by voting with the abovementioned five parse trees. After attachment, some PASs may be identical to each other, such as the matching result M2 and M3 of FIGURE 4(b). We only retain the one whose matching score is larger. FIGURE 5 – An example of gap word attachment using parse tree. 3.3 PAS Translation In the translation step, we translate each source element by a traditional translation method. Then we combine these candidates to translate th"
C12-1185,2009.eamt-1.30,0,0.538197,"eaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure that depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: per"
C12-1185,N09-2004,0,0.477147,"eaknesses restrict current translation models a lot, which urges us to seek a new translation framework to model both the semantics of languages and global reordering. Formally, predicate-argument structure (PAS) is a structure that depicts the relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: per"
C12-1185,W11-1003,0,0.268493,"Missing"
C12-1185,I11-1004,0,0.0730523,"based on its PAS. Since a correct PAS denotes the skeleton structure of the sentence, it performs both reasonable sentence segmentation and better global phrase reordering for translation. Furthermore, in the second example, our PASbased method successfully recognizes the [AM-TMP] argument “2005年” and move it to the end of sentence. However, the BTG system only performs translation without any reordering. 6 Related Work Previous work utilizing PAS in SMT can be roughly categorized into three directions. One direction is to do pre-processing or post-processing. Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to t"
C12-1185,P06-1066,0,0.158022,"Missing"
C12-1185,P12-1095,0,0.320708,"Komachi and Matsumoto (2006) and Wu et al. (2011) used PAS-based heuristic rules and automatic rules respectively to pre-order the 3031 input sentences. Wu and Fung (2009b) performed SRL on the outputs of phrase-based system Moses and then reordered the achieved semantic roles to match the roles of input sentences. Some other works tried to design proper PAS-based features and integrate them into decoder. Liu and Gildea (2010) projected source-side PASs to target side via word alignment and designed a “Semantic Role Re-ordering” feature and a “Deleted Roles” feature for tree-to-string model. Xiong et al. (2012) adopted semantic features to translate verbal predicates and predict the relative position between predicates and arguments. Some other works focused on utilizing semantic roles to refine the non-terminals of syntax-based translation model. Liu and Gildea (2008) substituted the syntactic labels with semantic roles or combined them together for a tree-to-string model. Aziz et al., (2011) used semantic roles and base-phrase tags to create shallow semantic trees. Gao and Vogel (2011) used target side semantic roles to create SRL-aware non-terminals for hierarchical phrase-based model. Our work i"
C12-1185,J08-2004,0,0.148475,"Missing"
C12-1185,2011.mtsummit-papers.29,1,0.85492,"Missing"
C12-1185,D10-1043,0,0.190317,"e relationship between a predicate and its associated arguments, and it always indicates the semantic frame and skeleton structure of a sentence. From the characteristics of PAS, we can see that it provides not only a good semantic representation for modeling semantics, but also a skeleton structure for global reordering. Moreover, Fung et al. (2006) and Wu and Fung (2009b) have shown that PASs of the both sides are more consistent with each other than syntax structures. Considering current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS will be a better alternative for building translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the r"
C12-1185,D11-1019,1,0.863801,"Missing"
C12-1185,Y09-2016,1,0.908229,"Missing"
C12-1185,D10-1030,1,0.89398,"ding translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the result of bilingual semantic role labeling (Zhuang and Zong, 2010b). Here, target-side-like PAS denotes a list of general non-terminals in target language order, where a non-terminal aligns to a source element. Henceforward, we use source elements to denote the predicate and arguments of sourceside PAS (similarly for target elements). Translation: just as FIGURE 1 shows, this step is further divided into two parts: (a) element translation is to translate each source element respectively; (b) translation by global reordering is to combine the translation candidates of source elements to translate the entire PAS based on the target-side-like PAS. This method"
C12-1185,C10-1153,1,0.392016,"ding translation models. Therefore, in this paper, aiming at building a PAS-based translation framework, we propose a novel translation method based on PAS transformation. FIGURE 1 is an overview of our method. Specifically, we divide the entire translation process into 3 steps: (1) (2) (3) PAS acquisition: perform semantic role labeling (SRL) on the input sentences to achieve their PASs, i.e., source-side PASs. Transformation: convert source-side PASs to target-side-like PASs by predicate-aware PAS transformation rules, which are extracted from the result of bilingual semantic role labeling (Zhuang and Zong, 2010b). Here, target-side-like PAS denotes a list of general non-terminals in target language order, where a non-terminal aligns to a source element. Henceforward, we use source elements to denote the predicate and arguments of sourceside PAS (similarly for target elements). Translation: just as FIGURE 1 shows, this step is further divided into two parts: (a) element translation is to translate each source element respectively; (b) translation by global reordering is to combine the translation candidates of source elements to translate the entire PAS based on the target-side-like PAS. This method"
C12-1186,N10-1028,0,0.126723,"Missing"
C12-1186,P09-1088,0,0.0849466,"Missing"
C12-1186,N10-1015,0,0.0527055,"nto packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem between tree structures a"
C12-1186,D08-1092,0,0.0482246,"ed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem"
C12-1186,D12-1079,0,0.0811569,"cording to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on bot"
C12-1186,P05-1033,0,0.452574,"Missing"
C12-1186,J07-2003,0,0.193766,"rees in the state-of-the-art string-to-tree translation system. 2 Related Work Our work focuses on inducing effective unsupervised tree structures, and meanwhile, resolving the incompatibility problem between tree structures and word alignment for tree-based translation. Several researchers have studied unsupervised tree structure induction for different objectives. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammar (SCFG) from a parallel corpus. The obtained SCFG grammar is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Denero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled tree structures for syntactic pre-reordering. Different from above works, we concentrate on producing effective and labeled unsupervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two d"
C12-1186,N09-1025,0,0.0427139,"Missing"
C12-1186,D09-1037,0,0.179081,"Missing"
C12-1186,W06-1628,0,0.194541,"Missing"
C12-1186,D07-1079,0,0.0335265,"Missing"
C12-1186,P07-1003,0,0.0645151,"word alignment 6 . For example, in FIGURE 2, node PRP…RB’s span is {we meet again} and it dominates span {我 们 再次 见面} at the other side. These two spans are consistent with word alignment. Therefore, node PRP…RB is a frontier node. Our frontier node assumption makes sense in tree-based translation model. This is because with the purpose of achieving better rule coverage, we tend to extract small minimal rules as many as possible and generate larger rules by composing them. Maximizing the number of frontier nodes supports this goal, while producing many interior (non-frontier) nodes hinders it (DeNero and Klein, 2007). Hence, in the forest constructor, we follow this assumption and only consider the tree structures with the largest number of frontier nodes. Denero and Uszkoreit (2011) utilized a similar heuristic to construct their unlabeled trees. They required that all spans in their trees must align continuously to the other side. Unlike their heuristic, our frontier node assumption only maximizes the number of frontier nodes. The interior nodes are also permitted in the tree structure, which is more flexible and appropriate for constructing forests. Today we meet again , jin-tian wo-men zai-ci jian-mia"
C12-1186,D11-1018,0,0.0888056,"Missing"
C12-1186,P05-1067,0,0.0684606,"significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-ba"
C12-1186,P03-2041,0,0.0968446,"pervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees."
C12-1186,P06-1121,0,0.456171,"ey are very beneficial for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence w"
C12-1186,N04-1035,0,0.70857,"plify the space, we take advantage of the following assumption during forest construction: Frontier Node Assumption: The more frontier nodes the tree structure has, the more reasonable it is for translation. The heuristic values in the constraints are chosen by a series of survey and experiments on a well‐aligned corpus. ？ ！} and {. , : ; ? !} as split anchors for Chinese and English, respectively. We take the position before and after the punctuations as split point candidates. We use {。 ， 3041 Frontier nodes are utilized to factor a tree structure into several fragments for rule extraction (Galley et al., 2004). Formally, a frontier node is a node that meets the following constraint: the span of the node and its dominated span at the other side are consistent with word alignment 6 . For example, in FIGURE 2, node PRP…RB’s span is {we meet again} and it dominates span {我 们 再次 见面} at the other side. These two spans are consistent with word alignment. Therefore, node PRP…RB is a frontier node. Our frontier node assumption makes sense in tree-based translation model. This is because with the purpose of achieving better rule coverage, we tend to extract small minimal rules as many as possible and generat"
C12-1186,N04-1014,0,0.109625,"in each derivation d in the set of all derivations D. That is p (t ei , f i , ai )    p ( r ) D rd To get the derivation set D, we employ the algorithm of Mi et al., (2008a) to transform our induced packed forests into synchronous derivation forests. Practically, in order to reduce the complexity of the derivation forest, we only utilize the minimal STSG translation rules extracted by the method of Galley et al., (2004) and Mi et al., (2008b) to construct derivation forests 11 . Using the synchronous derivation forests, the rule probabilities are estimated by the insideoutside algorithm (Graehl and Knight, 2004). Here, leaf(r) and root(r) denote the leaf nonterminals and root node of rule r respectively. The inside and outside probabilities of forest node N are defined as follows, p IN ( N )  pOUT ( N )    rR ( N )    p IN ( N l )   p (r )  N l  leaf ( r )      p IN ( N l )   p ( r )  pOUT ( root ( r ))   r : N leaf ( r )  N l  leaf ( r ) { N }  where R(N) denotes the set of matched rules rooted at node N. Therefore, the process of EM algorithm is shown as follows: )n the triple, te refers to the target tree structures, f denotes the source language sentences, and a is t"
C12-1186,P07-1019,0,0.113058,"Missing"
C12-1186,W06-3601,0,0.456543,"tring-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees"
C12-1186,N03-1017,0,0.0727375,"Missing"
C12-1186,W04-3250,0,0.0277905,"aining about 7.1 million Chinese words and 9.2 million English words. We generate the final symmetric word alignment using GIZA++ and the grow-diag-final-and balance strategy. We train a 5-gram language model on the target part of the training corpus and the Xinhua portion of English Gigaword corpus. We use the NIST MT 2003 evaluation data as the development set, and adopt NIST MT04 and MT05 as the test set. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). Our baseline system is an in-house string-to-tree system (named s2t) based on Galley et al. (2006) and Marcu et al. (2006). The English side of the training corpus is parsed with Berkeley parser (Petrov et al., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees a"
C12-1186,P07-2045,0,0.00886089,"Missing"
C12-1186,P09-4007,0,0.0141116,"., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees and apply the achieved binary trees to another string-to-tree system (abbreviated as s2t-hb) with the same settings of s2t. In addition, we also run the state-of-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. For inducing our unsupervised tree structures, we use Urheen 12 to get the POS tags of the English corpus. Just as we described in section 3.1.1, we reuse GIZA++ and the grow-diag-final-and strategy to re-align words based on the sub-sentence pairs and then combine the alignment result together to get a new word alignment for the whole sentence pair. We perform the EM algorithm to capture the final tree structures by 20 iterations. Then we build a string-to-tree system using our induced unsupervised tree structures (abbreviated as s2t-IT). Different from the above http://www.o"
C12-1186,D12-1078,0,0.112585,"d method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to do joint parsing and alignment. They utilized the bilingual Treebank to train a joint model and achieved better results on both parsing and word alignment. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Our work is different from theirs in that we are pursuing better unsupervised tree structures for better translation performance. As a whole, compared with previous works, our unsupervised trees are generated fully depending on word alignment. Therefore, by using our tree structures, the incompatibility problem between tree structures and word alignment can be well resolved. 3 Packed Forest Generation In this section, we introduce how to compress all the reasonable tree structures into a packed forest for the given flat se"
C12-1186,P06-1077,0,0.348814,"e shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a"
C12-1186,P09-1063,0,0.442132,"trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-based translation models, parse trees are essential to extrac"
C12-1186,W06-1606,0,0.335873,"l for the translation between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule e"
C12-1186,D08-1022,0,0.0389908,"Missing"
C12-1186,P08-1023,0,0.169254,"supervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the al"
C12-1186,P03-1021,0,0.0851695,"Missing"
C12-1186,P02-1040,0,0.0905556,"Missing"
C12-1186,P06-1055,0,0.0901048,"n the target part of the training corpus and the Xinhua portion of English Gigaword corpus. We use the NIST MT 2003 evaluation data as the development set, and adopt NIST MT04 and MT05 as the test set. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). Our baseline system is an in-house string-to-tree system (named s2t) based on Galley et al. (2006) and Marcu et al. (2006). The English side of the training corpus is parsed with Berkeley parser (Petrov et al., 2006). We extract the minimal GHKM rules (Galley et al., 2004) and the rules of SPMT Model 1 (Marcu et al., 2006) with phrases up to length L=5 on the source side. Then we extract the composed rules by composing two or three adjacent minimal GHKM rules (Galley et al., 2006). The beam size of the decoder is set as 500. We further implement head binarization on the English parse trees and apply the achieved binary trees to another string-to-tree system (abbreviated as s2t-hb) with the same settings of s2t. In addition, we also run the state-of-the-art hierarchical phrase-based system Joshua (Li et al"
C12-1186,P05-1034,0,0.129712,"rimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree tra"
C12-1186,P08-1066,0,0.0823393,"n between resource-poor languages. Our experimental results have shown that the string-to-tree translation system using our unsupervised trees significantly outperforms the stringto-tree system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE"
C12-1186,P09-1009,0,0.0444789,"Missing"
C12-1186,D07-1078,0,0.315679,"to induce unlabeled tree structures for syntactic pre-reordering. Different from above works, we concentrate on producing effective and labeled unsupervised trees for tree-based translation models. Moreover, since most of the current tree-based translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective"
C12-1186,J10-2004,0,0.114521,"Missing"
C12-1186,P06-1066,0,0.0309837,"ng yi da bu different . xiang-tong le (a) ROOT . NN... 0 , 1 NN...RB 2 .4 3 CC...JJ PRP...RB NN+PRP VBP+RB 0 NN Today jin-tian 今天 1 PRP 2 we wo-men 我们 VBP meet zai-ci 再次 3 RB 4 again , CC DT NN VBZ but the situation is jian-mian , qing-xing , 见面 情形 yi 已 da 大 RB JJ quite different . bu xiang-tong le 。 不 相同 了。 (b) FIGURE 3 – (a) An example of bilingual sentence segmentation. (b) The ultimate packed forest of the example sentence pair in (a). 3.2 Node Labeling To create packed forests for sentences, a problem that must be resolved is how to label the forest nodes without any syntactic knowledge. Xiong et al. (2006) showed that the boundary word of a phrase is a very effective indicator for phrase reordering. Zollmann and Vogel (2011) labeled hierarchical rules with word classes of boundary words and achieved better translation A node’s dominated span at the other side refers to the minimum continuous span covering all the words that are reachable from the node via word alignment. Two spans are consistent with word alignment means that words in one span only align to words in the other span via word alignment, and vice versa. 3042 performance. Inspired by their work, we combine word classes of boundary w"
C12-1186,2011.mtsummit-papers.29,1,0.680435,"Missing"
C12-1186,P11-1084,0,0.441569,"translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to"
C12-1186,N06-1033,0,0.178101,"Missing"
C12-1186,P09-1020,0,0.1385,"Missing"
C12-1186,D11-1019,1,0.914813,"translation models are based on synchronous tree substitution grammar (STSG), our unsupervised trees are thus learned according to STSG, rather than SCFG. On relieving the incompatibility problem between tree structures and word alignment for translation, previous works mainly focus on two directions: One direction is to adapt the parse tree structure. Wang et al., (2007) binarized the parse trees and adopted an EM algorithm to select the best binary tree from their parallel binarization forest. Mi et al., (2008b) and Liu et al., (2009) compressed thousands of parse trees into packed forests. Zhang et al. (2011a) applied a CKY binarization method on parse trees to get binary forests for forest-to-string model. Burkett and Klein (2012) adopted a transformation-based method to learn a sequence of monolingual tree transformations for translation. They differ from our work in that 3039 they were all based on parse trees. Compared with them, we construct effective unsupervised tree structures according to the word alignment and do not need any syntactic resource. The other direction is to integrate the alignment information into parsing. Burkett and Klein (2008) and Burkett et al. (2010) made efforts to"
C12-1186,Y09-2016,1,0.891605,"Missing"
C12-1186,2007.mtsummit-papers.71,0,0.814658,"Missing"
C12-1186,P08-1064,0,0.157759,"e system using parse trees. KEYWORDS : Tree-based translation; Unsupervised tree; EM algorithm. Proceedings of COLING 2012: Technical Papers, pages 3037–3054, COLING 2012, Mumbai, December 2012. 3037 1 Introduction Recently, tree-based models 1 have been widely studied in statistical machine translation (SMT). The existing tree-based models include string-to-tree models (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008), tree-to-string models (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006;), and tree-to-tree models (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). Due to the effective use of syntactic information, tree-based models have achieved comparable (Liu et al., 2009) and even better performance over phrase-based models (Marcu et al., 2006). target sentence parser target parse tree (a) target sentence word alignment target unsuperveised tree source sentence rule extraction word alignment source sentence word alignment rule extraction (b) FIGURE 1 – Rule extraction for string-to-tree translation model: (a) using parse trees versus (b) using our unsupervised trees. In the existing tree-based translation models, parse trees are"
C12-1186,W06-3119,0,0.171084,"Missing"
C12-1186,P11-1001,0,0.109517,"ay jin-tian 今天 1 PRP 2 we wo-men 我们 VBP meet zai-ci 再次 3 RB 4 again , CC DT NN VBZ but the situation is jian-mian , qing-xing , 见面 情形 yi 已 da 大 RB JJ quite different . bu xiang-tong le 。 不 相同 了。 (b) FIGURE 3 – (a) An example of bilingual sentence segmentation. (b) The ultimate packed forest of the example sentence pair in (a). 3.2 Node Labeling To create packed forests for sentences, a problem that must be resolved is how to label the forest nodes without any syntactic knowledge. Xiong et al. (2006) showed that the boundary word of a phrase is a very effective indicator for phrase reordering. Zollmann and Vogel (2011) labeled hierarchical rules with word classes of boundary words and achieved better translation A node’s dominated span at the other side refers to the minimum continuous span covering all the words that are reachable from the node via word alignment. Two spans are consistent with word alignment means that words in one span only align to words in the other span via word alignment, and vice versa. 3042 performance. Inspired by their work, we combine word classes of boundary words to label forest nodes. We divide the non-leaf forest nodes into three groups: one-word node, dominating only one wor"
C12-1186,N10-1016,0,\N,Missing
C14-1039,P12-2005,0,0.048869,"Missing"
C14-1039,P05-1033,0,0.234968,"Missing"
C14-1039,2011.eamt-1.28,0,0.0246839,"ame time, all matched TM phrase-pairs are dynamically merged into the phrase table. Moreover, this is the first unified framework for integrating TM into SMT at decoding when the TM database and the SMT training set are different. Although some previous works of the second and third categories can be also applied when the TM database and the SMT training set are different, they did not explicitly focus on and test this case. Last, since the example-based machine translation (EBMT, [Nagao, 1984]) is similar to that of using TM, some approaches (Watanabe and Sumita, 2003; Smith and Clark, 2009; Dandapat et al., 2011; 2012; Phillips, 2011) also combined EBMT with SMT. It would be interesting to compare our approaches with theirs in the future. 6 Conclusion Combining TM and SMT can greatly improve the translation performance and reduce human postediting effort. In comparison with those previous approaches, our work makes the following contributions: (1) Dynamically merge the matched TM phrase-pairs into the SMT phrase table to meet the real application; (2) Propose an improved integrated model to distinguish the original SMT phrase-pairs from the newly-added ones extracted from TM; (3) Adopt a simple but e"
C14-1039,W12-0106,0,0.0374585,"Missing"
C14-1039,N13-3003,0,0.0324372,"Missing"
C14-1039,P10-1064,0,0.441066,"Missing"
C14-1039,C10-2043,0,0.430716,"Missing"
C14-1039,2011.mtsummit-papers.52,0,0.0532933,"Missing"
C14-1039,W04-3250,0,0.264782,"Missing"
C14-1039,N03-1017,0,0.0143902,"ted as SMT) are adopted as our two baseline systems. Following (Wang et al., 2013), for TM, the word-based fuzzy match score is adopted as the similarity measure; also, for the phrasebased SMT system, the same Moses toolkit (Koehn et al., 2007) and the same set of following features are adopted: the phrase translation model, the language model, the distance-based reordering model, the lexicalized reordering model and the word penalty. The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use SRI Language Model 401 New TM Database SMT Training Set #Sentences 130,953 130,953 #Chn. Words 1,808,992 1,814,524 #Chn. VOC. 30,164 29,792 #Eng. Words 1,811,413 1,815,615 #Eng. VOC. 30,807 30,516 Table 1: Corpus Statistics for In-Domain Tests Intervals #Sentences #Words W/S [0.9, 1.0) 147 2,431 16.5 [0.8, 0.9) 255 3,438 13.5 [0.7, 0.8) 244 3,299 13.5 [0.6, 0.7) 355 4,674 13.2 [0.5, 0.6) 488 6,125 12.6 [0.4, 0.5) 514 7,525 14.6 [0.3, 0.4) 419 7,082 16.9 (0.0, 0.3) 154 4,074 26.5 (0.0, 1.0) 2,576 38,648 15.0 Table 2: Corpus Statistics for In-Domain Te"
C14-1039,2010.jec-1.4,0,0.62381,"thermore, the proposed approaches are significantly better than the TM, the SMT and previous integration works for both in-domain and cross-domain tests. 1 Introduction Since the translation memory (TM) system and the statistical machine translation (SMT) system complement each other in those matched sub-segments and unmatched sub-segments (Wang et al., 2013), combining them can improve the output quality significantly, especially when high-similarity fuzzy matches are available. Therefore, combining TM and SMT is drawing more and more attention in recent years (He et al., 2010a; 2010b; 2011; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Ma et al., 2011; Dara et al., 2013; Wang et al., 2013). Those previous works on combining TM and SMT can be classified into four categories: (1) selecting the better translation sentence from TM and SMT (He et al., 2010a; 2010b; Dara et al., 2013); (2) incorporating TM matched sub-segments into SMT in a pipelined manner (Koehn and Senellart, 2010; He et al., 2011; Ma et al., 2011); (3) only enhancing the SMT phrase table with new TM phrase-pairs (Biçici and Dymetman, 2008; Simard and Isabelle, 2009); and (4) incorporating the associated TM information with eac"
C14-1039,P11-1124,0,0.56858,"Missing"
C14-1039,P03-1021,0,0.0168466,"Missing"
C14-1039,J03-1002,0,0.00500548,"this work, the translation memory system (denoted as TM) and the phrase-based machine translation system (denoted as SMT) are adopted as our two baseline systems. Following (Wang et al., 2013), for TM, the word-based fuzzy match score is adopted as the similarity measure; also, for the phrasebased SMT system, the same Moses toolkit (Koehn et al., 2007) and the same set of following features are adopted: the phrase translation model, the language model, the distance-based reordering model, the lexicalized reordering model and the word penalty. The system configurations are as follows: GIZA++ (Och and Ney, 2003) is used to obtain the bidirectional word alignments. Afterwards, “intersection” refinement (Koehn et al., 2003) is adopted to extract phrase-pairs. We use SRI Language Model 401 New TM Database SMT Training Set #Sentences 130,953 130,953 #Chn. Words 1,808,992 1,814,524 #Chn. VOC. 30,164 29,792 #Eng. Words 1,811,413 1,815,615 #Eng. VOC. 30,807 30,516 Table 1: Corpus Statistics for In-Domain Tests Intervals #Sentences #Words W/S [0.9, 1.0) 147 2,431 16.5 [0.8, 0.9) 255 3,438 13.5 [0.7, 0.8) 244 3,299 13.5 [0.6, 0.7) 355 4,674 13.2 [0.5, 0.6) 488 6,125 12.6 [0.4, 0.5) 514 7,525 14.6 [0.3, 0.4) 4"
C14-1039,P02-1040,0,0.0894804,"Missing"
C14-1039,2009.mtsummit-papers.14,0,0.886927,"more attention in recent years (He et al., 2010a; 2010b; 2011; Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; Ma et al., 2011; Dara et al., 2013; Wang et al., 2013). Those previous works on combining TM and SMT can be classified into four categories: (1) selecting the better translation sentence from TM and SMT (He et al., 2010a; 2010b; Dara et al., 2013); (2) incorporating TM matched sub-segments into SMT in a pipelined manner (Koehn and Senellart, 2010; He et al., 2011; Ma et al., 2011); (3) only enhancing the SMT phrase table with new TM phrase-pairs (Biçici and Dymetman, 2008; Simard and Isabelle, 2009); and (4) incorporating the associated TM information with each source phrase to guide the SMT decoding (Wang et al., 2013). However, all previous works mentioned above only focus on the case in which the TM database and the SMT training set share the same data-set. Nonetheless, in real applications, the TM database will deviate from the SMT training set when time goes by, because the TM database will be dynamically enlarged when more translations are generated by the human translator. Therefore, this paper will concentrate on a more realistic case, in which the TM database and the SMT trainin"
C14-1039,2006.amta-papers.25,0,0.0872782,"Missing"
C14-1039,W10-3806,0,0.705605,"Missing"
C14-1039,D13-1050,0,0.0152337,"e training set (out). Therefore, the factor ( test set will possess a different probability distribution in comparison with that from the training set. However, the development set is not big enough (only a few hundreds sentence-pairs at each interval) to re-train all TM factors of the proposed model. Therefore, we simply add the following h1 feature to reflect the tendency of having high translation consistency in the development set: (   ) { Where  and  denote the source phrase, the target candidate, respectively. Furthermore, various source synonyms might generate the same translation (Zhu et al., 2013). Therefore, even SCM≠Same, we still favor the SMT phrase-pair candidate which exactly matches TM target phrase. For example, if source words are synonyms such as “需要” (want) and “要” (want), “如 果” (if) and “若” (if), “立即” (at once) and “马上” (at once), the target translations would be the same. Therefore, the issue of having high translation consistency in the technical domain is also applied. We thus further add the following h2 feature to reflect the tendency of having high translation consistency in this case (“High” and “Low” are grouped into “Other” for the SCM): (   ) { Afterwards, the a"
C14-1039,P07-2045,0,\N,Missing
C14-1039,P13-1002,1,\N,Missing
C16-1020,J07-4004,0,0.0911108,"Missing"
C16-1020,J07-3004,0,0.0324898,"Missing"
C16-1020,N16-1030,0,0.03397,"each token in a sequence. Typical examples include part-of-speech (POS) tagging and combinatory category grammar (CCG) supertagging. A regular feature of sequential tagging is that the input tokens in a sequence cannot be assumed to be independent since the same token in different contexts can be assigned to different tags. Therefore, the classifier should have memories to remember the contexts to make a correct prediction. Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016). The horizontal hierarchy of LSTMs with bidirectional processing can remember the long-range dependencies without affecting the short-term storage. Although the models have a deep horizontal hierarchy (the depth is the same as the sequence length), the vertical hierarchy is often shallow, which may not be efficient at representing each token. Stacked LSTMs are deep in both directions, but become harder to train due to the feed-forward structure of stacked layers. Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections across different layers ("
C16-1020,Q14-1026,0,0.0296003,"Missing"
C16-1020,N16-1026,0,0.0273145,"Missing"
C16-1020,J93-2004,0,0.0548993,"Missing"
C16-1020,D14-1162,0,0.0802863,"Missing"
C16-1020,P11-2009,0,0.0467262,"Missing"
C16-1020,N16-1027,0,0.0450828,"ng discrete labels to each token in a sequence. Typical examples include part-of-speech (POS) tagging and combinatory category grammar (CCG) supertagging. A regular feature of sequential tagging is that the input tokens in a sequence cannot be assumed to be independent since the same token in different contexts can be assigned to different tags. Therefore, the classifier should have memories to remember the contexts to make a correct prediction. Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016). The horizontal hierarchy of LSTMs with bidirectional processing can remember the long-range dependencies without affecting the short-term storage. Although the models have a deep horizontal hierarchy (the depth is the same as the sequence length), the vertical hierarchy is often shallow, which may not be efficient at representing each token. Stacked LSTMs are deep in both directions, but become harder to train due to the feed-forward structure of stacked layers. Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections acr"
C16-1020,N16-1025,0,0.0389185,"Missing"
C18-1035,W17-5310,0,0.0146148,"h intra-sentence attention (Parikh et al., 2015) (9) Binary Tree-LSTM + Structured Attention & Composition + dual-attention (Zhao et al., 2016) (10) 300D Full tree matching NTI-SLSTM-LSTM w/ global attention (Munkhdalai and Yu, 2016) (11) 300D Syntactic Tree-LSTM (Chen et al., 2017a) Human Performance (Gong et al., 2017) Our model Training Acc. 84.4 99.7 Test Acc. 83.3 89.2 85.3 92.0 89.5 82.1 83.2 83.5 86.1 86.3 90.5 86.8 87.7 87.2 88.5 87.3 92.9 97.2 90.3 87.8 87.7 87.4 77.6 78.2 Table 1. Performance on SNLI Model Matched Test Acc. Mismatched (1)BiLSTM (Williams et al., 2018) (2) Inner Att (Balazs et al., 2017) 67.0 72.1 67.6 72.1 (3) ESIM (Williams et al., 2018) (4) Gated-Att BiLSTM (Chen et al., 2017b) (5) Shortcut-Stacked encoder (Nie & Bansal, 2017) (6) DIIN (Gong et al., 2017) (7) Inner Att (ensemble) (Balazs et al., 2017) (8) Gated-Att BiLSTM (ensemble) (Chen et al., 2017b) (9) DIIN (ensemble) (Gong et al., 2017) Human Performance (Gong et al., 2017) Our model 72.3 73.2 74.6 78.8 72.2 74.9 80.0 88.5 75.1 72.1 73.6 73.6 77.8 72.8 74.9 78.7 89.2 74.7 Table 2. Performance on MultiNLI 3.2 Details of training In order to initialize the words in the triplets, we used 300 dimensional Glove embedding"
C18-1035,H05-1079,0,0.176455,"Missing"
C18-1035,D15-1075,0,0.295914,"006), information extraction (Romano et al., 2006), machine translation (Pado et al., 2009), automatic text summarization (Harabagiu et al., 2007) and so on. Some evaluations about this task have been organized in the past decades, such as the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005), SemEval-2014 (Marelli et al., 2014) and RITE (Shima et al., 2011). Many previous approaches adopt statistical frameworks (Heilman et al., 2010; Kouylekov and Magnini, 2005). However, neural network approaches have emerged after Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) was released. Most of them adopt an increasingly complicated network structure to represent text passages, and then predict the relationship between them (Bowman et al., 2016; Liu et al., 2016b). However, P might include extra words which are not directly related to H. Actually, only the words in P that are associated with the words in H should be paid attention to. Those relevant words should be emphasized more while the irrelevant words should be less weighted during decision making. Therefore, some approaches (Parikh et al., 2016; Chen et al., 2017a) adopt attention mechanism to implicitly"
C18-1035,P16-1139,0,0.217963,"bout this task have been organized in the past decades, such as the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005), SemEval-2014 (Marelli et al., 2014) and RITE (Shima et al., 2011). Many previous approaches adopt statistical frameworks (Heilman et al., 2010; Kouylekov and Magnini, 2005). However, neural network approaches have emerged after Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) was released. Most of them adopt an increasingly complicated network structure to represent text passages, and then predict the relationship between them (Bowman et al., 2016; Liu et al., 2016b). However, P might include extra words which are not directly related to H. Actually, only the words in P that are associated with the words in H should be paid attention to. Those relevant words should be emphasized more while the irrelevant words should be less weighted during decision making. Therefore, some approaches (Parikh et al., 2016; Chen et al., 2017a) adopt attention mechanism to implicitly align the words between two passages to yield a better performance. This idea is very similar to how human make the entailment judgment, and the result shows that it is very"
C18-1035,P17-1152,0,0.22122,"ral Language Inference (SNLI) dataset (Bowman et al., 2015) was released. Most of them adopt an increasingly complicated network structure to represent text passages, and then predict the relationship between them (Bowman et al., 2016; Liu et al., 2016b). However, P might include extra words which are not directly related to H. Actually, only the words in P that are associated with the words in H should be paid attention to. Those relevant words should be emphasized more while the irrelevant words should be less weighted during decision making. Therefore, some approaches (Parikh et al., 2016; Chen et al., 2017a) adopt attention mechanism to implicitly align the words between two passages to yield a better performance. This idea is very similar to how human make the entailment judgment, and the result shows that it is very effective for performing natural language inference on SNLI corpus in which most words in H can find their corresponding ones in P.  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 414 Proceedings of the 27th International Conference on Computational Linguistics, pages 414–425 Sant"
C18-1035,W17-5307,0,0.116303,"ral Language Inference (SNLI) dataset (Bowman et al., 2015) was released. Most of them adopt an increasingly complicated network structure to represent text passages, and then predict the relationship between them (Bowman et al., 2016; Liu et al., 2016b). However, P might include extra words which are not directly related to H. Actually, only the words in P that are associated with the words in H should be paid attention to. Those relevant words should be emphasized more while the irrelevant words should be less weighted during decision making. Therefore, some approaches (Parikh et al., 2016; Chen et al., 2017a) adopt attention mechanism to implicitly align the words between two passages to yield a better performance. This idea is very similar to how human make the entailment judgment, and the result shows that it is very effective for performing natural language inference on SNLI corpus in which most words in H can find their corresponding ones in P.  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 414 Proceedings of the 27th International Conference on Computational Linguistics, pages 414–425 Sant"
C18-1035,P06-1114,0,0.0404474,"ke the judgment more interpretable. Experimental results show that the performance of our approach is better than most of the approaches that use tree structures, and is comparable to other stateof-the-art approaches. 1 Introduction Natural language inference (NLI) refers to the following task: given a text passage P (Premise) (which might have more than one sentence) and a text passage H (Hypothesis), whether we can infer H from P, i.e., identifying a specific relationship among entailment, neutral and contradiction. It has many applications such as question answering (Bhaskar et al., 2013; Harabagiu and Hickl, 2006), information extraction (Romano et al., 2006), machine translation (Pado et al., 2009), automatic text summarization (Harabagiu et al., 2007) and so on. Some evaluations about this task have been organized in the past decades, such as the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005), SemEval-2014 (Marelli et al., 2014) and RITE (Shima et al., 2011). Many previous approaches adopt statistical frameworks (Heilman et al., 2010; Kouylekov and Magnini, 2005). However, neural network approaches have emerged after Stanford Natural Language Inference (SNLI) dataset (Bowm"
C18-1035,N10-1145,0,0.104047,"Missing"
C18-1035,S14-2055,0,0.0126783,"rresponding cell in Figure 4. Similarly, the corresponding cell for (nsubj, sits, man) of P and (vmod, man, sitting) of H shows a darker color, which also meets human judgment. This clearly shows that the alignment weights between these two triplet sets reflect the human interpretation closely. 4 Related Work Early approaches for natural language inference usually adopted statistical models such as SVM (Joachims, 1998), CRF (Hatoriet et al., 2009) and so on, which employed hand-crafted features, and utilized various external resources and specialized sub-components such as negation detection (Lai and Hockenmaier, 2014; Levy et al., 2013). Besides, all the adopted datasets are very small. After the SNLI corpus (Bowman et al., 2015) was released, a lot of work about natural language inference based on neural networks have been published in recent years (Bowman et al., 2016; Liu et al., 2016; Liu et al., 2016b; Munkhdalai and Yu, 2016; Mou et al., 2015; Sha et al., 2016). Basically, those neural network based approaches could be classified into 2 categories: (1) Merely computing the passage-embedding without introducing alignment (between the words in the sentences), and then comparing these passage-embedding"
C18-1035,P16-1098,0,0.0395729,"Missing"
C18-1035,P16-2022,0,0.135815,"t of this corpus, it selects half of the genres to create in-domain (matched) and outdomain (mismatched) development/test sets. Since the test set labels of this corpus are not released, the test performance is obtained through submission to Kaggle.com6. 4 https://nlp.stanford.edu/projects/snli/ http://www.nyu.edu/projects/bowman/multinli/ 6 Matched : https://www.kaggle.com/c/multinli-matched-open-evaluation ; Mismatched: https://www.kaggle.com/c/multinlimismatched-open-evaluation 5 419 Model (1) LSTM (Bowman et al., 2015) (2) Classifier (Bowman et al., 2015) (3) 300D tree-based CNN encoders (Mou et al., 2016) (4) 300D SPINN-PI encoders (Bowman et al. 2016) (5) 100D LSTMs w/ word-by-word attention (Rocktaschel et al., 2015) (6) 300D mLSTM word-by-word attention model (Wang & Jiang, 2016) (7) 200D decomposable attention model (Parikh et al., 2015) (8) 200D decomposable attention model with intra-sentence attention (Parikh et al., 2015) (9) Binary Tree-LSTM + Structured Attention & Composition + dual-attention (Zhao et al., 2016) (10) 300D Full tree matching NTI-SLSTM-LSTM w/ global attention (Munkhdalai and Yu, 2016) (11) 300D Syntactic Tree-LSTM (Chen et al., 2017a) Human Performance (Gong et al.,"
C18-1035,W17-5308,0,0.0266904,"0) 300D Full tree matching NTI-SLSTM-LSTM w/ global attention (Munkhdalai and Yu, 2016) (11) 300D Syntactic Tree-LSTM (Chen et al., 2017a) Human Performance (Gong et al., 2017) Our model Training Acc. 84.4 99.7 Test Acc. 83.3 89.2 85.3 92.0 89.5 82.1 83.2 83.5 86.1 86.3 90.5 86.8 87.7 87.2 88.5 87.3 92.9 97.2 90.3 87.8 87.7 87.4 77.6 78.2 Table 1. Performance on SNLI Model Matched Test Acc. Mismatched (1)BiLSTM (Williams et al., 2018) (2) Inner Att (Balazs et al., 2017) 67.0 72.1 67.6 72.1 (3) ESIM (Williams et al., 2018) (4) Gated-Att BiLSTM (Chen et al., 2017b) (5) Shortcut-Stacked encoder (Nie & Bansal, 2017) (6) DIIN (Gong et al., 2017) (7) Inner Att (ensemble) (Balazs et al., 2017) (8) Gated-Att BiLSTM (ensemble) (Chen et al., 2017b) (9) DIIN (ensemble) (Gong et al., 2017) Human Performance (Gong et al., 2017) Our model 72.3 73.2 74.6 78.8 72.2 74.9 80.0 88.5 75.1 72.1 73.6 73.6 77.8 72.8 74.9 78.7 89.2 74.7 Table 2. Performance on MultiNLI 3.2 Details of training In order to initialize the words in the triplets, we used 300 dimensional Glove embedding (Pennington et al., 2014). For the relation vectors (the dimension is set to 20), we use a standard normal distribution to randomly initialize th"
C18-1035,D16-1244,0,0.179938,"d after Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) was released. Most of them adopt an increasingly complicated network structure to represent text passages, and then predict the relationship between them (Bowman et al., 2016; Liu et al., 2016b). However, P might include extra words which are not directly related to H. Actually, only the words in P that are associated with the words in H should be paid attention to. Those relevant words should be emphasized more while the irrelevant words should be less weighted during decision making. Therefore, some approaches (Parikh et al., 2016; Chen et al., 2017a) adopt attention mechanism to implicitly align the words between two passages to yield a better performance. This idea is very similar to how human make the entailment judgment, and the result shows that it is very effective for performing natural language inference on SNLI corpus in which most words in H can find their corresponding ones in P.  This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 414 Proceedings of the 27th International Conference on Computational Linguistics,"
C18-1035,D14-1162,0,0.08582,"We define ^ : = ( , , ⋯ , ) and ^ : = (ℎ , ℎ , ⋯ , ℎ ) be two sets of RHD triplets, while and ℎ denote the RHD triplet and the RHD triplet in P and H, respectively; also, m and n indicate the number of associated triplets in P and H, respectively. We then instantiate the Input Layer with the corresponding word-embeddings and rel-embedding of RHD triplets (For conciseness, we will let rel/head/dep denote both the original meaning and the corresponding embedding interchangeably from now on). Each ℎ , ∈ is a word embedding of dimension which is initialized with pre-trained GloVe word embedding (Pennington et al., 2014), while ∈ is a relation embedding vector of dimension and is initialized randomly with a standard normal distribution (Please note, only will be tuned later during training). Each triplet-embedding will be presented as a triplet which contains three embedding corresponding to rel, head and dep respectively. 2.2 Network Architecture 2.2.1 Triplet Embedding Layer As we fix the value of word embedding during training, in order to obtain better relation/word embedding representations to compare for this task, we use a simple feed-forward structure to adapt the three parts of the triplet to the tas"
C18-1035,N15-2020,0,0.0529031,"Missing"
C18-1035,N16-1170,0,0.022258,"leased, the test performance is obtained through submission to Kaggle.com6. 4 https://nlp.stanford.edu/projects/snli/ http://www.nyu.edu/projects/bowman/multinli/ 6 Matched : https://www.kaggle.com/c/multinli-matched-open-evaluation ; Mismatched: https://www.kaggle.com/c/multinlimismatched-open-evaluation 5 419 Model (1) LSTM (Bowman et al., 2015) (2) Classifier (Bowman et al., 2015) (3) 300D tree-based CNN encoders (Mou et al., 2016) (4) 300D SPINN-PI encoders (Bowman et al. 2016) (5) 100D LSTMs w/ word-by-word attention (Rocktaschel et al., 2015) (6) 300D mLSTM word-by-word attention model (Wang & Jiang, 2016) (7) 200D decomposable attention model (Parikh et al., 2015) (8) 200D decomposable attention model with intra-sentence attention (Parikh et al., 2015) (9) Binary Tree-LSTM + Structured Attention & Composition + dual-attention (Zhao et al., 2016) (10) 300D Full tree matching NTI-SLSTM-LSTM w/ global attention (Munkhdalai and Yu, 2016) (11) 300D Syntactic Tree-LSTM (Chen et al., 2017a) Human Performance (Gong et al., 2017) Our model Training Acc. 84.4 99.7 Test Acc. 83.3 89.2 85.3 92.0 89.5 82.1 83.2 83.5 86.1 86.3 90.5 86.8 87.7 87.2 88.5 87.3 92.9 97.2 90.3 87.8 87.7 87.4 77.6 78.2 Table 1. Pe"
C18-1035,N18-1101,0,0.0590292,"in two different directions). We then concatenate them and use a multi-layer perceptron classifier Q (it has two hidden layers with Relu activation and a softmax output layer) to generate the final overall judgment vector ˆ (as shown in Eq. (8)), where ˆ ∈ (C equals the number of classes) are the scores for each class. The predicted class can be got by setting = argmax ˆ . When we train the model, we use multi-class cross-entropy loss with dropout regularization (Srivastava et al., 2014). ˆ = ([ 3 3.1 ; ]) (8) Experiments Dataset We adopt both SNLI (Bowman et al., 2015) corpus4 and MultiNLI (Williams et al., 2018) corpus5 to test the performance. They are briefly introduced as follows. SNLI - It contains 570k sentence pairs. The sentence pairs in this corpus are labelled with one of the following relationships: entailment, contradiction, neutral and “-”, where “-” means that it lacks of consensus from human annotators. In our experiments, we follow Bowman et al. (2015) to delete those sentence pairs labelled with “-“. Consequently, we end up with 549,367 pairs for training, 9,842 pairs for development and 9,824 pairs for testing. MultiNLI - This corpus has 433k sentence pairs, which are collected from"
C18-1035,C16-1212,0,0.0431901,"Missing"
C18-1035,S14-2044,0,0.0646143,"Missing"
C18-1079,P14-2009,0,0.0579872,"any other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in a document. In addi"
C18-1079,D17-1054,0,0.0765045,"pite the success of methods mentioned above, they typically only use text information. Two kinds of important information are ignored: users and overall ratings of reviews. The results of our statistical analysis are convincing that the two factors have strong correlations with aspect ratings (Section 2). As for users, different users may care about different aspects. When scoring aspects of a hotel, a business traveler may be critical to service but lenient with price or room. Such preference obviously affects the aspect ratings. Actually, many studies (Tang et al., 2015b; Chen et al., 2016; Dou, 2017) have shown that user preference can boost the performance of a related task, document-level sentiment classification that predicts an overall polarity instead of multi-aspect ratings. For our multi-aspect sentiment classification, the overall rating is given, and it can provide prior information to aspect ratings. Usually, the two types of rating are positive correlation. For example in Figure 1, the overall rating is 4 stars and the aspect ratings are all not less than 4 stars. Inspired by the above analysis, we propose a model called Hierarchical User Aspect Rating Network (HUARN) to consid"
C18-1079,D14-1181,0,0.004517,"ic baseline method, which assigns the majority sentiment category in the training set to aspect rating in the test dataset. OverallRatingSame is also a heuristic baseline method, which assigns the overall rating of a review to its aspect ratings. MajOverallRating splits the training instances into five clusters (per overall rating) and assigns the most frequent rating for the seven aspects per cluster in the test dataset. SVM and NBoW are SVM classifiers with different features. One with unigrams, bigrams as features and another with the mean of word embeddings in a document as features. CNN (Kim, 2014) performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer. HAN (Yang et al., 2016) models review in a hierarchical structure and utilizes an attention mechanism to capture important words and sentences, which is only based on text information and achieves state-of-the-art result in predicting overall rating of document. MHCNN is an extended model of CNN with hierarchical architecture and multi-task framework. MHAN is an extended model of HAN with multi-task framework. DMSCMC (Yin et al., 2017) use iterativ"
C18-1079,P08-2065,1,0.802,"n rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in a document. In addition to these methods, the work of Yin et al. (2017) is the most related to ours, which focuses on using iterative attention mechanism to build discriminative aspect-aware representation to perform document-level multi-aspect sentiment classification. However, it ignores the influences of users and overall ratings o"
C18-1079,P10-1043,0,0.0919064,"sumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in a document. In addition to these methods, the work of Yin et al. (2017) is the most related to ours, which focuses on using iterative attention mechanism to build discriminative aspect-aware representation to perform document-level multi-aspect sentiment classification. However, it ignores the influences of users and overall ratings on aspect ratings."
C18-1079,P14-5010,0,0.00702513,"Missing"
C18-1079,D14-1052,0,0.0211383,"ll rating information into DMSCMC as our future work. 933 6 Related Work Multi-aspect sentiment classification is an extensively studied task in sentiment analysis (Pang and Lee, 2008; Liu, 2012). Lu et al. (2011) propose Segmented Topic Model to model document and extract features, then exploit support vector regression to predict aspect ratings based on these features. McAuley et al. (2012) add a dependency term in final multi-class SVM objective to consider the correction between aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) em"
C18-1079,S14-2004,0,0.0677902,"ion between aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in"
C18-1079,D13-1170,0,0.00821597,"Missing"
C18-1079,D15-1167,0,0.643,"nd overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances. 1 Introduction The ever-increasing popularity of online consumer review platforms, such as Tripadvisor1 and Yelp2 , has led to large amounts of online reviews that are often too numerous for users to analyze. Consequently, there is a growing need for systems analyzing reviews automatically. Lots of approaches (Xia et al., 2011; Socher et al., 2013; Tang et al., 2015a; Yang et al., 2016) usually focus on determining the overall sentiment rating of a review. Actually, not only does a review express the general attitude of reviewer, but it also conveys fine-grained sentiments towards different aspects of corresponding products. Figure 1 shows an example where Bob posts a review about a hotel and gives scores on overall attitude, location, room, and service respectively. The analysis of these aspect ratings could not only benefit mining interested aspects for users, but also help companies better understand the major pros and cons of the product. However, co"
C18-1079,P15-1098,0,0.4894,"nd overall rating information to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances. 1 Introduction The ever-increasing popularity of online consumer review platforms, such as Tripadvisor1 and Yelp2 , has led to large amounts of online reviews that are often too numerous for users to analyze. Consequently, there is a growing need for systems analyzing reviews automatically. Lots of approaches (Xia et al., 2011; Socher et al., 2013; Tang et al., 2015a; Yang et al., 2016) usually focus on determining the overall sentiment rating of a review. Actually, not only does a review express the general attitude of reviewer, but it also conveys fine-grained sentiments towards different aspects of corresponding products. Figure 1 shows an example where Bob posts a review about a hotel and gives scores on overall attitude, location, room, and service respectively. The analysis of these aspect ratings could not only benefit mining interested aspects for users, but also help companies better understand the major pros and cons of the product. However, co"
C18-1079,D16-1021,0,0.0490299,"2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in a document. In addition to these methods, the work of Yin"
C18-1079,P08-1036,0,0.0400167,"on for review. It may alleviate this problem. We leave how to encode user and overall rating information into DMSCMC as our future work. 933 6 Related Work Multi-aspect sentiment classification is an extensively studied task in sentiment analysis (Pang and Lee, 2008; Liu, 2012). Lu et al. (2011) propose Segmented Topic Model to model document and extract features, then exploit support vector regression to predict aspect ratings based on these features. McAuley et al. (2012) add a dependency term in final multi-class SVM objective to consider the correction between aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang"
C18-1079,S16-1045,0,0.0315542,"933 6 Related Work Multi-aspect sentiment classification is an extensively studied task in sentiment analysis (Pang and Lee, 2008; Liu, 2012). Lu et al. (2011) propose Segmented Topic Model to model document and extract features, then exploit support vector regression to predict aspect ratings based on these features. McAuley et al. (2012) add a dependency term in final multi-class SVM objective to consider the correction between aspects. Many other studies (Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory"
C18-1079,D16-1058,0,0.0693896,"Titov and McDonald, 2008; Wang et al., 2010; Wang et al., 2011; Diao et al., 2014; Pappas and Popescu-Belis, 2014; Pontiki et al., 2016; Toh and Su, 2016) solve multiaspect sentiment classification as a subproblem by utilizing heuristic based methods or topic models. However, these approaches often rely on strict assumptions about words and sentences, for example, word syntax has been used to distinguish aspect word or sentiment word, or appending an specific aspect to a sentence. Another related problem is called aspect-level sentiment classification (Pontiki et al., 2014; Dong et al., 2014; Wang et al., 2016; Tang et al., 2016; Schouten and Frasincar, 2016). Wang et al. (2016) and Tang et al. (2016) employ attention-based LSTM and deep memory network for aspect-level sentiment classification, respectively. However, the task is sentence level. Document-level sentiment classification (Li and Zong, 2008; Li et al., 2010; Li et al., 2013; Xia et al., 2015; Yang et al., 2016) is also a related research field because we can treat single aspect sentiment classification as an individual document classification task. However, they did not consider multiple aspects in a document. In addition to these metho"
C18-1079,N16-1174,0,0.55229,"formation to predict aspect ratings of a review. Diverse aspects are treated differently and a multi-task framework is adopted. Empirical results on two real-world datasets show that HUARN achieves state-of-the-art performances. 1 Introduction The ever-increasing popularity of online consumer review platforms, such as Tripadvisor1 and Yelp2 , has led to large amounts of online reviews that are often too numerous for users to analyze. Consequently, there is a growing need for systems analyzing reviews automatically. Lots of approaches (Xia et al., 2011; Socher et al., 2013; Tang et al., 2015a; Yang et al., 2016) usually focus on determining the overall sentiment rating of a review. Actually, not only does a review express the general attitude of reviewer, but it also conveys fine-grained sentiments towards different aspects of corresponding products. Figure 1 shows an example where Bob posts a review about a hotel and gives scores on overall attitude, location, room, and service respectively. The analysis of these aspect ratings could not only benefit mining interested aspects for users, but also help companies better understand the major pros and cons of the product. However, compared with the overa"
C18-1079,D17-1217,0,0.801407,"ods significantly. The code and data for this paper are available at https://github.com/Junjieli0704/HUARN. 2 Data and Observations In this section, we first introduce real-world datasets used in our work and present some explorations about the impacts of user preference and overall ratings on aspect ratings. 2.1 Data We evaluate HUARN on two datasets: TripDMS and TripOUR. They are both crawled from Tripadvisor website and contain seven aspects (value, room, location, cleanliness, check in, service, and business service) which are provided by Tripadvisor website. The first dataset is built by Yin et al. (2017). However, there is no available user information in this dataset, thus we create the second one. Statistics 926 Datasets TripOUR TripDMS #docs 58,632 29,391 #users 1,702 N/A #docs/user 34.44 N/A #words/sen 17.80 18.0 #words/doc 181.03 251.7 Table 1: Statistics of our datasets. The rating scale of TripOUR and TripDMS are 1-5. Datasets TripOUR TripDMS value 43,258 28,778 room 41,295 29,140 location 42,354 23,401 cleanliness 42,601 29,184 check in 1,283 23,373 service 58,449 28,322 business service 801 15,939 Table 2: The absolute number of rating of different aspects in TripOUR and TripDMS. of"
C18-1121,H05-1079,0,0.0828249,"rence is considerable, leading to high ROUGE scores, the summary is invalid. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1430 Proceedings of the 27th International Conference on Computational Linguistics, pages 1430–1441 Santa Fe, New Mexico, USA, August 20-26, 2018. We argue that correctness is an essential requirement for summarization systems, while most existing systems ignore it. Generally, a correct summary is semantically entailed by the source sentence, thus we believe entailment1 (Bos and Markert, 2005) knowledge is beneficial to avoid producing contradictory or unrelated information in the summary. To incorporate entailment knowledge into abstractive summarization models, we propose in this work an entailment-aware encoder and an entailment-aware decoder. We share the encoder of the summarization generation system with the entailment recognition system, so that the encoder can grasp both the gist of the source sentence and be aware of entailment relationships. Furthermore, we propose an entailment Reward Augmented Maximum Likelihood (RAML) (Norouzi et al., 2016) training that encourages the"
C18-1121,D15-1075,0,0.0406554,"MLT 65 21.8 60 21.4 55 1 All words Figure 2: Average count of novel words (words that do not appear in the article). Seq2seq model generates more novel words, but less words are in the reference compared to our model. Source 1 (a) Words in the reference Reference Seq2seq Seq2seq + MTL + ERAML Source 2 Reference Seq2seq Seq2seq + MTL + ERAML Source 3 Reference Seq2seq Seq2seq + MTL + ERAML α 50 5 10 20 30 40 50 60 70 80 90 100 5 10 20 30 40 50 60 70 80 90 100 21 0 α (b) Figure 3: The performance of (a) summarization generation on Gigaword validation set and (b) entailment recognition on SNLI (Bowman et al., 2015) validation set with different task batch switches (α). brazilian stocks rose , led by consumer stocks , after the government said it would n’t impose restraints on consumer credit brazil stocks rise after government rules out credit restraints brazil stocks rise on consumer credit concerns brazil stocks rise after government says it wo n’t impose credit restrictions authorities have denied neo-nazi groups permission to stage a demonstration next week in the austrian capital , where skinheads planned to gather on the ##th anniversary of nazi germany ’s surrender ending world war ii in europe ."
C18-1121,P16-1046,0,0.0295625,"tion, where the proposal distribution is Hamming distance sampling2 . We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch"
C18-1121,N16-1012,0,0.422367,"rence, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and sum"
C18-1121,P16-1154,0,0.0501788,"t summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder."
C18-1121,P16-1014,0,0.0343157,"an et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solv"
C18-1121,S14-1010,0,0.0768699,"odel to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has used textual entailment recognition to reduce redundancy for extractive summarization task. Our work is partially inspired by the models of Pasunuru et al. (2017) with following differences: Pasunuru et al. (2017) model the entailment task as the seq2seq generation problem and enforce sharing of the same decoder between summarization and entailment. However, the entailment task is more reasonable to be considered as a multi-label classification problem rather than a generation problem. We thus design a multi-task learning framework in which the summarization generation task shares the sam"
C18-1121,D17-1114,1,0.833514,". We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate"
C18-1121,D17-1222,0,0.0326185,". We define entailment reward s(x, y, y∗ ) as follows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate"
C18-1121,D15-1166,0,0.0732554,"ABS. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They use an attentive CNN encoder and neural network language model decoder to summarize sentence. ABS+. Rush et al. (2015) further tune ABS model on DUC 2003 dataset, then test on DUC 2004 test set. CAs2s. Chopra et al. (2016) extend the ABS model with a convolutional encoder and RNN decoder, which performs better than the ABS model. Feats2s. Nallapati et al. (2016) use a full RNN seq2seq model and add some lexical features to enhance the encoder, including POS, NER tags and TF-IDF values. Luong-NMT. Luong et al. (2015) propose a neural machine translation model with two-layer LSTMs for the encoder-decoder. Seq2seq. This is a standard seq2seq model with attention mechanism. Seq2seq + MTL. This is our proposed model with entailment-aware encoder, which applies a multi-task learning (MTL) framework to seq2seq model. Seq2seq + MTL (Share decoder). Pasunuru et al. (2017) propose a multi-task learning (MTL) framework in which the decoder is shared for summarization generation and entailment generation task. Seq2seq + ERAML. This is our proposed model with entailment-aware decoder, which conducts an Entailment Rew"
C18-1121,P17-2100,0,0.0464098,"neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descrip"
C18-1121,W13-2117,0,0.0719189,"selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previous work (Mehdad et al., 2013; Gupta et al., 2014) has used textual entailment recognition to reduce redundancy for extractive summarization task. Our work is partially inspired by the models of Pasunuru et al. (2017) with following differences: Pasunuru et al. (2017) model the entailment task as the seq2seq generation problem and enforce sharing of the same decoder between summarization and entailment. However, the entailment task is more reasonable to be considered as a multi-label classification problem rather than a generation problem. We thus design a multi-task learning framework in which the summarization generatio"
C18-1121,P16-2022,0,0.0260826,"ation model is trained by minimizing negative log-likelihood loss as in Equation 4. 3.2.3 Matching-based Entailment Inference Model To infer entailment relation, input sentence pairs from the entailment recognition corpus are fed into sentence encoder to obtain hidden representation (hu1 , · · · , hun ) and (hv1 , · · · , hvn ), respectively. Then, → − ← − → − ← − the sentence pairs are encoded as vectors u = [ h un ; h u1 ] and v = [ h vn ; h v1 ], respectively. Next, the absolute difference and the element-wise product for the tuple [u, v] are concatenated with the original vectors u and v (Mou et al., 2016) as follows: q = [|u − v|; u ∗ v; u; v] (13) We then feed q into a 3-layer multilayer perceptron (MLP) classifier. The 3-class softmax output layer is on top of MLP. The entailment recognition model is trained by minimizing cross-entropy loss. 3.2.4 Multi-Task Learning (MTL) In our multi-task setup, we share the encoder parameters of both the tasks, as shown in Figure 1(a). Traditional MTL considers equal contribution for all tasks. In our model, two tasks are significantly different. The summary generation task is much more complicated than entailment recognition, leading to different learnin"
C18-1121,K16-1028,0,0.459457,"2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017)"
C18-1121,D16-1244,0,0.0921424,"Missing"
C18-1121,W17-4504,0,0.64199,", two tasks are significantly different. The summary generation task is much more complicated than entailment recognition, leading to different learning difficulties and convergence rates. Therefore, summarization generation is regarded as the main task and entailment recognition as the auxiliary task, and our goal is to optimize the main task with assistance of auxiliary task. To this end, we optimize the two loss functions alternatively during training. Let α be the number of mini-batches of training for entailment recognition after 100 mini-batches of training for summarization generation (Pasunuru et al., 2017). We adopt α = 10 and performance with different α is discussed in Section 6.6.3. 3.3 Entailment-aware Decoder In order to encourage the decoder of the summarization system to produce summary entailed by the source sentence, we apply an entailment-aware decoder by entailment RAML training (Norouzi et al., 2016). 3.3.1 Reward Augmented Maximum Likelihood (RAML) Training RAML provides a computationally efficient approach to optimize task-specific reward (loss) directly. In our work, we apply RAML to incorporate entailment-based reward into our summarization model, as shown in Figure 1(b). The RA"
C18-1121,D15-1044,0,0.755145,"lows: s(x, y, y∗ ) = min{e(x, y), e(x, y∗ )} (18) where e(x, y) denotes entailment score for sentence pairs (x, y). Our goal is to maximize the entailment reward of the summary towards the reference, given the source sentence. Here we adopt the model of Parikh et al. (2016) trained on the MultiNLI corpus3 (Williams et al., 2017) to obtain e(x, y). 4 Related work Text summarization methods can be categorized into extraction-based methods (Erkan and Radev, 2004; Wan et al., 2007; Cheng and Lapata, 2016; Zhang et al., 2016; Nallapati et al., 2017; Li et al., 2017a) and abstraction-based methods. Rush et al. (2015) first apply the seq2seq model to abstractive sentence summarization. They propose an attentive CNN encoder and a neural network language model (Bengio et al., 2003) decoder. Chopra et al. (2016) use RNN as the decoder and achieve better performance. Nallapati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to"
C18-1121,D16-1112,0,0.0661736,"Missing"
C18-1121,P17-1101,0,0.743107,"pati et al. (2016) further replace the encoder with an RNN, forming a full RNN seq2seq model. Gu et al. (2016) and Zeng et al. (2016) incorporate a copying mechanism into seq2seq learning and Gulcehre et al. (2016) propose a switch gate to control whether to copy from the source or generate a word by the decoder. Copying mechanism intends to replicate segments in the source to the target, which cannot guarantee the correctness of the summary as a whole. Ma et al. (2017) focus on improving the semantic relevance between source and summary by encouraging high similarity of their representation. Zhou et al. (2017) employ a selective encoding model to control the information flow from encoder to decoder. Li et al. (2017b) apply a deep recurrent generative decoder to seq2seq framework. Cao et al. (2017) solve the problem of fake facts in a summary. They use Open Information Extraction to extract fact descriptions in the source sentence and propose the dual-attention seq2seq framework to force the generation conditioned on both source sentence and the fact descriptions. To the best of our knowledge, our work is the first to directly explore the correctness of summary without any preprocessing. Some previo"
C18-1305,N13-1073,0,0.0244006,"Missing"
C18-1305,P16-1162,0,0.105057,"stripped off before entering the general translator. We then align the words between the source sentence and the translated sentence, in order to add back slot labels into the translated sentence. This is an indirect slot transferring approach. Some limitations of the naive translation approach come from both translation and alignment. There are plenty of slot values like song names and contact names in the original SLU corpus. Many of these words are out-of-vocabulary (OOV) for the translation model. Although the translation model can handle these OOV words with sub-word modeling techniques (Sennrich et al., 2016), there are still many slot values remain to be OOV or mistranslated. For example, the Chinese name “白晓霞” in Table 1 is literally translated to “white sunshine”. Furthermore, wrong translations might also result in wrong alignments, which will yield inaccurate positions for slot labels. 3.2 Token-added Translation To make the translator be aware of slots, we then propose a token-added translation approach. This approach uses some special tokens to mark the segmentation boundary for the slot value in the source sentence. These special tokens are common in both the source vocabulary and target v"
C18-1305,stepanov-etal-2014-development,0,0.0286835,"2. However, bilingual in-domain data is scarce and costly, making it difficult to deliver both quality and low cost. Jabaian et al. (2010; 2013) propose two language transferring schemes. One is transferring source language annotation indirectly through word alignment. The other one is forcing the SMT model to translate the segmentation and slot labels simultaneously. The authors report that the indirect alignment gives the best performance. However, they also point out that distant language pairs suffer severely in word alignment. Finally rather than relying on automatic machine translation, Stepanov et al. (2014) prefer using human professional translation services. In Stepanov et al. (2017), they extend their work via crowdsourcing for semantic annotation. 3598 Methods Naive Translation Token-added Translation Class-based / SCRT Source input 我想打个电话给白晓霞 我想打个电话给（ a 白晓霞 ） 我想打个电话给 $contact name Translation result I would like to make a call to telephone number of white sunshine I would like to make a call to ( a white sunshine ) ’s telephone number please I would love to make a call to $contact name ’s number please Table 1: Translation examples by different translation systems. 3 Translation Systems Thi"
D10-1030,D08-1092,0,0.152382,"et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for parsing (Burkett and Klein, 2008; Huang et al., 2008). In this paper, we show that the bilingual argument structure consistency can be leveraged to substantially improve SRL results on both sides of bitext. Formally, we present a joint inference model to preform bilingual SRL. Using automatic word alignment on bitext, we first identify a pair of predicates that align with each other. And we use monolingual SRL systems to produce argument candidates for each predicate. Then, our model jointly generate SRL results for both predicates from their argument candidates, using integer linear programming (ILP) technique. An overview"
D10-1030,W05-0620,0,0.0389455,"Missing"
D10-1030,2007.tmi-papers.10,0,0.12382,"method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argument alignment on Chinese-English parallel PropBank. They performed SRL on Chinese and English sides separately. Then, given the SRL result on both sides, they automatically induced the argument alignment between two sides. The major difference between our work and all existing research is that our model performs SRL inference on two sides of bitext simultaneously. In our model, we jointly consider three interrelated factors: SRL result on the source side, SRL result on the target side, and the argument alignment between them. 3 3.1 Generating Candidates fo"
D10-1030,P80-1024,0,0.135696,"Missing"
D10-1030,P03-1054,0,0.00916788,"ent outputted by a system is a triple (loc, l, p). For example, the A0 argument in Figure 3 is ((0, 2), A0, 0.94). Because these outputs are to be combined, we call such triple a candidate. 1 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html 306 3.3 Generating and Merging Candidates To generate candidates for joint inference, we need to have multiple SRL results on each side of bitext. Therefore, for both Chinese and English SRL systems, we use the 3-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse trees of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs. All the three parsers are multilingual parsers. The second and third best parse trees of Berkeley parser are used for their good quality. Therefore, each monolingual SRL system produces 5 different outputs. Candidates from different outputs may have the same loc and l but different p. So we merge all candidates with the same loc and l into one by averaging their probabilities. For a merged candidate (loc, l, p), we say that p is the probability of assigning l to loc. 4 Joint Inference Model Our model can be conceptually decomposed to three components: the source side, the target si"
D10-1030,W05-0625,0,0.0241282,". Experiments on Chinese-English parallel PropBank shows that our model significantly outperforms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 305 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped"
D10-1030,J08-2001,0,0.0377787,"Missing"
D10-1030,H05-1081,0,0.0430344,"Missing"
D10-1030,J03-1002,0,0.00456404,"er, we employ lpsolve2 to solve all ILP models. 5 Experiments 5.1 Experimental Setup In our experiments, we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3.0. This data is a Chinese-English parallel proposition bank described in (Palmer et al., 2005). It contains parallel proposition annotations for 325 files (chtb 0001.fid to chtb 0325.fid) from ChineseEnglish parallel Treebank. The English part of this data contains proposition annotations only for verbal predicates. Therefore, we only consider verbal predicates in this paper. We employ the GIZA++ toolkit (Och and Ney, 2003) to perform automatic word alignment. Besides the parallel PropBank data, we use additional 4,500K Chinese-English sentence pairs3 to induce word alignments for both directions, with the default GIZA++ settings. The alignments are symmetrized using the intersection heuristic (Och and Ney, 2003), which is known to produce high-precision alignments. We use 80 files (chtb 0001.fid to chtb 0080.fid) as test data, and 40 files (chtb 0081.fid to chtb 0120.fid) as development data. Although our joint inference model needs no training, we still need to train a log-linear argument alignment probability"
D10-1030,W05-0309,0,0.0892931,"rcCmb is to maximize Os , which is defined in equation (2). And the constraints of SrcCmb are defined by equations (3-5). Similarly, we will refer to the target side combination model as TrgCmb. The objective of TrgCmb is to maximize Ot defined in equation (6). And the constraints of TrgCmb are defined by equations (7-9). In this paper, we employ lpsolve2 to solve all ILP models. 5 Experiments 5.1 Experimental Setup In our experiments, we use the Xinhua News portion of Chinese and English data in LDC OntoNotes Release 3.0. This data is a Chinese-English parallel proposition bank described in (Palmer et al., 2005). It contains parallel proposition annotations for 325 files (chtb 0001.fid to chtb 0325.fid) from ChineseEnglish parallel Treebank. The English part of this data contains proposition annotations only for verbal predicates. Therefore, we only consider verbal predicates in this paper. We employ the GIZA++ toolkit (Och and Ney, 2003) to perform automatic word alignment. Besides the parallel PropBank data, we use additional 4,500K Chinese-English sentence pairs3 to induce word alignments for both directions, with the default GIZA++ settings. The alignments are symmetrized using the intersection h"
D10-1030,N07-1051,0,0.0323811,"r of its first word and last word; its semantic role label l; and its probability p. So each argument outputted by a system is a triple (loc, l, p). For example, the A0 argument in Figure 3 is ((0, 2), A0, 0.94). Because these outputs are to be combined, we call such triple a candidate. 1 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit .html 306 3.3 Generating and Merging Candidates To generate candidates for joint inference, we need to have multiple SRL results on each side of bitext. Therefore, for both Chinese and English SRL systems, we use the 3-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse trees of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs. All the three parsers are multilingual parsers. The second and third best parse trees of Berkeley parser are used for their good quality. Therefore, each monolingual SRL system produces 5 different outputs. Candidates from different outputs may have the same loc and l but different p. So we merge all candidates with the same loc and l into one by averaging their probabilities. For a merged candidate (loc, l, p), we say that p is the probability of assigning l to loc. 4 Joint Inference"
D10-1030,P05-1072,0,0.0294341,"ms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 305 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argum"
D10-1030,C04-1197,0,0.376225,"inear model to compute the probability of aligning two arguments. Experiments on Chinese-English parallel PropBank shows that our model significantly outperforms monolingual SRL combination systems on both Chinese and English sides. The rest of this paper is organized as follows: Section 2 introduces related work. Section 3 describes how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 305 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus."
D10-1030,D09-1153,0,0.0353636,"important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for parsing (Burkett and Klein, 2008; Huang"
D10-1030,J08-2002,0,0.059627,"Missing"
D10-1030,N09-2004,0,0.127864,"Missing"
D10-1030,J08-2004,0,0.427186,"orm SRL on bitext, which has important applications including machine translation (Wu and Fung, 2009). A conventional way to perform SRL on bitext is performing SRL on each side of bitext separately, as has been done by Fung et al. (2007) on Chinese-English bitext. However, it is very difficult to obtain good SRL results on both sides of bitext in this way. The reason is that even the state-ofthe-art SRL systems do not have very high accuracy on both English text (M`arquez et al., 2008; Pradhan et al., 2008; Punyakanok et al., 2008; Toutanova et al., 2008), and Chinese text (Che et al., 2008; Xue, 2008; Li et al., 2009; Sun et al., 2009). On the other hand, the semantic equivalence between two sides of bitext means that they should have consistent predicate-argument structures. This bilingual argument structure consistency can guide us to find better SRL results. For example, in Figure 1(a), the argument structure consistency can guide us to choose a correct SRL result on Chinese side. Consistency between two argument structures is reflected by sound argument alignments between them, as shown in Figure 1(b). Previous research has shown that bilingual constraints can be very helpful for pars"
D10-1030,C10-1153,1,0.822881,"s how we generate SRL candidates on each side of bitext. Section 4 presents our joint inference model. Section 5 presents our experiments. And Section 6 concludes our work. 305 2 Related Work Some existing work on monolingual SRL combination is related to our work. Punyakanok et al. (2004; 2008) formulated an ILP model for SRL. Koomen et al. (2005) combined several SRL outputs using ILP method. M`arquez et al. (2005) and Pradhan et al. (2005) proposed combination strategies that are not based on ILP method. Surdeanu et al. (2007) did a complete research on a variety of combination strategies. Zhuang and Zong (2010) proposed a minimum error weighting combination strategy for Chinese SRL combination. Research on SRL utilizing parallel corpus is also related to our work. Pad´o and Lapata (2009) did research on cross-lingual annotation projection on English-German parallel corpus. They performed SRL only on the English side, and then mapped the English SRL result to German side. Fung et al. (2007) did pioneering work on studying argument alignment on Chinese-English parallel PropBank. They performed SRL on Chinese and English sides separately. Then, given the SRL result on both sides, they automatically ind"
D10-1030,J04-4004,0,\N,Missing
D10-1030,J08-2005,0,\N,Missing
D10-1030,N07-1070,0,\N,Missing
D10-1030,D09-1133,0,\N,Missing
D10-1030,D09-1127,0,\N,Missing
D11-1019,P05-1033,0,0.152867,"ple, our proposed system moves the prepositional phrase at an early 212 date after the sibling verb phrase. It is more reasonable compared with the baseline system s2t. In the third example, the proposed system FT2ETDeepSim successfully recognizes the Chinese long prepositional phrase 在 与 中国 总理 温家宝 举行 峰 会 后 发布 的联合 声明 中 and short verb phrase 说, and obtains the correct phrase reordering at last. 7 Related Work Several studies have tried to incorporate source or target syntax into translation models in a fuzzy manner. Zollmann and Venugopal (2006) augment the hierarchical string-to-string rules (Chiang, 2005) with target-side syntax. They annotate the target side of each string-to-string rule using SAMT-style syntactic categories and aim to generate the output more syntactically. Zhang et al. (2010) base their approach on tree-to-string models, and generate grammatical output more reliably with the help of tree-to-tree sequence rules. Neither of them builds target syntactic trees using target syntax, however. Thus they can be viewed as integrating target syntax in a fuzzy manner. By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding"
D11-1019,J07-2003,0,0.916312,"annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the transla"
D11-1019,P10-1146,0,0.317476,"curacy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 204 Proceedings of the 2011 Conference on Empirical Methods in Natural Language"
D11-1019,N09-1025,0,0.422919,"ive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as sh"
D11-1019,P05-1067,0,0.192745,"rom bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 2"
D11-1019,N04-1035,0,0.370764,"e-syntax-decorated string-to-tree rule as a fuzzy-tree to exact-tree rule. We first briefly review issues of string-to-tree rule extraction; then we discuss how to augment the string-to-tree rules to yield fuzzy-tree to exact-tree rules. Figure 1: Two alternative derivations for a sample string-to-tree translation. The rules used are listed on the right. The target yield of the tree with solid lines is hussein and terrorist networks established relations. The target yield of the tree with dotted lines is hussein established relations with terrorist networks. 2.1 String-to-Tree Rule Extraction Galley et al. (2004) proposed the GHKM algorithm for extracting (minimal) string-to-tree translation rules from a triple (f, et, a), where f is the sourcelanguage sentence, et is a target-language parse tree whose yield e is the translation of f, and a is the set of word alignments between e and f. The basic idea of GHKM is to obtain the set of minimally-sized translation rules which can explain the mappings between source string and target parse tree. The minimal string-to-tree rules are extracted in three steps: (1) frontier set computation; (2) fragmentation; and (3) extraction. The frontier set (FS) is the se"
D11-1019,P06-1121,0,0.915213,"slation rules via fuzzy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well"
D11-1019,D10-1063,0,0.0605581,"Missing"
D11-1019,P07-1019,0,0.051178,"is an augmentation of the string-to-tree model. In the baseline string-to-tree model, the decoder searches for the optimal derivation d * that parses a source string f into a target tree et from all possible derivations D: d D  3 d  R  d |f  (8)    01 12  match   13  unmatch  Translation Model and Decoding d *  arg max 1 log pLM   d    2   d  (7) (5) The 0-1 matching 4 is triggered only when we set   01  1 . The other two fuzzy matching algorithms are triggered in a similar way. During decoding, we use a CKY-style parser with beam search and cube-pruning (Huang and Chiang, 2007) to decode the new source sentences. 6 6.1 Experiments Experimental Setup where the first element is a language model score in which   d  is the target yield of derivation d ; the second element is the translation length penalty; the third element is used to control the derivation length; and the last element is a translation score that includes six features: The experiments are conducted on Chinese-toEnglish translation, with training data consisting of about 19 million English words and 17 million Chinese words 5. We performed bidirectional word alignment using GIZA++, and employed the gr"
D11-1019,W06-3601,0,0.591091,"ntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up ove"
D11-1019,D10-1014,0,0.285965,"le will be 7.2e-6. Similar to 0-1 matching, likelihood matching will serve as an additional model feature representing the compatibility between categories and rules. 3.3 Deep Similarity Matching Considering the two algorithms above, we can see that the purpose of fuzzy matching is in fact to calculate a similarity. 0-1 matching assigns similarity 1 for exact matches and 0 for mismatch, while likelihood matching directly utilizes the likelihood to measure the similarity. Going one step further, we adopt a measure of deep similarity, computed using latent distributions of syntactic categories. Huang et al. (2010) proposed this method to compute the similarity between two syntactic tag sequences, used to impose soft syntactic constraints in hierarchical phrase-based models. Analogously, we borrow this idea to calculate the similarity between two SAMT-style syntactic categories, and then apply it to calculate the degree of matching between a translation rule and the syntactic category of a test source string for purposes of fuzzy matching. We call this procedure deep similarity matching. Instead of directly using the SAMT-style syntactic categories, we represent each category by a real-valued feature ve"
D11-1019,P07-2045,0,0.00802438,"Missing"
D11-1019,W04-3250,0,0.373957,"tem ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4. We further extracted composed rules by composing two or three minimal GHKM rules. We also ran the stateof-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. In all systems, we set the beam size to 200. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed using the re-sampling approach (Koehn, 2004). 6.2 Results Table 1 shows the translation results on development and test sets. First, we investigate the performance of the strong baseline string-to-tree model (s2t for short). As the table shows, s2t outperforms the hierarchical phrase-based system Joshua by more than 1.0 BLEU point in all translation tasks. This result verifies the superiority of the baseline string-to-tree model. With the s2t system providing a baseline, we further study the effectiveness of our sourcesyntax-augmented string-to-tree system with fuzzy-tree to exact-tree rules (we use FT2ET to denote our proposed system)."
D11-1019,W09-0424,0,0.0407669,"Missing"
D11-1019,P06-1077,0,0.420049,"via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it"
D11-1019,W06-1606,0,0.705211,"zy use of the source syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful"
D11-1019,P08-1114,0,0.342219,"syntactic categories. Since we need to deal with the potential problem that the rule is hit by the test string but the syntactic category of the test string is not in the category set of the rule’s source side, we apply the m-estimate of probability (Mitchell, 1997) to calculate a smoothed likelihood 0-1 Matching 0-1 matching is a straightforward approach that rewards rules whose source syntactic category exactly matches the syntactic category of the test string and punishes mismatches. It has mainly been employed in hierarchical phrase-based models for integrating source or both-side syntax (Marton and Resnik, 2008; Chiang et al., 2009; Chiang, 2010). Since it is verified to be very effective in hierarchical models, we borrow this idea in our source-syntax-augmented string-to-tree translation. In 0-1 matching, the rule’s source side must contain only one syntactic category, but a rule may have been decorated with more than one syntactic category on the source side. Thus we have to choose the most reliable category and discard the others. Here, we select the one with the highest frequency. For example, the tag P in the rule 和 P : 6, CC : 4  IN  with  appears more frequently, so the final rule used i"
D11-1019,P10-1145,0,0.0189445,"., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and Liu (2010) proposed to enhance the tree-to-string model with target dependency structures (as a language model). In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax. 204 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 204–215, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics First, we give a motivatin"
D11-1019,P03-1021,0,0.0349121,"rized rules. It should be noted that our strategy makes the annotated binarized rules equivalent to the original rule. 210 symmetric word alignment. We parsed both sides of the parallel text with the Berkeley parser (Petrov et al., 2006) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus. For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06). The development data set comes from MT06 in which sentences with more than 20 words are removed to speed up MERT 6 (Och, 2003). The test set includes MT03 to MT05. We implemented the baseline string-to-tree system ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4. We further extracted composed rules by composing two or three minimal GHKM rules. We also ran the stateof-the-art hierarchical phrase-based system Joshua (Li et al., 2009) for comparison. In all systems, we set the beam size to 200. The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length pena"
D11-1019,P06-1055,0,0.214001,"the unmatch_count is redundant. In practice, since the derivation may include glue rules which are not scored by fuzzy matching. Thus, ""unmatch_count + match_count + glue_rule_number = derivation_length"". 5 LDC catalog number: LDC2002E18, LDC2003E14, LDC2003E07, LDC2004T07 and LDC2005T06. We call it heuristic because there may be other syntactic annotation strategies for the binarized rules. It should be noted that our strategy makes the annotated binarized rules equivalent to the original rule. 210 symmetric word alignment. We parsed both sides of the parallel text with the Berkeley parser (Petrov et al., 2006) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus. For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06). The development data set comes from MT06 in which sentences with more than 20 words are removed to speed up MERT 6 (Och, 2003). The test set includes MT03 to MT05. We implemented the baseline string-to-tree system ourselves according to (Galley et al., 2006; Marcu et al., 2006). We extracted minimal GHKM rules and the rules of SPMT Model 1 with source"
D11-1019,P05-1034,0,0.434154,"Missing"
D11-1019,P08-1066,0,0.153032,"syntax. Our extensive experiments have shown significant improvements over the state-of-the-art string-to-tree system. 1 Introduction In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving trans"
D11-1019,P10-1076,0,0.0113477,"statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality. It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering. One of the most successful syntax-based models is the string-to-tree model (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008; Chiang et al., 2009). Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al.,"
D11-1019,N06-1033,0,0.168469,"source-side parsed, word-aligned parallel corpus. The EM algorithm is employed to induce the parameters. We simply follow the algorithm of (Huang et al., 2010), except that we replace the tag sequence with SAMT-style syntactic categories. 4 Rule Binarization In the baseline string-to-tree model, the rules are not in Chomsky Normal Form. There are several ways to ensure cubic-time decoding. One way is to prune the extracted rules using a scope-3 grammar and do SCFG decoding without binarization (Hopkins and Lengmead, 2010). The other, and most popular way is to binarize the translation rules (Zhang et al., 2006). We adopt the latter approach for efficient decoding with integrated ngram language models since this binarization technique has been well studied in string-to-tree translation. However, when the rules’ source string is decorated with syntax (fuzzy-tree to exact-tree rules), how should we binarize these rules? We use the rule rn in Figure 2 for illustration: rn : x2 x0 x1  PP *VP  VP  x0 : VB x1 : NP x2 : PP  . Without regarding the source-side syntax, we obtain the following two binarized rules: B1: B2 :  x2 x0*1  VP x0*1 : Vx0 * x1 x2 : PP  x0 x1  Vx0 * x1  x0 : VB x1 : NP  Since"
D11-1019,P09-1020,0,0.0848052,"et al., 2010) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al., 2007) and the formal syntax-based system Hiero (Chiang, 2007). However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones. The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006; Mi et al., 2008; Zhang et al., 2009). The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints. Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches. A natural idea is the tree-to-tree model (Ding and Palmer, 2005; Cowan et al., 2006; Liu et al., 2009). However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Alternatively, Mi and L"
D11-1019,W06-3119,0,0.852615,"te its source string? 2) Given the source-annotated string-to-tree rules, how should we match these rules according to the test source tree during decoding? 3) How should we binarize the sourceannotated string-to-tree rules for efficient decoding? For the first problem, one may require the source side of a string-to-tree rule to be a constituent. However, such excessive constraints will exclude many good string-to-tree rules whose source strings are not constituents. Inspired by Chiang (2010), we adopt a fuzzy way to label 205 every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). This method leads to a one-to-one correspondence between the new rules and the string-to-tree rules. We will detail our fuzzy labeling method in Section 2. For the second problem, it appears simple and intuitive to match rules by requiring a rule’s source syntactic category to be the same as the category of the test string. However, this hard constraint will greatly narrow the search space during decoding. Continuing to pursue the fuzzy methodology, we adopt a fuzzy matching procedure to enable matching of all the rules whose source strings match the test string, and then determine the degre"
D11-1019,D10-1043,0,\N,Missing
D11-1019,P08-1023,0,\N,Missing
D11-1019,P09-1063,0,\N,Missing
D11-1019,W06-1628,0,\N,Missing
D14-1041,P02-1034,0,0.0369451,"Missing"
D14-1041,P06-2034,0,0.0142406,"literatures, there is no work related to multipredicate semantic role labeling. On Discriminative Reranking Discriminative reranking is a common approach in the NLP community. Its general procedure is that a base system first generates n-best candidates and with the help of global features, we obtain better performance through reranking the n-best candidates. It has been shown to be effective for various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model"
D14-1041,J02-3001,0,0.967573,"al. (2009) are pioneer works in Chinese SRL. Our approach outperforms these approaches by about 3.4 and 1.9 F1 points respectively. In English SRL, we compare out method with Surdeanu and Turmo (2005) which is best result obtained with single parse tree as the input in CONLL 2005 SRL evaluation. Our approach is better than their approach which ignores the relation of multiple predicates’ SRL. 6 7 Discussion and Analysis Related work Our work is related to semantic role labeling and discriminative reranking. In this section, we briefly review these two types of work. On Semantic Role Labeling Gildea and Jurafsky (2002) first presented a system based on a statistical classifier which is trained on a hand-annotated corpora FrameNet. In their pioneering work, they used a gold or autoparsed syntax tree as the input and then extracted various lexical and syntactic features to identify the In this section, we discuss some case studies that illustrate the advantages of our model. Some examples from our experiments are shown in Table 9. In example (1), the argument is a prepositional phrase ‘( n Ê ] t I ¡ Y  „ ö’ (at the same time of compulsory education) and shared by two predicates ‘—0’ (witness) and ‘i &apos;’ (expa"
D14-1041,P02-1031,0,0.0189684,"e model, ME can easily incorporate arbitrary features and Our contributions can be summarized as fol364 • Verb class (Xue, 2008) Candidates • Predicate and Head word combination Phase 1: Argument Identification Base Features • Predicate and Phrase type combination New Features • Verb class and Head word combination Classifier • Verb class and Phrase type combination 3.2 Refined Argument Candidates Phase 2: Argument Classification Additional Features In the SRL community, it is widely recognized that the overall performance of a system is largely determined by the quality of syntactic parsers (Gildea and Palmer, 2002), which is particularly notable in the identification stage. Unfortunately, the state-of-the-art auto parsers fall short of the demands of applications. Moreover, when there are multiple predicates, or even multiple clauses in a sentence, the problem of syntactic ambiguity increases drastically (Kim et al., 2000). For example, in Figure 3, there is a sentence with two consecutive predicates ‘/’ (is) and ‘ Ñ’ (develop). Compared with the gold tree, the auto tree is less preferable, which makes the classifier easily mistake ‘úQ’ (building) as an argument of ‘ Ñ’ (develop) with base features. But"
D14-1041,P08-1067,0,0.28319,"ork for reranking. For an input x, the generic reranker selects the best output y ∗ among the set of candidates GEN (x) according to the scoring function: y ∗ = argmax score(y) y∈GEN (x) (2) In our task, GEN (x) is a set of the n-best candidates generated from the base model. As usual, we calculate the score of a candidate by the dot product between a high dimensional feature and a weight W: score(y) = W · f (y) (3) We estimate the weight W using the averaged perceptron algorithm (Collins, 2002a) which is well known for its fast speed and good performance in similar large-parameter NLP tasks (Huang, 2008). The training algorithm of the generic averaged perceptron is shown in Table 1. In line 5, the algorithm updates W with the difference (if any) between the feature representations of the best scoring candidate and the gold candidate. We also use a refinement called “averaged parameters” that the final weight vector W is the average of weight vectors over T iterations and N samples. This averaging effect has been shown to reduce overfitting and produces more stable results (Collins, 2002a). Pseudocode: Averaged Structured Perceptron 1: Input: training data(xt , yt∗ ) for t = 1, ..., T ; 2: w ¯"
D14-1041,kingsbury-palmer-2002-treebank,0,0.296535,"Missing"
D14-1041,P10-1113,0,0.0435234,"Missing"
D14-1041,C10-1081,0,0.121325,"Missing"
D14-1041,P98-1013,0,0.211007,"Missing"
D14-1041,D08-1082,0,0.0141918,"nowledge, in the literatures, there is no work related to multipredicate semantic role labeling. On Discriminative Reranking Discriminative reranking is a common approach in the NLP community. Its general procedure is that a base system first generates n-best candidates and with the help of global features, we obtain better performance through reranking the n-best candidates. It has been shown to be effective for various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) p"
D14-1041,W10-0907,0,0.194587,"Missing"
D14-1041,J08-2003,0,0.0779674,"er Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue an"
D14-1041,P05-1022,0,0.0696681,"oth syntactic parsing and SRL. However, to the best of our knowledge, in the literatures, there is no work related to multipredicate semantic role labeling. On Discriminative Reranking Discriminative reranking is a common approach in the NLP community. Its general procedure is that a base system first generates n-best candidates and with the help of global features, we obtain better performance through reranking the n-best candidates. It has been shown to be effective for various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the"
D14-1041,C04-1100,0,0.0326264,"Missing"
D14-1041,J05-1003,0,0.0448864,"ve the performance of both syntactic parsing and SRL. However, to the best of our knowledge, in the literatures, there is no work related to multipredicate semantic role labeling. On Discriminative Reranking Discriminative reranking is a common approach in the NLP community. Its general procedure is that a base system first generates n-best candidates and with the help of global features, we obtain better performance through reranking the n-best candidates. It has been shown to be effective for various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among"
D14-1041,C04-1197,0,0.0732233,"various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhu"
D14-1041,N04-1023,0,0.0281298,"nguage processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhu"
D14-1041,W05-0635,0,0.0711002,"Missing"
D14-1041,D09-1153,0,0.547517,"te with base features. Experimental Setting To evaluate the performance of our approach, we have conducted on two standard benchmarks: Chinese PropBank and English PropBank. The experimental setting is as follows: Chinese: We use Chinese Proposition Bank 1.0. All data are divided into three parts. 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. 40 files (from chtb 041.fid to chtb 080.fid) constitutes the development set. The test set consists of 72 files (chtb 001.fid to chtb 040.fid and chtb 900.fid to chtb 931.fid). This data setting is the same as in (Xue, 2008; Sun et al., 2009). We adopt Berkeley Parser1 to carry out auto parsing for SRL and the parser is retrained on the training set. We used n =10 joint assignments for training the joint model and testing. English: We choose English Propbank as the evaluation corpus. According to the traditional partition, the training set consists of the annotations in Sections 2 to 21, the development set is Section 24, and the test set is Section 23. This data setting is the same as in (Xue and Palmer, 2004; Toutanova et al., 2005). We adopt Charniak Parser2 to carry out auto parsing for SRL and the parser is retrained on the t"
D14-1041,P12-1039,0,0.0147194,"al procedure is that a base system first generates n-best candidates and with the help of global features, we obtain better performance through reranking the n-best candidates. It has been shown to be effective for various natural language processing tasks,such as syntactic parsing (Collins, 2000; Collins, 2002b; Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008), semantic parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been m"
D14-1041,P05-1073,0,0.0320965,"le labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer, 2003) was built, Xue and Palmer (2004) and Xue (2008) took a critical look at featur"
D14-1041,J08-2002,0,0.062501,"parsing (Lu et al., 2008; Ge and Mooney, 2006), part-of-speech tagging (Collins, 2002a), named entity recognition (Collins, 2002c), machine translation (Shen et al., 2004) and surface realization in generation (Konstas and Lapata, 2012). semantic roles for a given predicate. After Gildea and Jurafsky (2002), there have been a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labele"
D14-1041,2009.eamt-1.30,0,0.0341333,"Missing"
D14-1041,P12-1095,0,0.126555,"Missing"
D14-1041,W03-1707,0,0.58792,"2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer, 2003) was built, Xue and Palmer (2004) and Xue (2008) took a critical look at features of argument detection and argument classification. Unlike others’ using syntax trees as the input of SRL, Sun et al. (2009) performed Chinese semantic role labeling with shallow parsing. Li et al. (2010) explored joint syntactic and se8 Conclusion and Feature Work This paper investigates the interaction effect among multi-predicate’s SRL. Our investigation has shown that there is much interaction effect of multi-predicate’s SRL both in Argument Identification and in Argument Classification. In the stage of argume"
D14-1041,W04-3212,0,0.175689,"a large number of works on automatic semantic role labeling. Based on a basic discriminative model, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer, 2003) was built, Xue and Palmer (2"
D14-1041,J08-2004,0,0.328388,"e PropBank and English PropBank respectively, which suggests that there are substantial gains to be made by jointly modeling the shared arguments of multiple predicates. 3 Argument Identification In this section, we investigate multi-predicate’ mutual effects in Argument Identification. Argument Identification is to recognize the arguments from all candidates of each predicate. Here, we use the Maximum Entropy (ME) classifier to perform binary classification. As a discriminative model, ME can easily incorporate arbitrary features and Our contributions can be summarized as fol364 • Verb class (Xue, 2008) Candidates • Predicate and Head word combination Phase 1: Argument Identification Base Features • Predicate and Phrase type combination New Features • Verb class and Head word combination Classifier • Verb class and Phrase type combination 3.2 Refined Argument Candidates Phase 2: Argument Classification Additional Features In the SRL community, it is widely recognized that the overall performance of a system is largely determined by the quality of syntactic parsers (Gildea and Palmer, 2002), which is particularly notable in the identification stage. Unfortunately, the state-of-the-art auto pa"
D14-1041,C12-1185,1,0.902111,"Missing"
D14-1041,C10-1153,1,0.896024,"l, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer, 2003) was built, Xue and Palmer (2004) and Xue (2008) took a critical look at features of argument detection and argument classifica"
D14-1041,D10-1030,1,0.88779,"l, Punyakanok et al. (2004) constructed an integer linear programming architecture, in which the dependency relations among arguments are implied in the constraint conditions. Toutanova et al. (2008) proposed a joint model to explore relations of all arguments of the same predicate. Unlike them, this paper focus on mining relations of different predicates’ semantic roles in one sentence. And, there have been many extensions in machine learning models (Moschitti et al., 2008), feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Punyakanok et al., 2008; Zhuang and Zong, 2010a; Zhuang and Zong, 2010b). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without employing any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and utilized Support Vector Machine to identify and classify the arguments. They made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer, 2003) was built, Xue and Palmer (2004) and Xue (2008) took a critical look at features of argument detection and argument classifica"
D14-1041,W02-1001,0,\N,Missing
D14-1041,C98-1013,0,\N,Missing
D14-1041,P03-1002,0,\N,Missing
D16-1160,P16-1185,0,0.634478,"(SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to boost the translation fluency. They report promising improvements by using the target-side monolingual data. In contrast, the source-side monolingual data is not fully explored. Luong et al. (2015a) adopt a simple autoencoder or skip-thought method (Kiros et al., 2015) to exploit the source-side monolingual data, but no significant BLEU gains are reported. Note that, in parallel to our efforts, Cheng et al. (2016b) have explored the usage of both source and target monolingual data using a similar semi-supervised reconstruction method, in which two NMTs are employed. One translates the source-side monolingual data into target translations, and the other reconstructs the source-side monolingual data from the target translations. In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector representations. To achieve this goal, we propose two 1535 Proceedings of the 2016 Con"
D16-1160,P15-1166,0,0.021441,"ale monolingual data. Sennrich et al. (2015) further propose a new approach to use targetside monolingual data. They generate the synthetic bilingual data by translating the target monolingual sentences to source language sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual"
D16-1160,N16-1101,0,0.0958203,"Missing"
D16-1160,P15-1001,0,0.0725684,"n quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT. Gulcehre et al. (2015) first investigate the targetside monolingual data in NMT. They propose shallow and deep fusion methods to enhance the decoder network by training a big language model on targetside large-scale mono"
D16-1160,D13-1176,0,0.0933825,"aches to make full use of the sourceside monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT. 1 Introduction Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014) has become the novel paradigm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in convent"
D16-1160,P07-2045,0,0.0444293,"digm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in conventional statistical machine translation (SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to boost the translation fluency. They report promising improvements by using the target-side monolingual data. In contrast, the source-side monolingual data is not fully explored. Luong et al. (2015a) adopt a simple autoencoder or skip-thought method (Kiros et al., 2015) to exploit the source-side monolingual data, but no significant BLEU gains are reported. Note that, in parallel to our efforts, Cheng et al. (2016b) have"
D16-1160,D15-1166,0,0.679943,"using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT. 1 Introduction Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by (Kalchbrenner and Blunsom, 2013; Cho et al., 2014) has become the novel paradigm and obtained state-ofthe-art translation quality for several language pairs, such as English-to-French and English-to-German (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015b; Sennrich et al., 2015). This endto-end NMT typically consists of two recurrent neural networks. The encoder network maps the source Currently, most NMT methods utilize only the sentence aligned parallel corpus for model training, which limits the capacity of the model. Recently, inspired by the successful application of target monolingual data in conventional statistical machine translation (SMT) (Koehn et al., 2007; Chiang, 2007), Gulcehre et al. (2015) and Sennrich et al. (2015) attempt to enhance the decoder network model of NMT by incorporating the targetside monolingual data so as to b"
D16-1160,P15-1002,0,0.0633985,"Missing"
D16-1160,D16-1096,0,0.0201568,"since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT"
D16-1160,D16-1249,0,0.041127,"since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT"
D16-1160,P16-2021,0,0.0134864,"d retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,P02-1040,0,0.124744,"Missing"
D16-1160,P06-1055,0,0.0199578,"Missing"
D16-1160,P16-1159,0,0.0774709,"oder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT. Gulcehre et al. (2015) first investigate the targetside monolingual"
D16-1160,P16-5005,0,0.017437,"data are much smaller. It is reasonable since large-scale training data can make the parameters of the encoder-decoder parameters much stable. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality. 7 Related Work As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network (Cheng et al., 2016a; Luong et al., 2015b; Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016; Mi et al., 1542 2016a; Mi et al., 2016b), better objective functions for BLEU evaluation (Shen et al., 2016) and better strategies for handling unknown words (Luong et al., 2015c; Sennrich et al., 2015; Li et al., 2016) or large vocabularies (Jean et al., 2015; Mi et al., 2016c). Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) applying target-side monolingual data in NMT, 2) targeting knowledge sharing with multi-task NMT, and 3) using source-side mono"
D16-1160,P07-1004,0,0.155635,"monolingual data into target translations, and the other reconstructs the source-side monolingual data from the target translations. In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector representations. To achieve this goal, we propose two 1535 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535–1545, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics approaches. Inspired by (Ueffing et al., 2007; Wu et al., 2008) handling source-side monolingual corpus in SMT and (Sennrich et al., 2015) exploiting target-side monolingual data in NMT, the first approach adopts the self-learning algorithm to generate adequate synthetic parallel data for NMT training. In this method, we first build the baseline machine translation system with the available aligned sentence pairs, and then obtain more synthetic parallel data by translating the source-side monolingual sentences with the baseline system. The proposed second approach applies the multitask learning framework to predict the target translation"
D16-1160,D07-1077,0,0.0085868,"influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data. Luong et al. (2015a) also investigate the source-side monolingual data in the multi-task learning framework, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data. Our sentence"
D16-1160,K16-1004,0,0.0199067,"d retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,C16-1127,0,0.0138546,"Missing"
D16-1160,C08-1125,1,0.265167,"on model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data. Luong et al. (2015a) also investigate the source-side monolingual data in the multi-task learning framework, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data. Our sentence reordering model is"
D16-1160,P14-1011,1,0.454717,"Missing"
D16-1160,D16-1163,0,0.0662772,"n NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning f"
D16-1160,N16-1004,0,0.0398445,"sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our selflearning algorithm in which we concern the sourceside monolingual data. Furthermore, their method requires to train an additional NMT from target language to source language, which may negatively influence the attention model in the decoder network. Dong et al. (2015) propose a multi-task learning method for translating one source language into multiple target languages in NMT so that the encoder network can be shared when dealing with several sets of bilingual data. Zoph et al. (2016), Zoph and Knight (2016) and Firat et al. (2016) further deal with more complicated cases (e.g. multi-source languages). Note that all these methods require bilingual training corpus. Instead, we adapt the multitask learning framework to better accommodate the source-side monolingual data. Ueffing et al. (2007) and Wu et al. (2008) explore the usage of source-side monolingual data in conventional SMT with a self-learning algorithm. Although we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt"
D16-1160,P16-1008,0,\N,Missing
D16-1160,J07-2003,0,\N,Missing
D17-1029,D15-1092,0,0.37527,"represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chin"
D17-1029,D15-1177,0,0.0216389,"ibutions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich in"
D17-1029,N13-1092,0,0.0268,"Missing"
D17-1029,P16-1020,0,0.021302,"ts are cosine similarities between embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments o"
D17-1029,P14-1006,0,0.0292836,"he atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich information and are capable of indicating semantic meanings of words. As illustrated in"
D17-1029,N16-1162,0,0.0463897,"Missing"
D17-1029,P15-1162,0,0.0632206,"Missing"
D17-1029,P14-1062,0,0.038149,"“机(machine)” and “场(field)”. The color depth represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence represen"
D17-1029,D16-1157,0,0.141464,"te to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contrast to English, Chinese characters contain rich information and are capable of indicating semantic meanings of words. As illustrated in Figure 1, the internal structures of Chinese words express two characteristics: (1) Each character in a word contribute differently to the compositional word meaning (Wong et al., 2009) such as the"
D17-1029,N16-1119,0,0.0717882,"Processing, pages 298–303 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics beddings. The atomic word representation is calculated by projecting word level inputs into a highdimensional space by a look up table, while the compositional word representation is computed as a gated composition of character representations: The word internal structures have been proven to be useful for Chinese word representations. Chen et al. (2015b) proposes a character-enhanced word representation model by adding the averaged character embeddings to the word embedding. Xu et al. (2016) extends this work by using weighted character embeddings. The weights are cosine similarities between embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character"
D17-1029,C10-1142,0,0.0573697,"Missing"
D17-1029,D16-1209,0,0.0204481,"a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments on sentence similarity (as well as word simi"
D17-1029,C16-1030,0,0.0248623,"ween embeddings of a word’s English translation and its constituent characters’ English translations. However, their work calculates weights based on a bilingual dictionary, which brings lots of mistakes because words in two languages do not mantain one-to-one relationship. Furthermore, they only consider the first characteristic of word internal structures, but ignore the contributions of the atomic and compositional word to the final word meaning. Similar ideas of adaptively utilizing character level informations have also been investigated in English recently (Hashimoto and Tsuruoka, 2016; Rei et al., 2016; Miyamoto and Cho, 2016). It should be noted that these studies are not focus on learning sentence embeddings. In this paper, we explore word internal structures to learn generic sentence representations, and propose a mixed character-word architecture which can be integrated into various sentence composition models. In the proposed architecture, a mask gate is employed to model the relation among characters in a word, and pooling mechanism is leveraged to model the contributions of the atomic and compositional word embeddings to the final word representations. Experiments on sentence similar"
D17-1029,P15-1150,0,0.0280064,". The color depth represents (1) contributions of each character to the compositional word meaning, and (2) contributions of the atomic (which ignore inner structures) and compositional word to the final word meaning. The deeper color means more contributions. Introduction To understand the meaning of a sentence is a prerequisite to solve many natural language processing problems. Obviously, this requires a good representation of the meaning of a sentence. Recently, neural network based methods have shown advantage in learning task-specific sentence representations (Kalchbrenner et al., 2014; Tai et al., 2015; Chen et al., 2015a; Cheng and Kartsaklis, 2015) and generic sentence representations (Le and Mikolov, 2014; Hermann and Blunsom, 2014; Kiros et al., 2015; Kenter et al., 2016; Wang et al., 2017). To learn generic sentence representations that perform robustly across tasks as effective as word representations, Wieting et al. (2016b) proposes an architecture based on the supervision from the Paraphrase Database (Ganitkevitch et al., 2013). Despite the fact that Chinese has unique word internal structures, there is no work focusing on learning generic Chinese sentence representations. In contra"
D17-1114,J02-3001,0,0.0228951,"image vector vi are mapped to a joint space by a two-branch neural network as follows:  x = W2 · f (W1 · vs + bs ) (6) y = V2 · f (V1 · vi + bi ) Note that the images in Flickr30K are similar to our task. However, the image descriptions are much simpler than the text in news, so the model trained on Flickr30K cannot be directly used for our task. For example, some of the information contained in the news, such as the time and location of events, cannot be directly reflected by images. To solve this problem, we simplify each sentence and speech transcription based on semantic role labelling (Gildea and Jurafsky, 2002), in which each predicate indicates an event and the arguments express the relevant information of this event. ARG0 denotes the agent of the event, and ARG1 denotes the action. The assumption is that the concepts including agent, predicate and action compose the body of the event, so we extract “ARG0+predicate+ARG1” as the simplified sentence that is used to match the images. It is worth noting that there may be multiple predicateargument structures for one sentence and we extract all of them. After the text-image matching model is trained and the sentences are simplified, for each textimage p"
D17-1114,P03-1054,0,0.0133063,"Missing"
D17-1114,P03-1056,0,0.0149749,"Missing"
D17-1114,N03-1020,0,0.232444,"model uses generated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold Ttext , the image and the sentence match. Image alignment. The images are aligned to the text in the following ways: The images in a document are aligned to all the sentences in this document and the key-frames in a shot are aligned to all the speech transcriptions in this shot. Image match. The texts are matched with images using the approach introduced in Section 3.4. 4.3 4.4 Multi-modal Summarization Evaluation We use the ROUGE-1.5.5 toolkit (Lin and Hovy, 2003) to evaluate the output summaries. This evaluation metric measures the summary quality by matching n-grams between generated summary and reference summary. Table 3 and Table 4 show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regarding to the three reference summaries for each topic in English and Chinese. For the results of the English MMS, from the first three lines in Table 3 we can see that when summarizing without visual information, the method with guidance strategies performs slightly better than do the first two methods. Because Rouge mainly measures word ov"
D17-1114,N10-1134,0,0.0443431,"summary score obtained by Equation 8 and Mc is the summary score obtained by Equation 9. The aim of Ms and Mc is to balance the aspects of salience and coverage for images. λs , and λm are determined by testing on development set. Note that to guaranteed monotone of F, λs , and λm should be lower than the minimum salience score of sentences. To further improve non-redundancy, we make sure that similarity between any pair of sentences in the summary is lower than Ttext . Equations 8,9 and 10 are all monotone submodular functions under the budget constraint. Thus, we apply the greedy algorithm (Lin and Bilmes, 2010) guaranteeing near-optimization to solve the problem. Experiment 4.1 Dataset There is no benchmark dataset for MMS. We construct a dataset as follows. We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese. We set 5 topics for each language as a development set. For each topic, we collect 20 documents within the same period using Google News search6 and 5-10 videos in CCTV.com7 and Youtube8 . More details of the corpus are illustrated in Table 1. Some examples of news topics are provided Table 2. We employ 10 graduate students to write reference summaries after"
D17-1114,D14-1041,1,0.851946,"+ audio + guide Image caption Image caption match Image alignment Image match 0.422 0.422 0.440 0.435 0.429 0.409 0.442 0.114 0.109 0.117 0.111 0.115 0.082 0.133 0.166 0.164 0.171 0.167 0.166 0.082 0.187 Table 3: Experimental results (F-score) for English MMS. Implementation Details We perform sentence9 and word tokenization, and all the Chinese sentences are segmented by Stanford Chinese Word Segmenter (Tseng et al., 2005). We apply Stanford CoreNLP toolkit (Levy and D. Manning, 2003; Klein and D. Manning, 2003) to perform lexical parsing and use semantic role labelling approach proposed by Yang and Zong (2014). We use 300-dimension skipgram English word embeddings which are publicly available10 . Given that text-image matching model and image caption generation model are trained in English, to create summaries in Chinese, we first translate the Chinese text into English via Google Translation11 and then conduct text and image matching. Method Method R-1 R-2 R-SU4 Text only Text + audio Text + audio + guide Image caption match Image alignment Image match 0.409 0.407 0.411 0.381 0.368 0.414 0.113 0.111 0.115 0.092 0.096 0.125 0.167 0.166 0.173 0.149 0.143 0.173 Table 4: Experimental results (F-score)"
D17-1114,N06-2046,0,0.168436,"• To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions. • We introduce an MMS corpus in English and Chinese. The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods. 2 2.1 Related Work Multi-document Summarization Multi-document summarization (MDS) attempts to extract important information for a set of documents related to a topic to generate a short summary. Graph based methods (Mihalcea and Tarau, 2004; Wan and Yang, 2006; Zhang et al., 2016) are commonly used. LexRank (Erkan and Radev, 2011) first builds a graph of the documents, in which each node represents a sentence and the edges represent the relationship between sentences. Then, the importance of each sentence is computed through an iterative random walk. 2.2 Multi-modal Summarization In recent years, much work has been done to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia. Erol et al. (2003) aim to create important segments of a meeting recording based on audio, text and visual activity analysis. Tjondro"
D17-1114,N16-1008,0,0.328886,"Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watching videos from beginning to end. 1 http://www.nlpr.ia.ac.cn/cip/jjzhang.htm The existing applications related to MMS include meeting record summarization (Erol et al., 2003; Gross et al., 2000), sport video summarization (Tjondronegoro et al., 2011; Hasan et al., 2013), movie summarization (Evangelopoulos et al., 2013; Mademlis et al., 2016), pictorial storyline summarization (Wang et al., 2012), timeline summarization (Wang et al., 2016b) and social multimedia summarization (Del Fabro et al., 2012; Bian et al., 2013; Schinas et al., 2015; Bian et al., 2015; Shah et al., 2015, 2016). When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions. For the summarization of pictorial storylines, the input is a set of images with text descriptions. None of these applications focus on summarizing multimedia data that contain asynchronous information about general topics. In this paper, as shown in Figure 1, we propose an approach to a generate textual summary from a set"
D17-1114,Q14-1006,0,\N,Missing
D18-1011,C16-1175,0,0.0358277,"Missing"
D18-1011,D16-1235,0,0.0265375,"Missing"
D18-1011,N09-1003,0,0.0607332,"Missing"
D18-1011,Q17-1002,0,0.0187394,"ignore the associations between modalities, and thus lack the ability of information transferring between modalities. Consequently they cannot handle words without perceptual information. Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities. Third, they typically treat the representations from different modalities equally. This is inconsistent with many psychological findings that information from different modalities contributes differently to the meaning of words (Paivio, 1990; Anderson et al., 2017). In this work, we introduce the associative multichannel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues. Our model is built upon the stacked autoencoder (Bengio et al., 2007) to learn semantic representations by integrating textual and perceptual inputs. Inspired by the re-constructive and associative nature of human memory, we propose two associative memory modules as extensions. One is to learn associations between modalities (e.g., associations between textual and visual features), so as to reconstruct corresponding perceptual informatio"
D18-1011,D14-1032,0,0.0226054,"he fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment. 2 2.1 Multimodal Models 2.2.1 Jointly training models A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs. These methods can implicitly propagate perceptual information to word representations and at the same time learn multimod"
D18-1011,Q14-1023,0,0.300461,"tion and associated words is triggered and mediated by a linguistic input. The learned cross-modality mapping and reconstruction of associated words are inspired by the human mental model of associations between different modalities and related concepts. Moreover, word meaning is tied to both linguistic and physical environment, and relies differently on each modality in2.2.2 Separate training models The simplest approach is concatenation which fuses textual and visual vectors by concatenating them. It has been proven to be effective in learning multimodal representations (Bruni et al., 2014; Hill et al., 2014; Collell et al., 2017). Variations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decomposition (SVD) (Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014). There is also work using deep learning methods to project different modality inputs into a common 116 word2vec image2vec Multimo representa ... ... ... sound2vec There's nothing that In case youyou cheers need, upwe've quite as collected fast as the a cute cutest small dog doing dogsomething breeds to lift your peculiar. mood. dog"
D18-1011,D14-1005,0,0.0223777,"ut of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs. We initialize the network biases as zeros and network weights with He-initialisation (He et al., 2015). The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128, 5.2 Baseline Multimodal Models Most of existing multimodal models only utilize textual and visual modalities. For fair comparison, we re-implement several representative systems with our own textual and visual vectors. The Concatenation (CONC) model (Kiela and Bottou, 2014) is simple concatenation of normalized textual and visual vectors. The Mapping (Collell et al., 2017) and Ridge (Hill et al., 2014) models first learn a mapping matrix from textual to visual modality using feed-forward neural network and ridge regression respectively. After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary. Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations. The SVD (Bruni et al., 2014) and CCA (Hill et al., 2014) models first concatena"
D18-1011,D15-1293,0,0.0199505,"ual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs. These methods can implicitly propagate perceptual information to word representations and at the same time learn multimodal representations. However, they utilize raw text corpus in which words having perceptual information account for a small portion. This weakens the effect of introducing perceptual information and consequently leads to the slight improvement of textual vectors. Background and Related Work Cognitive Grounding A large body of research evidences that human semantic memory is inherently re-constructi"
D18-1011,D14-1162,0,0.0964741,"ix benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models. 1 Introduction Representing the meaning of a word is a prerequisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on. In recent years we have seen a surge of interest in building computational models that represent word meanings from patterns of word co-occurrence in corpora (Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Clark, 2015; Wang et al., 2018b). However, word meaning is also tied to the physical world. Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience (Landau et al., 1998; Barsalou, 2008). This has led to the development of multimodal representation models that utilize both textual and perceptual information (e.g., images, sounds). As evidenced by a range of evaluations (Andrews et al., 2009; Bruni et al., 2014; Silberer • We present a novel associative multichannel autoencoder for multimodal word representation, wh"
D18-1011,D13-1115,0,0.0357832,"Missing"
D18-1011,D12-1130,0,0.0176076,"emory in humans, suggesting that rich information contained in human cognitive processing can be used to enhance NLP models. Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment. 2 2.1 Multimodal Models 2.2.1 Jointly training models A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimoda"
D18-1011,P14-1068,0,0.0601988,"(Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014). There is also work using deep learning methods to project different modality inputs into a common 116 word2vec image2vec Multimo representa ... ... ... sound2vec There's nothing that In case youyou cheers need, upwe've quite as collected fast as the a cute cutest small dog doing dogsomething breeds to lift your peculiar. mood. dog Figure 1: Architecture of the multichannel autoencoder with inputs of textual, visual and auditory sources. Our model extends the unimodal and bimodal autoencoder (Ngiam et al., 2011; Silberer and Lapata, 2014) to induce semantic representations integrating textual, visual and auditory information. As shown in Figure 1, our model first transforms input textual vector xt , visual vector xv and auditory vector xa to hidden representations: ht = g(Wt xt + bt ) hv = g(Wv xv + bv ) (1) ha = g(Wa xa + ba ). Then the hidden representations are concatenated together and mapped to a common space: Associative Multichannel Autoencoder hm = g(Wm [ht ; hv ; ha ] + bm ). (2) The model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation hm : We first prov"
D18-1036,D14-1179,0,0.0198241,"Missing"
D18-1036,D17-1148,0,0.0190034,"ublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to improve the translation quality. There are"
D18-1036,N13-1073,0,0.0374239,"(hm 1 , h2 , ..., hT x ) by using m stacked Long Short Term Memory (LSTM) layers (Hochreiter and Schmidhuber, 1997) . hm j is the hidden state of the top layer in encoder. The bottom layer of encoder is a bi-direction LSTM layer to collect the context from the left side and right side. The decoder generates one target word at a time by computing pN i (yi |y<i , C) as follows: (1) where zei is the attention output: zei = tanh(Wz [zim ; ci ]) aij hm i j=1 Neural Machine Translation pN i (yi |y<i , C) = sof tmax(Wyi zei + bs ) Tx X 2 The word alignments A is extracted using the fast-align tool (Dyer et al., 2013) on the bilingual training data with both source-to-target and target-to-source directions. (2) 392 Arthur: alc percent in fo Baseline+ME 30 percent i 30 percent in fourth quarter of last year source sentence y1 y2 y3 input NMT model output probability of each N gold target word P i (y)i 0.80 0.18 0.02 ... 0.35 0.34 0.31 ... 0.75 0.20 0.05 ... alignment y1 y2 y3 x1 x3 Source: ae chengzhan Reference: last year gr Baseline: he in fourth q Arthur: al percent in Baseline+M 30 percent alignments NMT model If PNi (y)i word satisfied the exception criterion and x j aligns to y i x1 x j is an exceptio"
D18-1036,D17-1146,0,0.0730537,"lesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(chengzhang, growth)” in the"
D18-1036,D16-1162,0,0.547059,"ectly translate the troublesome words. The extensive experiments on Chineseto-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words. 1 Figure 1: The NMT model produces a wrong translation for the low-frequency word “aerkat”. While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word “chengzhang” into “growth”. that the low-frequency words can be represented by frequent subword sequences. Arthur et al. (2016) and Feng et al. (2017) try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the lowfrequency problem of subwords. And the latter one has a drawback that they use lexicons without considering specific contexts. Fig. 1 shows an example, in which “aerkate” is an infrequent word and the baseline NMT incorrectly translates it into a pronoun “he”. Incorporation of bilingual lexicon rectifies the mistake but wrongly converts “chengzhang” into an incorrect target word “growth” since an entry “(cheng"
D18-1036,P15-1001,0,0.062259,"st all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method which incorporates"
D18-1036,D07-1007,0,0.0487669,"inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also"
D18-1036,P16-1100,0,0.0442821,"the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later,"
D18-1036,1983.tc-1.13,0,0.743972,"Missing"
D18-1036,D15-1166,0,0.347565,"esome words can be correctly translated. The contributions are listed as follows: 1) We are the first to define and handle the troublesome words in neural machine translation. 2) We propose to memorize not only the bilingual lexicons but also their contexts with a contextual memory. 3) We design a dynamic approach to correctly translate the troublesome words by combining the contextual memory and the NMT model. 2 ci = (3) where ai,j is the attention weight: m hm j zi ai,j = P m m j hj zi (4) where zim is the hidden state of the top layer in decoder. More detailed introduction can be found in (Luong et al., 2015). Notation. In this paper, we denote the whole |VS | source vocabulary by VS = {sm }m=1 and target |VT | vocabulary by VT = {tn }n=1 , where sm is the source word and tn is the target word. We denote a source sentence by X and a target sentence by Y . Each source word in X is denoted by xj . Each target word in Y is denoted by yi . Accordingly, a target word can be denoted not only by tn , but also by yi . This does not contradict. tn means this target word is the nth word in vocabulary VT , and yi means this target word is the ith word in sentence Y . Similarly, we denote a source word by sm"
D18-1036,D16-1096,0,0.059563,"that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pairs into NMT to imp"
D18-1036,P13-1111,1,0.846978,"ng Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. C"
D18-1036,P02-1040,0,0.101068,"liable. Thus we design the dynamic weight λi according to the exception rate r(xj ): λi = sigmoid(βγ ∗ γi ) γi = Tx X ai,j ∗ r(xj ) sentence pairs whose length exceeds 100. We run a total of 20 iterations for all translation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN ."
D18-1036,W17-4702,0,0.0202248,"Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NM"
D18-1036,D16-1160,1,0.858302,"., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017"
D18-1036,P16-1162,0,0.809658,"achine translation (NMT) based on the encoder-decoder architecture becomes the new state-of-the-art due to distributed representation and end-to-end learning (Cho et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). However, the current NMT is a global model that maximizes the performance on the overall data and has problems in handling low-frequency words and ambiguous words1 , we refer these words as troublesome words and define them in Section 3.1. Some previous work attempt to tackle the translation problem of low-frequency words. Sennrich et al. (2016) propose to decompose the words into subwords which are used as translation units so 1 In this work, we consider a source word is ambiguous if it has multiple translations with high entropy of probability distribution. 391 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 391–400 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ci can be calculated as follows: address them. Our method first investigates different strategies to define the troublesome words. Then, these words and their contexts in the t"
D18-1036,P16-5005,0,0.0197893,"owfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate the lexicon pair"
D18-1036,P17-2060,1,0.804071,"s mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could alleviate this problem. Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in Zhang and Zong (2015). Later, the researchers transfer to NMT framework, e.g. (Wang et al., 2017b; Zhang and Zong, 2016; Zhou et al., 2017; Tu et al., 2016; Mi et al., 2016; He et al., 2016; Dahlmann et al., 2017; Wang et al., 2017c,d; Gu et al., 2018; Zhao et al., 2018). The most relevant studies are Arthur et al. (2016) and Results on EN-DE Translation We also test our method on EN-DE translation and the results are reported in Table 6. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word. 398 Feng et al. (2017). They incorporate"
D18-1036,N16-1004,0,0.0190829,"ranslation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For subword granularity, we use the BPE method (Sennrich et al., 2016) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU (Papineni et al., 2002) for translation quality evaluation. We compare our method with other relevant methods as follows: (12) j 1) Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). where βγ is a learnable parameter. From Eq. (12), the dynamic weight λi is determined by both of the attention weight ai,j , and the exception rate r(xj ). Training the parameters. As discussed above, our method contains some parameters (vd , Wh , Wc and βγ ) to be learned. We denote the parameters introduced by our method by θM and the parameters in NMT by θN . To make it efficient, given the aligned training data D =  (d) (d) |D| X ,Y , we keep θN unchanged and opd=1 M timize θ by maximizing the following objective function. 2) Arthur: It is the state-of-the-art method"
D18-1036,H05-1097,0,0.0707789,"s follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into smaller granularities, e.g, hybrid word-character model (Luong and Manning, 2016), sub-word model (Sennrich et al., 2016) or word piece model (Wu et al., 2016). These methods mainly focus on lowfrequency words that are just a subset of the troublesome words. Furthermore, our experimental results show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could allevi"
D18-1036,D17-1149,0,0.0883335,"n cases (Deterio) when rectifying the troublesome words. As a comparison, we also count the total rectification and deterioration numbers of Arthur(test). The results are reported in Table 5. These results show that our method could rectify more words (51 vs. 70) with less deterioration (17 vs. 11) than Arthur(test). 6 Unit 7 Related Work The related work can be divided into three categories and we describe each of them as follows: Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). (Wang et al., 2017a) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide temporary information from source to assist the decoding process. In contrast, our work uses memory to store contextual knowledge in the training data. Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous words (Vickrey et al., 2005; Zhai et al., 2013; Rios et al., 2017; Carpuat and Wu, 2007; Li et al., 2016). Among them, the most relevant is the work that decomposes the low-frequency words into small"
D18-1036,I17-1039,1,0.885788,"Missing"
D18-1173,P07-1056,0,0.0652066,"c could tackle the domain shift with a series of cross-domain sentiment analysis tasks. 5.1 Tasks and datasets Domain Specific NER We use BioNLP11species (Kim et al., 2011), AnatEMs (Pyysalo and Ananiadou, 2013) and NCBI-disease (Do˘gan et al., 2014) dataset, respectively from taxonomy, anatomy and pathology literatures. We train embeddings with tested methods to initialize the recognizer, whose performance then demonstrates whether the tested models learn representations well for rare words. Cross Domain Sentiment Classification cross domain sentiment classification on Amazon Review dataset (Blitzer et al., 2007) is chosen as a benchmark. This dataset includes reviews from 4 product categories: books, DVDs, kitchens and electronics, suitable for the cross-domain setting. Using one as source domain and one as the target, we get 16 pairs for experiments. We train the classifier with source domain data and directly test it on the target domain, using the pre-trained embeddings as input feature. Note that through this task we also test how Mem2Vec performs when transferring from a small text, since in all the above experiments we learn prior knowledge from a large corpora. 5.2 Baselines Except for the fou"
D18-1173,W06-1615,0,0.122983,"on the target domain, using the pre-trained embeddings as input feature. Note that through this task we also test how Mem2Vec performs when transferring from a small text, since in all the above experiments we learn prior knowledge from a large corpora. 5.2 Baselines Except for the four baselines considered in word similarity tasks, we also compare with DAREP (Bollegala et al., 2015) and CRE (Yang et al., 2017) in the NER and sentiment classification tasks. They are both pivot-based methods for cross domain embedding learning which fare well in some downstream tasks. Besides we introduce SCL(Blitzer et al., 2006), a well-cited crossdomain sentiment analyser, as a baseline only for the sentiment classification task. For NER, we use pre-trained embeddings by the tested methods as only input features for a LSTMCRF recognition model (Lample et al., 2016). We simply mix the Wikipedia corpora with a dump of PubMed as our source corpora. Note that N2V and SUM can’t be directly used to pre-train embeddings for downstream tasks since they focus on novel word learning. We thus explicitly divide words which occur less than 5 times as rare words while others as frequent words. N2V and SUM learn the frequent words"
D18-1173,P15-1071,0,0.0861744,"s with explicit background knowledge from a commonsense knowledge base. Different from this strand of work, our method doesn’t fall back on auxiliary information. We acquire knowledge from a large unlabeled general domain corpora which is widely available. Cross Domain Word Embedding The knowledge accumulation phase of our model aims to learn an embedding space from a large general domain corpora. This is partially in line with cross domain word embedding work. Among these work, a strand of approach hypothesizes that a word frequent in multiple domains should mean nearly across these domains. Bollegala et al. (2015) call such word pivot, share its embeddings across domains and use them to predict the surrounding non-pivots. Yang et al. (2017) selectively incorporate source domain information to target domain word embeddings with a word-frequencybased regularization. These pivot-based methods have delivered improvements on sentiment analysis and NER. However, they have a defect that only limited target domain words benefit from the knowledge transfer. Memory based Meta Learning Memory augmented neural networks (MANN) are widely used in different tasks for efficient recall of experience and fast adaptation"
D18-1173,D17-1030,0,0.391302,"wards me, barking and wagging its tail. Even this is the first time one hears about Labrador, we can guess it should be an animal or even further a dog easily, since it runs, barks and has a tail. Such ability to efficiently acquire representation from small data, namely fast mapping, is thought to be the hallmark of human intelligence that a cognitive plausible agent should strive to reach (Xu and Tenenbaum, 2007; Lake et al., 2015). However, as the mainstream of text representation learning in NLP, most distributed semantic models (DSMs) don’t fare well in tiny data (Lazaridou et al., 2017; Herbelot and Baroni, 2017; Wang et al., 2016). Even if they have learned a lot of words, they still need sufficient examples to acquire a high-quality representation for a novel word. This not only constitutes a blow to DSM’s cognitive plausibility but also limits its practical usage in NLP. Because plentiful enough data is not always available, especially in domain specific tasks. Even if a large corpora is at hand, low-frequency words in it are still more than highly frequent ones, according to the Zipfian distribution of natural language. Given the above reasons, it’s desirable to build a word embedding method capa"
D18-1173,P18-1002,0,0.0843667,"Missing"
D18-1173,W11-1801,0,0.0703567,"Missing"
D18-1173,W15-0108,0,0.0385312,"Missing"
D18-1173,N16-1030,0,0.0316557,"ge corpora. 5.2 Baselines Except for the four baselines considered in word similarity tasks, we also compare with DAREP (Bollegala et al., 2015) and CRE (Yang et al., 2017) in the NER and sentiment classification tasks. They are both pivot-based methods for cross domain embedding learning which fare well in some downstream tasks. Besides we introduce SCL(Blitzer et al., 2006), a well-cited crossdomain sentiment analyser, as a baseline only for the sentiment classification task. For NER, we use pre-trained embeddings by the tested methods as only input features for a LSTMCRF recognition model (Lample et al., 2016). We simply mix the Wikipedia corpora with a dump of PubMed as our source corpora. Note that N2V and SUM can’t be directly used to pre-train embeddings for downstream tasks since they focus on novel word learning. We thus explicitly divide words which occur less than 5 times as rare words while others as frequent words. N2V and SUM learn the frequent words with Word2Vec and learn the rare words in their own way. This setting also applies to the sentiment classification task. For sentiment classification, we use a multilayer perceptron (MLP) as the classifier, with one hidden layer of 400 nodes"
D18-1173,P13-1149,0,0.188228,"n be efficient adapted for novel words. Lazaridou et al. (2017) directly sum the context embedding of a novel word as its representation, based on a pre-trained embedding space. Though not explicitly stated, their idea actually matches the HC theory. They constrain the hypothesis solely within the current context of the target word which we think is not enough. We constrain the hypothesis with memory and the context. Another strand of solutions rely on auxiliary information, such as morphological structure (Luong et al., 2013; Kisselew et al., 2015) and external knowledge (Long et al., 2017). Lazaridou et al. (2013) derive morphologically complex words from sub-word parts with phrase composition methods. Ling et al. (2015) read characters of the rare word with a bidirectional LSTM to deal with open vocabulary problem in language modeling and NER. Hill et al. (2016) learn an embedding of a dictionary definition to match the pre-trained headword vector, while Weissenborn (2017) refines the word embeddings with explicit background knowledge from a commonsense knowledge base. Different from this strand of work, our method doesn’t fall back on auxiliary information. We acquire knowledge from a large unlabeled"
D18-1173,D15-1176,0,0.0640137,"Missing"
D18-1173,D17-1086,0,0.121162,"regression, which can be efficient adapted for novel words. Lazaridou et al. (2017) directly sum the context embedding of a novel word as its representation, based on a pre-trained embedding space. Though not explicitly stated, their idea actually matches the HC theory. They constrain the hypothesis solely within the current context of the target word which we think is not enough. We constrain the hypothesis with memory and the context. Another strand of solutions rely on auxiliary information, such as morphological structure (Luong et al., 2013; Kisselew et al., 2015) and external knowledge (Long et al., 2017). Lazaridou et al. (2013) derive morphologically complex words from sub-word parts with phrase composition methods. Ling et al. (2015) read characters of the rare word with a bidirectional LSTM to deal with open vocabulary problem in language modeling and NER. Hill et al. (2016) learn an embedding of a dictionary definition to match the pre-trained headword vector, while Weissenborn (2017) refines the word embeddings with explicit background knowledge from a commonsense knowledge base. Different from this strand of work, our method doesn’t fall back on auxiliary information. We acquire knowled"
D18-1173,W13-3512,0,0.106195,"rn a linear transformation with pretrained word vectors and linear regression, which can be efficient adapted for novel words. Lazaridou et al. (2017) directly sum the context embedding of a novel word as its representation, based on a pre-trained embedding space. Though not explicitly stated, their idea actually matches the HC theory. They constrain the hypothesis solely within the current context of the target word which we think is not enough. We constrain the hypothesis with memory and the context. Another strand of solutions rely on auxiliary information, such as morphological structure (Luong et al., 2013; Kisselew et al., 2015) and external knowledge (Long et al., 2017). Lazaridou et al. (2013) derive morphologically complex words from sub-word parts with phrase composition methods. Ling et al. (2015) read characters of the rare word with a bidirectional LSTM to deal with open vocabulary problem in language modeling and NER. Hill et al. (2016) learn an embedding of a dictionary definition to match the pre-trained headword vector, while Weissenborn (2017) refines the word embeddings with explicit background knowledge from a commonsense knowledge base. Different from this strand of work, our me"
D18-1173,D16-1147,0,0.0280784,"use them to predict the surrounding non-pivots. Yang et al. (2017) selectively incorporate source domain information to target domain word embeddings with a word-frequencybased regularization. These pivot-based methods have delivered improvements on sentiment analysis and NER. However, they have a defect that only limited target domain words benefit from the knowledge transfer. Memory based Meta Learning Memory augmented neural networks (MANN) are widely used in different tasks for efficient recall of experience and fast adaptation to new knowledge (Bahdanau et al., 2014; Merity et al., 2017; Miller et al., 2016; Grave et al., 2017; Sprechmann et al., 2018; Wang et al., 2017). Intuitively, Meta-learning, which aims to train a model that quickly adapts to a new task, should benefit from memory architecture, and empirically it does do (Santoro et al., 2016; Duan et al., 2016; Wang et al., 2016; Munkhdalai and Yu, 2017; Kaiser et al., 2017). The memory we use is closely related to (Kaiser et al., 2017), but still get three major differences. First,they only retrieve the single nearest neighbor from the memory while we retrieve an average of the K nearest neighbors weighted by how they match the current"
D18-1173,D17-1312,0,0.0944071,"ck on auxiliary information. We acquire knowledge from a large unlabeled general domain corpora which is widely available. Cross Domain Word Embedding The knowledge accumulation phase of our model aims to learn an embedding space from a large general domain corpora. This is partially in line with cross domain word embedding work. Among these work, a strand of approach hypothesizes that a word frequent in multiple domains should mean nearly across these domains. Bollegala et al. (2015) call such word pivot, share its embeddings across domains and use them to predict the surrounding non-pivots. Yang et al. (2017) selectively incorporate source domain information to target domain word embeddings with a word-frequencybased regularization. These pivot-based methods have delivered improvements on sentiment analysis and NER. However, they have a defect that only limited target domain words benefit from the knowledge transfer. Memory based Meta Learning Memory augmented neural networks (MANN) are widely used in different tasks for efficient recall of experience and fast adaptation to new knowledge (Bahdanau et al., 2014; Merity et al., 2017; Miller et al., 2016; Grave et al., 2017; Sprechmann et al., 2018;"
D18-1173,D14-1162,0,0.0895687,"ghly informative as definitions in the Nonce dataset. There are 3 sub-tasks in Chimera:L2, L4 and L6, respectively providing 2, 4, 6 sentences as context to for each of the 330 instances in the dataset. The tested model needs to learn target word representation from the provided contexts. The similarity between learned embeddings and each of the probe words is measured and compared to human judgments by Spearman correlation. The final score is the average Spearman across all test pairs. 4.2 Baselines Our model is compared to several baselines, including Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), SUM (Lazaridou et al., 2017) and N2V (Herbelot and Baroni, 2017). Glove and Word2Vec are representatives of traditional DSMs. With them we want to test how exactly traditional DSMs perform in the few shot representation learning without any additional mechanism for small data. SUM and N2V are proposed especially for learning on small corpus. They adapt Word2Vec’s skip-gram structure for incremental learning and show improvements on the Chimera dataset. They partially match the HC theory which Mem2Vec is based on. Note that several rare word learning methods (Long et al., 2017; Xu et al., 201"
D18-1326,C18-1263,0,0.155278,"an share more hidden units and languages with a great difference (En/Ja) may share less hidden units. 6 Related Work In this work, we explore the balancing problem of shared and unique parameters, and attempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper"
D18-1326,P17-1176,0,0.0323628,"porate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new parameter-sharing"
D18-1326,P15-1166,0,0.109787,"s attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an"
D18-1326,N16-1101,0,0.0872011,"uages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an appealing approach in which a universal encoder-decoder framew"
D18-1326,D16-1026,0,0.110205,"Missing"
D18-1326,N18-1032,0,0.0544922,"h different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new parameter-sharing mechanism. The empirical experiments on four language pairs demo"
D18-1326,C18-1054,0,0.0880812,"ts and languages with a great difference (En/Ja) may share less hidden units. 6 Related Work In this work, we explore the balancing problem of shared and unique parameters, and attempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed th"
D18-1326,W18-6309,0,0.349927,"onding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al. (2016) present an appealing approach in which a universal encoder-decoder framework is designed for manyto-one, many-to-many and one-to-many multilingual translation tasks. The network model is compact and the model size does not grow as the number of languages increases. However, Johnson et al. (2017) observe that only the many-toone paradigm"
D18-1326,P02-1040,0,0.102327,"ional hidden representations. During training, each mini-batch on one GPU contains a set of sentence pairs with roughly 3,072 source and 3,072 target tokens. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For our model, we train for 400,000 steps on one machine with 8 NVIDIA Tesla M40 GPUs. 5 Results and Analysis We show the results of one-to-many translation experiments using our proposed strategies. The translation performance is evaluated by case-insensitive BLEU4 for Zh→En translation, character-level BLEU5 for Zh→Ja translation, and case-sensitive BLEU4 (Papineni et al., 2002) for En→De/Fr translation task. 5.1 4 Experiments Settings In this section, we test the proposed methods on two one-to-many translation tasks, including (i) Chinese→English/Japanese in general domain, and (ii) English→French/German in WMT14 Our Strategies vs. Baseline Table 1 reports the main translation results of Zh→En/Ja and En→De/Fr translation tasks. We conduct universal one-to-many translation using 2957 1 2 http://www.statmt.org/wmt14/translation-task.html https://github.com/tensorflow/tensor2tensor Zh→En Methods Zh→Ja En→De En→Fr MT03 MT04 MT05 MT06 Ave test test test Indiv 43.59 43.95"
D18-1326,P17-2060,1,0.837479,"e than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong"
D18-1326,N16-1004,0,0.120711,"mework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multiple encoders or multiple decoders (Luong et al., 2016; Dong et al., 2015). Firat et al. (2016a) and Lu et al. (2018) further propose to share a universal attention mechanism for many-to-many translations. In these methods, encoder or decoder is language dependent and network parameters increase linearly with the number of languages. Johnson et al. (2017) and Ha et al."
D18-1326,D16-1163,0,0.0524242,"tempt to 2958 incorporate the language-dependent presentation features to distinguish different target languages under the scenario of one-to-many multilingual translation. Multilingual translation has been extensively studied in Dong et al. (2015), Firat et al. (2016a), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conduct translation of multiple languages based on the framework of Johnson et al. (2017) and Ha et al. (2016). As for low-resource translation scenario (Zoph et al., 2016; Chen et al., 2017; Wang et al., 2017b), similar to above method, Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages especially for lowresource multilingual NMT. Different from previous methods, our work mainly focuses on improving the one-to-many multilingual translation framework while sharing as many parameters as possible. 7 Conclusion In this paper, we have proposed three effective strategies to improve the universal one-to-many multilingual translation, including special label initialization, language-dependent positional embedding and a new"
D18-1326,P16-1009,0,0.0349212,"onstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios"
D18-1326,I17-1039,1,0.87635,"trategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-task learning framework to perform many-to-one or one-to-many translation using multi"
D18-1326,1983.tc-1.13,0,0.554923,"Missing"
D18-1326,D16-1160,1,0.864526,"sed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models. 1 Introduction Encoder-decoder based neural machine translation (NMT) has achieved the new state-of-the-art due to powerful end-to-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Hassan et al., 2018). Under this end-to-end framework, many researchers attempt to improve the translation quality between two languages by exploiting monolingual data (Sennrich et al., 2016; Zhang and Zong, 2016), taking advantage of both NMT and statistical machine translation (Wang et al., 2017a; Tang et al., 2016; Zhao et al., 2018; Zhou et al., 2017) and so on. ∗ Jiajun Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. Another research direction about how to perform multilingual translation within this encoderdecoder architecture has recently drawn more and more attention (Zoph and Knight, 2016; Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Firat et al., 2016b). In multilingual translation scenarios, one can employ multi-"
D18-1415,D17-1234,0,0.0451927,"Missing"
D18-1415,D17-1260,0,0.0158729,"ssociation for Computational Linguistics dialog features based on new ontology. Besides, new system actions may be added to deal with new user actions. The network architecture of the new system and the original one will be different. The new system can not inherit the parameters from the old one directly. It will make the original dialog manager model invalid. Therefore, developers have to retrain the new system by interacting with users from scratch. Though there are many methods to train a RL-based dialog manager efficiently (Su et al., 2016a, 2017; Lipton et al., 2017; Chang et al., 2017; Chen et al., 2017), the unmaintainable RL-based dialog systems will still be put on the shelf in real-world applications (Paek and Pieraccini, 2008; Paek, 2006). To alleviate this problem, we propose a teacherstudent framework to maintain the RL-based dialog manager without training from scratch. The idea is to transfer the knowledge of existing resources to a new dialog manager. Specifically, after the system is deployed, if developers find some intents and slots missing before, they can define a few simple dialog rules to handle such situations. For example, under the condition shown in Fig. 1, a reasonable s"
D18-1415,P17-1045,0,0.016006,"we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Shah et al. (2016) proposed to integrate turnlevel feedback with a task-level reward signal to"
D18-1415,P84-1044,0,0.416399,"Missing"
D18-1415,P16-1228,0,0.0315951,"t ots1 otu encoding encoding s atu Inform(area=Beihai) at 1 Request(area) Victorian Germy Do Eat otdb encoding 10 Godear Holiday Database Figure 2: An overview of the RL-based dialog manager used in our work3 . In the last turn, the system inquires “Where do you want to go?”. In current turn, the user input is “Find a restaurant in Beihai.”. a large highly regularized model into a smaller model. The knowledge which can be transferred has not been restricted to models. Stewart and Ermon (2017) proposed to distill the physics and domain knowledge to train neural networks without labeled data. Hu et al. (2016) enabled a neural network to learn simultaneously from labeled instances as well as logic rules. Zhang et al. (2017) integrated multiple prior knowledge sources into neural machine translation using posterior regularization. Our experiments are based on such insights. Through defining appropriate regularization terms, we can distill different knowledge (e.g., trained model or prior knowledge) to a new designed model, alleviating the need for new labeled data or expensive interaction environments. 3 RL-based Dialog Manager Before going to the details of our method, we provide some background on"
D18-1415,I17-1074,0,0.277254,"can be incrementally extended once developers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial"
D18-1415,W17-5518,0,0.0341068,"Missing"
D18-1415,P16-1230,0,0.0561139,"Missing"
D18-1415,P17-1062,0,0.355141,"n into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Shah et al. (2016) proposed to integrate turnlevel fee"
D18-1415,D17-1237,0,0.234987,"lly extended once developers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology ha"
D18-1415,P17-1139,0,0.0197507,"10 Godear Holiday Database Figure 2: An overview of the RL-based dialog manager used in our work3 . In the last turn, the system inquires “Where do you want to go?”. In current turn, the user input is “Find a restaurant in Beihai.”. a large highly regularized model into a smaller model. The knowledge which can be transferred has not been restricted to models. Stewart and Ermon (2017) proposed to distill the physics and domain knowledge to train neural networks without labeled data. Hu et al. (2016) enabled a neural network to learn simultaneously from labeled instances as well as logic rules. Zhang et al. (2017) integrated multiple prior knowledge sources into neural machine translation using posterior regularization. Our experiments are based on such insights. Through defining appropriate regularization terms, we can distill different knowledge (e.g., trained model or prior knowledge) to a new designed model, alleviating the need for new labeled data or expensive interaction environments. 3 RL-based Dialog Manager Before going to the details of our method, we provide some background on the RL-based dialog manager in this section. Fig. 2 shows an overview of such dialog manager. We describe each of t"
D18-1415,W16-3601,0,0.0710311,"evelopers find new intents and slots that are not taken into account before. As far as we know, we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically. 2 Related Work Dialog Manager The dialog manager of taskoriented dialog systems, which consists of a state tracker and a dialog policy module, controls the dialog flow. Recently, deep reinforcement learning (Mnih et al., 2013, 2015) has been applied to optimize the dialog manager in an “endto-end” way, including deep Q-Network (Lipton et al., 2017; Li et al., 2017b; Peng et al., 2017; Zhao and Eskenazi, 2016) and policy gradient methods (Williams et al., 2017; Su et al., 2016b; Dhingra et al., 2017). RL methods have shown great potential in building a robust dialog system automatically. However, RL-based approaches are rarely used in real-world applications because of the maintainability problem (Paek and Pieraccini, 2008; Paek, 2006). To extend the domain of dialog systems, Gaˇsic et al. (2014) explicitly defined kernel functions between the belief states that come from different domains. However, defining an appropriate kernel function is nontrivial when the ontology has changed drastically. Sha"
D18-1448,N18-1150,0,0.0169755,"score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visual, and linguistic representations. Li et al. (2017) generate the text summary fr"
D18-1448,D17-1114,1,0.881109,"lience and intermodality relevance. The experimental results show the effectiveness of MMAE. 1 summarize Researchers have discovered the fossilized remains of a small, lizard- like creature that is the missing ancestral link … Figure 1: The illustration of our proposed task – Multimodal Summarization with Multimodal Output (MSMO). The image can help better understand the text in the red font. Introduction Text summarization is to extract the important information from source documents. With the increase of multimedia data on the internet, some researchers (Li et al., 2016b; Shah et al., 2016; Li et al., 2017) focus on multimodal summarization in recent years. Existing experiments (Li et al., 2017, 2018a) have proven that, compared to text summarization, multimodal summarization can improve the quality of generated summary by using information in visual modality. However, the output of existing multimodal summarization systems is usually represented in a single modality, such as textual or visual (Li et al., 2017; Evangelopoulos et al., 2013; Mademlis et al., 2016). In this paper, we argue that multimodal output1 is necessary for the following three reasons: 1) It is much easier and faster 1 Tiny w"
D18-1448,W12-2601,0,0.0306175,"erence with that in model output, m2 is obtained by comparing the image set in reference with the image in model output, and m3 considers the image-text similarity in model output. To learn MMAE, we choose three simple methods to fit y with human judgment scores. These methods include Linear Regression (LR), and two nonlinear methods: Logistic Regression (Logis), and Multilayer Perceptron (MLP). 3.1 Salience of Text ROUGE (Lin, 2004b) is widely used to automatically assess the quality of text summarization systems. It has been shown that ROUGE correlates well with human judgments (Lin, 2004a; Owczarzak et al., 2012; Over and Yen, 2004). Therefore, we directly apply ROUGE to assess the salience of the text units. 3.2 Salience of Image We propose a metric, namely, image precision (IP), to measure the salience of image. The image precision is defined as follows: IP = |{ref img } ∩ {recimg }| |{recimg }| (18) where ref img , recimg denote reference images and recommended images by MSMO systems respectively. The reasons for this metric are as follows. A good summary should have good coverage of the events for both texts and images. The image in the output should be closely related to the events. So we formul"
D18-1448,D15-1044,0,0.125061,". Surprisingly, the model ATG achieves the highest MMAE score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visual, and linguistic"
D18-1448,P17-1099,0,0.744619,"ur dataset has been released to the public, which can be found in http://www.nlpr.ia.ac.cn/cip/ jjzhang.htm. 3 http://www.robots.ox.ac.uk/˜vgg/ research/very_deep decoder, which is a unidirectional LSTM, makes use of information from two modalities to generate the text summary and select the most relevant image according to visual coverage vector. Our text encoder and summary decoder are based on pointer-generator network which we will describe in Sec. 2.2. We then describe image encoder and multimodal attention layer in our multimodal attention model (Sec. 2.3). 2.2 Pointer-Generator Network See et al. (2017) propose a pointer-generator network which allows both copying words from the source text and generating words from a fixed vocabulary, achieving the best performance on CNN/Daily mail dataset. Their model consists of an encoder (a single-layer bidirectional LSTM) and an attentive decoder (a unidirectional LSTM). The encoder maps the article to a sequence of encoder hidden states hi . During decoding, the decoder receives the embedding of the previous word and reaches a new decoder state st . Then the context vector ct is computed by the attention mechanism (Bahdanau et al., 2015; Luong et al."
D18-1448,D15-1166,0,0.405232,"t al. (2017) propose a pointer-generator network which allows both copying words from the source text and generating words from a fixed vocabulary, achieving the best performance on CNN/Daily mail dataset. Their model consists of an encoder (a single-layer bidirectional LSTM) and an attentive decoder (a unidirectional LSTM). The encoder maps the article to a sequence of encoder hidden states hi . During decoding, the decoder receives the embedding of the previous word and reaches a new decoder state st . Then the context vector ct is computed by the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) as calculated in Eq. 1 and 2. To alleviate the problem of repetition, See et al. (2017) maintain a coverage vector cov t , which is the sum of attention distributions over all previous decoding timestepsP(initialized to zero vector at timestep 0): cov t = t−1 αt˜. The coverage vector is used as t˜=0 an extra input to the attention vector (Eq. 1) and is also used to calculate the coverage loss (Eq. 6). Next, the attention distribution is used to calculate the context vector as follows. eti = v T tanh(Wh hi + Ws st + Wc cov t ) t t α = softmax(e ) X αit hi ct = (1) (2) (3) i The important part"
D18-1448,N06-2046,0,0.0242991,"61 articles are left. Surprisingly, the model ATG achieves the highest MMAE score despite the mediocre performance in three individual metrics. The MAXsim score of ATG is much higher than ATL and HAN. It shows the global features can help to learn better image-text alignments. Since GR itself makes use of the image-caption pairs, it is natural to get a high image-text relevance score. Our proposed multimodal attention models all achieves higher performance than the extractive baseline GR, which further indicate the effectiveness of our models. 5 Related Work Different from text summarization (Wan and Yang, 2006; Rush et al., 2015; Zhu et al., 2017; See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018), Multimodal Summarization is a task to generate a condensed text summary or a few keyframes to help acquire the gist of multimedia data. One of the most significant advantages of the task is that it does not rely solely on text information, but it can also utilize the rich visual content from the images. In recent years, much work has focused on multimodal summarization. Evangelopoulos et al. (2013) detect salient events in a movie based on the saliency of individual features for aural, visu"
D18-1448,K16-1028,0,0.0426672,"than text summaries. It shows that users prefer this way of presenting information. It also confirms our motivation for MSMO. 4.3 Comparison with Text Summarization Our user satisfaction test in Sec. 4.2 is done in an ideal situation, comparing the text reference with the pictorial reference. To show the effectiveness of our model, we also compare our model with text summarization from ROUGE and human judgment scores. We compare several abstractive summarization methods with our multimodal summarization methods. PGC7 (See et al., 2017) refers to the pointer-generator network (Sec. 2.2). AED (Nallapati et al., 2016) uses an attentional encoder-decoder framework and adds some linguistic features such as POS, named-entities, and TF-IDF into the encoder. We also implement a seq2seq model with attention (S2S+attn). To compare the multimodal output with our multimodal model, we propose an extractive method 7 https://github.com/abisee/ pointer-generator based on GuideRank (GR) (Li et al., 2016a, 2018b). GuideRank applies LexRank (Erkan and Radev, 2004) with guidance strategy. In this strategy, captions recommend the sentences related to them. The rankings of sentences and captions are obtained through GR; we e"
D18-1448,N16-1008,0,0.315353,"tput (text) (more details can be found in Sec. 4.2). 3) Images help users to grasp events while texts provide more details related to the events. Thus the images and text can complement each other, assisting users to gain a more visualized understanding of events (Bian et al., 2013). We give an example in Fig. 1 to illustrate this phenomenon. For the output with only the text summary, user will be confused about the description of “four-legged creatures”; while with a relevant image, user will have a clearer understanding of the text. In recent years, some researchers(Bian et al., 2013, 2015; Wang et al., 2016) focus on incorporating multimedia contents into the output of summarization which all treat the image-text pair as a basic summarization unit. But in our work, our input comes from a document and a collection of images where there is no alignment between texts and images. So our biggest challenge is how to bridge the semantic gaps between texts and images. Based on the above discussion, in this work, we propose a novel task which we refer to as Multimodal Summarization with Multimodal Output (MSMO). To explore this task, we focus on the simplicity, we first consider only one image) and a piec"
D18-1448,Q14-1006,0,0.027754,"re, we regard the image-text relevance as one of metrics to measure the quality of the pictorial summary. We consider using visual-semantic embedding (Faghri et al., 2018; Wang et al., 2018) to calculate the cosine similarity between visual feature and textual feature, which we use as image-text relevance. Visual-semantic embedding has been widely used in cross-modal retrieval (Kiros et al., 2014) and image captioning (Karpathy and Fei-Fei, 2015). We apply VSE0 model of Faghri et al. (2018), which achieves state-of-the-art performance for image-caption retrieval task on the Flickr30K dataset (Young et al., 2014). The difference is that instead of training a CNN model to encode the image, we use the pretrained VGG19 to extract global features. The text is encoded by a unidirectional Gated Recurrent Unit (GRU) to a sequence of vector representations. Then we apply the max-over-time pooling (Collobert et al., 2011) to get a single vector representation. Next, the visual features and text features are projected to a joint semantic space by two feed-forward neural networks. The whole network is trained using a max-margin loss: X L= max(β − s(i, c) + s(i, cˆ), 0) our evaluation metrics, we calculate the co"
D19-1185,C18-1107,0,0.0287291,"Missing"
D19-1185,P17-1162,0,0.0182911,"tions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain dialogue management. Chen et al. (2018) used graph neural networks (Battaglia et al., 2018; Li et al., 2015; Scarselli et al., 2009; Niepert et al., 2016) to improve the sample-efficiency of reinforcement learning. He et al. (2017) used DynoNet to incorporate structured information in the collaborative dialogue setting. Compared with them, our method is a combination of the graph neural networks and hierarchical reinforcement learning, and experiments prove that they both work in the novel dialogue task. 7 Conclusion This paper proposes to detect identity fraud automatically via dialogue interactions. To achieve this goal, we present structured dialogue management to explore anti-fraud dialogue strategies based on a KG with reinforcement learning and a heuristic user simulator to evaluate our systems. Experiments have s"
D19-1185,I17-1074,0,0.0716819,"Missing"
D19-1185,D17-1237,0,0.0173809,"tem. Our work is also related to task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Li et al., 2017; Gaˇsi´c et al., 2011; Wang et al., 2018, 2019). Existing systems have mainly focused on slot-filling tasks (e.g., booking a hotel). In such tasks, a set of system actions can be pre-defined based on the business logic and slots. In contrast, the system actions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain dialogue management. Chen et al. (2018) used graph neural networks (Battaglia et al., 2018; Li et al., 2015; Scarselli et al., 2009; Niepert et al., 2016) to improve the sample-efficiency of reinforcement learning. He et al. (2017) used DynoNet to incorporate structured information in the collaborative dialogue setting. Compared with them, our method is a combination of the graph neural networks and hierarchical reinforcement learning, and experiments pr"
D19-1185,D15-1281,0,0.0549047,"Missing"
D19-1185,P19-1361,1,0.850777,"Missing"
D19-1185,D18-1415,1,0.830374,"an et al., 2018; Graciarena et al., 2006) or both (Krishnamurthy et al., 2018; P´erez-Rosas et al., 2015) to train a classification model. In their work, the definition of deception is telling a lie. Besides, existing work requires labeled data, which is often hard to get. In contrast, we focus on detecting identity fraud through multi-turn interactions and use reinforcement learning to explore the anti-fraud policy without any labeled data. Dialogue System. Our work is also related to task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Li et al., 2017; Gaˇsi´c et al., 2011; Wang et al., 2018, 2019). Existing systems have mainly focused on slot-filling tasks (e.g., booking a hotel). In such tasks, a set of system actions can be pre-defined based on the business logic and slots. In contrast, the system actions in our task are selecting nodes in the KG to generate questions. Thus, the structured information is important in our task. Besides, some works also try to model structured information in dialogue systems. For example, Peng et al. (2017) used hierarchical reinforcement learn1769 ing (Vezhnevets et al., 2017; Kulkarni et al., 2016; Florensa et al., 2017) to design multi-domain"
D19-1185,E17-1042,0,0.0769615,"Missing"
D19-1185,P17-1062,0,0.0317656,") πtw (va |Et (vp )) ∝ exp (Ww [Et (vp ), Et (va )] + bw ) πtw (dw |Et (vp )) ∝ exp (Ww [Et (vp ), E(dw )] + bw ) (4) where {Wm , Ww , bm , bw , E(dm ), E(dw )} are parameters, Et (vu ) and Et (vp ) are dialogue states of the manager and worker in the t-th turn, E(dm ) is the encoding of the manager’s terminal action which has the same dimension as Et (vp ), and E(dw ) is the encoding of the worker’s terminal action which has the same dimension as Et (va ). Besides, to prevent the two agents from making decisions in haste, domain rules are applied to their dialogue policies by “Action Mask” (Williams et al., 2017). Specifically, domain rules are defined as follows. First, only after all or at least three answer nodes related to a worker have been explored can the worker make the decision. Second, only after all workers have made decisions or at least one worker’s decision is “Fraud” can the manager make the final decision. 4 4.1 Training Reward Function We expect the system can give correct decisions about applicants within minimum turns. Thus, at the end of each dialogue, the manager receives m for correct decision, or a a positive reward rcrt m negative reward −rwrg for wrong decision. If the manager"
D19-1297,W14-4408,0,0.0520425,"Missing"
D19-1297,C10-1039,0,0.25822,"posted by a businessperson in our dataset shows the effect of attribute information on summarizing review. Underlined words in the review indicate the important sentences that the businessperson care about when summarizing the review. Bold word in businessperson-specific vocabulary shows the businessperson’s word-using habits may help to generate the summary. Introduction Review summarization aims to generate a condensed summary for a review or multiple reviews1 . Dominating studies can be divided into two groups: extractive and abstractive approaches. Extractive approaches (Hu and Liu, 2004; Ganesan, 2010) extract sentences or phrases from a review, while abstractive methods (Gerani et al., 2014; Wang and Ling, 2016; Yang et al., 2018a; Li et al., 2019; Gao et al., 2019) summarize a review by employing graph-based or sequence-tosequence (S2S) models which can generate new phrases and sentences that do not appear in the review. Despite the remarkable progress of previous studies, they typically only focus on review content and neglect the attribute information of users who post these reviews (e.g., gender, age, and occupation). Actually, such information is vital for generating summaries, which"
D19-1297,D14-1168,0,0.124873,"summarizing review. Underlined words in the review indicate the important sentences that the businessperson care about when summarizing the review. Bold word in businessperson-specific vocabulary shows the businessperson’s word-using habits may help to generate the summary. Introduction Review summarization aims to generate a condensed summary for a review or multiple reviews1 . Dominating studies can be divided into two groups: extractive and abstractive approaches. Extractive approaches (Hu and Liu, 2004; Ganesan, 2010) extract sentences or phrases from a review, while abstractive methods (Gerani et al., 2014; Wang and Ling, 2016; Yang et al., 2018a; Li et al., 2019; Gao et al., 2019) summarize a review by employing graph-based or sequence-tosequence (S2S) models which can generate new phrases and sentences that do not appear in the review. Despite the remarkable progress of previous studies, they typically only focus on review content and neglect the attribute information of users who post these reviews (e.g., gender, age, and occupation). Actually, such information is vital for generating summaries, which contains the following characteristics. (1) People with different attributes may care about"
D19-1297,W04-3252,0,0.0275123,"tor and σ is the sigmoid 4 Experiments 4.1 Evaluation Metric We exploit ROUGE (Lin, 2004) as our evaluation metric. ROUGE scores reported in this paper are computed by Pyrouge package 4 . 4.2 Comparison Methods In the experiments, we compare our model with several strong baseline methods, which can be divided into two types: extractive and abstractive approaches. L EAD 1 is an extractive approach which selects the first sentence in review as summary. L EX R ANK (Erkan and Radev, 2004) is also a famous extractive approach that computes text centrality based on PageRank algorithm. T EX T R ANK (Mihalcea and Tarau, 2004) is an unsupervised algorithm based on weighted-graphs. S2SATT is a sequence to sequence model with attention implemented by us. S EASS (Zhou et al., 2017) employs a selective encoding model to control the information flow from encoder to decoder. P GN (See et al., 2017) copies words from the source text via pointing, while retaining the ability to produce novel words through the generator. 4.3 Implementation Details Model Parameters The vocabulary is collected from the TripAtt training data. We lowercase the text, and there are 362,103 unique word types. We 3004 4 pypi.python.org/pypi/pyrouge"
D19-1297,C18-1188,0,0.0603942,"Missing"
D19-1297,E09-1059,0,0.0470133,"ing styles of female users in A SN, our model can generate “lovely” correctly, although it does not appear in the review. 6 Related Work Review summarization belongs to sentiment analysis (Liu, 2016; Xia et al., 2015), which is a large area in natural language processing and contains sentiment classification (Li and Zong, 2008; Xia et al., 2011; Li et al., 2016, 2018b), emotion detection (Li et al., 2015), spam detection (Wang et al., 2017) and so on. There are two mainstream approaches for the problem: extractive and abstractive approaches. A key task in extractive methods (Hu and Liu, 2004; Lerman et al., 2009; Xiong and Litman, 2014; Kunneman et al., 2018) is to identify important text units. For example, Hu and Liu (2004) first recognize the frequent product features and then attach extracted opinion sentences to the corresponding feature. Xiong and Litman (2014) exploit review helpfulness for review summarization. However, many studies (Carenini et al., 2013; Fabbrizio et al., 2014) have shown that abstractive approaches may be more appropriate for summarizing evaluative text than extractive ones. That is also the reason why we build our attribute-aware model based on abstractive methods. Abstra"
D19-1297,C18-1079,1,0.830889,"hard to generate the word “meeting” when summarizing the review due to its missing. Intuitively, “meeting” belongs to businessperson-specific vocabulary, and such an attribute-specific vocabulary can be incorporated to further improve the summarization performance. Inspired by the above observations, we propose a model called Attribute-aware Sequence Network (A SN) to consider attribute information into review summarization. Specifically, A SN is based on sequence to sequence models (S2S), which are popular methods in text summarization (Rush et al., 2015; See et al., 2017; Zhu et al., 2018; Li et al., 2018a) and review summarization (Wang and Ling, 2016; Ma et al., 2018). A SN updates over standard S2S are three-fold. First, except for standard encoder and decoder in S2S, we design an attribute encoder, which encodes attribute preference for using words into attribute embedding. Second, an attribute-aware review encoder is proposed to generate attribute-aware review representation. It utilizes a bidirectional-LSTM to encode a review, and then imports an attribute-based selective mechanism to select important information of it to obtain a better review representation. Third, we propose an attrib"
D19-1297,P15-1101,0,0.0309303,"te” to summarize review while these words rarely appear in summaries from male users. Without considering the gender bias, S2SATT and P GN can not generate the summarization well. Incorporating such writ3007 ing styles of female users in A SN, our model can generate “lovely” correctly, although it does not appear in the review. 6 Related Work Review summarization belongs to sentiment analysis (Liu, 2016; Xia et al., 2015), which is a large area in natural language processing and contains sentiment classification (Li and Zong, 2008; Xia et al., 2011; Li et al., 2016, 2018b), emotion detection (Li et al., 2015), spam detection (Wang et al., 2017) and so on. There are two mainstream approaches for the problem: extractive and abstractive approaches. A key task in extractive methods (Hu and Liu, 2004; Lerman et al., 2009; Xiong and Litman, 2014; Kunneman et al., 2018) is to identify important text units. For example, Hu and Liu (2004) first recognize the frequent product features and then attach extracted opinion sentences to the corresponding feature. Xiong and Litman (2014) exploit review helpfulness for review summarization. However, many studies (Carenini et al., 2013; Fabbrizio et al., 2014) have"
D19-1297,P08-2065,1,0.701026,"rent genders are different. Female users often utilize “lovely”, “beautiful”, “cute” to summarize review while these words rarely appear in summaries from male users. Without considering the gender bias, S2SATT and P GN can not generate the summarization well. Incorporating such writ3007 ing styles of female users in A SN, our model can generate “lovely” correctly, although it does not appear in the review. 6 Related Work Review summarization belongs to sentiment analysis (Liu, 2016; Xia et al., 2015), which is a large area in natural language processing and contains sentiment classification (Li and Zong, 2008; Xia et al., 2011; Li et al., 2016, 2018b), emotion detection (Li et al., 2015), spam detection (Wang et al., 2017) and so on. There are two mainstream approaches for the problem: extractive and abstractive approaches. A key task in extractive methods (Hu and Liu, 2004; Lerman et al., 2009; Xiong and Litman, 2014; Kunneman et al., 2018) is to identify important text units. For example, Hu and Liu (2004) first recognize the frequent product features and then attach extracted opinion sentences to the corresponding feature. Xiong and Litman (2014) exploit review helpfulness for review summarizat"
D19-1297,D15-1044,0,0.0327609,"ering the attribute information “businessperson”, it is hard to generate the word “meeting” when summarizing the review due to its missing. Intuitively, “meeting” belongs to businessperson-specific vocabulary, and such an attribute-specific vocabulary can be incorporated to further improve the summarization performance. Inspired by the above observations, we propose a model called Attribute-aware Sequence Network (A SN) to consider attribute information into review summarization. Specifically, A SN is based on sequence to sequence models (S2S), which are popular methods in text summarization (Rush et al., 2015; See et al., 2017; Zhu et al., 2018; Li et al., 2018a) and review summarization (Wang and Ling, 2016; Ma et al., 2018). A SN updates over standard S2S are three-fold. First, except for standard encoder and decoder in S2S, we design an attribute encoder, which encodes attribute preference for using words into attribute embedding. Second, an attribute-aware review encoder is proposed to generate attribute-aware review representation. It utilizes a bidirectional-LSTM to encode a review, and then imports an attribute-based selective mechanism to select important information of it to obtain a bett"
D19-1297,P17-1099,0,0.251939,"information “businessperson”, it is hard to generate the word “meeting” when summarizing the review due to its missing. Intuitively, “meeting” belongs to businessperson-specific vocabulary, and such an attribute-specific vocabulary can be incorporated to further improve the summarization performance. Inspired by the above observations, we propose a model called Attribute-aware Sequence Network (A SN) to consider attribute information into review summarization. Specifically, A SN is based on sequence to sequence models (S2S), which are popular methods in text summarization (Rush et al., 2015; See et al., 2017; Zhu et al., 2018; Li et al., 2018a) and review summarization (Wang and Ling, 2016; Ma et al., 2018). A SN updates over standard S2S are three-fold. First, except for standard encoder and decoder in S2S, we design an attribute encoder, which encodes attribute preference for using words into attribute embedding. Second, an attribute-aware review encoder is proposed to generate attribute-aware review representation. It utilizes a bidirectional-LSTM to encode a review, and then imports an attribute-based selective mechanism to select important information of it to obtain a better review represen"
D19-1297,N16-1007,0,0.202781,"Underlined words in the review indicate the important sentences that the businessperson care about when summarizing the review. Bold word in businessperson-specific vocabulary shows the businessperson’s word-using habits may help to generate the summary. Introduction Review summarization aims to generate a condensed summary for a review or multiple reviews1 . Dominating studies can be divided into two groups: extractive and abstractive approaches. Extractive approaches (Hu and Liu, 2004; Ganesan, 2010) extract sentences or phrases from a review, while abstractive methods (Gerani et al., 2014; Wang and Ling, 2016; Yang et al., 2018a; Li et al., 2019; Gao et al., 2019) summarize a review by employing graph-based or sequence-tosequence (S2S) models which can generate new phrases and sentences that do not appear in the review. Despite the remarkable progress of previous studies, they typically only focus on review content and neglect the attribute information of users who post these reviews (e.g., gender, age, and occupation). Actually, such information is vital for generating summaries, which contains the following characteristics. (1) People with different attributes may care about different aspects2 ."
D19-1297,P17-1034,1,0.826919,"e words rarely appear in summaries from male users. Without considering the gender bias, S2SATT and P GN can not generate the summarization well. Incorporating such writ3007 ing styles of female users in A SN, our model can generate “lovely” correctly, although it does not appear in the review. 6 Related Work Review summarization belongs to sentiment analysis (Liu, 2016; Xia et al., 2015), which is a large area in natural language processing and contains sentiment classification (Li and Zong, 2008; Xia et al., 2011; Li et al., 2016, 2018b), emotion detection (Li et al., 2015), spam detection (Wang et al., 2017) and so on. There are two mainstream approaches for the problem: extractive and abstractive approaches. A key task in extractive methods (Hu and Liu, 2004; Lerman et al., 2009; Xiong and Litman, 2014; Kunneman et al., 2018) is to identify important text units. For example, Hu and Liu (2004) first recognize the frequent product features and then attach extracted opinion sentences to the corresponding feature. Xiong and Litman (2014) exploit review helpfulness for review summarization. However, many studies (Carenini et al., 2013; Fabbrizio et al., 2014) have shown that abstractive approaches ma"
D19-1297,C14-1187,0,0.0226266,"users in A SN, our model can generate “lovely” correctly, although it does not appear in the review. 6 Related Work Review summarization belongs to sentiment analysis (Liu, 2016; Xia et al., 2015), which is a large area in natural language processing and contains sentiment classification (Li and Zong, 2008; Xia et al., 2011; Li et al., 2016, 2018b), emotion detection (Li et al., 2015), spam detection (Wang et al., 2017) and so on. There are two mainstream approaches for the problem: extractive and abstractive approaches. A key task in extractive methods (Hu and Liu, 2004; Lerman et al., 2009; Xiong and Litman, 2014; Kunneman et al., 2018) is to identify important text units. For example, Hu and Liu (2004) first recognize the frequent product features and then attach extracted opinion sentences to the corresponding feature. Xiong and Litman (2014) exploit review helpfulness for review summarization. However, many studies (Carenini et al., 2013; Fabbrizio et al., 2014) have shown that abstractive approaches may be more appropriate for summarizing evaluative text than extractive ones. That is also the reason why we build our attribute-aware model based on abstractive methods. Abstractive approaches (Ganesa"
D19-1297,W04-1013,0,0.0201493,"generating summaries, which is the 4-th strategy called Attribute Memory Generation strategy. The generation probability pmgn ∈ [0, 1] for timestep t is calculated from the context vector ct , the decoder state st and the vocabulary state mt : pmgn = σ(Wmg [ct ; st ; mt ] + bmg ) function. Next pmgn is used as a soft switch to choose between generating a word from the target vocabulary Vt or coping a word from attributespecific vocabulary. (9) where Wmg , bmg are learnable parameters, [; ] is the concatenating operator and σ is the sigmoid 4 Experiments 4.1 Evaluation Metric We exploit ROUGE (Lin, 2004) as our evaluation metric. ROUGE scores reported in this paper are computed by Pyrouge package 4 . 4.2 Comparison Methods In the experiments, we compare our model with several strong baseline methods, which can be divided into two types: extractive and abstractive approaches. L EAD 1 is an extractive approach which selects the first sentence in review as summary. L EX R ANK (Erkan and Radev, 2004) is also a famous extractive approach that computes text centrality based on PageRank algorithm. T EX T R ANK (Mihalcea and Tarau, 2004) is an unsupervised algorithm based on weighted-graphs. S2SATT i"
D19-1297,C18-1095,0,0.0811857,"he review indicate the important sentences that the businessperson care about when summarizing the review. Bold word in businessperson-specific vocabulary shows the businessperson’s word-using habits may help to generate the summary. Introduction Review summarization aims to generate a condensed summary for a review or multiple reviews1 . Dominating studies can be divided into two groups: extractive and abstractive approaches. Extractive approaches (Hu and Liu, 2004; Ganesan, 2010) extract sentences or phrases from a review, while abstractive methods (Gerani et al., 2014; Wang and Ling, 2016; Yang et al., 2018a; Li et al., 2019; Gao et al., 2019) summarize a review by employing graph-based or sequence-tosequence (S2S) models which can generate new phrases and sentences that do not appear in the review. Despite the remarkable progress of previous studies, they typically only focus on review content and neglect the attribute information of users who post these reviews (e.g., gender, age, and occupation). Actually, such information is vital for generating summaries, which contains the following characteristics. (1) People with different attributes may care about different aspects2 . For example, when"
D19-1297,P17-1101,0,0.342417,"represent Attribute Memory Prediction strategy and Attribute Memory Generation strategy, respectively. |a|K into vector {Ai }i=1 , and we get matrix A, which is also called attribute-specific vocabulary memory. Then, we use a nonlinear layer to merge A’s words belonging to attribute ai into embedding ai (See Equation (1)), which can only represent attribute ai . After that, we merge these |a |attribute embeddings a1 , a1 , ..., a|a |into a nonlinear layer to get attribute embedding a (See Equation (2)), which is used to represent attribute vector a. different content of a review. Inspired by (Zhou et al., 2017), we propose an attribute-based selective mechanism to select the important information from review for users with different attributes. The selective mechanism can construct a tailored representation of review x by considering a. In detail, our attribute-based selective network takes attribute vector a and the encoder hidden state hi as input, and outputs a gate vector gatei to select hi . ai = σ(Wa A(i−1)×K+1:i×K + ba ) gatei = σ(Wk [hi ; a] + bk ) (1) h0i i=|a| a = σ( X wa ai + ba ) (2) i=1 where Wa , wa , ba , and ba are learnable parameters, and σ denotes sigmoid function. 3.4 Attribute-a"
D19-1297,D18-1448,1,0.855053,"nessperson”, it is hard to generate the word “meeting” when summarizing the review due to its missing. Intuitively, “meeting” belongs to businessperson-specific vocabulary, and such an attribute-specific vocabulary can be incorporated to further improve the summarization performance. Inspired by the above observations, we propose a model called Attribute-aware Sequence Network (A SN) to consider attribute information into review summarization. Specifically, A SN is based on sequence to sequence models (S2S), which are popular methods in text summarization (Rush et al., 2015; See et al., 2017; Zhu et al., 2018; Li et al., 2018a) and review summarization (Wang and Ling, 2016; Ma et al., 2018). A SN updates over standard S2S are three-fold. First, except for standard encoder and decoder in S2S, we design an attribute encoder, which encodes attribute preference for using words into attribute embedding. Second, an attribute-aware review encoder is proposed to generate attribute-aware review representation. It utilizes a bidirectional-LSTM to encode a review, and then imports an attribute-based selective mechanism to select important information of it to obtain a better review representation. Third, we"
D19-1302,P15-1166,0,0.035152,"stance, En2ZhSum dataset contains a total of 370,687 documents with corresponding summaries in both MS-Decoder gray , 94 , had … Rod gray was (2) where dk is the dimension of the key. Finally, the output values are concatenated and projected by a feed-forward layer to get final values: Attention(QWiQ , KWiK , V CLS-Decoder (1) N X (1) (1) logP(yt |y&lt;t , x; θ) + (2) N X t=1 (2) (2) logP(yt |y&lt;t , x; θ) t=1 (4) where y (1) and y (2) are the outputs of two tasks. CLS+MT. Since CLS input-output pairs are different from MT input-output pairs, we consider adopting the alternating training strategy (Dong et al., 2015), which optimizes each task for a fixed number of mini-batches before switching to the next task, to jointly train CLS and MT. For MT task, we employ 2.08M6 sentence pairs from LDC corpora with CLS dataset to train CLS+MT. 4 Experiments 4.1 Experimental Settings For English, we apply two different granularities of segmentation, i.e., words and subwords (Sennrich et al., 2016). We lowercase all English characters. We truncate the input to 200 words and the output to 120 words (150 characters for Chinese output) . For Chinese, we employ three different 6 LDC2000T50, LDC2002L27, LDC2002T01, LDC20"
D19-1302,D17-1222,0,0.036895,"Missing"
D19-1302,P19-1305,0,0.25288,"terparts. The final summary is generated by maximizing both the salience and translation quality of the PAS elements. However, all these researches belong to the pipeline paradigm which not only relies heavily on hand-crafted features but also causes error propagation. End-to-end deep learning has proven to be able to alleviate these two problems, while it has been absent due to the lack of largescale training data. Recently, Ayana et al. (2018) present zero-shot cross-lingual headline generation based on existing parallel corpora of translation and monolingual headline generation. Similarly, Duan et al. (2019) propose to use monolingual abstractive sentence summarization system to teach zero-shot cross-lingual abstractive sentence summarization on both summary word generation and attention. Although great efforts have been made in cross-lingual summarization, how to automatically build a high-quality large-scale cross-lingual summarization dataset remains unexplored. In this paper, we focus on English-to-Chinese and Chinese-to-English CLS and try to automatically construct two large-scale corpora respectively. In addition, based on the two corpora, we perform several end-to-end training methods not"
D19-1302,W04-1013,0,0.0392245,"nsformer-based NCLS models where the input and output are different granularities combinations of units. CLS+MS: It refers to the multi-task NCLS model which accepts an input text and simultaneously performs text generation for both CLS and MS tasks and calculates the total losses. CLS+MT: It trains CLS and MT tasks via alternating training strategy. Specifically, we optimize the CLS task in a mini-batch, and we optimize the MT task in the next mini-batch. 4.3 Experimental Results and Analysis Comparison between NCLS with baselines. We evaluate different models with the standard ROUGE metric (Lin, 2004), reporting the F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L. The results are presented in Table 4. 7 https://translate.google.com/ The parameter for ROUGE script here is “-c 95 -r 1000 -n 2 -a”. 3058 8 Model Unit En2ZhSum En2ZhSum* Zh2EnSum Zh2EnSum* RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) RG1-RG2-RGL(↑) TETran – 26.12-10.59-23.21 26.15-10.60-23.24 22.81- 7.17-18.55 23.09- 7.33-18.74 GETran – 28.17-11.38-25.75 28.19-11.40-25.77 24.03- 8.91-19.92 24.34- 9.14-20.13 TLTran c-c w-w sw-sw – 30.20-12.20-27.02 – – 30.22-12.20-27.04 – 32.85-15.34-29.21 31.11-13.23-27.55 33.64-15.58-29.74 33.01-15"
D19-1302,W00-0405,0,0.14307,"2003) translate the Hindi document to English and then generate the English headline for it. Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization corpora are currently available. They train a neural abstractive summarization model on noisy English documents and clean English reference summaries. Then the model can learn to produce fluent summaries from disfluent inputs, which allows generating summaries for translated documents. Orasan and Chiorean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers. Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside"
D19-1302,P16-1154,0,0.0724329,"Missing"
D19-1302,D15-1229,0,0.426112,"cale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel approach to directly address the lack of data. Specifically, we propose a simple yet effective round-trip translation strategy to obtain cross-lingual documentsummary pairs from existing monolingual summarization datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015). More details can be found in Section 2 below. Based on the dataset that we have constructed, we propose end-to-end models on cross-lingual summarization, which we refer to as Neural CrossLingual Summarization (NCLS). Furthermore, we consider improving CLS with two related tasks: MS and MT. We incorporate the training process of MS and MT into that of CLS under the multitask learning framework (Caruana, 1997). Experimental results demonstrate that NCLS achieves remarkable improvement over traditional pipeline paradigm. In addition, both MS and MT can significantly help to produce better summa"
D19-1302,orasan-chiorean-2008-evaluation,0,0.889347,"here: http://www. nlpr.ia.ac.cn/cip/dataset.htm. 1 Introduction Given a document in one source language, crosslingual summarization aims to produce a summary in a different target language, which can help people efficiently acquire the gist of an article in a foreign language. Traditional approaches to CLS are based on the pipeline paradigm, which either first translates the original document into target language and then summarizes the translated document (Leuski et al., 2003) or first summarizes the original document and then translates the ∗ summary into target language (Lim et al., 2004; Orasan and Chiorean, 2008; Wan et al., 2010). However, the current machine translation (MT) is not perfect, which results in the error propagation problem. Although end-to-end deep learning has made great progress in natural language processing, no one has yet applied it to CLS due to the lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we i"
D19-1302,N19-1204,0,0.272835,"ent. The CLS+MS summary matches the gold summary better. The flaws of both of them are that they do not reflect the “scale” in the original text. In conclusion, our methods can produce more accurate summaries than baselines. 5 Related Work Cross-lingual summarization has been proposed to present the most salient information of a source document in a different language, which is very important in the field of multilingual information processing. Most of the existing methods handle the task of CLS via simply applying two typical translation schemes, i.e., early translation (Leuski et al., 2003; Ouyang et al., 2019) and late translation (Orasan and Chiorean, 2008; Wan et al., 2010). The early translation scheme first translates the original document into target language and then generates the summary of the translated document. The late translation scheme first summarizes the original document into a summary in the source language and then translates it into target language. 3061 Leuski et al. (2003) translate the Hindi document to English and then generate the English headline for it. Ouyang et al. (2019) present a robust abstractive summarization system for low resource languages where no summarization"
D19-1302,P17-1099,0,0.189683,"Missing"
D19-1302,P16-1162,0,0.0613843,"1 (2) (2) logP(yt |y&lt;t , x; θ) t=1 (4) where y (1) and y (2) are the outputs of two tasks. CLS+MT. Since CLS input-output pairs are different from MT input-output pairs, we consider adopting the alternating training strategy (Dong et al., 2015), which optimizes each task for a fixed number of mini-batches before switching to the next task, to jointly train CLS and MT. For MT task, we employ 2.08M6 sentence pairs from LDC corpora with CLS dataset to train CLS+MT. 4 Experiments 4.1 Experimental Settings For English, we apply two different granularities of segmentation, i.e., words and subwords (Sennrich et al., 2016). We lowercase all English characters. We truncate the input to 200 words and the output to 120 words (150 characters for Chinese output) . For Chinese, we employ three different 6 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07 3057 Model Gu et al. (2016) Li et al. (2017) Transformer ROUGE-1 ROUGE-2 ROUGE-L 35.00 36.99 39.71 22.30 24.15 27.45 32.00 34.21 37.13 Model See et al. (2017) Transformer Table 2: Performance of our implemented transformerbased monolingual summarization model on LCSTS. granularities of segmentation: characters, words, and"
D19-1302,P11-1155,0,0.51022,"orean (2008) summarize the Romanian news with the maximal marginal relevance method (Goldstein et al., 2000) and produce the English summaries for English speakers. Wan et al. (2010) adopt the late translation scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS. Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression. They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bil"
D19-1302,P10-1094,0,0.838602,"ac.cn/cip/dataset.htm. 1 Introduction Given a document in one source language, crosslingual summarization aims to produce a summary in a different target language, which can help people efficiently acquire the gist of an article in a foreign language. Traditional approaches to CLS are based on the pipeline paradigm, which either first translates the original document into target language and then summarizes the translated document (Leuski et al., 2003) or first summarizes the original document and then translates the ∗ summary into target language (Lim et al., 2004; Orasan and Chiorean, 2008; Wan et al., 2010). However, the current machine translation (MT) is not perfect, which results in the error propagation problem. Although end-to-end deep learning has made great progress in natural language processing, no one has yet applied it to CLS due to the lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel ap"
D19-1302,D15-1012,0,0.375044,"n scheme for the task of English-to-Chinese CLS. They extract English sentences considering both the informativeness and translation quality of sentences and automatically translate the English summary into the final Chinese summary. The above researches only make use of the information from only one language side. Some methods have been proposed to improve CLS with bilingual information. Wan (2011) proposes two graph-based summarization methods to leverage both the English-side and Chineseside information in the task of English-to-Chinese CLS. Inspired by the phrase-based translation models, Yao et al. (2015) introduce a compressive CLS, which simultaneously performs sentence selection and compression. They calculate the sentence scores based on the aligned bilingual phrases obtained by MT service and perform compression via deleting redundant or poorly translated phrases. Zhang et al. (2016) propose an abstractive CLS which constructs a pool of bilingual concepts represented by the bilingual elements of the source-side predicate-argument structures (PAS) and the target-side counterparts. The final summary is generated by maximizing both the salience and translation quality of the PAS elements. Ho"
D19-1302,D18-1448,1,0.906264,"he lack of large-scale supervised dataset. The input and output of CLS are in two different languages, which makes the data acquisition much more difficult than monolingual summarization (MS). To the best of our knowledge, no one has studied how to automatically build a high-quality large-scale CLS dataset. Therefore, in this work, we introduce a novel approach to directly address the lack of data. Specifically, we propose a simple yet effective round-trip translation strategy to obtain cross-lingual documentsummary pairs from existing monolingual summarization datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015). More details can be found in Section 2 below. Based on the dataset that we have constructed, we propose end-to-end models on cross-lingual summarization, which we refer to as Neural CrossLingual Summarization (NCLS). Furthermore, we consider improving CLS with two related tasks: MS and MT. We incorporate the training process of MS and MT into that of CLS under the multitask learning framework (Caruana, 1997). Experimental results demonstrate that NCLS achieves remarkable improvement over traditional pipeline paradigm. In addition, both MS and MT can significantly help to pr"
D19-1330,P15-1166,0,0.0443813,"redicted in the other language. Experimental results on IWSLT and WMT datasets demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of"
D19-1330,N16-1101,0,0.0597358,"demonstrate that our method can obtain significant improvements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when"
D19-1330,P07-2045,0,0.0070878,"respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as devlopment set and test set respectively. Besides, we also perform En→De/Fr translation in large scale WMT142 datasets. We use newstest2014 as test set. En→Zh/Ja: For this translation task, the training sets of En→Zh and En→Ja consist of 231K, 223K sentence pairs. We tokenize the English sentences using a script from Moses (Koehn et al., 2007), and we segment Chinese and Japanese data by jieba3 and mecab4 . We use BPE method (Sennrich et al., 2016b) to encode the source side sentences and the combination of target side sentences respectively and limit the vocabularies of both sides to the most frequent 10k tokens. En→De/Fr: We conduct this translation task on two different settings. One setting is using training set of IWSLT datasets which contains 206K sentence pairs for En→De and 233K sentence pairs for En→Fr. We follow the common practice to tokenize and lowercase all words. Sentences are encoded using BPE, which has a shared vo"
D19-1330,N16-1046,0,0.0605969,"Missing"
D19-1330,W18-6309,0,0.0131386,"g (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentence"
D19-1330,D18-1103,0,0.0205996,"l., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targe"
D19-1330,P02-1040,0,0.103586,"t sides. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For decoding, we set beam size to be k = 4 and length penalty α = 0.6. All our methods are trained and tested on single Nvidia P40 GPU. We investigate the impact of different λs in our synchronous attention model. As shown in Table 1, when λ=0.1, the translation results perform best on development set for both En→Zh/Ja and En→De/Fr tasks, and we will use this setting in the subsequent experiments. 5 Results and Analysis The translation performance of IWSLT datasets is evaluated by case-insensitive BLEU4 (Papineni et al., 2002) for En→De/Fr task and characterlevel BLEU5 for En→Zh/Ja task. For WMT14 datasets, we calculate the case-sensitive BLEU4 the same as previous work. In our experiments, the NMT models trained on individual language pair are denoted by Indiv. 5.1 Results on IWSLT Table 2 shows the main translation results of En→Zh/Ja and En→De/Fr on IWSLT datasets. We also conduct a typical one-to-many translation adopting Johnson et al. (2017) method on Transformer as our another baseline model, referred to Multi. Compared with Indiv, we can see that Multi achieves better results on all cases, which can be attr"
D19-1330,P16-1009,0,0.0332944,"data D = (x1, y 1, y 2?) ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as de"
D19-1330,P16-1162,0,0.0488093,"data D = (x1, y 1, y 2?) ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as de"
D19-1330,D18-1326,1,0.779143,"l., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targets can interact with each other. Introductio"
D19-1330,P19-1117,1,0.710862,"ements over both conventional Neural Machine Translation (NMT) model and multilingual NMT model. 1 Chinese Neural Machine Translation (NMT) based on the encoder-decoder framework has significantly improved translation quality due to its powerful endto-end modeling (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5"
D19-1330,D16-1160,1,0.852653,") ∪ (x2, y 1?, y 2 ) , which can be used to train our synchronous translation model mentioned above. |yi | Õ log P(yi2 |x)) (5) i=1 When calculating P(yi1 |x), except for the context from source side x, our synchronous method 1 as condiemploys not only previous reference y&lt;i tion, but the previous context of the other decoder 2 . The calculation process of P(y 2 |x) reference y&lt;i i is similar. However, the practical situation is that the triple data is limited and hard to be collected. In this work, we construct the trilingual training corpus by data augmenting method (Sennrich et al., 2016a; Zhang and Zong, 2016). To achieve this, we first learn two independent translation models Model-1 and Model-2 on the bilingual training data (x1, y 1 ) and (x2, y 2 ) separately. Then, Model-1 and Model-2 are employed to decode the input sentences x2 and x1 , resulting in pseudo training data (x2, y 1?) and (x1, y 2?), respectively. Data We evaluate our proposed synchronous method on two translation tasks, which include English→Chinese/Japanese (briefly, En→Zh/Ja) and English→German/French (briefly, En→De/Fr) on IWSLT1 datasets. The IWSLT.TED.tst2013 and IWSLT.TED.tst2014 are employed as devlopment set and test se"
D19-1330,Q19-1006,1,0.681102,"where x is source sentence, y 1, y 2 are target sentences corresponding to two different languages. At time-step i, we have generated the first i − 1 tokens of language-1 y 1 and the first i − 1 tokens of language-2 y 2 . Then both languages predictions can be utilized together with source sentence to generate tokens yi1 and yi2 . This interaction between two languages is realized by synchronous attention model, which will be detailed in the following subsection. It should be noted that the two language sentences can be generated in different directions (Liu et al., 2016; Zhang et al., 2018; Zhou et al., 2019), which means language-1 can be produced in leftto-right (L2R) manner while language-2 in rightto-left (R2L) manner. We will analyze the effect of different decoding manners in Sec. 5.2. 3.2 Synchronous Attention Model Synchronous attention model (SyncAtt) is shown in Figure 2, in which inputs of two decoders contain queries (Q1, Q2 ), keys (K1, K2 ), and values 0 (V1, V2 ) separately. The new hidden states (Hi ) can be computed by our proposed synchronous atten3351 tion as follows: 0 H1 =SyncAtt (Q1, [K1 ; K2 ], [V1 ; V2 ]) 0 H2 =SyncAtt (Q2, [K1 ; K2 ], [V1 ; V2 ]) (3) where synchronous atte"
D19-1330,N16-1004,0,0.0662303,"sentences respectively and limit the vocabularies of both sides to the most frequent 10k tokens. En→De/Fr: We conduct this translation task on two different settings. One setting is using training set of IWSLT datasets which contains 206K sentence pairs for En→De and 233K sentence pairs for En→Fr. We follow the common practice to tokenize and lowercase all words. Sentences are encoded using BPE, which has a shared vocabulary of 10K tokens. At last, we construct pseudo triple data by the method described in Sec. 3.3. For the other setting, we extract the trilingual subset in WMT14 inspired by Zoph and Knight (2016), which includes about 2.43M sentence triples. We use 37K shared BPE tokens as vocabulary. 4.2 Training Details We implement our synchronous translation based on the tensor2tensor5 library. We train our models using the configuration transformer base adopted 3352 1 https://wit3.fbk.eu 2 http://www.statmt.org/wmt14/translation-task.html 3 https://github.com/fxsjy/jieba 4 http://taku910.github.io/mecab 5 https://github.com/tensorflow/tensor2tensor Hyperparamter (λ) 0.1 0.2 0.3 0.4 0.5 En-De/Fr En-De En-Fr 30.95 43.01 30.77 42.99 30.55 42.99 29.60 42.52 29.19 41.87 En-Zh/Ja En-Zh En-Ja 16.33 18.8"
D19-1330,D18-1039,0,0.0154149,"al., 2015; Vaswani et al., 2017; Gehring et al., 2017; Hassan et al., 2018; Zhang and Zong, 2015). This paradigm facilitates the development of multilingual NMT (Dong et al., 2015; Luong et al., 2016; Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Lakew et al., 2017; Tan et al., 2019; Wang et al., 2019), which handles multiple language pairs in one model, with the benefit of simplifying offline model training and easing online maintenance cost. Although multilingual NMT attempts to utilize the complementary information of different languages (Lu et al., 2018; Neubig and Hu, 2018; Platanios et al., 2018; Wang et al., 2018), all of the models handle one language pair at each moment for both training and testing. However, we find that the generation process of different target languages can help each other. For example in Figure 1, when decoding the Chinese word “书” meaning “book” at step t = 5, the predicted Japanese word Zhang is the corresponding author. 我 I 买了 bought 一本 一本 a 书 book pad 今日 today 一冊 a の 本 book を 買い bought t=1 t=2 t=3 t=4 t=5 pad ました Japanese Figure 1: An example of an English sentence translated into Chinese and Japaneses sentences, in which two targets can interact with eac"
fafiotte-etal-2004-collecting,C94-1017,0,\N,Missing
I05-2002,C96-2102,0,0.0883846,"Missing"
I08-1017,A97-1049,0,0.0303589,"Missing"
I08-1017,P06-1039,0,0.0753995,"Missing"
I08-1017,J02-4001,0,0.027207,"gment level. In our approach, the semi-Markov CRF model is employed for segment labeling. The preliminary experiments have shown that the approach does outperform all other traditional supervised and unsupervised approaches to document summarization. 1 Introduction Document summarization has been a rapidly evolving subfield of Information Retrieval (IR) since (Luhn, 1958). A summary can be loosely defined as a text that is produced from one or more texts and conveys important information of the original text(s). Usually it is no longer than half of the original text(s) or, significantly less (Radev et al., 2002). Recently, many evaluation competitions (like the Document Understanding Conference DUC “http://duc.nist.gov”, in the style of NIST’s TREC), provided some sets of training corpus. It is obvious that, in the age of information explosion, document summarization will be greatly helpful to the internet users; besides, the techniques it uses can also find their applications in speech techniques and multimedia document retrieval, etc. The approach to summarizing can be categorized in many ways. Some of them are: 1) indicative, informative and evaluative, according to functionality; 2) single-docume"
I08-1017,P05-3013,0,\N,Missing
I11-1016,P04-1021,0,0.0719719,"is translated into different English variations (highlighted in aligned parts). NE translation referred to in this paper denotes bilingual NE transformation (either transliteration or meaning translation), and meaning translation is proposed as distinct from transliteration. Table 1．C2E Translation variations of a character “金” in different instances Furthermore, we randomly extract 100 Chinese characters from the person names of LDC2005T34 corpus, and find out all the characters have more than one translation variations. And each character has about average 7.8 translation variations. Also, (Li et al., 2004) have indicated that there is much confusion in C2E transliteration and Chinese NEs have much lower perplexity than English NEs. According to the above two problems, we find that a crucial problem of C2E NE translation is selecting a correct syllable/word at each step, 138 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 138–146, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP unlike traditional Statistical machine translation 2 Framework (SMT), which mainly focuses on (word, phrase Formally, given a source (Chinese) name or syntax) alignmen"
I11-1016,P07-1016,0,0.139861,"s to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantically labeled training corpus is hard to acquire. Hence this paper does not directly learn NE semantic information, but measures the semantic similarity between the input and global context to capture exact NE translation. 7 Conclusion In this paper, we present a novel semanticspecific model which could adaptively learn semantic information via instance-based similarity m"
I11-1016,J96-1002,0,0.0107575,"oss global context given the input and the source segments sc1K . Finally, P ( E ) is the probability to connect the target segments as the final translation E .Therefore, in our semantic-specific model, the traditional NE translation problem is transferred as searching the most probable (higher semantic similarity) mappings from the training data and then constructing the final translation. In the proposed model (Eq (3)), those features are equally weighted. However, they should be weighted differently according to their contributions. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to model the probability P ( E , S |C ) . Suppose that we have a set of M feature functions hm (C , E , S ), m = 1,...M . For each feature function, there exists a model parameter λm , m = 1,...M .The decision rule is used to choose the most probable target NE (Och and Ney, 2002): ^ ^ ( E , S ) = arg max E ,S {∑ M m =1 } λm hm (C , E , S ) (4) Here, the feature functions h1M (C , E , S ) are modeled by the probabilities of P( sc1K |C ) , P( se1K |sc1K , C ) , and P ( E ) respectively. Next, we discuss these three features in deta"
I11-1016,P05-1057,0,0.020172,"least one character of the input NE. And the candidates, of which the feature values are below a threshold, are discarded. 4.1 ME Parameter Training The weighting coefficients for the three features in Eq (3) can be learned from the development set via Maximum Entropy (ME) training. One way to get the associated weighting coefficients for those log-probability-factors adopted in the model is to regard each of them as realvalued features, and then use ME framework to find their corresponding lambda values, which are just the weighting coefficients that we look for. Following (Och et al. 2002; Liu et al. 2005), we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972) to train the model parameters λ1 ,...λM of the log-linear models according to Eq (4). In practice, YASMET 3 package is adopted here to train the model parameters λ1 ,...λM . In our case, M = 3 . 4.2 Search We use a greedy search algorithm to search the translation with highest probability in the space of all possible mappings. A state in this space is a partial mapping. A transition is defined as the addition of a single mapping to the current state. Our start state is the empty translation result, where th"
I11-1016,W03-1501,0,0.0231898,"Missing"
I11-1016,P02-1038,0,0.0279217,"ta and then constructing the final translation. In the proposed model (Eq (3)), those features are equally weighted. However, they should be weighted differently according to their contributions. Considering the advantages of the maximum entropy model (Berger et al., 1996) to integrate different kinds of features, we use this framework to model the probability P ( E , S |C ) . Suppose that we have a set of M feature functions hm (C , E , S ), m = 1,...M . For each feature function, there exists a model parameter λm , m = 1,...M .The decision rule is used to choose the most probable target NE (Och and Ney, 2002): ^ ^ ( E , S ) = arg max E ,S {∑ M m =1 } λm hm (C , E , S ) (4) Here, the feature functions h1M (C , E , S ) are modeled by the probabilities of P( sc1K |C ) , P( se1K |sc1K , C ) , and P ( E ) respectively. Next, we discuss these three features in detail. 3 3.1 Feature Functions Monolingual Similarity Model The First feature P( sc1K |C ) segments the source into several related segments assumed independence. h= P ( sc1K |c1K ) ≈ ∏ k =1 P ( sck |ck ) (5) 1 (C , E , S ) K The probability P( sck |ck ) describes the relationship of sck and the source NE segment ck . Since sck and ck are on the"
I11-1016,I05-1040,0,0.0145249,"k There are two strategies for NE translation. One is to extract NE translation pairs from the Web or from parallel/comparable corpora. This is essentially the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantic"
I11-1016,P10-1048,0,0.0221356,". One is to extract NE translation pairs from the Web or from parallel/comparable corpora. This is essentially the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantically labeled training corpus is hard to acquire."
I11-1016,P09-1016,0,0.012851,"ategies for NE translation. One is to extract NE translation pairs from the Web or from parallel/comparable corpora. This is essentially the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantically labeled training corp"
I11-1016,C90-3044,0,0.163366,"Missing"
I11-1016,P08-1062,0,0.0177137,"the TL-training set, the Chinese name of an NE pair is transformed into a character-based sequence and its aligned English name is split into syllables, of which the split rules are described in (Jiang et al., 2007). Afterwards, GIZA++ 4 tool is invoked to align characters to syllables. On the other hand, for TS-training set, the Chinese part of an NE is also treated as a character-based sequence, while the English part is regarded as a word-based sequence. The alignment between Chinese characters and English words are achieved by GIZA++ toolkit as well. We use the recall of top-N hypotheses (Yang et al, 2008) as the evaluation metrics, and also adopt the Mean Reciprocal Rank (MRR) metric (Kantor and Voorhees, 2000), a measure that is commonly used in information retrieval, assuming there is precisely one correct answer. Each NE translation generates at most top-50 hypotheses for each input when computing MRR. First, we will show the experimental results when setting different parameters for the semantic similarity model, which is done on the development set with equal feature weightings. We set 4 http://www.fjoch.com/GIZA++.html different ranges of the context window (the parameter n ) to find whi"
I11-1016,P09-1098,0,0.0279455,"person names is hard to acquire. By using semantic-specific model, we could directly cluster instances of similar origin, and generate final translation result for origin consistency. The experiments prove that the SS-model is effective on capturing NE origin information to assist NE translation, and it could further accommodate more different semantic information. 6 Related Work There are two strategies for NE translation. One is to extract NE translation pairs from the Web or from parallel/comparable corpora. This is essentially the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for orga"
I11-1016,P09-1044,0,0.017689,"ally the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantically labeled training corpus is hard to acquire. Hence this paper does not directly learn NE semantic information, but measures the semantic similarity"
I11-1016,I05-1053,0,0.0196895,"allel/comparable corpora. This is essentially the same as constructing NE-pair dictionary (lee et al., 2006; Jiang et al., 2009), which is usually not a real-time translation model and is limited by the coverage of the used corpus and the Web resource. The other is to directly translate an NE phonetically or according to its meaning. For transliteration, several transliteration approaches have been applied to various language pairs (Knight and Graehl, 1998; Tsuji 2002; Li et al. 2004; Oh and Choi, 2005; Pervouchine et al., 2009; Durrani et al., 2010). In contrast, for NE meaning translation, (Zhang et al., 2005; Chen and Zong, 2008; Yang et al., 2009) have proposed different statistical translation models only for organization names. So far, semantic transliteration has been proposed for learning language origin and gender information of person names (Li et al., 2007). However, semantic information is various for NE translation. It is complicated to define different semantic types, and is tedious to train a large number of models used for different semantic information. Moreover, a semantically labeled training corpus is hard to acquire. Hence this paper does not directly learn NE semantic informati"
I11-1016,H05-1055,0,\N,Missing
I11-1016,J98-4003,0,\N,Missing
I11-1069,P07-1056,0,0.707058,"this paper is organized as follows. Section 2 reviews related work. In Section 3, we introduce our motivation with detailed investigation. In Section 4, we propose our ensemble model for cross-domain sentiment classification. Experimental results are reported and discussed in Section 5 and 6 respectively. Section 7 draws conclusions and outlines directions for future work. 2 2.1 Related Work Domain Adaptation Existing approaches for cross-domain sentiment classification mostly belong to the feature-based transfer. Among them, the structural correspondence learning (SCL) algorithm proposed by (Blitzer et al., 2007) is the representative one. SCL tries to get the mapping matrix from non-pivot feature space to pivot feature space. Non-pivot features are then transferred though a projection over the principle components of the mapping matrix. (Li et al., 2009b) proposed to transfer lexical prior knowledge across domains via matrix factorization techniques. (Pan et al., 2010) proposed crossdomain sentiment classification via spectral feature alignment and compared their method with SCL. Another work (Aue and Gamon, 2005) combined small amounts of labeled data with large amounts of labeled data in target dom"
I11-1069,C00-1044,0,0.0253462,"e sets. Three types of ensemble models were finally conducted for three ensemble strategies, with the emphasis on the evaluation of the effectiveness of ensemble techniques for sentiment classification. Different from above methods, our focus in this paper is cross-domain sentiment classification. Compared to our former reports, we prove that the ensemble model is more effective for cross-domain tasks than for the in-domain ones. 3 3.1 Problem Investigation POS Tag Groups The POS information is supposed to be a significant indicator of sentiment expression. The work on subjectivity detection (Hatzivassiloglou and Wiebe, 2000) revealed a high correlation between the presence of adjectives and sentence subjectivity, yet this may not be taken to mean that other POS tags do not contribute. Indeed, it was resulted in (Pang et al., 2002; Benamara et al., 2007) that using only adjectives as features actually results in much worse performance than using the same number of most frequent unigrams. Other re615 searchers (Riloff et al., 2003) pointed out that certain verbs and nouns are also strong indicators of sentiment. According to their significance to sentiment classification, we categorize the POS tags into four groups"
I11-1069,C10-2153,1,0.0594512,"on is based on the observation that some types of POS tags are domain-free, while some others are domain-dependent. Therefore, an efficient ensemble of them would leverage distinct strengths and improve the classification performance. Experimental results show that when the labeled data in the target is few, the proposed ensemble model is quite effective to make use of the labeled data from the source domain to improve the classification performance in the target domain. We also update our previous conclusion drawn regarding the effectiveness of ensemble for indomain sentiment classification (Xia and Zong, 2010; Xia et al., 2011). We conclude that the POSbased ensemble model is more effective for crossdomain sentiment classification than in-domain tasks. In the future, we plan to extend the ensemble model to the tasks of cross-domain sentiment classification with multiple source domains. We also wish to make use of a large amount of unlabeled data in the target domain to help assist the ensemble performance for cross-domain sentiment classification in the framework of ensemble learning. Acknowledgment From Fig. 2, we can see that when the size of labeled data is small, the in-domain performance is f"
I11-1069,P09-1078,1,0.195532,"Missing"
I11-1069,P09-1028,0,0.00611806,"eported and discussed in Section 5 and 6 respectively. Section 7 draws conclusions and outlines directions for future work. 2 2.1 Related Work Domain Adaptation Existing approaches for cross-domain sentiment classification mostly belong to the feature-based transfer. Among them, the structural correspondence learning (SCL) algorithm proposed by (Blitzer et al., 2007) is the representative one. SCL tries to get the mapping matrix from non-pivot feature space to pivot feature space. Non-pivot features are then transferred though a projection over the principle components of the mapping matrix. (Li et al., 2009b) proposed to transfer lexical prior knowledge across domains via matrix factorization techniques. (Pan et al., 2010) proposed crossdomain sentiment classification via spectral feature alignment and compared their method with SCL. Another work (Aue and Gamon, 2005) combined small amounts of labeled data with large amounts of labeled data in target domain to learn the model parameters for a generative naïve Bayes classifier using the Expectation Maximization (EM) algorithm. The above work all need a large amount of unlabeled data in the target domain to help building the transfer procedure. Ou"
I11-1069,W02-1011,0,0.0169742,"ethods, our focus in this paper is cross-domain sentiment classification. Compared to our former reports, we prove that the ensemble model is more effective for cross-domain tasks than for the in-domain ones. 3 3.1 Problem Investigation POS Tag Groups The POS information is supposed to be a significant indicator of sentiment expression. The work on subjectivity detection (Hatzivassiloglou and Wiebe, 2000) revealed a high correlation between the presence of adjectives and sentence subjectivity, yet this may not be taken to mean that other POS tags do not contribute. Indeed, it was resulted in (Pang et al., 2002; Benamara et al., 2007) that using only adjectives as features actually results in much worse performance than using the same number of most frequent unigrams. Other re615 searchers (Riloff et al., 2003) pointed out that certain verbs and nouns are also strong indicators of sentiment. According to their significance to sentiment classification, we categorize the POS tags into four groups, as shown in Table 1. Group J V N O Contained POS tags adjectives, adverbs verbs nouns the other POS tags Table 1. Four groups of POS tags 3.2 Cross-domain K-L Distances When conducting transfer learning, it"
I11-1069,W03-0404,0,0.0139978,"n ones. 3 3.1 Problem Investigation POS Tag Groups The POS information is supposed to be a significant indicator of sentiment expression. The work on subjectivity detection (Hatzivassiloglou and Wiebe, 2000) revealed a high correlation between the presence of adjectives and sentence subjectivity, yet this may not be taken to mean that other POS tags do not contribute. Indeed, it was resulted in (Pang et al., 2002; Benamara et al., 2007) that using only adjectives as features actually results in much worse performance than using the same number of most frequent unigrams. Other re615 searchers (Riloff et al., 2003) pointed out that certain verbs and nouns are also strong indicators of sentiment. According to their significance to sentiment classification, we categorize the POS tags into four groups, as shown in Table 1. Group J V N O Contained POS tags adjectives, adverbs verbs nouns the other POS tags Table 1. Four groups of POS tags 3.2 Cross-domain K-L Distances When conducting transfer learning, it is crucial to find that from one domain to another, which part of knowledge changes and which part of knowledge remains similar. Then the “unchanged” part of knowledge should be kept during the learning p"
I11-1140,J04-4004,0,0.012281,"ture parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode grammatical relations between words and provide much more lexical conditio"
I11-1140,D08-1092,0,0.0516678,"Missing"
I11-1140,A00-2018,0,0.290074,"onstituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode grammatical relations between words and provide much more le"
I11-1140,P05-1022,0,0.282414,"neficial for constituent tree evaluation. The remainder of this paper is organized as follows: Section 2 briefly reviews related work and proposes our ideas. Section 3 describes our parsing approach. Section 4 describes our parse reranking algorithms based on higher-order lexical dependencies. In Section 5, we describe our training algorithms. We discuss and analyze our experiments in Section 6. Finally, we conclude and mention future work in Section 7. 2 Related Work and Our Ideas Over the past few years, two kinds of parse reranking methods have been proposed. The first is N-best reranking (Charniak and Johnson, 2005; Collins and Koo, 2005). In this method, an existing generative parser is used to enumerate N-best parse trees for an input sentence, and then a reranking model is used to rescore the N-best lists with the help of various sorts of features. However, the N-best reranking method suffers from the limited scope of the N-best list in that potentially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate"
I11-1140,W02-1001,0,0.0107667,"Missing"
I11-1140,P96-1025,0,0.474364,"red into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies withi"
I11-1140,J05-1003,0,0.274005,"for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies within a parse reranking framework. We evaluate our met"
I11-1140,P04-1015,0,0.0835963,"Missing"
I11-1140,N09-2064,0,0.0134145,"(a) gold-standard NN 2: α (0) ← 0 ; v ← 0; i ← 0 3: for n in 1…N do 4: for t in 1…T do NN 高 科技 项目 (b) generated by LPCFG 5: NP0,3 NP1,3 ADJP0,1 JJ 高 (high) NP1,2 NP2,3 NN 科技 (technology) (c) packed forest NN 项目 (project) Figure 4. Constituent trees and forest A forest is a compact representation of many parse trees. Figure 4(c) is a sample forest which is the compact representation of the constituent trees shown in Figures 4(a) and 4(b). To obtain forests, Huang (2008) tried to modify the Charniak parser to output forest directly. Inspired by parser combination methods (Sagae and Lavie, 2006; Fossum and Knight, 2009), we have designed a simple method of building forests starting from N-best lists. First, we convert each parse tree in an N-best list into context-free productions and label each constituent in each production with its span and syntactic category. Then these converted context-free productions are used to build the forest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Figure 4(c). The recombined"
I11-1140,W08-1007,0,0.0602922,"atives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 200"
I11-1140,W07-2444,0,0.110232,"ntially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996;"
I11-1140,P08-1067,0,0.746438,"ver the past few years, two kinds of parse reranking methods have been proposed. The first is N-best reranking (Charniak and Johnson, 2005; Collins and Koo, 2005). In this method, an existing generative parser is used to enumerate N-best parse trees for an input sentence, and then a reranking model is used to rescore the N-best lists with the help of various sorts of features. However, the N-best reranking method suffers from the limited scope of the N-best list in that potentially good alternatives may have been ruled out. The second method, called the forest reranking model, was proposed by Huang (2008). In Huang’s method, a forest, instead of an N-best list, is generated first. Then a beam search algorithm is used to generate N-best sub-trees for each node in bottom-up order and the best-first sub-tree of the root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with"
I11-1140,W05-1506,0,0.0292677,"orest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Figure 4(c). The recombined forest probably contains some parse trees that are not included in the N-best list, as will be shown in sub-section 6.1. Our algorithm for forest reranking is similar to Algorithm 1. The only difference is that there may be more than one hyperedge for each node in a forest. So we make use of a beam search algorithm (Huang and Chiang, 2005) and store Nbest sub-trees for each internal node. Finally, we choose the best-first sub-tree of the root node as the result. 5 α (i +1) ← update α (i ) according to ( xt , ct ) 6: v ← v + α (i +1) 7: i←i + 1 8: α ← v/(N*T) 9: return α NP0,2 Training Algorithm The training task is to tune the parameter weights α in Eq. (1) using the training examples as evidence. We employ the online-learning algorithm shown in Algorithm 2 because it has been proven  initial weights  N iterations  T training instances  averaging weights to be effective and efficient in many studies (Collins, 2002; Collins"
I11-1140,D09-1087,0,0.0656089,"Missing"
I11-1140,P03-1054,0,0.242069,"f lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence. In this paper, we propose a method for evaluating constituent trees using higher-order lexical dependencies within a parse reranking frame"
I11-1140,H01-1014,0,0.0393784,"e root node is chosen as the final parse tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its cor"
I11-1140,W03-3023,0,0.118149,"rocedure EvalSubTree ( CP ) (b) Lexicalized constituent tree (a) Constituent tree fragment B:A:E B:A:D B:A:C w0 w1 F:D:H F:D:G w2 w3 w4 w5 (c) Labeled dependency tree  Assume the constituent is P → N1  N n 8: Find the head-child N h for P 9: WP ← WN h Figure 2. Representation of constituent tree with labeled dependency tree with associated dependency trees. Our method includes the following two steps: Step 1: Lexicalize the constituent tree, i.e. annotate each node in the constituent tree with its head-word. First, find the head-child of each nonterminal node using a head percolation table (Yamada and Matsumoto, 2003). For example, in Figure 2(a), node B is identified as the head-child of rule A → B C D E. Then the head-words propagate up through the leaf nodes and each parent receives its head-word from its head-child. For example, in Figure 2(b), w 0 is propagated up from node B to A. According to this procedure, we can get the lexicalized constituent tree (shown in Figure 2(b)) for the constituent fragment shown in Figure 2(a). Step 2: Transform the lexicalized tree into a labeled dependency tree. First, let the head-word of each non-head-child depend on the head-word of the head-child for each rule. Fo"
I11-1140,D09-1161,0,0.0854744,"Missing"
I11-1140,zhang-etal-2004-interpreting,0,0.086996,"Missing"
I11-1140,P10-1001,0,0.0335465,"ons between words and provide much more lexical conditioning than lexical heads for PCFG. Dependency trees are usually factored into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g are words in a sentence"
I11-1140,E06-1011,0,0.0724329,"ey encode grammatical relations between words and provide much more lexical conditioning than lexical heads for PCFG. Dependency trees are usually factored into sets of lexical dependency parts for evaluation. The order of a lexical dependency part can be defined according to the number of dependency arcs it contains. For example, in Figure 1, dependency is first-order, sibling and grandchild are secondorder and grand-sibling and tri-sibling are thirdorder. During the past few years, higher-order 1 lexical dependencies have been successfully used for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010). But for constituent tree evaluation, only first-order (bigram) lexical dependencies have been used (Collins, 1996; Klein and Manning, 2003a; Collins and Koo, 2005). However, firstorder lexical dependency parts are quite limited and thus lose much of the contextual information within the dependency tree. To improve parsing performance, we propose to evaluate constituent trees with higher-order lexical dependencies. h h m dependency g h s g m sibling s grand-sibling m m h grandchild h t s m tri-sibling Figure 1. Lexical dependency types. The lowercase letters h, m, s, g"
I11-1140,P09-1006,0,0.0371333,"Missing"
I11-1140,P06-1055,0,0.167591,"een significantly improved as well. 1 Introduction The most commonly used grammar for constituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidate"
I11-1140,N07-1051,0,0.032417,"roved as well. 1 Introduction The most commonly used grammar for constituent structure parsing is probabilistic context-free grammar (PCFG). However, as demonstrated in Klein and Manning (2003a), PCFG estimated straightforwardly from Treebank does not perform well. The reason is that the basic PCFG has certain recognized drawbacks: its independence assumption is too strong, and it lacks of lexical conditioning (Jurafsky and Martin, 2008). To address these drawbacks, several variants of PCFG-based models have been proposed (Klein and Manning, 2003a; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Lexicalized PCFG (LPCFG) (Collins, 1999; Charniak, 2000; Bikel, 2004) is a representative work that tries to ameliorate the deficiency of lexical conditioning. In LPCFG, non-terminals are annotated with lexical heads and the probabilities of CFG rules are estimated conditioned upon these lexical heads. Thus LPCFG becomes sensitive to lexical heads, and its performance is improved. However, the information provided by lexical heads is limited. To obtain higher parsing performance, we must seek additional information. We believe that dependency trees are good candidates because they encode gra"
I11-1140,N06-2033,0,0.024041,"T NP NP NN JJ 高 科技 项目 (a) gold-standard NN 2: α (0) ← 0 ; v ← 0; i ← 0 3: for n in 1…N do 4: for t in 1…T do NN 高 科技 项目 (b) generated by LPCFG 5: NP0,3 NP1,3 ADJP0,1 JJ 高 (high) NP1,2 NP2,3 NN 科技 (technology) (c) packed forest NN 项目 (project) Figure 4. Constituent trees and forest A forest is a compact representation of many parse trees. Figure 4(c) is a sample forest which is the compact representation of the constituent trees shown in Figures 4(a) and 4(b). To obtain forests, Huang (2008) tried to modify the Charniak parser to output forest directly. Inspired by parser combination methods (Sagae and Lavie, 2006; Fossum and Knight, 2009), we have designed a simple method of building forests starting from N-best lists. First, we convert each parse tree in an N-best list into context-free productions and label each constituent in each production with its span and syntactic category. Then these converted context-free productions are used to build the forest. For example, in Figure 4, given two candidates (Figure 4(a) and Figure 4(b)), we first convert them into context-free productions, e.g. NP0,3ADJP0,1 NP1,3, NP0,3  NP0,2 NP2,3 and so on. Then we combine these productions into the forest shown in Fi"
I11-1140,wang-zhang-2010-hybrid,0,0.0182648,"tree. In recent years, there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its corresponding dependency tree may contain c"
I11-1140,C10-2148,1,0.855811,", there have been many attempts to use dependency trees for constituent parsing. All these approaches can be classified into three types. The first type is dependency-driven constituent parsing (Hall et al., 2007; Hall and Nivre, 2008). Given an input sentence, this approach first parses it into a labeled dependency tree (with complex arc labels, which makes it possible to recover the constituent tree) and then transforms the dependency tree into a constituent tree. The second approach is dependency-constrained constituent parsing (Xia and Palmer, 2001; Xia et al., 2008; Wang and Zhang, 2010; Wang and Zong, 2010). In this approach, dependency trees, once generated, are used to constrain the search space of a constituent parser. The third approach is dependency-based constituent parsing (Collins, 1996; Klein and Manning, 2003b). In this approach, the constituent tree is evaluated with the help of its corresponding lexical dependencies. All three existing approaches have certain limitations. In the first approach, the dependencydriven constituent parser is not constrained by the Treebank grammar, so a constituent tree transformed from its corresponding dependency tree may contain context-free production"
I11-1140,J03-4003,0,\N,Missing
I17-1039,D16-1026,0,0.387584,"model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Function (n) (n) a(θ)i,j × b ai,j ("
I17-1039,P07-2045,0,0.00651849,"radient decent algorithm. We set learning rate to 0.1 at the beginning and halve the threshold while the perplexity increases on the development set. Dropout is applied to our model, and the rate is set to 0.2. For Table 1: The statistics of monolingual dataset on the LDC corpus. 4.2 Training Details Data Preparing and Preprocessing Considering the fact that the amount of manually annotated phrase pairs is not enough, in order to imitate the environment of experiment, we extract phrase pairs from parallel corpora automatically to make up for the shortage of quantity. To do this, we use Moses (Koehn et al., 2007) in its training step to learn a phrase table from LDC corpus, which includes 0.63M sentence pairs. In order to simulate the experiment as far as possible, we adopt three strategies to filter low quality 1 388 https://github.com/isi-nlp/ZophRNN # System MT03 MT04 MT05 MT06 Ave 1 2 3 4 5 Phrase NMT Model Partially Aligned Model(MUL) Partially Aligned Model(MSE) Partially Aligned Model(MSE) + LimitedVocab Phrase NMT model + LimitedVocab 3.64 3.80 5.11 6.63 3.78 4.25 4.37 5.04 6.81 4.33 3.55 3.75 4.26 5.59 3.63 3.77 4.24 4.95 5.77 3.94 3.80 4.04 4.84 6.20 3.92 Table 2: Translation results (BLEU s"
I17-1039,P17-1176,0,0.0316832,"easily, and high quality phrase pairs can be obtained using some effective methods (Zhang et al., 2014). To learn a good NMT Pivot-based Scenario Pivot-based scenario assumes that there only exists source-pivot and pivot-target parallel corpora, which can be used to train source-to-pivot and pivot-to-target translation models. Cheng et al. (2017) propose to translate source language into pivot language, and then the pivot language will be translated into target language. According to the fact that parallel sentences should have close probabilities of generating a sentence in a third language, Chen et al. (2017) construct a TeacherStudent framework, in which existing pivot-target NMT model guides the learning process of the source-target model. 6.2 Monolingual Data Scenario Multilingual Scenario In multilingual scenario, there exists multiple language pairs but no source-target sentence pairs. Johnson et al. (2016) use parallel corpora of multiple languages to train a universal NMT model. This universal model learns translation knowledge from multiple different languages, which makes zero-shot translation feasible. Firat et al. (2016) present a multi-way, multilingual model to resolve the zero-resour"
I17-1039,C16-1291,0,0.0203739,"y2 and y3 . Therefore, the decoder in our model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Func"
I17-1039,P16-1185,0,0.0288224,"able translation result is still in need by incorporating additional data resource. G¨ulc¸ehre et al. (2015) propose to incorporate target-side corpora as a language model. Sennrich et al. (2016a) attempt to enhance the decoder network model of NMT by incorporating the target-side monolingual data. Luong et at. (2016) explore the sequence autoencoders and skip-thought vectors method to exploit the monolingual data of source language. Zhang and Zong (2016) propose two approaches, self-training algorithm and multitask learning framework, to incorporate sourceside monolingual data. Besides that, Cheng et al. (2016) have explored the usage of both source and target monolingual data using a semisupervised method to reconstruct both source and target side monolingual language, where two NMT frameworks will be used. Above methods are designed for different scenarios, and their work can achieve great results on these scenarios. However, when in the scenario we propose in this work, that is we only have monolingual sentences and some phrase pairs, their methods are hard to be utilized to train an NMT model. Under this scenario, monolingual data can be acquired easily, and high quality phrase pairs can be obta"
I17-1039,P06-1077,0,0.0400899,"l method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an"
I17-1039,C12-1186,1,0.831605,"an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an NMT model. Figure 1"
I17-1039,P14-1011,1,0.83923,"urce and target monolingual data using a semisupervised method to reconstruct both source and target side monolingual language, where two NMT frameworks will be used. Above methods are designed for different scenarios, and their work can achieve great results on these scenarios. However, when in the scenario we propose in this work, that is we only have monolingual sentences and some phrase pairs, their methods are hard to be utilized to train an NMT model. Under this scenario, monolingual data can be acquired easily, and high quality phrase pairs can be obtained using some effective methods (Zhang et al., 2014). To learn a good NMT Pivot-based Scenario Pivot-based scenario assumes that there only exists source-pivot and pivot-target parallel corpora, which can be used to train source-to-pivot and pivot-to-target translation models. Cheng et al. (2017) propose to translate source language into pivot language, and then the pivot language will be translated into target language. According to the fact that parallel sentences should have close probabilities of generating a sentence in a third language, Chen et al. (2017) construct a TeacherStudent framework, in which existing pivot-target NMT model guide"
I17-1039,D15-1166,0,0.649257,"these sentences as partially aligned sentences) with the Introduction Neural machine translation (NMT) proposed by Kalchbrenner et al.(2013), Sutskever et al.(2014) and Cho et al.(2014) has achieved significant progress in recent years. Different from traditional statistical machine translation(SMT) (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Zhai et al., 2012) which contains multiple separately tuned components, NMT builds an end-to-end framework to model the whole translation process. For several language pairs, NMT is reaching significantly better translation performance than SMT (Luong et al., 2015b; Wu et al., 2016). In general, in order to obtain an NMT model 384 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 384–393, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP monolingual data and phrase pairs. Then we can utilize these partially aligned sentences to train an NMT model. Figure 1 shows an example of our data. Source sentence and target sentence are not fully aligned but contain two translation fragments: (”外交部发言人”, ”foreign ministry deputy”) and (”在 例 行 的 记 者 招 待 会 上 说”, ”speaking at a regular press”). Intuitively, these"
I17-1039,D16-1160,1,0.931466,"sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned sentences) with the Intro"
I17-1039,P15-1002,0,0.054861,"Missing"
I17-1039,P17-2060,1,0.819411,"ts and +13.21 BLEU points respectively. When using more than 60K sentence pairs, we still get a relatively high promotion of translation quality. However, the promotion is not very remarkable as Row1-3 reveal in Table 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para"
I17-1039,C16-1205,0,0.0150104,"le 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para 1.60 12.36 1.22 15.07 1.05 11.64 1.70 14.61 1.39 13.42 40K NMT Model Partially Aligned Model(MSE) + Para 1.87 14.12 2.00 17.84 1.47 13.66 2.24 17.26 1.90 15.72 60K NMT Model Partially Aligned Model(MSE) + Para 3.72"
I17-1039,D16-1249,0,0.228359,"e decoder outputs y2 and y3 . Therefore, the decoder in our model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i="
I17-1039,P16-2021,0,0.271354,"model can be described as follows: (P (k) Tx j aij hi if yi ∈ Py (8) ci = (k) 0 if yi ∈ / Py λ > 0 is a hyper-parameter that balances the preference between likelihood and agreement. In this paper, it is set to 0.3. As shown in Eq. (8), our objective function does not only consider to maximize the loglikelihood of the target sentence, but also encourages the alignment aij produced by NMT to have a larger agreement with the prior alignment information. This objective function is similar to that used by the supervised attention method (Mi et al., 2016a; Liu et al., 2016). Inspired by Liu et al. (2016), the agreement between a(n) and b a(n) can be defined in different ways: (k) where aij is calculated as Eq. (2), Py is the target partial part, as shown in Eq. (5). In Eq. (8), our model generates the aligned target words based on the context vector ci and previously predicted words y&lt;i . When generating the unaligned target words, the model sets the context vector ci to zero, indicating that the model generates these words only based on the LSTM-based RNN language model. 3.2.3 i • Multiplication (MUL) (n) 4(a(n) , b ai,j ; θ) =− Ty X Tx X i=0 j=0 Objective Function (n) (n) a(θ)i,j × b ai,j ("
I17-1039,P02-1040,0,0.100406,"ade up of all the target words of the special phrase pairs whose corresponding source words belong to the source sentence. 4 Experiment In this section, we perform the experiment on Chinese-English translation tasks to test our method. 4.1 Dataset We evaluate our approach on large-scale monolingual data set from LDC corpus, which includes 13M Chinese sentences and 10M English sentences. Table 1 shows the detailed statistics of our training data. To test our model, we use NIST 2003(MT03) as development set, and NIST 20042006(MT04-06) as test set. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl. Corpus monolingual #Sent. #Word Vocab Chinese 13.33M 327.10M 1.83M English 10.03M 276.07M 1.07M 4.3 We build our described method based on the Zoph RNN toolkit1 written in C++/CUDA. Both encoder and decoder consist of two stacked LSTM layers. We set minibatch size to 128. The word embedding dimension of both source and target sides is 1000, and the dimensions of hidden layers unit is set to 1000. In our baseline model, we limit the vocabulary of both source and target languages to 30K most frequent words, and other words are replaced by a special symbol “"
I17-1039,P16-1009,0,0.475568,"rce sentence and target sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned s"
I17-1039,P16-1162,0,0.715196,"rce sentence and target sentence are not parallel but they include two parallel parts (highlight in blue and red respectively). of great translation quality, we usually need largescale parallel data. Unfortunately, the large-scale parallel data is always insufficient in many domains and language pairs. Without sufficient parallel sentence pairs, NMT tends to learn poor estimates on low-count events. Actually, there have been some effective methods to deal with the situation of translating language pairs with limited resource under different scenarios (Johnson et al., 2016; Cheng et al., 2017; Sennrich et al., 2016a; Zhang and Zong, 2016). In this paper, we address a new translation scenario in which we do not have any parallel sentences but have massive monolingual corpora and phrase pairs. The previous methods are hard to be used to learn an NMT model under this situation. In this paper, we propose a novel method to learn an NMT model using only monolingual data and phrase pairs. Our main idea is that although there does not exist the parallel sentences, we can derive the sentence pairs which are non-parallel but contain the parallel parts (in this paper, we call these sentences as partially aligned s"
I17-1039,P16-5005,0,0.027134,"ery remarkable as Row1-3 reveal in Table 4. We can see when the number of parallel corpora is 100K(Row 5), the improvement over NMT Model is +3.95 BLEU points, which indicates that as the size of parallel corpora increases, the improvement of fine-tuning model is decreasing. 8.21 8 #Sent. #Word Vocab 6 Related Work Most of existing work in neural machine translation focus on integrating SMT strategies (He et al., 2016; Zhou et al., 2017; Wang et al., 2017; Shen et al., 2015), handling rare words (Li et al., 2016; Sennrich et al., 2016b; Luong et al., 2015b) and designing the better framework (Tu et al., 2016; Luong et al., 2015a; Meng et al., 2016). As for translation scenarios, training NMT model under Effect of Adding Small Parallel Corpus We concern that when we have a tiny parallel corpus, whether the small scale parallel corpus can boost the translation performance of the partially aligned method. Here, we fine-tune the partially 390 #Sent. Method MT03 MT04 MT05 MT06 Ave 20K NMT Model Partially Aligned Model(MSE) + Para 1.60 12.36 1.22 15.07 1.05 11.64 1.70 14.61 1.39 13.42 40K NMT Model Partially Aligned Model(MSE) + Para 1.87 14.12 2.00 17.84 1.47 13.66 2.24 17.26 1.90 15.72 60K NMT Model"
I17-1039,1983.tc-1.13,0,0.765242,"Missing"
I17-1039,D13-1176,0,\N,Missing
I17-1039,P05-1033,0,\N,Missing
I17-1039,N03-1017,0,\N,Missing
J13-2001,P02-1051,0,0.0195841,"features), from which a NE translation dictionary was then constructed. Kumano et al. (2004) proposed a method to extract English–Chinese NE pairs from a content-aligned corpus. This approach tries to ﬁnd the correspondences between bilingual NE groups based on the similarity in their order of appearance in each document. Additionally, an abridged version of our work has been presented in our ACL-10 paper (Chen, Zong, and Su 2010). Among those symmetric approaches, only Huang, Vogel, and Waibel and Chen, Zong, and Su adopt the expansion strategy, described below. For the asymmetric strategy, Al-Onaizan and Knight (2002) proposed an algorithm to translate NEs from Arabic to English using monolingual and bilingual resources. Given an Arabic NE, they used transliteration models (including a phonetic-based and a spelling-based model), a bilingual dictionary, and an English news corpus to ﬁrst generate a list of English candidates, which were then re-scored by a Web resource. 261 Computational Linguistics Volume 39, Number 2 Moore (2003) developed an approach to learning phrase translations from a parallel corpus based on a sequence of cost models. A maximum entropy model for NE alignment was presented in Feng, L"
J13-2001,J96-1002,0,0.0681675,"Missing"
J13-2001,A97-1029,0,0.027549,"del, combined with semi-supervised learning, offers signiﬁcant improvement for semi-automatically updating the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prio"
J13-2001,W02-2002,0,0.0287446,"ditionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process to extract a smaller but cleaner NE translation dictionary and t"
J13-2001,J93-2003,0,0.037461,"Missing"
J13-2001,W02-2004,0,0.0461628,"Missing"
J13-2001,M98-1017,0,0.129819,"the alignment-baseline model, because information from the aligned sentence is utilized more effectively. Results demonstrate that the proposed joint model, combined with semi-supervised learning, offers signiﬁcant improvement for semi-automatically updating the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global i"
J13-2001,P98-1036,0,0.283962,"the alignment-baseline model, because information from the aligned sentence is utilized more effectively. Results demonstrate that the proposed joint model, combined with semi-supervised learning, offers signiﬁcant improvement for semi-automatically updating the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global i"
J13-2001,W03-1501,0,0.0609135,"Missing"
J13-2001,P10-1065,1,0.755665,"Missing"
J13-2001,M98-1001,0,0.0275264,"proposed joint model gains more from the learning process in comparison with the alignment-baseline model, because information from the aligned sentence is utilized more effectively. Results demonstrate that the proposed joint model, combined with semi-supervised learning, offers signiﬁcant improvement for semi-automatically updating the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore,"
J13-2001,W04-3248,0,0.0155113,"Missing"
J13-2001,P05-1045,0,0.0262696,"Missing"
J13-2001,J05-4005,0,0.021018,"Missing"
J13-2001,W03-1502,0,0.0097625,"ce and the target, Huang, Vogel, and Waibel (2003) proposed to ﬁrst identify the NEs in both the source and target, and then enlarge the obtained NE candidate sets for both languages before conducting alignment. Based on the observation that NE boundaries are frequently identiﬁed incorrectly, the enlarging procedure is done by treating the original recognition results as anchors and then increasing the number of candidates by expanding or shrinking the boundaries of those originally recognized NEs in both languages. Our approach also adopts the expansion strategy. It differs from the works of Huang et al. (2003) and others in several ways, however. First, in all the alignment papers mentioned here, the adopted probabilities are directly used as features for log-linear combination or ME training without derivation. In contrast, our work fully derives a probabilistic joint model, for both identiﬁcation and alignment, in a principled way. Second, unlike previous approaches that discard the information of initially identiﬁed NE anchors after the anchors have been expanded, our approach uses this information in the ﬁnal selection process. Third, we propose new features, such as translation mode and its ra"
J13-2001,P06-2055,0,0.0224415,"Missing"
J13-2001,P06-1027,0,0.0129572,"the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004) used an iterative p"
J13-2001,P06-1141,0,0.0115211,"hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process to extract a smaller but cleaner NE translation dictionary and then used the dictionary to improve the monolingual NE annotation quality. Ji and Grishman (2007) adopted several heuristic rules for using bilingual-text information to correct NE recognition errors. In aligning bilingual NEs from two given NE lists, the NE translation model is usually adopted. Typically, an NE is either transliterated or semantically translated. For transliteration, Knight a"
J13-2001,W03-0317,0,0.0328476,"veral heuristic rules for using bilingual-text information to correct NE recognition errors. In aligning bilingual NEs from two given NE lists, the NE translation model is usually adopted. Typically, an NE is either transliterated or semantically translated. For transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE semantic translation is less common. Zhan"
J13-2001,Y03-1035,0,0.0485385,"Missing"
J13-2001,P07-1016,0,0.016498,"c model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE semantic translation is less common. Zhang et al. (2005) proposed a phrase-based context-dependent joint probability model for semantic translation, which is similar to phrase-level translation models in statistical MT (Zong and Seligman 2005; Hu, Zong, and Xu 2006). Chen, Yang, and Lin (2003) and Chen et al. (2006) studied formulation and transformation rules for English–Chinese NEs"
J13-2001,P04-1021,0,0.0144383,"Missing"
J13-2001,W09-2208,0,0.0128217,"s languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process to extract a smaller but cleaner NE translation dictionary and then used the dictionary to improve the monolingual NE annotation quality. Ji and Grishman (2007) adopted several heuristic rules for using bilingual-text information to correc"
J13-2001,W02-2017,0,0.0341124,"n (2007) adopted several heuristic rules for using bilingual-text information to correct NE recognition errors. In aligning bilingual NEs from two given NE lists, the NE translation model is usually adopted. Typically, an NE is either transliterated or semantically translated. For transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE semantic translation"
J13-2001,W03-0430,0,0.0133557,"automatically updating the NE recognition model and the NE translation table. Additionally, the impact is greater when less time is available for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004)"
J13-2001,M98-1021,0,0.0350359,"Missing"
J13-2001,E03-1035,0,0.0483518,"t performance. One way to alleviate this error propagation problem is to jointly perform NE recognition and alignment. Such a combined approach is usually infeasible, however, due to the high computational cost of evaluating alignment scores for a large number2 of NE pair candidates. In order to make the problem computationally tractable, a sequential approach is usually used to ﬁrst identify NEs and then align them. Two such kinds of sequential strategies that alleviate the error propagation problem have been proposed. The ﬁrst strategy, named asymmetry alignment (Al-Onaizan and Knight 2002; Moore 2003; Feng, Lv, and Zhou 2004; Lee, Chang, and Jang 2006), identiﬁes NEs only on the source side and then ﬁnds their corresponding NEs on the target side. Although this approach avoids the NE recognition errors resulting from the target side, which would otherwise be brought into the alignment process, the NE recognition errors from the source side continue to affect alignment. To further reduce the errors from the source side, the second strategy, denoted symmetry alignment (Huang, Vogel, and Waibel 2003), expands the NE candidate sets in both languages before conducting the alignment. This is ac"
J13-2001,P03-1021,0,0.0341531,"Missing"
J13-2001,J03-1002,0,0.0235233,"Missing"
J13-2001,C02-1099,0,0.011184,"ight and Graehl (1998) were pioneers in adopting the probabilistic model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE semantic translation is less common. Zhang et al. (2005) proposed a phrase-based context-dependent joint probability model for semantic translation, which is similar to phrase-level translation models in statistical MT (Zong and Seligman 2005; Hu, Zong, and Xu 2006). Chen, Yang, and Lin (2003) and Chen et al. (2006) stu"
J13-2001,I05-1040,0,0.054327,"Missing"
J13-2001,P09-1016,0,0.0494324,"Missing"
J13-2001,C04-1089,0,0.0435839,"spelling-based model), a bilingual dictionary, and an English news corpus to ﬁrst generate a list of English candidates, which were then re-scored by a Web resource. 261 Computational Linguistics Volume 39, Number 2 Moore (2003) developed an approach to learning phrase translations from a parallel corpus based on a sequence of cost models. A maximum entropy model for NE alignment was presented in Feng, Lv, and Zhou (2004). Lee, Chang, and Jang (2006) proposed to align bilingual NEs in a bilingual corpus by incorporating a statistical model with multiple sources. Turning to comparable corpora, Shao and Ng (2004) presented a hybrid method to mine new translations from Chinese–English comparable corpora, combining both transliteration and context information. Sproat, Tao, and Zhai (2006) investigated the Chinese–English NE transliteration equivalence within comparable corpora. Although these asymmetry strategies can prevent NE recognition errors on the target side from affecting alignment, errors on the source side continue to propagate to later stages. To reduce error propagation from both the source and the target, Huang, Vogel, and Waibel (2003) proposed to ﬁrst identify the NEs in both the source a"
J13-2001,P06-1010,0,0.0141249,"Missing"
J13-2001,W98-1005,0,0.0525921,"used the dictionary to improve the monolingual NE annotation quality. Ji and Grishman (2007) adopted several heuristic rules for using bilingual-text information to correct NE recognition errors. In aligning bilingual NEs from two given NE lists, the NE translation model is usually adopted. Typically, an NE is either transliterated or semantically translated. For transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account f"
J13-2001,O03-5001,0,0.0770014,"Missing"
J13-2001,P98-2220,0,0.0385865,"quality. Ji and Grishman (2007) adopted several heuristic rules for using bilingual-text information to correct NE recognition errors. In aligning bilingual NEs from two given NE lists, the NE translation model is usually adopted. Typically, an NE is either transliterated or semantically translated. For transliteration, Knight and Graehl (1998) were pioneers in adopting the probabilistic model to align the components within an NE pair. Since then, similar approaches have been applied to various language pairs such as English/Arabic (Stalls and Knight 1998), English/Chinese (Chen et al. 1998b; Wan and Verspoor 1998; Lin and Chen 2002; Lee and Chang 2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE se"
J13-2001,W02-2035,0,0.0470152,"lable for labeling seed data. 7. Related Work There is signiﬁcant work on identifying NEs within monolingual texts across languages, such as English (Chinchor 1998; Mikheev, Grover, and Moens 1998; Borthwick 1999) and Chinese (Chen et al. 1998a; Sun, Zhou, and Gao 2003), to name a few. Various approaches to identifying NEs have also been proposed, such as hidden Markov models (Bikel et al. 1997; Bikel, Schwartz, and Weischedel 1999), conditional random ﬁelds (McCallum and Li 2003; Jiao et al. 2006), modiﬁed transformation-based learning (Black and Vasilakopoulos 2002), boosting (Collins 2002; Wu et al. 2002), AdaBoost (Carreras, Marquez, and Padro 2002), and adopting semi-supervised learning (Wong and Ng 2007; Liao and Veeramachaneni 2009). Furthermore, features including local information (e.g., token, part-of-speech) and global information (e.g., label consistency, context features) from monolingual resources have been adopted (Krishman and Manning 2006; Zhou and Su 2006). In prior work on the use of bilingual NE alignment for NE recognition, Huang and Vogel (2004) used an iterative process to extract a smaller but cleaner NE translation dictionary and then used the dictionary to improve the mo"
J13-2001,I05-1053,0,0.0214147,"2003; Lee, Chang, and Jang 2003; Gao, Wong, and Lam 2004; 260 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities Pervouchine, Li, and Lin 2009), English/Japanese (Knight and Graehl 1998; Tsuji 2002), and English/Korean (Lee and Choi 1997; Oh and Choi 2002, 2005). Moreover, Li, Zhang, and Su (2004), and Li et al. (2007) presented a joint source channel model for transliteration, and automated the semantic transliteration process, which takes origin and gender into account for personal names. In contrast, research on automatic NE semantic translation is less common. Zhang et al. (2005) proposed a phrase-based context-dependent joint probability model for semantic translation, which is similar to phrase-level translation models in statistical MT (Zong and Seligman 2005; Hu, Zong, and Xu 2006). Chen, Yang, and Lin (2003) and Chen et al. (2006) studied formulation and transformation rules for English–Chinese NEs. They adopted a frequency-based approach for extracting key words of NEs with or without dictionary assistance and constructed transformation rules from the bilingual NE corpus. Their studies focused on transformation rules with particular attention to distinguishing t"
J13-2001,zhang-etal-2004-interpreting,0,0.0234046,"Missing"
J13-2001,I08-4017,0,0.024645,"Missing"
J13-2001,M98-1004,0,\N,Missing
J13-2001,M98-1012,0,\N,Missing
J13-2001,M98-1014,0,\N,Missing
J13-2001,C98-1036,0,\N,Missing
J13-2001,C98-2215,0,\N,Missing
J13-2001,J98-4003,0,\N,Missing
J13-2001,P02-1062,0,\N,Missing
K16-2003,prasad-etal-2008-penn,0,0.402218,"Missing"
K16-2003,K15-2002,0,0.171723,"ng and Xue, 2012), punctuation marks play a significant role in Chinese discourse. Fortunately, CDTB has annotated those punctuations that may indicate discourse relations. Inspired by the above phenomena, we design our system by fully considering these Chinese characteristics. Besides the training data, we simply use skipgram neural word embeddings provided by the CoNLL-2016 organizers to replace words in some features. 3 Argument Extraction 3.1.1 Connective Identification A classifier is trained to recognize connectives. The features are chosen by referring to the best system in CoNLL-2015 (Wang and Lan, 2015). Zhou and Xue (2012) found that a discourse connective is almost always accompanied by punctuations, which help us to design the features. The features we used are as follows: System Architecture Zhou and Xue (2015) pointed out that discourse connectives and punctuation marks in Chinese can serve as anchors, which are clues of discourse relations. This opinion encourages us to treat explicit and non-explicit relations similarly. Therefore, the explicit and non-explicit parsers share the same • Lexical features: candidate itself, number of the candidate words, POS of the candidate, POS of the"
K16-2003,I11-1170,0,0.0354422,"Missing"
K16-2003,K16-2001,0,0.0684866,"Missing"
K16-2003,Q15-1024,0,0.0283668,"Missing"
K16-2003,D14-1008,0,0.0232098,"ions are divided into two types, explicit or non-explicit, depending on whether connectives exist or not. A complete discourse relation contains two discourse units called Argument1 (Arg1) and Argument2 (Arg2). An end-to-end parser usually consists of some components, such as discourse connective identification, argument extraction, explicit sense classification and implicit sense classification. Pitler and Nenkova (2009) used syntactic features to disambiguate explicit discourse connectives. For argument extraction, Lin et al. (2014) used a tree subtraction algorithm to extract arguments and Kong et al. (2014) proposed a • We implement a complete end-to-end PDTBstyle discourse parser for Chinese. • We design a uniform framework to recog27 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 27–32, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics framework shown in figure 1. We divide the shallow discourse parsing into four subtasks: anchor identification, argument extraction, sense classification and argument relabeling nize both explicit and non-explicit relations together. • We utilize an effective seed-expansion approa"
K16-2003,P12-1083,0,0.0201937,"from several words to sentences even to paragraphs. But in general, the span is in one sentence and the clauses split by punctuations can be regarded as the minimum constituent units. 3.1 Anchor Identification A full text is scanned to pick out the anchor candidate set. Then, a binary classifier is designed to check whether each candidate is anchor or not. The explicit connective candidate set is generated by matching the text with our connective dictionary. The non-explicit punctuation candidate set consists of all punctuations except for quotes, parentheses, and pause marks. • As shown in (Yang and Xue, 2012), punctuation marks play a significant role in Chinese discourse. Fortunately, CDTB has annotated those punctuations that may indicate discourse relations. Inspired by the above phenomena, we design our system by fully considering these Chinese characteristics. Besides the training data, we simply use skipgram neural word embeddings provided by the CoNLL-2016 organizers to replace words in some features. 3 Argument Extraction 3.1.1 Connective Identification A classifier is trained to recognize connectives. The features are chosen by referring to the best system in CoNLL-2015 (Wang and Lan, 201"
K16-2003,D09-1036,0,0.222568,"se Parser with Adaptation to Explicit and Non-explicit Relation Recognition Xiaomian Kang1,2 , Haoran Li1,2 , Long Zhou1,2 , Jiajun Zhang1,2 , Chengqing Zong1,2,3 1 National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences 2 The University of Chinese Academy of Sciences, Beijing, China 3 CAS Center for Excellence in Brain Science and Intelligence Technology {xiaomian.kang,haoran.li,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract constituent-based approach to solve it. Recent researches mainly focus on the implicit sense classification. In this subtask, Lin et al. (2009) and Rutherford and Xue (2014) explored rich features such as word-pairs, dependency rules, production rules and Brown cluster pairs. Some studies (Rutherford and Xue, 2015) paid attention to the data expansion. Neural network approaches (Ji and Eisenstein, 2015; Zhang et al., 2015) were also applied to improve the classification performance. Lin et al. (2014) implemented a full endto-end PDTB parser and Wang and Lan (2015) built a more refined system in the CoNLL-2015 Shared Task. In contrast to English, there are limited studies on Chinese discourse parsing (Huang and Chen, 2011; Zong, 2013;"
K16-2003,D15-1266,0,0.0292035,"Missing"
K16-2003,P12-1008,0,0.572677,"es, production rules and Brown cluster pairs. Some studies (Rutherford and Xue, 2015) paid attention to the data expansion. Neural network approaches (Ji and Eisenstein, 2015; Zhang et al., 2015) were also applied to improve the classification performance. Lin et al. (2014) implemented a full endto-end PDTB parser and Wang and Lan (2015) built a more refined system in the CoNLL-2015 Shared Task. In contrast to English, there are limited studies on Chinese discourse parsing (Huang and Chen, 2011; Zong, 2013; Tu et al., 2014). One of the main reasons is the shortage of Chinese discourse corpus. Zhou and Xue (2012) annotated a PDTBstyle Chinese Discourse TreeBank (CDTB), which is the data for Chinese shallow discourse parsing. In this paper, we describe our approaches to implement the Chinese shallow discourse parser which is participated in the CoNLL-2016 Shared Task (Xue et al., 2016). In view of some typical characteristics in CDTB (Section 2), we adopt and extend the state-of-the-art English parser in CoNLL-2015 (Wang and Lan, 2015). A unified framework for both explicit and non-explicit parsing is built and a seed-expansion approach is utilized for argument extraction. Some useful features are sele"
K16-2003,P09-2004,0,0.0298595,"08), discourse parsing has drawn more and more attention. The PDTB-style parser puts emphasis on shallow discourse parsing, which annotates a piece of text with a set of discourse relations. The relations are divided into two types, explicit or non-explicit, depending on whether connectives exist or not. A complete discourse relation contains two discourse units called Argument1 (Arg1) and Argument2 (Arg2). An end-to-end parser usually consists of some components, such as discourse connective identification, argument extraction, explicit sense classification and implicit sense classification. Pitler and Nenkova (2009) used syntactic features to disambiguate explicit discourse connectives. For argument extraction, Lin et al. (2014) used a tree subtraction algorithm to extract arguments and Kong et al. (2014) proposed a • We implement a complete end-to-end PDTBstyle discourse parser for Chinese. • We design a uniform framework to recog27 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 27–32, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics framework shown in figure 1. We divide the shallow discourse parsing into four subtasks:"
K16-2003,E14-1068,0,\N,Missing
K16-2003,N15-1081,0,\N,Missing
L16-1159,P02-1040,0,0.111085,"Missing"
L16-1159,A00-2002,0,0.236834,"Missing"
L16-1159,D08-1035,0,\N,Missing
L18-1143,D16-1162,0,0.0186183,"om the above methods, we treat this problem with another perspective, as we observe that the words need to be reordered during translation are more likely to be ignored by NMT. Thus we exploit the pre-ordering for NMT to alleviate this problem. Exploiting techniques in SMT for NMT. Our work is also inspired by the works which incorporating the techniques in SMT to NMT. The earlier related work is conducted on the SMT framework, which is deeply discussed in the reviewed paper (Zhang and Zong, 2015). Here, we only focus on the work which combines the SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive aut"
L18-1143,D14-1179,0,0.0511682,"Missing"
L18-1143,N16-1102,0,0.0260283,"m to pre-order the source sentences. Note that the word order of the target sentence does not change. 3.2. Position Embedding As mentioned before, the most noticeable feature of preordering is that it can make the word order in source more consistent with the word order in target. Intuitively, monotone translation is preferred. That is to say the words in the similar positions between the source and target sentences are more likely to be translation pairs. Thus, we further enhance the pre-ordering model with the position embedding to encourage monotone translation. Actually, previous studies (Cohn et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) have shown that the position information is effective for NMT, and these studies are all based on the following assumption: Assumption: a word at a given relative position j in the source (whose length is denoted as J) is more likely to align to a word at a similar relative position i in the target (whose length is denoted as I), i.e. Jj ≈ Ii . Obviously, pre-ordering can make more words satisfy this assumption. We design the procedure as follows: We first randomly generate the respective position embedding matrix for the source and target position"
L18-1143,P05-1066,0,0.290685,"C) = p(yi |y<i , ci ) = sof tmax(Wyi zei + bs ) (2) where Wy is an embedding matrix containing row vectors of the target words and zei is the attention output: zei = tanh(Wc [zim ; ci ]) (3) The attention model calculates ci as the weighted sum of the source-side context vectors: ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in th"
L18-1143,C10-1043,0,0.433597,"ning (18 times) is the case that the sub-sentences in source are totally dropped. From these statistics, we think that the first kind of under-translation, i.e. words need to be reordered are ignored, is a major problem affecting the final translation quality. Considering the fact that source words requiring reordering during translation are more likely to be ignored by the NMT model, we propose to exploit the pre-ordering approach which is commonly used in Statistical Machine Translation (SMT). The pre-ordering can make the word order of a source sentence closer to that of a target sentence (Genzel, 2010; Hitschler et al., 2016). We first pre-order the source sentences to approximate the target language word order. We then further combine the pre-ordering model with the position embedding strategy to enhance the monotone translation. Finally, to overcome the over-translation problem, we augment our model with the coverage mechanism. In this paper, we make the following contributions: 1) Through error analysis, we find that under-translation occurs more frequently than over-translation in NMT and source words that need reordering are more likely to be missed. We propose a pre-ordering approach"
L18-1143,J82-2005,0,0.699175,"Missing"
L18-1143,D13-1176,0,0.10621,"Missing"
L18-1143,N03-1017,0,0.0499309,": ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in this source sentence is adjusted to the word order in reference. When translating the pre-ordered source sentence, the translation system does not need to reorder the source words. Since we find that the source words should to be reordered during translation are more likely to be"
L18-1143,P07-2045,0,0.0133189,"per-parameters we used in Otedama are set as follows: window size is set to 3, matching feature is 10, and the max waiting time is 30 minute. The others are set to the default values. More details can be found in (Hitschler et al., 2016). 4.4. (9) k=1 ei,j =vaT tanh(Wa zi + Ua hj + where Ci−1,j is the coverage vector of source word xj before time i, and Va is the weight matrix for coverage vector. Translation Methods In the experiments, we compare our approaches with other models, and we list all the translation methods as follows: 1) Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007). Our system is built using the default settings. 2) Baseline: It is the baseline attention-based NMT system (Luong et al., 2015; Zoph and Knight, 2016). 2 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07. 3 https://github.com/isi-nlp/ZophRNN. We extend this toolkit with global attention, and change the attention model to the way shown in Eq. 6. 4 https://github.com/StatNLP/otedama. 895 3) +Pre-Ordering: It is the NMT system which only uses the pre-ordering approach. 4) +Position: It is the NMT system which only employs the position embedding. 5) +"
L18-1143,D15-1166,0,0.090334,"Missing"
L18-1143,D16-1096,0,0.0998901,"Missing"
L18-1143,P02-1040,0,0.100887,"Missing"
L18-1143,P16-2049,0,0.039992,"Missing"
L18-1143,P16-5005,0,0.037963,"Missing"
L18-1143,Y09-2016,1,0.757731,"= sof tmax(Wyi zei + bs ) (2) where Wy is an embedding matrix containing row vectors of the target words and zei is the attention output: zei = tanh(Wc [zim ; ci ]) (3) The attention model calculates ci as the weighted sum of the source-side context vectors: ci = Tx X ai,j hm i (4) j=1 Where aij can be computed by exp(ei,j ) ai,j = PT x k=1 exp(ei,k ) (5) ei,j = vaT tanh(Wa zi + Ua hj ) (6) and zik is computed using the following formula: k zjk = LST M (zj−1 , zjk−1 ) 3. (7) Exploiting Pre-Ordering for NMT In SMT, pre-ordering is a commonly used pre-processing technique (Collins et al., 2005; Zhang and Zong, 2009; Genzel, 2010; Hitschler et al., 2016), which makes the word order of a source sentence closer to that of a target sentence. This technology was originally proposed to alleviate the weakness of reordering in classical phrase-based SMT (Koehn et al., 2003). As SMT always penalizes the cases that move target phrases far away from their corresponding source positions. Fig. 1 shows an example of pre-ordering, in which when translating the original source sentence, the words in red and words in blue need to exchange their positions. With the pre-ordering, the word order in this source sentence is"
L18-1143,P14-1011,1,0.820879,"SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive auto-encoders to learn phrase embeddings, which can distinguish the phrases with different semantic meanings. (Tang et al., 2016) explores the possibility to incorporate phrase memory into NMT, in which the decoder can generate a sequence of multiple words all at once. In this work, we exploit another new technique in SMT, preordering, to NMT to improve the translation performance. 7. Conclusions and Future Work We have exploited the pre-ordering approach to alleviate the under-translation problem in NMT. Specifically, we preorder the source sentence"
L18-1143,P17-2060,1,0.765969,"SMT to NMT. The earlier related work is conducted on the SMT framework, which is deeply discussed in the reviewed paper (Zhang and Zong, 2015). Here, we only focus on the work which combines the SMT and NMT on NMT framework. Specifically, (Arthur et al., 2016) incorporates word translation table in attention part to adjust the final loss. (Zhang and Zong, 2016) moves forward further by incorporating a bilingual dictionaries in NMT. (Stahlberg et al., 2016) and (He et al., 2016) rescore word candidates with SMT features. (G¨ulc¸ehre et al., 2015) improves the beam search with language model. (Zhou et al., 2017) proposes a neural combination model to fuse the NMT translation results and SMT translation results. (Wang et al., 2017) improves the NMT system with the SMT recommendations. (Zhang et al., 2014) proposes bilingually-constrained recursive auto-encoders to learn phrase embeddings, which can distinguish the phrases with different semantic meanings. (Tang et al., 2016) explores the possibility to incorporate phrase memory into NMT, in which the decoder can generate a sequence of multiple words all at once. In this work, we exploit another new technique in SMT, preordering, to NMT to improve the"
L18-1143,N16-1004,0,0.0182704,"ers are set to the default values. More details can be found in (Hitschler et al., 2016). 4.4. (9) k=1 ei,j =vaT tanh(Wa zi + Ua hj + where Ci−1,j is the coverage vector of source word xj before time i, and Va is the weight matrix for coverage vector. Translation Methods In the experiments, we compare our approaches with other models, and we list all the translation methods as follows: 1) Moses: It is the state-of-the-art phrase-based SMT system (Koehn et al., 2007). Our system is built using the default settings. 2) Baseline: It is the baseline attention-based NMT system (Luong et al., 2015; Zoph and Knight, 2016). 2 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07. 3 https://github.com/isi-nlp/ZophRNN. We extend this toolkit with global attention, and change the attention model to the way shown in Eq. 6. 4 https://github.com/StatNLP/otedama. 895 3) +Pre-Ordering: It is the NMT system which only uses the pre-ordering approach. 4) +Position: It is the NMT system which only employs the position embedding. 5) +Pre-Ordering+Position: It is the NMT system using both pre-ordering and position embedding together. 6) +Coverage: It is the NMT system with the coverag"
L18-1146,2013.mtsummit-papers.5,0,0.0226409,"ong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accompl"
L18-1146,P16-1185,0,0.0199226,"the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translatio"
L18-1146,D14-1179,0,0.0966249,"Missing"
L18-1146,P14-1129,0,0.0360782,"bone, it can produce a correct translation for the testing sentence with a minor modification. Whereas the baseline skips the source word ”以色列” (Israel) and translates the source word ”秘书长” (secretary - general) twice. In the lower example, we can only find a not so similar sentence to the testing one, with a similarity score of 0.31. However, the sentence pair found in the example can remind the model how to translate the phrase “方括号”, whose translation is missing in the baseline system. 6. Related Work After a few pioneer work in exploring neural features in SMT systems (Zhang et al., 2014; Devlin et al., 2014), NMT quickly become the dominant approach for machine translation. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) first propose to use the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabula"
L18-1146,P15-1166,0,0.068233,"Missing"
L18-1146,P16-1227,0,0.0141056,"-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need careful"
L18-1146,P15-1001,0,0.15154,"uction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepancy between training"
L18-1146,D13-1176,0,0.446148,"In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available. Keywords: Neural machine translation, online learning, sentence similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is an"
L18-1146,2010.jec-1.4,0,0.0908634,"et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model fo"
L18-1146,2014.amta-researchers.19,0,0.0322218,"nd multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accomplished by two-stage training. An gener"
L18-1146,D12-1037,0,0.144875,"nism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity le"
L18-1146,D15-1166,0,0.335593,"similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepan"
L18-1146,P15-1002,0,0.105525,"similarity 1. Introduction Neural machine translation achieved great success recently (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Thanks to the end-to-end training paradigm and the powerful modeling capacity of neural network, NMT can produce comparable or even better results than traditional statistical machine translation, only after a few years of development. However, it also raises some new problems, such as how to use open vocabulary and how to avoid repeating and missing translations. These problems have been addressed by various recent approaches (Luong et al., 2015b; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016). How to learn a good set of parameters is another challenge for nowadays deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepan"
L18-1146,P11-1124,0,0.128975,"Missing"
L18-1146,1983.tc-1.13,0,0.672139,"Missing"
L18-1146,P02-1040,0,0.100924,"phrase in (s,t) i=1 The translation probability of each training sentence pair is calculated offline with the general network parameters. We don’t use the phrase pairs as training data to fine-tune the network parameters. There are two reasons. First, context information is not available for choosing the proper phrase translation. Second, training on phrase pairs will harm the recurrent weights of the network, because they are not complete sentences1 . 5. Experiments We evaluate our proposed method on the Chinese to English translation task. Translation quality is measured by the BLEU metric (Papineni et al., 2002). 5.1. Datasets We conduct experiments on two datasets. One is on the United Nations Parallel Corpus2 , which is composed of official records and other parliamentary documents of the United Nations. Since this data is from a narrow domain, it is relatively easy to find similar sentences for many testing sentences. The training data contains 1M sentence pairs extracted from the corpus, and the testing data contains 5 groups of sentence pairs, with 200 sentence pairs in each group. The most similar3 sentence we can find for the sentences in each group falls into the similarity range of 0-0.2, 0."
L18-1146,J82-2005,0,0.715304,"Missing"
L18-1146,P13-1002,1,0.90906,"and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013; Li et al., 2014). However, they need carefully designed features and only show improvement when similarity level is high. In comparison, our method don’t need any modification to the model, and it can bring improvement in all similarity level. 915 Finding similar sentences with inverted index is fast enough in our experiments. If the training data is much larger than ours, locality sensitive hash such as MinHash (Broder, 1997) may be a better choice. 7. Conclusion and Future Work In this paper, we propose to learn a specific model for each testing sentence. This is accomplished by two-stage"
L18-1146,D16-1137,0,0.0224659,"deep neural networks. There has been some work in the field of NMT. Shen et al. (2015) propose to use task specific optimization function. Specially, they propose to directly optimize BLEU score instead of likelihood of the training data. Bengio et al. (2015) take search into consideration during training. In common practice, the decoder uses gold reference as history during training, but it has to use generated output as history during testing. To fix this discrepancy between training and testing, the authors propose to moderately replace gold reference with generated output during training. Wiseman and Rush (2016) take a similar approach and regard training as beam search optimization. However, no matter how the network parameters are learnt, they are fixed after the training is finished in all current NMT practice. And the same model is applied to every testing sentence. A potential issue of this practice is that a neural network needs to be able to compress all translation knowledge into a fixed set of parameters, which is very hard in reality. So we propose to learn a specific model for each testing sentence by paying more attention to those related sentences. In particular, we propose a learning on"
L18-1146,P14-1011,1,0.792564,"e. Based on the backbone, it can produce a correct translation for the testing sentence with a minor modification. Whereas the baseline skips the source word ”以色列” (Israel) and translates the source word ”秘书长” (secretary - general) twice. In the lower example, we can only find a not so similar sentence to the testing one, with a similarity score of 0.31. However, the sentence pair found in the example can remind the model how to translate the phrase “方括号”, whose translation is missing in the baseline system. 6. Related Work After a few pioneer work in exploring neural features in SMT systems (Zhang et al., 2014; Devlin et al., 2014), NMT quickly become the dominant approach for machine translation. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) first propose to use the encoder-decoder architecture to do sequence to sequence mapping. At the same time, Sutskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability"
L18-1146,N16-1004,0,0.0213779,"tskever et al. (2014) apply it in end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically attend to different source words when generating different target words, which becomes the default component of current NMT systems. Recent advances in NMT include fixing defects of the model, such as inability to use large vocabulary (Luong et al., 2015b; Jean et al., 2015), unawareness of coverage (Tu et al., 2016; Mi et al., 2016) etc, making use of mono-lingual data (Cheng et al., 2016; Sennrich et al., 2015), extending to multi-lingual(Dong et al., 2015; Zoph and Knight, 2016) and multi-modal (Hitschler and Riezler, 2016) scenarios. In statistical machine translation, Liu et al. (2012) proposes a local training method which also learns sentencewise weights based on similar sentences. However, since there are only about a dozen of features in SMT, such as translation score and language model score, adjusting the relative weights of these features cannot making full use of the similar sentences. There are some other work making use of similar sentences by means of translation memory (Koehn and Senellart, 2010; Ma et al., 2011; Bertoldi et al., 2013; Wang et al., 2013"
P08-2065,P07-1056,0,0.888588,"d corpora is difficult and time-consuming. Given the limited multi-domain training data, an interesting task arises, how to best make full use of all training data to improve sentiment classification performance. We name Related Work Sentiment classification has become a hot topic since the publication work that discusses classification of movie reviews by Pang et al. (2002). This was followed by a great many studies into sentiment classification focusing on many domains besides that of movie. Research into sentiment classification over multiple domains remains sparse. It is worth noting that Blitzer et al. (2007) deal with the domain adaptation problem for sentiment classification where labeled data from one domain is used to train a classifier for classifying data from a different domain. Our work focuses on the problem of how to make multiple domains ‘help each other’ when all contain some labeled samples. These two problems are both important for real applications of sentiment classification. 3 3.1 Our Approaches Problem Statement In a standard supervised classification problem, we seek a predictor f (also called a classifier) that 257 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pa"
P08-2065,W02-1011,0,0.0208768,"ta from multiple domains. To achieve this, we propose two approaches of fusion, feature-level and classifier-level, to use training data from multiple domains simultaneously. Experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification (using the training data individually). 1 2 Introduction Sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of, or sentiment toward a given subject (e.g., if an opinion is supported or not) (Pang et al., 2002). This task has created a considerable interest due to its wide applications. Sentiment classification is a very domainspecific problem; training a classifier using the data from one domain may fail when testing against data from another. As a result, real application systems usually require some labeled data from multiple domains, guaranteeing an acceptable performance for different domains. However, each domain has a very limited amount of training data due to the fact that creating largescale high-quality labeled corpora is difficult and time-consuming. Given the limited multi-domain traini"
P09-1078,P07-1056,0,0.0295975,"e carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.edu/People/pabo/movie-review-data/ http://www.seas.upenn.edu/~mdredze/dataset"
P09-1078,P06-2079,0,0.0544922,"Missing"
P09-1078,W02-1011,0,0.0203942,"Missing"
P09-1078,P04-1035,0,0.00597525,"Experimental Studies 4.1 Experimental Setup Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.ed"
P09-1078,W06-1652,0,0.262065,"Missing"
P10-1065,P02-1051,0,0.161199,"tion In trans-lingual language processing tasks, such as machine translation and cross-lingual information retrieval, named entity (NE) translation is essential. Bilingual NE alignment, which links source NEs and target NEs, is the first step to train the NE translation model. Since NE alignment can only be conducted after its associated NEs have first been identified, the including-rate of the first recognition stage significantly limits the final alignment performance. To alleviate the above error accumulation problem, two strategies have been proposed in the literature. The first strategy (Al-Onaizan and Knight, 2002; Moore, 2003; Feng et al., 2004; Lee et al., 2006) identifies NEs only on the source side and then finds their corresponding NEs on the target side. In this way, it avoids the NE recognition errors which would otherwise be Keh-Yih Su Behavior Design Corporation Hsinchu, Taiwan, R.O.C. bdc.kysu@gmail.com brought into the alignment stage from the target side; however, the NE errors from the source side still remain. To further reduce the errors from the source side, the second strategy (Huang et al., 2003) expands the NE candidate-sets in both languages before conducting the alignment, which is"
P10-1065,zhang-etal-2004-interpreting,0,0.0439186,"Missing"
P10-1065,J96-1002,0,0.013354,"ttp://www.speech.sri.com/projects/srilm/ as the main data at Table 2. The second one (named type-sensitive) would also evaluate the associated NE type of each NE, and is given within parentheses in Table 2. A large degradation is observed when NE type is also taken into account. The highlighted entries are those that are statistically better 6 than that of the baseline system. 4.3 ME Approach with Primitive Features Although the proposed model has been derived above in a principled way, since all these proposed features can also be directly integrated with the well-known maximum entropy (ME) (Berger et al., 1996) framework without making any assumptions, one might wonder if it is still worth to deriving a model after all the related features have been proposed. To show that not only the features but also the adopted model contribute to the performance improvement, an ME approach is tested as follows for comparison. It directly adopts all those primitive features mentioned above as its inputs (including internal component mapping, initial and final NE type, NE bigram-based string, and left/right distance), without involving any related probability factors derived within the proposed model. This ME meth"
P10-1065,W03-1501,0,0.28829,"Missing"
P10-1065,W04-3248,0,0.0708686,"tasks, such as machine translation and cross-lingual information retrieval, named entity (NE) translation is essential. Bilingual NE alignment, which links source NEs and target NEs, is the first step to train the NE translation model. Since NE alignment can only be conducted after its associated NEs have first been identified, the including-rate of the first recognition stage significantly limits the final alignment performance. To alleviate the above error accumulation problem, two strategies have been proposed in the literature. The first strategy (Al-Onaizan and Knight, 2002; Moore, 2003; Feng et al., 2004; Lee et al., 2006) identifies NEs only on the source side and then finds their corresponding NEs on the target side. In this way, it avoids the NE recognition errors which would otherwise be Keh-Yih Su Behavior Design Corporation Hsinchu, Taiwan, R.O.C. bdc.kysu@gmail.com brought into the alignment stage from the target side; however, the NE errors from the source side still remain. To further reduce the errors from the source side, the second strategy (Huang et al., 2003) expands the NE candidate-sets in both languages before conducting the alignment, which is done by treating the original r"
P10-1065,W03-1502,0,0.0592741,"o, and P  RCNE |CNE , CType, CS , RType  are Monolingual Candidate Certainty Factors (Section 3.2) used to assign preference to each selected RCNE and RENE , based on the initially recognized NEs (which act as anchors). transliteration NTL n2 1   CNE, CType , CS,  P  MIC , RType, RCNE, RENE  [ENE, EType], ES    P  MIC RType, RCNE, RENE   P  RCNE |CNE, CType, CS, RType NTL (3) In the above equation, the mappings between internal components are trained from the syllable/word alignment of NE pairs of different NE types. In more detail ， for transliteration, the model adopted in (Huang et al., 2003), which first Romanizes Chinese characters and then transliterates them into English characters, is 633 tributions. The corresponding weighting coefficients are obtained using the well-known Minimum Error Rate Training (Och, 2003; commonly abbreviated as MERT) algorithm by minimizing the number of associated errors in the development set. used for P(cpn n  |TLn , ew[ n ] , RType) . For translation, conditional probability is directly used for P (cpn n  |TS n , ew[ n ] , RType) . Lastly, the bilingual type re-assignment factor P  RType |CNE , ENE , CType, EType  proposed in Eq (2) is deri"
P10-1065,P06-2055,0,0.0901131,"Missing"
P10-1065,E03-1035,0,\N,Missing
P10-1065,P03-1021,0,\N,Missing
P11-2028,W05-0909,0,0.217552,"ord as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be a Chinese word or a Chinese character. As shown in Figure 1, given an English sentence “how much are the umbrellas?” a Chinese system translation (or a reference translation) can be segmented into characters (Figure 1(a)) or words (Figure 1(b)). A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (exact) (Banerjee and Lavie, 2005), GTM (Melamed et al., 2003), and TER 160 Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). The version we used was released on 2008-05-21 and the standard adopted is CTB. Urheen: Urheen is a CWS tool developed by (Wang et al., 2010a; Wang et al., 2010b), and it outperformed most of the state-of-the-art CWS systems in the CIPS-SIGHAN’2010 evaluation. This tool is trained on Chinese Treebank 6.0. 4 4.1 Experimental Results Data To compare the word-level automatic MT evaluation metrics with the character-level met"
P11-2028,W07-0718,0,0.0483886,"per due to length constraint. The NIST&apos;08 English-to-Chinese translation task evaluated 127 documents with 1,830 segments. Each segment has 4 reference translations and the system translations of 11 MT systems, released in the corpus LDC2010T01. We asked native speakers of Chinese to perform fluency and adequacy judgment on a five-point scale. Human assessment was done on the first 30 documents (355 segments) (document id “AFP_ENG_20070701.0026” to “AFP_ENG_20070731.0115”). The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al., 2007). The adequacy score indicates the overlap of the meaning expressed in the reference translations with a system translation, while the fluency score indicates how fluent a system translation is. 4.2 tion Segment-Level Consistency or CorrelaFor human fluency and adequacy judgments, the Pearson correlation coefficient is used to compute the segment-level correlation between human judgments and automatic metrics. Human rank judgment is not an absolute score and thus Pearson correlation coefficient cannot be used. We calculate segment-level consistency as follows:  The consistent number of pair"
P11-2028,W08-0336,0,0.141387,"Missing"
P11-2028,I05-3025,1,0.867134,"results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ (b) Segmented into words by Urheen. Figure 1. An example to show an MT system translation and multiple reference translations being segmented into characters or words. To evaluate English translation output, automatic MT evaluation metrics take an English word as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be"
P11-2028,N03-2021,0,0.149985,"tching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use in matching can be a Chinese word or a Chinese character. As shown in Figure 1, given an English sentence “how much are the umbrellas?” a Chinese system translation (or a reference translation) can be segmented into characters (Figure 1(a)) or words (Figure 1(b)). A variety of automatic MT evaluation metrics have been developed over the years, including BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (exact) (Banerjee and Lavie, 2005), GTM (Melamed et al., 2003), and TER 160 Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). The version we used was released on 2008-05-21 and the standard adopted is CTB. Urheen: Urheen is a CWS tool developed by (Wang et al., 2010a; Wang et al., 2010b), and it outperformed most of the state-of-the-art CWS systems in the CIPS-SIGHAN’2010 evaluation. This tool is trained on Chinese Treebank 6.0. 4 4.1 Experimental Results Data To compare the word-level automatic MT evaluation metrics with the character-level metrics, we conducted experimen"
P11-2028,W04-3236,1,0.794426,"rent segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ (b) Segmented into words by Urheen. Figure 1. An example to show an MT system translation and multiple reference translations being segmented into characters or words. To evaluate English translation output, automatic MT evaluation metrics take an English word as the smallest unit when matching a system translation and a reference translation. On the other hand, to evaluate Chinese translation output, the smallest unit to use"
P11-2028,P02-1040,0,0.100113,"sentence into words, an alternative is to split a Chinese sentence into characters, which can be readily done with perfect accuracy. However, it has been reported that a Chinese-English phrase-based SMT system (Xu et al., 2004) that relied on characters (without CWS) performed slightly worse than when it used segmented words. It has been recognized that varying segmentation granularities are needed for SMT (Chang et al., 2008). To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT&apos;2005) used the word-level BLEU metric (Papineni et al., 2002). However, IWSLT&apos;08 and NIST&apos;08 adopted character-level evaluation metrics to rank the submitted systems. Although there is much work on automatic evaluation of machine translation (MT), whether word or character is more suitable for automatic evaluation of Chinese translation output has not been systematically investigated. In this paper, we utilize various machine translation evaluation metrics to evaluate the quality of Chinese translation output, and compare their correlation with human assessment when the Chinese translation output is segmented into words versus characters. Since there ar"
P11-2028,2006.amta-papers.25,0,0.0912873,"Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics trics correlate with human assessment better than word-level metrics. That is, CWS is not essential for automatic evaluation of Chinese translation output. Our analysis suggests several key reasons behind this finding. 2 Chinese Translation Evaluation Automatic MT evaluation aims at formulating automatic metrics to measure the quality of MT output. Compared with human assessment, automatic evaluation metrics can assess the quality of MT output quickly and objectively without much human labor. Translation: 多_少_钱_的_伞_吗_？ (Snover et al., 2006). Some automatic MT evaluation metrics perform deeper linguistic analysis, such as part-of-speech tagging, synonym matching, semantic role labeling, etc. Since part-of-speech tags are only defined for Chinese words and not for Chinese characters, we restrict the automatic MT evaluation metrics explored in this paper to those metrics listed above which do not require part-ofspeech tagging. 3 CWS Tools Since there are a number of CWS tools and they give different segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully u"
P11-2028,I05-3027,0,0.289782,"Missing"
P11-2028,C10-1132,1,0.892928,"Missing"
P11-2028,W10-4133,1,0.895636,"Missing"
P11-2028,W04-1118,0,0.290667,"pore 13 Computing Drive, Singapore 117417 nght@comp.nus.edu.sg machine translation (SMT), it can happen that the most accurate word segmentation as judged by the human gold-standard segmentation may not produce the best translation output (Zhang et al., 2008). While state-of-the-art Chinese word segmenters achieve high accuracy, some errors still remain. Instead of segmenting a Chinese sentence into words, an alternative is to split a Chinese sentence into characters, which can be readily done with perfect accuracy. However, it has been reported that a Chinese-English phrase-based SMT system (Xu et al., 2004) that relied on characters (without CWS) performed slightly worse than when it used segmented words. It has been recognized that varying segmentation granularities are needed for SMT (Chang et al., 2008). To evaluate the quality of Chinese translation output, the International Workshop on Spoken Language Translation in 2005 (IWSLT&apos;2005) used the word-level BLEU metric (Papineni et al., 2002). However, IWSLT&apos;08 and NIST&apos;08 adopted character-level evaluation metrics to rank the submitted systems. Although there is much work on automatic evaluation of machine translation (MT), whether word or cha"
P11-2028,W03-1709,0,0.337126,"n metrics perform deeper linguistic analysis, such as part-of-speech tagging, synonym matching, semantic role labeling, etc. Since part-of-speech tags are only defined for Chinese words and not for Chinese characters, we restrict the automatic MT evaluation metrics explored in this paper to those metrics listed above which do not require part-ofspeech tagging. 3 CWS Tools Since there are a number of CWS tools and they give different segmentation results in general, we experimented with four different CWS tools in this paper. ICTCLAS: ICTCLAS has been successfully used in a commercial product (Zhang et al., 2003). The version we adopt in this paper is ICTCLAS2009. Ref 1: 这_些_雨_伞_多_少_钱_？ …… Ref 7: 这_些_雨_伞_的_价_格_是_多_少_？ (a) Segmented into characters. NUS Chinese word segmenter (NUS): The NUS Chinese word segmenter uses a maximum entropy approach to Chinese word segmentation, which achieved the highest F-measure on three of the four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff (Ng and Low, 2004; Low et al., 2005). The segmentation standard adopted in this paper is CTB (Chinese Treebank). Translation: 多少_钱_的_伞_吗_？ Ref 1: 这些_雨伞_多少_钱_？ …… Ref 7: 这些_雨伞_的_价格_是_多少_？ ("
P11-2028,2008.iwslt-evaluation.1,0,\N,Missing
P13-1002,W09-0437,0,0.0323477,"Missing"
P13-1002,P11-1124,0,0.702859,"Missing"
P13-1002,P10-1064,0,0.595665,"Missing"
P13-1002,2011.mtsummit-papers.52,0,0.597744,"Missing"
P13-1002,P03-1021,0,0.0314314,"Missing"
P13-1002,J03-1002,0,0.00987415,"Missing"
P13-1002,W04-3250,0,0.22636,"Missing"
P13-1002,P02-1040,0,0.0859264,"Missing"
P13-1002,J82-2005,0,0.795373,"Missing"
P13-1002,N03-1017,0,0.162764,"er to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Besides, the proposed models also outperform previous approaches significantly. 1 Introduction Statistical machine translation (SMT), especially the phrase-based model (Koehn et al., 2003), has developed very fast in the last decade. For certain language pairs and special applications, SMT output has reached an acceptable level, especially in the domains where abundant parallel corpora are available (He et al., 2010). However, SMT is rarely applied to professional translation because its output quality is still far from satisfactory. Especially, there is no guarantee that a SMT system can produce translations in a consistent manner (Ma et al., 2011). In contrast, translation memory (TM), which uses the most similar translation sentence (usually above a certain fuzzy match thres"
P13-1002,2009.mtsummit-papers.14,0,0.497295,"is4 associated5”, and “an1 object2 that3 is4 associated5 with6”, etc. And it is hard to tell which one should be adopted in the merging stage. Thirdly, the pipeline approach does not utilize the SMT probabilistic information in deciding whether a matched TM phrase should be adopted or not, and which target phrase should be selected when we have multiple candidates. Therefore, the possible improvements resulted from those pipeline approaches are quite limited. On the other hand, instead of directly merging TM matched phrases into the source sentence, some approaches (Biçici and Dymetman, 2008; Simard and Isabelle, 2009) simply add the longest matched pairs into SMT phrase table, and then associate them with a fixed large probability value to favor the corresponding TM target phrase at SMT decoding. However, since only one aligned target phrase will be added for each matched source phrase, they share most drawbacks with the pipeline approaches mentioned above and merely achieve similar performance. To avoid the drawbacks of the pipeline approach (mainly due to making a hard decision before decoding), we propose several integrated models to completely make use of TM information during decoding. For each TM sou"
P13-1002,2010.jec-1.4,0,0.375723,"is very repetitive. In general, for those matched segments1, TM provides more reliable results than SMT does. One reason is that the results of TM have been revised by human according to the global context, but SMT only utilizes local context. However, for those unmatched segments, SMT is more reliable. Since TM and SMT complement each other in those matched and unmatched segments, the output quality is expected to be raised significantly if they can be combined to supplement each other. In recent years, some previous works have incorporated TM matched segments into SMT in a pipelined manner (Koehn and Senellart, 2010; Zhechev and van Genabith, 2010; He et al., 2011; Ma et al., 2011). All these pipeline approaches translate the sentence in two stages. They first determine whether the extracted TM sentence pair should be adopted or not. Most of them use fuzzy match score as the threshold, but He et al. (2011) and Ma et al. (2011) use a classifier to make the judgment. Afterwards, they merge the relevant translations of matched segments into the source sentence, and then force the SMT system to only translate those unmatched segments at decoding. There are three obvious drawbacks for the above pipeline appro"
P13-1002,2006.amta-papers.25,0,0.113287,"Missing"
P13-1002,W10-3806,0,0.576925,"Missing"
P13-1002,C08-1144,0,0.0494857,"Missing"
P13-1002,D10-1091,0,\N,Missing
P13-1002,P07-2045,0,\N,Missing
P13-1111,J04-4004,0,0.0118426,"English Gigaword corpus and target part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together to get a combined rule set. We discard the rules with fewer than 5 appearances. Using this set, we can train our MEPD model directly. As to translation, we match the 5 SRL results with transformation rules respectively, and then apply the resulting target-side-like PASs for decoding. As we mentioned in section 2.3, we use the state-of-the-art BTG system to translat"
P13-1111,J07-2003,0,0.19644,"Missing"
P13-1111,2007.tmi-papers.6,0,0.0208573,"), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable impr"
P13-1111,D07-1007,0,0.0657799,"), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable impr"
P13-1111,P07-1005,0,0.0498005,"a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable improvements on translation quality."
P13-1111,P03-2041,0,0.0185004,") 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1] [X2] [X3]>, while in ] [ X1 [X2] [ X3 ] the location of the olympic village is the best for athletes (c) Figure 1. An example of"
P13-1111,2007.tmi-papers.10,0,0.0527801,"Missing"
P13-1111,W11-1012,0,0.0288647,"Missing"
P13-1111,P03-1021,0,0.012381,"to substitute PASTR for building a PAS-based translation system directly. We use “IC-PASTR” to denote this system. In addition, since our method of rule extraction is different from (Zhai et al., 2012), we also use PASTR to construct a translation system as the baseline system, which we call “PASTR”. On the basis of PASTR and IC-PASTR, we further integrate our MEPD model into translation. Specifically, we take the score of the MEPD model as another informative feature for the decoder to distinguish good target-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better"
P13-1111,J04-4002,0,0.205085,"Missing"
P13-1111,P03-1054,0,0.00365332,"t part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together to get a combined rule set. We discard the rules with fewer than 5 appearances. Using this set, we can train our MEPD model directly. As to translation, we match the 5 SRL results with transformation rules respectively, and then apply the resulting target-side-like PASs for decoding. As we mentioned in section 2.3, we use the state-of-the-art BTG system to translate the non-PAS spans. source-side PAS counts nu"
P13-1111,N03-1017,0,0.0162886,"Missing"
P13-1111,W04-3250,0,0.0739352,"s. The LDC category number : LDC2000T50, LDC2002E18, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34. 1131 whose lengths are among 10 and 30 words. Finally, the development set includes 595 sentences from NIST MT03 and the test set contains 1,786 sentences from NIST MT04 and MT05. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and target part of the training data. The translation quality is evaluated by case-insensitive BLEU-4 with shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). We perform SRL on the source part of the training set, development set and test set by the Chinese SRL system used in (Zhuang and Zong, 2010b). To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). Therefore, at last, we can get 5 SRL result for each sentence. For the training set, we use these SRL results to do rule extraction respectively. We combine the obtained rules together"
P13-1111,P07-2045,0,0.0048156,"the source and target predicate are “是(shi)” and “is” respectively. The predicate feature is thus “PredF=是(shi)+is”. The target predicate is determined by: t -pred = arg max p (t j |s -pred ) j∈t_range ( PAS ) is 3 where s-pred is the source predicate and t-pred the corresponding target predicate. http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.htm l 1130 t_range(PAS) refers to the target range covering all the words that are reachable from the PAS via word alignment. tj refers to the jth word in t_range(PAS). The utilized lexical translation probabilities are from the toolkit in Moses (Koehn et al., 2007).  Syntax Features. These features include st(Ei), i.e., the highest syntax tag for each argument, and fst(PAS) which is the lowest father node of sp in the parse tree. For example, for the rule shown in Figure 4(b), syntax features are st([A0]1)=NP, st([A1]4)=CP, and fst(PAS)=IP respectively. Using these features, we can train the MEPD model. We set the Gaussian prior to 1.0 and perform 100 iterations of the L-BFGS algorithm for each MEPD model. At last, we build 160 and 215 different MEPD classifiers, respectively, for the PASTRs and IC-PASTRs. Note that since the training procedure of maxi"
P13-1111,2006.iwslt-evaluation.11,0,0.0706908,"Missing"
P13-1111,W08-0308,0,0.0296806,"Missing"
P13-1111,C10-1081,0,0.0731141,"Missing"
P13-1111,D08-1010,0,0.545389,"-side-like PASs from bad ones. The weights of the MEPD feature can be tuned by MERT (Och, 2003) together with other translation features, such as language model. 6 Related Work The method of PAS disambiguation for SMT is relevant to the previous work on context dependent translation. Carpuat and Wu (2007a, 2007b) and Chan et al. (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. They combine rich context information to do disambiguation for words or phrases, and achieve improved translation performance. Differently, He et al. (2008), Liu et al. (2008) and Cui et al. (2010) designed maximum entropy (ME) classifiers to do better rule section for hierarchical phrase-based model and tree-to-string model respectively. By incorporating the rich context information as features, they chose better rules for translation and yielded stable improvements on translation quality. Our work differs from the above work in the following two aspects: 1) in our work, we focus on the problem of disambiguates on PAS; 2) we define two kinds of PAS ambiguities: role ambiguity and gap ambiguity. 3) towards the two different ambiguities, we design two specific metho"
P13-1111,P06-1077,0,0.0472421,"Missing"
P13-1111,W06-1606,0,0.0418659,"Missing"
P13-1111,P02-1040,0,0.0863687,"Missing"
P13-1111,P06-1055,0,0.0324144,"Missing"
P13-1111,2009.eamt-1.30,0,0.0390937,"rovements on translation quality. 1 防洪 是 首要 的 任务 [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 X3 ] ] [ X2 ] [ flood prevention is the primary mission (a) 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1"
P13-1111,N09-2004,0,0.0491553,"rovements on translation quality. 1 防洪 是 首要 的 任务 [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 X3 ] ] [ X2 ] [ flood prevention is the primary mission (a) 中国 和 俄罗斯 是 两个 大国 ， 应 … [ A0 ]1 [Pred]2 [ A1 ]3 [ X1 ] [X2] [ ] X3 being two major countries , China and Russia should … (b) 奥运村 的 位置 对 运动员 是 最好的 [ A0 ]1 [Pred]2 [ A1 ]3 Introduction Predicate-argument structure (PAS) depicts the relationship between a predicate and its associated arguments, which indicates the skeleton structure of a sentence on semantic level. Basically, PAS agrees much better between two languages than syntax structure (Fung et al., 2006; Wu and Fung, 2009b). Considering that current syntaxbased translation models are always impaired by cross-lingual structure divergence (Eisner, 2003; Zhang et al., 2010), PAS is really a better representation of a sentence pair to model the bilingual structure mapping. However, since a source-side PAS might correspond to multiple different target-side PASs, there usually exist many PAS ambiguities during translation. For example, in Figure 1, (a) and (b) carry the same source-side PAS <[A0]1 [Pred( 是 )]2 [A1]3> for Chinese predicate “ 是 ”. However, in Figure 1(a), the corresponding target-side-like PAS is <[X1"
P13-1111,W11-1003,0,0.0238699,"Missing"
P13-1111,I11-1004,0,0.127938,"Missing"
P13-1111,P06-1066,0,0.0351635,"lgorithm to decode the entire sentence. In the algorithm, they organized the space of translation candidates into a hypergraph. For the span covered by PAS (PAS span), a multiplebranch hyperedge is employed to connect it to the PAS’s elements. For the span not covered by PAS (non-PAS span), the decoder considers all the possible binary segmentations of it and utilizes binary hyperedges to link them. 1128 During translation, the decoder fills the spans with translation candidates in a bottom-up manner. For the PAS span, the PAS-based translation framework is adopted. Otherwise, the BTG system (Xiong et al., 2006) is used. When the span covers the whole sentence, we get the final translation result. Obviously, PAS ambiguities are not considered in this framework at all. The targetside-like PAS is selected only according to the language model and translation probabilities, without considering any context information of PAS. Consequently, it would be difficult for the decoder to distinguish the source-side PAS from different context. This harms the translation quality. Thus to overcome this problem, we design two novel methods to cope with the PAS ambiguities: inside-context integration and a maximum ent"
P13-1111,P12-1095,0,0.08467,"first converted into target-side-like PASs by PAS transformation rules, and then perform translation based on the obtained target-side-like PASs. 2.1  Pred means the predicate where the rule is extracted.  SP denotes the list of source elements in source language order.  TP refers to the target-side-like PAS, i.e., a list of general non-terminals in target language order. Sentence Decoding with the PAS-based translation framework Sometimes, the source sentence cannot be fully covered by the PAS, especially when there are several predicates. Thus to translate the whole sentence, Zhai et al. (2012) further designed an algorithm to decode the entire sentence. In the algorithm, they organized the space of translation candidates into a hypergraph. For the span covered by PAS (PAS span), a multiplebranch hyperedge is employed to connect it to the PAS’s elements. For the span not covered by PAS (non-PAS span), the decoder considers all the possible binary segmentations of it and utilizes binary hyperedges to link them. 1128 During translation, the decoder fills the spans with translation candidates in a bottom-up manner. For the PAS span, the PAS-based translation framework is adopted. Other"
P13-1111,J08-2004,0,0.0576515,"Missing"
P13-1111,C12-1185,1,0.523176,"Missing"
P13-1111,C10-1153,1,0.937944,"the PAS in Figure 3; (b) The extracted IC-PASTR from (a). Using the IC-PASs, we look for the aligned target span for each element of the IC-PAS. We demand that every element and its corresponding target span must be consistent with word alignment. Otherwise, we discard the IC-PAS. Afterwards, we can easily extract a rule for PAS transformation, which we call IC-PASTR. As an example, Figure 4(b) is the extracted IC-PASTR from Figure 4(a). Note that we only apply the source-side PAS and word alignment for IC-PASTR extraction. By contrast, Zhai et al. (2012) utilized the result of bilingual SRL (Zhuang and Zong, 2010b). Generally, bilingual SRL could give a better alignment between bilingual elements. However, bilingual SRL usually achieves a really low recall on PASs, about 226,968 entries in our training set while it is 882,702 by using monolingual SRL system. Thus to get a high recall for PASs, we only utilize word alignment instead of capturing the relation between bilingual elements. In addition, to guarantee the accuracy of ICPASTRs, we only retain rules with more than 5 occurrences. 4 Maximum Entropy PAS Disambiguation (MEPD) Model In order to handle the role ambiguities, in this section, we concen"
P13-1111,D10-1030,1,0.906277,"the PAS in Figure 3; (b) The extracted IC-PASTR from (a). Using the IC-PASs, we look for the aligned target span for each element of the IC-PAS. We demand that every element and its corresponding target span must be consistent with word alignment. Otherwise, we discard the IC-PAS. Afterwards, we can easily extract a rule for PAS transformation, which we call IC-PASTR. As an example, Figure 4(b) is the extracted IC-PASTR from Figure 4(a). Note that we only apply the source-side PAS and word alignment for IC-PASTR extraction. By contrast, Zhai et al. (2012) utilized the result of bilingual SRL (Zhuang and Zong, 2010b). Generally, bilingual SRL could give a better alignment between bilingual elements. However, bilingual SRL usually achieves a really low recall on PASs, about 226,968 entries in our training set while it is 882,702 by using monolingual SRL system. Thus to get a high recall for PASs, we only utilize word alignment instead of capturing the relation between bilingual elements. In addition, to guarantee the accuracy of ICPASTRs, we only retain rules with more than 5 occurrences. 4 Maximum Entropy PAS Disambiguation (MEPD) Model In order to handle the role ambiguities, in this section, we concen"
P13-1111,D10-1043,0,\N,Missing
P13-1111,C08-1041,0,\N,Missing
P13-1111,P10-2002,0,\N,Missing
P13-1111,W11-2136,0,\N,Missing
P13-1140,W09-0432,0,0.208778,"2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reordering parameters using monolingual data given the"
P13-1140,2010.iwslt-papers.3,0,0.0609028,"ce sentence with 的 商业 信息, then we compare the position relationship between s and 的 商业 信息. We increment the swap count if s is just before 的 商业 信息. After counting, we finally use maximum likelihood estimation method to compute the reordering probabilities. 6 Related Work As far as we know, few researchers study phrase pair induction from only monolingual data. There are three research works that are most related with ours. One is using an in-domain probabilistic bilingual lexicon to extract subsentential parallel fragments from comparable corpora (Munteanu and Marcu, 2006; Quirk et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Berto"
P13-1140,P10-1146,0,0.0190282,"gual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and"
P13-1140,P11-2071,0,0.263272,"Missing"
P13-1140,D12-1025,0,0.194896,"e usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012) viewed the transla"
P13-1140,J93-1003,0,0.0707835,"8 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the numb"
P13-1140,P98-1069,0,0.0350499,"estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies  s1 , s2 , , sN  and t1 , t2 , , tM  , the source-side phrase s and target-side phrase t can be respectively represented in an N- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: wk  ns , k   log  nmax / nk   1 (3) where ns , k and nk denotes the number of times sk occurs in the context of s and in the entire source language monolingual data, and nmax is the maximum number of occurrence of any source-side word in the source language monolingual data. The k-th element of t’s vector can be computed with the same method. We finally normalize these vectors with L2-norm. With the s’s and t’s contextual vector representations, we calculate two similarities: 1) project s’s vector into target side t with the lexical mapping p(t|s), and then get the similari"
P13-1140,P06-1121,0,0.0425915,"Missing"
P13-1140,P08-1088,0,0.364694,"these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiri"
P13-1140,W06-3601,0,0.135366,"ata on News. Here, we utilize about 2.08 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-"
P13-1140,E12-1014,0,0.460447,"SMT there are four translation probabilities and the reordering probability for each phrase pair. The translation probabilities in the traditional phrase-based SMT include bidirectional phrase translation probabilities and bidirectional lexical weights. For the lexical weights, we can use the plex  s |t , a  and plex t |s, a  computed in the above section without length normalization. However, for the phrase-level probability, we cannot use maximum likelihood estimation since the phrase pairs are not extracted from parallel sentences. 1429 In this paper, we borrow and extend the idea of (Klementiev et al., 2012) to calculate the phraselevel translation probability with context information in source and target monolingual corpus. The value is calculated using a vector space model. With source and target vocabularies  s1 , s2 , , sN  and t1 , t2 , , tM  , the source-side phrase s and target-side phrase t can be respectively represented in an N- and M-dimensional vector. The k-th component of s’s contextual vector is computed using the method of (Fung and Yee, 1998) as follows: wk  ns , k   log  nmax / nk   1 (3) where ns , k and nk denotes the number of times sk occurs in the context of s an"
P13-1140,W04-3250,0,0.0284093,"ortion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives the results. It is obvious that, it is relatively high when using the News training data to evaluate the same News test"
P13-1140,P07-2045,0,0.00688328,"slation rule induction into two steps: bilingual lexicon induction and phrase pair induction. Since many researchers have studied the bilingual lexicon induction, in this paper, we mainly concentrate ourselves on phrase pair induction given a probabilistic bilingual lexicon and two in-domain large monolingual data (source and target language). In addition, we will further introduce how to refine the induced phrase pairs and estimate the parameters of the induced phrase pairs, such as four standard translation features and phrase reordering feature used in the conventional phrase-based models (Koehn et al., 2007). The induced phrase-based model will be used to help domain adaptation for machine translation. In the rest of this paper, we first explain with examples to show what new translation knowledge can be learned with our proposed phrase pair induction method (Section 2), and then we introduce the approach for probabilistic bilingual lexicon acquisition in Section 3. In Section 4 and 5, we respectively present our method for phrase pair induction and introduce an approach for phrase pair refinement and parameter estimation. Section 6 will show the detailed experiments for the task of domain adapta"
P13-1140,W02-0902,0,0.237777,"rmance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-par"
P13-1140,P06-1077,0,0.0769455,"Missing"
P13-1140,W04-3243,0,0.109029,"News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-of-domain data. In the final lexicon, the number of average translations i"
P13-1140,P06-1011,0,0.383114,"ata on News. Here, we utilize about 2.08 million sentence pairs 1 in News domain to learn a probabilistic bilingual lexicon. Basically, we can use GIZA++ (Och, 2003) to get the probabilistic lexicon. However, the problem is that each source-side word associates too many possible translations which contain much noise. For instance, in the lexicon obtained with GIZA++, each source-side word has about 13 translations on average. The noise of the lexicon can influence the accuracy of the induced phrase pairs to a large extent. To learn a lexicon with a high precision, we follow Munteanu and Marcu (2006) to apply Log-Likelihood-Ratios (Dunning, 1993; Melamed, 2000; Moore, 2004a, 2004b) to estimate how strong the association is between a source-side word and its aligned target-side word. We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GIZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. After using the log-likelihood-ratios algorithm2, we obtain a probabilistic bilingual lexicon with bidirectional translation probabilities from the out-"
P13-1140,P12-1017,0,0.0510177,"e to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2"
P13-1140,J03-1002,0,0.00749702,"Missing"
P13-1140,P02-1040,0,0.0958777,"olkit (Stolcke, 2002) to train the 5-gram English language model with the target part of the parallel sentences and the Xinhua portion of the English Gigaword. For the in-domain electronic data, we first consider the lexicon as a phrase table in which we assign a constant 1.0 for each of the four probabilities, and then we combine this initial phrase table and the induced phrase pairs to form the new phrase table. The in-domain reordering table is created for the induced phrase pairs. An in-domain 5gram English language model is trained with the target 1 million monolingual data. We use BLEU (Papineni et al., 2002) score with shortest length penalty as the evaluation metric and apply the pairwise re-sampling approach (Koehn, 2004) to perform the significance test. 7.2 Experimental Results In this section, we first conduct experiments to figure out how the translation performance degrades when the domain changes. To better illustrate the comparison, we first use News data to evaluate the NIST evaluation tests and then use the same News data to evaluate the electronic test sets. For the NIST evaluation, we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives t"
P13-1140,P95-1050,0,0.708182,"translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs betw"
P13-1140,P99-1067,0,0.199578,"Missing"
P13-1140,P11-1002,0,0.0896217,"rpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods for bilingual lexicon induction from non-parallel data (Rapp, 1995, 1999; Koehn and Knight, 2002; Haghighi et al., 2008; Daumé III and Jagarlamudi, 2011) and proposed unsupervised statistical machine translation (bilingual lexicon is a byproduct) with only monolingual corpora (Ravi and Knight, 2011; Nuhn et al., 2012; Dou and Knight, 2012). In the bilingual lexicon induction (Koehn and Knight, 2002; Haghighi et al., 2008; DauméIII and Jagarlamudi, 2011), with the help of the orthographic and context features, researchers adopted an unsupervised method, such as canonical correlation analysis (CCA) model, to automatically induce the word translation pairs between two languages from non-parallel data only requiring that the monolingual data in each language are from a fairly comparable domain. The unsupervised statistical machine translation method (Ravi and Knight, 2011; Nuhn et al., 2012"
P13-1140,2008.iwslt-papers.6,0,0.168941,"k et al., 2007; Cettolo et al., 2010). Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. Compared with their work, our focus is to induce phrase pairs directly from monolingual data rather than comparable data. Thus, finding the candidate parallel sentences is not possible in our situation. Another is to make full use of monolingual data with transductive learning (Ueffing et al., 2007; Schwenk, 2008; Wu et al., 2008; Bertoldi and Federico, 2009). For the target-side monolingual data, they just use it to train language model, and for the source-side monolingual data, they employ a baseline (word-based SMT or phrasebased SMT trained with small-scale bitext) to first translate the source sentences, combining the source sentence and its target translation as a bilingual sentence pair, and then train a new phrase-base SMT with these pseudo sentence pairs. This method cannot learn idiom translations and unknown word translations. The third is to estimate the translation parameters and reorderi"
P13-1140,C08-1125,1,0.219596,"we employ Chinese-to-English NIST MT03 as the tuning set and NIST MT05 as the test set. Table 3 gives the results. It is obvious that, it is relatively high when using the News training data to evaluate the same News test set. However, when the test domain is changed, the translation performance decreases to a large extent. Given the in-domain bilingual lexicon and two monolingual data, previous works also proposed some good methods to explore the potential of the given data to improve the translation quality. Here, we implement their approaches and use them as our strong baseline. Wu et al. (2008) regards the in-domain lexicon with corpus translation probability as another phrase table and further use the in-domain language model besides the out-of-domain language model. Table 4 gives the results. We can see from the table that the domain lexicon is much helpful and significantly outperforms the baseline with more than 4.0 BLEU points. When it is enhanced with the in-domain language model, it can further improve the translation performance by more than 2.5 BLEU points. This method has made good use of in-domain lexicon and the target-side indomain monolingual data, but it does not take"
P13-1140,2011.mtsummit-papers.29,1,0.831128,"y-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in both source and target languages, and proposed methods"
P13-1140,C12-1186,1,0.886692,"Missing"
P13-1140,P08-1064,0,0.0114137,"el from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1 Introduction During the last decade, statistical machine translation has made great progress. Novel translation models, such as phrase-based models (Koehn et a., 2007), hierarchical phrase-based models (Chiang, 2007) and linguistically syntax-based models (Liu et a., 2006; Huang et al., 2006; Galley, 2006; Zhang et al, 2008; Chiang, 2010; Zhang et al., 2011; Zhai et al., 2011, 2012) have been proposed and achieved higher and higher translation performance. However, all of these state-of-the-art translation models rely on the parallel corpora to induce translation rules and estimate the corresponding parameters. It is unfortunate that the parallel corpora are very expensive to collect and are usually not available for resource-poor languages and for many specific domains even in a resource-rich language pair. Recently, more and more researchers concentrated on taking full advantage of the monolingual corpora in b"
P13-1140,C98-1066,0,\N,Missing
P13-1140,P07-1004,0,\N,Missing
P13-1140,D11-1019,1,\N,Missing
P13-1140,J00-2004,0,\N,Missing
P13-1140,J07-2003,0,\N,Missing
P13-1140,2007.mtsummit-papers.50,0,\N,Missing
P13-1140,P04-1066,0,\N,Missing
P13-2066,P09-1075,0,0.0196808,"ecoding, the pseudo codes are given as below. 1: Nodes={[]} 2: Parser(0,End) 3: Parser(s,e): // recursive parser function 4: if s &gt; e or e is -1: return -1; 5: m = GetMaxM(s,e) //compute m through FormuTable 1: 9 features used in our Bayesian model 7: 8: e’ = if m or e’ equals to -1: return -1; Rel=GetRelation(s,m,e’) //compute relation by F 9: 10: 11: 12: 13: 14: 15: push [Rel,s,m,e’] into Nodes Parser(s,m) Parser(m+1,e’) Parser(e’+1,e) Rel=GetRelation(s,e’,e) push [Rel,s,e’,e] into Nodes return e 6: Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. In this model, 9 features in Table 1 are used. In the table, punctuations include comma, semicolons, period and question mark. We view explicit connectives as cue words in this paper. la(1);if no cue words found, then m=-1; GetMaxE(s,m,e) //compute e’ through F (2); (3) 371 For example in Figure 1, for the first iteration, s=0 and m will be chosen from {1-20}. We get m=9 through Formula (1). Then, similar with m, we get e=21 through Formula (2). Finally, the relation is fi"
P13-2066,D11-1084,0,0.118345,"tical machine translation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann an"
P13-2066,N03-1017,0,0.0383944,"procedure of a discourse can be derived from the original decoding formula eI1 = argmaxeI P (eI1 jf1J ) . Given the rhetorical 1 structure of a source sentence and the corresponding rule-table, the translating process is to find an optimal path to get the highest score under structure constrains, which is, Translation Model argmaxes fP (es j; ft )g Y = argmaxes f P (eu1 ; eu2 ; ¿ jfn )g Rule Extraction fn 2ft As shown in Figure 1, the RST tree-to-string alignment provides us with two types of translation rules. One is common phrase-based rules, which are just like those in phrase-based model (Koehn et al., 2003). The other is RST tree-tostring rule, and it’s defined as, where f t is a source RST tree combined by a set of node f n . es is the target string combined by series of en (translations of f n ). f n consists of U1 and U2. eu1 and eu2 are translations of U1 and U2 respectively. This global optimization problem is approximately simplified to local optimization to reduce the complexity, relation ::U1 (®; X)=U2 (°; Y ) ) U1 (tr(®); tr(X)) » U2 (tr(°); tr(Y )) Y where the terminal characters α and γ represent the cue words which are optimum match for maximizing Formula (3). While the nonterminals"
P13-2066,A00-2002,0,0.714324,"ance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework. Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al., 2"
P13-2066,N03-1030,0,0.492052,"-English translation as an example, our translation framework works as the following steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST-tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RSTtree is translated into target sentence with extracted translation rules. Experiments on Chinese-to-English sentencelevel discourses demonstrate that this method achieves significant improvements. 2 Chinese RST Parser 2.1 Annotation of Chinese RST Tree Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U1 and U2 , U1 starts from word position s and stops at word position m. U2 starts from m+1 and ends with e. Under the guidance of definition of RST, Yue (2008) defined 12 groups1 of 1 They are Parallel, Alternative, Condition, Reason, Elaboration, Means, Preparation, Enablement, Antithesis, Background, Evidences, Others. 370 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370–374, c Sofia, Bulgaria, August 4-9 2013. 2013 Association"
P13-2066,D12-1097,0,0.0638346,"cial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a goo"
P13-2066,2011.mtsummit-papers.13,0,0.0673691,"lation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) p"
P13-2066,P09-2035,0,0.19223,"language model(LM) is used for translating edus in Formula(5),(6),(7),(8), but Task Precision Recall F1 not for reordering the upper spans because with Segmentation 0.74 0.83 0.78 the bottom-to-up combination, the spans become Labeling 0.71 0.78 0.75 longer and harder to be judged by a traditional Table 2: Segmentation and labeling result. language model. So we only use RST rules to 5.3 Results of Translation guide the reordering. But LM will be properly considered in our future work. Table 3 presents the translation comparison results. In this table, XD represents the method in 5 Experiment (Xiong et al., 2009). D1 stands for Decoder-1, 5.1 Setup and D2 for Decoder-2. Values with boldface are the highest scores in comparison. D2 performs In order to do Chinese RST parser, we annotated best on the test data with 2.3/0.77/1.43/1.16 over 1,000 complicated sentences on CTB (Xue points. Compared with XD, our results also outet al., 2005), among which 1,107 sentences are perform by 0.52 points on the whole test data. used for training, and 500 sentences are used for Observing and comparing the translation re2 testing. Berkeley parser is used for getting the sults, we find that our translation results are"
P13-2093,H05-1044,0,0.111715,"Missing"
P13-2093,I08-1039,0,0.0832963,"Missing"
P13-2093,C04-1200,0,0.270078,"Missing"
P13-2093,Y09-1033,1,0.888859,"Missing"
P13-2093,C10-1072,1,0.885018,"ing as training data, and the remaining one fold serving as test data. All of the following results are reported in terms of an average of 5-fold cross validation. 4.2 Evaluated Systems We evaluate four machine learning systems that are proposed to address polarity shift in document-level polarity classification: 1) Baseline: standard machine learning methods based on the BOW model, without handling polarity shift; 2) Das-2001: the method proposed by Das and Chen (2001), where “NOT” is attached to the words in the scope of negation as a preprocessing step; 3) Li-2010: the approach proposed by Li et al. (2010). The details of the algorithm is introduced in related work; 4) DTDP: our approach proposed in Section 3. The WordNet dictionary is used for sample reversion. The empirical value of the parameter a and t are used in the evaluation. 4.3 Comparison of the Evaluated Systems In table 1, we report the classification accuracy of four evaluated systems using unigram features. We consider two widely-used classification algorithms: SVM and Naïve Bayes. For SVM, the LibSVM toolkit3 is used with a linear kernel and the default penalty parameter. For Naïve Bayes, the OpenPR-NB toolkit4 is used. 2 http://"
P13-2093,W02-1011,0,0.0228982,"Missing"
P13-2093,P02-1053,0,0.00767782,"Missing"
P13-2110,D12-1046,0,0.577613,"orithm adjusts the Lagrange multiplier values based on the differences between ( ) and ( ) (line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original decoding algorithms, because the Lagrange multiplier can be regarded as adjustments for lexical rule probabilities and word probabilities. 4 Experiments We conduct experiments on the Chinese Treebank Version 5.0 and use the standard data split 625 (Petrov and Klein, 2007). The traditional evaluation metrics for POS tagging and parsing are not suitable for the joint task. Following with Qian and Liu (2012), we redefine precision and recall by computing the span of a constituent based on character offsets rather than word offsets. 4.1 from our word segmentation system as input and “Lattice-based Parser” represents the system taking the compacted word lattice as input. We find the lattice-based parser gets better performance than the pipeline system among all three subtasks. Performance of the Basic Sub-systems We train the word segmentation system with 100 iterations of the Maximum Entropy model using the OpenNLP toolkit. Table 1 shows the performance. It shows that our word segmentation system"
P13-2110,D10-1001,0,0.0231916,"timization problem by optimizing the dual problem. First, we introduce a vector of Lagrange multipliers ( , , ) for each equality constraint. Then, the Lagrangian is formulated as: ( , , )= + ( )+ , , ( ) ( , , )( ( , , ) − ( , , )) ( )− , , (, , ) (, , ) (, , ) (, , ) , , Then, the dual objective is Combined Optimization Between The Lattice-based POS Tagger and The Lattice-based Parser , ( )+ and , ( ) = max ( , , ) = max ( )+ max ( )− , , , , , (, , ) (, , ) + (, , ) (, , ) The dual problem is to find min ( ). We use the subgradient method (Boyd et al., 2003) to minimize the dual. Following Rush et al. (2010), we define the subgradient of ( ) as: ( , , ) = ( , , ) − ( , , ) for all ( , , ) Then, adjust ( , , ) as follows: ( , , ) = ( , , ) − ( ( , , ) − ( , , )) where &gt;0 is a step size. Algorithm 1: Combined Optimization 1: Set ( ) ( , , )=0, for all ( , , ) 2: For k=1 to K ( ) + ∑ , , ( )( , , ) ( , , ) 3: ( ) ← argmax ( ) ( ) − ∑ , , ( )( , , ) ( , , ) 4: ← argmax ( )( ( ) ( , , ) for all ( , , ) 5: If , , )= 6: Return ( ( ) , ( ) ) 7: Else ( ) (, , )= 8: ( )( , , ) − ( ( ) ( , , ) − ( ) ( , , )) Algorithm 1 presents the subgradient method to solve the dual problem. The algorithm initializes the"
P13-2110,P11-1139,0,0.0605215,"Missing"
P13-2110,A00-2018,0,0.323933,"Missing"
P13-2110,O03-4002,1,0.833333,"Missing"
P13-2110,W02-1001,0,0.0115278,"one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for al"
P13-2110,D10-1082,0,0.157435,"Missing"
P13-2110,P11-2124,0,0.0248862,"o each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calcula"
P13-2110,C08-1049,0,0.135876,"compact the N-best lists into a word lattice by collapsing all the identical words into one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a"
P13-2110,P09-1058,0,0.0593855,"Missing"
P13-2110,P05-1010,0,0.0149397,"word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : = argmax ∈ ( ) ∙ ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in Jiang et al. (2008) and a similar decoding algorithm. We use the perceptron algorithm (Collins, 2002) for parameter estimation. Goldberg and Elhadad (2011) proposed a lattice-based parser for Heberw based on the PCFG-LA model (Matsuzaki et al., 2005). We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree 624 is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: ( [ ]) = ( [ ] → [ ] [ ]) × × ( ) ( [ ]→ By grouping the terms that depend on we rewrit"
P13-2110,P06-1055,0,0.0363155,"S tagger with 20 iterations of the average perceptron algorithm. Table 2 presents the joint word segmentation and POS tagging performance and shows that our lattice-based POS tagger obtains results that are comparable with state-of-the-art systems. (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Qian and Liu, 2012) (Sun, 2011) Lattice-based POS tagger P 93.28 93.1 93.64 R 94.07 93.96 93.87 F 93.67 93.67 93.53 94.02 93.75 Table 2: POS tagging evaluation. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al., 2006). Table 3 shows the performance, where the “Pipeline Parser” represents the system taking one-best segmentation result Pipeline Parser Lattice-based Parser P 96.97 92.01 80.86 97.73 93.24 81.83 Seg. POS Parse Seg. POS Parse R 98.06 93.04 81.47 97.66 93.18 81.71 F 97.52 92.52 81.17 97.70 93.21 81.77 Table 3: Parsing evaluation. 4.2 Performance of the Framework For the lattice-based framework, we set the maximum iteration in Algorithm 1 as K = 20. The step size is tuned on the development set and empirically set to be 0.8. Table 4 shows the parsing performance on the test set. It shows that the"
P13-2110,N07-1051,0,0.253157,"based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks. 1 Introduction Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure. Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokenization) is a non-trivial research problem by itself. Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing. More importantly, Chinese is a language that lacks the morphological clues that"
P13-2110,J03-4003,0,\N,Missing
P13-2110,I11-1035,0,\N,Missing
P13-2110,P12-1026,0,\N,Missing
P14-1011,D13-1106,0,0.00699973,"ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic mea"
P14-1011,W04-3250,0,0.0534067,"sh translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans"
P14-1011,D13-1054,0,0.266933,"ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the"
P14-1011,D12-1088,0,0.0249865,"Missing"
P14-1011,P13-1078,0,0.0249237,"ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase emb"
P14-1011,P13-2119,0,0.0122932,"resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase e"
P14-1011,2007.mtsummit-papers.22,0,0.123108,"Missing"
P14-1011,D11-1014,0,0.55051,"sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or"
P14-1011,D07-1103,0,0.030171,"Missing"
P14-1011,P13-1045,0,0.574395,"syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully"
P14-1011,D13-1176,0,0.788727,"ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that"
P14-1011,2009.mtsummit-papers.17,0,0.0492238,"Missing"
P14-1011,D13-1140,0,0.00759691,"(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, n"
P14-1011,J97-3002,0,0.185251,"Missing"
P14-1011,P06-1066,0,0.0963857,"Missing"
P14-1011,P13-1017,1,0.245382,"ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilingu"
P14-1011,D12-1089,0,0.0382796,"Missing"
P14-1011,D13-1141,0,0.206555,"didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word 111 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics of its internal words, we propose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core i"
P14-1011,D13-1170,0,\N,Missing
P14-1080,J91-1002,0,0.262906,"an be roughly divided into two groups: 1) research on lexical cohesion, which mainly contributes to the selection of generated target words; 2) efforts to improve the grammatical cohesion, such as disambiguation of references and connectives. In lexical cohesion work, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) built discourse-based models to ensure lexical cohesion or consistency. In (Xiong et al., 2013a), three different features were designed to capture the lexical cohesion for document-level machine translation. (Xiong et al., 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. They generated the target lexical chains based on the source 857 Reference 过去三年中，已有三对染色体完成排序， 包括第二十对、第二十一对和第二十二 对 。 In the past three years, the sequencing of three chromosomes has been completed, including chromosomes 20 , 21 , and 22 . In the past three years , now has three terms of the completion of the chromosomes , 20 , 21 and 22 . In the past three years , there are three chromosomes to accomplish , including 20 , 21 and 22 . 上述主张构成了一个中国原则的基本涵义，核心是维护中国的主权和领土完整。 The above-mentioned propositions constitute the basic connotation of this one-china principle with s"
P14-1080,N03-2002,0,0.0411018,"Missing"
P14-1080,W10-1737,0,0.0446851,"ee-tostring translation method for complex sentences with explicit relations inspired by (Marcu et al., 2000), but their models worked only for explicit functional relations, and they were concerned mainly with the translation integrity of semantic span rather than cohesion. (Meyer and PopescuBelis, 2012) used sense-labeled discourse connectives for machine translation from English to French. They added the labels assigned to connectives as an additional input to an SMT system, but their experimental results show that the improvements under the evaluation metric of BLEU were not significant. (Nagard and Koehn, 2010) addresses the problems of reference or anaphora resolution inspired by work of Mitkov et al. (1995). To the best of our knowledge, our work is the first attempt to exploit the source functional relationship to generate the target transitional expressions for grammatical cohesion, and we have successfully incorporated the proposed models into an SMT system with significant improvement of BLEU metrics. 6 Conclusion In this paper, we focus on capturing cohesion information to enhance the grammatical cohesion of machine translation. By taking the source CSS into consideration, we build bridges to"
P14-1080,P05-1033,0,0.133288,"anation that is ,; the first is; first is the;… adversative however , the ; but it is; … this is a; which is an; … flowing consequence so that the; to ensure that… causal Table 2. Chinese functional relations and their corresponding English left-frontier phrases learned by our transfer model. The noun phrases starting with a definite / indefinite word are filtered because they are unlikely to be the transitional phrases. 4.3 Results on SMT with Different Strategies For this work, we use an in-house decoder to build the SMT baseline; it combines the hierarchical phrase-based translation model (Chiang, 2005; Chiang, 2007) with the BTG (Wu, 1996) reordering model (Xiong et al., 2006; Zens and Ney, 2006; He et al., 2010). To test the effectiveness of the proposed models, we have compared the translation quality of different integration strategies. First, we adopted only the tagged-flattened rules in the hierarchical translation system. Next, we added the log probability generated by the transfer model as a feature into the baseline features. The baseline features include bi-directional phrase translation probabilities, bi-directional lexical translation Baseline +Flattened Rule +TFS (without EUC)"
P14-1080,P02-1038,0,0.0155377,"ildren’s positions always play a strong role in choosing cohesive expressions because transitional expressions vary for children with different positions. For example, when translating the last child of a parallel relation, we always use word “and” as the transitional expression seen in Figure 3, but we will not use it for the first child of a parallel relation. Therefore, in the training process we just keep the information of relationships and children’s positions when converting and the final translation e s is obtained with the following formula: e s  arg max{P(es |ft )} (1) eS Following Och and Ney (2002), our model is framed as a log-linear model: P (e s |f t )  exp k k hk (es , ft ) e exp k k hk (e's , ft ) (2) s where h(es , ft ) is a feature with weight  . Then, the best translation is: e s  arg max exp k k hk (es , ft ) (3) es Our models make use of CSS with two strategies: 1) CSS-based translation model: following formula (1), we obtain the cohesion information by modifying the translation rules with their probabilities P(es |ft ) based on word alignments between the source CSS-tree and the target string; 2) CSS-based transfer model: following formula (3), we introduce a trans"
P14-1080,P02-1040,0,0.0913182,"SaveCurrentPath(Path[m]); /*add current index to Path*/ 8: }//end m 9:}//end n 10: OptimalPath = arg max{Score[ N ][m]} ; is modified by our dynamic cleaning method. During the cleaning process, the maximum size of hypothesis is limited to 5. A 5-gram language model is trained with SRILM5 on the combination of the Xinhua portion of the English Gigaword corpus combined with the English part of FBIS. For tuning and testing, we use NIST03 evaluation data as the development set. NIST04/05/06, CWMT08-Development 6 and CWMT08-Evaluation data are used for testing under the measure metric of BLEU-4 (Papineni et al. 2002) with the shortest length penalty. Table 1 shows how the CSS is distributed in all testing sets. According to the statistics in Table 1, we see that CSS is really widely distributed in the NIST and CWMT corpora, which implies that the translation quality may benefit substantially from the CSS information, if it is well considered in SMT. NIST04 NIST05 NIST06 CWMT08-Dev. CWMT08-Eval. Path [ m ] Figure 5. The pseudo code of dynamic cleaning method. 4 Table 1. The numbers of sentences and the CSS ratios of all sentences. CWMT08-Dev. is short for CWMT08 Development data and CWMT08-Eval. is CWMT08"
P14-1080,D11-1084,0,0.22196,"Missing"
P14-1080,prasad-etal-2008-penn,0,0.042289,"Missing"
P14-1080,W13-3302,0,0.0995508,"Missing"
P14-1080,D10-1054,0,0.0140345,"n; … flowing consequence so that the; to ensure that… causal Table 2. Chinese functional relations and their corresponding English left-frontier phrases learned by our transfer model. The noun phrases starting with a definite / indefinite word are filtered because they are unlikely to be the transitional phrases. 4.3 Results on SMT with Different Strategies For this work, we use an in-house decoder to build the SMT baseline; it combines the hierarchical phrase-based translation model (Chiang, 2005; Chiang, 2007) with the BTG (Wu, 1996) reordering model (Xiong et al., 2006; Zens and Ney, 2006; He et al., 2010). To test the effectiveness of the proposed models, we have compared the translation quality of different integration strategies. First, we adopted only the tagged-flattened rules in the hierarchical translation system. Next, we added the log probability generated by the transfer model as a feature into the baseline features. The baseline features include bi-directional phrase translation probabilities, bi-directional lexical translation Baseline +Flattened Rule +TFS (without EUC) +TFS +TFS+ Flattened Rule probabilities, the BTG re-ordering features, and the language model feature. The tri-gra"
P14-1080,D12-1106,0,0.0307863,"Missing"
P14-1080,1995.tmi-1.6,0,0.429962,"2000), but their models worked only for explicit functional relations, and they were concerned mainly with the translation integrity of semantic span rather than cohesion. (Meyer and PopescuBelis, 2012) used sense-labeled discourse connectives for machine translation from English to French. They added the labels assigned to connectives as an additional input to an SMT system, but their experimental results show that the improvements under the evaluation metric of BLEU were not significant. (Nagard and Koehn, 2010) addresses the problems of reference or anaphora resolution inspired by work of Mitkov et al. (1995). To the best of our knowledge, our work is the first attempt to exploit the source functional relationship to generate the target transitional expressions for grammatical cohesion, and we have successfully incorporated the proposed models into an SMT system with significant improvement of BLEU metrics. 6 Conclusion In this paper, we focus on capturing cohesion information to enhance the grammatical cohesion of machine translation. By taking the source CSS into consideration, we build bridges to connect the source functional relationships in CSS to target transitional expressions; such a proce"
P14-1080,W12-0117,0,0.329901,"Missing"
P14-1080,N03-1030,0,0.132691,"Missing"
P14-1080,P13-2066,1,0.812328,"Missing"
P14-1080,P12-1033,0,0.0429289,"Missing"
P14-1080,2011.mtsummit-papers.13,0,0.476391,"al expressions by introducing the source compoundcomplex sentence structure (CSS). Our models include a CSS-based translation model, which generates new CSS-based translation rules, and a generative transfer model, which encourages producing transitional expressions during decoding. The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness. The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth. 1 the lexical cohesion of SMT (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012; Xiong, 2013). These efforts focus mainly on the cooccurrence of lexical items in a similar environment. Grammatical cohesion1 (Halliday and Hassan, 1976) in SMT has been little mentioned in previous work. Translations without grammatical cohesion is hard to read, mostly due to loss of cohesive and transitional expressions between two sentence fragments. Thus, generating transitional expressions is necessary for achieving grammatical cohesion. However, it is not easy to produce such transitional expressions in SMT. As an example, consider the Chinese-to-English translation"
P14-1080,P06-1066,0,0.038264,"the ; but it is; … this is a; which is an; … flowing consequence so that the; to ensure that… causal Table 2. Chinese functional relations and their corresponding English left-frontier phrases learned by our transfer model. The noun phrases starting with a definite / indefinite word are filtered because they are unlikely to be the transitional phrases. 4.3 Results on SMT with Different Strategies For this work, we use an in-house decoder to build the SMT baseline; it combines the hierarchical phrase-based translation model (Chiang, 2005; Chiang, 2007) with the BTG (Wu, 1996) reordering model (Xiong et al., 2006; Zens and Ney, 2006; He et al., 2010). To test the effectiveness of the proposed models, we have compared the translation quality of different integration strategies. First, we adopted only the tagged-flattened rules in the hierarchical translation system. Next, we added the log probability generated by the transfer model as a feature into the baseline features. The baseline features include bi-directional phrase translation probabilities, bi-directional lexical translation Baseline +Flattened Rule +TFS (without EUC) +TFS +TFS+ Flattened Rule probabilities, the BTG re-ordering features, and t"
P14-1080,D13-1163,0,0.171911,"05T06, LDC2002L27, LDC2005T10 and LDC2005T34. 5 Related Work Improving cohesion for complex sentences or discourse translation has attracted much attention in recent years. Such research efforts can be roughly divided into two groups: 1) research on lexical cohesion, which mainly contributes to the selection of generated target words; 2) efforts to improve the grammatical cohesion, such as disambiguation of references and connectives. In lexical cohesion work, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) built discourse-based models to ensure lexical cohesion or consistency. In (Xiong et al., 2013a), three different features were designed to capture the lexical cohesion for document-level machine translation. (Xiong et al., 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. They generated the target lexical chains based on the source 857 Reference 过去三年中，已有三对染色体完成排序， 包括第二十对、第二十一对和第二十二 对 。 In the past three years, the sequencing of three chromosomes has been completed, including chromosomes 20 , 21 , and 22 . In the past three years , now has three terms of the completion of the chromosomes , 20 , 21 and 22 . In the past three years , there"
P14-1080,W06-3108,0,0.0319642,"his is a; which is an; … flowing consequence so that the; to ensure that… causal Table 2. Chinese functional relations and their corresponding English left-frontier phrases learned by our transfer model. The noun phrases starting with a definite / indefinite word are filtered because they are unlikely to be the transitional phrases. 4.3 Results on SMT with Different Strategies For this work, we use an in-house decoder to build the SMT baseline; it combines the hierarchical phrase-based translation model (Chiang, 2005; Chiang, 2007) with the BTG (Wu, 1996) reordering model (Xiong et al., 2006; Zens and Ney, 2006; He et al., 2010). To test the effectiveness of the proposed models, we have compared the translation quality of different integration strategies. First, we adopted only the tagged-flattened rules in the hierarchical translation system. Next, we added the log probability generated by the transfer model as a feature into the baseline features. The baseline features include bi-directional phrase translation probabilities, bi-directional lexical translation Baseline +Flattened Rule +TFS (without EUC) +TFS +TFS+ Flattened Rule probabilities, the BTG re-ordering features, and the language model fe"
P14-1080,J07-2003,0,\N,Missing
P14-2126,J07-2003,0,0.0480851,"ction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is m"
P14-2126,D11-1014,0,0.509728,"s a better translation. However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines it with [ÙŸ;Bush] incorrectly, leading to a bad translation. To explore the derivation structure’s potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding. • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 779 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779–784, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 The DSP Model The basic idea of DSP model is to represent the deriva"
P14-2126,P06-1121,0,0.0266993,"ms to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translatio"
P14-2126,P13-1045,0,0.563181,". However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines it with [ÙŸ;Bush] incorrectly, leading to a bad translation. To explore the derivation structure’s potential on yielding good translations, in this paper, we propose a novel derivation structure prediction (DSP) model for SMT decoding. • We propose a novel RNN-based model to do derivation structure prediction for SMT decoding. To our best knowledge, this is the first work on this issue in SMT community; • In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al., 2011a; Socher et al., 2013). We go a step further, and design a bilingual RNN to represent the derivation structure; • To train the RNN-based DSP model, we propose a max-margin objective that prefers gold derivations yielded by forced decoding to n-best derivations generated by the conventional BTG translation model. 779 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 779–784, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 The DSP Model The basic idea of DSP model is to represent the derivation structure by RNN ("
P14-2126,J97-3002,0,0.772466,"representations for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chine"
P14-2126,W06-3601,0,0.0284407,"Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “† â9” to “and S"
P14-2126,P06-1066,0,0.265425,"tions for phrase pairs; (2) derivation structure prediction, to build a bilingual RNN that aims to distinguish good derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9"
P14-2126,D13-1054,0,0.016505,"the DSP score for sentence ui ’s derivation d. It is computed by summing LNN score (Equation (2)) and DSN score (Equation (5)): s(θ, u, d) = LN Nθ (d) + DSNθ (d) p (7) ˆ G(ui )) is the structure loss margin, which ∆(d, penalizes derivation dˆ more if it deviates more from gold derivations. It is formulated as:  ˆ G(ui ) ∆ d, X ˆ ref ) (8) = αs δ{π 6∈ G(ui )} + αt Dist(y(d), where d denotes the derivation structure and p is the non-leaf node in d. Obviously, by this score, we can easily assess different derivations. Good derivations will get higher scores while bad ones will get lower scores. Li et al. (2013) presented a network to predict how to merge translation candidates, in monotone or inverted order. Our DSN differs from Li’s work in two points. For one thing, DSN can not only predict how to merge candidates, but also evaluate whether two candidates should be merged. For another, DSN focuses on the entire derivation structure, rather than only the two candidates for merging. Therefore, the translation decoder will pursue good derivation structures via DSN. Actually, Li’s work can be easily integrated into our work. We leave it as our future work. 3 Max-Margin Framework π∈dˆ The margin includ"
P14-2126,D13-1112,0,0.0317023,"Missing"
P14-2126,P06-1077,0,0.0375188,"res from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phr"
P14-2126,W06-1606,0,0.0399372,"d derivation structures from bad ones. Extensive experiments show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wron"
P14-2126,Y09-2016,1,0.832998,"language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: ∂J 1 X ∂s(θ, ui , dˆm ) ∂s(θ, ui , dm ) = − +λθ ∂θ N ∂θ ∂θ 3.3 Experiments (1) Using the BTG system to perform force decoding on FBIS part of the bilingual training data5 , and collect the sentences succeeded in force decoding (86,902 sentences in total)6 . We then collect the corresponding force de"
P14-2126,P02-1040,0,0.0905293,"del, we perform experiments on Chinese-to-English translation. The training data contains about 2.1M sentence pairs with about 27.7M Chinese words and 31.9M English words2 . We train a 5-gram language model by the Xinhua portion of Gigaword corpus and the English part of the training data. We obtain word alignment by GIZA++, and adopt the grow-diag-final-and strategy to generate the symmetric alignment. We use NIST MT 2003 data as the development set, and NIST MT04-083 as the test set. We use MERT (Och, 2004) to tune parameters. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002). The statistical significance test is performed by the re-sampling approach (Koehn, 2004). The baseline system is our in-house BTG system (Wu, 1997; Xiong et al., 2006; Zhang and Zong, 2009). To train the DSP model, we first use Word2Vec4 toolkit to pre-train the word embedding on largescale monolingual data. The used monolingual data contains about 1.06B words for Chinese and 1.12B words for English. The dimensionality of our vectors is 50. The detiled training process is as follows: ∂J 1 X ∂s(θ, ui , dˆm ) ∂s(θ, ui , dm ) = − +λθ ∂θ N ∂θ ∂θ 3.3 Experiments (1) Using the BTG system to perfor"
P14-2126,P08-1064,0,0.0219994,"ts show that the proposed DSP model significantly improves the translation quality, and thus verify the effectiveness of derivation structure on indicating good translations. We make the following contributions in this work: Introduction Derivation structure is important for SMT decoding, especially for the translation model based on nested structures of languages, such as BTG (bracket transduction grammar) model (Wu, 1997; Xiong et al., 2006), hierarchical phrase-based model (Chiang, 2007), and syntax-based model (Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008; Zhang et al., 2011; Zhai et al., 2013). In general, derivation structure refers to the tuple that records the used translation rules and their compositions during decoding, just as Figure 1 shows. Intuitively, a good derivation structure usually yields a good translation, while bad derivations always result in bad translations. For example in Figure 1, (a) and (b) are two different derivations for Chinese sentence “ÙŸ † â9 Þ1 ¬ !”. Comparing the two derivations, (a) is more reasonable and yields a better translation. However, (b) wrongly translates phrase “† â9” to “and Sharon” and combines"
P14-2126,D11-1019,1,0.886265,"Missing"
P14-2126,Q13-1020,1,\N,Missing
P17-2060,C08-1005,0,0.0702694,"Missing"
P17-2060,D10-1092,0,0.00709113,"h are often able to translate rare words in training data. As shown in Table 2, the number of unknown words of our proposed model is 137 fewer than original NMT model. Table 4 shows an example of system combination. The Chinese word zˇuzh¯ıwˇang is an out-ofvocabulary(OOV) for NMT and the baseline NMT cannot correctly translate this word. Although PBMT and HPMT translate this word well, they does not conform to the grammar. By combining the merits of NMT and SMT, our model gets the correct translation. Word Order of Translation We evaluate word order by the automatic evaluation metrics RIBES (Isozaki et al., 2010), whose score is a metric based on rank correlation coefficients with word precision. RIBES is known to have stronger correlation with human judgements than BLEU for English as discussed in Isozaki et al. (2010). 4.6 Effect of Ensemble Decoding The performance of candidate systems is very important to the result of system combination, and we use ensemble strategy with four NMT models to improve the performance of original NMT system. As shown in Table 3, the E-NMT with 2 We use four neural combination models in ensemble model. 381 Source Pinyin Reference PBMT HPMT NMT Jane Multi °é • † ™Ý |„ ï"
P17-2060,W16-2316,0,0.157174,"ion framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combination method, which is adapted from multi-source Corresponding author. 378 Proc"
P17-2060,P09-1106,0,0.0248177,"eijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural s"
P17-2060,D13-1176,0,0.0485095,"antages of both NMT and SMT. In this paper, we propose a neural system combination framework leveraging multi-source NMT, which takes as input the outputs of NMT and SMT systems and produces the final translation. Extensive experiments on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combinat"
P17-2060,P16-1185,0,0.0565554,"ovement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et"
P17-2060,P05-1033,0,0.119106,"ents on the Chineseto-English translation task show that our model archives significant improvement by 5.3 BLEU points over the best single system output and 3.4 BLEU points over the state-of-the-art traditional system combination methods. 1 Introduction Neural machine translation has significantly improved the quality of machine translation in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Junczys-Dowmunt et al., 2016a). Although most sentences are more fluent than translations by statistical machine translation (SMT) (Koehn et al., 2003; Chiang, 2005), NMT has a problem to address translation adequacy especially for the rare and unknown words. Additionally, it suffers from over-translation and under-translation to some extent (Tu et al., 2016). Compared to NMT, SMT, such as phrase-based machine translation (PBMT, (Koehn et al., 2003)) and hierarchical phrase-based machine translation (HPMT, ∗ • We propose a neural system combination method, which is adapted from multi-source Corresponding author. 378 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 378–384 c Vancouver, Canada, Ju"
P17-2060,N04-1022,0,0.0888887,"aboratory of Pattern Recognition, CASIA, Beijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation result"
P17-2060,2009.iwslt-evaluation.13,1,0.80314,", 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT an"
P17-2060,D09-1115,0,0.0263568,"cognition, CASIA, Beijing, China ‡ CAS Center for Excellence in Brain Science and Intelligence Technology, Shanghai, China {long.zhou,wenpeng.hu,jjzhang,cqzong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, w"
P17-2060,N16-1101,0,0.0204172,"ns and terrorist group . hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is"
P17-2060,D15-1166,0,0.0931622,"Missing"
P17-2060,D16-1026,0,0.0325604,"SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT and the adequacy of SMT, but also can accommodate the source sentences as input. Furthermore, our approach can further use ensemble decoding to boost the performance compared to traditional system combination methods. Experiments on Chinese-English datasets show that our approaches obtain signific"
P17-2060,P15-1002,0,0.0491635,"l NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past s"
P17-2060,E14-2008,0,0.477432,"005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed to model the semantics of the source langua"
P17-2060,D15-1122,0,0.0181203,"gual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method cannot only address the fluency of NMT and the adequacy of SMT, but also can accommodate the source sentences a"
P17-2060,C16-1133,0,0.0349222,". hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for sys"
P17-2060,D07-1105,0,0.0863732,"Missing"
P17-2060,C16-1205,0,0.0257155,"est sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level"
P17-2060,D16-1096,0,0.0780792,"BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word"
P17-2060,N16-1004,0,0.0729918,"r decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed to model the semantics of the source language input and each best translation produced by different NMT and SMT systems. The encoders produce multiple context vector representations, from which the decoder generates the final output word by word. Since the same training data is used for NMT, SMT and neural system combination, we further design a smart strategy to simulate the real training data for neural system combination. Specifically, we make the following contributions in this paper: Neural machine translation (NMT) becomes a new approach to machine trans"
P17-2060,D16-1249,0,0.0332343,"BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word"
P17-2060,C16-1172,0,0.121712,"Missing"
P17-2060,2001.mtsummit-papers.46,0,0.2154,"rks . hussein also has established relations and terrorist group . hussein also and terrorist group established relations . hussein also established relations with UNK . hussein also has established relations with . hussein also has established relations with the terrorist group . Table 4: Translation examples of single system, Jane and our proposed model. based SMT to pre-translate the inputs into target translations. Then a NMT system generates the final hypothesis using the pre-translation. Moreover, multi-source MT has been proved to be very effective to combine multiple source languages (Och and Ney, 2001; Zoph and Knight, 2016; Firat et al., 2016a,b; Garmash and Monz, 2016). Unlike previous works, we adapt multi-source NMT for system combination and design a good strategy to simulate the real training data for our neural system combination. ensemble strategy outperforms the original NMT system by +1.40 BLEU points, and it has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments furt"
P17-2060,P02-1040,0,0.12722,"other half into target translations. The MT translations and the gold target reference can be available. (3) where sj−1 is previous hidden state, s˜j−1 is an intermediate state. And cj is the context vector of system combination obtained by attention mechanism, which is computed as weighted sum of the context vectors of three MT systems, just as illustrated in the middle part of Figure 1. cj = K X βjk cjk 4 We perform our experiments on the ChineseEnglish translation task. The MT systems participating in system combination are PBMT, HPMT and NMT. The evaluation metric is caseinsensitive BLEU (Papineni et al., 2002). (4) k=1 where K is the number of MT systems, and βjk is a normalized item calculated as follows: βjk exp(˜ sj−1 · cjk ) =P sj−1 · cjk0 ) k0 exp(˜ 4.1 m X k αji hi Data preparation Our training data consists of 2.08M sentence pairs extracted from LDC corpus. We use NIST 2003 Chinese-English dataset as the validation set, NIST 2004-2006 datasets as test sets. We list all the translation methods as follows: (5) Here, we calculate kth MT system context cjk as a weighted sum of the source annotations: cjk = Experiments • PBMT: It is the start-of-the-art phrase-based SMT system. We use its default"
P17-2060,P07-1040,0,0.0412155,"zong}@nlpr.ia.ac.cn Abstract (Chiang, 2005)), does not need to limit the vocabulary and can guarantee translation coverage of source sentences. It is obvious that NMT and SMT have different strength and weakness. In order to take full advantages of both NMT and SMT, system combination can be a good choice. Traditionally, system combination has been explored respectively in sentence-level, phrase-level, and word-level (Kumar and Byrne, 2004; Feng et al., 2009; Chen et al., 2009). Among them, word-level combination approaches that adopt confusion network for decoding have been quite successful (Rosti et al., 2007; Ayan et al., 2008; Freitag et al., 2014). However, these approaches are mainly designed for SMT without considering the features of NMT results. NMT opts to produce diverse words and free word order, which are quite different from SMT. And this will make it hard to construct a consistent confusion network. Furthermore, traditional system combination approaches cannot guarantee the fluency of the final translation results. In this paper, we propose a neural system combination framework, which is adapted from the multi-source NMT model (Zoph and Knight, 2016). Different encoders are employed t"
P17-2060,W08-0329,0,0.0259165,"2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neural system combination model which combines the advantages of NMT and SMT efficiently. Recently, Niehues et al. (2016) use phraseConclusion and Future Work In this paper, we propose a novel neural system combination framework for machine translation. The central idea is to take advantage of NMT and SMT by adapting the multi-source NMT model. The neural system combination method canno"
P17-2060,P16-1009,0,0.0351505,"ms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and"
P17-2060,P16-1162,0,0.052455,"ms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and"
P17-2060,P16-1159,0,0.0515303,"ffective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et al., 2009; Heafield and Lavie, 2010; Freitag et al., 2014; Ma and Mckeown, 2015; Zhu et al., 2016), and reported stateof-the-art performances in benchmarks for SMT. Here, we propose a neu"
P17-2060,P16-1008,0,0.0223345,"has become the best sytem in all MT systems, which is +0.68 BLEU points higher than HPMT. After replacing original NMT with strong ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level"
P17-2060,D16-1160,1,0.248013,"g ENMT , Jane outperforms original result by +0.45 BLEU points, and our model gets an improvement of +3.08 BLEU points over Jane. Experiments further demonstrate that our proposed model is effective and robust for system combination. 5 Related Work 6 The recently proposed neural machine translation has drawn more and more attention. Most of the existing approaches and models mainly focus on designing better attention models (Luong et al., 2015a; Mi et al., 2016a,b; Tu et al., 2016; Meng et al., 2016), better strategies for handling rare and unknown words (Luong et al., 2015b; Li et al., 2016; Zhang and Zong, 2016a; Sennrich et al., 2016b) , exploiting large-scale monolingual data (Cheng et al., 2016; Sennrich et al., 2016a; Zhang and Zong, 2016b), and integrating SMT techniques (Shen et al., 2016; Junczys-Dowmunt et al., 2016b; He et al., 2016). Our focus in this work is aiming to take advantage of NMT and SMT by system combination, which attempts to find consensus translations among different machine translation systems. In past several years, word-level, phrase-level and sentence-level system combination methods were well studied (Bangalore et al., 2001; Rosti et al., 2008; Li and Zong, 2008; Li et"
P17-2060,N03-1017,0,\N,Missing
P19-1117,N18-1008,0,0.0128204,"on does not support zero-shot translation because there is no explicit training set for this specific translation task. Therefore, we employ hybrid-attention mechanism in our zero-shot experiments. 3) Language-Sensitive Discriminator: In our method, the representor which shares encoder and decoder makes full use of language commonality, but it weakens the model ability to distinguish different languages. Hence we introduce a new language-sensitive discriminator to strengthen model representation. In NMT framework, the hidden states on the top layer can be viewed as a fine-grained abstraction (Anastasopoulos and Chiang, 2018). For this language-sensitive module, we first employ a neurep ral model fdis on the top layer of reprensentor htop , and the output of this model is a language judgment score Plang . rep hdis = fdis (htop ) Plang (d) = softmax(Wdis ∗ hdis d + bdis ) (8) where Plang (d) is language judgment score for sentence pair d, Wdis , bdis are parameters, which are denoted as θ dis . We test two different types of neural models for fdis , including convolutional network with max pooling layer and two-layer feedforward network. And then, we obtain an discriminant objective function as follows: Ldis (θ dis"
P19-1117,C18-1263,0,0.0486432,"→It translation task. 6 Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmentin"
P19-1117,P17-1176,0,0.0268642,"2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of multilinguality in Multi-NMT to address the zero-shot problem. In this work, we propose a method for Multi-NMT to boost the accuracy of the multilingua"
P19-1117,P15-1166,0,0.637577,"ngual transr to en res Rep lation from M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decode"
P19-1117,N16-1101,0,0.210155,"M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usual"
P19-1117,D15-1166,0,0.0692789,"14; Cho et al., 2014) framework and self-attentionbased Transformer (Vaswani et al., 2017). Encoder-Decoder Framework Transformer Network Transformer is a stacked network with several layers containing two or three basic blocks in each layer. For a single layer in the encoder, it consists of a multi-head self-attention and a position-wise 1214 lang-1 feed-forward network. For the decoder model, besides the above two basic blocks, a multi-head cross-attention follows multi-head self-attention. In this block, the calculation method of similarity score et in Equation 3 is a little different from Luong et al. (2015) and Bahdanau et al. (2015): ei,t = √ 1 Wk hienc ∗ Wq htdec dm Residual&Norm Position Embdding Embeddings src lang-1 tgt lang-n + In Multi-NMT model, the encoder and decoder are two key components, which play analogous ... + A Compact Representor lang-2 Language-Sensitive Embedding L |D M Õ Õl |Õ l log P(ytl |x l, y<t ; θ rep, θ attn ) (7) This representor (θ rep ) coordinates the semantic presentation of multiple languages in a closely related universal level, which also increases the utilization of commonality for different languages. 3.2 3.1 lang-1 l=1 d=1 t=1 (6) In this section, we introd"
P19-1117,D18-1103,0,0.0244931,"any researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of mu"
P19-1117,N18-1032,0,0.0872909,"ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared"
P19-1117,D13-1176,0,0.0505833,"lation scenarios. 1 N Encoder M Src Decoder Lang-Sensi Attention N Tgt Lang-Sensi Embedding Ds n im rcr o at Figure 1: Our proposed compact representor, replacing N encoder and decoder, can perform multilingual transr to en res Rep lation from M source languages to N target languages. ginShgar-ns n WeiSeLa also introduce three specific modules consisting of t rco iat nt En o eD d cn language-sensitive embedding, language-sensitive attention, and language-sensitive discriminator. t Tg N cM Sr g-S n iLa ens nant iEm d ged nb Introduction Encoder-decoder based sequence-to-sequence architecture (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Zhang and Zong, 2015; Vaswani et al., 2017; Gehring et al., 2017) facilitates the development of multilingual neural machine translation (Multi-NMT) (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The domi∗ Jiajun Sharing Representor Zhang is the corresponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 201"
P19-1117,D16-1139,0,0.0144293,"on Nl-Ro and It-Nl with all sentence pairs in IWSLT-17 (about 200k), which is similar to other training pairs in our balanced corpus. As shown in part II, Multi-NMT Baselines underperform the NMT Baselines on all cases. However, our method performs better than NMT Baselines, and it achieves the improvement up to 1.76 BLEU points on Nl→It translation task. 6 Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target language"
P19-1117,C18-1054,0,0.0650449,"Related Work Our work is related to two lines of research, and we describe each of them as follows: Model Compactness and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2"
P19-1117,W18-6309,0,0.053982,"ponding author and the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association f"
P19-1117,P02-1040,0,0.111001,"batch contains roughly 3,072 source and 3,072 target tokens, which belongs to one translation direction. We use Adam optimizer (Kingma and Ba, 2014) with β1 =0.9, β2 =0.98, and =10−9 . For evaluation, we use beam search with a beam size of k = 4 and length penalty α = 0.6. All our methods are trained and tested on a single Nvidia P40 GPU. 5 Results and Analysis In this section, we discuss the results of our experiments about our compact and language-sensitive method on Multi-NMT. The translation performance is evaluated by character-level BLEU5 for En→Zh translation and case-sensitive BLEU4 (Papineni et al., 2002) for other translation tasks. In our experiments, the models trained on individual language pair are denoted by NMT Baselines, and the baseline Multi-NMT models are denoted by Multi-NMT Baselines. 5.1 We implement our compact and languagesensitive method for Multi-NMT based on the tensor2tensor7 library. We use wordpiece method (Wu et al., 2016; Schuster and Nakajima, 2012) to 1 http://www.statmt.org/wmt14/translation-task.html WMT Language pair En-De En-Lv En-Fi En-Zh En-It En-Ro En-Nl Ro-It En-Vi 5.1.1 One-to-Many Translation Main Results The main results on the one-to-many translation scena"
P19-1117,D18-1039,0,0.176192,"d the work is done while Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguis"
P19-1117,E17-2025,0,0.0333905,"normalization (Ba et al., 2016). Since the Transformer network contains no recurrence, positional embeddings are used in the model to make use of sequence order. More details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared"
P19-1117,K16-1029,0,0.0437771,"Missing"
P19-1117,P16-1009,0,0.259203,"e details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared input embeddings for all languages) and three-way weight tying manner (TWWT) proposed in Press and Wolf (2017), in which the output embedding of the target side is a"
P19-1117,P16-1162,0,0.491605,"e details regarding the architecture can be found in Vaswani et al. (2017). 2.3 lang-n lang-2 ... Language-Sensitive Modules The compact representor maximizes the sharing of parameters and makes full use of language commonality. However, it lacks the ability to discriminate different languages. In our method, we introduce three language-sensitive modules to enhance our model as follows: 1) Language-Sensitive Embedding: Previously, Press and Wolf (2017) conduct the weight tying of input and output embedding in NMT model. Generally, a shared vocabulary is built 1215 upon subword units like BPE (Sennrich et al., 2016b) and wordpiece (Wu et al., 2016; Schuster and Nakajima, 2012). However, it remains under-exploited which kind of embedding sharing is best for Multi-NMT. We divide the sharing manners into four categories including languagebased manner (LB, different languages have separate input embeddings), direction-based manner (DB, languages in source side and target side have different input embeddings), representorbased manner (RB, shared input embeddings for all languages) and three-way weight tying manner (TWWT) proposed in Press and Wolf (2017), in which the output embedding of the target side is a"
P19-1117,D18-1326,1,0.836692,"Yining Wang is doing research intern at Sogou Inc. paradigm of Multi-NMT contains one encoder to represent multiple languages and one decoder to generate output tokens of separate languages (Johnson et al., 2017; Ha et al., 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics structures but"
P19-1117,1983.tc-1.13,0,0.308962,"Missing"
P19-1117,D16-1160,1,0.881723,"conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) propose to make use of multilinguality in Multi-NMT to address the zero-shot problem. In thi"
P19-1117,J82-2005,0,0.627478,"Missing"
P19-1117,Q19-1006,1,0.813644,"and Multi-NMT: To reduce the model size in NMT, weight pruning, knowledge distillation, quantization, and weight sharing (Kim and Rush, 2016; See et al., 2016; He et al., 2018; Zhou et al., 2018) have been ex1220 plored. Due to the benefit of compactness, multilingual translation has been extensively studied in Dong et al. (2015), Luong et al. (2016) and Johnson et al. (2017). Owing to excellent translation performance and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utiliz"
P19-1117,N16-1004,0,0.0607207,", 2016). This paradigm is widely used in Multi-NMT systems due to simple implementation and convenient deployment. However, this paradigm has two drawbacks. For one hand, using single encoder-decoder framework for all language pairs usually yields inferior performance compared to individually trained single-pair models in most cases (Lu et al., 2018; Platanios et al., 2018; Wang et al., 2018). For the other hand, although this paradigm saves lots of parameters compared to another Multi-NMT framework which employs separate encoders and decoders to handle different languages (Dong et al., 2015; Zoph and Knight, 2016; Luong et al., 2016; Firat et al., 2016), parameter sharing between encoder and decoder are not fully explored. Since both encoder and decoder have similar 1213 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1213–1223 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics structures but use different parameters, the commonality of languages cannot be fully exploited in this paradigm. A natural question arises that why not share the parameters between encoder and decoder on multilingual translation scenario? T"
P19-1117,D16-1163,0,0.0216677,"and ease of use, many researchers (Blackwood et al., 2018; Lakew et al., 2018) have conducted translation based on the framework of Johnson et al. (2017) and Ha et al. (2016). Zhou et al. (2019) propose to perform decoding in two translation directions synchronously, which can be applied on different target languages and is a new research area for Multi-NMT. In our method, we present a compact method for Multi-NMT, which can not only compress the model but also yield superior performance. Low-Resource and Zero-Shot NMT: Many researchers have explored low-resource NMT using transfer learning (Zoph et al., 2016; Neubig and Hu, 2018) and data augmenting (Sennrich et al., 2016a; Zhang and Zong, 2016) approaches. For zero-shot translation, Cheng et al. (2017) and Chen et al. (2017) utilize a pivot-based method, which bridges the gap between sourceto-pivot and pivot-to-target two steps. Multilingual translation is another direction to deal with both low-resource and zero-shot translation. Gu et al. (2018) enable sharing of lexical and sentence representation across multiple languages, especially for extremely low-resource Multi-NMT. Firat et al. (2016), Lakew et al. (2017), and Johnson et al. (2017) pro"
P19-1361,D17-1236,0,0.0306029,"on methods (Tong and Koller, 2001; Culotta and McCallum, 2005) in active learning (Balcan et al., 2009; Dasgupta et al., 2005) can be adopted. When learning new concepts, the cumulative learning system should avoid retraining the whole system and catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017). But the catastrophic forgetting does not happen if the dialogue system is trained with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans"
P19-1361,I17-1074,0,0.250225,"ed to define user needs in advance and avoid collecting biased training data laboriously. (2) To achieve this goal, we introduce IDS which is robust to new user actions and can extend itself online to accommodate new user needs. (3) We propose a new benchmark dataset to study the inconsistency of training and testing in task-oriented dialogue systems. 2 Background and Problem Definition Existing work on data-driven task-oriented dialogue systems includes generation based methods (Wen et al., 2016; Eric and Manning, 2017) and retrieval based methods (Bordes et al., 2016; Williams et al., 2017; Li et al., 2017). In this paper, we focus on the retrieval based methods, because they always return fluent responses. In a typical retrieval based system, a user gives an utterance xt to the system at the t-th turn. Let (xt,1 , ..., xt,N ) denote the tokens of xt . Then, the system chooses an answer yt = (yt,1 , ..., yt,M ) from the candidate response set R based on the conditional distribution p(yt |Ct ), where Ct = (x1 , y1 , ..., xt−1 , yt−1 , xt ) is the dialogue context consisting of all user utterances and responses up to the current turn. By convention, the dialogue system is designed to handle predef"
P19-1361,W15-4640,0,0.0286325,"of RSP SubD1 SubD2 SubD3 SubD4 SubD5 41 41 66 72 137 Table 1: The number of normalized response candidates in each sub-dataset after entity replacement, both training and test data included. 5.2 Baselines We compare IDS with several baselines: 3 We use special tokens to anonymize all private information in our corpus. 3714 • IR: the basic tf-idf match model used in (Bordes et al., 2016; Dodge et al., 2015). • Supervised Embedding Model (SEM): the supervised word embedding model used in (Bordes et al., 2016; Dodge et al., 2015). • Dual LSTM (DLSTM): the retrieval-based dialogue model used in (Lowe et al., 2015). • Memory Networks (MemN2N): the scoring model which is used in QA (Sukhbaatar et al., 2015) and dialogue systems (Bordes et al., 2016; Dodge et al., 2015). • IDS− : IDS without updating model parameters during testing. That is, IDS− is trained only with human intervention data on the training set and then we freeze parameters. 5.3 Implementation Details Our word embeddings are randomly initialized. The dimensions of word embeddings and GRU hidden units are both 32. The size of the latent variable z is 20. In uncertainty estimation, the repetition time K is 50. In all experiments, the average"
P19-1361,E09-1081,0,0.0418212,"Koller, 2001; Culotta and McCallum, 2005) in active learning (Balcan et al., 2009; Dasgupta et al., 2005) can be adopted. When learning new concepts, the cumulative learning system should avoid retraining the whole system and catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017). But the catastrophic forgetting does not happen if the dialogue system is trained with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans. Specifically, we sample vari"
P19-1361,D18-1415,1,0.816486,"encountering unconsidered user needs such as ”how to update the operating system”, the system will give unreasonable responses. Introduction Data-driven task-oriented dialogue systems have been a focal point in both academic and industry research recently. Generally, the first step of building a dialogue system is to clarify what users are allowed to do. Then developers can collect data to train dialogue models to support the defined capabilities. Such systems work well if all possible combinations of user inputs and conditions are considered in the training stage (Paek and Pieraccini, 2008; Wang et al., 2018). However, as shown 1 https://github.com/Leechikara/ Incremental-Dialogue-System in Fig. 1, if users have unanticipated needs, the system will give unreasonable responses. This phenomenon is mainly caused by a biased understanding of real users. In fact, before system deployment, we do not know what the customers will request of the system. In general, this problem can be alleviated by more detailed user studies. But we can never guarantee that all user needs are considered in the system design. Besides, the user inputs are often diverse due to the complexity of natural language. Thus, it is i"
P19-1361,P17-1062,0,0.568476,"m, developers do not need to define user needs in advance and avoid collecting biased training data laboriously. (2) To achieve this goal, we introduce IDS which is robust to new user actions and can extend itself online to accommodate new user needs. (3) We propose a new benchmark dataset to study the inconsistency of training and testing in task-oriented dialogue systems. 2 Background and Problem Definition Existing work on data-driven task-oriented dialogue systems includes generation based methods (Wen et al., 2016; Eric and Manning, 2017) and retrieval based methods (Bordes et al., 2016; Williams et al., 2017; Li et al., 2017). In this paper, we focus on the retrieval based methods, because they always return fluent responses. In a typical retrieval based system, a user gives an utterance xt to the system at the t-th turn. Let (xt,1 , ..., xt,N ) denote the tokens of xt . Then, the system chooses an answer yt = (yt,1 , ..., yt,M ) from the candidate response set R based on the conditional distribution p(yt |Ct ), where Ct = (x1 , y1 , ..., xt−1 , yt−1 , xt ) is the dialogue context consisting of all user utterances and responses up to the current turn. By convention, the dialogue system is designe"
P19-1361,W16-3601,0,0.0314926,"cases are located in the confidence boundary. In addition, there are multiple clusters in each class. It is due to the fact the same system response can appear in different dialogue scenes. For example, “the system requesting user’s phone number” appears in scenes of both exchange and return goods. Although these contexts have the same response, their representations should be different if they belong to different dialogue scenes. 7 Related Work Task-oriented dialogue systems have attracted numerous research efforts. Data-driven methods, such as reinforcement learning (Williams et al., 2017; Zhao and Eskenazi, 2016; Li et al., 2017) and supervised learning (Wen et al., 2016; Eric and Manning, 2017; Bordes et al., 2016), have been applied to optimize dialogue systems automatically. These advances in task-oriented dialogue systems have resulted in impressive gains in performance. However, prior work has mainly focused on building task-oriented dialogue systems in a closed environment. Due to the biased assumptions of real users, such systems will break down when encountering unconsidered situations. Several approaches have been adopted to address this problem. Gaˇsic et al. (2014) explicitly defined kerne"
P19-1361,P17-1061,0,0.0206155,"with all possible user needs alternatively from scratch. The uncertainty estimation and online learn6 The term “incremental” refers to systems able to operate on a word by word basis in the previous work (Eshghi et al., 2017; Schlangen and Skantze, 2009). In our work, it refers to the system which can adapt to new dialogue scenarios after deployment. 3717 ing methods in our work are inspired by variational inference approach (Rezende et al., 2014; Kingma and Welling, 2014). In the existing work, this approach was used to generate diverse machine responses in both open domain dialogue systems (Zhao et al., 2017; Serban et al., 2016) and task-oriented dialogue systems (Wen et al., 2017). In contrast, our work makes use of the Bayesian nature of variational inference to estimate the uncertainty and learn from humans. Specifically, we sample variables from the prior network as the random perturbation to estimate the model uncertainty following the idea of QueryBy-Committee (Seung et al., 1992) and optimize model parameters by maximizing the ELBO. 8 Conclusion This paper presents a novel incremental learning framework to design dialogue systems, which we call IDS. In this paradigm, users are not expecte"
P19-1541,C18-1305,1,0.834495,"zon Alexa, and Microsoft Cortana. A typical pipeline of SLU includes domain classification, intent detection, and slot filling(Tur and De Mori, 2011), to parse user utterances into semantic frames. Example semantic frames (Chen et al., 2018) are shown in Figure 1 for a restaurant reservation. Traditionally, domain classification and intent detection are treated as classification tasks with popular classifiers such as support vector machine and deep neural network (Haffner et al., 2003; Sarikaya et al., 2011). They can also be combined into one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014). Considering that pipeline approaches usually suffer from error propagation, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effecti"
P19-1541,W17-5514,0,0.0892639,"intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effective SLU system, the contextual information has been shown useful (Bhargava et al., 2013; Xu and Sarikaya, 2014), as natural language utterances are often ambiguous. For example, the number 6 of utterance u2 in Figure 1 may refer to either B-time or B-people without considering the context. Popular contextual SLU models (Chen et al., 2016; Bapna et al., 2017) exploit the dialogue history with the memory network (Weston et al., 2014), which covers all three main stages of memory process: encoding (write), storage (save) and retrieval (read) (Baddeley, 1976). With such a memory mechanism, SLU model can retrieve context knowledge to reduce the ambiguity of the current utterance, contributing to a stronger SLU model. However, the memory consolidation, a well-recognized operation for maintaining and updating memory in cognitive psy5448 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5448–5453 c Florence, I"
P19-1541,C18-3006,0,0.0305638,"t O B-people O B-date s Which restaurant would you like to book a table for? u1 Cascal for 6 S2 B-rest O B-time B-people Figure 1: Example semantic frames of utterances u1 and u2 with domain (D), intent (I) and semantic slots in IOB format (S1 , S2 ). Introduction Spoken language understanding (SLU) is a key technique in today’s conversational systems such as Apple Siri, Amazon Alexa, and Microsoft Cortana. A typical pipeline of SLU includes domain classification, intent detection, and slot filling(Tur and De Mori, 2011), to parse user utterances into semantic frames. Example semantic frames (Chen et al., 2018) are shown in Figure 1 for a restaurant reservation. Traditionally, domain classification and intent detection are treated as classification tasks with popular classifiers such as support vector machine and deep neural network (Haffner et al., 2003; Sarikaya et al., 2011). They can also be combined into one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014"
P19-1541,W17-5506,0,0.151755,"ogue logistic inference (DLI), defined as sorting a shuffled dialogue session into its original logical order. DLI can be trained with contextual SLU jointly if utterances are sorted one by one: selecting the right utterance from remaining candidates based on previously sorted context. In other words, given a response and its context, the DLI task requires our model to infer whether the response is the right one that matches the dialogue context, similar to the next sentence prediction task (Logeswaran and Lee, 2018). We conduct our experiments on the public multi-turn dialogue dataset KVRET (Eric and Manning, 2017), with two popular memory based contextual SLU models. According to our experimental results, noticeable improvements are observed, especially on slot filling. 2 Model Architecture This section first explains the memory mechanism for contextual SLU, including memory encoding and memory retrieval. Then we introduce the SLU tagger with context knowledge, the definition of DLI and how to optimize the SLU and DLI jointly. The overall model architecture is illustrated in Figure 2. memory embedding M = {m1 , m2 , ...mk } with a BiGRU (Chung et al., 2014) layer and then encode the current utterance x"
P19-1541,N18-2118,0,0.0268652,"nto one task if there are not many intents of each domain(Bai et al., 2018). Slot filling task is usually treated as a sequence labeling task. Popular approaches for slot filling include conditional random fields (CRF) and recurrent neural network (RNN) (Raymond and Riccardi, 2007; Yao et al., 2014). Considering that pipeline approaches usually suffer from error propagation, the joint model for slot filling and intent detection has been proposed to improve sentence-level semantics via mutual enhancement between two tasks (Xu and Sarikaya, 2013; Hakkani-T¨ur et al., 2016; Zhang and Wang, 2016; Goo et al., 2018), which is a direction we follow. To create a more effective SLU system, the contextual information has been shown useful (Bhargava et al., 2013; Xu and Sarikaya, 2014), as natural language utterances are often ambiguous. For example, the number 6 of utterance u2 in Figure 1 may refer to either B-time or B-people without considering the context. Popular contextual SLU models (Chen et al., 2016; Bapna et al., 2017) exploit the dialogue history with the memory network (Weston et al., 2014), which covers all three main stages of memory process: encoding (write), storage (save) and retrieval (read"
P19-1541,E17-1042,0,0.0497988,"Missing"
Q13-1020,P09-1088,0,0.0892939,"power on tree construction than SCFG. In a STSG-based U-tree or a STSG rule, although not linguistically informed, the nodes labeled by POS tags are also effective on distinguishing different ones. However, with SCFG, we have to discard all the internal nodes (i.e., flattening the Utrees or rules) to express the same sequence, leading to a poor ability of distinguishing different U-trees and production rules. Thus, using STSG, we can build more specific U-trees for translation. In addition, we find that the Bayesian SCFG grammar cannot even significantly outperform the heuristic SCFG grammar (Blunsom et al. 2009) 5. This would indicate that the SCFG-based derivation tree as by-product is also not such good for tree-based translation models. Considering the above reasons, we believe that the STSG-based learning procedure would result in a better translation grammar for tree-based models. 5 4 p (ri |N i ) i 1 In (Blunsom et al., 2009), for Chinese-to-English translation, the Bayesian SCFG grammar only outperform the heuristic SCFG grammar by 0.1 BLEU points on NIST MT 2004 and 0.6 BLEU points on NIST MT 2005 in the NEWS domain. 4 Bayesian Model In this section, we present a Bayesian model to learn STSG"
Q13-1020,N10-1028,0,0.04127,"Missing"
Q13-1020,D08-1092,0,0.0168634,"model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on gene"
Q13-1020,N10-1015,0,0.0209123,"ively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervi"
Q13-1020,D12-1079,0,0.015431,"244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG tr"
Q13-1020,J07-2003,0,0.0843774,"ees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the"
Q13-1020,P96-1021,0,0.0379502,"Missing"
Q13-1020,J97-3002,0,0.614637,"Missing"
Q13-1020,D09-1037,0,0.155905,"s further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended"
Q13-1020,P07-1003,0,0.0294755,"variables into the string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,D11-1018,0,0.020342,"Bayesian model efficiently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used"
Q13-1020,N10-1033,0,0.0421939,"Missing"
Q13-1020,P03-2041,0,0.0527913,"Missing"
Q13-1020,N04-1035,0,0.298489,"ow that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, ra"
Q13-1020,P06-1121,0,0.0523496,"set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse"
Q13-1020,W11-2160,0,0.0309381,"Missing"
Q13-1020,W06-3601,0,0.690714,"Missing"
Q13-1020,N03-1017,0,0.0509465,"Missing"
Q13-1020,W04-3250,0,0.0422056,"utilize the U-trees from random 1 for further analysis hereafter. 1.060E+07 Total Number of Frontier Nodes the training data. For tuning and testing, we use the NIST MT 2003 evaluation data as the development set, and use the NIST MT04 and MT05 data as the test set. We use MERT (Och, 2004) to tune parameters. Since MERT is prone to search errors, we run MERT 5 times and select the best tuning parameters in the tuning set. The translation quality is evaluated by case-insensitive BLEU-4 with the shortest length penalty. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). To create the baseline system, we use the opensource Joshua 4.0 system (Ganitkevitch et al., 2012) to build a hierarchical phrase-based (HPB) system, and a syntax-augmented MT (SAMT) 11 system (Zollmann and Venugopal, 2006) respectively. The translation system used for testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up t"
Q13-1020,P07-2045,0,0.00436685,"he string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,D12-1021,0,0.0293399,"Missing"
Q13-1020,W09-0424,0,0.0370304,"Missing"
Q13-1020,D07-1078,0,0.0219737,"he string. The variables are inserted one at a time using a uniform distribution over the possible positions. This factor discourages more variables. For the example rule in Figure 1, the generative process of the source string is: a. Decide to generate one source word; b. Generate the source word “ (wo-men) ”; c. Insert the first variable after the word; d. Insert the second variable between the word and the first variable. Intuitively, a good translation grammar should carry both small translation rules with enough generality and large rules with enough context information. DeNero and Klein (2007) proposed this statement, and Cohn and Blunsom (2009) has verified it in their experiments with parse trees. Our base distribution is also designed based on this intuition. Considering the two factors in our base distribution, we penalize both large target tree fragments with many nodes and long source strings with many words and variables. The Bayesian model tends to select both small and frequent STSG production rules to construct the U-trees. With these types of trees, we can extract small rules with good generality and simultaneously 247 obtain large rules with enough context information b"
Q13-1020,J10-2004,0,0.014262,"y built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual mapping between language pairs. This indicates that parse trees are usually not the optimal choice for training tree-based translation models (Wang et al., 2010). Based on the above analysis, we can conclude that the tree structure that is independent from Tree-bank resources and simultaneously considers the bilingual mapping inside the bilingual sentence pairs would be a good choice for building treebased translation models. Therefore, complying with the above conditions, we propose an unsupervised tree structure for treebased translation models in this study. In the structures, tree nodes are labeled by combining the word classes of their boundary words rather than by syntactic labels, such as NP, VP. Furthermore, using these node labels, we design"
Q13-1020,C12-1186,1,0.400255,"on process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SC"
Q13-1020,N06-1033,0,0.0579748,"Missing"
Q13-1020,D12-1078,0,0.0123537,"ian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint parsing and alignment. They utilized the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word clas"
Q13-1020,C08-1136,0,0.055176,"uild a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used in a phrase-based and hierarchical phrase-based system (Chiang, 2007). Levenberg et al. (2012) employed a Bayesian method to learn discontinuous SCFG rules. This study differs from their work because 244 we concentrate on constructing tree structures for tree-based translation models. Our U-trees are learned based on STSG, which is more appropriate for tree-based translation models than SCFG. Burkett and Klein (2008) and Burkett et al. (2010) focused on joint p"
Q13-1020,P06-1077,0,0.103477,"s on the tree nodes. Experimental results show that the string-totree translation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to"
Q13-1020,P11-1084,0,0.0251479,"Missing"
Q13-1020,P09-1020,0,0.0385606,"Missing"
Q13-1020,W06-1606,0,0.634183,"ranslation system using our Bayesian tree structures significantly outperforms the strong baseline string-to-tree system using parse trees. 1 Introduction In recent years, tree-based translation models 1 are drawing more and more attention in the community of statistical machine translation (SMT). Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process, tree-based translation models have shown promising progress in improving translation quality (Liu et al., 2006, 2009; Quirk et al., 2005; Galley et al., 2004, 2006; Marcu et al., 2006; Shen et al., 2008; Zhang et al., 2011b). However, tree-based translation models always suffer from two major challenges: 1) They are usually built directly from parse trees, which are generated by supervised linguistic parsers. 1 A tree-based translation model is defined as a model using tree structures on one side or both sides. However, for many language pairs, it is difficult to acquire such corresponding linguistic parsers due to the lack of Tree-bank resources for training. 2) Parse trees are actually only used to model and explain the monolingual structure, rather than the bilingual ma"
Q13-1020,P03-1021,0,0.0190538,"Missing"
Q13-1020,P02-1040,0,0.0866083,"Missing"
Q13-1020,P06-1055,0,0.0582151,"testing the effectiveness of our U-trees is our in-house stringto-tree system (abbreviated as s2t). The system is implemented based on (Galley et al., 2006) and (Marcu et al. 2006). In the system, we extract both the minimal GHKM rules (Galley et al., 2004), and the rules of SPMT Model 1 (Galley et al., 2006) with phrases up to length L=5 on the source side. We then obtain the composed rules by composing two or three adjacent minimal rules. To build the above s2t system, we first use the parse tree, which is generated by parsing the English side of the bilingual data with the Berkeley parser (Petrov et al., 2006). Then, we binarize the English parse trees using the head binarization approach (Wang et al., 2007) and use the resulting binary parse trees to build another s2t system. For the U-trees, we run the Gibbs sampler for 1000 iterations on the whole corpus. The sampler uses 1,087s per iteration, on average, using a single core, 2.3 GHz Intel Xeon machine. For the hyperparameters, we set to 0.1 and pexpand = 1/3 to give a preference to the rules with small fragments. We built an s2t translation system with the achieved U-trees after the 1000th iteration. We only use one sample to extract the transl"
Q13-1020,P05-1034,0,0.118254,"Missing"
Q13-1020,D11-1019,1,0.925641,"ently. The remainder of the paper is organized as follows. Section 2 introduces the related work. Section 3 describes the STSG generation process, and Section 4 depicts the adopted Bayesian model. Section 5 describes the Gibbs sampling algorithm and Gibbs operators. In Section 6, we analyze the achieved U-trees and evaluate their effectiveness. Finally, we conclude the paper in Section 7. 2 Related Work In this study, we move in a new direction to build a tree-based translation model with effective unsupervised U-tree structures. For unsupervised tree structure induction, DeNero and Uszkoreit (2011) adopted a parallel parsing model to induce unlabeled trees of source sentences for syntactic pre-reordering. Our previous work (Zhai et al., 2012) designed an EMbased method to construct unsupervised trees for tree-based translation models. This work differs from the above work in that we design a novel Bayesian model to induce unsupervised U-trees, and prior knowledge can be encoded into the model more freely and effectively. Blunsom et al. (2008, 2009, 2010) utilized Bayesian methods to learn synchronous context free grammars (SCFG) from a parallel corpus. The obtained SCFG is further used"
Q13-1020,2007.mtsummit-papers.71,0,0.0664862,"Missing"
Q13-1020,W06-3119,0,0.214328,"the bilingual Tree-bank to train a joint model for both parsing and word alignment. Cohn and Blunsom (2009) adopted a Bayesian method to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution gram"
Q13-1020,P11-1001,0,0.0524772,"hod to infer an STSG by exploring the space of alignments based on parse trees. Liu et al. (2012) re-trained the linguistic parsers bilingually based on word alignment. Burkett and Klein (2012) utilized a transformation-based method to learn a sequence of monolingual tree transformations for translation. Compared to their work, we do not rely on any Tree-bank resources and focus on generating effective unsupervised tree structures for tree-based translation models. Zollmann and Venugopal (2006) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories. Zollmann and Vogel (2011) further labeled the SCFG rules with POS tags and unsupervised word classes. Our work differs from theirs in that we present a Bayesian model to learn effective STSG translation rules and U-tree structures for tree-based translation models, rather than designing a labeling strategy for translation rules. 3 The STSG Generation Process In this work, we induce effective U-trees for the string-to-tree translation model, which is based on a synchronous tree substitution grammar (STSG) between source strings and target tree fragments. We take STSG as the generation grammar to match the translation m"
Q13-1020,P08-1064,0,\N,Missing
Q13-1020,P09-1063,0,\N,Missing
Q13-1020,W06-1628,0,\N,Missing
Q13-1020,P08-1066,0,\N,Missing
Q13-1024,P11-1131,0,0.0366234,"Missing"
Q13-1024,J93-2003,0,0.086468,"Missing"
Q13-1024,P03-1012,0,0.590582,"nt technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree produced the highest degree of cohesion. So Cherry and Lin (2003; 2006a) used dependency cohesion as a hard constraint to restrict the alignment space, where all potential alignments violating cohesion constraint are ruled out directly. Although the alignment quality is improved, they ignored situations where a small set of correct alignments can violate cohesion. To address this limitation, Cherry and Lin (2006b) proposed a soft constraint approach, which took dependency cohesion as a feature of a discriminative model, and verified that the soft constraint works better than the hard constraint. However, the training procedure is very timeconsuming, and th"
Q13-1024,E06-1019,0,0.0898349,"ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o"
Q13-1024,P06-2014,0,0.741722,"ive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized as follows: Section 2 introduces dependency cohesion 291 Transactions o"
Q13-1024,P07-1003,0,0.479084,"linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sa"
Q13-1024,P11-1042,0,0.0741043,"Missing"
Q13-1024,W02-1039,0,0.601548,"et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on"
Q13-1024,N04-1035,0,0.175101,"algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et a"
Q13-1024,D08-1036,0,0.028431,"Missing"
Q13-1024,P03-1011,0,0.0747953,"sion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but 298 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of re-ordering operations over child nodes of a syntactic tree. Gildea (2003) introduced a “loosely” tree-based alignment technique, which allows alignments to violate syntactic constraints by incurring a cost in probability. Pauls et al. (2010) gave a new instance of the ITG formalism, in which one side of the synchronous derivation is constrained by the syntactic tree. Fox (2002) measured syntactic cohesion in gold standard alignments and showed syntactic cohesion is generally maintained between English and French. She also compared three variant syntactic representations (phrase tree, verb phrase flattening tree and dependency tree), and found the dependency tree pr"
Q13-1024,D11-1137,0,0.0676682,"Missing"
Q13-1024,P10-1017,0,0.228077,"he past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assu"
Q13-1024,N07-1018,0,0.0184611,"ignment with Eq. (2) among neighbor alignments of the initial point, and taking ??? as the higher-order node: then make the best alignment as the initial point for ??? (??? |?[1,?] ) = ∏?? ,?∈?? ?? (??? ,? |?? , ??? , ??? ) the next iteration. The algorithm iterates until no ? ? ? update could be made. (6) 4.2 Gibbs Sampling Algorithm where ???,? ∈ {??ℎ?????, ????????} is the modifiermodifier cohesion relationship between ??? and Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in one of its sibling ?? , ?? is the corresponding the literatures (Johnson et al., 2007; Gao and probability, ??? and ??? are the aligned words for Johnson, 2008), there are two types of Gibbs ? samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take 4 Parameter Estimation advantage of explicit Gibbs sampling to ma"
Q13-1024,D11-1046,0,0.0565794,"ent models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint depende"
Q13-1024,W04-3250,0,0.0824144,"ent tasks. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Tree-Distance Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs gold-standard forward HCP MCP 60.53 63.94 60.57 62.53 66.48 74.65 75.52 66.61 81.37 74.69 98.70 97.43 85.21 81.96 88.74 85.55 88.43 95.82 reverse HCP MCP 56.15 64.80 66.49 65.68 67.19 72.32 73.88 66.07 78.00 71.73 98.25 97.84 82.96 81.36 87.81 84.83 81.53 91.62 worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). Table 5: HCPs and MCPs on the development set. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Hard-Cohesion Soft-Cohesion-EM Soft-C"
Q13-1024,P12-2060,0,0.154069,"Missing"
Q13-1024,P07-2045,0,0.030301,"nd a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Mo"
Q13-1024,N10-1050,0,0.0147816,"nerative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominate"
Q13-1024,N06-1014,0,0.218471,"se and English. 3 A Generative Word Alignment Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence ?1? and the alignment ?1? are generated conditioning on the target sentence ?1? . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as ?1? = ?1 , ?"
Q13-1024,N03-2017,0,0.123944,"air, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to train the model in an unsupervised manner. Experiments on a large-scale Chinese-English translation task demonstrate that our model achieves improvements in both word alignment quality and machine translation quality. The remainder of this paper is organized"
Q13-1024,W05-0812,0,0.81294,"ent models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm"
Q13-1024,P11-2032,0,0.245202,"??, ????????} is the modifiermodifier cohesion relationship between ??? and Gibbs sampling is another effective algorithm for unsupervised learning problems. As is described in one of its sibling ?? , ?? is the corresponding the literatures (Johnson et al., 2007; Gao and probability, ??? and ??? are the aligned words for Johnson, 2008), there are two types of Gibbs ? samplers: explicit and collapsed. An explicit sampler represents and samples the model parameters in addition to the word alignments, while in a collapsed sampler the parameters are integrated out and only alignments are sampled. Mermer and Saraçlar (2011) proposed a collapsed sampler for IBM Model 1. However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks. Instead, we take 4 Parameter Estimation advantage of explicit Gibbs sampling to make a To align sentence pairs with the model in Eq. (2), highly parallelizable sampler. Our Gibbs sampler we have to estimate some parameters: ?? , ??? , ?ℎ is similar to the MCMC algorithm in Zhao and and ?? . The traditional approach for sequence- Gildea (2010), but we assume Dirichlet priors based models uses Expectation Maximization (EM) when sampling model p"
Q13-1024,H05-1011,0,0.0668312,"07; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency c"
Q13-1024,W99-0604,0,0.146059,"ble alignments, which is required Our sampler performs a sequence of consecutive in the E-step of EM algorithm. Therefore, we iterations. Each iteration consists of two sampling propose an approximate EM algorithm and a Gibbs steps. The first step samples the aligned position sampling algorithm for parameter estimation. for each dependency node according to the BUTorder. Concretely, when sampling the aligned ??? and ?? . Both ?ℎ and ?? in Eq. (5) and Eq. (6) are conditioned on three words, which would make them very sparse. To cope with this problem, we use the word clustering toolkit, mkcls (Och et al., 1999), to cluster all words into 50 classes, and replace the three words with their classes. 295 (?+1) position ??? for node ??? on iteration ?+1, the aligned positions for ?[1,?−1] are fixed on the new sampling results ?(?+1) [1,?−1] on iteration ? +1, and the aligned positions for ?[?+1,?] are fixed on the old sampling results ?(?) [?+1,?] on iteration ? . Therefore, we sample the aligned position ??(?+1) as follows: ? (?+1) ??? (?+1) (?) ~ ? (??? |?[1,?−1] , ?[?+1,?] , ?1? , ?1? ) = ?? = where ? ? (?+1) ?[1,?−1] ?? |?1? ) ? (?1? , ? ? ∑?? ? ? ?? |?1? ) ∈{0,1,…,?} ? (?1 , ? ? ∪ ??? ∪ (?) ?[?+1"
Q13-1024,J03-1002,0,0.226771,"rd alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quali"
Q13-1024,P02-1040,0,0.0916915,"on We then evaluate the effect of word alignment on machine translation quality using the phrase-based translation system Moses (Koehn et al., 2007). We take NIST MT03 test data as the development set, NIST MT05 test data as the testing set. We train a 5-gram language model with the Xinhua portion of English Gigaword corpus and the English side of the training set using the SRILM Toolkit (Stolcke, 2002). We train machine translation models using GDFA alignments of each system. BLEU scores on NIST MT05 are listed in Table 7, where BLEU scores are calculated using lowercased and tokenized data (Papineni et al., 2002). Although the IBM4-L0, Agree-HMM, Tree-Distance and Hard-Cohesion systems improve word alignment than IBM4, they fail to outperform the IBM4 system on machine translation. The BLEU score of our Soft-Cohesion-EM system is better than the IBM4 system when using the FBIS training set, but 298 6 Related Work There have been many proposals of integrating syntactic knowledge into generative alignment models. Wu (1997) proposed the inversion transduction grammar (ITG) to model word alignment as synchronous parsing for a sentence pair. Yamada and Knight (2001) represented translation as a sequence of"
Q13-1024,N10-1014,0,0.555753,"d features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an approximate EM algorithm and an explicit Gibbs sampling algorithm to t"
Q13-1024,P06-1055,0,0.00759307,"hether our model is adaptable for large-scale task. For word alignment quality evaluation, we take the handaligned data sets from SSMT20072, which contains 2 http://nlp.ict.ac.cn/guidelines/guidelines-2007SSMT(English).doc 296 505 sentence pairs in the testing set and 502 sentence pairs in the development set. Following Och and Ney (2003), we evaluate word alignment quality with the alignment error rate (AER), where lower AER is better. Because our model takes dependency trees as input, we parse both sides of the two training sets, the development set and the testing set with Berkeley parser (Petrov et al., 2006), and then convert the generated phrase trees into dependency trees according to Wang and Zong (2010; 2011). Our model is an asymmetric model, so we perform word alignment in both forward (ChineseEnglish) and reverse (EnglishChinese) directions. Train Set FBIS Source Corpus FBIS newswire data LARGE LDC2000T50, LDC2003E14, LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27, LDC2005T10, LDC2005T34 # Words Ch: 7.1M En: 9.1M Ch: 27.6M En: 31.8M Table 2: The size and the source corpus of the two training sets. 5.1 Effectiveness of Cohesion Constraints In Eq. (3), the distortion probability ?? is deco"
Q13-1024,H05-1010,0,0.0658408,"al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) i"
Q13-1024,P12-1033,0,0.090783,"Missing"
Q13-1024,C96-2141,0,0.899422,"model achieves improvements in both alignment quality and translation quality. 1 Introduction Word alignment is the task of identifying word correspondences between parallel sentence pairs. Word alignment has become a vital component of statistical machine translation (SMT) systems, since it is required by almost all state-of-the-art SMT systems for the purpose of extracting phrase tables or even syntactic transformation rules (Koehn et al., 2007; Galley et al., 2004). During the past two decades, generative word alignment models such as the IBM Models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) have been widely used, primarily because they are trained on bilingual sentences in an unsupervised manner and the implementation is freely available in the GIZA++ toolkit (Och and Ney, 2003). However, the word alignment quality of generative models is still far from satisfactory for SMT systems. In recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data."
Q13-1024,J97-3002,0,0.672161,"recent years, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candida"
Q13-1024,P01-1067,0,0.813018,"rs, discriminative alignment models incorporating linguistically motivated features have become increasingly popular (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010; Saers et al., 2010; Riesa et al., 2011). These models are usually trained with manually annotated parallel data. However, when moving to a new language pair, large amount of hand-aligned data are usually unavailable and expensive to create. A more practical way to improve large-scale word alignment quality is to introduce syntactic knowledge into a generative model and train the model in an unsupervised manner (Wu, 1997; Yamada and Knight, 2001; Lopez and Resnik, 2005; DeNero and Klein, 2007; Pauls et al., 2010). In this paper, we take dependency cohesion (Fox, 2002) into account, which assumes phrases dominated by disjoint dependency subtrees tend not to overlap after translation. Instead of treating dependency cohesion as a hard constraint (Lin and Cherry, 2003) or using it as a feature in discriminative models (Cherry and Lin, 2006b), we treat dependency cohesion as a distortion constraint, and integrate it into a modified HMM word alignment model to softly influence the probabilities of alignment candidates. We also propose an a"
Q13-1024,zhang-etal-2004-interpreting,0,0.014757,"M4 IBM4-L0 IBM4-Prior Agree-HMM Tree-Distance Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs gold-standard forward HCP MCP 60.53 63.94 60.57 62.53 66.48 74.65 75.52 66.61 81.37 74.69 98.70 97.43 85.21 81.96 88.74 85.55 88.43 95.82 reverse HCP MCP 56.15 64.80 66.49 65.68 67.19 72.32 73.88 66.07 78.00 71.73 98.25 97.84 82.96 81.36 87.81 84.83 81.53 91.62 worse when using the LARGE training set. Our Soft-Cohesion-Gibbs system produces the best BLEU score when using both training sets. We also performed a statistical significance test using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). Experimental results show the Soft-Cohesion-Gibbs system is significantly better (p&lt;0.05) than the IBM4 system. The IBM4-Prior system slightly outperforms IBM4, but it’s not significant. FBIS LARGE IBM4 30.7 33.1 IBM4-L0 30.4 32.3 IBM4-Prior 30.9 33.2 Agree-HMM 27.2 30.1 Tree-Distance 28.2 N/A Hard-Cohesion 30.4 32.2 Soft-Cohesion-EM 30.9 33.1 Soft-Cohesion-Gibbs 31.6* 33.9* Table 7: BLEU scores, where * indicates significantly better than IBM4 (p&lt;0.05). Table 5: HCPs and MCPs on the development set. IBM4 IBM4-L0 IBM4-Prior Agree-HMM Hard-Cohesion Soft-Cohesion-EM Soft-Cohesion-Gibbs forward"
Q13-1024,D10-1058,0,0.206812,"t Model with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 1-5 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5). The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011). So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model. 3.1 The Sequence-based Alignment Model According to Brown et al. (1993) and Och and Ney (2003), the sequence-based model is built as a noisy channel model, where the source sentence ?1? and the alignment ?1? are generated conditioning on the target sentence ?1? . The model assumes each source word is assigned to exactly one target word, and defines an asymmetric alignment for the sentence pair as ?1? = ?1 , ?2 , … , ?? , … , ?? , where each ?? ∈ [0, ?] is"
Q13-1024,I11-1140,1,\N,Missing
Q13-1024,C10-2148,1,\N,Missing
Q15-1020,W08-2134,0,0.0230942,"ng the LFR on test and unlabeled data, we can parse the data using LatSynModel. Experiments in later sections show that the LatSynModel adapts much better to the target domain than the OriSynModel. 4.2.2 Adapting the Semantic Parser The situation here is similar to the adaptation of the syntactic parser. Features on training data can be directly extracted. To extract features on unlabeled data, we need to have syntactic dependency trees on this data. So we use our LatSynModel to parse the unlabeled data first. And we automatically identify predicates on unlabeled data using a classifier as in Che et al., (2008). Then we extract the original features for semantic parsing on unlabeled data. By feeding original features extracted on these data to our DBN model, we learn the LFR for semantic dependency parsing. Using the LFR, we can train the semantic parsing model LatSemModel. 5 Experiments 5.1 Experiment Setup 5.1.1 Experiment Data We use the English data in the CoNLL 2009 shared task for experiments. The training data and in-domain test data are from the WSJ corpus, whereas the out-of-domain test data is from the Brown corpus. We also use unlabeled data consisting of the following sections of the Bro"
Q15-1020,C08-1015,0,0.0406062,"Missing"
Q15-1020,P07-1033,0,0.356036,"Missing"
Q15-1020,J02-3001,0,0.366224,"Missing"
Q15-1020,D07-1097,0,0.0689777,"Missing"
Q15-1020,D08-1008,0,0.0508184,"Missing"
Q15-1020,P08-1068,0,0.111154,"Missing"
Q15-1020,J08-2001,0,0.0252036,"Missing"
Q15-1020,H05-1066,0,0.204054,"Missing"
Q15-1020,W06-2933,0,0.091304,"Missing"
Q15-1020,P05-1013,0,0.0560737,"educe method as in Nivre et al., (2006). It has four basic actions: left-arc, right-arc, shift, and reduce. A classifier is used to determine an action at each step. To decide the label for each dependency link, we extend the left/right-arc actions to their corresponding multi-label actions, leading to 31 left-arc and 66 right-arc actions. Altogether a 99class problem is yielded for parsing action classification. We add arcs to the dependency graph in an arc eager manner as in Hall et al., (2007). We also projectivize the non-projective sequences in training data using the transformation from Nivre and Nilsson (2005). A maximum entropy classifier is used to make decisions at each step. The features utilized are the same as those in Zhao et al., (2008). 4.1.2 Semantic Dependency Parsing Our semantic dependency parser is similar to the one in Che et al., (2009). We first train a predicate sense classifier on training data, using the same features as in Che et al., (2009). Again, a maximum en276 Adapting the Basic System to Target Domain In our basic pipeline system, both the syntactic and semantic dependency parsers are built using discriminative models. We train a syntactic parsing model and a semantic par"
Q15-1020,P11-1007,0,0.07292,"and an argument is a syntactic feature used in semantic dependency parsing (Johansson and Nugues, 2008). Figure 1 shows an example of this relation path feature. Obviously, syntactic features like this are also very sparse and usually specific to each domain. The method of clustering fails in generalizing these kinds of features. Our method, however, is very different from clustering specific features and substituting these features using their clusters. Instead, we attack the domain adaption problem by learning a latent feature representation (LFR) for different domains, which is similar to Titov (2011). Formally, we propose a Deep Belief Network (DBN) model to represent a data sample using a vector of latent features. This latent feature vector is inferred by our DBN model 271 Transactions of the Association for Computational Linguistics, vol. 3, pp. 271–282, 2015. Action Editor: Hal Daum´e III. Submission batch: 3/2015; Published 5/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. P ROOT OBJ SBJ She OPRD IM wants to OBJ pay you NMOD a visit . Figure 1: A path feature example. The red edges are the path between She and visit and thus the re"
Q15-1020,P10-1040,0,0.0214588,"LP community. The stateof-the-art model performs well if the test data comes from the domain of the training data. But if the test data comes from a different domain, the performance drops severely. The results of the shared tasks of CoNLL 2008 and 2009 (Surdeanu et al., 2008; Hajiˇc et al., 2009) also substantiates the argument. To relieve the domain adaptation, in this paper, we propose a deep learning method for both syntactic and semantic parsers. We focus on the situation that, Previous work have shown that using word clusters to replace the sparse lexicalized features (Koo et al., 2008; Turian et al., 2010), helps relieve the performance degradation on the target domain. But for syntactic and semantic parsing, people also use a lot of syntactic features, i.e., features extracted from syntactic trees. For example, the relation path between a predicate and an argument is a syntactic feature used in semantic dependency parsing (Johansson and Nugues, 2008). Figure 1 shows an example of this relation path feature. Obviously, syntactic features like this are also very sparse and usually specific to each domain. The method of clustering fails in generalizing these kinds of features. Our method, however"
Q15-1020,W04-3212,0,0.0612116,"ties. So the autoencoder is in essence a feed-forward neural network. We tune the parameters of our DBN model on this autoencoder using backpropagation algorithm. 4 Domain Adaptation with Our DBN Model In this section, we introduce how to use our DBN model to adapt a basic syntactic and semantic de... ... ... ... ... ... ... ... tropy classifier is employed. Given a predicate, we need to decide its semantic dependency relation with each word in the sentence. To reduce the number of argument candidates, we adopt the pruning strategy in Zhao et al., (2009), which is adapted from the strategy in Xue and Palmer (2004). In the semantic role classification stage, we use a maximum entropy classifier to predict the probabilities of a candidate to be each semantic role. We train two different classifiers for verb and noun predicates using the same features as in Che et al., (2009). We use a simple method for post processing. If there are duplicate arguments for ARG0∼ARG5, we preserve the one with the highest classification probability and remove its duplicates. ... ... ... ... ... ... ... ... ... Figure 5: Unrolling the DBN. pendency parsing system to target domain. 4.2 4.1 The Basic Pipelined System We build a"
Q15-1020,D14-1041,1,0.893204,"Missing"
Q15-1020,W09-1209,0,0.109366,"y hidden variables are replaced by its activation probabilities. So the autoencoder is in essence a feed-forward neural network. We tune the parameters of our DBN model on this autoencoder using backpropagation algorithm. 4 Domain Adaptation with Our DBN Model In this section, we introduce how to use our DBN model to adapt a basic syntactic and semantic de... ... ... ... ... ... ... ... tropy classifier is employed. Given a predicate, we need to decide its semantic dependency relation with each word in the sentence. To reduce the number of argument candidates, we adopt the pruning strategy in Zhao et al., (2009), which is adapted from the strategy in Xue and Palmer (2004). In the semantic role classification stage, we use a maximum entropy classifier to predict the probabilities of a candidate to be each semantic role. We train two different classifiers for verb and noun predicates using the same features as in Che et al., (2009). We use a simple method for post processing. If there are duplicate arguments for ARG0∼ARG5, we preserve the one with the highest classification probability and remove its duplicates. ... ... ... ... ... ... ... ... ... Figure 5: Unrolling the DBN. pendency parsing system to"
Q15-1020,W08-2127,0,0.0435028,"Missing"
Q15-1020,C10-1153,1,0.893897,"Missing"
Q15-1020,D10-1030,1,0.878016,"Missing"
Q15-1020,W08-2121,0,\N,Missing
Q15-1020,W09-1207,0,\N,Missing
Q15-1020,D07-1112,0,\N,Missing
Q15-1020,W06-1615,0,\N,Missing
Q15-1020,D07-1096,0,\N,Missing
Q19-1006,W18-6408,0,0.0195983,"xplored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left-to-right and right-to-left decoding. (2) Our model can lever"
Q19-1006,D09-1117,0,0.21497,"Missing"
Q19-1006,P18-1008,0,0.0309062,"an Translation We further demonstrate the effectiveness of our model in WMT14 English-German translation tasks, and we also display the performances of some competitive models including GNMT (Wu et al., 2016), Conv (Gehring et al., 2017), and AttIsAll (Vaswani et al., 2017). As shown in Table 4, our model also significantly outperforms others and gets an improvement of 1.49 more 4.7 Analysis We conduct analyses on Chinese-English translation to better understand our model from different perspectives. 11 The BLEU scores for Transformer model are our reproduced results. Similar to footnote 7 in Chen et al. (2018), our performance is slightly lower than those reported in Vaswani et al. (2017). Additionally, we only use 3 GPUs for English-German, whereas most papers employ 8 GPUs for model training. Parameters and Speeds In contrast to the standard Transformer, our model does not increase any parameters except for a hyper-parameter λ, as 99 Model Transformer Transformer (R2L) Rerank-NMT ABD-NMT Our Model Param 207.8M 207.8M 415.6M 333.8M 207.8M Speed Train Test 2.07 19.97 2.07 19.81 1.03 6.51 1.18 7.20 1.26 17.87 Table 6: Statistics of parameters, training, and testing speeds. Train denotes the number o"
Q19-1006,C16-1172,0,0.0398605,"Missing"
Q19-1006,W17-3204,0,0.0225535,"significantly improved the quality of machine translation in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Zhang and Zong, 2015; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Recent approaches to sequence-to-sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017), or attention (Vaswani et al., 2017) as basic building blocks. Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right. Despite their remarkable success, NMT models suffer from several weaknesses (Koehn and Knowles, 2017). One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes (Liu et al., 2016). We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions, respectively. As shown in Table 1, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in terms of the last 4 tokens. This problem is mainly caused by the left-toright unidirectional decoding, which conditions each output word on previously generated outputs"
Q19-1006,P02-1040,0,0.104028,"ataset as the validation set and NIST 2003-2006 (MT03-06) as our test sets. We use BPE (Sennrich et al., 2016b) to encode Chinese and English, respectively. We learn 30K merge operations and limit the source and target vocabularies to the most frequent 30K tokens. For English-German translation, the training set consists of about 4.5 million bilingual sentence pairs from WMT 2014.5 We use newstest2013 as the validation set and newstest2014 as the test set. Sentences are encoded using BPE, which has a shared vocabulary of about 37,000 tokens. To evaluate the models, we compute the BLEU metric (Papineni et al., 2002) on tokenized, truecase output.6 For Russian-English translation, we use the following resources from the WMT parallel data7 : ParaCrawl corpus, Common Crawl corpus, News Commentary v13, and Yandex Corpus. We do not use Wiki Headlines and UN Parallel Corpus V1.0. The training corpus consists of 14M sentence pairs. We employ the Moses Tokenizer8 for preprocessing. For subword segmentation, we use 50,000 joint BPE operations and choose the most frequent 52,000 tokens as vocabularies. We use newstest2017 as the development set and the newtest2018 as the test set. Training We design a simple yet e"
Q19-1006,N18-1125,0,0.0196439,"bidirectional decoding. We discuss these topics in the following. Future Modeling Standard neural sequence decoders generate target sentences from left to right, and it has been proven to be important to establish the direct information flow between currently predicted word and previously generated words (Zhou et al., 2017b; Vaswani et al., 2017). However, current methods still fail to estimate some desired information in the future. To address this problem, reinforcement learning methods have been applied to predict future properties (Li et al., 2017; Bahdanau et al., 2017; He et al., 2017). Li et al. (2018) presented a target foresight based attention which uses the POS tag as the partial information of a target foresight word to improve alignment and translation. Inspired by human cognitive behaviors, Xia et al. (2017) proposed a deliberation network, which leverages global information by observing both back and forward information in sequence decoding through a deliberation process. Zheng et al. (2018) introduced two additional recurrent layers to model translated past contents and untranslated future contents. The most relevant models in future modeling are twin networks (Serdyuk et al., 2018"
Q19-1006,W17-4739,0,0.115793,"( y g ). More specifically, we first train an − L2R model using (x, → y g ) and an R2L model ← − using (x, y g ). Then we use the two models to translate source sentences x into pseudo target − − sentences → y p and ← y p , respectively. Finally, we − − − − get two triples (x, → y p, ← y g ) and (x, → y g, ← y p ) as our training data. Once the proposed model is trained, we employ the bidirectional beam search algorithm to predict the target sequence, as illustrated in Figure 4. Compared with previous work that usually adopts a two-phase scheme to translate input sentences (Liu et al., 2016; Sennrich et al., 2017; Zhang et al., 2018), our decoding approach is more compact and effective. 4 The corpora includes LDC2000T50, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, and LDC2004T07. Following previous work, we also use case-insensitive tokenized BLEU to evaluate ChineseEnglish which have been segmented by Stanford word segmentation and Moses Tokenizer, respectively. 5 http://www.statmt.org/wmt14/translation-task.html. All preprocessed datasets and vocab can be directly download in tensor2tensor website https://drive.google.com/ open?id=0B_bZck-ksdkpM25jRUN2X2UxMm8. 6 This procedure is use"
Q19-1006,N16-1046,0,0.475567,"t al., 2017; Vaswani et al., 2017). Recent approaches to sequence-to-sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017), or attention (Vaswani et al., 2017) as basic building blocks. Typically, NMT adopts the encoder-decoder architecture and generates the target translation from left to right. Despite their remarkable success, NMT models suffer from several weaknesses (Koehn and Knowles, 2017). One of the most prominent issues is the problem of unbalanced outputs in which the translation prefixes are better predicted than the suffixes (Liu et al., 2016). We analyze translation accuracy of the first and last 4 tokens for left-to-right (L2R) and right-toleft (R2L) directions, respectively. As shown in Table 1, the statistical results show that L2R performs better in the first 4 tokens, whereas R2L translates better in terms of the last 4 tokens. This problem is mainly caused by the left-toright unidirectional decoding, which conditions each output word on previously generated outputs only, but leaving the future information from target-side contexts unexploited during translation. The future context is commonly used in reading and writing in h"
Q19-1006,W16-2323,0,0.380916,"n of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional NMT framework because they require two NMT models or decoders. Furthermore, the L2R and R2L decoders are independent from each other (Liu et al"
Q19-1006,P16-1162,0,0.883079,"n of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional NMT framework because they require two NMT models or decoders. Furthermore, the L2R and R2L decoders are independent from each other (Liu et al"
Q19-1006,D15-1166,0,0.080125,"ta. Results on Chinese-English Translation Effect of Fusion Mechanism We first investigate the impact of different fusion mechanisms with different λs on the development set. As shown in Table 2, we find that linear interpolation is sensitive to parameters λ. Nonlinear interpolation, which is more robust than linear interpolation, achieves the best performance when we use tanh with λ = 0.1. Compared with gate mechanism, nonlinear interpolation is much simpler and needs less parameters. Therefore, we will use nonlinear interpolation with tanh and λ = 0.1 for all experiments thereafter. • RNMT (Luong et al., 2015): it is a state-ofthe-art RNN-based NMT system with default setting. • Transformer: it has obtained the state-ofthe-art performance on machine translation, which predicts target sentence from left to right relying on self-attention (Vaswani et al., 2017). Translation Quality Table 3 shows translation performance for Chinese-English. Specifically, the proposed model significantly outperforms Moses, RNMT, Transformer, Transformer (R2L), Rerank-NMT, and ABD-NMT by 13.23, 8.54, 3.92, 4.90, 2.91, and 2.82 BLEU points, respectively. Compared with Transformer and Transformer (R2L), our model exhibits"
Q19-1006,P16-1159,0,0.0248224,"eling and optimizing with left-to-right and right-to-left decoding behaves better in leveraging bidirectional decoding. 4.5 DEV 35.28 35.22 36.38 Results on Russian-English Translation Table 5 shows the results of large-scale WMT18 Russian-English translation, and our approach still significantly outperforms the state-of-the-art Transformer model in development and test sets by 1.10 and 1.04 BLEU points, respectively. Note that the BLEU score gains of English-German and Russian-English are not as significant as that on Chinese-English. The underlying reasons, which have also been mentioned in Shen et al. (2016) and Zhang et al. (2018), are that (1) the Chinese-English datasets contain four reference translations for each source sentence while the English-German and Russian-English datasets only have a single reference; (2) English is more distantly related to Chinese than German and Russian, leading to the predominant improvements for Chinese-English translation when leveraging bidirectional decoding. Results on English-German Translation We further demonstrate the effectiveness of our model in WMT14 English-German translation tasks, and we also display the performances of some competitive models in"
Q19-1006,D16-1096,0,0.0356452,"sh translation tasks. L2R denotes left-to-right decoding and R2L means right-to-left decoding for conventional NMT. Figure 1: Illustration of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than the conventional N"
Q19-1006,D18-1342,0,0.0362405,"Missing"
Q19-1006,W17-4740,0,0.0201058,"e. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left"
Q19-1006,P16-1008,0,0.0277988,"IST Chinese-English translation tasks. L2R denotes left-to-right decoding and R2L means right-to-left decoding for conventional NMT. Figure 1: Illustration of the decoder in the synchronous bidirectional NMT model. L2R denotes left-to-right decoding guided by the start token hl2ri and R2L means right-to-left decoding indicated by the start token hr2li. SBAtt is our proposed synchronous bidirectional attention (see § 3.2). For instance, the generation of y3 does not only rely on y1 and y2 , but also depends on yn and yn−1 of R2L. (Xia et al., 2017), and it is crucial to avoid undertranslation (Tu et al., 2016; Mi et al., 2016). To alleviate the problems, existing studies usually used independent bidirectional decoders for NMT (Liu et al., 2016; Sennrich et al., 2016a). Most of them trained two NMT models with leftto-right and right-to-left directions, respectively. Then, they translated and re-ranked candidate translations using two decoding scores together. More recently, Zhang et al. (2018) presented an asynchronous bidirectional decoding algorithm for NMT, which extended the conventional encoder-decoder framework by utilizing a backward decoder. However, these methods are more complicated than"
Q19-1006,N13-1002,0,0.0189311,"::::: bomb :. they are developing a super-large scale , called the mother of the bomb . ::::::::::::::::::::::::: Table 8: Chinese-English translation examples of Transformer decoding in left-to-right and right-to-left way, and our proposed models. L2R performs well in the first half sentence, whereas R2L translates well in the second half :::::::::::: sentence. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backwar"
Q19-1006,C02-1050,0,0.382588,"r mother ::::: called::: the:::::: mother:: of:: a ::::: bomb :. they are developing a super-large scale , called the mother of the bomb . ::::::::::::::::::::::::: Table 8: Chinese-English translation examples of Transformer decoding in left-to-right and right-to-left way, and our proposed models. L2R performs well in the first half sentence, whereas R2L translates well in the second half :::::::::::: sentence. ::::::: Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009; Zhang et al., 2013). To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encod"
Q19-1006,P17-2060,1,0.820297,"(2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models. Similarly, some work attempted to re-rank the left-to-right decoding results by right-to-left decoding, leading to diversified translation results (Sennrich et al., 2016a; Hoang et al., 2017; Tan et al., 2017; Sennrich et al., 2017; Liu et al., 2018; Deng et al., 2018). Recently, Zhang et al. (2018) proposed asynchronous bidirectional decoding for NMT, which extended the conventional attentional encoder-decoder framework by introducing a backward decoder. Additionally, both Niehues et al. (2016) and Zhou et al. (2017a) combined the strengths of NMT and SMT, which can also be used to combine the advantages of bidirectional translation texts (Zhang et al., 2018). Compared with previous methods, our method has the following advantages: (1) We use a single model to achieve the goal of synchronous left-to-right and right-to-left decoding. (2) Our model can leverage and combine the two decoding directions in every layer of the Transformer decoder, which can run in parallel. (3) By using synchronous bidirectional attention, our model is an end-to-end joint framework and can optimize L2R and R2L decoding simultan"
Q19-1006,D17-1014,0,\N,Missing
Q19-1006,Q18-1011,0,\N,Missing
W02-0709,W02-0700,0,0.202904,"Missing"
W02-0709,P98-1070,0,0.037312,"Missing"
W02-0709,W97-0404,0,0.0454114,"Missing"
W02-0709,C00-2174,1,0.893786,"Missing"
W02-0709,C98-1067,0,\N,Missing
W03-1703,J96-1002,0,0.00581688,"Missing"
W03-1703,P98-1070,0,0.0704004,"Missing"
W03-1703,P99-1026,0,0.0653555,"Missing"
W03-1703,A97-1004,0,0.11641,"Missing"
W03-1703,A00-1012,0,0.0131681,"ent the utterance before further language processing. We believe that accurate segmentation can greatly improve the performance of language analysis modules. Stevenson et al. have demonstrated the difficulties of text segmentation through an experiment in which six people, educated to at least the Bachelor’s degree level, were required to segment into sentences broadcast transcripts from which all punctuation symbols had been removed. The experimental results show that humans do not always agree on the insertion of punctuation symbols, and that their segmentation performance is not very good (Stevenson and Gaizauskas, 2000). Thus it is a great challenge for computers to perform the task automatically. To solve this problem, many methods have been proposed, which can be roughly classified into two categories. One approach is based on simple acoustic criteria, such as nonspeech intervals (e.g. pauses), pitch and energy. We can call this approach acoustic segmentation. The other approach, which can be called linguistic segmentation, is based on linguistic clues, including lexical knowledge, syntactic structure, semantic information etc. Acoustic segmentation can not always work well, because utterance boundaries do"
W03-1703,C98-1067,0,\N,Missing
W06-0112,P98-1034,0,0.0900366,"Missing"
W06-0112,W00-0726,0,0.329952,"Missing"
W06-0112,N03-1028,0,0.159612,"ward-backward algorithm. We define a transition matrix as following: M i ( y ' , y |x ) = exp(∑ λ j f j ( y ' , y , x, i )) j Then, and let * denote component-wise matrix product, E p (Y |xk ,λ ) F (Y , xk ) = ∑ p (Y = y |xk , λ ) F ( y, xk ) y =∑ α i −1 ( fi ∗ M i ) βiT Z ( x) i Z ( x) = an ⋅1 T Where α i , βi as the forward and backward state-cost vectors defined by Conditional Random Fields ⎧α M α i = ⎨ i −1 i ⎩1 Lafferty et al.( 2001) present the Conditional Random Fields for building probabilistic models to segment and label sequence data, which was used effectively for base NP chunking (Sha & Pereira, 2003). Lafferty et al. (2001) point out that each of the random variable label sequences Y conditioned on the random observation sequence X. The joint distribution over the label sequence Y given X has the form p ( y |x, λ ) = 1 n +1 ∏ M i ( yi −1 , yi |x) Z ( x) i =1 p ( y |x, λ ) = 0&lt;i≤n i=0 ⎧ M i +1βiT+1 1 ≤ i &lt; n ,β = ⎨ i=n ⎩1 T i Sha & Pereira (2003) provided a thorough discussion of CRF training methods including preconditioned Conjugate Gradient, limitedMemory Quasi-Newton and voted perceptron. They also present a novel approach to model construction and feature selection in shallow parsing."
W06-0112,W95-0107,0,0.0488576,"opose a hybrid approach to extract the Chinese base NPs with the help of the conditional probabilities derived from the CRF algorithm and some appropriate grammar rules. According to our preliminary experiments on SVM and CRF, our approach outperforms both of them. The remainder of the paper is organized as follows. Section 2 gives a brief introduction of the data representations and methods. We explain our motivations of the hybrid approach in section 3. The experimental results and conclusions are introduced in section 4 and section 5 respectively. 2 Task Description 2.1 Data Representation Ramshaw and Marcus (1995) gave mainly two kinds of base NPs representation － the open/close bracketing and IOB tagging. For example, a bracketed Chinese sentence, [ 外商(foreign businessmen) 投资(investment)] 成 为 (become) [ 中 国 (Chinese) 外 贸 (foreign trade)] [ 重要(important) 增长点(growth)] 。 The IOB tags are used to indicate the boundaries for each base NP where letter ‘B’ means the current word starts a base NP, ‘I’ for a word inside a base NP and ‘O’ for a word outside a NP chunk. In this case the tokens for the former sentence would be labeled as follows: 外商/B 投资/I 成为/V 中国/B 外贸/I 重要/B 增长点/O 。/O Currently, most of the work"
W06-0112,P05-1001,0,0.0967018,"Pierce, 1998, 1999) applied a scoring method to select new rules and a naive heuristic for matching rules to evaluate the results' accuracy. CoNLL-2000 proposed a shared task (Tjong and Buchholz, 2000), which aimed at dividing a text in syntactically correlated parts of words. The eleven systems for the CoNLL-2000 shared task used a wide variety of machine learning methods. The best system in this workshop is on the basis of Support Vector Machines used by (Kudo and Matsumoto, 2000). Recently, some new statistical techniques, such as CRF (Lafferty et al. 2001) and structural learning methods (Ando and Zhang, 2005) have been applied on the base NP chunking. (Fei and Fernando, 2003) considered chunking as a sequence labeling task and achieved good performance by an improved training methods of CRF. (Ando and Zhang, 2005) presented a novel semisupervised learning method on chunking and produced performances higher than the previous best results. The research on Chinese Base NP Chunking is, however, still at its developing stage. Researchers apply similar methods of English Base NP chunking to Chinese. Zhao and Huang (1998) made a strict definition of Chinese base NP and put forward a quasi-dependency mode"
W06-0112,P98-1001,1,0.409711,"Missing"
W06-0112,W02-1818,0,\N,Missing
W06-0112,C98-1034,0,\N,Missing
W06-0112,C00-2124,0,\N,Missing
W06-0112,C98-1001,1,\N,Missing
W06-0112,N01-1025,0,\N,Missing
W10-4133,P08-1102,0,0.037602,"in performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and enhance the OOV words segmentation. However, the performance of out-of-domain text is still"
W10-4133,I05-3027,0,0.092844,"model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based discriminative model and a character-based generative model. The generative module holds a robust performance on IV words, while the discriminative module can handle the extra features easily and enhance the OOV words segmentation. However, the pe"
W10-4133,Y09-2047,1,0.808331,"here tk is a member of {Begin, Middle, End, Single} (abbreviated as B, M, E and S from now on) to indicate the corresponding position of character ck in its associated word. For example, the word “ 北京市 (Beijing City)” will be assigned with the corresponding tags as: “ 北 /B (North) 京/M (Capital) 市/E (City)”. This discriminative module can flexibly incorporate extra features and it is implemented with the ME package 1 given by Zhang Le. All training experiments are done with Gaussian prior 1.0 and 200 iterations. The character-based generative module is a character-tag-pair-based trigram model (Wang et al., 2009) and can be expressed as below: n P ([c, t ]1n ) ≈ ∏ P([c, t ]i [c, t ]ii −−12 ). (2) i =1 In our experiments, SRI Language Modeling Toolkit 2 (Stolcke, 2002) is used to train the generative trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998). The character-based joint model combines the above discriminative module and the generative module with log-linear interpolation as follows: Score(tk ) = α × log( P ([c, t ]k [c, t ]kk −−12 )) + (1 − α ) × log( P(tk tk −1 , ckk−+22 )) (3) Where the parameter α (0.0 ≤ α ≤ 1.0) is the weight for the generative model. Score(tk) will be"
W10-4133,C10-1132,1,0.778035,"Missing"
W10-4133,O03-4002,0,0.533507,"se Word Segmentation system for the closed track of CIPS-SIGHAN Word Segmentation Bakeoff 2010. This system adopts a character-based joint approach, which combines a character-based generative model and a character-based discriminative model. To further improve the crossdomain performance, we use an additional semi-supervised learning procedure to incorporate the unlabeled corpus. The final performance on the closed track for the simplified-character text shows that our system achieves comparable results with other state-of-the-art systems. 1 Introduction The character-based tagging approach (Xue, 2003) has become the dominant technique for Chinese word segmentation (CWS) as it can tolerate out-of-vocabulary (OOV) words. In the last few years, this method has been widely adopted and further improved in many previous works (Tseng et al., 2005; Zhang et al., 2006; Jiang et al., 2008). Among various character-based tagging approaches, the character-based joint model (Wang et al., 2010) achieves a good balance between in-vocabulary (IV) words recognition and OOV words identification. In this work, we adopt the character-based joint model as our basic system, which combines a character-based disc"
W10-4133,P06-2123,0,\N,Missing
W10-4149,A00-2018,0,0.370444,"Missing"
W10-4149,P05-1022,0,0.13125,"Missing"
W10-4149,J03-4003,0,0.160504,"Missing"
W10-4149,D09-1087,0,0.0134932,"ors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameter D ) can be expressed as: 1 We will use BerkeleyParser as our baseline parser, which is a PCFG-LA based parser. Feature templates The label of the current constituent; The label of the left most child, the middle child and the"
W10-4149,J93-2004,0,0.034718,"Missing"
W10-4149,N06-1020,0,0.0328367,"rted into every tag in target Treebank with various probabilities. So there is a converting matrix representing the converting probabilities, and we can calculate the converting matrix through source Treebank and N-best conversion trees. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets o"
W10-4149,P09-1006,0,0.0721924,"ing set. Experimental result shows their algorithm is effective. Collins et al. (1999) performed statistical constituency parsing of Czech on a Treebank that was converted from the Prague Dependency Treebank under the guidance of conversion rules and heuristic rules, and the final performance was also improved. Xia and Palmer (2001) proposed three methods to convert dependency trees into phrase structure trees with some hand-written heuristic rules. For acquisition of better conversion rules, Xia et al. (2008) proposed a method to automatically extract conversion rules from a target Treebank. Niu et al. (2009) tried to exploit heterogeneous Treebanks for parsing. They proposed a grammar formalism conversion algorithm to convert dependency formalism Treebank into phrase structure formalism, and did phrase structure parsing with the conversion trees. Their experiments are done in Chinese parsing, and the final performance is improved indeed. In summary, from the existing work we are confident that the strategies of self-training and Treebank conversion are effective to improve the performance of parser. 3 3.1 Our Strategy Parsing Algorithm Although self-training and Treebank Conversion are effective"
W10-4149,P06-1055,0,0.0279084,"e Treebank and N-best conversion trees. 3.3 Corpus weighting technique In line 12 of Algorithm 1, we train a new parser with target Treebank and conversion trees. However, the errors in automatically conversion trees are unavoidable and they would limit the accuracy of the self-trained model. So we have to take some measures to weight the gold target Treebank and the automatically conversion trees. McClosky et al. (2006) and Niu et al. (2009) take the strategy that duplicates the gold Treebank data many times. However, this strategy isn’t suitable for PCFG-LA parser 1 (Matsuzaki et al., 2005; Petrov et al., 2006), because PCFG-LA employs an EM algorithm in training stage, so duplicating gold Treebank would increase the training time tremendously. Instead, according to Huang and Harper (2009), we weight the posterior probabilities computed for the gold and automatically converted trees to balance their importance. Let count ( A o E |t ) be the count of rule A o E in a parse tree t . Tt and Ts ot are the sets of target Treebank and automatically converted trees from source Treebank respectively. The posterior probability of rule A o E (with weighting parameter D ) can be expressed as: 1 We will use Berk"
W10-4149,P94-1034,0,0.0930985,"train and development set Tdev (line 3). And we train an Algorithm 1 2:  initialize 3: {Ttrain , Tdev } m Split (Tt ) 5:  Iter iterations 6: for i m 1… Iter do i Ts ot m I 8: for k m 1… N do 10: 11: ParseList k m Nbest ( Parseri 1 , sk ) pˆ k i s ot T In line 10 of Algorithm 1, we select the highest quality parse pˆ k from ParseList k according to function Score( ps , ps ot ) , where ps denotes a tree in source Treebank and ps ot denotes a conversion tree with target Treebank grammar formalism for ps . Score( ps , ps ot ) compares taken ps as a reference. According to the idea proposed in Wang et al. (1994), we use the number of aligned constituents in the source and target trees to construct Score( ps , ps ot ) . We 4: Parser0 m Train(Ttrain , Tdev ) 9: Parse selection ps ot with ps and computes a score for ps ot 1: Input: Tt and Ts 7: 3.2 arg max p ParseList Score( ps , k , p j ) j k m pˆ k i 12: Parseri m Train (Ttrain , Tdev , Ts ot ) 13: return ParserIter initial parser with Ttrain and Tdev in line 4. From line 6 to line 12, we train parsers with SSPTC strategy Iter times iteratively. Let Tsiot be the automatically converted Treebank from source Treebank to target Treebank grammar formalis"
W10-4149,H01-1014,0,0.0715911,"Missing"
W10-4149,P04-1013,0,\N,Missing
W10-4149,P05-1010,0,\N,Missing
W10-4149,P99-1065,0,\N,Missing
W14-6831,mcnamee-etal-2010-evaluation,0,0.0302947,"Missing"
W17-6005,2007.mtsummit-papers.9,0,0.142563,"Missing"
W17-6005,C96-2141,0,0.0623041,"e term “interprocess communication”, c1 = “所以”, c2 = “只能”, c3 = “使用”, c4 = “进程”, c5 = “间”, c6 = “通讯”, e1 = “interprocess”, e2 = “communication”. And the left boundary is incorrectly recognized by our baseline system as c5 , namely, the target term is c5 c6 =“间 通 讯”. In order to correct the detection error, we enlarge or shrink the anchor from the left boundary to re-generate target terms, including the correct target term c4 c5 c6 =“进程 间 通讯”. Then, we select a best regenerated term which maximizes the joint probability according to Equation 8. In this work, the HMM-based word alignment model (Vogel et al., 1996) is employed to align words. 3.1 Experimental Setup All the experiments are conducted on our in-house developed MT toolkit which has a typical phrasebased decoder (Xiong et al., 2006) and a series of tools, including word alignment and phrase table extraction. We test our method on English-to-Chinese translation in the field of software localization. The training data (1,199,589 sentences) and annotated test data (1,100 sentences) are taken from Microsoft Translation Memory, which is a domain-specific dataset. And additional data employed by this paper includes: the seed dictionary (102,308 so"
W17-6005,W04-3250,0,0.0151133,"nspermia Determinism Target 拂多诞 威卡尔 弗朗西斯达希武德 宗教学 宗教科学引论 宗教史学 宗教现象学 无法则一元论 感质 泛种论 决定论 Table 1: Extracted English-Chinese term translation candidates crosoft Terminology Collection), Chinese Pinyin table (7,809 Chinese characters3 ). The gold standard of term translation of test data are human annotated. All the MT systems are tuned by the development set (1,000 sentences) using ZMERT (Zaidan, 2009) with the objective to optimize BLEU (Papineni et al., 2002). The higher the BLEU score, the better the translation is. And the statistical significance test is performed by the re-sampling approach (Koehn, 2004). 3.2 In contrast to the baseline approach, the figures in Table 2 show that the precision of Chinese terms has been increased by 2.9 points, and the precision of term pairs has been increased by 4.1 points. Thus, according to the bold figures in Table 2, we can draw a conclusion that term extraction can be substantially increased by the proposed framework. (2) The SMT Translation Tests Secondly, we test whether the proposed framework can further improve the performance of term and sentence translation, compared with the baseline system. The strong baseline system, e.g., well tuned Moses, is d"
W17-6005,P07-2045,0,0.00735632,"Missing"
W17-6005,P06-1066,0,0.0181661,"ecognized by our baseline system as c5 , namely, the target term is c5 c6 =“间 通 讯”. In order to correct the detection error, we enlarge or shrink the anchor from the left boundary to re-generate target terms, including the correct target term c4 c5 c6 =“进程 间 通讯”. Then, we select a best regenerated term which maximizes the joint probability according to Equation 8. In this work, the HMM-based word alignment model (Vogel et al., 1996) is employed to align words. 3.1 Experimental Setup All the experiments are conducted on our in-house developed MT toolkit which has a typical phrasebased decoder (Xiong et al., 2006) and a series of tools, including word alignment and phrase table extraction. We test our method on English-to-Chinese translation in the field of software localization. The training data (1,199,589 sentences) and annotated test data (1,100 sentences) are taken from Microsoft Translation Memory, which is a domain-specific dataset. And additional data employed by this paper includes: the seed dictionary (102,308 source words2 , 24,094 terms from Mi2 http://www.mdbg.net/chindict/chindict.php?page=cccedict 42 Source Mihr-Ohrmazd Wicca Francis Dashwood Religious Studies Introduction to the Science"
W17-6005,zhang-etal-2008-comparative,0,0.0550332,"Missing"
W17-6005,E09-1057,0,0.0159903,"the important boundary information of terms and can be adopted as training data of term recognizers. s ∈page where |page |refers to the number of sentences in this page. In this paper, the default values of λ are set to the following weights: λ1 = λ2 = 0.2, λ3 = λ4 = 0.3. 2.3 Term Translation Knowledge Extractor In order to extract bilingual term translation candidates, the key task is to identify the left boundary of a target term. However, traditional term recognition methods employing statistical measures to rand the candidates terms (n-gram sequences), such as log likelihood (Cohen, 1995; Lefever et al., 2009), TF-IDF (Evans and Lefferts, 1995; Medelyan and Witten, 2006), C-value/NCvalue (Frantzi et al., 2000) and many others (Ahmad et al., 2000; Park et al., 2002; Kozakov et al., 2004; Sclano and Velardi, 2007; Zhou et al., 2008; Zhang et al., 2008; Kostoff et al., 2009), leads to very low recall for some domains. What’s worse, some approaches apply frequency threshold to reduce the algorithm’s search space by filtering out low frequency term candidates. Such methods have not taken into account Zipf’s law, again leading to the reduced recall. In this paper, to achieve a higher recall, we adopt nat"
W17-6005,P02-1040,0,0.0983223,"Ohrmazd Wicca Francis Dashwood Religious Studies Introduction to the Science of Religion History of Religions Phenomenology of Religion anomalous monism qualia Panspermia Determinism Target 拂多诞 威卡尔 弗朗西斯达希武德 宗教学 宗教科学引论 宗教史学 宗教现象学 无法则一元论 感质 泛种论 决定论 Table 1: Extracted English-Chinese term translation candidates crosoft Terminology Collection), Chinese Pinyin table (7,809 Chinese characters3 ). The gold standard of term translation of test data are human annotated. All the MT systems are tuned by the development set (1,000 sentences) using ZMERT (Zaidan, 2009) with the objective to optimize BLEU (Papineni et al., 2002). The higher the BLEU score, the better the translation is. And the statistical significance test is performed by the re-sampling approach (Koehn, 2004). 3.2 In contrast to the baseline approach, the figures in Table 2 show that the precision of Chinese terms has been increased by 2.9 points, and the precision of term pairs has been increased by 4.1 points. Thus, according to the bold figures in Table 2, we can draw a conclusion that term extraction can be substantially increased by the proposed framework. (2) The SMT Translation Tests Secondly, we test whether the proposed framework can furth"
W17-6005,C02-1142,0,0.0123359,"is page. In this paper, the default values of λ are set to the following weights: λ1 = λ2 = 0.2, λ3 = λ4 = 0.3. 2.3 Term Translation Knowledge Extractor In order to extract bilingual term translation candidates, the key task is to identify the left boundary of a target term. However, traditional term recognition methods employing statistical measures to rand the candidates terms (n-gram sequences), such as log likelihood (Cohen, 1995; Lefever et al., 2009), TF-IDF (Evans and Lefferts, 1995; Medelyan and Witten, 2006), C-value/NCvalue (Frantzi et al., 2000) and many others (Ahmad et al., 2000; Park et al., 2002; Kozakov et al., 2004; Sclano and Velardi, 2007; Zhou et al., 2008; Zhang et al., 2008; Kostoff et al., 2009), leads to very low recall for some domains. What’s worse, some approaches apply frequency threshold to reduce the algorithm’s search space by filtering out low frequency term candidates. Such methods have not taken into account Zipf’s law, again leading to the reduced recall. In this paper, to achieve a higher recall, we adopt naturally annotated resources for term In this paper, we design following features for the term recognizer: the four words immediately to the left of the term,"
Y09-1025,W06-2927,0,0.342618,"he conference declared that decision chair DT NN IN DT NN VBD DT NN of the RH_NMOD O LH_NMOD RH_NMOD O O RH_NOMD O conference declared The chair of the conference declared that decision that decision DT NN IN DT NN VBD DT NN RH_NMOD O LH_NMOD RH_NMOD O O RH_NOMD O r o o r o o r o (b) (a) Figure 1: Example of (a) the sequential neighboring relation labeling, (b) the reduce decision labeling. Wu et al. (2007) employed linear chain conditional random fields (CRFs) as the labeling algorithm to capture the higher order features and avoid the greedy search when labeling with sequential classifiers (Cheng et al., 2006). To prevent the error propagation, they regarded the labeling results as features of the subsequent parsing stage instead of reducing the child words. However, this weakens the strength that neighboring parsing can provide. In our approach, besides the CRF-based relation labeler, an additional tagger is introduced to examine whether a dependent child can be reduced, i.e., whether it has found its head and has already been a complete sub-tree. The reduce tagger tries to guarantee safe reductions and ensures the parsed structures can be formed into a tree after several passes of analysis. In Fi"
Y09-1025,P08-2060,0,0.0138192,"tors on the English dataset. However, considering the parsing efficiency, the LDParsers are quite competitive. They have 235 lower complexity than graph-based models and accordingly parse faster than them under the current implementations in projective case. Transition-based models can be implemented in linear time but SVMs which have been proved to achieve the highest performance in parser learning (Cheng et al., 2005; Wang et al., 2006) are not regarded as fast algorithms especially when the number of classes is large. The Classifier Splitting heuristic strategy and SVM speeding up methods (Goldberg and Elhadad, 2008) are gold choices to accelerate these implementations. However, even considering these cases, the parsing speed of the proposed LDParsers (up to 480 English words per second) is still desirable. Moreover, the speed boosting of SVMs is usually accompanied with the decrease of the accuracies or more memory consumption. 3.2 Chinese Results We compared LDParser (LDP1div) with MaltParser, Yamada03, MSTParser and Duan07 in the Chinese experiments. Arc-standard algorithm (Nivre, 2004) is adopted in MaltParser because the experiments on the development set revealed that it got a higher performance tha"
Y09-1025,P06-2041,0,0.0916894,"RA is the proportion of sentences in which the root word is correctly identified. All the metrics are calculated excluding the punctuations besides CM. We also present the detailed comparisons with the baselines in aspects of the computational complexity and the testing time (the CPU time). All the experiments were done on a 32-bit Intel Xeon 2.33GHz processor. 3.1 English Results In the English experiments, all the parsers listed above except Duan07 were compared. For MaltParser, we chose the arc-eager algorithm (Nivre, 2004) and the feature set which got the best performance for English in (Hall et al., 2006) (the feature model Φ5 in their work). Hall et al. (2006) reported that the SVMs learning algorithm outperformed memory-based learning (MBL) on this feature set and could parse faster. It is the same case for Chinese. Therefore, SVMs were used for both our English and Chinese experiments. We also compared the split MaltParser which utilizes the efficient Classifiers Splitting in the experiment where the POS tag of the next input token was selected for splitting and the split threshold was 1,000. For Yamada03, the optimal feature context window size six was chosen and the dependency relation ty"
Y09-1025,D07-1097,0,0.0859957,"get the final dependency graph of the input sentence. In each layer, the neighboring dependency relations and reduce decisions are traded off at different 232 sequence positions to obtain a globally optimal depth-one dependency sub-graph. Between the layers, the pre-built structure is handed on through the surviving tokens as well as their children. Since dependency relations only exist between two consecutive tokens, the child appearing in the observation sequence is always the leftmost or rightmost one of the parent token. Previous work based on deterministic models (Nivre and Scholz, 2004; Hall et al., 2007) has verified that the information of the children at these positions is more useful than that of others. For training, the parsing process described above is repeated on each sentence in the training set to pick up instances on different layers. In addition, the reduce examiner in the two-time labeling algorithm described above relies too much on the relation labeling results since it takes the relation labels as features. Therefore, a one-time labeling framework is introduced to be an alteration of the two-time labeling one. Figure 4 shows an example. The strings in the third column are the"
Y09-1025,D07-1123,0,0.033886,"Missing"
Y09-1025,N01-1025,0,0.0339261,"under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. Copyright 2009 by Ping Jian and Chengqing Zong 23rd Pacific Asia Conference on Language, Information and Computation, pages 230–239 230 It is well known that chunking, which is deemed to be a useful and tractable precursor to full parsing, has been successfully handled by sequence labeling techniques (Kudo and Matsumoto, 2001; Sha and Pereira, 2003). Inspired by this scheme, we adopt the globally optimal sequence labeling to search the best depth-one sub-graph in the dependency layer. We believe that the line-typed sequential models are potent complementarities to the tree-typed hierarchical ones or even the latent substitutes. The experiments show that our layer-based parser yields comparable dependency attachment accuracies to the state-of-the-art dependency parsers on both English and Chinese datasets. Especially, it is quite efficient due to the layer-based search and sequence typed analysis. The remainder of"
Y09-1025,J93-2004,0,0.0313491,"Missing"
Y09-1025,P05-1012,0,0.286404,"graph-based models which search for a whole dependency graph and alleviates the error propagation that transition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed. Keywords: dependency parsing, dependency layer, sequence labeling 1 Introduction Graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors. Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence"
Y09-1025,E06-1011,0,0.192425,"h search for a whole dependency graph and alleviates the error propagation that transition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed. Keywords: dependency parsing, dependency layer, sequence labeling 1 Introduction Graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors. Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence (the whole dependency graph)"
Y09-1025,D07-1013,0,0.0202094,"nce labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed. Keywords: dependency parsing, dependency layer, sequence labeling 1 Introduction Graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors. Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence (the whole dependency graph) both in training and inference procedures, which results in a cubic computational complexity (in projective case). By contrast, transition-based approaches train a class"
Y09-1025,W04-0308,0,0.400648,"eriments were evaluated on the Penn Chinese Treebank (CTB) version 5.0 (Xue et al., 2005). The corpus was split into training, development, and testing data as Duan et al. (2007) did to balance the different resources. 16,079 sentences were for training, 803 for development, and 1,905 (about 50,319 words) for testing. The head-finding rules and dependency type set also followed Hall et al. (2006). 2 Gold standard POS tags were used. Eight parsers involved in our main experiments are concisely introduced as following: MaltParser (Nivre et al., 2006): adopts transition-based model described in (Nivre, 2004). Here, MaltParser version 1.1 is employed. Yamada03: our implementation of another typical transition-based model proposed in (Yamada and Matsumoto, 2003). MSTParser1: The first-order paradigm of MSTParser3 which implements the graph-based models described in (McDonald et al., 2005; McDonald and Pereira, 2006). Version 0.2 is used. MSTParser2: The second-order paradigm of MSTParser. Duan07: A probabilistic parsing action model proposed by Duan et al. (2007) which globally seeks the optimal action sequence above the transition-based model described in (Yamada and Matsumoto, 2003) with beam sea"
Y09-1025,C04-1010,0,0.241793,"sition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed. Keywords: dependency parsing, dependency layer, sequence labeling 1 Introduction Graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors. Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence (the whole dependency graph) both in training and inference procedures, which results in a cubic computation"
Y09-1025,nivre-etal-2006-maltparser,0,0.0676997,"Missing"
Y09-1025,P08-1108,0,0.0191875,"n transition-based models but our parser introduced the sequence labeling technique. Some existing work tried to combine the graph-based and transition-based models for dependency parsing. Sagae and Lavie (2006) built a graph-based model to reparse a dependency graph of which the arc scores were created by the outputs of three transition-based parsers. Hall et al. (2007) followed Sagae’s methodology and blended six transition-based parsers. This kind of combinations can be seen as a structural voting of the graph-based and transition-based models. A more effective integration was developed by Nivre and McDonald (2008) who treated the outputs of one model as features for the other. However, all these combination approaches just made use of the outputs of the component parsers without modifying their structures or parsing algorithms. It is quite different from ours. Our parsing framework inherits the benefits of the graph-based and transition-based models with both new structure and algorithm. Cheng et al. (2006) and Wu et al. (2007) used neighboring dependency attachment taggers to improve the performance of the deterministic parser. In Cheng’s method, neighboring relations were decided by greedy sequential"
Y09-1025,W96-0213,0,0.342085,"Missing"
Y09-1025,N06-2033,0,0.0213639,"iple passes 237 (layers) to form the dependency tree, and integrates global search like Johansson and Nugues (2007) and Duan et al. (2007) did to find the optimal combination of dependency relations. However, they scored all the graph space as what graph-based models do while we focalized it in a parsing layer for the sake of efficiency. In addition, they inherited the hierarchical analyzing mechanism used in transition-based models but our parser introduced the sequence labeling technique. Some existing work tried to combine the graph-based and transition-based models for dependency parsing. Sagae and Lavie (2006) built a graph-based model to reparse a dependency graph of which the arc scores were created by the outputs of three transition-based parsers. Hall et al. (2007) followed Sagae’s methodology and blended six transition-based parsers. This kind of combinations can be seen as a structural voting of the graph-based and transition-based models. A more effective integration was developed by Nivre and McDonald (2008) who treated the outputs of one model as features for the other. However, all these combination approaches just made use of the outputs of the component parsers without modifying their s"
Y09-1025,N03-1028,0,0.317107,"60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. Copyright 2009 by Ping Jian and Chengqing Zong 23rd Pacific Asia Conference on Language, Information and Computation, pages 230–239 230 It is well known that chunking, which is deemed to be a useful and tractable precursor to full parsing, has been successfully handled by sequence labeling techniques (Kudo and Matsumoto, 2001; Sha and Pereira, 2003). Inspired by this scheme, we adopt the globally optimal sequence labeling to search the best depth-one sub-graph in the dependency layer. We believe that the line-typed sequential models are potent complementarities to the tree-typed hierarchical ones or even the latent substitutes. The experiments show that our layer-based parser yields comparable dependency attachment accuracies to the state-of-the-art dependency parsers on both English and Chinese datasets. Especially, it is quite efficient due to the layer-based search and sequence typed analysis. The remainder of the paper is organized a"
Y09-1025,P06-1054,0,0.0209446,"them more in root accuracy. Thanks to the global search over the whole dependency tree the graph-based models realized by MSTParser gain the best performance among the competitors on the English dataset. However, considering the parsing efficiency, the LDParsers are quite competitive. They have 235 lower complexity than graph-based models and accordingly parse faster than them under the current implementations in projective case. Transition-based models can be implemented in linear time but SVMs which have been proved to achieve the highest performance in parser learning (Cheng et al., 2005; Wang et al., 2006) are not regarded as fast algorithms especially when the number of classes is large. The Classifier Splitting heuristic strategy and SVM speeding up methods (Goldberg and Elhadad, 2008) are gold choices to accelerate these implementations. However, even considering these cases, the parsing speed of the proposed LDParsers (up to 480 English words per second) is still desirable. Moreover, the speed boosting of SVMs is usually accompanied with the decrease of the accuracies or more memory consumption. 3.2 Chinese Results We compared LDParser (LDP1div) with MaltParser, Yamada03, MSTParser and Duan"
Y09-1025,D07-1131,0,0.0642762,"n the latent substitutes. The experiments show that our layer-based parser yields comparable dependency attachment accuracies to the state-of-the-art dependency parsers on both English and Chinese datasets. Especially, it is quite efficient due to the layer-based search and sequence typed analysis. The remainder of the paper is organized as follows: Section 2 describes the details of the algorithm and feature set. Section 3 presents the experimental results. The related work is discussed in Section 4. Conclusion and future work comprise Section 5. 2 2.1 Layer-based Parsing Approach Algorithms Wu et al. (2007) designed a neighbor parser to identify the neighboring parent-child relations between two consecutive tokens in the input sentence. Following their framework we label the dependency relations in our parsing layer. An example is shown in Figure 1(a). The first and second columns represent the words and part-of-speech (POS) tags respectively. The third column implies whether the token modifies its left neighbor (LH, left-headed) or right neighbor (RH, right-headed) or neither (O). The string behind the character “_” indicates the dependency type of the neighboring link. The The chair of the con"
Y09-1025,W03-3023,0,0.331083,"error propagation that transition-based models suffer from to some extent. Furthermore, our parser adopts the sequence labeling models to find the optimal sub-graph of the layer which demonstrates that the sequence labeling techniques are also competent for hierarchical structure analysis tasks. Experimental results indicate that the proposed approach offers desirable accuracies and especially a fast parsing speed. Keywords: dependency parsing, dependency layer, sequence labeling 1 Introduction Graph-based models (McDonald et al., 2005; McDonald and Pereira, 2006) and transitionbased models (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) are two dominant paradigms in the dependency parsing community. McDonald and Nivre (2007) have made elaborate analyses about the very different theoretical properties of these two kinds of models and the corresponding experimental behaviors. Generally, graph-based approaches learn a model for scoring possible dependency graphs of an input sentence and apply exhaustive search algorithms to find the one that maximizes the score. The unit graph-based models calculate is the whole sentence (the whole dependency graph) both in training and inference procedures, which resul"
Y09-1035,P07-1083,0,0.0145782,"e domain, and desire a model that performs well in the target domain. Wu et al. (2008) trained a baseline system using out-of-domain corpora and then used in-domain resource to improve the in-domain performance. The development set is composed of in-domain data and out-of-domain data. We select the sentences which are domain-matched and use them to get optimal parameters. The concept of similarity is very important in NLP applications, such as the information extraction, information retrieval and document clustering. There are many similarity measures have been proposed by former researchers. Bergsma and Kondrak (2007) proposed an alignment-based discriminative framework for string similarity. They gathered features from substring pairs consistent with a character-based alignment of the two strings. Li et al. (2006) presented an algorithm which takes account of semantic information and word order information implied in the sentence to calculate the similarity between very short texts of sentence length. Budanitsky and Hirst (2006) evaluated five of WordNet based semantic measures, by comparing their performance in detecting and correcting real-word spelling errors. However, there are some differences betwee"
Y09-1035,J06-1003,0,0.00805063,"in NLP applications, such as the information extraction, information retrieval and document clustering. There are many similarity measures have been proposed by former researchers. Bergsma and Kondrak (2007) proposed an alignment-based discriminative framework for string similarity. They gathered features from substring pairs consistent with a character-based alignment of the two strings. Li et al. (2006) presented an algorithm which takes account of semantic information and word order information implied in the sentence to calculate the similarity between very short texts of sentence length. Budanitsky and Hirst (2006) evaluated five of WordNet based semantic measures, by comparing their performance in detecting and correcting real-word spelling errors. However, there are some differences between our task and the common similarity measures. If we can find similar sentences for each sentence in the test set, we are sure that the parameters trained on them can improve the performance significantly. However, in the limited scale of the development set, the sparse data problem becomes too serious to find a similar sentence for each sentence in the test set. The normal string or semantic similarity measures are"
Y09-1035,P07-1033,0,0.136909,"Missing"
Y09-1035,N03-1017,0,0.00409841,"s in the DEV by giving each sentence a score. At last, we select the New DEV based on the score. There are two basic methods to calculate the similarity. One is based on the surface features, such as the words and phrases; the other is based on some deeper features, such as the sentences structure. We propose similarity measures to select the sentences in these two ways respectively. Figure 1: Framework of selecting development set 3.1 Phrase-based Method For the phrase-based statistical machine translation model, the basic translate unit is phrase, that is to say, a continuous word sequence (Koehn et al., 2003). It is a natural idea that using the phrase to measure the similarity between the test set and the development set. If the sentences which we selected contain more phrases in the test set, the sentences are more similar to the test set. Then we try to select sentences from the development set which can cover more phrases of the test set corpus. In this method, the phrases in test set play a vital role. So, firstly we extract all the phrases from the test set and assign them different weights. We take two aspects into account to estimate the weight of phrase: the information it contained and t"
Y09-1035,P03-1056,0,0.0276269,"Missing"
Y09-1035,D09-1074,0,0.0150648,"a problem becomes too serious to find a similar sentence for each sentence in the test set. The normal string or semantic similarity measures are not suit for our task. Some researchers tried to optimize the training set to improve the performance of translation system. Yasuda et al. (2008) selected the training set for translation model training using linear translation model interpolation and a language model technique. They focused on the training set, and they used the fixed weights in the translation. On the contrary, we focus on the development set, and use parameters optimized by MERT. Matsoukas et al. (2009) proposed a discriminative training method to assign a weight for each sentence in the training set. In this way, they limited the negative effects of low quality training data. We pay more attention to the parameter estimation, and eager to get a group of optimized parameters for the translation model. 3 Criterion for Selecting Development Data As mentioned above, the normal similarity measures on sentence level are not easy to be applied for our task because of the sparse data problem. In order to achieve our purpose, we need to select some sentences which are similar to the entire test set."
Y09-1035,P03-1021,0,0.00946566,"T system. Keywords: Development set selecting, Domain adaption, Phrase-based statistical machine translation, Similarity measure 1 Introduction In recent years, the phrase-based statistical machine translation model obtains more attention and achieves good translation performance. But the quality of the translation results is greatly influenced by the model’s parameters. How to get a group of parameters which can make good translation results becomes a problem which we focus on. In general, we could get the optimized parameters though minimum error rate training (MERT) on the development set (Och, 2003). The MERT will continue running until the BLEU score on the development set convergences and then we translate the test set to the target language using this group of parameters. Since we get the optimized parameters on the development set, it becomes an important factor to the quality of the final translation results. Usually, we run the MERT on all of the development set, but when the development set is in large-scale and there are many long sentences included in it, the MERT will consume too long time on translation and parameters’ adjusting. Nevertheless we can not sure whether the parame"
Y09-1035,I08-2088,0,0.0201839,"ling errors. However, there are some differences between our task and the common similarity measures. If we can find similar sentences for each sentence in the test set, we are sure that the parameters trained on them can improve the performance significantly. However, in the limited scale of the development set, the sparse data problem becomes too serious to find a similar sentence for each sentence in the test set. The normal string or semantic similarity measures are not suit for our task. Some researchers tried to optimize the training set to improve the performance of translation system. Yasuda et al. (2008) selected the training set for translation model training using linear translation model interpolation and a language model technique. They focused on the training set, and they used the fixed weights in the translation. On the contrary, we focus on the development set, and use parameters optimized by MERT. Matsoukas et al. (2009) proposed a discriminative training method to assign a weight for each sentence in the training set. In this way, they limited the negative effects of low quality training data. We pay more attention to the parameter estimation, and eager to get a group of optimized p"
Y09-1035,C08-1125,1,\N,Missing
Y09-2016,E09-1011,0,0.0561839,"e rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences with syntactic reordering rules so as to m"
Y09-2016,P08-1009,0,0.019079,"eordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences"
Y09-2016,P05-1066,0,0.349976,"ior to translation, and then instead of reordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is"
Y09-2016,2007.mtsummit-papers.29,0,0.027998,"sentence list for decoding. The former method depends much on the author’s professional knowledge in linguistics and the performance in parsing technology. The latter approach is more robust to the errors in parsing stage but increases the burden of decoding as it has to translate an n-best sentences, and furthermore, it might still produce pre-reordering errors prior to translation because the n-best list includes only part of but not all of the reordering hypotheses. It should be noted that both the two methods are implemented directly in parse trees, and it is pointed out in previous work (Habash, 2007) that ∗ We would like to thank Yu Zhou for her suggestions to revise the earlier draft and thank anonymous reviewers for their helpful comments. The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. Copyright 2009 by Jiajun Zhang and Chengqing Zong 23rd Pacific Asia Conf"
Y09-2016,P03-1054,0,0.00948754,"Experimental Settings We carried out the experiments on Chinese-to-English translation using NIST05 test set. The development set including 571 Chinese sentences is chosen from the test set of NIST06 and NIST08. The training set consists of 297K parallel sentences which are filtered from LDC. Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results Before giving the experimental results, some notations of our new systems are first introduced here. The system INcorporating the Hard Rules into the Modified Baseline Decoder is named 585 IN-HR-MBDecoder. Likewise, IN-SR-MBDecoder"
Y09-2016,W04-3250,0,0.03522,"Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 2004). 5.3 Experimental Results Before giving the experimental results, some notations of our new systems are first introduced here. The system INcorporating the Hard Rules into the Modified Baseline Decoder is named 585 IN-HR-MBDecoder. Likewise, IN-SR-MBDecoder is used to denote the system incorporating the soft rules into modified baseline decoder. In Table 2, we present our results. Like (Wang et al., 2007) and (Zhang et al., 2007), we find that reordering the source sentences whether with hard rules or with soft rules can both obtain a significant improvement over the baseline MEBTG by 0.58 an"
Y09-2016,P07-1091,0,0.258903,"on, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the source sentences with syntactic reordering rules so as to make the input much closer to the target language in word order. (Collins et al., 2005; Wang et al, 2007 and Badr et al., 2009) used hard syntactic rules (namely manually created) obtained from source parse trees to directly reorder the input sentences. (Li et al., 2007) employed soft syntactic rules (namely probabilistic) to get an n-best reordered sentence list for decoding. The former method depends much on the author’s professional knowledge in linguistics and the performance in parsing technology. The latter approach is more robust to the errors in parsing stage but increases the burden of decoding as it has to translate an n-best sentences, and furthermore, it might still produce pre-reordering errors prior to translation because the n-best list includes only part of but not all of the reordering hypotheses. It should be noted that both the two methods"
Y09-2016,P00-1056,0,0.0361074,"Rules Pre-Reordering. Likewise, the system using soft rules is called MEBTG+SRP indicating MEBTG system with Soft Rules Pre-reordering (only 1-best reordered source sentence used for source-side of training data and 10-best for test data). 5.2 Corpora and Experimental Settings We carried out the experiments on Chinese-to-English translation using NIST05 test set. The development set including 571 Chinese sentences is chosen from the test set of NIST06 and NIST08. The training set consists of 297K parallel sentences which are filtered from LDC. Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The target fourgram language model was built with the English part of training data using the SRI Language Modeling Toolkit (Stolcke, 2002). In order to acquire syntactic rules, we parse the Chinese sentence using the Stanford parser (Klein and Manning, 2003) with its default Chinese grammar. We built the maximum entropy model with a MaxEnt Toolkit developed by (Zhang, 2004). All the models were optimized and tested using the case-sensitive BLEU-4 with “shortest” reference length. Statistical significance in BLEU score difference was measured by using paired bootstrap re-sampling (Koehn, 200"
Y09-2016,P03-1021,0,0.0354385,"O is employed. To emphasize the importance of syntactic phrase reordering, we further create another feature to enhance syntactic reordering (because weights tuning cannot promise the weight of syntactic reordering model bigger and more importance than that of non-syntactic reordering model). The final score of merging rules are calculated as follows: ; ; 78 L / & MO ;N ·PQ R · MS ;T ·PU R · WS <X · 7GHI 2 (5) In which WS is a binary feature in order to reward syntactic reordering and it equals to 1 if ΩS is active. All the ten feature weights J ~J in our new model are tuned with MERT (Och, 2003). 4.3 Algorithm of Integrating Syntactic Rules After knowing the translation model and the decoding algorithm we have used, the most important thing we care about is how to integrate the syntactic rules during decoding. The ultimate format of syntactic rule we adopt is designed as (spanN  , spanN  , Pi *, and the merging rules used in decoding always handle two continuous phrases, so if spanN  and spanN  are successive, then Pi will be used to replace the syntactic reordering score MS which is predicted with lexical boundary words in baseline. However, spanN  and spanN  584 will"
Y09-2016,D07-1077,0,0.408572,"d then instead of reordering the source sentence directly, the rules are used as a strong feature integrated into our elaborately designed model to help phrase reordering in the decoding stage. The experiments on NIST Chinese-to-English translation show that our approach, whether incorporating hard or soft rules, significantly outperforms the previous methods. Keywords: hard syntactic rules, soft syntactic rules, effective integration, phrase-based translation 1 Introduction Adding syntax into phrase-based translation has become a hot research topic. Many works, such as (Collins et al., 2005; Wang et al., 2007; Cherry 2008; Marton and Resnik, 2008; and Badr, 2009), have investigated how to use the linguistic information in phrase-based SMT and empirically proved that syntactic knowledge is very helpful to improve translation performance especially in phrase reordering. For example, in Chinese-to-English translation, the Chinese phrase PP-VP is translated into English VP-PP in most cases. Thus, if a special rule is designed to deal with the case of this kind, the translation result will be better. The popular way of integrating the linguistic information into phrase reordering is to reorder the sour"
Y09-2016,J97-3002,0,0.010852,"nslation is a source phrase but not a parse tree node, we have to make a conversion from tree nodes to source phrases in order to incorporate the syntactic rules. Since each tree node can be projected to be a span on the source sentence, we can just use spans to denote the tree nodes. Finally, any syntactic rule can be represented as a triple (spanN , spanN , Pi *. 4 Integrating Syntactic Rules We integrate the syntactic rules into a phrase-based SMT to help the decoder performs more linguistically. In this paper, we choose the decoder with Bracket Transduction Grammar (BTG) style model (Wu, 1997; Xiong et al., 2006) as our baseline. 4.1 BTG-based Model The BTG-based translation can be viewed as a monolingual parsing process, in which only lexical rules / 0 1, 2 and two binary merging rules / 0 3/4 , /5 6 and / 0 (/4 , /5 * are allowed. During decoding, the source sentence is first divided into phrase sequence, then the lexical rule / 0 1, 2 translates the source phrase 1 into target phrase 2 and forms a block /. The 583 straight rule / 0 3/4 , /5 6 and the inverted rule / 0 (/4 , /5 * merge the two neighboring blocks into a bigger one until the whole source sentence is covered. It"
Y09-2016,P06-1066,0,0.729605,"s a source phrase but not a parse tree node, we have to make a conversion from tree nodes to source phrases in order to incorporate the syntactic rules. Since each tree node can be projected to be a span on the source sentence, we can just use spans to denote the tree nodes. Finally, any syntactic rule can be represented as a triple (spanN , spanN , Pi *. 4 Integrating Syntactic Rules We integrate the syntactic rules into a phrase-based SMT to help the decoder performs more linguistically. In this paper, we choose the decoder with Bracket Transduction Grammar (BTG) style model (Wu, 1997; Xiong et al., 2006) as our baseline. 4.1 BTG-based Model The BTG-based translation can be viewed as a monolingual parsing process, in which only lexical rules / 0 1, 2 and two binary merging rules / 0 3/4 , /5 6 and / 0 (/4 , /5 * are allowed. During decoding, the source sentence is first divided into phrase sequence, then the lexical rule / 0 1, 2 translates the source phrase 1 into target phrase 2 and forms a block /. The 583 straight rule / 0 3/4 , /5 6 and the inverted rule / 0 (/4 , /5 * merge the two neighboring blocks into a bigger one until the whole source sentence is covered. It is natural to adopt a"
Y09-2016,C08-1127,0,0.0117942,"t list. However, all these methods are separated from decoder and reorder the source sentences arbitrarily prior to translation. Once a pre-reordering error happens, it is very difficult to make up for the mistake in later translation steps. In our approach, we just retain the syntactic rules rather than use them to reorder the source sentences directly. During decoding, the syntactic rules will serve as a strong feature to guide and enhance the phrase reordering. Zhang et al., (2007) only allowed reordering between syntactic phrases and enforced the non-syntactic phrases translated in order. Xiong et al. (2008) proposed a linguistically annotated BTG for SMT. The method used some heuristic rules to linguistically annotate every source phrase with the source-side parse tree in decoding and built a linguistical reordering model. The two approaches both acquired and applied the syntactic rules in the decoding stage but meanwhile increased the decoding time to a large extent. Our work differs from theirs in three ways. First, when translating a test sentence, we obtain the corresponding syntactic rules prior to translation rather than in decoding stage and thus alleviate the decoding complexity. Second,"
Y09-2016,D07-1056,0,0.0874122,"English translation to determine whether the children of a node should be reordered or not, and finally to obtain a reordered n-best list. However, all these methods are separated from decoder and reorder the source sentences arbitrarily prior to translation. Once a pre-reordering error happens, it is very difficult to make up for the mistake in later translation steps. In our approach, we just retain the syntactic rules rather than use them to reorder the source sentences directly. During decoding, the syntactic rules will serve as a strong feature to guide and enhance the phrase reordering. Zhang et al., (2007) only allowed reordering between syntactic phrases and enforced the non-syntactic phrases translated in order. Xiong et al. (2008) proposed a linguistically annotated BTG for SMT. The method used some heuristic rules to linguistically annotate every source phrase with the source-side parse tree in decoding and built a linguistical reordering model. The two approaches both acquired and applied the syntactic rules in the decoding stage but meanwhile increased the decoding time to a large extent. Our work differs from theirs in three ways. First, when translating a test sentence, we obtain the co"
Y09-2047,I05-3017,0,0.182258,"apability to handle OOV words, because it also regards the character as a unit. Also, since the generative form is adopted, the dependency between adjacent characters is now directly (and separately) modeled for each class (within-words and between-words), which will give sharper preference when the history of assignment is given. In contrast, the adhesion between adjacent characters is not explicitly modeled in the character-based discriminative approach, and is thus not used to assign tags. 3 Experiments and Results We carried out our experiments on the data provided by SIGHAN Bakeoff 2005 (Emerson, 2005). To make a comparison with the baseline and previous work, only the closed tests3 are F 3 F According to Sighan Bakeoff 2005 regulation, the closed test could only use the training data directly provided. Any other data or information is forbidden, including the knowledge of characters set, punctuation and so on. 830 conducted. The metrics Precision (P), Recall (R), F-measure (F), Recall of OOV (ROOV) and Recall of IV (RIV) are used to evaluate the segmentation results. The balanced F-measure is F=2PR/(P+R). 3.1 Word-Based Generative Model and Character-Based Discriminative Model We first ext"
Y09-2047,P03-1035,0,0.193082,"is the first task, which aims to find the corresponding word sequence from the given Chinese character sequence. Among various approaches for CWS, statistical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the wellknown character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word “model” will be loosely exchanged with the word “approach” when there is no confusion. ∗ The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under"
Y09-2047,W04-3236,0,0.0756136,"ches for CWS, statistical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the wellknown character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word “model” will be loosely exchanged with the word “approach” when there is no confusion. ∗ The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. The authors thank Behavior De"
Y09-2047,C04-1081,0,0.371661,"Missing"
Y09-2047,P98-2206,0,0.0843937,"Missing"
Y09-2047,I05-3027,0,0.656992,"istical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the wellknown character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word “model” will be loosely exchanged with the word “approach” when there is no confusion. ∗ The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. The authors thank Behavior Design Corporation for"
Y09-2047,O03-4002,0,0.931721,"ious approaches for CWS, statistical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the wellknown character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word “model” will be loosely exchanged with the word “approach” when there is no confusion. ∗ The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Program) of China under grant No. 2006AA010108-4, and also supported by the China-Singapore Institute of Digital Media as well. The authors"
Y09-2047,W03-1730,0,0.307963,"d segmentation (CWS) is the first task, which aims to find the corresponding word sequence from the given Chinese character sequence. Among various approaches for CWS, statistical methods have been increasingly applied in the past two decades. According to the basic unit adopted to extract features, statistical approaches could be classified as either a word-based approach or a character-based approach. Besides, the word segmentation problem could also be formulated as either a generative model or a discriminative model. In terms of the above classification, the time-honored word-based model (Zhang et al., 2003; Gao et al., 2003) will be called as the word-based generative approach, while the wellknown character-based tagging model (Xue, 2003; Ng and Low, 2004; Tseng et al., 2005) will be named as the character-based discriminative approach. Also, the word “model” will be loosely exchanged with the word “approach” when there is no confusion. ∗ The research work has been partially funded by the Natural Science Foundation of China under grant No.60736014, 60723005 and 90820303, the National Key Technology R&D Program under grant No. 2006BAH03B02, the Hi-Tech Research and Development Program (863 Progr"
Y09-2047,P07-1106,0,0.122583,"rst identify the OOV candidates and then integrate them into the system. Their system achieves the best result in the AS corpus in Sighan Bakeoff 2005 contest. Tseng et al. (2005) add the information of word-prefixes and word-suffixes to overcome the drawbacks of character-based approaches, and they get the best results in the remaining three corpora in that contest. Afterwards, Zhang et al. (2006) use a sub-word tagging approach to utilize the sub-word information. All of them adopt the character-based discriminative approaches. The only state-of-the-art word-based model proposed recently is Zhang and Clark (2007), which uses Perceptron, a discriminative method. The comparison between those models mentioned above is given in Table 4. It shows that the proposed model achieves a good balance between those IV words and OOV words, and also competitive results. Table 4: Segmentation results of different Models AS Asahara Zhang (CRF) Our model Zhang & Clark MSR Tseng Zhang (CRF) Our model Zhang & Clark 5 R 0.952 0.956 0.958 N/A R 0.962 0.972 0.974 N/A P 0.951 0.947 0.938 N/A P 0.966 0.969 0.967 N/A F 0.952 0.951 0.948 0.946 F 0.964 0.971 0.971 0.972 ROOV 0.696 0.649 0.518 N/A ROOV 0.717 0.712 0.568 N/A RIV 0"
Y09-2047,P06-2123,0,\N,Missing
Y09-2047,C98-2201,0,\N,Missing
Y09-2047,I05-3018,0,\N,Missing
Y13-1010,C04-1081,0,0.0638947,"Missing"
Y13-1010,P03-1021,0,0.056177,"Missing"
Y13-1010,I08-4017,0,0.0530928,"Missing"
Y13-1010,zhao-etal-2010-large,0,0.0146567,"n many other cases such as “医学奖” (medicine-prize) and “一等奖” (first-prize)). Corpus PKU AS CITYU MSR qs 0.996 0.993 0.997 0.992 ql 0.977 0.970 0.976 0.970 qr 0.923 0.899 0.919 0.898 qf 0.686 0.662 0.653 0.662 Table 3: The matching rates of various tagging bias factors in the training set Corpus PKU AS CITYU MSR qs 0.457 0.374 0.515 0.299 ql 0.135 0.083 0.148 0.060 qr 0.135 0.082 0.149 0.060 qf 0.002 0.004 0.008 0.0003 Table 4: Unseen ratios for qs , ql , qr and qf in the testing set 5.2 Required context is frequently unobserved for testing instances However, according to the empirical study of Zhao et al., (2010), the OOV rate can be linearly reduced only with an exponential increasing of 123 PACLIC-27 corpus size, roughly due to Zipf’s law; and ngram is expected to also follow this pattern (Marco, 2009). Therefore, the sparseness problem gets more serious for the n-gram with a larger “n” (i.e., with wider context) because its number of possible distinct types would become much greater. As a consequence, there will be much more unseen bigrams than unseen unigrams in the testing set (Of course, unseen trigrams will be even more). Table 4 shows the unseen ratios for qs, ql, qr and qf in the testing set."
Y13-1010,W03-1730,0,0.0321201,"for performance comparison, we will focus on the second category to investigate how to use suffix related features in this paper. Generally speaking, Chinese suffixes are very productive and many words can be formed in this way. For example, the word “旅行者” (traveler) is composed of a stem (“旅行”, travel) and a suffix (“ 者”, -er). Although the character and character co-occurrence features (adopted in most current approaches) are able to partially characterize the internal structure of words (Sun, 2010), and some OOV words are indeed correctly handled when compared to pure wordbased approaches (Zhang et al., 2003; Gao et al., 2005), suffix related errors still remain as an important type of errors. Therefore, it is natural to expect that suffixes can be explicitly utilized to provide further help. Furthermore, prefix/suffix related features were claimed to be useful for CWS in some previous works (Tseng et al., 2005; Zhang et al., 2006). However, in their works, the prefix/suffix features are just a part of adopted features. The performances before and after adopting prefix/suffix features are never directly compared. So we could not know how much improvement actually results from those prefix/suffix"
Y13-1010,I05-3027,0,0.380437,"a suffix (“ 者”, -er). Although the character and character co-occurrence features (adopted in most current approaches) are able to partially characterize the internal structure of words (Sun, 2010), and some OOV words are indeed correctly handled when compared to pure wordbased approaches (Zhang et al., 2003; Gao et al., 2005), suffix related errors still remain as an important type of errors. Therefore, it is natural to expect that suffixes can be explicitly utilized to provide further help. Furthermore, prefix/suffix related features were claimed to be useful for CWS in some previous works (Tseng et al., 2005; Zhang et al., 2006). However, in their works, the prefix/suffix features are just a part of adopted features. The performances before and after adopting prefix/suffix features are never directly compared. So we could not know how much improvement actually results from those prefix/suffix related features. Besides, those features have only been adopted under discriminative approaches (Xue, 2003; Peng, 2004). We would also like to know whether the suffix related features would be effective for the generative approach (Wang et al., 2009; Wang et al., 2010). In comparison with the discriminative"
Y13-1010,I05-3025,0,0.0961974,"Missing"
Y13-1010,Y09-2047,1,0.863998,"were claimed to be useful for CWS in some previous works (Tseng et al., 2005; Zhang et al., 2006). However, in their works, the prefix/suffix features are just a part of adopted features. The performances before and after adopting prefix/suffix features are never directly compared. So we could not know how much improvement actually results from those prefix/suffix related features. Besides, those features have only been adopted under discriminative approaches (Xue, 2003; Peng, 2004). We would also like to know whether the suffix related features would be effective for the generative approach (Wang et al., 2009; Wang et al., 2010). In comparison with the discriminative model, the generative model has the drawback that it cannot utilize trailing context in selecting the position tag (i.e. Beginning, Middle, End and Single) (Xue, 2003) of the current character. Therefore, incorporating suffix information of the next character is supposed to be a promising supplement for the generative approach. So the real benefit of using suffixes is checked for the generative model first. To make use of the suffix information more completely, a novel quantitative tagging bias feature is first proposed to replace the"
Y13-1010,P12-1027,0,0.0229162,"Missing"
Y13-1010,zhang-etal-2004-interpreting,0,0.0611807,"Missing"
Y13-1010,C10-1132,1,0.787266,"Missing"
Y13-1010,P07-1106,0,0.0563439,"Missing"
Y13-1010,O03-4002,0,0.857807,"improvement can hardly be achieved by incorporating suffix related features into those widely adopted surface features, which is against the commonly believed supposition. Error analysis reveals that the main problem behind this surprising finding is the conflict between the degree of reliability and the coverage rate of suffix related features. 1 Introduction As words are the basic units for text analysis, Chinese word segmentation (CWS) is critical for many Chinese NLP tasks such as parsing and machine translation. Although steady improvements have been observed in previous CWS researches (Xue, 2003; Zhang and Clark, 2007; Wang et al., 2012; Sun et al., 2012), their performances are only acceptable for invocabulary (IV) words and are still far from satisfactory for those out-of-vocabulary (OOV) words. According to the Zipf's law (Zipf, 1949), which states that the frequency of a word is inversely proportional to its rank in the frequency table for a given corpus, it is unlikely to cover all the words of a language in the training corpus. OOV words are thus inevitable in real applications. To further improve the performance for OOV words, various approaches have been proposed. Most of the"
Y13-1010,I05-3017,0,0.0274704,"zing suffixes Nonetheless, we cannot distinguish suffixes from those non-suffixes by just checking each character because whether a character is a suffix highly depends on the context. For example, the character ‘化’ is a suffix in the word “初始化” (initial-ize). However, it becomes a prefix when it comes to the word “化纤” (chemical-fibre). Also, whether a character is a suffix varies with different annotation standards adopted by various corpora. For example, the character ‘ 厂 ’ (factory) is a suffix in words such as “服装厂” (clothing-factory) in the PKU corpus provided by the SIGHAN 2005 Bakeoff (Emerson, 2005). Nevertheless, it is regarded as a single-character 1 119 Extracting suffix information http://zh.wikipedia.org/wiki/%E8%A9%9E%E7%B6%B4 PACLIC-27 word in similar occasions in the MSR corpus. For these two reasons, suffixes cannot be directly recognized by simply locating some prespecified characters prepared by the linguist. 2.2 2.3 Adopting tagging bias information There are two drawbacks to adopt the above suffix-like list: (1) The associated context that is required to decide whether a character should be regarded as a suffix is either completely not taken into account (in previous approac"
Y13-1010,C10-2139,0,0.0143333,"on, and Computation pages 118－125 PACLIC-27 SIGHAN closed test (Emerson, 2005), which is widely adopted for performance comparison, we will focus on the second category to investigate how to use suffix related features in this paper. Generally speaking, Chinese suffixes are very productive and many words can be formed in this way. For example, the word “旅行者” (traveler) is composed of a stem (“旅行”, travel) and a suffix (“ 者”, -er). Although the character and character co-occurrence features (adopted in most current approaches) are able to partially characterize the internal structure of words (Sun, 2010), and some OOV words are indeed correctly handled when compared to pure wordbased approaches (Zhang et al., 2003; Gao et al., 2005), suffix related errors still remain as an important type of errors. Therefore, it is natural to expect that suffixes can be explicitly utilized to provide further help. Furthermore, prefix/suffix related features were claimed to be useful for CWS in some previous works (Tseng et al., 2005; Zhang et al., 2006). However, in their works, the prefix/suffix features are just a part of adopted features. The performances before and after adopting prefix/suffix features a"
Y13-1010,D11-1090,0,0.190672,"Missing"
Y13-1010,C12-1101,1,0.81007,"Missing"
Y13-1010,P06-2123,0,\N,Missing
Y13-1010,P11-1141,0,\N,Missing
Y13-1010,W10-4101,0,\N,Missing
