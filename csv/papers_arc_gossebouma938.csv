2021.iwpt-1.15,From Raw Text to Enhanced {U}niversal {D}ependencies: The Parsing Shared Task at {IWPT} 2021,2021,-1,-1,1,1,5827,gosse bouma,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"We describe the second IWPT task on end-to-end parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task."
2020.iwpt-1.16,Overview of the {IWPT} 2020 Shared Task on Parsing into Enhanced {U}niversal {D}ependencies,2020,-1,-1,1,1,5827,gosse bouma,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,0,"This overview introduces the task of parsing into enhanced universal dependencies, describes the datasets used for training and evaluation, and evaluation metrics. We outline various approaches and discuss the results of the shared task."
2020.emnlp-main.180,{UD}apter: Language Adaptation for Truly {U}niversal {D}ependency Parsing,2020,34,0,3,1,3854,ahmet ustun,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success."
W19-4206,"Multi-Team: A Multi-attention, Multi-decoder Approach to Morphological Analysis.",2019,0,0,3,1,3854,ahmet ustun,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper describes our submission to SIGMORPHON 2019 Task 2: Morphological analysis and lemmatization in context. Our model is a multi-task sequence to sequence neural network, which jointly learns morphological tagging and lemmatization. On the encoding side, we exploit character-level as well as contextual information. We introduce a multi-attention decoder to selectively focus on different parts of character and word sequences. To further improve the model, we train on multiple datasets simultaneously and use external embeddings for initialization. Our final model reaches an average morphological tagging F1 score of 94.54 and a lemma accuracy of 93.91 on the test data, ranking respectively 3rd and 6th out of 13 teams in the SIGMORPHON 2019 shared task."
R19-1140,Cross-Lingual Word Embeddings for Morphologically Rich Languages,2019,0,0,2,1,3854,ahmet ustun,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Cross-lingual word embedding models learn a shared vector space for two or more languages so that words with similar meaning are represented by similar vectors regardless of their language. Although the existing models achieve high performance on pairs of morphologically simple languages, they perform very poorly on morphologically rich languages such as Turkish and Finnish. In this paper, we propose a morpheme-based model in order to increase the performance of cross-lingual word embeddings on morphologically rich languages. Our model includes a simple extension which enables us to exploit morphemes for cross-lingual mapping. We applied our model for the Turkish-Finnish language pair on the bilingual word translation task. Results show that our model outperforms the baseline models by 2{\%} in the nearest neighbour ranking."
W18-6003,Expletives in {U}niversal {D}ependency Treebanks,2018,0,0,1,1,5827,gosse bouma,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"Although treebanks annotated according to the guidelines of Universal Dependencies (UD) now exist for many languages, the goal of annotating the same phenomena in a cross-linguistically consistent fashion is not always met. In this paper, we investigate one phenomenon where we believe such consistency is lacking, namely expletive elements. Such elements occupy a position that is structurally associated with a core argument (or sometimes an oblique dependent), yet are non-referential and semantically void. Many UD treebanks identify at least some elements as expletive, but the range of phenomena differs between treebanks, even for closely related languages, and sometimes even for different treebanks for the same language. In this paper, we present criteria for identifying expletives that are applicable across languages and compatible with the goals of UD, give an overview of expletives as found in current UD treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases."
W17-0403,Increasing Return on Annotation Investment: The Automatic Construction of a {U}niversal {D}ependency Treebank for {D}utch,2017,11,2,1,1,5827,gosse bouma,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,"We present a method for automatically converting the Dutch Lassy Small treebank, a phrasal dependency treebank, to UD. All of the information required to produce accurate UD annotation appears to be available in the underlying annotation. However, we also note that the close connection between POS-tags and dependency labels that is present in UD is missing in the Lassy treebanks. As a consequence, annotation decisions in the Dutch data for such phenomena as nominalization and clausal complements of prepositions seem to differ to some extent from comparable data in English and German. Because the conversion is automatic, we can now also compare three state-of-theart dependency parsers trained on UD Lassy Small with Alpino, a hybrid Dutch parser which produces output that is compatible with the original Lassy annotations."
W13-5637,The Automatic Identification of Discourse Units in {D}utch Text,2013,26,2,2,0,40523,nynke vliet,Proceedings of the 19th Nordic Conference of Computational Linguistics ({NODALIDA} 2013),0,"The identification of discourse units is an essential step in discourse parsing, the automatic construction of a discourse structure from a text. We present a rule-based algorithm to identify elementary discourse units (EDUs) in Dutch written text. Contrary to approaches that focus on the determination of segment boundaries, we identify complete discourse units, which is especially helpful for the recognition of interrupted EDUs that contain embedded discourse units. We use syntactic and lexical information to decompose sentences into EDUs. Experimental results show that our algorithm for EDU identification performs well on texts of various genres."
redeker-etal-2012-multi,Multi-Layer Discourse Annotation of a {D}utch Text Corpus,2012,36,5,4,0,31288,gisela redeker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We have compiled a corpus of 80 Dutch texts from expository and persuasive genres, which we annotated for rhetorical and genre-specific discourse structure, and lexical cohesion with the goal of creating a gold standard for further research. The annota{\^A}{\neg}tions are based on a segmentation of the text in elementary discourse units that takes into account cues from syntax and punctuation. During the labor-intensive discourse-structure annotation (RST analysis), we took great care to thoroughly reconcile the initial analyses. That process and the availability of two independent initial analyses for each text allows us to analyze our disagreements and to assess the confusability of RST relations, and thereby improve the annotation guidelines and gather evidence for the classification of these relations into larger groups. We are using this resource for corpus-based studies of discourse relations, discourse markers, cohesion, and genre differences, e.g., the question of how discourse structure and lexical cohesion interact for different genres in the overall organization of texts. We are also exploring automatic text segmentation and semi-automatic discourse annotation."
P10-1135,On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds,2010,25,18,2,0,6994,ashwin ittoo,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"An important relation in information extraction is the part-whole relation. Ontological studies mention several types of this relation. In this paper, we show that the traditional practice of initializing minimally-supervised algorithms with a single set that mixes seeds of different types fails to capture the wide variety of part-whole patterns and tuples. The results obtained with mixed seeds ultimately converge to one of the part-whole relation types. We also demonstrate that all the different types of part-whole relations can still be discovered, regardless of the type characterized by the initializing seeds. We performed our experiments with a state-of-the-art information extraction algorithm."
bouma-2010-cross,Cross-lingual Ontology Alignment using {E}uro{W}ord{N}et and {W}ikipedia,2010,12,10,1,1,5827,gosse bouma,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes a system for linking the thesaurus of the Netherlands Institute for Sound and Vision to English WordNet and dbpedia. The thesaurus contains subject (concept) terms, and names of persons, locations, and miscalleneous names. We used EuroWordNet, a multilingual wordnet, and Dutch Wikipedia as intermediaries for the two alignments. EuroWordNet covers most of the subject terms in the thesaurus, but the organization of the cross-lingual links makes selection of the most appropriate English target term almost impossible. Precision and recall of the automatic alignment with WordNet for subject terms is 0.59. Using page titles, redirects, disambiguation pages, and anchor text harvested from Dutch Wikipedia gives reasonable performance on subject terms and geographical terms. Many person and miscalleneous names in the thesaurus could not be located in (Dutch or English) Wikipedia. Precision for miscellaneous names, subjects, persons and locations for the alignment with Wikipedia ranges from 0.63 to 0.94, while recall for subject terms is 0.62."
W09-1604,Cross-lingual Alignment and Completion of {W}ikipedia Templates,2009,13,33,1,1,5827,gosse bouma,Proceedings of the Third International Workshop on Cross Lingual Information Access: Addressing the Information Need of Multilingual Societies ({CLIAWS}3),0,"For many languages, the size of Wikipedia is an order of magnitude smaller than the English Wikipedia. We present a method for cross-lingual alignment of template and infobox attributes in Wikipedia. The alignment is used to add and complete templates and infoboxes in one language with information derived from Wikipedia in another language. We show that alignment between English and Dutch Wikipedia is accurate and that the result can be used to expand the number of template attribute-value pairs in Dutch Wikipedia by 50%. Furthermore, the alignment provides valuable information for normalization of template and attribute names and can be used to detect potential inconsistencies."
W09-0107,Parsed Corpora for Linguistics,2009,17,7,2,0,370,gertjan noord,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"Knowledge-based parsers are now accurate, fast and robust enough to be used to obtain syntactic annotations for very large corpora fully automatically. We argue that such parsed corpora are an interesting new resource for linguists. The argument is illustrated by means of a number of recent results which were established with the help of parsed corpora."
hendrickx-etal-2008-coreference,A Coreference Corpus and Resolution System for {D}utch,2008,24,18,2,0,16715,iris hendrickx,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present the main outcomes of the COREA project: a corpus annotated with coreferential relations and a coreference resolution system for Dutch. In the project we developed annotation guidelines for coreference resolution for Dutch and annotated a corpus of 135K tokens. We discuss these guidelines, the annotation tool, and the inter-annotator agreement. We also show a visualization of the annotated relations. The standard approach to evaluate a coreference resolution system is to compare the predictions of the system to a hand-annotated gold standard test set (cross-validation). A more practically oriented evaluation is to test the usefulness of coreference relation information in an NLP application. We run experiments with an Information Extraction module for the medical domain, and measure the performance of this module with and without the coreference relation information. We present the results of both this application-oriented evaluation of our system and of a standard cross-validation evaluation. In a separate experiment we also evaluate the effect of coreference information produced by a simple rule-based coreference module in a Question Answering application."
W07-1503,Mining Syntactically Annotated Corpora with {XQ}uery,2007,18,18,1,1,5827,gosse bouma,Proceedings of the Linguistic Annotation Workshop,0,"This paper presents a uniform approach to data extraction from syntactically annotated corpora encoded in XML. XQuery, which incorporates XPath, has been designed as a query language for XML. The combination of XPath and XQuery offers flexibility and expressive power, while corpus specific functions can be added to reduce the complexity of individual extraction tasks. We illustrate our approach using examples from dependency treebanks for Dutch."
W06-2609,Learning to Identify Definitions using Syntactic Features,2006,15,14,2,0,49697,ismail fahmi,Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,0,"This paper describes an approach to learning concept definitions which operates on fully parsed text. A subcorpus of the Dutch version of Wikipedia was searched for sentences which have the syntactic properties of definitions. Next, we experimented with various text classification techniques to distinguish actual definitions from other sentences. A maximum entropy classifier which incorporates features referring to the position of the sentence in the document as well as various syntactic features, gives the best results."
W06-1802,Linguistic Knowledge and Question Answering,2006,21,1,1,1,5827,gosse bouma,Proceedings of the Workshop {KRAQ}{'}06: Knowledge and Reasoning for Language Processing,0,"The availability of robust and deep syntactic parsing can improve the performance of Question Answering systems. This is illustrated using examples from Joost, a Dutch QA system which has been used for both open (CLEF) and closed domain QA."
I05-7011,Automatic Acquisition of Lexico-semantic Knowledge for {QA},2005,20,15,2,0,12096,lonneke plas,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"We present an experiment for finding semantically similar words on the basis of a parsed corpus of Dutch text and show that the acquired information correlates with relations found in Dutch EuroWordNet. Next, we demonstrate how the acquired knowledge can be used to boost the performance of an open-domain question answering system for Dutch. Automatically acquired lexico-semantic information is used to improve the recall of a method for extracting function relations (such as Wim Kok is the prime minister of the Netherlands) from corpora, and to improve the precision of our QA system on general WH-questions and definition questions."
kis-etal-2004-new,A New Approach to the Corpus-based Statistical Investigation of {H}ungarian Multi-word Lexemes,2004,8,6,3,0,52153,balazs kis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We apply statistical methods to perform automatic extraction of Hungarian collocations from corpora. Due to the complexity of Hungarian morphology, a complex resource preparation tool chain has been developed. This tool chain implements a reusable and, in principle, language independent framework. In the first part, the paper describes the tool chain itself, then, in the second part, an experiment using this framework. The experiment deals with the extraction of patterns from the corpus as collocation candidates, in order to compare results to an experiment on Dutch V  PP patterns (Villada, 2004). Statistical processing on this dataset provided interesting observations, briefly explained in the evaluation section. We conclude by providing a summary of further steps required to improve the extraction process. This is not restricted to improvements in the resource preparation for statistical processing, but a proposal to use nonstatistical means as well, thus acquiring an efficient blend of different methods."
bouma-kloosterman-2002-querying,Querying Dependency Treebanks in {XML},2002,14,24,1,1,5827,gosse bouma,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
A00-2040,A Finite State and Data-Oriented Method for Grapheme to Phoneme Conversion,2000,21,25,1,1,5827,gosse bouma,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"A finite-state method, based on leftmost longestmatch replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large rule and a 'lazy' variant of Brill's algoritm), trained on only 40K words, reaches 99%."
W99-0802,A Modern Computational Linguistics Course Using {D}utch,1999,17,1,1,1,5827,gosse bouma,{EACL} 1999: Computer and {I}nternet Supported Education in Language and Speech Technology,0,"This paper describes material for a course in computational linguistics which concentrates on building (parts of) realistic language technology applications for Dutch. We present an overview of the reasons for developing new material, rather than using existing text-books. Next we present an overview of the course in the form of six exercises, covering advanced use of finite state methods, grammar development, and natural language interfaces. The exercises emphasise the benefits of special-purpose development tools, the importance of testing on realistic data-sets, and the possibilities for web-applications based on natural language processing."
W97-1513,Hdrug. A Flexible and Extendible Development Environment for Natural Language Processing.,1997,0,1,2,0.603689,370,gertjan noord,Computational Environments for Grammar Development and Linguistic Engineering,0,None
W97-0614,Grammatical analysis in the {OVIS} spoken-dialogue system,1997,12,10,2,0,5269,markjan nederhof,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,"We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system. We discuss the structure of the grammar, the properties of the parser, and a method for achieving robustness. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input."
P94-1021,Constraint-Based Categorial Grammar,1994,9,15,1,1,5827,gosse bouma,32nd Annual Meeting of the Association for Computational Linguistics,1,"We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints. In particular, the introduction of relational constraints allows one to capture the effects of (recursive) lexical rules in a computationally attractive manner. We illustrate the linguistic merits of the new approach by showing how it accounts for the syntax of Dutch cross-serial dependencies and the position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints."
C94-1039,Adjuncts and the Processing of Lexical Rules,1994,3,50,2,0.603689,370,gertjan noord,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The standard HPSG analysis of germanic verb clusters can not explain the observed narrowscope readings of adjuncts in such verb clusters.We present an extension of the HPSG analysis that accounts for the systematic ambiguity of the scope of adjuncts in verb cluster constructions, by treating adjuncts as members of the subcat list. The extension uses powerful recursive lexical rules, implemented as complex constraints. We show how 'delayed evaluation' techiniques from constraint-logic programming can be used to process such lexical rules."
E93-1010,Head-driven Parsing for Lexicalist Grammars: Experimental Results,1993,8,24,1,1,5827,gosse bouma,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present evidence that head-driven parsing strategies lead to efficiency gains over standard parsing strategies, for lexicalist, concatenative and unification-based grammars. A head-driven parser applies a rule only after a phrase matching the head has been derived. By instantiating the head of the rule important information is obtained about the left-hand-side and the other elements of the ritht-hand-side. We have used two different head-driven parsers and a number of standard parsers to parse with lexicalist grammars for English and for Dutch. The results indicate that for important classes of lexicalist grammars it is fruitful to apply parsing strategies which are sensitive to the linguistic notion 'head'."
J92-2003,Feature Structures and Nonmonotonicity,1992,15,39,1,1,5827,gosse bouma,Computational Linguistics,0,"Unification-based grammar formalisms use feature structures to represent linguistic knowledge. The only operation defined on feature structures, unification, is information-combining and monotonic. Several authors have proposed nonmonotonic extensions of this formalism, as for a linguistically adequate description of certain natural language phenomena some kind of default reasoning seems essential. We argue that the effect of these proposals can be captured by means of one general, nonmonotonic, operation on feature structures, called default unification. We provide a formal semantics of the operation and demonstrate how some of the phenomena used to motivate nonmonotonic extensions of unification-based formalisms can be handled."
C92-1018,A Lexicalist Account of {I}celandic Case Marking,1992,12,0,1,1,5827,gosse bouma,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Recent theoretical descriptions of the Icelandic case system distinguish between lexical and structural case. Lexical case is assigned in the lexicon, whereas structural case is assigned in syntax, under the provision that it does not override lexical case assignment. This analysis is problematic for grammatical theories such as Categorial Unification Grammar (CUG) and Head driven Phrase Structure Grammar (HPSG) as the introduction of a syntactic case component is incompatible with the lexicalist ideology underlying these frameworks. Furthermore, the default character of syntactic case introduces a procedural aspect into the grammar which goes against the declarative spirit of unification-based frameworks in general. In this paper, I propose an alternative analysis, formulated in terms of CUG, in which all case constraints are expressed lexically and in which default reasoning is restricted to nonmonotonic inheritance of lexical information only."
E91-1031,Prediction in Chart Parsing Algorithms for Categorial Unification Grammar,1991,12,3,1,1,5827,gosse bouma,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Natural language systems based on Categorial Unification Grammar (CUG) have mainly employed bottom-up parsing algorithms for processing. Conventional prediction techniques to improve the efficiency of the parsing process, appear to fall short when parsing CUG. Nevertheless, prediction seems necessary when parsing grammars with highly ambiguous lexicons or with noncanonical categorial rules. In this paper we present a lexicalist prediction technique for CUG and show that this may lead to considerable gains in efficiency for both bottom-up and top-down parsing."
P90-1021,Defaults in Unification Grammar,1990,16,27,1,1,5827,gosse bouma,28th Annual Meeting of the Association for Computational Linguistics,1,"Incorporation of defaults in grammar formalisms is important for reasons of linguistic adequacy and grammar organization. In this paper we present an algorithm for handling default information in unification grammar. The algorithm specifies a logical operation on feature structures, merging with the non-default structure only those parts of the default feature structure which are not constrained by the non-default structure. We present various linguistic applications of default unification."
E89-1003,Efficient Processing of Flexible Categorial Grammar,1989,10,10,1,1,5827,gosse bouma,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"From a processing point of view, however, flexible categorial systems are problematic, since they introduce spurious ambiguity. In this paper, we present a flexible categorial grammar which makes extensive use of the product-operator, first introduced by Lambek (1958). The grammar has the property that for every reading of a sentence, a strictly left-branching derivation can be given. This leads to the definition of a subset of the grammar, for which the spurious ambiguity problem does not arise and efficient processing is possible."
