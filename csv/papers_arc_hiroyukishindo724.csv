2021.findings-acl.164,Structured Refinement for Sequential Labeling,2021,-1,-1,2,1,7900,yiran wang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.acl-long.275,Nested Named Entity Recognition via Explicitly Excluding the Influence of the Best Path,2021,-1,-1,2,1,7900,yiran wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper presents a novel method for nested named entity recognition. As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path. Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level. In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme. We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method."
2020.emnlp-main.523,{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention,2020,-1,-1,3,1,12644,ikuya yamada,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke."
2020.emnlp-demos.4,{W}ikipedia2{V}ec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from {W}ikipedia,2020,-1,-1,4,1,12644,ikuya yamada,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Python-based open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a state-of-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io/."
2020.coling-main.271,Coordination Boundary Identification without Labeled Data for Compound Terms Disambiguation,2020,-1,-1,6,0,21367,yuya sawada,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a simple method for nominal coordination boundary identification. As the main strength of our method, it can identify the coordination boundaries without training on labeled data, and can be applied even if coordination structure annotations are not available. Our system employs pre-trained word embeddings to measure the similarities of words and detects the span of coordination, assuming that conjuncts share syntactic and semantic similarities. We demonstrate that our method yields good results in identifying coordinated noun phrases in the GENIA corpus and is comparable to a recent supervised method for the case when the coordinator conjoins simple noun phrases."
W19-2609,Playing by the Book: An Interactive Game Approach for Action Graph Extraction from Text,2019,0,5,2,0,10802,ronen tamari,Proceedings of the Workshop on Extracting Structured Knowledge from Scientific Publications,0,"Understanding procedural text requires tracking entities, actions and effects as the narrative unfolds. We focus on the challenging real-world problem of action-graph extraction from materials science papers, where language is highly specialized and data annotation is expensive and scarce. We propose a novel approach, Text2Quest, where procedural text is interpreted as instructions for an interactive game. A learning agent completes the game by executing the procedure correctly in a text-based simulated lab environment. The framework can complement existing approaches and enables richer forms of learning compared to static texts. We discuss potential limitations and advantages of the approach, and release a prototype proof-of-concept, hoping to encourage research in this direction."
P19-1158,Stochastic Tokenization with a Language Model for Neural Text Classification,2019,0,1,2,0,7542,tatsuya hiraoka,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"For unsegmented languages such as Japanese and Chinese, tokenization of a sentence has a significant impact on the performance of text classification. Sentences are usually segmented with words or subwords by a morphological analyzer or byte pair encoding and then encoded with word (or subword) representations for neural networks. However, segmentation is potentially ambiguous, and it is unclear whether the segmented tokens achieve the best performance for the target task. In this paper, we propose a method to simultaneously learn tokenization and text classification to address these problems. Our model incorporates a language model for unsupervised tokenization into a text classifier and then trains both models simultaneously. To make the model robust against infrequent tokens, we sampled segmentation for each sentence stochastically during training, which resulted in improved performance of text classification. We conducted experiments on sentiment analysis as a text classification task and show that our method achieves better performance than previous methods."
N19-1286,Relation Classification Using Segment-Level Attention-based {CNN} and Dependency-based {RNN},2019,0,1,3,0,11007,vanhien tran,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Recently, relation classification has gained much success by exploiting deep neural networks. In this paper, we propose a new model effectively combining Segment-level Attention-based Convolutional Neural Networks (SACNNs) and Dependency-based Recurrent Neural Networks (DepRNNs). While SACNNs allow the model to selectively focus on the important information segment from the raw sequence, DepRNNs help to handle the long-distance relations from the shortest dependency path of relation entities. Experiments on the SemEval-2010 Task 8 dataset show that our model is comparable to the state-of-the-art without using any external lexical features."
N19-1343,Decomposed Local Models for Coordinate Structure Parsing,2019,0,0,2,0,21369,hiroki teranishi,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We propose a simple and accurate model for coordination boundary identification. Our model decomposes the task into three sub-tasks during training; finding a coordinator, identifying inside boundaries of a pair of conjuncts, and selecting outside boundaries of it. For inference, we make use of probabilities of coordinators and conjuncts in the CKY parsing to find the optimal combination of coordinate structures. Experimental results demonstrate that our model achieves state-of-the-art results, ensuring that the global structure of coordinations is consistent."
K19-1052,Neural Attentive Bag-of-Entities Model for Text Classification,2019,47,1,2,1,12644,ikuya yamada,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"This study proposes a Neural Attentive Bag-of-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for text classification. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/wikipedia2vec/wikipedia2vec."
Y18-1046,Automatic Error Correction on {J}apanese Functional Expressions Using Character-based Neural Machine Translation,2018,0,0,4,0,10636,jun liu,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-4922,Cooperating Tools for {MWE} Lexicon Management and Corpus Annotation,2018,0,0,3,0,991,yuji matsumoto,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"We present tools for lexicon and corpus management that offer cooperating functionality in corpus annotation. The former, named Cradle, stores a set of words and expressions where multi-word expressions are defined with their own part-of-speech information and internal syntactic structures. The latter, named ChaKi, manages text corpora with part-of-speech (POS) and syntactic dependency structure annotations. Those two tools cooperate so that the words and multi-word expressions stored in Cradle are directly referred to by ChaKi in conducting corpus annotation, and the words and expressions annotated in ChaKi can be output as a list of lexical entities that are to be stored in Cradle."
P18-4010,Sentence Suggestion of {J}apanese Functional Expressions for {C}hinese-speaking Learners,2018,0,0,2,0,10636,jun liu,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present a computer-assisted learning system, Jastudy, which is particularly designed for Chinese-speaking learners of Japanese as a second language (JSL) to learn Japanese functional expressions with suggestion of appropriate example sentences. The system automatically recognizes Japanese functional expressions using a free Japanese morphological analyzer MeCab, which is retrained on a new Conditional Random Fields (CRF) model. In order to select appropriate example sentences, we apply a pairwise-based machine learning tool, Support Vector Machine for Ranking (SVMrank) to estimate the complexity of the example sentences using Japanese{--}Chinese homographs as an important feature. In addition, we cluster the example sentences that contain Japanese functional expressions with two or more meanings and usages, based on part-of-speech, conjugation forms of verbs and semantic attributes, using the K-means clustering algorithm in Scikit-Learn. Experimental results demonstrate the effectiveness of our approach."
L18-1175,{PDFA}nno: a Web-based Linguistic Annotation Tool for {PDF} Documents,2018,0,0,1,1,7901,hiroyuki shindo,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1356,Chemical Compounds Knowledge Visualization with Natural Language Processing and Linked Data,2018,0,1,5,0,29899,kazunari tanaka,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1396,Construction of Large-scale {E}nglish Verbal Multiword Expression Annotated Corpus,2018,0,0,2,1,28108,akihiko kato,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1191,A Span Selection Model for Semantic Role Labeling,2018,41,0,2,1,9339,hiroki ouchi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively."
C18-1016,Representation Learning of Entities and Documents from Knowledge Base Descriptions,2018,0,0,2,1,12644,ikuya yamada,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this paper, we describe TextEnt, a neural network model that learns distributed representations of entities and documents directly from a knowledge base (KB). Given a document in a KB consisting of words and entity annotations, we train our model to predict the entity that the document describes and map the document and its target entity close to each other in a continuous vector space. Our model is trained using a large number of documents extracted from Wikipedia. The performance of the proposed model is evaluated using two tasks, namely fine-grained entity typing and multiclass text classification. The results demonstrate that our model achieves state-of-the-art performance on both tasks. The code and the trained representations are made available online for further academic research."
Q17-1028,Learning Distributed Representations of Texts and Entities from Knowledge Base,2017,3,33,2,1,12644,ikuya yamada,Transactions of the Association for Computational Linguistics,0,"We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks. Our code and trained models are publicly available for further academic research."
P17-2068,{E}nglish Multiword Expression-aware Dependency Parsing Including Named Entities,2017,8,1,2,1,28108,akihiko kato,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities. Further, we explore models that predict both MWE-spans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWE-span features achieves an MWE recognition improvement of 1.35 points over a pipeline model."
P17-1146,Neural Modeling of Multi-Predicate Interactions for {J}apanese Predicate Argument Structure Analysis,2017,19,3,2,1,9339,hiroki ouchi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The performance of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to the joint modeling of interactions between multiple predicates. However, this approach relies heavily on syntactic information predicted by parsers, and suffers from errorpropagation. To remedy this problem, we introduce a model that uses grid-type recurrent neural networks. The proposed model automatically induces features sensitive to multi-predicate interactions from the word sequence information of a sentence. Experiments on the NAIST Text Corpus demonstrate that without syntactic information, our model outperforms previous syntax-dependent models."
K17-1042,Joint Prediction of Morphosyntactic Categories for Fine-Grained {A}rabic Part-of-Speech Tagging Exploiting Tag Dictionary Information,2017,9,4,2,0,513,go inoue,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"Part-of-speech (POS) tagging for morphologically rich languages such as Arabic is a challenging problem because of their enormous tag sets. One reason for this is that in the tagging scheme for such languages, a complete POS tag is formed by combining tags from multiple tag sets defined for each morphosyntactic category. Previous approaches in Arabic POS tagging applied one model for each morphosyntactic tagging task, without utilizing shared information between the tasks. In this paper, we propose an approach that utilizes this information by jointly modeling multiple morphosyntactic tagging tasks with a multi-task learning framework. We also propose a method of incorporating tag dictionary information into our neural models by combining word representations with representations of the sets of possible tags. Our experiments showed that the joint model with tag dictionary information results in an accuracy of 91.38{\%} on the Penn Arabic Treebank data set, with an absolute improvement of 2.11{\%} over the current state-of-the-art tagger."
I17-2017,Segment-Level Neural Conditional Random Fields for Named Entity Recognition,2017,12,5,2,0,25550,motoki sato,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present Segment-level Neural CRF, which combines neural networks with a linear chain CRF for segment-level sequence modeling tasks such as named entity recognition (NER) and syntactic chunking. Our segment-level CRF can consider higher-order label dependencies compared with conventional word-level CRF. Since it is difficult to consider all possible variable length segments, our method uses segment lattice constructed from the word-level tagging model to reduce the search space. Performing experiments on NER and chunking, we demonstrate that our method outperforms conventional word-level CRF with neural networks."
I17-1027,Coordination Boundary Identification with Similarity and Replaceability,2017,0,0,2,0,21369,hiroki teranishi,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,We propose a neural network model for coordination boundary detection. Our method relies on the two common properties - similarity and replaceability in conjuncts - in order to detect both similar pairs of conjuncts and dissimilar pairs of conjuncts. The model improves identification of clause-level coordination using bidirectional RNNs incorporating two properties as features. We show that our model outperforms the existing state-of-the-art methods on the coordination annotated Penn Treebank and Genia corpus without any syntactic information from parsers.
Y16-2006,Multiple Emotions Detection in Conversation Transcripts,2016,-1,-1,2,0,29970,ducanh phan,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,None
W16-3918,{J}apanese Text Normalization with Encoder-Decoder Model,2016,15,5,2,0,33661,taishi ikeda,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Text normalization is the task of transforming lexical variants to their canonical forms. We model the problem of text normalization as a character-level sequence to sequence learning problem and present a neural encoder-decoder model for solving it. To train the encoder-decoder model, many sentences pairs are generally required. However, Japanese non-standard canonical pairs are scarce in the form of parallel corpora. To address this issue, we propose a method of data augmentation to increase data size by converting existing resources into synthesized non-standard forms using handcrafted rules. We conducted an experiment to demonstrate that the synthesized corpus contributes to stably train an encoder-decoder model and improve the performance of Japanese text normalization."
W16-3813,Identification of Flexible Multiword Expressions with the Help of Dependency Structure Annotation,2016,-1,-1,4,0,33679,ayaka morimoto,Proceedings of the Workshop on Grammar and Lexicon: interactions and interfaces ({G}ram{L}ex),0,"This paper presents our ongoing work on compilation of English multi-word expression (MWE) lexicon. We are especially interested in collecting flexible MWEs, in which some other components can intervene the expression such as {``}a number of{''} vs {``}a large number of{''} where a modifier of {``}number{''} can be placed in the expression and inherit the original meaning. We fiest collect possible candidates of flexible English MWEs from the web, and annotate all of their occurrences in the Wall Street Journal portion of Ontonotes corpus. We make use of word dependency strcuture information of the sentences converted from the phrase structure annotation. This process enables semi-automatic annotation of MWEs in the corpus and simultanaously produces the internal and external dependency representation of flexible MWEs."
L16-1263,Construction of an {E}nglish Dependency Corpus incorporating Compound Function Words,2016,7,1,2,1,28108,akihiko kato,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The recognition of multiword expressions (MWEs) in a sentence is important for such linguistic analyses as syntactic and semantic parsing, because it is known that combining an MWE into a single token improves accuracy for various NLP tasks, such as dependency parsing and constituency parsing. However, MWEs are not annotated in Penn Treebank. Furthermore, when converting word-based dependency to MWE-aware dependency directly, one could combine nodes in an MWE into a single node. Nevertheless, this method often leads to the following problem: A node derived from an MWE could have multiple heads and the whole dependency structure including MWE might be cyclic. Therefore we converted a phrase structure to a dependency structure after establishing an MWE as a single subtree. This approach can avoid an occurrence of multiple heads and/or cycles. In this way, we constructed an English dependency corpus taking into account compound function words, which are one type of MWEs that serve as functional expressions. In addition, we report experimental results of dependency parsing using a constructed corpus."
K16-1025,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,2016,29,30,2,1,12644,ikuya yamada,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document to their correct references in a knowledge base (KB) (e.g., Wikipedia). In this paper, we propose a novel embedding method specifically designed for NED. The proposed method jointly maps words and entities into the same continuous vector space. We extend the skip-gram model by using two models. The KB graph model learns the relatedness of entities using the link structure of the KB, whereas the anchor context model aims to align vectors such that similar words and entities occur close to one another in the vector space by leveraging KB anchors and their context words. By combining contexts based on the proposed embedding with standard NED features, we achieved state-of-the-art accuracy of 93.1% on the standard CoNLL dataset and 85.2% on the TAC 2010 dataset."
D16-1109,Joint Transition-based Dependency Parsing and Disfluency Detection for Automatic Speech Recognition Texts,2016,5,10,2,0,25534,masashi yoshikawa,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
Y15-2015,An Efficient Annotation for Phrasal Verbs using Dependency Information,2015,10,1,2,0,36270,masayuki komai,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"In this paper, we present an efficient semiautomatic method for annotating English phrasal verbs on the OntoNotes corpus. Our method first constructs a phrasal verb dictionary based on Wiktionary, then annotates each candidate example on the corpus as an either a phrasal verb usage or a literal one. For efficient annotation, we use the dependency structure of a sentence to filter out highly plausible positive and negative cases, resulting in a drastic reduction of annotation cost. We also show that a naive binary classification achieves better MWE identification performance than rule-based and sequence-labeling methods."
P15-2140,Semantic Structure Analysis of Noun Phrases using {A}bstract {M}eaning {R}epresentation,2015,18,5,2,0,37469,yuichiro sawai,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We propose a method for semantic structure analysis of noun phrases using Abstract Meaning Representation (AMR). AMR is a graph representation for the meaning of a sentence, in which noun phrases (NPs) are manually annotated with internal structure and semantic relations. We extract NPs from the AMR corpus and construct a data set of NP semantic structures. We also propose a transition-based algorithm which jointly identifies both the nodes in a semantic structure tree and semantic relations between them. Compared to the baseline, our method improves the performance of NP semantic structure analysis by 2.7 points, while further incorporating external dictionary boosts the performance by 7.1 points."
P15-1093,Joint Case Argument Identification for {J}apanese Predicate Argument Structure Analysis,2015,11,2,2,1,9339,hiroki ouchi,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Existing methods for Japanese predicate argument structure (PAS) analysis identify case arguments of each predicate without considering interactions between the target PAS and others in a sentence. However, the argument structures of the predicates in a sentence are semantically related to each other. This paper proposes new methods for Japanese PAS analysis to jointly identify case arguments of all predicates in a sentence by (1) modeling multiple PAS interactions with a bipartite graph and (2) approximately searching optimal PAS combinations. Performing experiments on the NAIST Text Corpus, we demonstrate that our joint analysis methods substantially outperform a strong baseline and are comparable to previous work."
P12-1046,{B}ayesian Symbol-Refined Tree Substitution Grammars for Syntactic Parsing,2012,31,29,1,1,7901,hiroyuki shindo,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each nonterminal symbol can be refined (subcategorized) to fit the training data. We aim to provide a unified model where TSG rules and symbol refinement are learned from training data in a fully automatic and consistent fashion. We present a novel probabilistic SR-TSG model based on the hierarchical Pitman-Yor Process to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on Markov Chain Monte Carlo (MCMC) sampling. Our SR-TSG parser achieves an F1 score of 92.4% in the Wall Street Journal (WSJ) English Penn Treebank parsing task, which is a 7.7 point improvement over a conventional Bayesian TSG parser, and better than state-of-the-art discriminative reranking parsers."
P11-2036,Insertion Operator for {B}ayesian Tree Substitution Grammars,2011,15,4,1,1,7901,hiroyuki shindo,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG."
S10-1086,{MSS}: Investigating the Effectiveness of Domain Combinations and Topic Features for Word Sense Disambiguation,2010,8,4,5,0,44776,sanae fujita,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16) and focused on the following: (1) investigating domain differences, (2) incorporating topic features, and (3) predicting new unknown senses. We experimented with Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers. We achieved 80.1% accuracy in our experiments."
P10-2025,Word Alignment with Synonym Regularization,2010,13,3,1,1,7901,hiroyuki shindo,Proceedings of the {ACL} 2010 Conference Short Papers,0,We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.
