2020.acl-main.529,P19-1425,1,0.0874245,"hmarks show that AdvAug achieves significant improvements over the Transformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g. back-translation) without using extra corpora. 1 Introduction Recent work in neural machine translation (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has led to dramatic improvements in both research and commercial systems (Wu et al., 2016). However, a key weakness of contemporary systems is that performance can drop dramatically when they are exposed to input perturbations (Belinkov and Bisk, 2018; Cheng et al., 2019), even when these perturbations are not strong enough to alter the meaning of the input sentence. Consider a Chinese sentence, “zhejia feiji meiyou zhuangshang zhujia huo yiyuan, shizai shi qiji”. If we change the word “huo (或)” to its synonym“ji (及)”, the Transformer model will generate contradictory results of “It was indeed a miracle that the plane did not touch down at home or hospital.” versus “It was a miracle that the plane landed at home and hospital.” Such perturbations can readily be found in many public benchmarks and real-world applications. This lack of stability not only lowers t"
2020.acl-main.529,P18-1163,1,0.935015,"os. At the root of this problem are two interrelated issues: first, machine translation training sets are insufficiently diverse, and second, NMT architectures are powerful enough to overfit — and, in extreme cases, memorize — the observed training examples, without learning to generalize to unseen perturbed examples. One potential solution is data augmentation which introduces noise to make the NMT model training more robust. In general, two types of noise can be distinguished: (1) continuous noise which is modeled as a realvalued vector applied to word embeddings (Miyato et al., 2016, 2017; Cheng et al., 2018; Sato et al., 2019), and (2) discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences (Belinkov and Bisk, 2018; Sperber et al., 2017; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Karpukhin et al., 2019). In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an L2 -norm ball but does not guarantee to maintain semantics. While constructing semantics-preserving continuous noise in a high-dimensio"
2020.acl-main.529,C18-1055,0,0.161361,"nd, in extreme cases, memorize — the observed training examples, without learning to generalize to unseen perturbed examples. One potential solution is data augmentation which introduces noise to make the NMT model training more robust. In general, two types of noise can be distinguished: (1) continuous noise which is modeled as a realvalued vector applied to word embeddings (Miyato et al., 2016, 2017; Cheng et al., 2018; Sato et al., 2019), and (2) discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences (Belinkov and Bisk, 2018; Sperber et al., 2017; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Karpukhin et al., 2019). In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an L2 -norm ball but does not guarantee to maintain semantics. While constructing semantics-preserving continuous noise in a high-dimensional space proves to be non-trivial, state-of-the-art NMT models are currently based on adversarial examples of discrete noise. For instance, Cheng et al. (2019) generate adversarial sentences using"
2020.acl-main.529,D18-1045,0,0.417258,"novelty of our paper is the new vicinity distribution for adversarial examples and the augmentation algorithm for sequence-to-sequence learning. Extensive experimental results on three translation benchmarks (NIST Chinese-English, IWSLT English-French, and WMT English-German) show that our approach achieves significant improvements of up to 4.9 BLEU points over the Transformer (Vaswani et al., 2017), outperforming the former state-of-the-art in adversarial learning (Cheng et al., 2019) by up to 3.3 BLEU points. When compared with widely-used data augmentation methods (Sennrich et al., 2016a; Edunov et al., 2018), we find that our approach yields better performance even without using extra corpora. We conduct ablation studies to gain further insights into which parts of our approach matter most. In summary, our contributions are as follows: 1. We propose to sample adversarial examples from a new vicinity distribution and utilize their embeddings, instead of their data points, to augment the model training. 2. We design an effective augmentation algorithm for learning sequence-to-sequence NMT models via mini-batches. 3. Our approach achieves significant improvements over the Transformer and prior state"
2020.acl-main.529,P17-2090,0,0.0296109,". In contrast, our model incorporates embeddings of virtual sentences that contain “danshi(但是)” or its synonym “dan(但)”. This encourages our model to learn to push their embeddings closer during training, and make our model more robust to small perturbations in real sentences. 5 Related Work Data Augmentation. Data augmentation is an effective method to improve machine translation performance. Existing methods in NMT may be divided into two categories, based upon extra corpora (Sennrich et al., 2016a; Cheng et al., 2016; Zhang and Zong, 2016; Edunov et al., 2018) or original parallel corpora (Fadaee et al., 2017; Wang et al., 2018; Cheng et al., 2019). Recently, mixup (Zhang et al., 2018) has become a popular data augmentation technique for semi-supervised learning (Berthelot et al., 2019) and overcoming real-world noisy data (Jiang et al., 2019). Unlike prior works, we introduce a new method to augment the representations of the adversarial examples in sequence-tosequence training of the NMT model. Even without extra monolingual corpora, our approach substantially outperforms the widely-used back-translation methods (Sennrich et al., 2016a; Edunov et al., 2018). Furthermore, we can obtain even bette"
2020.acl-main.529,P02-1040,0,0.108757,"4.81 46.13 48.73 47.96 49.81 MT04 46.20 45.99 46.49 47.39 47.81 47.80 46.61 47.54 48.60 48.86 50.61 MT05 44.96 45.32 45.88 46.58 45.69 46.81 46.08 46.88 48.76 49.88 50.72 MT08 35.11 35.84 35.90 37.38 36.43 36.79 36.00 37.21 39.03 39.63 40.45 Table 1: Baseline comparison on NIST Chinese-English translation. * indicates the model uses extra corpora and means not elaborating on its training loss. 4 4.1 Experiments Setup We verify our approach on translation tasks for three language pairs: Chinese-English, EnglishFrench, and English-German. The performance is evaluated with the 4-gram BLEU score (Papineni et al., 2002) calculated by the multi-bleu.perl script. We report case-sensitive tokenized BLEU scores for English-French and English-German, and caseinsensitive tokenized BLEU scores for ChineseEnglish. Note that all reported BLEU scores in our approach are from a single model rather than averaging multiple models (Vaswani et al., 2017). For the Chinese-English translation task, the training set is the LDC corpus consisting of 1.2M sentence pairs. The NIST 2006 dataset is used as the validation set, and NIST 02, 03, 04, 05, 08 are used as the test sets. We apply byte-pair encoding (BPE) (Sennrich et al.,"
2020.acl-main.529,P19-1020,0,0.28066,"his problem are two interrelated issues: first, machine translation training sets are insufficiently diverse, and second, NMT architectures are powerful enough to overfit — and, in extreme cases, memorize — the observed training examples, without learning to generalize to unseen perturbed examples. One potential solution is data augmentation which introduces noise to make the NMT model training more robust. In general, two types of noise can be distinguished: (1) continuous noise which is modeled as a realvalued vector applied to word embeddings (Miyato et al., 2016, 2017; Cheng et al., 2018; Sato et al., 2019), and (2) discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences (Belinkov and Bisk, 2018; Sperber et al., 2017; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Karpukhin et al., 2019). In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an L2 -norm ball but does not guarantee to maintain semantics. While constructing semantics-preserving continuous noise in a high-dimensional space proves to"
2020.acl-main.529,P16-1009,0,0.520508,"o in the data space. The novelty of our paper is the new vicinity distribution for adversarial examples and the augmentation algorithm for sequence-to-sequence learning. Extensive experimental results on three translation benchmarks (NIST Chinese-English, IWSLT English-French, and WMT English-German) show that our approach achieves significant improvements of up to 4.9 BLEU points over the Transformer (Vaswani et al., 2017), outperforming the former state-of-the-art in adversarial learning (Cheng et al., 2019) by up to 3.3 BLEU points. When compared with widely-used data augmentation methods (Sennrich et al., 2016a; Edunov et al., 2018), we find that our approach yields better performance even without using extra corpora. We conduct ablation studies to gain further insights into which parts of our approach matter most. In summary, our contributions are as follows: 1. We propose to sample adversarial examples from a new vicinity distribution and utilize their embeddings, instead of their data points, to augment the model training. 2. We design an effective augmentation algorithm for learning sequence-to-sequence NMT models via mini-batches. 3. Our approach achieves significant improvements over the Tran"
2020.acl-main.529,P16-1162,0,0.70201,"o in the data space. The novelty of our paper is the new vicinity distribution for adversarial examples and the augmentation algorithm for sequence-to-sequence learning. Extensive experimental results on three translation benchmarks (NIST Chinese-English, IWSLT English-French, and WMT English-German) show that our approach achieves significant improvements of up to 4.9 BLEU points over the Transformer (Vaswani et al., 2017), outperforming the former state-of-the-art in adversarial learning (Cheng et al., 2019) by up to 3.3 BLEU points. When compared with widely-used data augmentation methods (Sennrich et al., 2016a; Edunov et al., 2018), we find that our approach yields better performance even without using extra corpora. We conduct ablation studies to gain further insights into which parts of our approach matter most. In summary, our contributions are as follows: 1. We propose to sample adversarial examples from a new vicinity distribution and utilize their embeddings, instead of their data points, to augment the model training. 2. We design an effective augmentation algorithm for learning sequence-to-sequence NMT models via mini-batches. 3. Our approach achieves significant improvements over the Tran"
2020.acl-main.529,D19-5506,1,0.907372,"without learning to generalize to unseen perturbed examples. One potential solution is data augmentation which introduces noise to make the NMT model training more robust. In general, two types of noise can be distinguished: (1) continuous noise which is modeled as a realvalued vector applied to word embeddings (Miyato et al., 2016, 2017; Cheng et al., 2018; Sato et al., 2019), and (2) discrete noise which adds, deletes, and/or replaces characters or words in the observed sentences (Belinkov and Bisk, 2018; Sperber et al., 2017; Ebrahimi et al., 2018; Michel et al., 2019; Cheng et al., 2019; Karpukhin et al., 2019). In both cases, the challenge is to ensure that the noisy examples are still semantically valid translation pairs. In the case of continuous noise, it only ensures that the noise vector lies within an L2 -norm ball but does not guarantee to maintain semantics. While constructing semantics-preserving continuous noise in a high-dimensional space proves to be non-trivial, state-of-the-art NMT models are currently based on adversarial examples of discrete noise. For instance, Cheng et al. (2019) generate adversarial sentences using discrete word replacements in both the source and target, guided"
2020.acl-main.529,D18-1100,0,0.0247312,"del incorporates embeddings of virtual sentences that contain “danshi(但是)” or its synonym “dan(但)”. This encourages our model to learn to push their embeddings closer during training, and make our model more robust to small perturbations in real sentences. 5 Related Work Data Augmentation. Data augmentation is an effective method to improve machine translation performance. Existing methods in NMT may be divided into two categories, based upon extra corpora (Sennrich et al., 2016a; Cheng et al., 2016; Zhang and Zong, 2016; Edunov et al., 2018) or original parallel corpora (Fadaee et al., 2017; Wang et al., 2018; Cheng et al., 2019). Recently, mixup (Zhang et al., 2018) has become a popular data augmentation technique for semi-supervised learning (Berthelot et al., 2019) and overcoming real-world noisy data (Jiang et al., 2019). Unlike prior works, we introduce a new method to augment the representations of the adversarial examples in sequence-tosequence training of the NMT model. Even without extra monolingual corpora, our approach substantially outperforms the widely-used back-translation methods (Sennrich et al., 2016a; Edunov et al., 2018). Furthermore, we can obtain even better performance by in"
2020.acl-main.529,D16-1160,0,0.0179344,"the word “danshi(但是)” seldom occurs in this context in our training data. In contrast, our model incorporates embeddings of virtual sentences that contain “danshi(但是)” or its synonym “dan(但)”. This encourages our model to learn to push their embeddings closer during training, and make our model more robust to small perturbations in real sentences. 5 Related Work Data Augmentation. Data augmentation is an effective method to improve machine translation performance. Existing methods in NMT may be divided into two categories, based upon extra corpora (Sennrich et al., 2016a; Cheng et al., 2016; Zhang and Zong, 2016; Edunov et al., 2018) or original parallel corpora (Fadaee et al., 2017; Wang et al., 2018; Cheng et al., 2019). Recently, mixup (Zhang et al., 2018) has become a popular data augmentation technique for semi-supervised learning (Berthelot et al., 2019) and overcoming real-world noisy data (Jiang et al., 2019). Unlike prior works, we introduce a new method to augment the representations of the adversarial examples in sequence-tosequence training of the NMT model. Even without extra monolingual corpora, our approach substantially outperforms the widely-used back-translation methods (Sennrich et"
2020.findings-emnlp.138,2020.findings-emnlp.414,0,0.0217891,"’s resulting smoothies before aggregating for categories. 14 Examples include bow + person = boerson and junk + time = junime. Figure 2: Pretrained BERT’s similarity measures for each semantic relation with n &gt; 15 instances. sion of Figure 1(a) using this tokenization is almost identical to the original. Upon further examination, we find that while pre-tokenizing with PAXOBS results in a larger number of wordpiece tokens (an average of 4.55 vs. 3.30), a similar leap occurs in compounds (3.41 vs. 2.48), suggesting that WP does not produce morphologically accurate segments for compounds either (Bostrom and Durrett, 2020). The crux of the issue must therefore lie within BERT. In conclusion, we have shown that the root cause of blend mistreatment in large contextual transformer models is their form, although knowing only their sequence structure is not sufficient. Therefore, in the following section we suggest models which attempt to identify blend segmentation points, but also ones which attempt recovery of their original bases, in order to place them in an appropriate topical context. 4 Will it Unblend? We next test to what extent existing models can help systems understand the meaning of novel blends, an asp"
2020.findings-emnlp.138,A00-1031,0,0.0525756,"from blend formation. Then, we assess how easily different models can recognize the structure and recover the origin of blends, and find that context-aware embedding systems outperform character-level and contextfree embeddings, although their results are still far from satisfactory. 1 Introduction For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level, where novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016; Heigold et al., 2017). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016).1 But while this ap1 Another approach is to operate directly at the character level (e.g., Ling et al., 2015), but this has not been widely proach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOV terms that cannot be cleanly decomposed into meaningful subtoken segments. In this pa"
2020.findings-emnlp.138,D11-1052,0,0.0347626,"application of such a model to blends is complicated by the relative lack of labeled training data, as well as the irregularity of the underlying phenomenon. Novel blends are an example of linguistic creativity, which frequently operates at the subword level. Related phenomena include eggcorns, which are alternative spellings that yield an apparently more transparent relationship between form and function (Reddy, 2009); puns, which substitute words in new contexts based on phonological similarity (Jaech et al., 2016); respellings that attempt to reintroduce prosodic expression into spelling (Brody and Diakopoulos, 2011); intentional obfuscation (Zalmout et al., 2019); and typographical errors (Heigold et al., 2017). We therefore view blends as an instance of a broad set of creative phenomena that poses challenges for the token-based approaches that currently dominate natural language processing. 6 Conclusion This work focuses on the challenge of interpreting novel blends, which requires integrating subword structure and contextual features. We present a new dataset annotated using a novel characterlevel schema as well as for semantic tags, and offer preliminary evaluations showing that (a) blends are handled"
2020.findings-emnlp.138,chrupala-etal-2008-learning,0,0.12639,"Missing"
2020.findings-emnlp.138,P19-4007,0,0.0357857,"Missing"
2020.findings-emnlp.138,J10-1005,0,0.828527,"variant (+OTHER - BASE) we add the true base from the other side to the context, in order to level the playing field with the baselines, which we describe next (see Appendix A.3 for implementation details): • Character RNN. We separately train a forward and a backward character-level RNN on over 100,000 documents from the Westbury corpus (Shaoul, 2010). We feed the blend’s left (right) context to the forward (backward) RNN, then record the probability of each A (B) candidate as a continuation of the context, computed as the average of character log-likelihoods. • Edit distance (ED). Following Cook and Stevenson (2010), we compare the string similarity (Levenshtein distance) between base candidate pairs’ orthographic forms.22 Single-side 20 The full lists are available on the repository. This is crucial, since predicting a ‘##’-initial suffix token effectively attaches it to the preceding token 22 A variant using phonological forms, extracted from a phonological lexicon (Lee et al., 2020), was limited by only having pronunciations for a fraction of bases and candidates. In cases where both base pronunciations were found the ranking was good, hinting at a promising avenue for future work by implementing auto"
2020.findings-emnlp.138,Q18-1003,0,0.211314,"Missing"
2020.findings-emnlp.138,I17-1058,0,0.0319155,"ER to predict bases perfectly relative to the CONTEXT variant shows that they typically contain one or more of the bases in their entirety (e.g., eggcessories appears near multiple occurrences of the word eggs). By contrast, in some longer contexts containing diverse topics, the inclusion of context wipes out the accessibility of the component bases, typically the first one (e.g. chesticle, in which the context does not mention body parts, or cancerchondria which mentions the word condition but neither of the bases). 5 Related Work Prior work on blends has largely focused on generation (e.g., Das and Ghosh, 2017; Simon, 2018; Deri and Knight, 2015; Kulkarni and Wang, 2018; Smith et al., 2014). While Gangal et al. (2017) provide a unified dataset of 1,579 blends, annotated for bases, they do not provide contexts for real-world appearances of the blends, nor a breakdown of the semantic relationship between their constituents. Moreover, some are synthetically generated by a seq2seq model. In addition, these works all restrict their models to linear two-word blends. Our PAXOBS scheme handles nonlinear and multi-base blends. Cook and Stevenson (2010) presented a noncontextual method for blend base detecti"
2020.findings-emnlp.138,N15-1021,0,0.400618,"enting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016).1 But while this ap1 Another approach is to operate directly at the character level (e.g., Ling et al., 2015), but this has not been widely proach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOV terms that cannot be cleanly decomposed into meaningful subtoken segments. In this paper we address a particularly interesting and challenging source of OOV terms: novel blends (Algeo, 1977), also known as portmanteaux (Deri and Knight, 2015). Blends are constructed from the combination of multiple bases into a new form, in which some characters is shared across both bases: for example, shop + optics = shoptics. In this way, blends differ from other lexical compounds (e.g., watermelon = water + melon), which are formed by simple concatenation. Examples of OOV blends and their bases from our novel English blends dataset, collected from a natural source linked to the blends’ originating contexts (§2), are presented in Table 1. OOV blends are especially challenging to process, due to their combination of function-level semantic novel"
2020.findings-emnlp.138,N19-1423,0,0.189266,"tructure and recover the origin of blends, and find that context-aware embedding systems outperform character-level and contextfree embeddings, although their results are still far from satisfactory. 1 Introduction For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level, where novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016; Heigold et al., 2017). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016).1 But while this ap1 Another approach is to operate directly at the character level (e.g., Ling et al., 2015), but this has not been widely proach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOV terms that cannot be cleanly decomposed into meaningful subtoken segments. In this paper we address a particularly interesting and challenging source of OOV terms: novel blends ("
2020.findings-emnlp.138,W15-0122,0,0.0558008,"Missing"
2020.findings-emnlp.138,D17-1315,0,0.117999,"while a lenient evaluation permits the inclusion of shared material: [shop;tics] is leniently sound, but [sh;op;tics] is strictly sound as well. We report micro-level precision, as well as F1 computed with both lenient and strict recall, and lenient exact match. We ignore prefixes and suffixes, and allow models to freely separate or include them in the adjacent base. Systems. We compare the following systems (see Appendix A.2 for implementation details): • All-chars. A baseline which marks every character as its own segment (perfect recall). • Sequence Tagger. We annotated the 1,579 blends in Gangal et al. (2017)’s dataset17 for PAXOBS tags and used them for training a supervised neural character-level tagger, whose results are converted into segmentations. • WordPiece. We run WP “out of the box”. • In-domain Subwords. We train BPE (Sennrich et al., 2016) and Unigram LM (Kudo, 2018) subword tokenizers on news data from the Corpus of Contemporary American English (1990– 2015; Davies, 2008) using the sentencepiece package (Kudo and Richardson, 2018), set to the same vocabulary size as the WP model. Results. The results in Table 2 show that all models struggle to find correct segmentation, even compared"
2020.findings-emnlp.138,2020.acl-main.365,0,0.0477064,"Missing"
2020.findings-emnlp.138,N16-1079,0,0.0206978,"gs of morphemes, thereby touching on two of the main tasks undertaken in our paper. However, the application of such a model to blends is complicated by the relative lack of labeled training data, as well as the irregularity of the underlying phenomenon. Novel blends are an example of linguistic creativity, which frequently operates at the subword level. Related phenomena include eggcorns, which are alternative spellings that yield an apparently more transparent relationship between form and function (Reddy, 2009); puns, which substitute words in new contexts based on phonological similarity (Jaech et al., 2016); respellings that attempt to reintroduce prosodic expression into spelling (Brody and Diakopoulos, 2011); intentional obfuscation (Zalmout et al., 2019); and typographical errors (Heigold et al., 2017). We therefore view blends as an instance of a broad set of creative phenomena that poses challenges for the token-based approaches that currently dominate natural language processing. 6 Conclusion This work focuses on the challenge of interpreting novel blends, which requires integrating subword structure and contextual features. We present a new dataset annotated using a novel characterlevel s"
2020.findings-emnlp.138,P19-1356,0,0.0482618,"Missing"
2020.findings-emnlp.138,W19-4203,0,0.02274,"corporate context with context-sensitive language models, and add the task of blend segmentation. Extracting the semantics of constituents from larger phrases is not a problem unique to singletoken blends. Shwartz and Waterson (2018) worked on multi-word compounds; Maddela et al. 1532 (2019) segment hashtags, roughly half of which are akin to our notion of compounds, by training a neural scoring system over features extracted from word form, dictionary lookup and language model probabilities. Another connection is to the learning of morphological rules, e.g., for processes such as derivation (Kondratyuk, 2019) and lemmatization (Chrupala, 2006; Ullman et al., 1976; Hirschberg, 1977). Cotterell and Sch¨utze (2018) present a supervised model of derivational morphology that jointly accounts for segmentation as well as composition of static word embeddings from the embeddings of morphemes, thereby touching on two of the main tasks undertaken in our paper. However, the application of such a model to blends is complicated by the relative lack of labeled training data, as well as the irregularity of the underlying phenomenon. Novel blends are an example of linguistic creativity, which frequently operates"
2020.findings-emnlp.138,P18-1007,0,0.0585292,"Missing"
2020.findings-emnlp.138,D18-2012,0,0.0443669,"Missing"
2020.findings-emnlp.138,N18-1129,0,0.217148,"r compounds. Smoothies. If differences in surface form are what drives differences in contextualized representations, then transforming the compounds into mock-blends, which we term “smoothies”, should eliminate the differences between the two complex word types: we would expect the function of the similarity of a contextual encoding of a blend to its bases given a context it naturally occurs in to be approximately the same function of simi1528 larity of a contextual encoding of a smoothie to its bases given the context the original compound occurs in. We create our smoothies using C OPYC AT (Kulkarni and Wang, 2018), a model which generates blends from two base forms via a sequence of character copy and delete actions learned over features extracted from an language model, an LSTM, and length-based heuristics. We train an ensemble of 50 C OPY C AT models on the blends from Deri and Knight (2015) and apply them to our novel compounds.13 Since C OPY C AT can produce only linear blends, we compare the BERT correspondence for smoothies against linear blends only (whose aggregate similarities are notably similar to those of blends as a whole). In creating the smoothies,14 we made sure that the overall rate of"
2020.findings-emnlp.138,2020.lrec-1.521,0,0.0274755,"ht) context to the forward (backward) RNN, then record the probability of each A (B) candidate as a continuation of the context, computed as the average of character log-likelihoods. • Edit distance (ED). Following Cook and Stevenson (2010), we compare the string similarity (Levenshtein distance) between base candidate pairs’ orthographic forms.22 Single-side 20 The full lists are available on the repository. This is crucial, since predicting a ‘##’-initial suffix token effectively attaches it to the preceding token 22 A variant using phonological forms, extracted from a phonological lexicon (Lee et al., 2020), was limited by only having pronunciations for a fraction of bases and candidates. In cases where both base pronunciations were found the ranking was good, hinting at a promising avenue for future work by implementing automatic text-to-phone modeling. 1531 21 most of its power lies in processing context and not in word form representation. This conclusion is further supported by the superior performance of the static type-level GloVe embeddings, whose lead over fastText and BERT CONTEXT in all MRR measures suggests that word form is less helpful even in uncontextualized settings. The particul"
2020.findings-emnlp.138,D15-1176,0,0.0925002,"Missing"
2020.findings-emnlp.138,P19-1242,0,0.0497503,"Missing"
2020.findings-emnlp.138,L18-1008,0,0.0310683,"MRRModel A B ! P@1 Lower bound .115 .257 .036 .014 Character RNN Edit distance fastText GloVe .162 .176⇤ .357⇤ .449⇤ .368 .432⇤ .610⇤ .734⇤ .060 .066 .167 .188 .021 .014 .127 .127 BERT RANKER +OTHER - BASE .392 .403⇤ .379 .379⇤ .711 .703⇤ .675 .668⇤ .288 .264 .147 .127 CONTEXT +OTHER - BASE Table 3: Results for component recovery. ⇤ Results dependent on knowledge of the correct base on the other side. prediction fixes one base and ranks candidates from the other side based on similarity. • Static embeddings. We calculate cosine similarity between candidate base pairs’ embeddings in fastText (Mikolov et al., 2018) and GloVe (Pennington et al., 2014). fastText includes character n-grams, allowing an assessment of the utility of subword information. To summarize, both ED and Static methods are contextless pair-matchers which operate in the +OTHER - BASE knowledge setup when evaluated for MRR-A and MRR-B; Character RNN is a single-base ranker which uses context from one side only and cannot be helped by knowledge of the other base. Results. Results are presented in Table 3. We note the higher performance on B bases achieved by all models, a fact which advantages WordPiece which leaves word-initial pieces"
2020.findings-emnlp.138,D14-1162,0,0.0849958,"115 .257 .036 .014 Character RNN Edit distance fastText GloVe .162 .176⇤ .357⇤ .449⇤ .368 .432⇤ .610⇤ .734⇤ .060 .066 .167 .188 .021 .014 .127 .127 BERT RANKER +OTHER - BASE .392 .403⇤ .379 .379⇤ .711 .703⇤ .675 .668⇤ .288 .264 .147 .127 CONTEXT +OTHER - BASE Table 3: Results for component recovery. ⇤ Results dependent on knowledge of the correct base on the other side. prediction fixes one base and ranks candidates from the other side based on similarity. • Static embeddings. We calculate cosine similarity between candidate base pairs’ embeddings in fastText (Mikolov et al., 2018) and GloVe (Pennington et al., 2014). fastText includes character n-grams, allowing an assessment of the utility of subword information. To summarize, both ED and Static methods are contextless pair-matchers which operate in the +OTHER - BASE knowledge setup when evaluated for MRR-A and MRR-B; Character RNN is a single-base ranker which uses context from one side only and cannot be helped by knowledge of the other base. Results. Results are presented in Table 3. We note the higher performance on B bases achieved by all models, a fact which advantages WordPiece which leaves word-initial pieces unmarked (see §3.2), as opposed to m"
2020.findings-emnlp.138,N18-2035,0,0.0950488,"s among blends extracted from a curated lexicon (Cook and Stevenson, 2010). One possible explanation for this discrepancy is that words that make it into common use may be simpler in their surface quality. 7 The model trained on this split, set to de = dh = 50, slightly outperformed an identical one trained on the LEXI CAL split, and its test set accuracy on the original dataset is .721, close to replication. The LEXICAL split was created to correct an over-representation of some compound bases in RANDOM , one which biases statistical models toward lexical memorization (see details in §4.1 of Shwartz and Waterson (2018)), but has no bearing on our dataset. 8 The numbers for blends only are .148 / .079 vs. .063 / .020 for majority class. 9 May be pronounced like “pack sobs”. 10 This framework is loosely similar to edit scripts (Chrupala et al., 2008), but rather than transducing one string into another, the task is to combine two strings into a third. 3 Blends in Context Novel blends are a unique linguistic phenomenon, posing challenges for automated systems on many different levels. However, the sparsity of their appearances in real-world text, as well as the expertise required for creating a natural languag"
2020.findings-emnlp.138,P10-1070,0,0.0618798,"Missing"
2020.findings-emnlp.138,2020.coling-main.572,1,0.832939,"ovel unsupervised base recovery method using contextualized masked language models, BERT RANKER. While this system performs well relative to others, we find substantial room for improvement. In our view, these results demonstrate the need for future work on our novel dataset and associated tasks.2 2 Complex Words Dataset Our proposed investigation of the behavior of NLP systems on novel complex words requires a highquality, reliable resource of truly novel blends and compounds in their original contexts, annotated for character sequence composition and semantic properties. The NYTWIT dataset (Pinter et al., 2020) contains English words new to the New York Times extracted by a bot3 between the dates of November 2017 and March 2019 with associated news article contexts. Words were annotated for their type of novelty. We extract and further annotate three types from this dataset (version 1.1): blends (142 items), transparent compounds (121), and opaque compounds (49).4 The difference between the compound classes is semantic and somewhat subjective: transparent compounds have meanings which are comprehensible with little context (e.g. quizmaker, a person who makes quizzes), while 2 We release our code and"
2020.findings-emnlp.138,D19-5555,0,0.0214338,"y the relative lack of labeled training data, as well as the irregularity of the underlying phenomenon. Novel blends are an example of linguistic creativity, which frequently operates at the subword level. Related phenomena include eggcorns, which are alternative spellings that yield an apparently more transparent relationship between form and function (Reddy, 2009); puns, which substitute words in new contexts based on phonological similarity (Jaech et al., 2016); respellings that attempt to reintroduce prosodic expression into spelling (Brody and Diakopoulos, 2011); intentional obfuscation (Zalmout et al., 2019); and typographical errors (Heigold et al., 2017). We therefore view blends as an instance of a broad set of creative phenomena that poses challenges for the token-based approaches that currently dominate natural language processing. 6 Conclusion This work focuses on the challenge of interpreting novel blends, which requires integrating subword structure and contextual features. We present a new dataset annotated using a novel characterlevel schema as well as for semantic tags, and offer preliminary evaluations showing that (a) blends are handled differently than compounds by BERT, due mostly"
2020.findings-emnlp.138,W09-2003,0,0.0381024,"accounts for segmentation as well as composition of static word embeddings from the embeddings of morphemes, thereby touching on two of the main tasks undertaken in our paper. However, the application of such a model to blends is complicated by the relative lack of labeled training data, as well as the irregularity of the underlying phenomenon. Novel blends are an example of linguistic creativity, which frequently operates at the subword level. Related phenomena include eggcorns, which are alternative spellings that yield an apparently more transparent relationship between form and function (Reddy, 2009); puns, which substitute words in new contexts based on phonological similarity (Jaech et al., 2016); respellings that attempt to reintroduce prosodic expression into spelling (Brody and Diakopoulos, 2011); intentional obfuscation (Zalmout et al., 2019); and typographical errors (Heigold et al., 2017). We therefore view blends as an instance of a broad set of creative phenomena that poses challenges for the token-based approaches that currently dominate natural language processing. 6 Conclusion This work focuses on the challenge of interpreting novel blends, which requires integrating subword"
2020.findings-emnlp.138,P16-1162,0,0.407364,"s are still far from satisfactory. 1 Introduction For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level, where novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016; Heigold et al., 2017). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016).1 But while this ap1 Another approach is to operate directly at the character level (e.g., Ling et al., 2015), but this has not been widely proach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOV terms that cannot be cleanly decomposed into meaningful subtoken segments. In this paper we address a particularly interesting and challenging source of OOV terms: novel blends (Algeo, 1977), also known as portmanteaux (Deri and Knight, 2015). Blends are constructed from the combination of multiple bases into a new form, in which some characte"
2021.naacl-main.184,P11-1137,1,0.801572,"Missing"
2021.naacl-main.184,C18-1152,0,0.0200379,"n the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dial"
2021.naacl-main.184,N19-1004,0,0.0214281,"Missing"
2021.naacl-main.184,N18-1108,0,0.0275352,"an improve performance on the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork f"
2021.naacl-main.184,P19-1478,0,0.013874,"tures that could be recognized directly from surface forms, or in some cases, from part-of-speech (POS) sequences. In contrast, we show that it is possible to learn to recognize features from examples, enabling the recognition of features for which it is difficult or impossible to craft surface or POS patterns. Minimal pairs in NLP. A distinguishing aspect of our approach is the use of minimal pairs rather than conventional labeled data. Minimal pairs are well known in natural language processing from the Winograd Schema (Levesque et al., 2012), which is traditionally used for evaluation, but Kocijan et al. (2019) show that fine-tuning on a related dataset of minimal pairs can improve performance on the Winograd Schema itself. A similar idea arises in counterfactually-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe ne"
2021.naacl-main.184,2020.acl-main.442,0,0.0257183,"Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performance of NLP systems across the diverse range of linguistic phenomena that may occur in any given language. 8 Ethical Considerations Our objective in building dialect feature recognizers is to aid developers and researchers to effectively benchmark NLP model performance across and within different dialects, and to assist social scientists and dialectologists studying dialect use. The capability to detect dialectal features may enable developers to test for and mitigate any unintentional and undesirable biases in their models towards or against individuals speaking particul"
2021.naacl-main.184,2020.acl-main.479,0,0.0198767,"ally-augmented data (Kaushik et al., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performan"
2021.naacl-main.184,E14-3004,0,0.0116155,"ct features from corpora. For example, Dunn (2018, 2019) induces a set of constructions (short sequences of words, parts-of-speech, or constituents) from a “neutral” corpus, and then identifies constructions with distinctive distributions over the geographical subcorpora of the International Corpus of English (ICE). In social media, features of African American Vernacular English (AAVE) can be identified by correlating linguistic frequencies with the aggregate demographic statistics of the geographical areas from which geotagged social media was posted (Eisen6 Related Work stein et al., 2011; Stewart, 2014; Blodgett et al., 2016). In contrast, we are interested in detecting Dialect classification. Prior work on dialect in natural language processing has focused on distin- predefined dialect features from well-validated reguishing between dialects (and closely-related lan- sources such as dialect atlases. guages). For example, the VarDial 2014 shared task Along these lines, Jørgensen et al. (2015) and required systems to distinguish between nation- Jones (2015) designed lexical patterns to identify level language varieties, such as British versus U.S. non-standard spellings that match known phon"
2021.naacl-main.184,2020.emnlp-main.355,0,0.0209874,"l., 2019) and contrast sets (Gardner et al., 2020), in which annotators are asked to identify the minimal change to an example that is sufficient to alter its label. However, those approaches use counterfactual examples to augment an existing training set, while we propose minimal pairs as a replacement for large-scale labeled data. Minimal pairs have also been used to design controlled experiments and probe neural models’ ability to capture various linguistic phenomena (Gulordava et al., 2018; Ettinger et al., 2018; Futrell et al., 2019; Gardner et al., 2020; Schuster et al., 2020). Finally, Liang et al. (2020) use contrastive explanations as part of an active learning framework to improve data efficiency. Our work shares the objective of Liang et al. (2020) to improve data efficiency, but is methodologically closer to probing work that uses minimal pairs to represent specific linguistic features. 7 Conclusion not exist. Future work will extend this approach to multiple dialects, focusing on cases in which features are shared across two or more dialects. This lays the groundwork for the creation of dialectbased “checklists” (Ribeiro et al., 2020) to assess the performance of NLP systems across the d"
2021.naacl-main.184,P19-1335,0,0.0166315,"dialect features. The architecture can be trained from two possible sources of supervision: (1) thousands of labeled corpus examples, (2) a small set of minimal pairs, which are hand-crafted examples designed to highlight the key aspects of each dialect feature (as in the “typical example” field of Figure 1). Because most dialects have little or no labeled data, the latter scenario is more realistic for most dialects. We also consider a multitask architecture that learns across multiple features by encoding the feature names, similar to recent work on few-shot or zero-shot multitask learning (Logeswaran et al., 2019; Brown et al., 2020). In Sections 4 and 5, we discuss empirical evaluations of these models. Our main findings are: • It is possible to detect individual dialect features: several features can be recognized with reasonably high accuracy. Our best models achieve a macro-AUC of .848 across ten grammatical features for which a large test set is available. • This performance can be obtained by training on roughly five minimal pairs per feature. Minimal pairs are significantly more effective for training than a comparable number of corpus examples. • Dialect feature recognizers can be used to rank"
2021.naacl-main.184,W17-1201,0,0.0538797,"Missing"
2021.scil-1.61,A00-1031,0,0.0774177,"ll it Unblend? Jacob Eisenstein Cassandra L. Jacobs Yuval Pinter Google Research School of Interactive Computing Department of Psychology Seattle, WA, USA Georgia Institute of Technology University of Wisconsin jeisenstein@google.com Madison, WI, USA Atlanta, GA, USA cjacobs2@wisc.edu uvp@gatech.edu For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level. Such novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016). While this approach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOVs that cannot be cleanly decomposed into meaningful segments. Our work addresses a challenging source of OOV terms: novel blends (Algeo, 1977), also known as portmanteaux. Blends are constructed from the combination of multiple bases"
2021.scil-1.61,J10-1005,0,0.00913123,"gure 1, this process eliminates the representation gap, leading us to conclude that the difficulty in representing blends is due primarily to the complex relationship between their surface forms and meanings. Unblending. As part of blend understanding, blends may be understood through segmentation and component identification, which we term recovery. We cast the problem as a two-step pipeline, beginning with detection of morphological boundaries within blends, followed by a selection of the correct bases from a list of candidates constructed given the segmentation and a vocabulary (similar to Cook and Stevenson, 2010). Even under favorable conditions, we find that systems proposed previously for similar tasks struggle on blends, showing limitations of form-based and distributional similarity approaches: BERT’s WordPiece segmentation (which is based on byte-pair encoding, or BPE (Sennrich et al., 2016)) reaches an F1 score of .562 for segmentation, compared with .427 by a baseline which treats each character as its own segment. Neither a BPE model retrained 1 We note that blends, or compounds, are not generally substitutable with the phrase composed of the originating bases, syntactically speaking. Our eval"
2021.scil-1.61,N19-1423,0,0.00594864,"r Google Research School of Interactive Computing Department of Psychology Seattle, WA, USA Georgia Institute of Technology University of Wisconsin jeisenstein@google.com Madison, WI, USA Atlanta, GA, USA cjacobs2@wisc.edu uvp@gatech.edu For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level. Such novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016). While this approach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOVs that cannot be cleanly decomposed into meaningful segments. Our work addresses a challenging source of OOV terms: novel blends (Algeo, 1977), also known as portmanteaux. Blends are constructed from the combination of multiple bases into a new form, in which some characters are shared across both bases:"
2021.scil-1.61,N18-1129,0,0.0131793,"1 Figure 1 shows that compounds are represented much more similarly to their bases than blends are. This result can be due to either a functional preference for creating blends in certain semantic conditions, or due to the form-level pathology of blends, i.e. the missing and joined characters. We annotated all blends and compounds for their semantic relations and found that despite matching semantic roles, blends were still highly dissimilar from their decomposed forms in context. We thus subject the compounds to a process of artificial blending using an existing algorithm for string merging (Kulkarni and Wang, 2018) and repeat the experiment on these “smoothies”. As shown in Figure 1, this process eliminates the representation gap, leading us to conclude that the difficulty in representing blends is due primarily to the complex relationship between their surface forms and meanings. Unblending. As part of blend understanding, blends may be understood through segmentation and component identification, which we term recovery. We cast the problem as a two-step pipeline, beginning with detection of morphological boundaries within blends, followed by a selection of the correct bases from a list of candidates c"
2021.scil-1.61,2020.coling-main.572,1,0.819818,"ovel blends (Algeo, 1977), also known as portmanteaux. Blends are constructed from the combination of multiple bases into a new form, in which some characters are shared across both bases: for example, shop + optics = shoptics. In this way, blends differ from other lexical formations such as compounds (e.g., water + melon = watermelon), which are formed by simple concatenation. Dataset and annotation. We collected a dataset of novel blends from the New York Times (NYT), starting from the output of a Twitter bot extracting all novel words with their originating contexts, a process described in Pinter et al. (2020). For each blend, we annotated the bases and the semantic relation between them, following the taxonomy defined by Tratz and Hovy (2010). We also define a character-level schema we call PAXOBS after its tagset, where each character in the blend is identified as being part of a single base (given sequential letters of the alphabet, so typically A or B), part of both (X), an extra-base prefix (P) or suffix (S), or a Figure 1: Similarity of BERT representations between base components of complex words, and naturally and artificially blended forms (“smoothies”). All representations computed in the"
2021.scil-1.61,P16-1162,0,0.0232611,"e.com Madison, WI, USA Atlanta, GA, USA cjacobs2@wisc.edu uvp@gatech.edu For the token-based architectures that dominate contemporary natural language processing, a particularly difficult form of linguistic generalization arises from unseen phenomena at the word level. Such novel sequences of characters, morphemes, or phonemes are known as out-ofvocabulary (OOV) terms (Brants, 2000; Plank, 2016). Pretrained transformers like BERT (Devlin et al., 2019) handle OOV terms by subtokenization: segmenting all whitespace-delimited tokens into smaller units, from which any OOV term can be constructed (Sennrich et al., 2016). While this approach is well suited for phenomena like concatenative English morphology, many linguistic processes generate OOVs that cannot be cleanly decomposed into meaningful segments. Our work addresses a challenging source of OOV terms: novel blends (Algeo, 1977), also known as portmanteaux. Blends are constructed from the combination of multiple bases into a new form, in which some characters are shared across both bases: for example, shop + optics = shoptics. In this way, blends differ from other lexical formations such as compounds (e.g., water + melon = watermelon), which are formed"
2021.scil-1.61,P10-1070,0,0.00898104,"n which some characters are shared across both bases: for example, shop + optics = shoptics. In this way, blends differ from other lexical formations such as compounds (e.g., water + melon = watermelon), which are formed by simple concatenation. Dataset and annotation. We collected a dataset of novel blends from the New York Times (NYT), starting from the output of a Twitter bot extracting all novel words with their originating contexts, a process described in Pinter et al. (2020). For each blend, we annotated the bases and the semantic relation between them, following the taxonomy defined by Tratz and Hovy (2010). We also define a character-level schema we call PAXOBS after its tagset, where each character in the blend is identified as being part of a single base (given sequential letters of the alphabet, so typically A or B), part of both (X), an extra-base prefix (P) or suffix (S), or a Figure 1: Similarity of BERT representations between base components of complex words, and naturally and artificially blended forms (“smoothies”). All representations computed in the original context in which the words appear. Error bars are standard error for the class. new character added to the blend for prosodic"
2021.teachingnlp-1.22,N19-1423,0,0.0260871,"quite a lot of work for the writer. In total, I kept hardly any 3 As my editor put it, “if missing deadlines was a crime, of the original text, although I was able to reuse the the prisons would be full of authors.” 4 https://languagelog.ldc.upenn.edu high-level structure of roughly half of the chapters. 128 3.3 Revision(s) The reviews were generally positive, but one reviewer was quite critical of the early chapters; although the publisher didn’t require it, I made substantial changes based on this feedback. At this point I also tried to add a few notes about very recent work, such as BERT (Devlin et al., 2019), which appeared on arXiv while I was doing the revisions. Had the original reviews been more negative, the publisher might have required another round before accepting my revisions, but luckily this wasn’t required in my case. The reviewers were very helpful, but I am skeptical that any of them read the whole thing, and I recommend seeking external reviews, especially for the later chapters that the reviewers are likely to skip or skim. 3.4 Proofs The remaining steps involve details of the writing style and typesetting. At this stage I handed over the source documents to the production team,"
D08-1035,A00-2004,0,0.557241,"e transcript corpus described by Malioutov and Barzilay (2006). Our system does not require parameter tuning; priors are re-estimated as described in Sections 3.4 and 4.3. U&I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided. Preprocessing Standard preprocessing techniques are applied to the text for all comparisons. The Porter (1980) stemming algorithm is applied to group equivalent lexical items. A set of stop-words is also removed, using the same list originally employed by several competitive systems (Choi, 2000; 341 Textbook U&I M CS LCS EG BAYES S EG BAYES S EG - CUE BAYES S EG - CUE - PROP Meetings U&I M CS LCS EG BAYES S EG BAYES S EG - CUE BAYES S EG - CUE - PROP Pk .370 .368 .370 .339 .339 .343 Pk .297 .370 .309 .264 .261 .258 WD .376 .382 .385 .353 .353 .355 WD .347 .411 .322 .319 .316 .312 Table 1: Comparison of segmentation algorithms. Both metrics are penalties, so lower scores indicate better performance. BAYES S EG is the cohesion-only Bayesian system with marginalized language models. BAYES S EG - CUE is the Bayesian system with cue phrases. BAYES S EG - CUE - PROP adds the linguisticall"
D08-1035,P08-1095,0,0.0141009,"is topic, see (Grosz, 1977). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation. More recently, cue phrases have been applied to topic segmentation in the supervised setting. In a supervised system that is distinct from the unsupervised model described above, Galley et al. (2003) automatically identify candidate cue phrases by mining labeled data for words that are especially likely to appear at segment boundaries; the presence of cue phrases is then used as a feature in a rule-based classifier for linear topic segmentation. Elsner and Charniak (2008) specify a list of cue phrases by hand; the cue phrases are used as a feature in a maximum-entropy classifier for conversation disentanglement. Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision. the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment). To obtain a high likelihood, the language models associated with each segment should conce"
D08-1035,P05-1045,0,0.00974588,"Missing"
D08-1035,P02-1026,0,0.401694,"uding the benchmark ICSI meeting dataset (Janin et al., 2003) and a new textual corpus constructed from the contents of a medical textbook. In both cases our model achieves performance surpassing multiple state-of-the-art baselines. Moreover, we demonstrate that the addition of cue phrases can further improve segmentation performance over cohesion-based methods. In addition to the practical advantages demonstrated by these experimental results, our model reveals interesting theoretical properties. Other re335 searchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002). We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment. This finding demonstrates that a relationship between discourse segmentation and entropy is a natural consequence of modeling topic structure in a generative Bayesian framework. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. 2 Related Work Existing unsupervised cohesion-based approaches can be characterized in terms of the metric used to quant"
D08-1035,P07-1094,0,0.0467381,"Missing"
D08-1035,J86-3001,0,0.744062,"neous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not ge"
D08-1035,P94-1002,0,0.0597817,"tors that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 2003). In this paper, we situate lexical cohesion in a Bayesian framework, allowing other sources of information to be incorporated without the need for labeled data. We formalize lexical cohesion in a generative model in which the text for each seg334 Proceedin"
D08-1035,J93-3003,0,0.110594,"ons within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple"
D08-1035,P95-1015,0,0.117445,"boundary from a special language model φ, which is shared across all topics and all documents in the dataset. For sentences that are not at segment boundaries, the likelihood is as before: p(xt |z, Θ, φ) = Q i∈xt θzt ,i . For sentences that immediately follow segment boundaries, we draw the first ` words from (`) φ instead. Writing xt for the ` cue words in xt , ˜ t for the remaining words, the likelihood for a and x segment-initial sentence is, Y Y p(xt |zt 6= zt−1 , Θ, φ) = φi θzt ,i . (`) i∈xt i∈˜ xt We draw φ from a symmetric Dirichlet prior φ0 . Following prior work (Galley et al., 2003; Litman and Passonneau, 1995), we consider only the first word of each sentence as a potential cue phrase; thus, we set ` = 1 in all experiments. a stationary Markov chain by repeatedly sampling among the hidden variables in the model. The most commonly-used sampling-based technique is Gibbs sampling, which iteratively samples from the conditional distribution of each hidden variable (Bishop, 2006). However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled. This is the case in linear topic segmentation, due to the constraint that zt ∈ {zt−1 , zt−1 + 1} (see Sect"
D08-1035,P06-1004,1,0.893175,"tation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s T EXT T ILING (1994) introduced the idea that unsupervised segmentation 1 Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot"
D08-1035,P93-1020,0,0.0510594,"ahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau and Litman, 1993; Galley et al., 2003), but cannot be incorporated by current unsupervised models. One reason for this is that existing unsupervised methods use arbitrary, hand-crafted metrics for quantifying lexical cohesion, such as weighted cosine similarity (Hearst, 1994; Malioutov and Barzilay, 2006). Without supervision, it is not possible to combine such metrics with additional sources of information. Moreover, such hand-crafted metrics may not generalize well across multiple datasets, and often include parameters which must be tuned on development sets (Malioutov and Barzilay, 2006; Galley et al., 200"
D08-1035,J02-1002,0,0.522371,"s transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries. For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author. This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter). Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. Pk and WindowDiff are penalties, so lower values indicate better segmentations. We use the evaluation source code provided by Malioutov and Barzilay (2006). System configuration W"
D08-1035,P06-1003,0,0.395561,"Missing"
D08-1035,P01-1064,0,0.306539,"yesian framework.1 1 Introduction Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments. Hearst’s T EXT T ILING (1994) introduced the idea that unsupervised segmentation 1 Code and materials for this work are available at http://groups.csail.mit.edu/rbg/code/ bayesseg/. can be driven by lexical cohesion, as high-quality segmentations feature homogeneous lexical distributions within each topic segment. Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation. But despite the effectiveness of lexical cohesion for unsupervised topic segmentation, it is clear that there are other important indicators that are ignored by the current generation of unsupervised systems. For example, consider cue phrases, which are explicit discourse markers such as “now” or “however” (Grosz and Sidner, 1986; Hirschberg and Litman, 1993; Knott, 1996). Cue phrases have been shown to be a useful feature for supervised topic segmentation (Passonneau"
D08-1035,P03-1071,0,\N,Missing
D08-1109,C04-1080,0,0.0289439,"rain highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incor"
D08-1109,P05-1033,0,0.0470847,"Missing"
D08-1109,P91-1017,0,0.422414,"tropy measure corresponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multi"
D08-1109,P02-1033,0,0.0358833,"ch efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since"
D08-1109,feldman-etal-2006-cross,0,0.071316,"sing cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Baye"
D08-1109,H05-1110,0,0.0700005,"Missing"
D08-1109,P07-1094,0,0.677275,"rk assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particula"
D08-1109,N06-1041,0,0.137328,"994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speech level will differ in syst"
D08-1109,D07-1031,0,0.0365362,"ast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual"
D08-1109,J94-2001,0,0.271465,"Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Hagh"
D08-1109,P03-1058,0,0.0062772,"ferentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentatio"
D08-1109,P06-1146,0,0.0361479,"Missing"
D08-1109,W97-0213,0,0.0603417,"ponds with the observed differentials in performance. 2 Related Work Multilingual Learning A number of approaches for multilingual learning have focused on inducing cross-lingual structures, with applications to machine translation. Examples of such efforts include work on the induction of synchronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphol"
D08-1109,P05-1044,0,0.30992,"the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of ambiguity classes thereby leveraging morphological features. In addition, a number of approaches have focused on developing discriminative approaches for unsupervised and semi-supervised tagging (Smith and Eisner, 2005; Haghighi and Klein, 2006). Our focus is on developing a simple model that effectively incorporates multilingual evidence. We view this direction as orthogonal to refining monolingual tagging models for any particular language. 3 Model We propose a bilingual model for unsupervised partof-speech tagging that jointly tags parallel streams of text in two languages. Once the parameters have been learned using an untagged bilingual parallel text, the model is applied to a held-out monolingual test set. Our key hypothesis is that the patterns of ambiguity found in each language at the part-of-speec"
D08-1109,P08-1084,1,0.538531,"hen annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include the use of a fully Bayesian HMM (Johnson, 2007; Goldwater and Griffiths, 2007). In very recent work, Toutanova and Johnson (2008) depart from this framework and propose an LDA-based generative model that groups words through a latent layer of a"
D08-1109,P98-2230,0,0.200773,"Missing"
D08-1109,H05-1107,0,0.0265007,"chronous grammars (Wu and Wong, 1998; Chiang, 2005) and learning multilingual lexical resources (Genzel, 2005). Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has be"
D08-1109,N01-1026,0,0.131354,"Another thread of work using cross-lingual links has been in word-sense disambiguation, where senses of words can be defined based on their translations (Brown et al., 1991; Dagan et al., 1991; Resnik and Yarowsky, 1997; Ng et al., 2003). When annotations for a task of interest are available in a source language but are missing in the 1042 target language, the annotations can be projected across a parallel corpus (Yarowsky et al., 2000; Diab and Resnik, 2002; Pad´o and Lapata, 2006; Xi and Hwa, 2005). In fact, projection methods have been used to train highly accurate part-of-speech taggers (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, our own work assumes that annotations exist for neither language. Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008). In this paper, we demonstrate that unsupervised multilingual learning can be successfully applied to the sentence-level task of part-of-speech tagging. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Recent advances in these approaches include"
D08-1109,H01-1035,0,\N,Missing
D08-1109,P91-1034,0,\N,Missing
D08-1109,C98-2225,0,\N,Missing
D09-1100,D08-1082,0,0.023721,"work also considered the supervision signal obtained by interpreting natural language in the context of a formal domain. Branavan et al. (2009) use feedback from a world model as a supervision signal. Chen and Mooney (2008) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervised generative model that accounts for both syntax and lexical choice. The semantic abstract is represented as a set of predicate logic formulae, which are applied as higher-order features for learning. We demonstrate that these features improve learning performance, and that both the lexica"
D09-1100,P09-1010,0,0.123404,"tic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This contrasts to previous work which interprets “directions” and thus assumes a direct correspondence between text and world state (Branavan et al., 2009; Chen and M"
D09-1100,C92-2106,0,0.223445,"Missing"
D09-1100,W05-0602,0,0.495566,"tation is not due merely to the added expressivity of our features. The third row compares against N O - SYNTAX, a crippled version of our model that incorporates lexical features but not the syntactic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be"
D09-1100,W00-1308,0,0.0113873,"owest, highest higher, sequence, sequential black, red, color suit, club, diamond, spade, heart onto bottom, available, top empty obtained from the Internet. Due to the popularity of the Microsoft implementation of Freecell, instructions often contain information specific to playing Freecell on a computer. We manually removed sentences which did not focus on the card aspects of Freecell (e.g., how to set up the board and information regarding where to click to move cards). In order to use our semantic abstraction model, the instructions were part-of-speech tagged with the Stanford POS Tagger (Toutanova and Manning, 2000) and dependency parses were obtained using Malt (Nivre, 2006). Table 1: Predicates in the Freecell world model, with natural language glosses obtained from the development set text. Glosses Our reading to learn setting requires a small set of glosses, which are surface forms commonly used to represent predicates from the world model. We envision an application scenario in which a designer manually specifies a few glosses for each predicate. However, for the purposes of evaluation, it would be unprincipled for the experimenters to handcraft the ideal set of glosses. Instead, we gathered a devel"
D09-1100,P07-1121,0,0.146856,"Missing"
D09-1100,P06-1085,0,0.0178021,"forward. For word slots to which no literals are aligned, the lexical item is drawn from a language model θ, estimated from the entire document collection. For slots to which at least one literal is aligned, we construct a language model φ in which the probability mass is divided equally among all glosses of aligned predicates. The language model θ is used as a backoff, so that there is a strong bias in favor of generating glosses, but some probability mass is reserved for the other lexical items. 1 There are many recent applications of Dirichlet processes in natural language processing, e.g. Goldwater et al. (2006). 961 4 Inference To compute the probability of a parsed sentence given a formula, we sum over alignments, This section describes a sampling-based inference procedure for obtaining a set of formulae f that explain the observed text s and dependency structures t. We perform Gibbs sampling over the formulae assigned to each sentence. Using the Chinese Restaurant Process interpretation of the Dirichlet Process (Aldous, 1985), we marginalize π, the infinite multinomial over all possible formulae: at each sampling step we select either an existing formula, or stochastically generate a new formula."
D09-1100,P09-1011,0,0.199814,"erely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning. This will allow us to directly evaluate the contribution of the extracted relations for learning. We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no label"
D09-1100,W05-0600,0,\N,Missing
D10-1124,N10-1038,1,0.204566,"tions. Using term frequency features xd for each author, we predict locations with wordgeography weights a ∈ R2W : lat T lon f (xd ; a) = (xT d a , xd a ) 1282 Weights are trained to minimize the sum of squared Euclidean distances, subject to L1 regularization: X lat lat 2 T lon (xT − ydlon )2 d a − yd ) + (xd a d + λlat ||alat ||1 + λlon ||alon ||1 The minimization problem decouples into two separate latitude and longitude models, which we fit using the glmnet elastic net regularized regression package (Friedman et al., 2010), which obtained good results on other text-based prediction tasks (Joshi et al., 2010). Regularization parameters were tuned on the development set. The L1 penalty outperformed L2 and mixtures of L1 and L2 . Note that for both word-level linear regression here, and the topic-level linear regression in SLDA, the choice of squared Euclidean distance dovetails with our use of spatial Gaussian likelihoods in the geographic topic models, since optimizing a is equivalent to maximum likelihood estimation under the assumption that locations are drawn from equivariant circular Gaussians centered around each f (xd ; a) linear prediction. We experimented with decorrelating the location di"
D13-1007,P06-2005,0,0.233763,"cized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar v"
D13-1007,P10-1079,0,0.0670416,"has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated usin"
D13-1007,N10-1083,0,0.137776,"ork. There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates. We treat the local context using standard language modeling techniques; we treat string similarity with a log-linear model that includes features for both surface similarity and word-word pairs. Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion. Nor can we apply dynamic programming techniques for unsupervised training of locally-normalized conditional models (Berg-Kirkpatrick et al., 2010), as their complexity is quadratic in the size of label space; in normalization, the label space is the vocabulary itself, with at least 104 elements. Instead, we present a new training approach using Monte Carlo techniques to compute an approximate gradient on the feature weights. This training method may be applicable in other unsupervised learning problems with a large label space. This model is implemented in a normalization system called UN LOL (unsupervised normalization in a LOg-Linear model). It is a lightweight proba61 Proceedings of the 2013 Conference on Empirical Methods in Natural"
D13-1007,D11-1052,0,0.0421122,"s, which each put weight on different orthographic rules. Because the loadings are constrained to be non-negative, the factorization can be seen as sparsely assigning varying amounts of each style to each author. We choose the factorization that minimizes the Frobenius norm of the reconstruction error, using the NIMFA software package (http://nimfa.biolab.si/). The resulting styles are shown in Table 3, for k = 10; other values of k give similar overall results with more or less detail. The styles incorporate a number of linguistic phenomena, including: expressive lengthening (styles 7-9; see Brody and Diakopoulos, 2011); g- and t-dropping (style 5, see Eisenstein 2013a) ; th-stopping (style 6); and the dropping of several word-final vowels (styles 1-3). Some of these styles, such as t-dropping and th-stopping, have direct analogues in spoken language varieties (Tagliamonte and Temple, 2005; Green, 2002), while others, like expressive lengthening, seem more unique to social media. The relationships between these orthographic styles and social variables such as geography and demograph2 We tried adding these rules as features and retraining the normalization system, but this hurt performance. style 1. you; o-dr"
D13-1007,D11-1120,0,0.00581709,"idgeon soo, noo, doo, oohh, loove, thoo, helloo mee, ive, retweet, bestie, lovee, nicee, heey, likee, iphone, homie, ii, damnit ima, outta, needa, shoulda, woulda, mm, comming, tomm, boutt, ppreciate Table 3: Orthographic styles induced from automatically normalized Twitter text ics must be left to future research, but they offer a promising generalization of prior work that has focused almost exclusively on exclusively on lexical variation (Argamon et al., 2007; Eisenstein et al., 2010; Eisenstein et al., 2011), with a few exceptions for character-level features (Brody and Diakopoulos, 2011; Burger et al., 2011). Note that style 10 is largely the result of mistaken normalizations. The tokens ima, outta, and needa all refer to multi-word expressions in standard English, and are thus outside the scope of the normalization task as defined by Han et al. (2013). UN LOL has produced incorrect single-token normalizations for these terms: i/ima, out/outta, and need/needa. But while these normalizations are wrong, the resulting style nonetheless captures a coherent orthographic phenomenon. 8 Conclusion We have presented a unified, unsupervised statistical model for normalizing social media text, attaining the"
D13-1007,C10-2022,0,0.258123,"ing words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining an “exception dictionary” of stronglyassociated word pairs such as you/u. Like Contractor et al. (2010), we apply string edit distance, and like Gouws et al. (2011), we capture strongly related word pairs. However, rather than applying these properties as filtering steps in a pipeline, we add them as features in a unified log-linear model. Recent approaches have sought to improve accuracy by bringing more exter"
D13-1007,W09-2010,0,0.226101,"s Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining"
D13-1007,D10-1124,1,0.175652,"Missing"
D13-1007,P11-1137,1,0.14876,"ol, fone, dese, dha, shid, dhat, dat’s idk, fuckk, okk, backk, workk, badd, andd, goodd, bedd, elidgible, pidgeon soo, noo, doo, oohh, loove, thoo, helloo mee, ive, retweet, bestie, lovee, nicee, heey, likee, iphone, homie, ii, damnit ima, outta, needa, shoulda, woulda, mm, comming, tomm, boutt, ppreciate Table 3: Orthographic styles induced from automatically normalized Twitter text ics must be left to future research, but they offer a promising generalization of prior work that has focused almost exclusively on exclusively on lexical variation (Argamon et al., 2007; Eisenstein et al., 2010; Eisenstein et al., 2011), with a few exceptions for character-level features (Brody and Diakopoulos, 2011; Burger et al., 2011). Note that style 10 is largely the result of mistaken normalizations. The tokens ima, outta, and needa all refer to multi-word expressions in standard English, and are thus outside the scope of the normalization task as defined by Han et al. (2013). UN LOL has produced incorrect single-token normalizations for these terms: i/ima, out/outta, and need/needa. But while these normalizations are wrong, the resulting style nonetheless captures a coherent orthographic phenomenon. 8 Conclusion We ha"
D13-1007,W13-1102,1,0.429537,"ex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011"
D13-1007,N13-1037,1,0.428495,"ex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011"
D13-1007,P11-2008,1,0.154112,"Missing"
D13-1007,W11-2210,0,0.0291899,"ed. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters are then estimated using expectation maximization. This work stands apart from most of the other unsupervised models, which are pipelines. Contractor et al. (2010) use string edit distance to identify closely-related candidate orthographic forms and then decode the message using a language model. Gouws et al. (2011) refine this approach by mining an “exception dictionary” of stronglyassociated word pairs such as you/u. Like Contractor et al. (2010), we apply string edit distance, and like Gouws et al. (2011), we capture strongly related word pairs. However, rather than applying these properties as filtering steps in a pipeline, we add them as features in a unified log-linear model. Recent approaches have sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing t"
D13-1007,P11-1038,0,0.286219,"dly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, string edit distances, off-the-shelf spellcheckers, and curated slang dictionaries (Liu et al., 2011; Han and Baldwin, 2011; Han et al., 2013). We propose a different approach, performing normalization in a maximum-likelihood framework. There are two main sources of information to be exploited: local context, and surface similarity between the observed strings and normalization candidates. We treat the local context using standard language modeling techniques; we treat string similarity with a log-linear model that includes features for both surface similarity and word-word pairs. Because labeled examples of normalized text are not available, this model cannot be trained in the standard supervised fashion. Nor can"
D13-1007,P13-1155,0,0.533896,"ges (Choudhury et al., 2007b). It has become still more salient in the era of widespread social media, particularly Twitter. Han and Baldwin (2011) formally define a normalization task for Twitter, focusing on normalizations between single tokens, and excluding multi-word tokens like lol (laugh out loud). The normalization task has been criticized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar"
D13-1007,C08-1056,0,0.0992402,"work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with transformation models (favoring words that are similar to the observed text). Our approach can also be seen as a noisy channel model, but unlike this prior work, no labeled data is required. Unsupervised methods Cook and Stevenson (2009) manually identify several word formation types within a noisy channel framework. They parametrize each formation type with a small num62 ber of scalar values, so that all legal transformations of a given type are equally likely. The scalar parameters"
D13-1007,P11-2013,0,0.75945,"ion system called UN LOL, which achieves the best known results on two normalization datasets, outperforming more complex systems. We use the output of UN LOL to automatically normalize a large corpus of social media text, revealing a set of coherent orthographic styles that underlie online language variation. 1 Introduction Social media language can differ substantially from other written text. Many of the attempts to characterize and overcome this variation have focused on normalization: transforming social media language into text that better matches standard datasets (Sproat et al., 2001; Liu et al., 2011). Because there is little available training data, and because social media language changes rapidly (Eisenstein, 2013b), fully supervised training is generally not considered appropriate for this task. However, due to the extremely high-dimensional output space — arbitrary sequences of words across the vocabulary — it is a very challenging problem for unsupervised learning. Perhaps it is for these reasons that the most successful systems are pipeline architectures that cobble together a diverse array of techniques and resources, including statistical language models, dependency parsers, strin"
D13-1007,P12-1109,0,0.78604,"sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing to identify contextuallysimilar words. Liu et al. (2011) extract noisy training pairs from the search snippets that result from carefully designed queries to Google, and then train a conditional random field (Lafferty et al., 2001) to estimate a character-based translation model. They later extend this work by adding a model of visual priming, an off-the-shelf spell-checker, and local context (Liu et al., 2012a). Hassan and Menezes (2013) use a random walk framework to capture contextual similarity, which they then interpolate with an edit distance metric. Rather than seeking additional external resources or designing more complex metrics of context and similarity, we propose a unified statistical model, which learns feature weights in a maximum-likelihood framework. 3 Approach Our approach is motivated by the following criteria: • Unsupervised. We want to be able to train a model without labeled data. At present, labeled data for Twitter normalization is available only in small quantities. Moreove"
D13-1007,P12-1055,0,0.512411,"sought to improve accuracy by bringing more external resources and complex architectures to bear. Han and Baldwin (2011) begin with a set of string similarity metrics, and then apply dependency parsing to identify contextuallysimilar words. Liu et al. (2011) extract noisy training pairs from the search snippets that result from carefully designed queries to Google, and then train a conditional random field (Lafferty et al., 2001) to estimate a character-based translation model. They later extend this work by adding a model of visual priming, an off-the-shelf spell-checker, and local context (Liu et al., 2012a). Hassan and Menezes (2013) use a random walk framework to capture contextual similarity, which they then interpolate with an edit distance metric. Rather than seeking additional external resources or designing more complex metrics of context and similarity, we propose a unified statistical model, which learns feature weights in a maximum-likelihood framework. 3 Approach Our approach is motivated by the following criteria: • Unsupervised. We want to be able to train a model without labeled data. At present, labeled data for Twitter normalization is available only in small quantities. Moreove"
D13-1007,W10-0513,0,0.0919659,"Missing"
D13-1007,P05-1044,0,0.252279,"to-word transformations without supervision would be impossible without the strong additional cue of local context. For example, in the phrase None of these tokens are standard (except 2, which appears in a nonstandard sense here), so without joint inference, it would not be possible to use context to help normalize suttin. Only by jointly reasoning over the entire message can we obtain the correct normalization. These desiderata point towards a featurized sequence model, which must be trained without labeled examples. While there is prior work on training sequence models without supervision (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010), there is an additional complication not faced by models for tasks such as part-of-speech tagging and named entity recognition: the potential label space of standard words is large, on the order of at least 104 . Naive application of Viterbi decoding — which is a component of training for both Contrastive Estimation (Smith and Eisner, 2005) and the locally-normalized sequence labeling model of Berg-Kirkpatrick et al. (2010) — will be stymied by Viterbi’s quadratic complexity in the dimension of the label space. While various pruning heuristics may be applied, w"
D13-1007,P13-1114,0,0.12172,"ttained popularity in the context of SMS messages (Choudhury et al., 2007b). It has become still more salient in the era of widespread social media, particularly Twitter. Han and Baldwin (2011) formally define a normalization task for Twitter, focusing on normalizations between single tokens, and excluding multi-word tokens like lol (laugh out loud). The normalization task has been criticized by Eisenstein (2013b), who argues that it strips away important social meanings. In recent work, normalization has been shown to yield improvements for part-of-speech tagging (Han et al., 2013), parsing (Zhang et al., 2013), and machine translation (Hassan and Menezes, 2013). As we will show in Section 7, accurate automated normalization can also improve our understanding of the nature of social media language. Supervised methods Early work on normalization focused on labeled SMS datasets, using approaches such as noisy-channel modeling (Choudhury et al., 2007a) and machine translation (Aw et al., 2006), as well as hybrid combinations of spelling correction and speech recognition (Kobus et al., 2008; Beaufort et al., 2010). This work sought to balance language models (favoring words that fit in context) with tra"
D13-1090,D12-1050,0,0.381014,"on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who treat sentences as pseudo-documents in a"
D13-1090,P12-1047,0,0.010615,"mprovements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-i"
D13-1090,P09-1053,0,0.591246,"mentary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder"
D13-1090,C04-1051,0,0.978751,"nd because of the short length of individual sentences, which means that standard bag-of-words representations will be hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called T F -KL D, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004), this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF; Guo and Diab, 2012). Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification. We then add “finegrained” features about the lexical similarity of the sentence pair. The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets pr"
D13-1090,N13-1092,0,0.0465217,"Missing"
D13-1090,P12-1091,0,0.698056,"hopelessly sparse. Distributional methods address this problem by transforming the high-dimensional bag-of-words representation into a lower-dimensional latent space. In this paper, we show how labeled data can considerably improve distributional methods for measuring semantic similarity. First, we develop a new discriminative term-weighting metric called T F -KL D, which is applied to the term-context matrix before factorization. On a standard paraphrase identification task (Dolan et al., 2004), this method improves on both traditional TF-IDF and Weighted Textual Matrix Factorization (WTMF; Guo and Diab, 2012). Next, we convert the latent representations of each sentence pair into a feature vector, which is used as input to a linear SVM classifier. This yields further improvements and substantially outperforms the current state-of-the-art on paraphrase classification. We then add “finegrained” features about the lexical similarity of the sentence pair. The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in"
D13-1090,N06-1058,0,0.00677668,"pting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a recursive auto-encoder to propagate meaning through the parse tree. We take a different approach: rather than representing the meanings of individual words, we directly obtain a distributional representation for the entire sentence. This is inspired by Mihalcea et al. (2006) and Guo and Diab (2012), who"
D13-1090,N12-1019,0,0.490288,"f latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. S"
D13-1090,N10-1021,0,0.0552515,"Missing"
D13-1090,P10-1040,0,0.0229103,"Missing"
D13-1090,U06-1019,0,0.906948,"The combination of latent and finegrained features yields further improvements in accuracy, demonstrating that these feature sets provide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of"
D13-1090,W05-1205,0,0.0570707,"ide complementary information on semantic similarity. 891 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891–896, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Without attempting to do justice to the entire literature on paraphrase identification, we note three high-level approaches: (1) string similarity metrics such as n-gram overlap and BLEU score (Wan et al., 2006; Madnani et al., 2012), as well as string kernels (Bu et al., 2012); (2) syntactic operations on the parse structure (Wu, 2005; Das and Smith, 2009); and (3) distributional methods, such as latent semantic analysis (LSA; Landauer et al., 1998), which are most relevant to our work. One application of distributional techniques is to replace individual words with distributionally similar alternatives (Kauchak and Barzilay, 2006). Alternatively, Blacoe and Lapata (2012) show that latent word representations can be combined with simple elementwise operations to identify the semantic similarity of larger units of text. Socher et al. (2011) propose a syntactically-informed approach to combine word representations, using a r"
D13-1090,J13-3001,0,\N,Missing
D13-1090,U04-1000,0,\N,Missing
D15-1256,D11-1120,0,0.0784618,"Missing"
D15-1256,E14-1011,0,0.0568101,"These papers draw similar conclusions, showing that the the distribution of geotagged tweets over the US population is not random, and that higher usage is correlated with urban areas, high income, more ethnic minorities, and more young people. However, this prior work did not consider the biases introduced by relying on geotagged messages, nor the consequences for geo-linguistic analysis. Twitter has often been used to study the geographical distribution of linguistic information, and of particular relevance are Twitter-based studies of regional dialect differences (Eisenstein et al., 2010; Doyle, 2014; Gonc¸alves and S´anchez, 2014; Eisenstein, 2015) and text-based geolocation (Cheng et al., 2010; Hong et al., 2012; Han et al., 2014). This prior work rarely considers the impact of the demographic confounds, or of the geographical biases mentioned in § 3. Recent research shows that accuracies of core language technology tasks such as part-of-speech tagging are correlated with author demographics such as author age (Hovy and Søgaard, 2015); our results on location prediction are in accord with these findings. Hovy (2015) show that including author demographics can improve text classification"
D15-1256,D10-1124,1,0.918603,"Missing"
D15-1256,P11-1137,1,0.415625,"r, and living conditions (Zillien and Hargittai, 2009). Hargittai and Litt (2011) use a longitudinal survey methodology to compare the effects of gender, race, and topics of interest on Twitter usage among young adults. Geographic variation in Twitter adoption has been considered both internationally (Kulshrestha et al., 2012) and within the United States, using both the Twitter location field (Mislove et al., 2011) and per-message GPS coordinates (Hecht and Stephens, 2014). Aggregate demographic statistics of Twitter users’ geographic census blocks were computed by O’Connor et al. (2010) and Eisenstein et al. (2011b); Malik et al. (2015) use census demographics in spatial error model. These papers draw similar conclusions, showing that the the distribution of geotagged tweets over the US population is not random, and that higher usage is correlated with urban areas, high income, more ethnic minorities, and more young people. However, this prior work did not consider the biases introduced by relying on geotagged messages, nor the consequences for geo-linguistic analysis. Twitter has often been used to study the geographical distribution of linguistic information, and of particular relevance are Twitter-b"
D15-1256,P09-1080,0,0.0105518,"tures associated with each city. The keywords identified in GPS-MSABALANCED dataset feature more geographicallyspecific non-standard words, which occur at a rate of 3.9 × 10−4 in GPS-MSA- BALANCED, versus 2.6 × 10−4 in L OC -MSA- BALANCED; this difference is statistically significant (p &lt; .05, t = 3.2).4 3 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 1e 4 Male Female 0.00015 0.00009 0.00004 0.00005 0-17 0.00002 0.00002 18-29 30-39 Age group 0.00001 0.00002 40+ (a) non-standard words Per-word frequency ai ai gi wi ni π Per-word frequency π Binning is often employed in work on text-based age prediction (Garera and Yarowsky, 2009; Rao et al., 2010; Rosenthal and McKeown, 2011); it enables word and name counts to be shared over multiple ages, and avoids the complexity inherent in regressing a high-dimensional textual predictors against a numerical variable. 4 We employ a paired t-test, comparing the difference in frequency for each word across the two datasets. Since we cannot test the complete set of entity names or non-standard words, this quantifies whether the observed difference is robust across the subset of the vocabulary that we have selected. 1.2 1.0 0.8 0.6 0.4 0.2 0.0 1e 3 Male Female 0.00117 0.00075 0.00027"
D15-1256,P15-2079,0,0.0347348,"e geographical distribution of linguistic information, and of particular relevance are Twitter-based studies of regional dialect differences (Eisenstein et al., 2010; Doyle, 2014; Gonc¸alves and S´anchez, 2014; Eisenstein, 2015) and text-based geolocation (Cheng et al., 2010; Hong et al., 2012; Han et al., 2014). This prior work rarely considers the impact of the demographic confounds, or of the geographical biases mentioned in § 3. Recent research shows that accuracies of core language technology tasks such as part-of-speech tagging are correlated with author demographics such as author age (Hovy and Søgaard, 2015); our results on location prediction are in accord with these findings. Hovy (2015) show that including author demographics can improve text classification, a similar approach might improve text-based geolocation as well. We address the question about the impact of geographical biases and demographic confounds by measuring differences between three sampling techniques, in both language use and in the accuracy of text-based geolocation. Recent unpublished work proposes reweighting Twitter data to 2145 0.24 0.22 0.20 0.18 0.16 0.14 Male Female Accuracy Accuracy Accuracy 0.28 LOC-MSA-Balanced 0.2"
D15-1256,P15-1073,0,0.0175917,"based studies of regional dialect differences (Eisenstein et al., 2010; Doyle, 2014; Gonc¸alves and S´anchez, 2014; Eisenstein, 2015) and text-based geolocation (Cheng et al., 2010; Hong et al., 2012; Han et al., 2014). This prior work rarely considers the impact of the demographic confounds, or of the geographical biases mentioned in § 3. Recent research shows that accuracies of core language technology tasks such as part-of-speech tagging are correlated with author demographics such as author age (Hovy and Søgaard, 2015); our results on location prediction are in accord with these findings. Hovy (2015) show that including author demographics can improve text classification, a similar approach might improve text-based geolocation as well. We address the question about the impact of geographical biases and demographic confounds by measuring differences between three sampling techniques, in both language use and in the accuracy of text-based geolocation. Recent unpublished work proposes reweighting Twitter data to 2145 0.24 0.22 0.20 0.18 0.16 0.14 Male Female Accuracy Accuracy Accuracy 0.28 LOC-MSA-Balanced 0.26 GPS-MSA-Balanced 0.24 0.22 0.20 0.18 0.16 0.14 0-2 3-5 6-10 11-15 >15 Number of m"
D15-1256,C14-1184,0,0.0194192,"Missing"
D15-1256,D12-1137,0,0.108215,"hese text-based geolocation methods are most and least accurate. 5.1 Methods Our data is drawn from the ten largest metropolitan areas in the United States, and we formulate text-based geolocation as a ten-way classification problem, similar to Han et al. (2014).6 Using our 6 Many previous papers have attempted to identify the precise latitude and longitude coordinates of individual authors, but obtaining high accuracy on this task involves much more complex methods, such as latent variable models (Eisenstein et al., 2010; Hong et al., 2012), or multilevel grid structures (Cheng et al., 2010; Roller et al., 2012). Tuning such 2144 user-balanced samples, we apply ten-fold cross validation, and tune the regularization parameter on a development fold, using the vocabulary of the sample as features. 5.2 Results Many author-attribute prediction tasks become substantially easier as more data is available (Burger et al., 2011), and text-based geolocation is no exception. Since GPS-MSABALANCED and L OC -MSA- BALANCED have very different usage rates (Figure 2), perceived differences in accuracy may be purely attributable to the amount of data available per user, rather than to users in one group being inherent"
D15-1256,P11-1077,0,0.0270719,"identified in GPS-MSABALANCED dataset feature more geographicallyspecific non-standard words, which occur at a rate of 3.9 × 10−4 in GPS-MSA- BALANCED, versus 2.6 × 10−4 in L OC -MSA- BALANCED; this difference is statistically significant (p &lt; .05, t = 3.2).4 3 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 1e 4 Male Female 0.00015 0.00009 0.00004 0.00005 0-17 0.00002 0.00002 18-29 30-39 Age group 0.00001 0.00002 40+ (a) non-standard words Per-word frequency ai ai gi wi ni π Per-word frequency π Binning is often employed in work on text-based age prediction (Garera and Yarowsky, 2009; Rao et al., 2010; Rosenthal and McKeown, 2011); it enables word and name counts to be shared over multiple ages, and avoids the complexity inherent in regressing a high-dimensional textual predictors against a numerical variable. 4 We employ a paired t-test, comparing the difference in frequency for each word across the two datasets. Since we cannot test the complete set of entity names or non-standard words, this quantifies whether the observed difference is robust across the subset of the vocabulary that we have selected. 1.2 1.0 0.8 0.6 0.4 0.2 0.0 1e 3 Male Female 0.00117 0.00075 0.00027 0.00037 0.00038 0.00050 0.00021 0-17 18-29 30-3"
D15-1256,D08-1102,0,0.105445,"Missing"
D15-1256,P11-1096,0,0.304898,"ch’), ight (’alright’), oomf (’one of my followers’). Older authors write more about local entities (manhattan, nyc, houston), with men focusing on sports-related entities (harden, watt, astros, mets, texans), and women above the age of 40 emphasizing religiously-oriented terms (proverb, islam, rejoice, psalm). 5 But see Bamman et al. (2014) for a much more detailed discussion of gender and standardness. 5 Impact on text-based geolocation A major application of geotagged social media is to predict the geolocation of individuals based on their text (Eisenstein et al., 2010; Cheng et al., 2010; Wing and Baldridge, 2011; Hong et al., 2012; Han et al., 2014). Text-based geolocation has obvious commercial implications for location-based marketing and opinion analysis; it is also potentially useful for researchers who want to measure geographical phenomena in social media, and wish to access a larger set of individuals than those who provide their locations explicitly. Previous research has obtained impressive accuracies for text-based geolocation: for example, Hong et al. (2012) report a median error of 120 km, which is roughly the distance from Los Angeles to San Diego, in a prediction space over the entire c"
D15-1263,P13-1025,0,0.0348675,"Missing"
D15-1263,P12-1007,0,0.019066,"g all words except those in the top-most nucleus within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document. 2.2 Sentiment analysis There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular in"
D15-1263,D14-1168,0,0.169725,"timately covering the entire document. Discourse relations may involve a nucleus and a satellite, or they may be multinuclear. In the example in Figure 1, the unit 1C is the satellite of a relationship with its nucleus 1B; together they form a larger discourse unit, which is involved in a multinuclear CONJUNCTION relation. The nuclearity structure of RST trees suggests a natural approach to evaluating the importance of segments of text: satellites tend to be less important, and nucleii tend to be more important (Marcu, 1999). This idea has been leveraged extensively in document summarization (Gerani et al., 2014; Uzˆeda et al., 2010; Yoshida et al., 2014), and was the inspiration for Voll and Taboada (2007), who examined intra-sentential relations, eliminating all words except those in the top-most nucleus within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has"
D15-1263,P97-1023,0,0.0333149,"negative sentiment, but may focus on other categories, such as politeness (Danescu-NiculescuMizil et al., 2013), narrative frames (Jurafsky et al., 2014), or a multidimensional spectrum of emotions (Kim et al., 2012). In these cases, labeled documents may not be available, so users often employ a simpler method: counting matches against lists of words associated with each category. Such lists may be built manually from introspection, as in LIWC (Tausczik and Pennebaker, 2010) and the General Inquirer (Stone, 1966). Alternatively, they may be induced by bootstrapping from a seed set of words (Hatzivassiloglou and McKeown, 1997; Taboada et al., 2011). While lexicon-based methods may be less accurate than supervised classifiers, they are easier to apply to 2213 1 https://github.com/jiyfeng/DPLP 1H 1A 1B 1C 1D 1E 1F 1G Figure 2: Dependency-based discourse tree representation of the discourse in Figure 1 new domains and problem settings. Our proposed approach can be used in combination with either method for sentiment analysis, and in principle, could be directly applied to other document-level categories, such as politeness. 2.3 Datasets We evaluate on two review datasets. In both cases, the goal is to correctly class"
D15-1263,P14-1002,1,0.379482,"on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document. 2.2 Sentiment analysis There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document. Bagof-"
D15-1263,J15-3002,0,0.248525,"us within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document. 2.2 Sentiment analysis There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment"
D15-1263,P13-1160,0,0.0366283,"Missing"
D15-1263,D14-1220,0,0.031021,"ourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document. 2.2 Sentiment analysis There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document. Bagof-words models are w"
D15-1263,P84-1076,0,0.233589,"Missing"
D15-1263,P97-1013,0,0.217837,"has been leveraged extensively in document summarization (Gerani et al., 2014; Uzˆeda et al., 2010; Yoshida et al., 2014), and was the inspiration for Voll and Taboada (2007), who examined intra-sentential relations, eliminating all words except those in the top-most nucleus within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST pars"
D15-1263,D13-1158,0,0.0958071,"Missing"
D15-1263,P04-1035,0,0.164863,"er to apply to 2213 1 https://github.com/jiyfeng/DPLP 1H 1A 1B 1C 1D 1E 1F 1G Figure 2: Dependency-based discourse tree representation of the discourse in Figure 1 new domains and problem settings. Our proposed approach can be used in combination with either method for sentiment analysis, and in principle, could be directly applied to other document-level categories, such as politeness. 2.3 Datasets We evaluate on two review datasets. In both cases, the goal is to correctly classify the opinion polarity as positive or negative. The first dataset is comprised of 2000 movie reviews, gathered by Pang and Lee (2004). We perform ten-fold crossvalidation on this data. The second dataset is larger, consisting of 50,000 movie reviews, gathered by Socher et al. (2013), with a predefined 50/50 split into training and test sets. Documents are scored on a 1-10 scale, and we treat scores ≤ 4 as negative, ≥ 7 as positive, and ignore scores of 5-6 as neutral — although in principle nothing prevents extension of our approaches to more than two sentiment classes. 3 Discourse depth reweighting Our first approach to incorporating discourse information into sentiment analysis is based on quantifying the importance of ea"
D15-1263,W09-3813,0,0.0198535,"atic RST parsing, but there has been a recent spike of interest in this task, with contemporary approaches employing discriminative learning (Hernault et al., 2010), rich features (Feng and Hirst, 2012), structured prediction (Joty et al., 2015), and representation learning (Ji and Eisenstein, 2014; Li et al., 2014). With many strong systems to choose from, we employ the publiclyavailable DPLP parser (Ji and Eisenstein, 2014),1 . To our knowledge, this system currently gives the best F-measure on relation identification, the most difficult subtask of RST parsing. DPLP is a shiftreduce parser (Sagae, 2009), and its time complexity is linear in the length of the document. 2.2 Sentiment analysis There is a huge literature on sentiment analysis (Pang and Lee, 2008; Liu, 2012), with particular interest in determining the overall sentiment polarity (positive or negative) of a document. Bagof-words models are widely used for this task, as they offer accuracy that is often very competitive with more complex approaches. Given labeled data, supervised learning can be applied to obtain sentiment weights for each word. However, the effectiveness of supervised sentiment analysis depends on having training"
D15-1263,D13-1170,0,0.206558,"iked the son of the leader of the Samurai.]1E [He was a likable chap,]1F [and I hated to see him die.]1G [But, :::: other than all that, this movie is nothing more than hid:::::: 1H den rip-offs.] ::::: Introduction Sentiment analysis and opinion mining are among the most widely-used applications of language technology, impacting both industry and a variety of other academic disciplines (Feldman, 2013; Liu, 2012; Pang and Lee, 2008). Yet sentiment analysis is still dominated by bag-of-words approaches, and attempts to include additional linguistic context typically stop at the sentence level (Socher et al., 2013). Since document-level opinion mining inherently involves multi-sentence texts, it seems that analysis of document-level structure should have a role to play. A classic example of the potential relevance of discourse to sentiment analysis is shown in Figure 1. In this review of the film The Last Samurai, the positive sentiment words far outnumber the :::::::: negative ::::::::: sentiment words. But the discourse structure — indicated here with Rhetorical Structure Theory (RST; Mann and Thompson, 1988) — ∗ Code is available at https://github.com/ parry2403/R2N2 Figure 1: Example adapted from Vo"
D15-1263,C14-1053,0,0.0551197,"Missing"
D15-1263,C12-2128,0,0.0615981,"Missing"
D15-1263,D09-1018,0,0.0158704,"Missing"
D15-1263,H05-1044,0,0.105724,"Missing"
D15-1263,N03-1030,0,0.0547809,"Missing"
D15-1263,P14-1031,0,0.0606579,"Missing"
D15-1263,D14-1196,0,0.0451498,"course relations may involve a nucleus and a satellite, or they may be multinuclear. In the example in Figure 1, the unit 1C is the satellite of a relationship with its nucleus 1B; together they form a larger discourse unit, which is involved in a multinuclear CONJUNCTION relation. The nuclearity structure of RST trees suggests a natural approach to evaluating the importance of segments of text: satellites tend to be less important, and nucleii tend to be more important (Marcu, 1999). This idea has been leveraged extensively in document summarization (Gerani et al., 2014; Uzˆeda et al., 2010; Yoshida et al., 2014), and was the inspiration for Voll and Taboada (2007), who examined intra-sentential relations, eliminating all words except those in the top-most nucleus within each sentence. More recent work focuses on reweighting each discourse unit depending on the relations in which it participates (Heerschop et al., 2011; Hogenboom et al., 2015). We consider such an approach, and compare it with a compositional method, in which sentiment polarity is propagated up the discourse tree. Marcu (1997) provides the seminal work on automatic RST parsing, but there has been a recent spike of interest in this tas"
D15-1263,J11-2001,0,0.0621115,"n other categories, such as politeness (Danescu-NiculescuMizil et al., 2013), narrative frames (Jurafsky et al., 2014), or a multidimensional spectrum of emotions (Kim et al., 2012). In these cases, labeled documents may not be available, so users often employ a simpler method: counting matches against lists of words associated with each category. Such lists may be built manually from introspection, as in LIWC (Tausczik and Pennebaker, 2010) and the General Inquirer (Stone, 1966). Alternatively, they may be induced by bootstrapping from a seed set of words (Hatzivassiloglou and McKeown, 1997; Taboada et al., 2011). While lexicon-based methods may be less accurate than supervised classifiers, they are easier to apply to 2213 1 https://github.com/jiyfeng/DPLP 1H 1A 1B 1C 1D 1E 1F 1G Figure 2: Dependency-based discourse tree representation of the discourse in Figure 1 new domains and problem settings. Our proposed approach can be used in combination with either method for sentiment analysis, and in principle, could be directly applied to other document-level categories, such as politeness. 2.3 Datasets We evaluate on two review datasets. In both cases, the goal is to correctly classify the opinion polarit"
D15-1263,N13-1100,1,0.916982,"Missing"
D15-1263,D11-1015,0,0.0283908,"Missing"
D15-1263,I11-1038,0,0.0235737,"Missing"
D15-1264,P13-2013,0,0.46516,"xplicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (T EMP.), Comparison (C OMP.), Expansion (E XP.) and Contingency (C ONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Treebank. To test this idea,"
D15-1264,W06-1615,0,0.0282585,"elegant form of denoising autencoder, by marginalizing over the noising process. Their single-layer marginalized denoising autoencoder (mDA) solves the following problem: min Ex˜ i |xi [kxi − W˜ xi k2 ] W (1) where the parameter W ∈ Rd×d is a projection matrix. After learning the projection matrix, we use tanh(Wx) as the representation for our relation identification task. Usually, xi ∈ Rd is a sparse vector with more than 105 dimensions. Solving the optimization problem defined in equation 1 will produce a d × d dense matrix W, and is prohibitively expensive. We employ the trick proposed by Blitzer et al. (2006) to select κ pivot features to be reconstructed. We then split all features into nonoverlapping subsets of size ≤ K. Then, a set of projection matrices are learned, so as to transform each feature subset to the pivot feature set. The final projection matrix W is the stack of all projection matrices learned from the feature subsets. 3.2 Handling mismatched label distributions: Resampling with minimal supervision There is a notable mismatch between the relation distributions for implicit and explicitly-marked discourse relations in the Penn Discourse Treebank: as shown in Figure 1, the EXPANSION"
D15-1264,P07-1034,0,0.0584732,"tances, rather than supplementing a training set of manually-labeled implicit examples. Learning good feature representations (BenDavid et al., 2007) and reducing mismatched label distributions (Joshi et al., 2012) are two main ways to make a domain adaptation task successful. Structural correspondence learning is an early example of representation learning for domain adaptation (Blitzer et al., 2006); we build on the more computationally tractable approach of marginalized denoising autoencoders (Chen et al., 2012). Instance weighting is an approach for correcting label distribution mismatch (Jiang and Zhai, 2007); we apply a simpler approach of resampling the source domain according to an estimate of the target domain label distribution. 3 Domain Adaptation for Implicit Relation Identification We employ two domain adaptation techniques: learning feature representations, and resampling to match the target label distribution. 3.1 Learning feature representation: Marginalized denoising autoencoders The goal of feature representation learning is to obtain dense features that capture feature correlations between the source and target domains. Denoising autoencoders (Glorot et al., 2011) do this by first “c"
D15-1264,D12-1119,0,0.0209391,"Missing"
D15-1264,P13-1047,0,0.611154,"Missing"
D15-1264,prasad-etal-2008-penn,0,0.274768,"n; instead, we call it resampling with minimal supervision. It may also be desirable to ensure that the source and target training instances are similar in terms of their observed features; this is the idea behind the instance weighting approach to domain adaptation (Jiang and Zhai, 2007). Motivated by this idea, we require that sampled instances from the source domain have a cosine similarity of at least τ with at least one target domain instance (Rutherford and Xue, 2015). 4 Experiments Our experiments test the utility of the two domain adaptation methods, using the Penn Discourse Treebank (Prasad et al., 2008) and some extra-training data collected from a external resource. 4.1 Experimental setup Datasets The test examples are implicit relation instances from section 21-22 in the PDTB. For the domain adaptation setting, the training set consists of the explicitly-marked examples extracted from sections 02-20 and 23-24, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every"
D15-1264,D09-1036,0,0.0603484,"42.25 39.46 Table 1: Performance of cross-domain learning for implicit discourse relation identification. 0.85; pilot studies found that results are not sensitive to the value of τ across a range of values. Features All features are motivated by prior work on implicit discourse relation classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The F ULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times. The P IVOT feature set includes κ high-frequency features from the F ULL feature set. To focus on testing the domain adaptation techniques, we"
D15-1264,P11-1100,0,0.0270081,"Missing"
D15-1264,W10-4327,0,0.016589,"Missing"
D15-1264,P14-5010,0,0.00169827,"tion classification: from each training example with two arguments, we extract (1) Lexical features, including word pairs, the first and last words from both arguments (Pitler et al., 2009); (2) Syntactic features, including production rules from each argument, and the shared production rules between two arguments (Lin et al., 2009); (3) Other features, including modality, Inquirer tags, Levin verb classes, and argument polarity (Park and Cardie, 2012). We re-implement these features as closely as possible to the cited works, using the Stanford CoreNLP Toolkit to obtain syntactic annotations (Manning et al., 2014). The F ULL feature set for domain adaptation is constructed by collecting all features from the training set, and then removing features that occur fewer than ten times. The P IVOT feature set includes κ high-frequency features from the F ULL feature set. To focus on testing the domain adaptation techniques, we use the same F ULL and P IVOT set for all four relations, and leave feature set optimization for each relation as a future work (Park and Cardie, 2012). To obtain the upper bound, we employ the same feature categories and frequency threshold to extract features from the in-domain data,"
D15-1264,N15-1081,0,0.587465,"otated datasets of implicit discourse relations are expensive to produce, so it would be preferable to use weak supervision, by automatically labeling instances with explicit connectives (Marcu and Echihabi, 2003). However, Sporleder and Lascarides (2008) show that models trained on explicitly marked examples generalize poorly to implicit relation identification. They argued that explicit and implicit examples may be linguistically dissimilar, as writers tend to avoid discourse connectives if the discourse relation could be inferred from context (Grice, 1975). Similar observations are made by Rutherford and Xue (2015), who attempt to add automatically-labeled instances to improve supervised classification of implicit discourse relations. In this paper, we approach this problem from the perspective of domain adaptation. Specifically, we argue that the reason that automatically-labeled examples generalize poorly is due to domain mismatch from the explicit relations (source domain) to the implicit relations (target domain). We propose to close the gap by using two simple methods from domain adaptation: (1) feature representation learning: mapping the source domain and target domain to a shared latent feature"
D15-1264,D09-1018,0,0.0302458,"Missing"
D15-1264,D14-1196,0,0.114561,"Missing"
D15-1264,W12-1614,0,0.0863576,"t set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (T EMP.), Comparison (C OMP.), Expansion (E XP.) and Contingency (C ONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such as the Penn Discourse Tr"
D15-1264,C08-2022,0,0.0338908,"Missing"
D15-1264,P09-1077,0,0.695975,"4, and the development set consists of the explicit relations from sections 21-22. All relations in the explicit examples are automatically labeled by using the connective-to-relation mapping from Table 2 in (Prasad et al., 2007), where we only keep the majority relation type for every connective. For each identified connective, we use its annotated arguments in the PDTB. As an upper bound, we also train a supervised discourse relation classifier, using the implicit examples in sections 02-20 and 23-24 as the training set, and using sections 00-01 as the development set. Following prior work (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013), we consider the firstlevel discourse relations in the PDTB — Temporal (T EMP.), Comparison (C OMP.), Expansion (E XP.) and Contingency (C ONT.). We train binary classifiers and report F1 score on each binary classification task. Extension of this approach to multi-class classification is important, but since this is not the setting considered in most of the prior research, we leave it for future work. The true power of learning from automatically labeled examples is that we could leverage much larger datasets than hand-annotated corpora such a"
D15-1264,P02-1047,0,\N,Missing
D16-1047,J92-4003,0,0.227365,"Missing"
D16-1047,D14-1082,0,0.0117828,"s based on subword units: morphemes (Luong et al., 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al., 2015; Kim et al., 2016). Such models leverage the fact that word meaning is often compositional, arising from subword components. By learning representations of subword units, it is possible to generalize to rare and unseen words. Introduction Word embeddings have been shown to improve many natural language processing applications, from language models (Mikolov et al., 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et al., 2014). Word embeddings leverage a classical idea in natural language processing: use distributional statistics from large amounts of unlabeled data to learn representations that allow sharing ∗ The first two authors contributed equally. Code is available at https://github.com/rguthrie3/ MorphologicalPriorsForWordEmbeddings. But while morphology and orthography are sometimes a signal of semantics, there are also many cases similar spellings do not imply similar meanings: better-batter, melon-felon, dessert-desert, etc. If each word’s embedding is constrained"
D16-1047,N15-1140,0,0.101035,"Missing"
D16-1047,P16-1156,0,0.0867437,"Missing"
D16-1047,W02-0603,0,0.370323,"nitial vocabulary of 50,000 words, with a special hUNKi token for words that are not among the 50,000 most common. We then perform downcasing and convert all numeric tokens to a special hNUMi token. After these 494 steps, the vocabulary size decreases to 48,986. Note that the method can impute word embeddings for out-of-vocabulary words under the prior distribution P (b; M, u); however, it is still necessary to decide on a vocabulary size to determine the number of variational parameters γ and output embeddings to estimate. Unsupervised morphological segmentation is performed using Morfessor (Creutz and Lagus, 2002), with a maximum of sixteen morphemes per word. This results in a total of 14,000 morphemes, which includes stems for monomorphemic words. We do not rely on any labeled information about morphological structure, although the incorporation of gold morphological analysis is a promising topic for future work. 3.2 Learning details The LSTM parameters are initialized uniformly in the range [−0.08, 0.08]. The word embeddings are initialized using pre-trained word2vec embeddings. We train the model for 15 epochs, with an initial learning rate of 0.01, a decay of 0.97 per epoch, and minibatches of siz"
D16-1047,N15-1184,0,0.0380029,"stributional statistics. 5 Related work Adding side information to word embeddings An alternative approach to incorporating additional information into word embeddings is to constrain the embeddings of semantically-related words to be similar. Such work typically draws on existing lexical semantic resources such as WordNet. For example, Yu and Dredze (2014) define a joint training objective, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by Bian et al. (2014). Alternatively, Faruqui et al. (2015) propose to “retrofit” pre-trained word embeddings over a semantic network. Both retrofitting and our own approach treat the true word embeddings as latent variables, from which the pretrained word embeddings are stochastically emitted. However, a key difference from our approach is that the underlying representation in these prior works is relational, and not generative. These methods can capture similarity between words in a relational lexicon such as WordNet, but they do not offer a generative account of how (approximate) meaning is constructed from orthography or morphology. Word embedding"
D16-1047,W16-2506,0,0.0190494,"on both datasets. On the subset of in-vocabulary words, WORD 2 VEC gives slightly better results on the wordsim words that are in the NANT vocabulary, but is not applicable to the complete dataset. On the rare words dataset, WORD 2 VEC performs considerably worse than both morphology-based models, matching the findings of Luong et al. (2013) and Botha and Blunsom (2014) regarding the importance of morphology for doing well on this dataset. 4.2 Alignment with lexical semantic features Recent work questions whether these word similarity metrics are predictive of performance on downstream tasks (Faruqui et al., 2016). The QVEC statistic is another intrinsic evaluation method, which has been shown to be better correlated with downstream tasks (Tsvetkov et al., 2015). This metric measures the alignment between word embeddings and a set of lexical semantic features. Specifically, we use the semcor noun verb supersenses oracle provided at the qvec github repository.2 As shown in Table 2, VAR E MBED outperforms S UM E MBED on the full lexicon, and gives similar performance to WORD 2 VEC on the set of invocabulary words. We also consider the morpheme embeddings alone. For S UM E MBED, this means that we constru"
D16-1047,P16-1193,0,0.0244349,"ddings. Vilnis and McCallum (2014) propose to model Gaussian densities over dense vector word embeddings. They estimate the parameters of the Gaussian directly, and, unlike our work, do not consider using orthographic information as a prior distribution. This is easy to do in the latent binary framework proposed here, which is also a better fit for some theoretical models of lexical semantics (Katz and Fodor, 1963; Reisinger et al., 2015). This view is shared by Kruszewski et al. (2015), who induce binary word representations using labeled data of lexical semantic entailment relations, and by Henderson and Popa (2016), who take a mean field approximation over binary representations of lexical semantic features to induce hyponymy relations. More broadly, our work is inspired by recent efforts to combine directed graphical models with discriminatively trained “deep learning” architectures. The variational autoencoder (Kingma and Welling, 2014), neural variational inference (Mnih and Gregor, 2014; Miao et al., 2016), and black box variational inference (Ranganath et al., 2014) all propose to use a neural network to compute the variational approximation. These ideas are employed by Chung et al. (2015) in the v"
D16-1047,Q15-1027,0,0.0168946,"erved only a handful of times. It is therefore natural to consider whether it might be beneficial to model uncertainty over word embeddings. Vilnis and McCallum (2014) propose to model Gaussian densities over dense vector word embeddings. They estimate the parameters of the Gaussian directly, and, unlike our work, do not consider using orthographic information as a prior distribution. This is easy to do in the latent binary framework proposed here, which is also a better fit for some theoretical models of lexical semantics (Katz and Fodor, 1963; Reisinger et al., 2015). This view is shared by Kruszewski et al. (2015), who induce binary word representations using labeled data of lexical semantic entailment relations, and by Henderson and Popa (2016), who take a mean field approximation over binary representations of lexical semantic features to induce hyponymy relations. More broadly, our work is inspired by recent efforts to combine directed graphical models with discriminatively trained “deep learning” architectures. The variational autoencoder (Kingma and Welling, 2014), neural variational inference (Mnih and Gregor, 2014; Miao et al., 2016), and black box variational inference (Ranganath et al., 2014)"
D16-1047,D15-1176,0,0.110973,"Missing"
D16-1047,W13-3512,0,0.81638,"irectly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging. 1 Recent work has proposed to address this issue by replacing word-level embeddings with embeddings based on subword units: morphemes (Luong et al., 2013; Botha and Blunsom, 2014) or individual characters (Santos and Zadrozny, 2014; Ling et al., 2015; Kim et al., 2016). Such models leverage the fact that word meaning is often compositional, arising from subword components. By learning representations of subword units, it is possible to generalize to rare and unseen words. Introduction Word embeddings have been shown to improve many natural language processing applications, from language models (Mikolov et al., 2010) to information extraction (Collobert and Weston, 2008), and from parsing (Chen and Manning, 2014) to machine translation (Cho et"
D16-1047,Q15-1034,0,0.0329312,"Missing"
D16-1047,D15-1243,0,0.159426,"but is not applicable to the complete dataset. On the rare words dataset, WORD 2 VEC performs considerably worse than both morphology-based models, matching the findings of Luong et al. (2013) and Botha and Blunsom (2014) regarding the importance of morphology for doing well on this dataset. 4.2 Alignment with lexical semantic features Recent work questions whether these word similarity metrics are predictive of performance on downstream tasks (Faruqui et al., 2016). The QVEC statistic is another intrinsic evaluation method, which has been shown to be better correlated with downstream tasks (Tsvetkov et al., 2015). This metric measures the alignment between word embeddings and a set of lexical semantic features. Specifically, we use the semcor noun verb supersenses oracle provided at the qvec github repository.2 As shown in Table 2, VAR E MBED outperforms S UM E MBED on the full lexicon, and gives similar performance to WORD 2 VEC on the set of invocabulary words. We also consider the morpheme embeddings alone. For S UM E MBED, this means that we construct the word embedding from the sum of the embeddings for its morphemes, without the additional embedding per word. For VAR E MBED, we use the expected"
D16-1047,P14-2089,0,0.0304315,"ears that this simple additive model is better, since the distributional statistics are too sparse to offer much improvement. The probabilistic VAR E MBED embeddings are best for all other frequency groups, showing that it effectively combines morphology and distributional statistics. 5 Related work Adding side information to word embeddings An alternative approach to incorporating additional information into word embeddings is to constrain the embeddings of semantically-related words to be similar. Such work typically draws on existing lexical semantic resources such as WordNet. For example, Yu and Dredze (2014) define a joint training objective, in which the word embedding must predict not only neighboring word tokens in a corpus, but also related word types in a semantic resource; a similar approach is taken by Bian et al. (2014). Alternatively, Faruqui et al. (2015) propose to “retrofit” pre-trained word embeddings over a semantic network. Both retrofitting and our own approach treat the true word embeddings as latent variables, from which the pretrained word embeddings are stochastically emitted. However, a key difference from our approach is that the underlying representation in these prior work"
D16-1152,E06-1002,0,0.068515,"disambiguates entities for mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities throug"
D16-1152,D07-1074,0,0.0153479,"or mentions such as ‘Sox’ (Boston Red Sox vs. Chicago White Sox), ‘Sanders’ (Bernie Sanders vs. Barry Sanders), and ‘Memphis’ (Memphis Grizzlies vs. Memphis, Tennessee), which are mistakenly linked to the other entities or Nil by the mentionentity model. Another example is that the social network information helps the system correctly link ‘Kim’ to Lil’ Kim instead of Kim Kardashian, despite that the latter entity’s wikipedia page is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagat"
D16-1152,Q14-1021,1,0.880186,"performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems"
D16-1152,N13-1122,1,0.94033,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,D13-1085,0,0.619753,"ce on embedding information networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distribut"
D16-1152,P15-1073,0,0.0584107,"mpossible to disambiguate between these entities solely based on the individual text message. We propose to overcome the difficulty and improve the entity disambiguation capability of the entity linking system by employing social network structures. The sociological theory of homophily asserts that socially connected individuals are more likely to have similar behaviors or share similar interests (McPherson et al., 2001). This property has been used to improve many natural language processing tasks such as sentiment analysis (Tan et al., 2011; Yang and Eisenstein, 2015), topic classification (Hovy, 2015) and user attribute inference (Li et al., 2015). We assume Twitter users will have similar interests in real world entities to their near neighbors — an assumption of entity homophily — which 1452 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1452–1461, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics is demonstrated in Figure 1. The social relation between users u1 and u2 may lead to more coherent topics in tweets t1 and t2 . Therefore, by successfully linking the less ambiguous mention ‘Red Sox’ in tweet t2 to"
D16-1152,P14-1036,0,0.01724,"age is considerably more popular. 6 Related Work Tweet entity linking Previous work on entity linking mainly focuses on well-written documents (Bunescu and Pasca, 2006; Cucerzan, 2007; Milne and Witten, 2008), where entity disambigua1459 tion is usually performed by maximizing the global topical coherence between entities. However, these approaches often yield unsatisfactory performance on Twitter messages, due to the short and noisy nature of the tweets. To tackle this problem, collective tweet entity linking methods that leverage enriched context and metadata information have been proposed (Huang et al., 2014). Guo et al. (2013b) search for textually similar tweets for a target tweet, and encourage these Twitter messages to contain similar entities through label propagation. Shen et al. (2013) employ Twitter user account information to improve entity linking, based on the intuition that all tweets posted by the same user share an underlying topic distribution. Fang and Chang (2014) demonstrate that spatial and temporal signals are critical for the task, and they advance the performance by associating entity prior distributions with different timestamps and locations. Our work overcomes the difficul"
D16-1152,N15-1142,0,0.0125348,"mes that Twitter users sharing many neighbors are close to each other in the embedding space. According to the original paper, the second-order proximity yields slightly better performances than the firstorder proximity, which assumes connecting users are close to each other, on a variety of downstream tasks. be written as: Mention embeddings The representation of a mention is the average of embeddings of words it contains. As each mention is typically one to three words, the simple representations often perform surprisingly well (Socher et al., 2013). We adopt the structured skip-gram model (Ling et al., 2015) to learn the word embeddings E(w) on a Twitter corpus with 52 million tweets (Owoputi et al., 2013). The mention vector of the t-th mention candidate can be written as: X 1 (m) (w) vt = (w) vw , (4) |xt | (w) where W(u,e) and W(m,e) are D(u) × D(e) and D(w) × D(e) bilinear transformation matrices. Similar bilinear formulation has been used in the literature of knowledge base completion and inference (Socher et al., 2013; Yang et al., 2014). The parameters of the composition model are Θ2 = {W(u,e) , W(m,e) , E(u) , E(w) , E(e) }. w∈xt (w) where xt is the set of words in the mention. Entity emb"
D16-1152,N13-1039,0,0.0370358,"Missing"
D16-1152,W10-0510,1,0.916876,"Missing"
D16-1152,D11-1141,0,0.0551077,"onger social network ties than directed links (Kwak et al., 2010; Wu et al., 2011). The numbers of social relations for the networks are 1,604, 379 and 342 respectively. 1 We are able to obtain at most 3,200 tweets for each Twitter user, due to the Twitter API limits. Network F OLLOWER M ENTION R ETWEET sim(i ↔ j) sim(i ↔ / j) 0.128 0.121 0.173 0.025 0.025 0.025 Table 2: The average entity-driven similarity results for the networks. Metrics We propose to use the entity-driven similarity between authors to test the hypothesis of entity homophily. For a user ui , we employ a Twitter NER system (Ritter et al., 2011) to detect entity mentions in the timeline, which we use to construct (ent) (ent) a user entity vector ui , so that ui,j = 1 iff user i has mentioned entity j.2 The entity-driven similarity between two users ui and uj is defined as the cosine similarity score between the vectors (ent) (ent) ui and uj . We evaluate the three networks by calculating the average entity-driven similarity of the connected user pairs and that of the disconnected user pairs, which we name as sim(i ↔ j) and sim(i ↔ / j). Results The entity-driven similarity results of these networks are presented in Table 2. As shown,"
D16-1152,W11-2207,0,0.106461,"work overcomes the difficulty by leveraging social relations — socially connected individuals are assumed to share similar interests on entities. As the Twitter post information is often sparse for some users, our assumption enables the utilization of more relevant information that helps to improve the task. NLP with social relations Most previous work on incorporating social relations for NLP problems focuses on Twitter sentiment analysis, where the existence of social relations between users is considered as a clue that the sentiment polarities of messages from the users should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions. We work on entity linking in Twitter messages, where the label space is much larger than that of sentiment classification. The soci"
D16-1152,P15-1049,1,0.811948,"ormation networks (Tang et al., 2015), which induces low-dimensional representations for author nodes based on the network structure. By learning the semantic interactions between the author embeddings and the pre-trained Freebase entity embeddings, the entity linking system can incorporate more disambiguating context from the social network. We also consider lowdimensional representations of mentions, another source of related information for entity linking, with the intuition that semantically related mentions can refer to similar entities. Previously proposed approaches (Guo et al., 2013a; Yang and Chang, 2015) are based on hand-crafted features and off-the-shelf machine learning algorithms. Our preliminary study suggests that simply augmenting the traditional surface features with the distributed representations barely improves the performance of these entity linking systems. Therefore, we propose N TEL, a Neural model for Tweet Entity Linking, to leverage the distributed representations of authors, mentions, and entities. N TEL can not only make efficient use of statistical surface features built from a knowledge base, but also learn the interactions between these distributed representations. Our"
D16-1152,P15-1128,1,0.786133,"Missing"
D17-1010,W13-2408,0,0.0604151,"Missing"
D17-1010,P98-1080,0,0.430737,"Missing"
D17-1010,W13-3520,0,0.0150447,"output of the training phase is the character embeddings matrix C and the parameters of the neural network: M = {C, F, B, Th , bh , OT , bT }, where F, B are the forward and backward LSTM component parameters, respectively. 3.1 MIMICK Nearest-neighbor examination. As a preliminary sanity check for the validity of our protocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish. Table 1 presents selected English OOV words with Polyglot Embeddings The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort. Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language’s training corpus (of variable size), as well as an UNK embedding to be used for OOV words. Even with this vocabulary size, querying words from respective UD corpora (train + dev + test) yields high 1 Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD. Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein wo"
D17-1010,D16-1047,1,0.890705,"new tool for tagging tasks in settings where there is limited labeled data. Models and code are available at www.github.com/ yuvalpinter/mimick . 2 Related Work Compositional models for embedding rare and unseen words. Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure. Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings. While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes. Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings. In all of these cases, the model for"
D17-1010,D15-1176,0,0.0936953,"Missing"
D17-1010,W13-3512,0,0.817822,"ain a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word’s spelling and mimick its distributional embedding. Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features. As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors. Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words. As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset (De Marneffe et al., 2014). Our model shows significant improvement Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data. However, the effectiveness of word"
D17-1010,de-marneffe-etal-2014-universal,0,0.0386628,"Missing"
D17-1010,D13-1032,0,0.0606509,"Missing"
D17-1010,dzeroski-etal-2000-morphosyntactic,0,0.134559,"han words. phological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute. precision are calculated over the entire set, with F1 defined as their harmonic mean. 5.1 We implement and test the following models: 5.2 Metrics As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages4 with the supplied train/dev/test division. We measure partof-speech tagging by overall token-level accuracy. For morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance. Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene. Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5 Agi´c et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncms"
D17-1010,N15-1184,0,0.0183124,"ample, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings. In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus. In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained. This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally. Morphosyntactic attribute tagging. We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense. The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For"
D17-1010,A94-1024,0,0.0321046,"Missing"
D17-1010,Q16-1001,0,0.0806201,"her than smoothing embeddings over a graph, we learn a function to build embeddings compositionally. Morphosyntactic attribute tagging. We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense. The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014). For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger. In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016). 3 MIMICK Word Embeddings We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordformbased protocol for creating these embeddings. By training a model over the existing vocabulary,"
D17-1010,petrov-etal-2012-universal,0,0.0240437,"shape and part-of-speech learning patterns with some loose semantics: for example, the plural adjective form prenatales is similar to other familyrelated plural adjectives such as patrimoniales and generacionales. Tamil displays some semantic similarities as well: e.g. enjineer (‘engineer’) predicts similarity to other professional terms such as kalviyiyal (‘education’), thozhilnutpa (‘technical’), and iraanuva (‘military’). 4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories. For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish. These additional morphosyntactic attributes are marked in the UD dataset as optional per-token attribute-value pairs. Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM. We extend this approach to morphosyntactic tagging by dupl"
D17-1010,P16-2067,0,0.298981,"ear in similar contexts. 102 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 102–112 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics acters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process. Ling et al. (2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words. Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging. Because these methods learn from labeled data, they can cover only as much of the lexicon as appears in their labeled training sets. As we show, they struggle in several settings: lowresource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scrip"
D17-1010,E12-1050,0,0.029422,"does not seem to be an agreed-upon metric for reporting performance. Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene. Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5 Agi´c et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag. Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680. M¨uller et al. (2013) do the same for six other languages. We report micro F1: each token’s value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment. Recall and Models No-Char. Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector. Following tuning experiments on all languages with ca"
D17-1010,W17-0415,0,0.0407757,"Missing"
D17-1010,D16-1157,0,0.0231159,"gs of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings. While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes. Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings. In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus. In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained. This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally. Morphosyntactic attribute tagging."
D17-1010,P16-1128,0,0.0368319,"Missing"
D18-1201,D15-1038,0,0.0285404,"Missing"
D18-1201,P15-1016,0,0.0196773,"ge. On the local level, they describe connections between specific semantic concepts, or synsets, through individual edges representing relations such as hypernymy (‘is-a’) or meronymy (‘is-part-of’); on the global level, they encode emergent regular properties in the induced relation graphs. Local properties have been subject to extensive study in recent years via the task of relation prediction, where individual edges are found based mostly on distributional methods that embed synsets and relations into a vector space (e.g. Socher et al., 2013; Bordes et al., 2013; Toutanova and Chen, 2015; Neelakantan et al., 2015). In contrast, while the structural regularity and significance of global aspects of semantic graphs is well-attested (Sigman and Cecchi, 2002), global properties have rarely been used in prediction settings. In this paper, we show how global semantic graph features can facilitate in local tasks such as relation prediction. To motivate this approach, consider the hypothetical hypernym graph fragments in Figure 1: in (a), the semantic concept (synset) ‘catamaran’ has 1741 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1741–1751 c Brussels, Belgium,"
D18-1201,N18-2053,0,0.209524,"Missing"
D18-1201,D14-1162,0,0.0961873,"line), they are excluded from the sampling protocol described in eq. (5), although their edges do contribute to the combinatory graph feature vector f . Our default setting backpropagates loss into only the graph weight vector θ. We experiment with a model variant which backpropagates into the association model and synset embeddings as well. 4.4 Synset Embeddings For the association component of our model, we require embedding representations for WordNet synsets. While unsupervised word embedding techniques go a long way in representing wordforms (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014), they are not immediately applicable to the semantically-precise domain of synsets. We explore two methods of transforming pre-trained word embeddings into synset embeddings. Averaging. A straightforward way of using word embeddings to create synset embeddings is to collect the words representing the synset as surface form within the WordNet dataset and average their embeddings (Socher et al., 2013). We apply this method to pre-trained GloVe embeddings (Pennington et al., 2014) and pre-trained FastText embeddings (Bojanowski et al., 2017), averaging over the set of all wordforms in all lemmas"
D18-1201,D17-1010,1,0.803267,"etrofitting + Mimick. AutoExtend is a method developed specifically for embedding WordNet synsets (Rothe and Sch¨utze, 2015), in which pre-trained word embeddings are retrofitted to the tripartite relation graph connecting wordforms, lemmas, and synsets. The resulting synset embeddings occupy the same space as the word embeddings. However, some WordNet senses are not represented in the underlying set of pre-trained word embeddings.5 To handle these cases, we trained a character-based model called M IMICK, which learns to predict embeddings for out-of-vocabulary items based on their spellings (Pinter et al., 2017). We do not modify the spelling conventions of WordNet synsets before passing them to Mimick, so e.g. ‘mask.n.02’ (the second synset corresponding to ‘mask’ as a noun) acts as the input character sequence as is. Random initialization. In preliminary experiments, we attempted training the association models using randomly-initialized embeddings. These proved to be substantially weaker than distributionally-informed embeddings and we do not report their performance in the results section. We view this finding as strong evidence to support the necessity of a distributional signal in a typelevel s"
D18-1201,N13-1008,0,0.0159179,"ww.github. com/yuvalpinter/m3gm. 1742 Some recent works compose single edges into more intricate motifs, such as Guu et al. (2015), who define a task of path prediction and compose various functions to solve it. They find that compositionalized bilinear models perform best on WordNet. Minervini et al. (2017) train link-prediction models against an adversary that produces examples which violate structural constraints such as symmetry and transitivity. Another line of work builds on local neighborhoods of relation interactions and automatic detection of relations from syntactically parsed text (Riedel et al., 2013; Toutanova et al., 2015). Schlichtkrull et al. (2017) use Graph Convolutional Networks to predict relations while considering high-order neighborhood properties of the nodes in question. In general, these methods aggregate information over local neighborhoods, but do not explicitly model structural motifs. Our model introduces interaction features between relations (e.g., hypernyms and meronyms) for the goal of relation prediction. To our knowledge, this is the first time that relation interaction is explicitly modeled into a relation prediction task. Within the ERGM framework, Lu et al. (201"
D18-1201,P15-1173,0,0.0584578,"Missing"
D18-1201,W15-4007,0,0.110305,"sentation of human knowledge. On the local level, they describe connections between specific semantic concepts, or synsets, through individual edges representing relations such as hypernymy (‘is-a’) or meronymy (‘is-part-of’); on the global level, they encode emergent regular properties in the induced relation graphs. Local properties have been subject to extensive study in recent years via the task of relation prediction, where individual edges are found based mostly on distributional methods that embed synsets and relations into a vector space (e.g. Socher et al., 2013; Bordes et al., 2013; Toutanova and Chen, 2015; Neelakantan et al., 2015). In contrast, while the structural regularity and significance of global aspects of semantic graphs is well-attested (Sigman and Cecchi, 2002), global properties have rarely been used in prediction settings. In this paper, we show how global semantic graph features can facilitate in local tasks such as relation prediction. To motivate this approach, consider the hypothetical hypernym graph fragments in Figure 1: in (a), the semantic concept (synset) ‘catamaran’ has 1741 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 174"
D18-1201,D15-1174,0,0.0149016,"inter/m3gm. 1742 Some recent works compose single edges into more intricate motifs, such as Guu et al. (2015), who define a task of path prediction and compose various functions to solve it. They find that compositionalized bilinear models perform best on WordNet. Minervini et al. (2017) train link-prediction models against an adversary that produces examples which violate structural constraints such as symmetry and transitivity. Another line of work builds on local neighborhoods of relation interactions and automatic detection of relations from syntactically parsed text (Riedel et al., 2013; Toutanova et al., 2015). Schlichtkrull et al. (2017) use Graph Convolutional Networks to predict relations while considering high-order neighborhood properties of the nodes in question. In general, these methods aggregate information over local neighborhoods, but do not explicitly model structural motifs. Our model introduces interaction features between relations (e.g., hypernyms and meronyms) for the goal of relation prediction. To our knowledge, this is the first time that relation interaction is explicitly modeled into a relation prediction task. Within the ERGM framework, Lu et al. (2010) train a limited set of"
D18-1467,J10-1005,0,0.0339542,"hen a word’s popularity begins to decline. All tests indicate that linguistic dissemination plays an important role in explaining the growth and decline of nonstandard words. 2 Related Work Lexical change online Language changes constantly, and one of the most notable forms of change is the adoption of new words (Metcalf, 2004), sometimes referred to as lexical entrenchment (Chesley and Baayen, 2010). New nonstandard words may arise through the mutation of existing forms by processes such as truncation (e.g, favorite to fave; Grieve et al., 2016) and blending (e.g., web+log to weblog to blog; Cook and Stevenson, 2010). The fast pace and interconnected nature of online communication is particularly conducive to innovation, and social media provides a “birds-eye view” on the process of change (Danescu-Niculescu-Mizil et al., 2013; Kershaw et al., 2016; Tsur and Rappoport, 2015). The most closely related work is a contemporaneous study that explored the role of weak social ties in the dissemination of linguistic innovations on Reddit, which also proposed the task of quantitatively predicting the success or failure of lexical innovations (Tredici and Fernández, 2018). One distinguishing feature of our work is"
D18-1467,P12-2027,0,0.62709,"diffusion across individuals and social groups (Bucholtz, 1999). Such diffusion can be quantified with social dissemination, which Altmann et al. (2011) define as the count of social units (e.g., users) who have adopted a word, normalized by the expected count under a null model in which the word is used with equal frequency across the entire population. Altmann et al. (2011) use dissemination of words across forum users and threads to predict the words’ change in frequency in Usenet, finding a positive correlation between frequency change and both kinds of social dissemination. In contrast, Garley and Hockenmaier (2012) use the same metric to predict the growth of English loanwords on German hip-hop forums, and find that social dissemination has less predictive power than expected. We seek to replicate these prior findings, and to extend them to the broader context of Reddit. Linguistic dissemination In historical linguistics, the distribution of a new word or construction across lexical contexts can signal future growth (Partington, 1993). Furthermore, grammatical and lexical factors can explain a speaker’s choice of linguistic variant (Ito and Tagliamonte, 2003; Cacoullos and Walker, 2009) and can provide"
D18-1467,P11-2008,1,0.431369,"Missing"
D18-1467,C18-1135,0,0.0469944,"and blending (e.g., web+log to weblog to blog; Cook and Stevenson, 2010). The fast pace and interconnected nature of online communication is particularly conducive to innovation, and social media provides a “birds-eye view” on the process of change (Danescu-Niculescu-Mizil et al., 2013; Kershaw et al., 2016; Tsur and Rappoport, 2015). The most closely related work is a contemporaneous study that explored the role of weak social ties in the dissemination of linguistic innovations on Reddit, which also proposed the task of quantitatively predicting the success or failure of lexical innovations (Tredici and Fernández, 2018). One distinguishing feature of our work is the emphasis on linguistic (rather than social) context in explaining these successes and failures. In addition to predicting the binary distinction between success and failure, we also take on the more finegrained task of predicting the length of time that each nonstandard word will survive. Social dissemination Language changes as a result of transmission across generations (Labov, 2007) as well as diffusion across individuals and social groups (Bucholtz, 1999). Such diffusion can be quantified with social dissemination, which Altmann et al. (2011)"
D18-1467,N18-1129,0,0.0651386,"Missing"
D18-1467,P12-3005,0,0.0504528,"Missing"
D19-1433,P18-1099,0,0.0548336,"tackle unsupervised domain adaptation, which assumes only unlabeled instances in the target domain. In this setting, prior work has focused on domain-adversarial objectives, which construct an auxiliary loss based on the capability of an adversary to learn to distinguish the domains based on a shared encoding of the input (Ganin et al., 2016; Purushotham et al., 2017). However, adversarial methods require balancing between at least two and as many as six different objectives (Kim et al., 2017), which can lead to instability (Arjovsky et al., 2017) unless the objectives are carefully balanced (Alam et al., 2018). In addition to supervised and unsupervised domain adaptation, there are “distantly supervised” methods that construct noisy target domain instances, e.g., by using a bilingual dictionary (Fang and Cohn, 2017). Normalization dictionaries exist for Early Modern English (Baron and Rayson, 2008) and social media (Han et al., 2012), but we leave their application to distant supervision for future work. Finally, language modeling objectives have previously been used for domain adaptation of text classifiers (Ziser and Reichart, 2018), but this prior work has focused on representation learning from"
D19-1433,I13-1041,0,0.0246511,"mapped to the preposition IN. AdaptaBERT often incorrectly tags that as WDT , when IN would be correct. These examples point to the inherent limitations of unsupervised domain adaptation when there are inconsistencies in the annotation protocol. 6 Social Media Microblogs As an additional evaluation, we consider social media microblogs, of which Twitter is the best known example in English. Twitter poses some of the same challenges as historical text: orthographic variation leads to a substantial mismatch in vocabulary between the target domain and source training documents such as Wikipedia (Baldwin et al., 2013; Eisenstein, 2013). We hypothesize that domain-adaptive finetuning can help to produce better contextualized word embeddings for microblog text. Our evaluation is focused on the problem of identifying named entity spans in Tweets, which was the shared task of the 2016 Workshop on Noisy User Text (WNUT; Strauss et al., 2016). In 4243 the shared task, systems were given access to labeled data in the target domain; in contrast, we are interested to measure whether it is possible to perform this task without access to such data. As training data, we use the canonical C O NLL 2003 shared task data"
D19-1433,N19-1149,0,0.0287268,". BioBERT is an application of BERT to the biomedical domain, which was achieved by pretraining on more than 10 billion tokens of biomedical abstracts and full-text articles from PubMed (Lee et al., 2019); SciBERT is a similar approach for scientific texts (Beltagy et al., 2019). Data on this scale is not available in Early Modern English, but retraining might be applicable to social media text, assuming the user has access to both large-scale data and sufficient computing power. The mismatch between pretrained contextualized embeddings and technical NER corpora was explored in recent work by Dai et al. (2019). 8 Conclusion This paper demonstrates the applicability of contextualized word embeddings to two difficult un4245 supervised domain adaptation tasks. On the task of adaptation to historical text, BERT works relatively well out-of-the-box, yielding equivalent performance to the best prior unsupervised domain adaptation approach. Domain-adaptive fine tuning on unlabeled target domain data yields significant further improvements, especially on OOV terms. On the task of adaptation to contemporary social media, a straightforward application of BERT yields competitive results, and domainadaptive fi"
D19-1433,W18-1601,0,0.0172762,"equires data and computational resources that are often unavailable. 4238 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4238–4248, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics language change (Hilpert and Gries, 2016), social norms (Garg et al., 2018), and the history of ideas and culture (Michel et al., 2011). Syntactic analysis can play an important role: researchers have used part-of-speech tagging to identify syntactic changes (Degaetano-Ortlieb, 2018) and dependency parsing to quantify gender-based patterns of adjectival modification and possession in classic literary texts (Vuillemot et al., 2009; Muralidharan and Hearst, 2013). But despite the appeal of using NLP in historical linguistics and literary analysis, there is relatively little research on how performance is impacted by diachronic transfer. Indeed, the evidence that does exist suggests that accuracy degrades significantly, especially if steps are not taken to adapt: for example, Yang and Eisenstein (2015) compare the accuracy of tagging 18th century and 16th century Portuguese"
D19-1433,N19-1423,0,0.688901,"embeddings are adapted by masked language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-ofvocabulary words. We conclude that domainadaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.1 1 • Wikipedia in BERT (Devlin et al., 2019) and ULMFiT (Howard and Ruder, 2018); • Newstext (Chelba et al., 2013) in ELMo (Peters et al., 2018); • BooksCorpus (Zhu et al., 2015) in BERT (Devlin et al., 2019) and GPT (Radford et al., 2018). Introduction Contextualized word embeddings are becoming a ubiquitous component of natural language processing (Dai and Le, 2015; Devlin et al., 2019; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018). Pretrained contextualized word embeddings can be used as feature for downstream ∗ XH is now at Carnegie Mellon University and JE is now at Google Research. Some of the work was perform"
D19-1433,N13-1037,1,0.768035,"tion IN. AdaptaBERT often incorrectly tags that as WDT , when IN would be correct. These examples point to the inherent limitations of unsupervised domain adaptation when there are inconsistencies in the annotation protocol. 6 Social Media Microblogs As an additional evaluation, we consider social media microblogs, of which Twitter is the best known example in English. Twitter poses some of the same challenges as historical text: orthographic variation leads to a substantial mismatch in vocabulary between the target domain and source training documents such as Wikipedia (Baldwin et al., 2013; Eisenstein, 2013). We hypothesize that domain-adaptive finetuning can help to produce better contextualized word embeddings for microblog text. Our evaluation is focused on the problem of identifying named entity spans in Tweets, which was the shared task of the 2016 Workshop on Noisy User Text (WNUT; Strauss et al., 2016). In 4243 the shared task, systems were given access to labeled data in the target domain; in contrast, we are interested to measure whether it is possible to perform this task without access to such data. As training data, we use the canonical C O NLL 2003 shared task dataset, in which named"
D19-1433,P17-2093,0,0.0304107,"based on the capability of an adversary to learn to distinguish the domains based on a shared encoding of the input (Ganin et al., 2016; Purushotham et al., 2017). However, adversarial methods require balancing between at least two and as many as six different objectives (Kim et al., 2017), which can lead to instability (Arjovsky et al., 2017) unless the objectives are carefully balanced (Alam et al., 2018). In addition to supervised and unsupervised domain adaptation, there are “distantly supervised” methods that construct noisy target domain instances, e.g., by using a bilingual dictionary (Fang and Cohn, 2017). Normalization dictionaries exist for Early Modern English (Baron and Rayson, 2008) and social media (Han et al., 2012), but we leave their application to distant supervision for future work. Finally, language modeling objectives have previously been used for domain adaptation of text classifiers (Ziser and Reichart, 2018), but this prior work has focused on representation learning from scratch, rather than adaptation of a pretrained contextualized embedding model. Our work shows that models that are pretrained on large-scale data yield very strong performance even when applied out-of-domain,"
D19-1433,D12-1039,0,0.131454,"et al., 2016; Purushotham et al., 2017). However, adversarial methods require balancing between at least two and as many as six different objectives (Kim et al., 2017), which can lead to instability (Arjovsky et al., 2017) unless the objectives are carefully balanced (Alam et al., 2018). In addition to supervised and unsupervised domain adaptation, there are “distantly supervised” methods that construct noisy target domain instances, e.g., by using a bilingual dictionary (Fang and Cohn, 2017). Normalization dictionaries exist for Early Modern English (Baron and Rayson, 2008) and social media (Han et al., 2012), but we leave their application to distant supervision for future work. Finally, language modeling objectives have previously been used for domain adaptation of text classifiers (Ziser and Reichart, 2018), but this prior work has focused on representation learning from scratch, rather than adaptation of a pretrained contextualized embedding model. Our work shows that models that are pretrained on large-scale data yield very strong performance even when applied out-of-domain, making them a natural starting point for further adaptation. Semi-supervised learning. Universal Language Model Fine-tu"
D19-1433,P18-1031,0,0.210869,"language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-ofvocabulary words. We conclude that domainadaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.1 1 • Wikipedia in BERT (Devlin et al., 2019) and ULMFiT (Howard and Ruder, 2018); • Newstext (Chelba et al., 2013) in ELMo (Peters et al., 2018); • BooksCorpus (Zhu et al., 2015) in BERT (Devlin et al., 2019) and GPT (Radford et al., 2018). Introduction Contextualized word embeddings are becoming a ubiquitous component of natural language processing (Dai and Le, 2015; Devlin et al., 2019; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018). Pretrained contextualized word embeddings can be used as feature for downstream ∗ XH is now at Carnegie Mellon University and JE is now at Google Research. Some of the work was performed while JE was visiting Facebook AI"
D19-1433,P17-1119,0,0.0458647,"output layers individually (e.g., Yang et al., 2016) or simultaneously (Lin and Lu, 2018). Unlike these approaches, we tackle unsupervised domain adaptation, which assumes only unlabeled instances in the target domain. In this setting, prior work has focused on domain-adversarial objectives, which construct an auxiliary loss based on the capability of an adversary to learn to distinguish the domains based on a shared encoding of the input (Ganin et al., 2016; Purushotham et al., 2017). However, adversarial methods require balancing between at least two and as many as six different objectives (Kim et al., 2017), which can lead to instability (Arjovsky et al., 2017) unless the objectives are carefully balanced (Alam et al., 2018). In addition to supervised and unsupervised domain adaptation, there are “distantly supervised” methods that construct noisy target domain instances, e.g., by using a bilingual dictionary (Fang and Cohn, 2017). Normalization dictionaries exist for Early Modern English (Baron and Rayson, 2008) and social media (Han et al., 2012), but we leave their application to distant supervision for future work. Finally, language modeling objectives have previously been used for domain ad"
D19-1433,W16-3920,0,0.0601292,"Missing"
D19-1433,D18-1226,0,0.0260047,"rn English, we find that domain-adaptive fine-tuning does not impair performance on the source domain (C O NLL), but supervised in-domain training increases the source domain error rate dramatically. 7 Related Work Adaptation in neural sequence labeling. Most prior work on adapting neural networks for NLP has focused on supervised domain adaptation, in which a labeled data is available in the target domain (Mou et al., 2016). RNN-based models for sequence labeling can be adapted across domains by manipulating the input or output layers individually (e.g., Yang et al., 2016) or simultaneously (Lin and Lu, 2018). Unlike these approaches, we tackle unsupervised domain adaptation, which assumes only unlabeled instances in the target domain. In this setting, prior work has focused on domain-adversarial objectives, which construct an auxiliary loss based on the capability of an adversary to learn to distinguish the domains based on a shared encoding of the input (Ganin et al., 2016; Purushotham et al., 2017). However, adversarial methods require balancing between at least two and as many as six different objectives (Kim et al., 2017), which can lead to instability (Arjovsky et al., 2017) unless the objec"
D19-1433,J93-2004,0,0.0648939,"nces of the pronoun thee. 2.2 Part-of-Speech Tags in the Penn Parsed Corpora of Historical English The Penn Parsed Corpora of Historical English (PPCHE) include part-of-speech annotations for texts from several historical periods (Kroch et al., 2004). We focus on the corpus covering Early Modern English, which we refer to as PPCEME. As discussed in § 7, prior work has generally treated tagging the PPCEME as a problem of domain adaptation, with a post-processing stage to map deterministically between the tagsets. Specifically, we train on the Penn Treebank (PTB) corpus of 20th century English (Marcus et al., 1993), and then evaluate on the PPCEME test set, using a mapping between the PPCHE and PTB tagsets defined by Moon and Baldridge (2007). Unfortunately, there are some fundamental differences in the approaches to verbs taken by each tagset. Unlike the PTB, the PPCHE has distinct tags for the modal verbs have, do, and be (and their infections); unlike the PPCHE, the PTB has distinct tags for third-person singular present indicative (VBZ) and other present indicative verbs (VBP). Moon and Baldridge map only to VBP, and Yang and Eisenstein report an error when VBZ is predicted, even though the correspo"
D19-1433,D07-1041,0,0.253326,"Historical English (PPCHE) include part-of-speech annotations for texts from several historical periods (Kroch et al., 2004). We focus on the corpus covering Early Modern English, which we refer to as PPCEME. As discussed in § 7, prior work has generally treated tagging the PPCEME as a problem of domain adaptation, with a post-processing stage to map deterministically between the tagsets. Specifically, we train on the Penn Treebank (PTB) corpus of 20th century English (Marcus et al., 1993), and then evaluate on the PPCEME test set, using a mapping between the PPCHE and PTB tagsets defined by Moon and Baldridge (2007). Unfortunately, there are some fundamental differences in the approaches to verbs taken by each tagset. Unlike the PTB, the PPCHE has distinct tags for the modal verbs have, do, and be (and their infections); unlike the PPCHE, the PTB has distinct tags for third-person singular present indicative (VBZ) and other present indicative verbs (VBP). Moon and Baldridge map only to VBP, and Yang and Eisenstein report an error when VBZ is predicted, even though the corresponding PPCHE tag would be identical in both cases. We avoid this issue by focusing most of our evaluations on a coarse-grained vers"
D19-1433,D16-1046,0,0.0360602,"T shared task. The state-of-the-art system makes use of character-level information that is not available to our models (Limsopatham and Collier, 2016). As with the evaluation on Early Modern English, we find that domain-adaptive fine-tuning does not impair performance on the source domain (C O NLL), but supervised in-domain training increases the source domain error rate dramatically. 7 Related Work Adaptation in neural sequence labeling. Most prior work on adapting neural networks for NLP has focused on supervised domain adaptation, in which a labeled data is available in the target domain (Mou et al., 2016). RNN-based models for sequence labeling can be adapted across domains by manipulating the input or output layers individually (e.g., Yang et al., 2016) or simultaneously (Lin and Lu, 2018). Unlike these approaches, we tackle unsupervised domain adaptation, which assumes only unlabeled instances in the target domain. In this setting, prior work has focused on domain-adversarial objectives, which construct an auxiliary loss based on the capability of an adversary to learn to distinguish the domains based on a shared encoding of the input (Ganin et al., 2016; Purushotham et al., 2017). However,"
D19-1433,N18-1202,0,0.249067,"roach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-ofvocabulary words. We conclude that domainadaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.1 1 • Wikipedia in BERT (Devlin et al., 2019) and ULMFiT (Howard and Ruder, 2018); • Newstext (Chelba et al., 2013) in ELMo (Peters et al., 2018); • BooksCorpus (Zhu et al., 2015) in BERT (Devlin et al., 2019) and GPT (Radford et al., 2018). Introduction Contextualized word embeddings are becoming a ubiquitous component of natural language processing (Dai and Le, 2015; Devlin et al., 2019; Howard and Ruder, 2018; Radford et al., 2018; Peters et al., 2018). Pretrained contextualized word embeddings can be used as feature for downstream ∗ XH is now at Carnegie Mellon University and JE is now at Google Research. Some of the work was performed while JE was visiting Facebook AI Research. 1 Trained models for Early Modern English and Twitter"
D19-1433,W19-4302,0,0.0467454,"Missing"
D19-1433,N18-2113,0,0.0298974,"hand-crafted features. This was shown to significantly reduce the transfer loss in Portuguese, and later in English (Yang and Eisenstein, 2016). However, this approach relies on hand-crafted features, and does not benefit from contemporary neural pretraining architectures. We show that pretrained contextualized embeddings yield significant improvements over feature-based methods. Normalization. Another approach to historical texts and social media is spelling normalization (e.g., Baron and Rayson, 2008; Han et al., 2012), which has been shown to offer improvements in tagging historical texts (Robertson and Goldwater, 2018). In Early Modern English, Yang and Eisenstein (2016) found that domain adaptation and normalization are complementary. In this paper, we have shown that domain-adaptive finetuning (and wordpiece segmentation) significantly improves the OOV tagging accuracy from FEMA, so future research must explore whether normalization is still necessary for state-of-the-art tagging of historical texts. Domain-specific pretraining. Given sufficient unlabeled data in the target domain, the simplest approach may be to retrain a BERT-like model from scratch. BioBERT is an application of BERT to the biomedical d"
D19-1433,W16-3919,0,0.149516,"Missing"
D19-1433,N15-1069,1,0.935914,"researchers have used part-of-speech tagging to identify syntactic changes (Degaetano-Ortlieb, 2018) and dependency parsing to quantify gender-based patterns of adjectival modification and possession in classic literary texts (Vuillemot et al., 2009; Muralidharan and Hearst, 2013). But despite the appeal of using NLP in historical linguistics and literary analysis, there is relatively little research on how performance is impacted by diachronic transfer. Indeed, the evidence that does exist suggests that accuracy degrades significantly, especially if steps are not taken to adapt: for example, Yang and Eisenstein (2015) compare the accuracy of tagging 18th century and 16th century Portuguese text (using a model trained on 19th century text), and find that the error rate is twice as high for the older text. 2 Tagging Historical Texts Before describing our modeling approach, we highlight some of the unique aspects of tagging historical texts, focusing on the target dataset of the Penn-Helsinki Corpus of Early Modern English (Kroch et al., 2004). 2.1 Early Modern English (EME) refers to the dominant language spoken in England during the period spanning the 15th-17th centuries, which includes the time of Shakesp"
D19-1433,N16-1157,1,0.890174,"get domain causes catastrophic forgetting (McCloskey and Cohen, 1989), with a significant deterioration in tagging performance in the source domain. Early Modern English If this marsch waulle (marsh wall) were not kept, and the canales of eche partes of Sowey river kept from abundance of wedes, al the plaine marsch ground at sodaine raynes (sudden rains) wold be overflowen, and the profite of the meade lost. While these differences are not too difficult for fluent human readers of English, they affect a large number of tokens, resulting in a substantial increase in the out-of-vocabulary rate (Yang and Eisenstein, 2016). Some of the spelling differences are purely typographical, such as the substitution of v for u in words like vnto, and the substitution of y for i in words like hym. These are common sources of errors for baseline models. Another source of out-of-vocabulary words is the addition of a silent e to the end of many words. This generally did not cause errors for wordpiece-based models (like BERT), perhaps because the final ‘e’ is segmented as a separate token, which does not receive a tag. Capitalization is also used inconsistently, making it difficult to distinguish proper and common nouns, as i"
D19-1433,N18-1112,0,0.0481801,"y (Arjovsky et al., 2017) unless the objectives are carefully balanced (Alam et al., 2018). In addition to supervised and unsupervised domain adaptation, there are “distantly supervised” methods that construct noisy target domain instances, e.g., by using a bilingual dictionary (Fang and Cohn, 2017). Normalization dictionaries exist for Early Modern English (Baron and Rayson, 2008) and social media (Han et al., 2012), but we leave their application to distant supervision for future work. Finally, language modeling objectives have previously been used for domain adaptation of text classifiers (Ziser and Reichart, 2018), but this prior work has focused on representation learning from scratch, rather than adaptation of a pretrained contextualized embedding model. Our work shows that models that are pretrained on large-scale data yield very strong performance even when applied out-of-domain, making them a natural starting point for further adaptation. Semi-supervised learning. Universal Language Model Fine-tuning (ULMFiT) also involves finetuning on a language modeling task on the target 4244 C O NLL WNUT System Unsupervised domain adaptation 1. Task-tuned BERT 2. AdaptaBERT 3. AdaptaBERT 4. AdaptaBERT Supervi"
D19-5506,P18-1163,0,0.0999062,"of characters, while Sakaguchi et al. (2017) use character-level recurrent neural networks combined with special representations for the first and last characters of each token. These models are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We conducted preliminary experiments with noise-invariant encoders, but obtained better results by adding noise at training time. A related idea is to optimize an adversarial objective, in which a discriminator tries to distinguish noised and clean examples from their encoded representations (Cheng et al., 2018). This improves performance on clean data, but it makes optimization unstable, which is a well-known defect of adversarial learning (Arjovsky et al., 2017). Cheng et al. (2018) do not evaluate on natural noise. Table 5: The performance of a machine translation model on the MTNT task. non-standard spellings inherent to the dataset. As shown in Table 5, noised training has minimal impact on performance. We did not exhaustively explore the space of possible noising strategies, and so these negative results should be taken only as a preliminary finding. Nonetheless, there are reasons to believe th"
D19-5506,P18-2006,0,0.0531483,"generally trained on clean data, without spelling errors. Yet many translation scenarios require robustness to such errors: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018), and interactive settings in which users must enter text on a mobile device. Systems trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; Belinkov and Bisk, 2018). One potential solution is to introduce noise at training time, similar in spirit to the use of adversarial examples (Goodfellow et al., 2014; Ebrahimi et al., 2018). So far, using synthetic noise at training time has been found to improve performance only on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; Belinkov and Bisk, 2018). We desire methods that perform well on both clean text and naturally-occurring noise, but this is beyond the current state of the art. ∗ 2 Noise Models We focus on orthographical noise; character-level noise that affects the spelling of individual terms. Orthographical noise is problematic for machine translation systems that operate"
D19-5506,N19-1190,0,0.0570227,"r, we find that deleting and inserting random characters play a key role in preparing the model for test-time typos. While our method works well on misspellings, it does not appear to generalize to non-standard text in social media. We conjecture that spelling mistakes constitute a small part of the deviations from standard text, and that the main challenges in this 1 Contemporaneous work shows that MTNT performance can be improved by a domain-specific noising distribution that includes character insertions and deletions, as well as the random insertion of emoticons, stopwords, and profanity (Vaibhav et al., 2019). The specific impact of spelling noise is not evaluated, nor is the impact on clean text. 45 domain stem from other linguistic phenomena. Lasse Holmstrom and Petri Koistinen. 1992. Using additive noise in back-propagation training. IEEE Transactions on Neural Networks, 3(1):24–38. Acknowledgments Thanks to the anonymous reviewers for their feedback. We also thank Luke Zettlemoyer and our colleagues at FAIR for valuable feedback. Specifically, we thank Abdelrahman Mohamed for sharing his expertise on nonautoregressive models. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016."
D19-5506,N13-1037,1,0.800142,"cter encoders to spelling errors across multiple input languages (German, French, and Czech). Of the different noise types we use at training, we find that random character deletions are particularly useful, followed by character insertions. However, noisy training does not improve translations of social media text, as indicated by performance on the MTNT dataset of Reddit posts (Michel and Neubig, 2018). This finding aligns with previous work arguing that the distinctive feature of social media text is not noise or orthographical errors, but rather, variation in writing style and vocabulary (Eisenstein, 2013). Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natura"
D19-5506,E12-1054,0,0.0614443,"Missing"
D19-5506,max-wisniewski-2010-mining,0,\N,Missing
J17-3003,D07-1093,0,0.0323834,"Missing"
J17-3003,E14-1011,0,0.0739954,"Missing"
J17-3003,D10-1124,1,0.814353,"Missing"
J17-3003,K15-1011,0,0.0302065,"Missing"
J17-3003,D12-1137,0,0.018534,"es do not perform hypothesis testing on geographical dependence. A common approach is to aggregate geotagged social media content into geographical bins. Some studies rely on politically defined units such as nations and states (Hovy, Johannsen, and Søgaard 2015); however, isoglosses (the geographical boundaries between linguistic features) need not align with politically defined geographical units (Nerbonne and Kretzschmar, Jr. 2013). Other approaches rely on automatically defined geographical units, induced by computational methods such as geodesic grids (Wing and Baldridge 2011), KD-trees (Roller et al. 2012), Gaussians (Eisenstein et al. 2010), and mixtures of Gaussians (Hong et al. 2012). While these approaches offer insights about the nature of geographical language variation, they do not provide test statistics that allow us to quantify the geographical dependence of various linguistic features. As described in the next section, our approach is based on Reproducing Kernel Hilbert Spaces, which enable us to nonparametrically compare probability distributions. Another way in which kernel methods can be applied to spatial analysis is in Gaussian Processes, which are often used to represent spatia"
J17-3003,W12-0210,0,0.38887,"w counts. A statistical metric for comparing the strength of geographical associations across potential linguistic variables would allow linguists to determine whether finite geographical samples—such as the one shown in Figure 1—reveal a statistically meaningful association. The use of statistical methods to analyze spatial dependence has been only lightly studied in sociolinguistics and dialectology. Existing approaches use classical statistics such as Moran’s I (e.g., Grieve, Speelman, and Geeraerts 2011), join count analysis (e.g., Lee and Kretzschmar Jr. 1993), and the Mantel Test (e.g., Scherrer 2012); we review these statistics in Section 2. These classical approaches suffer from a common problem: Each type of test can capture only a specific parametric form of spatial linguistic variation. As a result, these tests can incorrectly fail to reject the null hypothesis if the nature of the geolinguistic dependence does not match the underlying assumptions of the test. To address these limitations, we propose a new test statistic that builds on a rich and growing literature on kernel embeddings for nonparametric statistics (ShaweTaylor and Cristianini 2004). In these methods, probability distr"
J17-3003,W11-4112,0,0.0403308,"Missing"
J17-3003,P11-1096,0,0.0279758,"egions, although most of these studies do not perform hypothesis testing on geographical dependence. A common approach is to aggregate geotagged social media content into geographical bins. Some studies rely on politically defined units such as nations and states (Hovy, Johannsen, and Søgaard 2015); however, isoglosses (the geographical boundaries between linguistic features) need not align with politically defined geographical units (Nerbonne and Kretzschmar, Jr. 2013). Other approaches rely on automatically defined geographical units, induced by computational methods such as geodesic grids (Wing and Baldridge 2011), KD-trees (Roller et al. 2012), Gaussians (Eisenstein et al. 2010), and mixtures of Gaussians (Hong et al. 2012). While these approaches offer insights about the nature of geographical language variation, they do not provide test statistics that allow us to quantify the geographical dependence of various linguistic features. As described in the next section, our approach is based on Reproducing Kernel Hilbert Spaces, which enable us to nonparametrically compare probability distributions. Another way in which kernel methods can be applied to spatial analysis is in Gaussian Processes, which are"
J18-4007,W11-1701,0,0.0347011,"reasons. In addition, we find that these “stickiness patterns” are also sensitive to the subreddit in which they occur, which we take to be a proxy for a difference in community norms of stancetaking. Finally, we show that these patterns are connected to different uses of stance markers in conversation (Biber and Finegan 1989; Pavalanathan et al. 2017), enabling classification of stance dimensions from lexical features. These findings demonstrate that the concept of stance and 1 Interactional stancetaking is distinct from argumentative stances, a term used to characterize positions in debate (Anand et al. 2011). 684 Kiesling et al. Interactional Stancetaking in Online Forums stancetaking in conversations is a useful way to explore how interpersonal relationships are created in conversations, and moreover how stancetaking can be computationally extracted from such conversations. To summarize, the article makes the following contributions: • We introduce interactional stancetaking to the computational linguistics community, and operationalize it through a set of annotation guidelines (available in the Appendix). • We provide quantitative annotations of three stance dimensions on social media text, dem"
J18-4007,J17-1006,0,0.0599775,"Missing"
J18-4007,P13-1025,0,0.0269265,"social and cognitive phenomena. Several of LIWC’s word lists touch on the stancetaking dimensions identified in this article: affect and positive and negative emotion, certainty and tentativeness, and inclusion and social phenomena. It is an open question as to whether these phenomena are best understood by annotating individual examples, or by listing words and phrases in the abstract. Researcher intuitions about a word’s stancetaking properties may not always match reality: A classic counter-intuitive finding is that sentences starting with “please” are judged to be less polite on average (Danescu-Niculescu-Mizil et al. 2013); we observe a similar result with “please” indicating negative affect. Furthermore, the stancetaking properties of individual words and phrases are always shaped by the pragmatic context, including discourse and (especially in social media) extralinguistic factors (Benamara, Taboada, and Mathieu 2017). A further concern for lexicon-based methods is whether predefined word lists can possibly keep up with the variety and rapid change that predominate in online social contexts (Eisenstein 2013). Unsupervised Learning. A third computational approach to characterizing interpersonal meaning in lang"
J18-4007,N13-1037,1,0.800265,"is that sentences starting with “please” are judged to be less polite on average (Danescu-Niculescu-Mizil et al. 2013); we observe a similar result with “please” indicating negative affect. Furthermore, the stancetaking properties of individual words and phrases are always shaped by the pragmatic context, including discourse and (especially in social media) extralinguistic factors (Benamara, Taboada, and Mathieu 2017). A further concern for lexicon-based methods is whether predefined word lists can possibly keep up with the variety and rapid change that predominate in online social contexts (Eisenstein 2013). Unsupervised Learning. A third computational approach to characterizing interpersonal meaning in language has focused on the use of unsupervised techniques such as clustering, topic modeling, and matrix factorization (e.g., Schwartz et al. 2013). To ensure that the resulting latent dimensions are focused on interpersonal meaning rather than topic or genre differences, these approaches are often applied to restricted vocabularies, such as address terms (Krishnan and Eisenstein 2015) or stance markers (Pavalanathan et al. 2017). An advantage of unsupervised methods is that they can be applied"
J18-4007,N15-1185,1,0.906386,"Missing"
J18-4007,S16-1003,0,0.0198987,"and-Target Belief and Sentiment Evaluation,13 which unifies annotation for belief and sentiment toward entities within the text. However, the notion of stancetaking is different, again due to its inherent dialogic and interactional nature. Stance and Stancetaking. Stancetaking is also distinct from other notions of stance in computational linguistics. One such notion is argumentative stance, which is about the position or stance (pro vs. con) that a speaker/writer takes on an issue in a debate (Walker et al. 2012). A slightly different notion of stance is presented in the SemEval-2016 Task 6 (Mohammad et al. 2016) on detecting stance from tweets. This task defined stance detection as the task of automatically determining whether the author of the text is in favor of, against, or neutral toward a (pre-chosen) proposition or target entity. However, our notion of stancetaking is different from these. We consider stancetaking as a multidimensional construct indicating the relationship between the audience, topic, and talk itself; and we capture it through the dimensions of A FFECT, A LIGNMENT, and I NVESTMENT. 13 https://tac.nist.gov/2017/KBP/. 708 Kiesling et al. Interactional Stancetaking in Online Forum"
J18-4007,P17-1082,1,0.903576,"king, taking both qualitative and quantitative perspectives. First, we investigate patterns of stance “stickiness” through a conversation. We find that some types of stance tend to persist more through a conversation than in others, for both structural and interpersonal reasons. In addition, we find that these “stickiness patterns” are also sensitive to the subreddit in which they occur, which we take to be a proxy for a difference in community norms of stancetaking. Finally, we show that these patterns are connected to different uses of stance markers in conversation (Biber and Finegan 1989; Pavalanathan et al. 2017), enabling classification of stance dimensions from lexical features. These findings demonstrate that the concept of stance and 1 Interactional stancetaking is distinct from argumentative stances, a term used to characterize positions in debate (Anand et al. 2011). 684 Kiesling et al. Interactional Stancetaking in Online Forums stancetaking in conversations is a useful way to explore how interpersonal relationships are created in conversations, and moreover how stancetaking can be computationally extracted from such conversations. To summarize, the article makes the following contributions: •"
J18-4007,Q16-1005,0,0.0207663,"kes the distinction we suggest in the three dimensions. 6.2 Studies of Social Meaning in Computational Linguistics Social and interactional meaning has garnered increasing interest in the computational linguistics community in recent years. Linguistic and social constructs such as sentiment (Wiebe, Wilson, and Cardie 2005), subjectivity (Riloff and Wiebe 2003), opinion mining (Pang, Lee et al. 2008), factuality (Saur´ı and Pustejovsky 2009), belief (Prabhakaran, Rambow, and Diab 2010; Werner et al. 2015), politeness (DanescuNiculescu-Mizil et al. 2013), respect (Voigt et al. 2017), formality (Pavlick and Tetreault 2016), and power differences (Prabhakaran, Rambow, and Diab 2012) have been operationalized for computational investigation. Our operationalization of stancetaking using the dimensions of A FFECT, A LIGNMENT, and I NVESTMENT is related to, although not the same as, constructs such as sentiment, subjectivity, opinion mining, and argumentation. One important way in which the notion of stancetaking differs from constructs such as sentiment, subjectivity, and opinion is that these constructs are studied in the context of a single utterance by a speaker/writer, whereas our model of stancetaking is not b"
J18-4007,N12-1057,0,0.0760437,"Missing"
J18-4007,W03-1014,0,0.0189448,"how such stance accretion (in Bucholtz and Hall’s [2005] terminology) could take place. Overall, our analyses are a vindication of using stance as an explanatory concept in sociolinguistics generally, and specifically a systematic model of stancetaking that makes the distinction we suggest in the three dimensions. 6.2 Studies of Social Meaning in Computational Linguistics Social and interactional meaning has garnered increasing interest in the computational linguistics community in recent years. Linguistic and social constructs such as sentiment (Wiebe, Wilson, and Cardie 2005), subjectivity (Riloff and Wiebe 2003), opinion mining (Pang, Lee et al. 2008), factuality (Saur´ı and Pustejovsky 2009), belief (Prabhakaran, Rambow, and Diab 2010; Werner et al. 2015), politeness (DanescuNiculescu-Mizil et al. 2013), respect (Voigt et al. 2017), formality (Pavlick and Tetreault 2016), and power differences (Prabhakaran, Rambow, and Diab 2012) have been operationalized for computational investigation. Our operationalization of stancetaking using the dimensions of A FFECT, A LIGNMENT, and I NVESTMENT is related to, although not the same as, constructs such as sentiment, subjectivity, opinion mining, and argumentat"
J18-4007,W15-4625,0,0.0311668,"al comment arrival pattern, are found to determine member interactions such as re-entry to previously contributed discussions (Backstrom et al. 2013); length of the conversational thread is found to reveal interactivity among members (Whittaker et al. 1998); and depth and breadth of the threads are found to be the characteristics of the topic and authors of the discussions (Kumar, Mahdian, and McGlohon 2010). Conversational structure and meta-thread features are found to significantly improve agreement/disagreement detection in online debate forums, compared with using lexical features alone (Rosenthal and McKeown 2015). The conversational context in Twitter discussions is also found to be useful in sentiment classification (Ren et al. 2016). Recently, there has been work in modeling threaded conversations using neural networks, considering both the hierarchical structure and timing of comment arrival (Zayats and Ostendorf 2018) to predict the popularity of comments. In this work, we focus on another social phenomena, interactional stancetaking, and investigate how the thread structure interacts with various patterns of stancetaking utterances throughout threaded conversations. 7. Conclusion One of the bigge"
J18-4007,J11-2001,0,0.00594361,"ework of interactional stancetaking, but the terms of this framework are not likely to be known to the typical crowdworker. For this reason, we have focused on “expert” annotations, mainly from students trained by the authors, who performed the task over the course of a semester or more. The applicability of crowdsourcing to the annotation of interactional stancetaking is a question for future work. Lexicons. Another expert-driven perspective on social meaning is the use of carefully curated lexicons. Although there are many examples of this approach (e.g., Stone 1966; Biber and Finegan 1989; Taboada et al. 2011), the dominant example today is the Linguistic Inquiry and Word Count (LIWC) set of lexicons (Tausczik and Pennebaker 2010), designed by social psychologists to capture a broad range of social and cognitive phenomena. Several of LIWC’s word lists touch on the stancetaking dimensions identified in this article: affect and positive and negative emotion, certainty and tentativeness, and inclusion and social phenomena. It is an open question as to whether these phenomena are best understood by annotating individual examples, or by listing words and phrases in the abstract. Researcher intuitions ab"
J18-4007,N12-1072,0,0.0126071,"ty and difference with respect to these evaluations. Another relevant effort is the TAC 2017 Source-and-Target Belief and Sentiment Evaluation,13 which unifies annotation for belief and sentiment toward entities within the text. However, the notion of stancetaking is different, again due to its inherent dialogic and interactional nature. Stance and Stancetaking. Stancetaking is also distinct from other notions of stance in computational linguistics. One such notion is argumentative stance, which is about the position or stance (pro vs. con) that a speaker/writer takes on an issue in a debate (Walker et al. 2012). A slightly different notion of stance is presented in the SemEval-2016 Task 6 (Mohammad et al. 2016) on detecting stance from tweets. This task defined stance detection as the task of automatically determining whether the author of the text is in favor of, against, or neutral toward a (pre-chosen) proposition or target entity. However, our notion of stancetaking is different from these. We consider stancetaking as a multidimensional construct indicating the relationship between the audience, topic, and talk itself; and we capture it through the dimensions of A FFECT, A LIGNMENT, and I NVESTM"
J18-4007,W15-1304,0,0.0244704,"explanatory concept in sociolinguistics generally, and specifically a systematic model of stancetaking that makes the distinction we suggest in the three dimensions. 6.2 Studies of Social Meaning in Computational Linguistics Social and interactional meaning has garnered increasing interest in the computational linguistics community in recent years. Linguistic and social constructs such as sentiment (Wiebe, Wilson, and Cardie 2005), subjectivity (Riloff and Wiebe 2003), opinion mining (Pang, Lee et al. 2008), factuality (Saur´ı and Pustejovsky 2009), belief (Prabhakaran, Rambow, and Diab 2010; Werner et al. 2015), politeness (DanescuNiculescu-Mizil et al. 2013), respect (Voigt et al. 2017), formality (Pavlick and Tetreault 2016), and power differences (Prabhakaran, Rambow, and Diab 2012) have been operationalized for computational investigation. Our operationalization of stancetaking using the dimensions of A FFECT, A LIGNMENT, and I NVESTMENT is related to, although not the same as, constructs such as sentiment, subjectivity, opinion mining, and argumentation. One important way in which the notion of stancetaking differs from constructs such as sentiment, subjectivity, and opinion is that these const"
J18-4007,Q18-1009,0,0.0197693,"characteristics of the topic and authors of the discussions (Kumar, Mahdian, and McGlohon 2010). Conversational structure and meta-thread features are found to significantly improve agreement/disagreement detection in online debate forums, compared with using lexical features alone (Rosenthal and McKeown 2015). The conversational context in Twitter discussions is also found to be useful in sentiment classification (Ren et al. 2016). Recently, there has been work in modeling threaded conversations using neural networks, considering both the hierarchical structure and timing of comment arrival (Zayats and Ostendorf 2018) to predict the popularity of comments. In this work, we focus on another social phenomena, interactional stancetaking, and investigate how the thread structure interacts with various patterns of stancetaking utterances throughout threaded conversations. 7. Conclusion One of the biggest difficulties in studying aspects of interaction such as stancetaking is subjective variability: Speakers and listeners often disagree about whether a single utterance is rude, offensive, or any other interpretation. Nevertheless, discourse analysts have been able to find ways of explicating stance, as described"
N04-1004,W98-1119,0,0.0256101,"or number with the referential pronoun. Constraints may be used in combination with a salience metric, to prune away unlikely choices before searching. The advantage is that enforcing constraints could be substantially less computationally expensive than searching through the space of all possible bindings for the one with the highest salience. One possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effect of these constraints on both accuracy and speed. Ge, Hale, and Charniak propose a data-driven approach to anaphora resolution (Ge et al., 1998). For a given pronoun, their system can compute a probability for each candidate antecedent. Their approach of seeking to maximize this probability is similar to the saliencemaximizing approach that we have described. However, instead of using a parametric salience function, they learn a set of conditional probability distributions directly from the data. If this approach could be applied to gesturespeech alignment, it would be advantageous because the binding probabilities could be combined with the output of probabilistic recognizers to produce a pipeline architecture, similar to that propos"
N04-1004,C00-1054,0,0.19774,"modal User Interfaces Discussion of multimodal user interfaces begins with the seminal “Put-That-There” system (Bolt, 1980), which allowed users to issue natural language commands and use deictic hand gestures to resolve references from speech. Commands were subject to a strict grammar and alignment was straightforward: keywords created holes in the semantic frame, and temporally-aligned gestures filled the holes. More recent systems have extended this approach somewhat. Johnston and Bangalore describe a multimodal parsing algorithm that is built using a 3-tape, finite state transducer (FST) (Johnston and Bangalore, 2000). The speech and gestures of each multimodal utterance are provided as input to an FST whose output is a semantic representation conveying the combined meaning. A similar system, based on a graph-matching algorithm, is described in (Chai et al., 2004). These systems perform mutual disambiguation, where each modality helps to correct errors in the others. However, both approaches restrict users to a predefined grammar and lexicon, and rely heavily on having a complete, formal ontology of the domain. In (Kettebekov et al., 2002), a co-occurrence model relates the salient prosodic features of the"
N04-1004,J94-4002,0,0.0642552,"words. Potentially, the two approaches could compliment each other in a unified system. 2.2 Anaphora Resolution Anaphora resolution involves linking an anaphor to its corresponding antecedent in the same or previous sentence. In many cases, speech/gesture multimodal fusion works in a very similar way, with gestures grounding some of the same anaphoric pronouns (e.g., “this”, “that”, “here”). One approach to anaphora resolution is to assign a salience value to each noun phrase that is a candidate for acting as a grounding referent, and then to choose the noun phrase with the greatest salience (Lappin and Leass, 1994). Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998). Salience values are typically computed by applying linguistic knowledge; e.g., recent noun phrases are more salient, gender and number should agree, etc. This knowledge is applied to derive a salience value through the application of a set of predefined salience weights on each feature. Salience weights may be defined by hand, as in (Lappin and Leass, 1994), or learned from data (Mitkov et al., 2002). Anaphora resolution and gesture-speech"
N04-1004,P98-2143,0,0.016171,"ts corresponding antecedent in the same or previous sentence. In many cases, speech/gesture multimodal fusion works in a very similar way, with gestures grounding some of the same anaphoric pronouns (e.g., “this”, “that”, “here”). One approach to anaphora resolution is to assign a salience value to each noun phrase that is a candidate for acting as a grounding referent, and then to choose the noun phrase with the greatest salience (Lappin and Leass, 1994). Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998). Salience values are typically computed by applying linguistic knowledge; e.g., recent noun phrases are more salient, gender and number should agree, etc. This knowledge is applied to derive a salience value through the application of a set of predefined salience weights on each feature. Salience weights may be defined by hand, as in (Lappin and Leass, 1994), or learned from data (Mitkov et al., 2002). Anaphora resolution and gesture-speech alignment are very similar problems. Both involve resolving ambiguous words which reference other parts of the utterance. In the case of anaphora resoluti"
N04-1004,W97-1401,0,0.306124,"ematic when considering that the output of speech recognizers is far from perfect. Any approach that requires significant parsing or other grammatical analysis may be ill-suited to meet these goals. Instead, we identify keywords that are likely to require gestural referents for resolution. Our goal is to produce an alignment – a set of bindings – that match at least some of the identified keywords with one or more gestures. There are several things that are known to contribute to the salience of candidate gesture-speech bindings: • The relevant gesture is usually close in time to the keyword (Oviatt et al., 1997; Cohen et al., 2002) • The gesture usually precedes the keyword (Oviatt et al., 1997). • A one-to-one mapping is preferred. Multiple keywords rarely align with a single gesture, and multiple gestures almost never align with a single keyword (Eisenstein and Davis, 2003). • Some types of gestures, such as deictic pointing gestures, are more likely to take part in keyword bindings. Other gestures (i.e., beats) do not carry this type of semantic content, and instead act to moderate turn taking or indicate emphasis. These gestures are unlikely to take part in keyword bindings (Cassell, 1998). • So"
N04-1004,A88-1003,0,0.0603531,"se 2), a deictic gesture is usually crucial for understanding the sentence. Thus, the penalty for not assigning this keyword should be very high. Finally, in the third case, when the keyword follows a preposition, a trajectory gesture is more likely, and the penalty for any such binding should be lowered. 7.3 Other Anaphora Resolution Techniques We have based this research on salience values, which is just one of several possible alternative approaches to anaphora resolution. One such alternative is the use of constraints: rules that eliminate candidates from the list of possible antecedents (Rich and Luperfoy, 1988). An example of a constraint in anaphora resolution is a rule requiring the elimination of all candidates that disagree in gender or number with the referential pronoun. Constraints may be used in combination with a salience metric, to prune away unlikely choices before searching. The advantage is that enforcing constraints could be substantially less computationally expensive than searching through the space of all possible bindings for the one with the highest salience. One possible future project would be to develop a set of constraints for speech-gesture alignment, and investigate the effe"
N04-1004,C98-2138,0,\N,Missing
N06-2010,W02-1001,0,0.0120788,"an features φ(xi , xj ) induced from the pair of noun phrases: s(xi , xj ) = λφ(xi , xj ). The maximum likelihood weights can be approximated by a voted perceptron, where, in the iteration t of the perceptron training: X ∗ λt = λt−1 + φ(xi , xj )(yi,j − yˆi,j ) (2) i,j,i6=j In equation 2, y∗ is the ground truth partitioning from ˆ is the partitioning that maximizes the labeled data. y equation 1 given the set of weights λt−1 . As before, average-link clustering with an adaptive cutoff is used to partition the graph. The weights are then averaged across all iterations of the perceptron, as in (Collins, 2002). 3 Evaluation The results of our experiments are computed using mention-based CEAF scoring (Luo, 2005), and are reported in Table 2. Leave-one-out evaluation was used to form 16 cross-validation folds, one for each document in the corpus. Using a planned, one-tailed pairwise t-test, the gesture features improved performance significantly MARKABLE DIST EXACT MATCH STR MATCH NONPRO MATCH NUMBER MATCH PRONOUN DEF NP DEM NP INDEF NP pronouns FOCUS DIST WHICH HAND The number of markables between the candidate NPs True if the candidate NPs have identical surface forms True if the candidate NPs matc"
N06-2010,H05-1004,0,0.0186594,"lihood weights can be approximated by a voted perceptron, where, in the iteration t of the perceptron training: X ∗ λt = λt−1 + φ(xi , xj )(yi,j − yˆi,j ) (2) i,j,i6=j In equation 2, y∗ is the ground truth partitioning from ˆ is the partitioning that maximizes the labeled data. y equation 1 given the set of weights λt−1 . As before, average-link clustering with an adaptive cutoff is used to partition the graph. The weights are then averaged across all iterations of the perceptron, as in (Collins, 2002). 3 Evaluation The results of our experiments are computed using mention-based CEAF scoring (Luo, 2005), and are reported in Table 2. Leave-one-out evaluation was used to form 16 cross-validation folds, one for each document in the corpus. Using a planned, one-tailed pairwise t-test, the gesture features improved performance significantly MARKABLE DIST EXACT MATCH STR MATCH NONPRO MATCH NUMBER MATCH PRONOUN DEF NP DEM NP INDEF NP pronouns FOCUS DIST WHICH HAND The number of markables between the candidate NPs True if the candidate NPs have identical surface forms True if the candidate NPs match after removing articles True if the candidate NPs are not pronouns and have identical surface forms T"
N06-2010,P03-1070,0,0.207706,"Missing"
N06-2010,P03-1022,0,0.307398,"Missing"
N06-2010,W98-1501,0,\N,Missing
N06-3002,P01-1016,0,0.0184004,"he NLP community relates to multimodal dialogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the poss"
N06-3002,N06-2010,1,0.809968,"ked marker, c) no presentational aids were allowed. The first condition was designed to be relevant to presentations involving pre-created presentation materials, such as Powerpoint slides. The second condition was intended to be similar to classroom lectures or design presentations. The third condition was aimed more at direct one-on-one interaction. My preliminary work has involved data from the first condition, in which speakers gestured at preprinted diagrams. An empirical study on this part of the corpus has identified several gesture features that are relevant to coreference resolution (Eisenstein and Davis, 2006a). In particular, gesture similarity can be measured by hand position and the choice of the hand which makes the gesture; these similarities correlate with the likelihood of coreference. In addition, the likelihood of a gestural hold – where the hand rests in place for a period of time – acts as a meta-feature, indicating that gestural cues are likely to be particularly important to disambiguate the meaning of the associated noun phrase. In (Eisenstein and Davis, 2006b), these features are combined with traditional textual features for coreference resolution, with encouraging results. The han"
N06-3002,C00-1054,0,0.0292926,"gmatic function, if there are characteristic gestures that indicate restarts or other repairs. In one case, we are looking for a similarity between the disfluency and the repair point; in the other case, we are looking for similarities across all disfluencies, or across all repair points. It is hoped that this research will not only improve processing of spoken natural language, but also enhance our understanding of how speakers use gesture to structure their discourse. 217 4 Related Work The bulk of research on multimodality in the NLP community relates to multimodal dialogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in"
N06-3002,N04-1018,0,0.0319391,"ed by gesture features; Cassell et al. (2001) make a similar argument using body posture. (Nakano et al., 2003) describes how head gestures and eye gaze relate to turn taking and dialogue grounding. All of the studies listed in this paragraph identify relevant correlations between non-verbal communication and linguistic phenomena, but none construct a predictive system that uses the non-verbal modalities to improve performance beyond a text-only system. Prosody has been shown to improve performance on several NLP problems, such as topic and sentence segmentation (e.g., (Shriberg et al., 2000; Kim et al., 2004)). The prosody literature demonstrates that non-verbal features can improve performance on a wide variety of NLP tasks. However, it also warns that performance is often quite sensitive, both to the representation of prosodic features, and how they are integrated with other linguistic features. The literature on prosody would suggest parallels for gesture features, but little such work has been reported. (Chen et al., 2004) shows that gesture may improve sentence segmentation; however, in this study, the improvement afforded by gesture is not statistically significant, and evaluation was perfor"
N06-3002,P03-1070,0,0.0262208,"alogue systems (e.g., (Johnston and Bangalore, 2000)). This research differs fundamentally from mine in that it addresses human-computer interaction, whereas I am studying human-human interaction. Multimodal dialogue systems tackle many interesting challenges, but the grammar, vocabulary, and recognized gestures are often pre-specified, and dialogue is controlled at least in part by the computer. In my data, all of these things are unconstrained. Another important area of research is the generation of multimodal communication in animated agents (e.g., (Cassell et al., 2001; Kopp et al., 2006; Nakano et al., 2003)). While the models developed in these papers are interesting and often wellmotivated by the psychological literature, it remains to be seen whether they are both broad and precise enough to apply to gesture recognition. There is a substantial body of empirical work describing relationships between non-verbal and linguistic phenomena, much of which suggests that gesture could be used to improve the detection of such phenomena. (Quek et al., 2002) describe examples in which gesture correlates with topic shifts in the discourse structure, raising the possibility that topic segmentation and summa"
N09-1010,P07-1092,0,0.0144888,"Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these models train bilingual models separately, and then use their output to select a final translation. The selection criterion can be learned at training time since these models have access to the correct translation. In unsupervised settings, however, we do not have a principled means for selecting among outputs of different bilingual models. By developing a joint multilingual model we can automatically achieve performance that rivals that of t"
N09-1010,erjavec-2004-multext,0,0.0360144,"tion with mean set to the previous value, and variance to one-tenth of the mean. 4 Experimental Setup We test our model in an unsupervised framework where only raw parallel text is available for each of the languages. In addition, we assume that for each language a tag dictionary is available that covers some subset of words in the text. The task is to learn an independent tagger for each language that can annotate non-parallel raw text using the learned parameters. All reported results are on non-parallel monolingual test data. Data For our experiments we use the MultextEast parallel corpus (Erjavec, 2004) which has been used before for multilingual learning (Feldman et al., 2006; Snyder et al., 2008). The tagged portion of the corpus includes a 100,000 word English text, Orwell’s novel “Nineteen Eighty Four”, and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene and Serbian. The corpus also includes a tag lexicon for each of these languages. We use the first 3/4 of the text for learning and the last 1/4 as held-out non-parallel test data. The corpus provides sentence level alignments. To obtain word level alignments, we run GIZA ++ (Och and Ney, 200"
N09-1010,feldman-etal-2006-cross,0,0.166213,"monolingual performance is cut by nearly two thirds. We also examined scenarios where the tag lexicon is reduced in size. In all cases, the multilingual model yielded substantial performance gains. Finally, we examined the performance of our model when trained on all possible subsets of the eight languages. We found that performance improves steadily as the number of available languages increases. 2 Related Work Bilingual Part-of-Speech Tagging Early work on multilingual tagging focused on projecting annotations from an annotated source language to a target language (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, we assume no labeled data at all; our unsupervised model instead symmetrically improves performance for all languages by learning cross-lingual patterns in raw parallel data. An additional distinction is that projection-based work utilizes pairs of languages, while our approach allows for continuous improvement as languages are added to the mix. In recent work, Snyder et al. (2008) presented a model for unsupervised part-of-speech tagging trained from a bilingual parallel corpus. This bilingual model and the model presented here share a number of similarities: both are Bayesian"
N09-1010,H05-1110,0,0.0684736,"exponentially in the number of languages. In addition, crossing alignments must be removed so that the resulting graph structure remains acyclic. In contrast, our multilingual model posits latent cross-lingual tags without explicitly joining or directly connecting the part-of-speech tags across languages. Besides permitting crossing alignments, this structure allows the model to scale gracefully with the number of lan84 guages. Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these mode"
N09-1010,P07-1094,0,0.293151,"current tag given the previous tag and superlingual tags, and (ii) the next tag given the current tag and superlingual tags. These two quantities are similar to Distribution 1, except here we integrate over the transition parameter φyi−1 and the superlingual tag parameters ωzℓ . We end up with a product of integrals. Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and superlingual parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). For example, the closed form for integrating over the parameter of a superlingual tag with value z is given by: Z count(z, yi , ℓ) + ω0 ωzℓ (yi )P (ωzℓ |ω0 )dωzℓ = count(z, ℓ) + T ℓ ω0 where count(z, yi , ℓ) is the number of times that tag yi is observed together with superlingual tag z in language ℓ, count(z, ℓ) is the total number of times that superlingual tag z appears with an edge into language ℓ, and ω0 is a hyperparameter. The third term in the sampling formula is the emission probability of the current word xℓi given the current tag and all other words and sampled tags, as well as a"
N09-1010,2001.mtsummit-papers.46,0,0.0554812,"e gracefully with the number of lan84 guages. Beyond Bilingual Learning While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. His model first induces bilingual models for each pair of languages and then combines them. Our work takes a different approach by simultaneously learning from all languages, rather than combining bilingual results. A related thread of research is multi-source machine translation (Och and Ney, 2001; Utiyama and Isahara, 2006; Cohn and Lapata, 2007) where the goal is to translate from multiple source languages to a single target language. Rather than jointly training all the languages together, these models train bilingual models separately, and then use their output to select a final translation. The selection criterion can be learned at training time since these models have access to the correct translation. In unsupervised settings, however, we do not have a principled means for selecting among outputs of different bilingual models. By developing a joint multilingual model we can auto"
N09-1010,J03-1002,0,0.002086,"(Erjavec, 2004) which has been used before for multilingual learning (Feldman et al., 2006; Snyder et al., 2008). The tagged portion of the corpus includes a 100,000 word English text, Orwell’s novel “Nineteen Eighty Four”, and its translation into seven languages: Bulgarian, Czech, Estonian, Hungarian, Romanian, Slovene and Serbian. The corpus also includes a tag lexicon for each of these languages. We use the first 3/4 of the text for learning and the last 1/4 as held-out non-parallel test data. The corpus provides sentence level alignments. To obtain word level alignments, we run GIZA ++ (Och and Ney, 2003) on all 28 pairings of the 8 languages. Since we want each latent superlingual variable to span as many languages as possible, we aggregate the pairwise lexical alignments into larger sets of aligned words. These sets of aligned words are generated as a preprocessing step. During sampling they remain fixed and are treated as observed data. We use the set of 14 basic part-of-speech tags provided by the corpus. In our first experiment, we assume that a complete tag lexicon is available, so that for each word, its set of possible parts-of-speech is known ahead of time. In this setting, the averag"
N09-1010,D08-1109,1,0.44881,"at scales well and shows improved performance for individual languages as the total number of languages increases. Languages exhibit ambiguity at multiple levels, making unsupervised induction of their underlying structure a difficult task. However, sources of linguistic ambiguity vary across languages. For example, the word fish in English can be used as either a verb or a noun. In French, however, the noun poisson (fish) is entirely distinct from the verbal form pˆecher (to fish). Previous work has leveraged this idea by building models for unsupervised learning from aligned bilingual data (Snyder et al., 2008). However, aligned data is often available for many languages. The benefits of bilingual learning vary markedly depending on which pair of languages is selected, and without labeled data it is unclear how to determine which supplementary language is most helpful. In this paper, we show that it is possible to leverage all aligned languages simultaneously, achieving accuracy that in most cases outperforms even optimally chosen bilingual pairings. Even in expressing the same meaning, languages take different syntactic routes, leading to variation in part-of-speech sequences. Therefore, an effecti"
N09-1010,N01-1026,0,0.131032,"supervised and supervised monolingual performance is cut by nearly two thirds. We also examined scenarios where the tag lexicon is reduced in size. In all cases, the multilingual model yielded substantial performance gains. Finally, we examined the performance of our model when trained on all possible subsets of the eight languages. We found that performance improves steadily as the number of available languages increases. 2 Related Work Bilingual Part-of-Speech Tagging Early work on multilingual tagging focused on projecting annotations from an annotated source language to a target language (Yarowsky and Ngai, 2001; Feldman et al., 2006). In contrast, we assume no labeled data at all; our unsupervised model instead symmetrically improves performance for all languages by learning cross-lingual patterns in raw parallel data. An additional distinction is that projection-based work utilizes pairs of languages, while our approach allows for continuous improvement as languages are added to the mix. In recent work, Snyder et al. (2008) presented a model for unsupervised part-of-speech tagging trained from a bilingual parallel corpus. This bilingual model and the model presented here share a number of similarit"
N09-1010,N07-1061,0,\N,Missing
N09-1040,D08-1035,1,0.800677,"expansion program is underway to extend existing lines into the outer neighborhoods. Track length is expected to reach 89 km... Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. 353 The two sections are both part of a high-level segment on transportation. Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transportation section; words in italics occur throughout the high-l"
N09-1040,P08-1095,0,0.014603,"must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic framework. We note two orthogonal but related approaches to extracting nonlinear discourse structures from text. Rhetorical structure theory posits a hierarchical structure of discourse relations between spans of text (Mann and Thompson, 1988). This structure is richer than hierarchical topic segmentation, and the base level of analysis is typically more fine-grained – at the level of individual clauses. Unsupervised approaches based purely on cohesion are unlikely to succeed at this level of granularity. Elsner and Charniak (2008) propose the task of conversation disentanglement from internet chatroom logs. Unlike hierarchical topic segmentation, conversational threads may be disjoint, with unrelated threads interposed between two utterances from the same thread. Elsner and Charniak present a supervised approach to this problem, but the development of cohesion-based unsupervised methods is an interesting possibility for future work. 3 Model Topic modeling is premised on a generative framework in which each word wt is drawn from a multinomial θyt , where yt is a hidden topic indexing the language model that generates wt"
N09-1040,J86-3001,0,0.38184,"m unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. 353 The two sections are both part of a high-level segment on transportation. Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transportation section; words in italics occur throughout the high-level section, but not elsewhere in the article. This paper shows how multi-scale cohesion can be captured in a Bayesian generative model and exploited for unsupervised hiera"
N09-1040,P94-1002,0,0.0821798,"es, 74 stations, and 52.3 km of track. An expansion program is underway to extend existing lines into the outer neighborhoods. Track length is expected to reach 89 km... Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. 353 The two sections are both part of a high-level segment on transportation. Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transport"
N09-1040,P98-2244,0,0.0370833,". A simpler inference procedure would be a greedy approach that makes a fixed decision about the toplevel segmentation, and then applies recursion to achieve segmentation at the lower levels. The greedy approach will not be optimal if the best top-level segmentation leads to unsatisfactory results at the lower levels, or if the lower levels could help to disambiguate high-level segmentation. In contrast, the algorithm presented here maximizes the overall score across all levels of the segmentation hierarchy. 1 The use of dynamic programming for linear topic segmentation goes back at least to (Heinonen, 1998); however, we are aware of no prior work on dynamic programming for hierarchical segmentation. 357 Scale-level marginals The hidden variable zt represents the level of the segmentation hierarchy from which the word wt is drawn. Given language models Θ, each wt can be thought of as a draw from a Bayesian mixture model, with zt as the index of the component that generates wt . However, as we are marginalizing the language models, standard mixture model inference techniques do not apply. One possible solution would be to instantiate the maximum a posteriori language models after segmenting, but w"
N09-1040,E06-1035,0,0.0538712,") extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. All of these papers consider only linear topic segmentation; we introduce multi-scale lexical cohesion, which posits that the distribution of some 354 words changes slowly with high-level topics, while others change rapidly with lower-level subtopics. This gives a principled mechanism to model hierarchical topic segmentation. The literature on hierarchical topic segmentation is relatively sparse. Hsueh et al. (2006) describe a supervised approach that trains separate classifiers for topic and sub-topic segmentation; more relevant for the current work is the unsupervised method of Yaari (1997). As in T EXT T ILING, cohesion is measured using cosine similarity, and agglomerative clustering is used to induce a dendrogram over paragraphs; the dendrogram is transformed into a hierarchical segmentation using a heuristic algorithm. Such heuristic approaches are typically brittle, as they include a number of parameters that must be hand-tuned. These problems can be avoided by working in a Bayesian probabilistic"
N09-1040,P06-1004,0,0.537572,"f metrics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. This eval2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. 359 uation uses source code provided by Malioutov and Barzilay (2006). Experimental system The joint hierarchical Bayesian model described in this paper is called H IER BAYES. It performs a three-level hierarchical segmentation, in which the lowest level is for subchapter sections, the middle level is for chapters, and the top level spans the entire part. This top-level has the effect of limiting the influence of words that are common throughout the document. Baseline systems As noted in Section 2, there is little related work on unsupervised hierarchical segmentation. However, a straightforward baseline is a greedy approach: first segment at the top level, and"
N09-1040,J02-1002,0,0.653058,"sections, given gold standard chapter boundaries. Practical applications of topic segmentation typically relate to more informal documents such as blogs or speech transcripts (Hsueh et al., 2006), as formal texts such as books already contain segmentation markings provided by the author. The premise of this evaluation is that textbook corpora provide a reasonable proxy for performance on less structured data. However, further clarification of this point is an important direction for future research. Metrics All experiments are evaluated in terms of the commonly-used Pk and WindowDiff metrics (Pevzner and Hearst, 2002). Both metrics pass a window through the document, and assess whether the sentences on the edges of the window are properly segmented with respect to each other. WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not. This eval2 The full text of this book is available for free download at http://onlinebooks.library.upenn.edu. 359 uation uses source code provided by Malioutov and Barzilay (2006). Exper"
N09-1040,P06-1003,0,0.0871097,"Missing"
N09-1040,P01-1064,0,0.547419,"s, and 52.3 km of track. An expansion program is underway to extend existing lines into the outer neighborhoods. Track length is expected to reach 89 km... Introduction Recovering structural organization from unformatted texts or transcripts is a fundamental problem in natural language processing, with applications to classroom lectures, meeting transcripts, and chatroom logs. In the unsupervised setting, a variety of successful systems have leveraged lexical cohesion (Halliday and Hasan, 1976) – the idea that topically-coherent segments display consistent lexical distributions (Hearst, 1994; Utiyama and Isahara, 2001; Eisenstein and Barzilay, 2008). However, such systems almost invariably focus on linear segmentation, while it is widely believed that discourse displays a hierarchical structure (Grosz and Sidner, 1986). This paper introduces the concept of multi-scale lexical cohesion, and leverages this idea in a Bayesian generative model for hierarchical topic segmentation. 353 The two sections are both part of a high-level segment on transportation. Words in bold are characteristic of the subsections (buses and trains, respectively), and do not occur elsewhere in the transportation section; words in ita"
N09-1040,P03-1071,0,\N,Missing
N09-1040,C98-2239,0,\N,Missing
N13-1037,P11-1040,0,0.0111484,"ent research, at the expense of email and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP resear"
N13-1037,W12-2108,0,0.0124605,"Missing"
N13-1037,D11-1052,0,0.0465474,"2001), but they can also be seen as playing a pragmatic function: marking an utterance as facetious, or demonstrating a non-confrontational, less invested stance (Dresner and Herring, 2010). In many cases, phrasal abbreviations like lol (laugh out loud), lmao (laughing my ass off ), smh (shake my head), and ikr (i know, right?) play a similar role: yea she dnt like me lol; lmao I’m playin son. A key difference from emoticons is that abbreviations can act as constituents, as in smh at your ignorance. Another form of non-standard language is expressive lengthening (e.g., coooolllllll), found by Brody and Diakopoulos (2011) to indicate subjectivity and sentiment. In running dialogues — such as in online multiplayer games — the symbols * and ˆ can play an explicit pragmatic function (Collister, 2011; Collister, 2012). 2.5 Social variables A series of papers has documented the interactions between social media text and social variables such as age (Burger and Henderson, 2006; Argamon et al., 2007; Rosenthal and McKeown, 2011), gender (Burger et al., 2011; Rao et al., 2010), race (Eisenstein et al., 2011), and location (Eisenstein et al., 2010; Wing and Baldridge, 2011). From this literature, it is clear that many"
N13-1037,D11-1120,0,0.0184515,"Missing"
N13-1037,W09-2010,0,0.0123425,"ortance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace nonstandard words with “the contextually appropriate word or sequence of"
N13-1037,W12-0601,0,0.0206579,"lf-training on unlabeled social media text (Foster et al., 2011) • distributional features to address the sparsity of bag-of-words features (Gimpel et al., 2011; Owoputi et al., 2013; Ritter et al., 2011) • joint normalization, incorporated directly into downstream application (Liu et al., 2012) • distant supervision, using named entity ontologies and topic models (Ritter et al., 2011) Only a few of these techniques (normalization and new annotation systems) are specific to social media; the rest can found in other domain adaptation settings. Is domain adaptation appropriate for social media? Darling et al. (2012) argue that social media is not a coherent domain at all, and that a POS tagger for Twitter will not necessarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely varying expectations for the degree of standardness and dimensions of variation (Androutsopoulos, 2011). I am the co-author on a paper entitled “Part-of-speech tagging for Twitter,” but if we take this title literally, it is impossible on a trivial level: Twitter contains text in dozens or hundreds of languages, including man"
N13-1037,D10-1124,1,0.27262,"Missing"
N13-1037,P11-1137,1,0.378803,"Missing"
N13-1037,W13-1102,1,0.743893,"ich to express multiple types of meaning. The fact many such neologisms are closely circumscribed in geography and demographics may reflect diffusion through social networks that are assortative on exactly these dimensions (Backstrom et al., 2010; Thelwall, 2009). But an additional consideration is that non-standard language is deliberately deployed in the performance of identity work and stancetaking. This seems a particularly salient explanation for the use of lexical variables that originate in spoken language (jawn, hella), and for the orthographic transcription of phonological variation (Eisenstein, 2013). Determining the role and relative importance of social network diffusion and identity work as factors in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normaliz"
N13-1037,W10-0713,0,0.116502,"08) review traces of instant messaging conversations among students, arguing that they “pick and choose ... from the entire stylistic repertoire of the language” in a way that would be impossible without skilled command of both formal and informal registers. While news text is usually more carefully composed and edited than much of the language in social media, there is little evidence that bad language results from an inability to speak anything else. 2.2 Length limits In the case of Twitter, the limit of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also s"
N13-1037,P05-1045,0,0.00840592,"Missing"
N13-1037,I11-1100,0,0.154226,"Missing"
N13-1037,P11-2008,1,0.142679,"Missing"
N13-1037,W11-2210,0,0.0435748,"of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also spellings for standard words, manual examination of one hundred randomly selected examples for each surface form revealed only one standard your you’re with going know about length 85.1 ± 0.4 90.0 ± 0.1 87.9 ± 0.3 82.7 ± 0.5 86.1 ± 0.4 88.9 ± 0.4 alternative length ur 81.9 ± 0.6 wit goin kno bout 78.8 ± 0.7 72.2 ± 1.0 78.4 ± 1.0 74.5 ± 0.7 Table 1: Average length of messages containing standard forms and their shortenings case in which the standard meaning was intended for wit, and none for bout. The ave"
N13-1037,W11-0704,0,0.135957,"of 140 characters for each message is frequently cited as an explanation for bad language (Finin et al., 2010). Does Twitter’s character limit cause users to prefer shorter words, such as u instead of you? If so, one might expect shortening to be used most frequently in messages that are near the 140-character limit. Using a dataset of one million English-language tweets (Bamman et al., 2012), I have computed the average length of messages containing both standard words and their non-standard alternatives, focusing on the top five non-standard shortenings identified by the automatic method of Gouws et al. (2011a). The shortening ur can substitute for both your and you’re. While wit and bout are also spellings for standard words, manual examination of one hundred randomly selected examples for each surface form revealed only one standard your you’re with going know about length 85.1 ± 0.4 90.0 ± 0.1 87.9 ± 0.3 82.7 ± 0.5 86.1 ± 0.4 88.9 ± 0.4 alternative length ur 81.9 ± 0.6 wit goin kno bout 78.8 ± 0.7 72.2 ± 1.0 78.4 ± 1.0 74.5 ± 0.7 Table 1: Average length of messages containing standard forms and their shortenings case in which the standard meaning was intended for wit, and none for bout. The ave"
N13-1037,P11-1038,0,0.182518,"in the diversification of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace nonstandard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewri"
N13-1037,D12-1039,0,0.0770519,"of social media language is an exciting direction for future research. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace nonstandard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto i"
N13-1037,P11-2013,0,0.063217,"esearch. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace nonstandard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to find ca"
N13-1037,P11-1037,0,0.050657,"esearch. 3 What can we do about it? Having surveyed the landscape of bad language and its possible causes, let us now turn to the responses offered by the language technology research community. 3.1 Normalization One approach to dealing with bad language is to turn it good: “normalizing” social media or SMS messages to better conform to the sort of language that our technology expects. Approaches to normalization include the noisy-channel model (Cook and Stevenson, 2009), string and distributional similarity (Han and Baldwin, 2011; Han et al., 2012), sequence labeling (Choudhury et al., 2007; Liu et al., 2011a), and machine translation (Aw et al., 2006). As this task has been the focus of substantial attention in recent years, labeled datasets have become available and accuracies have climbed. That said, it is surprisingly difficult to find a precise definition of the normalization task. Writing before social media was a significant focus for NLP, Sproat et al. (2001) proposed to replace nonstandard words with “the contextually appropriate word or sequence of words.” In some cases, this seems clear enough: we can rewrite dats why pluto is pluto with that’s why... But it is not difficult to find ca"
N13-1037,P12-1055,0,0.00971365,"e or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled social media text (Foster et al., 2011) • distributional features to address the sparsity of bag-of-words features (Gimpel et al., 2011; Owoputi et al., 2013; Ritter et al., 2011) • joint normalization, incorporated directly into downstream application (Liu et al., 2012) • distant supervision, using named entity ontologies and topic models (Ritter et al., 2011) Only a few of these techniques (normalization and new annotation systems) are specific to social media; the rest can found in other domain adaptation settings. Is domain adaptation appropriate for social media? Darling et al. (2012) argue that social media is not a coherent domain at all, and that a POS tagger for Twitter will not necessarily generalize to other social media. One can go further: Twitter itself is not a unified genre, it is composed of many different styles and registers, with widely va"
N13-1037,N10-1004,0,0.0100487,"rs ranging from hashtag wordplay (Naaman et al., 2011) to the official pronouncements of the British Monarchy. And even if all good language is alike, bad language can be bad in many different ways — as Androutsopoulos (2011) notes when contrasting the types of variation encountered when “visiting a gamer forum” versus “joining the Twitter profile of a rap star.” bigrams. While there are many techniques for comparing word distributions, I apply the relatively simple method of counting out-of-vocabulary (OOV) bigrams. The relationship between OOV rate and domain adaptation has been explored by McClosky et al. (2010), who use it as a feature to predict how well a parser will perform when applied across domains.2 Specifically, the datasets A and B are compared by counting the number of bigram tokens in A that are unseen in B. The following corpora are compared: • Twitter-month: randomly selected tweets from each month between January 2010 to October 2012 (Eisenstein et al., 2012). • Twitter-hour: randomly selected tweets from each hour of the day, randomly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not includ"
N13-1037,N13-1039,0,0.154196,"Missing"
N13-1037,N10-1020,0,0.0158985,"graphic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled social"
N13-1037,D11-1141,0,0.0480532,"ne. Another potential benefit of this research is to better understand the underlying orthographic processes that lead to the diversity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) • new annotation schemes specifically"
N13-1037,P11-1077,0,0.137886,"Missing"
N13-1037,P11-1036,0,0.0101114,"il and SMS text messages, which they found to be both linguistically distinct from Twitter and significantly more prevalent (in 2010). This matches earlier research arguing that email contained relatively little “neography,” compared with text messages and chat (Anis, 2007). A crucial advantage for Twitter is that it is public by default, while SMS and email are private. This makes Twitter data less problematic from a privacy standpoint,1 far easier to obtain, and more amenable to target applications such as large-scale mining of events (Sakaki et al., 2010; Benson et al., 2011) and opinions (Sauper et al., 2011). Similar argument could be made on behalf of other public social media, such as blog comments (Ali-Hasan and Adamic, 2007), forums, and chatrooms (Paolillo, 2001). The main advantage of Twitter over these media is convenience in gathering large datasets through a single streaming interface. More comparative evaluation is needed to determine linguistic similarities and differences between Twitter and these other media; Section 4 presents an evaluation of the lexical similarity between Twitter and political blogs. 2 A tour of bad language While many NLP researchers and engineers have wrestled w"
N13-1037,N10-1100,0,0.0160517,"sity of language in social media, how these processes diffuse over social networks, and how they impact comprehensibility for both the target and non-target audiences. 3.2 Domain adaptation Rather than adapting text to fit our tools, we may instead adapt our tools to fit the text. A series of papers has followed the mold of “NLP for Twitter,” including part-of-speech tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Liu et al., 2011b), parsing (Foster et al., 2011), dialogue modeling (Ritter et al., 2010) and summarization (Sharifi et al., 2010). These papers adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques: • preprocessing to normalize expressive lengthening, and eliminate or group all hashtags, usernames, and URLs (Gimpel et al., 2011; Foster et al., 2011) • new labeled data, enabling the application of semi-supervised learning (Finin et al., 2010; Gimpel et al., 2011; Ritter et al., 2011) • new annotation schemes specifically customized for social media text (Gimpel et al., 2011) • self-training on unlabeled social media text (Foster et al., 2011) • distri"
N13-1037,N03-1033,0,0.00971854,"Missing"
N13-1037,N12-1001,0,0.0103937,"month between January 2010 to October 2012 (Eisenstein et al., 2012). • Twitter-hour: randomly selected tweets from each hour of the day, randomly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not included in the bigram counts; see below for more details on which bigrams are included. • Twitter-@: tweets in which the first token is a username. The username itself is not included in the bigram counts. • Penn Treebank: sections 2-21 • Infinite Jest: the text of the 1996 novel by David Foster Wallace (Wallace, 2012). Consists of only 482,558 tokens. • Blog articles: A randomly-sampled subset of the American political blog posts gathered by Yano et al. (2009). • Blog comments: A randomly-selected subset of comments associated with the blog posts described above. In all corpora, only fully alphabetic tokens are counted; thus, all hashtags and usernames are discarded. The Twitter text is tokenized using Tweet2 4 The lexical coherence of social media The internal coherence of social media — and its relationship to other types of text — can be quantified in terms of the similarity of distributions over 364 A"
N13-1037,P11-1096,0,0.0226718,"Missing"
N13-1037,N09-1054,0,0.0141536,"domly sampled during the period from January 2010 to October 2012. • Twitter-#: tweets in which the first token is a hashtag. The hashtag itself is not included in the bigram counts; see below for more details on which bigrams are included. • Twitter-@: tweets in which the first token is a username. The username itself is not included in the bigram counts. • Penn Treebank: sections 2-21 • Infinite Jest: the text of the 1996 novel by David Foster Wallace (Wallace, 2012). Consists of only 482,558 tokens. • Blog articles: A randomly-sampled subset of the American political blog posts gathered by Yano et al. (2009). • Blog comments: A randomly-selected subset of comments associated with the blog posts described above. In all corpora, only fully alphabetic tokens are counted; thus, all hashtags and usernames are discarded. The Twitter text is tokenized using Tweet2 4 The lexical coherence of social media The internal coherence of social media — and its relationship to other types of text — can be quantified in terms of the similarity of distributions over 364 A very recent study compares Twitter with other corpora, using a number of alternative metrics, such as the use of high and low frequency words, pr"
N13-1037,P06-2005,0,\N,Missing
N13-1100,D08-1035,1,0.132455,"t over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsupervised model of topic segmentation, but this work only considered the role of such markers to differentiate adjoining segments of text, and not to identify their roles with respect to one another. That work was also not capable of learning from supervised annotations in a downstream task. In contrast, our approach uses document-level sentiment annotations to learn about the role of discourse connectors in sentence-level subjectivity. 6 Conclusion Latent variable machine learning is a powerful tool for inducing linguistic structure directly from"
N13-1100,P11-1015,0,0.0208904,"in the vocabulary. Thus, there are four connector sets: • true-unigram-connectors: unigram connectors from the Penn Discourse Treebank and the Spanish language education website • true-multiword-connectors: unigram and multiword connectors from these same resources Experiments To evaluate the utility of adding discourse connectors to latent subjectivity sentiment analysis, we compare several models on movie review datasets in English and Spanish. 3.1 • all-unigram-connectors: all words are potential connectors Data 3.3 We use two movie review datasets: • 50,000 English-language movie reviews (Maas et al., 2011). Each review has a rating from 1-10; we marked ratings of 5 or greater as positive. Half the dataset is used for test and half for training. Parameter tuning is performed by cross-validation. • 5,000 Spanish-language movie reviews (Cruz et al., 2008). Each review has a rating from 1-5; we marked 3-5 as positive. We randomly created a 60/20/20 split for training, validation, and test. 3.2 • auto-unigram-connectors: automatically2 selected connectors using the χ test Connectors We first consider single-word discourse connectors: in English, we use a list of all 57 one-word connectors from the P"
N13-1100,P07-1055,0,0.072533,"opic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed struc812 tured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessenalina et al. (2010b) with the insight of Polanyi and Zaenen (2006) that connectors play a key role in transitions between subjectivity and sentiment. Eisenstein and Barzilay (2008) incorporated discourse connectors into an unsuper"
N13-1100,P04-1035,0,0.356835,"the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed struc812 tured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent variable version of the structured support vector machine (Yu and Joachims, 2009). Our work can be seen as a combination of the machine learning approach of Yessena"
N13-1100,prasad-etal-2008-penn,0,0.291971,"tivity annotations nor an accurate domain-independent discourse parser. But while the “knowledge-free” nature of this approach is appealing, it is unsatisfying that it fails to exploit decades of research on discourse structure. In this paper, we explore a lightweight approach to injecting linguistic knowledge into latent variable models of subjectivity. The entry point is a set of discourse connectors: words and phrases that signal a shift or continuation in the discourse structure. Such connectors have been the subject of extensive study in the creation of the Penn Discourse Treebank (PDTB: Prasad et al. 2008). The role of discourse connectors in sentiment analysis can be clearly seen in examples, such as “It’s hard to imagine the studios hiring another manic German maverick to helm a cop thriller. But that’s exactly why the movie is unmissable.” (Huddleston, 2010) 808 Proceedings of NAACL-HLT 2013, pages 808–813, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics We present a new approach to incorporate discourse connectors in a latent subjectivity model (Yessenalina et al., 2010b). This approach requires no manually-specified information about the meaning of the co"
N13-1100,perez-rosas-etal-2012-learning,0,0.0227672,"tor machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/˜cnyu/latentssvm/), which is in turn built on the SVM-Light distribution (http:// svmlight.joachims.org/). The regularization parameter C was chosen by cross-validation. 4 Results Table 1 shows the sentiment analysis accuracy with each system and feature set. The best overall results in both language are given by the models with system true-multiword-connectors true-unigram-connectors English 91.25 91.36 Spanish 79.80 77.50 auto-connec"
N13-1100,N12-1085,0,0.0273534,"5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central s"
N13-1100,D12-1110,0,0.0376367,"lude multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to id"
N13-1100,D09-1018,0,0.317183,"the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentence"
N13-1100,N03-1030,0,0.245567,"oduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b) treat sentence level subjectivity as a latent variable, automatically inducing the “annotator rationale” (Zaidan et al., 2007; Yessenalina et al., 2010a) for each training sentence so as to focus sentiment learning on the subjective parts of the document. This yields significant improvements over bag-of-ngrams supervised sentiment"
N13-1100,J94-2004,0,0.222664,"towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discourse parser (Soricut and Marcu, 2003), but find that training a decision tree classifier to identify topic-central sentences yields positive results. Wiebe (1994) argues that in coherent narratives, objectivity and subjectivity are usually consistent between adjacent sentences, an insight exploited by Pang and Lee (2004) in a supervised system for subjectivity analysis. Later work employed struc812 tured graphical models to model the flow of subjectivity and sentiment over the course of the document (Mao and Lebanon, 2006; McDonald et al., 2007). All of these approaches depend on labeled training examples of subjective and objective sentences, but Yessenalina et al. (2010b) show that subjectivity can be modeled as a latent variable, using a latent vari"
N13-1100,H05-2018,0,0.0321028,"ell.famaf.unc.edu.ar/˜laura/ shallowdisc4summ/discmar/ 810 Systems The connector-augmented transition features are incorporated into a latent variable support vector machine (SVM). We also consider two baselines: • no-connectors: the same latent variable SVM, but without the connector features. This is identical to the prior work of Yessenalina et al. (2010b). • SVM: a standard SVM binary classifier The latent variable models require an initial guess for the subjectivity of each sentence. Yessenalina et al. (2010b) compare several initializations and find the best results using OpinionFinder (Wilson et al., 2005). For the Spanish data, we performed initial subjectivity analysis by matching against a publiclyavailable full-strength Spanish lexicon set (Rosas et al., 2012). 3.4 Implementation details Both our implementation and the baselines are built on the latent structural SVM (Yu and Joachims, 2009; http://www.cs.cornell. edu/˜cnyu/latentssvm/), which is in turn built on the SVM-Light distribution (http:// svmlight.joachims.org/). The regularization parameter C was chosen by cross-validation. 4 Results Table 1 shows the sentiment analysis accuracy with each system and feature set. The best overall r"
N13-1100,D11-1016,0,0.0267449,"fied by this approach were extremely poor, possibly because so many more of the Spanish connectors include multiple words. 5 Related Work Polanyi and Zaenen (2006) noted the importance of accounting for valence shifters in sentiment analysis, identifying relevant connectors at the sentence and discourse levels. They propose a heuristic approach to use shifters to modify the contributions of sentiment words. There have been several subsequent efforts to model within-sentence valence shifts, including compositional grammar (Moilanen and Pulman, 2007), matrix-vector products across the sentence (Yessenalina and Cardie, 2011), and methods that reason about polarity shifters within the parse tree (Socher et al., 2012; Sayeed et al., 2012). The value of discourse structure towards predicting opinion polarity has also demonstrated in the context of multi-party dialogues (Somasundaran et al., 2009). Our approach functions at the discourse level within single-author documents, so it is complementary to this prior work. Voll and Taboada (2007) investigate various techniques for focusing sentiment analysis on sentences that are central to the main topic. They obtain negative results with the general-purpose SPADE discour"
N13-1100,P10-2062,0,0.615384,"arn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available. 1 Introduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b"
N13-1100,D10-1102,0,0.623402,"arn the relevance of discourse connectors for subjectivity transitions, without subjectivity annotations. This yields significantly improved performance on documentlevel sentiment analysis in English and Spanish. We also describe a simple heuristic for automatically identifying connectors when no predefined list is available. 1 Introduction Document-level sentiment analysis can benefit from consideration of discourse structure. Voll and Taboada (2007) show that adjective-based sentiment classification is improved by examining topicality (whether each sentence is central to the overall point); Yessenalina et al. (2010b) show that bag-ofngrams sentiment classification is improved by examining subjectivity (whether a sentence expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b"
N13-1100,N07-1033,0,0.0147916,"expresses a subjective opinion or objective fact). However, it is unclear how best to obtain the appropriate discourse analyses. Voll and Taboada (2007) find that domain-independent discourse parsing (Soricut and Marcu, 2003) offers little improvement for sentiment analysis, so they resort to training a domainspecific model for identifying topic sentences in reviews. But this requires a labeled dataset of topic sentences, imposing a substantial additional cost. Yessenalina et al. (2010b) treat sentence level subjectivity as a latent variable, automatically inducing the “annotator rationale” (Zaidan et al., 2007; Yessenalina et al., 2010a) for each training sentence so as to focus sentiment learning on the subjective parts of the document. This yields significant improvements over bag-of-ngrams supervised sentiment classification. Latent variable subjectivity analysis is attractive because it requires neither subjectivity annotations nor an accurate domain-independent discourse parser. But while the “knowledge-free” nature of this approach is appealing, it is unsatisfying that it fails to exploit decades of research on discourse structure. In this paper, we explore a lightweight approach to injecting"
N15-1069,P14-2134,0,0.00803711,"with neural word embeddings from Collobert and Weston (2008) and Mnih and Hinton (2009). Word embeddings can also be computed via neural language models (Mikolov et al., 2013b), or from canonical correlation analysis (Dhillon et al., 2011). Xiao and Guo (2013) induce word embeddings across multiple domains, and concatenate these representations into a single feature vector for labeled instances in each domain, following EasyAdapt (Daum´e III, 2007). However, they do not apply this idea to unsupervised domain adaptation, and do not work in the structured feature setting that we consider here. Bamman et al. (2014) learn geographically-specific word embeddings, in an approach that is similar to our multi-domain feature embeddings, but they do not consider the application to domain adaptation. We can also view the distributed representations in FLORS as a sort of word embedding, computed directly from rescaled bigram counts (Schnabel and Sch¨utze, 2014). Feature embeddings are based on a different philosophy than word embeddings. While many NLP features are lexical in nature, the role of a word towards linguistic structure prediction may differ across feature templates. Applying a single word representat"
N15-1069,W06-1615,0,0.982722,"natural language processing is to be successfully employed in highimpact application areas such as social media, patient medical records, and historical texts. Unsupervised domain adaptation is particularly appealing, since it requires no labeled data in the target domain. Some of the most successful approaches to unsupervised domain adaptation are based on representation learning: transforming sparse high-dimensional surface features into dense vector representations, 1 Source code and a demo are available at https:// github.com/yiyang-gt/feat2vec which are often more robust to domain shift (Blitzer et al., 2006; Glorot et al., 2011). However, these methods are computationally expensive to train, and often require special task-specific heuristics to select good “pivot features.” A second, more subtle challenge for unsupervised domain adaptation is that it is normally framed as adapting from a single source domain to a single target domain. For example, we may be given partof-speech labeled text from 19th Century narratives, and we hope to adapt the tagger to work on academic dissertations from the 16th Century. This ignores text from the intervening centuries, as well as text that is related by genre"
N15-1069,P07-1056,0,0.416924,"learn to reconstruct a subset of “pivot features”, as shown in Figure 2(a). The reconstruction function — which is learned from unlabeled data in both domains — is then employed to project each instance into a dense representation, which will hopefully be better suited to cross-domain generalization. The pivot features are chosen to be both predictive of the label and general across domains. Meeting these two criteria requires task-specific heuristics; for example, differ673 ent pivot selection techniques are employed in SCL for syntactic tagging (Blitzer et al., 2006) and sentiment analysis (Blitzer et al., 2007). Furthermore, the pivot features correspond to a small subspace of the feature co-occurrence matrix. In Denoising Autoencoders, each pivot feature corresponds to a dense feature in the transformed representation, but large dense feature vectors impose substantial computational costs at learning time. In SCL, each pivot feature introduces a new classification problem, which makes computation of the cross-domain representation expensive. In either case, we face a tradeoff between the amount of feature co-occurrence information that we can use, and the computational complexity for representation"
N15-1069,P07-1033,0,0.329636,"Missing"
N15-1069,N09-1068,0,0.085658,"and shape features. The tradeoff is that feature embeddings must be recomputed for each set of feature templates, unlike word embeddings, which can simply be downloaded and plugged into any NLP problem. However, computing feature embeddings is easy in practice, since it requires 680 only a light modification to existing well-optimized implementations for computing word embeddings. Multi-domain adaptation The question of adaptation across multiple domains has mainly been addressed in the context of supervised multi-domain learning, with labeled data available in all domains (Daum´e III, 2007). Finkel and Manning (2009) propagate classification parameters across a tree of domains, so that classifiers for sibling domains are more similar; Daum´e III (2009) shows how to induce such trees using a nonparametric Bayesian model. Dredze et al. (2010) combine classifier weights using confidence-weighted learning, which represents the covariance of the weight vectors. Joshi et al. (2013) formulate the problem of multi-attribute multi-domain learning, where all attributes are potential distinctions between domains; Wang et al. (2013) present an approach for automatically partitioning instances into domains according t"
N15-1069,D13-1205,0,0.0116438,"iction problems and the selection of pivot features both involve heuristic decisions, which may vary depending on the task. F EMA avoids the selection of pivot features by directly learning a low-dimensional representation, through which features in each template predict the other templates. An alternative is to link unsupervised learning in the source and target domains with the label distribution in the source domain, through the framework of posterior regularization (Ganchev et al., 2010). This idea is applied to domain adaptation by Huang and Yates (2012), and to cross-lingual learning by Ganchev and Das (2013). This approach requires a forward-backward computation for representation learning, while F EMA representations can be learned without dynamic programming, through negative sampling. Word embeddings Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features. Early work focused on discrete clusters (Brown et al., 1990), while more recent approaches induce dense vector representations; Turian et al. (2010) compare Brown clusters with neural word embeddin"
N15-1069,N06-2015,0,0.00860493,"f the shared embedding h(0) . The regularization penalty is selected by grid search over {0.001, 0.01, 0.1, 1.0, 10.0}. In general, we find that the hyperparameters that yield good word embeddings tend to yield good feature embeddings too. 4.2 Evaluation 1: Web text Recent work in domain adaptation for natural language processing has focused on the data from the shared task on Syntactic Analysis of Non-Canonical Language (SANCL; Petrov and McDonald, 2012), which contains several web-related corpora (newsgroups, reviews, weblogs, answers, emails) as well as the WSJ portion of OntoNotes corpus (Hovy et al., 2006). Following Schnabel and Sch¨utze (2014), we use sections 02-21 of WSJ for training and section 22 for development, and use 100,000 unlabeled WSJ sentences from 1988 for learning representations. On the web text side, each of the five target domains has an unlabeled training set of 100,000 sentences (except the ANSWERS domain, which has 27,274 unlabeled sentences), along with development and test sets of about 1000 labeled sentences each. In the spirit of truly unsupervised domain adaptation, we do not use any target domain data for parameter tuning. Settings For F EMA, we consider only the si"
N15-1069,D12-1120,0,0.0174955,"source domain training data. The design of auxiliary prediction problems and the selection of pivot features both involve heuristic decisions, which may vary depending on the task. F EMA avoids the selection of pivot features by directly learning a low-dimensional representation, through which features in each template predict the other templates. An alternative is to link unsupervised learning in the source and target domains with the label distribution in the source domain, through the framework of posterior regularization (Ganchev et al., 2010). This idea is applied to domain adaptation by Huang and Yates (2012), and to cross-lingual learning by Ganchev and Das (2013). This approach requires a forward-backward computation for representation learning, while F EMA representations can be learned without dynamic programming, through negative sampling. Word embeddings Word embeddings can be viewed as special case of representation learning, where the goal is to learn representations for each word, and then to supply these representations in place of lexical features. Early work focused on discrete clusters (Brown et al., 1990), while more recent approaches induce dense vector representations; Turian et al"
N15-1069,N13-1080,0,0.0745404,"Missing"
N15-1069,N15-1142,0,0.00734312,"04 65.95 40.09 59.94 48.39 49.97 63.91 F EMA-current F EMA-prev F EMA-next F EMA-all F EMA-prev Table 5: Label consistency of the Q-most similar words in each embedding. F EMA-all is the concatenation of the current, previous, and next-word F EMA embeddings. also used to obtain the most common tag for each word. Table 5 shows that the F EMA embeddings are more consistent with the type-level POS tags than WORD 2 VEC embeddings. This is not surprising, since they are based on feature templates that are specifically designed for capturing syntactic regularities. In simultaneously published work, Ling et al. (2015) present “position-specific” word embeddings, which are an alternative method to induce more syntactically-oriented word embeddings. Table 6 shows the most similar words for three query keywords, in each of four different embeddings. The next-word and previous-word embeddings are most related to syntax, because they help to predict each other and the current-word feature; the current-word embedding brings in aspects of orthography, because it must help to predict the affix features. In morphologically rich languages such as Portuguese, this can help to compute good embeddings for rare inflecte"
N15-1069,W96-0213,0,0.26321,"thods can be extremely competitive on these datasets, at a fraction of the computational cost. Specifically, we apply a support vector machine (SVM) classifier, Component Feature template Lexical (5) wi−2 = X, wi−1 = Y, . . . Affixes (8) X is prefix of wi , |X |≤ 4 X is suffix of wi , |X |≤ 4 Orthography (3) wi contains number, uppercase character, or hyphen Table 1: Basic feature templates for token wi . adding dense features from F EMA (and the alternative representation learning techniques) to a set of basic features. 4.1.1 Basic features We apply sixteen feature templates, motivated by by Ratnaparkhi (1996). Table 1 provides a summary of the templates; there are four templates each for the prefix and suffix features. Feature embeddings are learned for all lexical and affix features, yielding a total of thirteen embeddings per instance. We do not learn embeddings for the binary orthographic features. Santos and Zadrozny (2014) demonstrate the utility of embeddings for affix features. 4.1.2 Competitive systems We consider three competitive unsupervised domain adaptation methods. Structural Correspondence Learning (Blitzer et al., 2006, SCL) creates a binary classification problem for each pivot fe"
N15-1069,Q14-1002,0,0.0238776,"Missing"
N15-1069,P10-1040,0,0.301924,"gs, which are dense representations of individual features. Each embedding is selected to help predict the features that fill out the other templates: for example, an embedding for the current word feature is selected to help predict the previous word feature and successor word feature, and vice versa; see Figure 2(b). The embeddings for each active feature are then concatenated together across templates, giving a dense representation for the entire instance. Our approach is motivated by word embeddings, in which dense representations are learned for individual words based on their neighbors (Turian et al., 2010; Xiao and Guo, 2013), but rather than learning a single embedding for each word, we learn embeddings for each feature. This means that the embedding of, say, ‘toughness’ will differ depending on whether it appears in the current-word template or the previous-word template (see Table 6). This provides additional flexibility for the downstream learning algorithm, and the increase in the dimensionality of the overall dense representation can be offset by learning shorter embeddings for each feature. In Section 4, we show that feature embeddings convincingly outperform word embeddings on two part"
N15-1069,D13-1086,0,0.0379065,"Missing"
N15-1069,P14-2088,1,0.860135,"nsider three competitive unsupervised domain adaptation methods. Structural Correspondence Learning (Blitzer et al., 2006, SCL) creates a binary classification problem for each pivot feature, and uses the weights of the resulting classifiers to project the instances into a dense representation. Marginalized Denoising Autoencoders (Chen et al., 2012, mDA) learn robust representation across domains by reconstructing pivot features from artificially corrupted input instances. We use structured dropout noise, which has achieved state-of-art results on domain adaptation for part-of-speech tagging (Yang and Eisenstein, 2014). We also directly compare with WORD 2 VEC3 word embeddings, and with a “no-adaptation” baseline in which only surface features are used. 4.1.3 Parameter tuning All the hyperparameters are tuned on development data. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in 3 https://code.google.com/p/word2vec/ 676 all the domains for SCL and mDA. In SCL, the parameter K selects the number of singular vectors of the projection matrix to consider; we try values between 10 and 100, and also employ feature normalization and rescaling. For embedding-based methods"
N15-1069,J92-4003,0,\N,Missing
N15-1185,I13-1171,0,0.0989726,"edia. Leskovec et al. (2010b) find three social media datasets from which they are able to identify edge polarity; this enables them to compare the frequency of signed triads against baseline expectations, and to build a classifier to predict edge labels (Leskovec et al., 2010a). However, in many of the most popular social media platforms, such as Twitter and Facebook, there is no metadata describing edge labels. We are also interested in new applications of signed social network analysis to datasets outside the realm of social media, such as literary texts (Moretti, 2005; Elson et al., 2010; Agarwal et al., 2013) and movie scripts, but in such corpora, edge labels are not easily available. In many datasets, it is possible to obtain the textual content exchanged between members of the network, and this content can provide a signal for network structure. For example, Hassan et al. (2012) characterize the sign of each network edge in terms MR. GARRISON SADDAM HUSSEIN CARTMAN'S PRESIDENT MOM BIG THING KENNY SATAN THE MOLE CARTMAN IKE PRINCIPAL VICTORIA STAN KYLE'S MOM CHEF KYLE PHILLIP GREGORY WENDY CANADIAN MINISTER OF MOVIES TERRANCE MR. MACKEY Figure 4: Induced signed social network from the film South"
N15-1185,W11-1701,0,0.00880073,"nge in formality over time. 1624 More broadly, a number of recent papers have proposed to detect various types of social relationships from linguistic content. Of particular interest are power relationships, which can be induced from n-gram features (Bramsen et al., 2011; Prabhakaran et al., 2012) and from coordination, where one participant’s linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al. (2013) describe an approach to recognizing politeness in text, lexical and syntactic features motivated by politeness theory. Anand et al. (2011) detect “rebuttals” in argumentative dialogues, and Hasan and Ng (2013) employ extra-linguistic structural features to improve the detection of stances in such debates. In all of these cases, labeled data is used to train supervised model; our work shows that social structural regularities are powerful enough to support accurate induction of social relationships (and their linguistic correlates) without labeled data. 8 Conclusion This paper represents a step towards unifying theoretical models of signed social network structures with linguistic accounts of the expression of social relationship"
N15-1185,P11-1078,0,0.0152771,"without labeled data or parallel text, by leveraging regularities across network structures; however, this requires the assumption that the level of formality for a pair of individuals is constant over time. The combination of our unsupervised approach with annotation projection might yield models that attain higher performance while capturing change in formality over time. 1624 More broadly, a number of recent papers have proposed to detect various types of social relationships from linguistic content. Of particular interest are power relationships, which can be induced from n-gram features (Bramsen et al., 2011; Prabhakaran et al., 2012) and from coordination, where one participant’s linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al. (2013) describe an approach to recognizing politeness in text, lexical and syntactic features motivated by politeness theory. Anand et al. (2011) detect “rebuttals” in argumentative dialogues, and Hasan and Ng (2013) employ extra-linguistic structural features to improve the detection of stances in such debates. In all of these cases, labeled data is used to train supervised model; our work sho"
N15-1185,1996.amta-1.36,0,0.160573,"le, and others being unstable (Cartwright and Harary, 1956); conversely, in status theory (Leskovec et al., 2010b), edges indicate status differentials, and triads should obey transitivity. But these theoretical models can only be applied when the sign of each social network connection is known, and they do not answer the sociolinguistic question of how the sign of a social tie relates to the language that is exchanged across it. We present a unified statistical model that incorporates both network structure and linguistic content. The model connects signed social networks with address terms (Brown and Ford, 1961), which include names, titles, and “placeholder names,” such as dude. The choice of address terms is an indicator of the level of formality between the two parties: for example, in contemporary North American English, a formal relationship is signaled by the use of titles such as Ms and Mr, while an informal relationship is signaled by the use of first names and 1616 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1616–1626, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics placeholder names. These"
N15-1185,W11-0609,0,0.115381,"dress terms that are used across each edge type, thus revealing the social meaning of these address terms; • weights for triadic features of signed networks, which can then be compared with the predictions of existing social theories. Such inferences can be viewed as a form of sociolinguistic structure induction, permitting social meanings to be drawn from linguistic data. In addition to the model and the associated inference procedure, we also present an approach for inducing a lexicon of address terms, and for tagging them in dialogues. We apply this procedure to a dataset of movie scripts (Danescu-Niculescu-Mizil and Lee, 2011). Quantitative evaluation against human ratings shows that the induced clusters of address terms correspond to intuitive perceptions of formality, and that the network structural features improve predictive likelihood over a purely text-based model. Qualitative evaluation shows that the model makes reasonable predictions of the level of formality of social network ties in well-known movies. We first describe our model for linking network structure and linguistic content in general terms, as it can be used for many types of linguistic content and edge labels. Next we describe a procedure which"
N15-1185,P13-1025,0,0.0579927,"tant over time. The combination of our unsupervised approach with annotation projection might yield models that attain higher performance while capturing change in formality over time. 1624 More broadly, a number of recent papers have proposed to detect various types of social relationships from linguistic content. Of particular interest are power relationships, which can be induced from n-gram features (Bramsen et al., 2011; Prabhakaran et al., 2012) and from coordination, where one participant’s linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al. (2013) describe an approach to recognizing politeness in text, lexical and syntactic features motivated by politeness theory. Anand et al. (2011) detect “rebuttals” in argumentative dialogues, and Hasan and Ng (2013) employ extra-linguistic structural features to improve the detection of stances in such debates. In all of these cases, labeled data is used to train supervised model; our work shows that social structural regularities are powerful enough to support accurate induction of social relationships (and their linguistic correlates) without labeled data. 8 Conclusion This paper represents a ste"
N15-1185,P10-1015,0,0.027485,"k models to social media. Leskovec et al. (2010b) find three social media datasets from which they are able to identify edge polarity; this enables them to compare the frequency of signed triads against baseline expectations, and to build a classifier to predict edge labels (Leskovec et al., 2010a). However, in many of the most popular social media platforms, such as Twitter and Facebook, there is no metadata describing edge labels. We are also interested in new applications of signed social network analysis to datasets outside the realm of social media, such as literary texts (Moretti, 2005; Elson et al., 2010; Agarwal et al., 2013) and movie scripts, but in such corpora, edge labels are not easily available. In many datasets, it is possible to obtain the textual content exchanged between members of the network, and this content can provide a signal for network structure. For example, Hassan et al. (2012) characterize the sign of each network edge in terms MR. GARRISON SADDAM HUSSEIN CARTMAN'S PRESIDENT MOM BIG THING KENNY SATAN THE MOLE CARTMAN IKE PRINCIPAL VICTORIA STAN KYLE'S MOM CHEF KYLE PHILLIP GREGORY WENDY CANADIAN MINISTER OF MOVIES TERRANCE MR. MACKEY Figure 4: Induced signed social netw"
N15-1185,P11-2082,0,0.0384718,"Missing"
N15-1185,E12-1064,0,0.0318933,"Missing"
N15-1185,P13-2142,0,0.014312,"s have proposed to detect various types of social relationships from linguistic content. Of particular interest are power relationships, which can be induced from n-gram features (Bramsen et al., 2011; Prabhakaran et al., 2012) and from coordination, where one participant’s linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al. (2013) describe an approach to recognizing politeness in text, lexical and syntactic features motivated by politeness theory. Anand et al. (2011) detect “rebuttals” in argumentative dialogues, and Hasan and Ng (2013) employ extra-linguistic structural features to improve the detection of stances in such debates. In all of these cases, labeled data is used to train supervised model; our work shows that social structural regularities are powerful enough to support accurate induction of social relationships (and their linguistic correlates) without labeled data. 8 Conclusion This paper represents a step towards unifying theoretical models of signed social network structures with linguistic accounts of the expression of social relationships in dialogue. By fusing these two phenomena into a joint probabilistic"
N15-1185,W12-4102,0,0.0318359,". However, in many of the most popular social media platforms, such as Twitter and Facebook, there is no metadata describing edge labels. We are also interested in new applications of signed social network analysis to datasets outside the realm of social media, such as literary texts (Moretti, 2005; Elson et al., 2010; Agarwal et al., 2013) and movie scripts, but in such corpora, edge labels are not easily available. In many datasets, it is possible to obtain the textual content exchanged between members of the network, and this content can provide a signal for network structure. For example, Hassan et al. (2012) characterize the sign of each network edge in terms MR. GARRISON SADDAM HUSSEIN CARTMAN'S PRESIDENT MOM BIG THING KENNY SATAN THE MOLE CARTMAN IKE PRINCIPAL VICTORIA STAN KYLE'S MOM CHEF KYLE PHILLIP GREGORY WENDY CANADIAN MINISTER OF MOVIES TERRANCE MR. MACKEY Figure 4: Induced signed social network from the film South Park: Bigger, Longer & Uncut. Blue solid edges are in the V-cluster, red dashed edges are in the T-cluster. of the sentiment expressed across it, finding that the resulting networks cohere with the predictions of structural balance theory; similar results are obtained by West"
N15-1185,P14-5010,0,0.00165298,"We build an automatically-labeled dataset from the corpus of movie dialogues provided by DanescuNiculescu-Mizil and Lee (2011); see Section 6 for more details. This dataset gives the identity of the speaker and addressee of each line of dialogue. These identities constitute a minimal form of manual annotation, but in many settings, such as social media dialogues, they could be obtained automatically. We augment this data by obtaining the first (given) and last (family) names of each character, which we mine from the website rottentomatoes.com. Next, we apply the CoreNLP part-of-speech tagger (Manning et al., 2014) to identify sequences of the NNP tag, which indicates a proper noun in the Penn Treebank Tagset (Marcus et al., 1993). For Titles To induce a lexicon of titles, we consider terms that are frequently labeled with the tag BADDR across a variety of dialogues, performing a binomial test to obtain a list of terms whose frequency of being labeled as B-ADDR is significantly higher than chance. Of these 34 candidate terms, we manually filter out 17, which are mainly common first names, such as John; such names are frequently labeled as B-ADDR across movies. After this manual filtering, we obtain the"
N15-1185,J93-2004,0,0.0555496,"(2011); see Section 6 for more details. This dataset gives the identity of the speaker and addressee of each line of dialogue. These identities constitute a minimal form of manual annotation, but in many settings, such as social media dialogues, they could be obtained automatically. We augment this data by obtaining the first (given) and last (family) names of each character, which we mine from the website rottentomatoes.com. Next, we apply the CoreNLP part-of-speech tagger (Manning et al., 2014) to identify sequences of the NNP tag, which indicates a proper noun in the Penn Treebank Tagset (Marcus et al., 1993). For Titles To induce a lexicon of titles, we consider terms that are frequently labeled with the tag BADDR across a variety of dialogues, performing a binomial test to obtain a list of terms whose frequency of being labeled as B-ADDR is significantly higher than chance. Of these 34 candidate terms, we manually filter out 17, which are mainly common first names, such as John; such names are frequently labeled as B-ADDR across movies. After this manual filtering, we obtain the following titles: agent, aunt, captain, colonel, commander, cousin, deputy, detective, dr, herr, inspector, judge, lor"
N15-1185,N12-1057,0,0.190686,"r parallel text, by leveraging regularities across network structures; however, this requires the assumption that the level of formality for a pair of individuals is constant over time. The combination of our unsupervised approach with annotation projection might yield models that attain higher performance while capturing change in formality over time. 1624 More broadly, a number of recent papers have proposed to detect various types of social relationships from linguistic content. Of particular interest are power relationships, which can be induced from n-gram features (Bramsen et al., 2011; Prabhakaran et al., 2012) and from coordination, where one participant’s linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al. (2013) describe an approach to recognizing politeness in text, lexical and syntactic features motivated by politeness theory. Anand et al. (2011) detect “rebuttals” in argumentative dialogues, and Hasan and Ng (2013) employ extra-linguistic structural features to improve the detection of stances in such debates. In all of these cases, labeled data is used to train supervised model; our work shows that social structural r"
N15-1185,W09-1119,0,0.00542156,"Missing"
N15-1185,Q14-1024,0,0.0834197,"T } 2. Iterate until convergence: E-step update each qij in closed form, based on Equation 5. M-step: content Update θ in closed form from Equations 6 and 7. M-step: structure Update β, η, and c by applying L-BFGS to the noise-contrastive estimation objective in Equation 8. Table 1: Expectation-maximization estimation procedure Obtaining estimates for β and η is more challenging, as it would seem to involve computing the partition function Z(η, β; G), which sums over all possible labeling of each network G(t) . The number of such labelings is exponential in the number of edges in the network. West et al. (2014) show that for an objective function involving features on triads and dyads, it is NP-hard to find even the single optimal labeling. We therefore apply noise-contrastive estimation (NCE; Gutmann and Hyv¨arinen, 2012), which transforms the problem of estimating the density P (y) into a classification problem: distinguishing the observed graph labelings y (t) from randomly˜ (t) ∼ Pn , where Pn generated “noise” labelings y is a noise distribution. NCE introduces an additional parameter c for the partition function, so that log P (y; β, η, c) = log P 0 (y; β, η)+c, with P 0 (y) representing the u"
N16-1037,D15-1262,0,0.161131,"formation as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the main goal of prior efforts in this space (Kingma and Welling, 2014; Chung et al., 2015). From this perspective, our model with discourse relations as latent variables shares the same mer"
N16-1037,P15-1033,0,0.0205422,"network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or training time, elegantly modeling multiple linguistic phenomena in a joint framework (Finkel et al., 2006). But because these graphical models represent uncertainty for every element in the model, adding too many layers of latent variables makes them difficult to train. In t"
N16-1037,W06-1673,0,0.0565286,"thJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or training time, elegantly modeling multiple linguistic phenomena in a joint framework (Finkel et al., 2006). But because these graphical models represent uncertainty for every element in the model, adding too many layers of latent variables makes them difficult to train. In this paper, we present a hybrid architecture that combines a recurrent neural network language model with a latent variable model over shallow discourse structure. In this way, the model learns a discriminatively-trained distributed representation of the local contextual features that drive word choice at the intra-sentence level, using techniques that are now state-of-the-art in language modeling (Mikolov et al., 2010). However"
N16-1037,P14-1002,1,0.828815,"age model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capac"
N16-1037,D12-1083,0,0.0230579,"amework to model the context and current sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabil"
N16-1037,W13-3214,0,0.614919,"he relationships between pairs of adjacent sentences — as a latent variable. As a result, the model can act as both a discourse relation classifier and a language model. Specifically: • If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classification in the Penn Discourse Treebank (Rutherford and Xue, 2015) and dialog act classification in Switch332 Proceedings of NAACL-HLT 2016, pages 332–342, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics board (Kalchbrenner and Blunsom, 2013). The model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse relations (Ji and Eisenstein, 2015). • If the model is trained to maximize the joint likelihood of the discourse relations and the text, it is possible to marginalize over discourse relations at test time, outperforming language models that do not account for discourse structure. In contrast to recent work on continuous latent variables in recurrent neural networks (Chung et al., 2015), which require complex v"
N16-1037,D14-1220,0,0.0211294,"the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the"
N16-1037,D09-1036,0,0.853274,"paragraph, and need not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discourse relations. During training, we collapse all relation types other than implicit (explicit, E NT R EL, and N O R EL) into a single dummy relation type, which holds between all adjacent sentence pairs that do not share an implicit relation. As in the prior work on first-level discourse relation identification (e.g., Park and Cardie, 2012), we use sections 2-20 of the PDTB"
N16-1037,D15-1106,0,0.0802735,"classification, which characterizes the structure of interpersonal communication in the Switchboard corpus (Stolcke et al., 2000), and is a key component of contemporary dialog systems (Williams and Young, 2007). Our model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. 2 Background Our model scaffolds on recurrent neural network (RNN) language models (Mikolov et al., 2010), and recent variants that exploit multiple levels of linguistic detail (Ji et al., 2015; Lin et al., 2015). 333 RNN Language Models Let us denote token n in a sentence t by yt,n ∈ {1 . . . V }, and write yt = {yt,n }n∈{1...Nt } to indicate the sequence of words in sentence t. In an RNN language model, the probability of the sentence is decomposed as, p(yt ) = Nt Y p(yt,n |yt,&lt;n ), (1) n where the probability of each word yt,n is conditioned on the entire preceding sequence of words yt,&lt;n through the summary vector ht,n−1 . This vector is computed recurrently from ht,n−2 and from the embedding of the current word, Xyt,n−1 , where X ∈ RK×V and K is the dimensionality of the word embeddings. The lang"
N16-1037,W14-1505,0,0.00993717,"ging has been widely studied in both NLP and speech communities. We follow the setup used by Stolcke et al. (2000) to conduct experiments, and adopt the following systems for comparison: Stolcke et al. (2000) employ a hidden Markov model, with each HMM state corresponding to a dialogue act. Kalchbrenner and Blunsom (2013) employ a complex neural architecture, with a convolutional network at each utterance and a recurrent network over the length of the dialog. To our knowledge, this model attains state-of-the-art accuracy on this task, outperforming other prior work such as (Webb et al., 2005; Milajevs and Purver, 2014). Results As shown in Table 2, the conditionallytrained discourse relation language model (D R LM) outperforms all competitive systems on this task. A binomial test shows the result in line 6 is significantly better than the previous state-of-the-art (line 4). All comparisons are against published results, and Macro-F1 scores are not available. Accuracy 338 is more reliable on this evaluation, since no single class dominates, unlike the PDTB task. 5.3 Discourse-aware language modeling As a joint model for discourse and language modeling, D R LM can also function as a language model, assigning"
N16-1037,C08-2022,0,0.0306807,"el discourse annotation on written texts. In the PDTB, each discourse relation is annotated between two argument spans, Arg1 and Arg2. There are two types of relations: explicit and implicit. Explicit relations are signalled by discourse markers (e.g., “however”, “moreover”), and the span of Arg1 is almost totally unconstrained: it can range from a single clause to an entire paragraph, and need not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discours"
N16-1037,P09-1077,0,0.114386,"d not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discourse relations. During training, we collapse all relation types other than implicit (explicit, E NT R EL, and N O R EL) into a single dummy relation type, which holds between all adjacent sentence pairs that do not share an implicit relation. As in the prior work on first-level discourse relation identification (e.g., Park and Cardie, 2012), we use sections 2-20 of the PDTB as the training set,"
N16-1037,prasad-etal-2008-penn,0,0.355405,"mation ct impacts the hidden state ht+1 , rather than going directly to the outputs yt+1 . They obtain slightly better perplexity with this approach, which has fewer trainable parameters. However, this model would couple zt with all subsequent sentences y>t , making prediction and marginalization of discourse relations considerably more challenging. Sequential Monte Carlo algorithms offer a possible solution (de Freitas et al., ; Gu et al., 2015), which may be considered in future work. 4 Data and Implementation We evaluate our model on two benchmark datasets: (1) the Penn Discourse Treebank (Prasad et al., 2008, PDTB), which is annotated on a corpus of Wall Street Journal acticles; (2) the Switchboard di336 there are an equal number of instances with and without each relation type (Park and Cardie, ; Biran and McKeown, 2013; Rutherford and Xue, 2014). In this paper, we target the more challenging multiway classification problem, so this strategy is not applicable; in any case, since our method deals with entire documents, it is not possible to balance the training set in this way. The Switchboard Dialog Act Corpus (SWDA) is annotated on the Switchboard Corpus of humanhuman conversational telephone s"
N16-1037,E14-1068,0,0.0566142,"quent sentences y>t , making prediction and marginalization of discourse relations considerably more challenging. Sequential Monte Carlo algorithms offer a possible solution (de Freitas et al., ; Gu et al., 2015), which may be considered in future work. 4 Data and Implementation We evaluate our model on two benchmark datasets: (1) the Penn Discourse Treebank (Prasad et al., 2008, PDTB), which is annotated on a corpus of Wall Street Journal acticles; (2) the Switchboard di336 there are an equal number of instances with and without each relation type (Park and Cardie, ; Biran and McKeown, 2013; Rutherford and Xue, 2014). In this paper, we target the more challenging multiway classification problem, so this strategy is not applicable; in any case, since our method deals with entire documents, it is not possible to balance the training set in this way. The Switchboard Dialog Act Corpus (SWDA) is annotated on the Switchboard Corpus of humanhuman conversational telephone speech (Godfrey et al., 1992). The annotations label each utterance with one of 42 possible speech acts, such as AGREE, HEDGE, and WH - QUESTION . Because these speech acts form the structure of the dialogue, most of them pertain to both the pre"
N16-1037,N15-1081,0,0.54352,"t drive word choice at the intra-sentence level, using techniques that are now state-of-the-art in language modeling (Mikolov et al., 2010). However, the model treats shallow discourse structure — specifically, the relationships between pairs of adjacent sentences — as a latent variable. As a result, the model can act as both a discourse relation classifier and a language model. Specifically: • If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classification in the Penn Discourse Treebank (Rutherford and Xue, 2015) and dialog act classification in Switch332 Proceedings of NAACL-HLT 2016, pages 332–342, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics board (Kalchbrenner and Blunsom, 2013). The model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse relations (Ji and Eisenstein, 2015). • If the model is trained to maximize the joint likelihood of the discourse relations and the text, it is possible to marginalize over discourse relations at"
N16-1037,W09-3813,0,0.0317933,"ent sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text"
N16-1037,D13-1170,0,0.0012234,"on in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. 1 Introduction Natural language processing (NLP) has recently experienced a neural network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or"
N16-1037,N15-1020,1,0.781846,"etwork language model. An alternative neural approach for dialogue act tagging is the combined convolutionalrecurrent architecture of Kalchbrenner and Blunsom (2013). Our modeling framework is simpler, relying on a latent variable parametrization of a purely recurrent architecture. This paper draws on previous work in both discourse modeling and language modeling. Language modeling There are an increasing number of attempts to incorporate document-level context information into language modeling. For example, Mikolov and Zweig (2012) introduce LDAstyle topics into RNN based language modeling. Sordoni et al. (2015) use a convolutional structure to summarize the context from previous two utterances as context vector for RNN based language modeling. Our models in this paper provide a unified framework to model the context and current sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a l"
N16-1037,J00-3003,0,0.979587,"Missing"
N16-1037,P10-1040,0,0.0155191,"ation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. 1 Introduction Natural language processing (NLP) has recently experienced a neural network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic g"
N16-1037,K15-2001,0,0.0454434,"olkits such as Theano, Torch, and CNN. We focus on a class of shallow discourse relations, which hold between pairs of adjacent sentences (or utterances). These relations describe how the adjacent sentences are related: for example, they may be in CONTRAST, or the latter sentence may offer an answer to a question posed by the previous sentence. Shallow relations do not capture the full range of discourse phenomena (Webber et al., 2012), but they account for two well-known problems: implicit discourse relation classification in the Penn Discourse Treebank, which was the 2015 CoNLL shared task (Xue et al., 2015); and dialog act classification, which characterizes the structure of interpersonal communication in the Switchboard corpus (Stolcke et al., 2000), and is a key component of contemporary dialog systems (Williams and Young, 2007). Our model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. 2 Background Our model scaffolds on recurrent neural network (RNN) language models (Mikolov et al., 2010), and recent variants that exploit multiple levels of linguistic detail ("
N16-1037,D15-1266,0,0.0952637,"adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the main goal of prior efforts in this space (Kingma and Welling, 2014; Chung et al., 2015). From this perspective, our model with discourse relations as latent varia"
N16-1037,J15-4006,0,\N,Missing
N16-1157,W06-1615,0,0.879931,"ges in the meanings of words (Kulkarni et al., 2015). In the example above, the word ‘ryottours’ is not successfully normalized to ‘rioters’; the syntax is comprehensible to contemporary English speakers, but usages such as ‘wild disposed’ and ‘drew unto’ are sufﬁciently unusual as to pose problems for NLP systems trained on contemporary texts. Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a distinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptatio"
N16-1157,J92-4003,0,0.197326,"storical text but has no labeled data in the target domain (e.g., Muralidharan and Hearst, 2013). This best ﬁts the paradigm of unsupervised domain adaptation, when labeled data from the source domain (e.g., the PTB) is combined with unlabeled data from the target domain. Representational differences between source and target domains can be a major source of errors in domain adaptation (BenDavid et al., 2010), and so several representation learning approaches have been proposed. The most straightforward approach is to replace lexical features with word representations, such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010), such as word2vec (Mikolov et al., 2013). Lexical features can then be replaced or augmented with the resulting word representations. This can assist in domain adaptation by linking out-of-vocabulary words to invocabulary words with similar distributional properties. Word representations are suitable for adapting lexical features, but a more general solution is to adapt the entire feature representation. One such method is Structural Correspondence Learning (Blitzer et al., 2006, SCL). In SCL, we create artiﬁcial binary classiﬁcation"
N16-1157,P07-1033,0,0.218938,"Missing"
N16-1157,N13-1037,1,0.919687,"Missing"
N16-1157,N09-1068,0,0.0819637,"Missing"
N16-1157,gimenez-marquez-2004-svmtool,0,0.117732,"Missing"
N16-1157,P07-1034,0,0.0877654,"Missing"
N16-1157,N13-1080,0,0.0393106,"Missing"
N16-1157,P12-3029,0,0.065559,"Missing"
N16-1157,P14-5010,0,0.00422553,"texts in the period from 1500 until 1710, and it is divided into three 70-year time periods similar to the PPCMBE corpus. The statistics of the corpus by time period is summarized in Table 2. The PPCEME consists of text from the same eighteen genres as the PPCMBE. Penn Treebank Release 3 The Penn Treebank (Marcus et al., 1993) is the de facto standard syntactically annotated corpus for English, 2 All the statistics in this section include punctuation, but exclude extra-linguistic material such as page numbers or token ID numbers. 1320 which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying unsupervised domain adaptation, we use all WSJ sections, together with texts from the PPCMBE and the PPCEME. Tagsets The Penn Corpora of Historical English (PCHE) use a tagset that differs from the Penn Treebank, mainly in the direction of greater speciﬁcity. Auxiliary verbs ‘do’, ‘have’, and ‘be’ all have their own tags, as do words like ‘one’ and ‘else’, due to their changing syntactic function over time. Overall, there are"
N16-1157,J93-2004,0,0.0562741,"The PPCEME is a collection of text samples from the Helsinki Corpus (Rissanen et al., 1993), as well as two supplements mainly consisting of text material by the same authors and from the same editions as the material in the Helsinki Corpus. The corpus contains nearly two million words from texts in the period from 1500 until 1710, and it is divided into three 70-year time periods similar to the PPCMBE corpus. The statistics of the corpus by time period is summarized in Table 2. The PPCEME consists of text from the same eighteen genres as the PPCMBE. Penn Treebank Release 3 The Penn Treebank (Marcus et al., 1993) is the de facto standard syntactically annotated corpus for English, 2 All the statistics in this section include punctuation, but exclude extra-linguistic material such as page numbers or token ID numbers. 1320 which is used to train software such as Stanford CoreNLP (Manning et al., 2014). When using this dataset for supervised training, we follow Toutanova et al. (2003) and use WSJ sections 0-18 for training, and sections 19-21 for tuning. When applying unsupervised domain adaptation, we use all WSJ sections, together with texts from the PPCMBE and the PPCEME. Tagsets The Penn Corpora of H"
N16-1157,D07-1041,0,0.566422,"the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME), and the Penn Parsed Corpus of Modern British English (Kroch and Taylor, 2000, PPCMBE). The corpora are annotated with part-of-speech tags and syntactic parsing trees in an annotation style similar to that of the Penn Treebank. In this work, we focus on POS tagging the PPCMBE and the PPCEME.1 1 Middle English is outside the scope of this paper, because it is sufﬁciently unintelligible to modern English speakers that texts such as Canterbury Tales are published in translation. In tagging Middle English texts, Moon and Baldridge (2007) apply bitext projection techniques from multilingual learning, rather than domain adaptation. Period # Sentence # Token 1840-1914 1770-1839 1700-1769 17,770 23,462 16,083 322,255 427,424 343,024 Total 57,315 1,092,703 Table 1: Statistics of the Penn Parsed Corpus of Modern British English (PPCMBE), by time period. Period 1640-1710 1570-1639 1500-1569 Total # Sentence # Token 29,181 39,799 31,416 614,315 706,587 640,255 100,396 1,961,157 Table 2: Statistics of the Penn Parsed Corpus of Early Modern English (PPCEME), by time period. The Penn Parsed Corpus of Modern British English The PPCMBE is"
N16-1157,W11-1512,0,0.0709538,"Missing"
N16-1157,W96-0213,0,0.835857,"Missing"
N16-1157,W11-1503,0,0.0426855,"rical texts Historical texts differ from modern texts in spellings, syntax and semantics, posing signiﬁcant challenges for standard NLP systems, which are usually trained with modern news text. Numerous resources have been created for overcoming the difﬁculties, including syntactically annotated corpora (Kroch et al., 2004; Kroch et al., 2010; Galves and Faria, 2010) and spelling normalization tools (Giusti et al., 2007; Baron and Rayson, 2008). Most previous work focuses on normalization, which can signiﬁcantly increase tagging accuracy on historical English (Rayson et al., 2007) and German (Scheible et al., 2011). Similar im1326 provements have been obtained for syntactic parsing (Schneider et al., 2014). Domain adaptation offers an alternative approach which is more generic — for example, it can be applied to any corpus without requiring the design of a set of normalization rules. As shown above, when normalization is possible, it can be combined with domain adaptation to yield better performance than that obtained by either approach alone. 7 Conclusion Syntactic analysis is a key ﬁrst step towards processing historical texts, but it is confounded by changes in spelling and usage over time. We empiri"
N16-1157,Q14-1002,0,0.0293341,"Missing"
N16-1157,N03-1033,0,0.0417227,"Missing"
N16-1157,P10-1040,0,0.0609493,"(e.g., Muralidharan and Hearst, 2013). This best ﬁts the paradigm of unsupervised domain adaptation, when labeled data from the source domain (e.g., the PTB) is combined with unlabeled data from the target domain. Representational differences between source and target domains can be a major source of errors in domain adaptation (BenDavid et al., 2010), and so several representation learning approaches have been proposed. The most straightforward approach is to replace lexical features with word representations, such as Brown clusters (Brown et al., 1992; Lin et al., 2012) or word embeddings (Turian et al., 2010), such as word2vec (Mikolov et al., 2013). Lexical features can then be replaced or augmented with the resulting word representations. This can assist in domain adaptation by linking out-of-vocabulary words to invocabulary words with similar distributional properties. Word representations are suitable for adapting lexical features, but a more general solution is to adapt the entire feature representation. One such method is Structural Correspondence Learning (Blitzer et al., 2006, SCL). In SCL, we create artiﬁcial binary classiﬁcation problems for thousands of cross-domain “pivot” features, an"
N16-1157,P14-2088,1,0.758274,"x is comprehensible to contemporary English speakers, but usages such as ‘wild disposed’ and ‘drew unto’ are sufﬁciently unusual as to pose problems for NLP systems trained on contemporary texts. Domain adaptation A more generic machine learning approach is to apply unsupervised domain adaptation techniques, which transform the representations of the training and target texts to be more similar, typically using feature co-occurrence statistics (Blitzer et al., 2006; Ben-David et al., 2010). It is natural to think of historical texts as a distinct domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can potentially exploit these metadata attrib"
N16-1157,N15-1069,1,0.828081,"domain from contemporary training corpora, and Yang and Eisenstein (2014, 2015) show that the accuracy of historical Portuguese POS tagging can be signiﬁcantly improved by domain adaption. However, we are unaware of prior work that empirically evaluates the efﬁcacy of this approach on Early Modern English texts. Furthermore, historical texts are often associated with multiple metadata attributes (e.g., author, genre, and epoch), each of which may inﬂuence the text’s linguistic properties. Multi-domain adaptation (Mansour et al., 2009) and multi-attribute domain adaptation (Joshi et al., 2013; Yang and Eisenstein, 2015) can potentially exploit these metadata attributes to obtain further improvements. This paper presents the ﬁrst comprehensive empirical comparison of effectiveness of these approaches for part-of-speech tagging on historical texts. We focus on the two historical treebanks of the Penn Corpora of Historical English — the Penn Parsed Corpus of Modern British English (Kroch et al., 2010, PPCMBE) and the Penn-Helsinki Parsed Corpus of Early Modern English (Kroch et al., 2004, PPCEME). These datasets enable a range of analyses, which isolate the key issues in dealing with historical corpora: • In on"
N18-1100,D14-1181,0,0.104292,"ppets of text from the input document. We perform a human evaluation of the quality of the explanations provided by the attention mechanism, asking a physician to rate the informativeness of a set of automatically generated explanations.1 2 Method We treat ICD-9 code prediction as a multilabel text classification problem (McCallum, 1999).2 Let  represent the set of ICD-9 codes; the labeling problem for instance ? is to determine ??,? ∈ {0, 1} for all ? ∈ . We train a neural network which passes text through a convolutional layer to compute a base representation of the text of each document (Kim, 2014), and makes | |binary classifiOur code, data splits, and pre-trained models are available at github.com/jamesmullenbach/ caml-mimic. 2 We focus on codes from the ICD-9 taxonomy, rather than the more recent ICD-10, for the simple reason that this is the version of ICD used in the MIMIC datasets. cation decisions. Rather than aggregating across this representation with a pooling operation, we apply an attention mechanism to select the parts of the document that are most relevant for each possible code. These attention weights are then applied to the base representation, and the result is passed"
N18-1100,D16-1011,0,0.148324,"Missing"
N18-1100,N16-1082,0,0.0320547,"anism, which focuses on the most critical features for each code, rather than applying a uniform pooling operation for all codes. We also observed that convolution-based models are at least as effective, and significantly more computationally efficient, than recurrent neural networks such as the Bi-GRU. Explainable text classification A goal of this work is that the code predictions be explainable from features of the text. Prior work has also em1108 phasized explainability. Lei et al. (2016) model “rationales” through a latent variable, which tags each word as relevant to the document label. Li et al. (2016) compute the salience of individual words by the derivative of the label score with respect to the word embedding. Ribeiro et al. (2016) use submodular optimization to select a subset of features that closely approximate a specific classification decision (this work is also notable for extensive human evaluations). In comparison to these approaches, we employ a relatively simple attentional architecture; this simplicity is motivated by the challenge of scaling to multi-label classification with thousands of possible labels. Other prior work has emphasized the use of attention for highlighting"
N18-1100,N16-3020,0,0.0775009,"e also observed that convolution-based models are at least as effective, and significantly more computationally efficient, than recurrent neural networks such as the Bi-GRU. Explainable text classification A goal of this work is that the code predictions be explainable from features of the text. Prior work has also em1108 phasized explainability. Lei et al. (2016) model “rationales” through a latent variable, which tags each word as relevant to the document label. Li et al. (2016) compute the salience of individual words by the derivative of the label score with respect to the word embedding. Ribeiro et al. (2016) use submodular optimization to select a subset of features that closely approximate a specific classification decision (this work is also notable for extensive human evaluations). In comparison to these approaches, we employ a relatively simple attentional architecture; this simplicity is motivated by the challenge of scaling to multi-label classification with thousands of possible labels. Other prior work has emphasized the use of attention for highlighting salient features of the text (e.g., Rush et al., 2015; Rocktäschel et al., 2016), although these papers did not perform human evaluation"
N18-1100,D15-1044,0,0.0256036,"words by the derivative of the label score with respect to the word embedding. Ribeiro et al. (2016) use submodular optimization to select a subset of features that closely approximate a specific classification decision (this work is also notable for extensive human evaluations). In comparison to these approaches, we employ a relatively simple attentional architecture; this simplicity is motivated by the challenge of scaling to multi-label classification with thousands of possible labels. Other prior work has emphasized the use of attention for highlighting salient features of the text (e.g., Rush et al., 2015; Rocktäschel et al., 2016), although these papers did not perform human evaluations of the interpretability of the features selected by the attention mechanism. 6 Conclusions and Future Work We present CAML, a convolutional neural network for multi-label document classification, which employs an attention mechanism to adaptively pool the convolution output for each label, learning to identify highly-predictive locations for each label. CAML yields strong improvements over previous metrics on several formulations of the ICD-9 code prediction task, while providing satisfactory explanations for"
N18-1100,W14-3409,0,0.0141301,"compute attention over specific locations in the text. Our work differs in that we compute separate attention weights for each label in our label space, which is better tuned to our goal of selecting locations in a document which are most important for predicting specific labels. Automatic ICD coding ICD coding is a longstanding task in the medical informatics community, which has been approached with machine learning and handcrafted methods (Scheurwegs et al., 2015). Many recent approaches, like ours, use unstructured text data as the only source of information (e.g., Kavuluru et al., 2015; Subotin and Davis, 2014), though some incorporates struc1107 Model AUC Macro Micro – – – – Macro Logistic Regression CNN Bi-GRU 0.690 0.742 0.780 0.934 0.941 0.954 0.025 0.030 0.024 Flat SVM (Perotte et al., 2013) HA-GRU (Baumel et al., 2018) CAML DR-CAML 0.820 0.826 0.966* 0.966* – – F1 Micro P@8 0.314 0.332 0.359 0.425 0.388 0.420 0.048 0.049 0.293 0.366 0.442 0.457* – – 0.523* 0.515 Table 6: Results on MIMIC-II full, 5031 labels. Method CAML Code Descriptions Logistic Regression CNN Informative 46 48 41 36 Highly informative 22 20 18 13 Table 7: Qualitative evaluation results. The columns show the number of exampl"
N18-1100,N16-1174,0,0.350177,"Missing"
N18-1100,Q16-1019,0,0.0400185,"Missing"
N18-1100,W17-2333,0,0.0455226,"7* – – 0.523* 0.515 Table 6: Results on MIMIC-II full, 5031 labels. Method CAML Code Descriptions Logistic Regression CNN Informative 46 48 41 36 Highly informative 22 20 18 13 Table 7: Qualitative evaluation results. The columns show the number of examples (out of 100) for which each method was selected as “informative” or “highly informative”. tured data as well (e.g., Scheurwegs et al., 2017; Wang et al., 2016). Most previous methods have either evaluated only on a strict subset of the full ICD label space (Wang et al., 2016), relied on datasets that focus on a subset of medical scenarios (Zhang et al., 2017), or evaluated on data that are not publicly available, making direct comparison difficult (Subotin and Davis, 2016). A recent shared task for ICD-10 coding focused on coding of death certificates in English and French (Névéol et al., 2017). This dataset also contains shorter documents than those we consider, with an average of 18 tokens per certificate in the French corpus. We use the open-access MIMIC datasets containing de-identified, general-purpose records of intensive care unit stays at a single hospital. Perotte et al. (2013) use “flat” and “hierarchical” SVMs; the former treats each co"
N18-2022,E17-1116,0,0.2944,"utral hashtags, based on translations of associated tweet content.3 After including ASCII-equivalent variants of special characters, as well as lowercased variants, our final hashtag set comprises 111 unique strings. Next, all tweets containing any referendum hashtag were extracted from T , yielding 190,061 tweets. After removing retweets and tweets from users whose tweets frequently contained URLs (i.e., likely bots), our final “Catalonian Independence Tweets” (CT) dataset is made up of 11,670 tweets from 10,498 users (cf. the Scottish referendum set IT with 59,664 tweets and 18,589 users in Shoemark et al. (2017)). 36 referendum-related hashtags appear in the filtered dataset. They are shown with their frequencies (including variants) in Table 1 (cf. the 47 hashtags and similar frequency distribution in Table 1 of Shoemark et al. (2017)). To address the control condition, all authors of 1977). While many studies have examined codeswitching in the spoken context (Auer, 2013), social media platforms such as Twitter provide an opportunity to study code-switching in online discussions (Androutsopoulos, 2015). In the online context, choice of language may reflect the writer’s intended audience (Kim et al.,"
N18-2022,P12-3005,0,0.102027,"Missing"
N19-5003,C18-1135,0,0.0665606,"Missing"
N19-5003,N18-1044,0,0.0312699,"time 2.4 Detecting changes in meaning Question: When did money become something you can launder? Data: Legal opinions from courtlistener. com Methods: dynamic word embeddings Tracking changes in word frequency Word embeddings capture lexical semantics in vector form, but word meaning can change over time through a variety of linguistic mechanisms (Tahmasebi et al., 2018). This section will survey methods for computing diachronic word embeddings, which are parameterized by time (Wijaya and Yeniterzi, 2011; Kulkarni et al., 2015; Hamilton et al., 2016; Garg et al., 2018; Rudolph and Blei, 2018; Rosenfeld and Erk, 2018). We will investigate the application of one such method to a corpus of historical texts, identifying words with particularly fluid semantics, and teasing apart these different meanings. Question: Are people happier on the weekend? Data: Twitter sentiment (Golder and Macy, 2011) Methods: hypothesis testing, regression, python dataframes In a seminal paper in social media analysis, Golder and Macy (2011) use Twitter data to quantify sentiment by time-of-day and day-of-the-week. This provides an opportunity to apply fundamental methods in quantitative social science to a timestamped corpus of te"
N19-5003,P15-1157,0,0.15362,"ust different? • Who is a leader, and who is a follower? • How can we get internet users to be more polite and objective? 1.2 Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are turning to computational techniques for answers (e.g., Evans and Aceves, 2016; Underwood et al., 2018; Barron et al., 2018). Meanwhile, the ACL community is increasingly engaged with data that varies across time (e.g., Rayson et al., 2007; Yang and Eisenstein, 2016), and with the social insights that can be offered by analyzing temporal patterns and trends (e.g., Tsur et al., 2015). The purpose of this tutorial is to facilitate this convergence in two main ways. First, by synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful text analytic tools for social scientists and digital humanities scholars, such as dynamic topic models and dynamic word embeddings. Second, the tutorial will provide an overview of techniques and datasets from the quantitative Scope This tutorial is focused on corpus-based method"
N19-5003,P14-2051,0,0.0743201,"Missing"
N19-5003,N16-1157,1,0.847921,"b repository for the tutorial.1 • When did “gay” stop meaning “happy”? • Are gender stereotypes getting weaker, stronger, or just different? • Who is a leader, and who is a follower? • How can we get internet users to be more polite and objective? 1.2 Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are turning to computational techniques for answers (e.g., Evans and Aceves, 2016; Underwood et al., 2018; Barron et al., 2018). Meanwhile, the ACL community is increasingly engaged with data that varies across time (e.g., Rayson et al., 2007; Yang and Eisenstein, 2016), and with the social insights that can be offered by analyzing temporal patterns and trends (e.g., Tsur et al., 2015). The purpose of this tutorial is to facilitate this convergence in two main ways. First, by synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful text analytic tools for social scientists and digital humanities scholars, such as dynamic topic models and dynamic word embeddings. Second, the tutorial will prov"
P07-1045,W99-0611,0,0.477181,"fferent set of feature weights for each case: wv,1 when the non-verbal features are included, and wv,2 otherwise. The formal definition of the potential function for conditional modality fusion is: ψ(y, m, x; w) ≡ ( T x + wT x ) + wT x y(wv,1 v nv nv m m T T ywv,2 xv − wm xm 4 m=1 (3) m = −1. Application to coreference resolution We apply conditional modality fusion to coreference resolution – the problem of partitioning the noun phrases in a document into clusters, where all members of a cluster refer to the same semantic entity. Coreference resolution on text datasets is wellstudied (e.g., (Cardie and Wagstaff, 1999)). This prior work provides the departure point for our investigation of coreference resolution on spontaneous and unconstrained speech and gesture. 354 4.1 Form of the model The form of the model used in this application is slightly different from that shown in Equation 3. When determining whether two noun phrases corefer, the features at each utterance must be considered. For example, if we are to compare the similarity of the gestures that accompany the two noun phrases, it should be the case that gesture is relevant during both time periods. For this reason, we create two hidden variables,"
P07-1045,N04-1018,0,0.029177,"moves up. And it – everything moves up. 1 2 Figure 1: An example where gesture helps to disambiguate meaning. ity combination technique that they consider trains a single classifier with all modalities combined into a single feature vector; this is sometimes called “early fusion.” Shriberg et al. also consider training separate classifiers and combining their posteriors, either through weighted addition or multiplication; this is sometimes called “late fusion.” Late fusion is also employed for gesture-speech combination in (Chen et al., 2004). Experiments in both (Shriberg et al., 2000) and (Kim et al., 2004) find no conclusive winner among early fusion, additive late fusion, and multiplicative late fusion. Toyama and Horvitz (2000) introduce a Bayesian network approach to modality combination for speaker identification. As in late fusion, modalityspecific classifiers are trained independently. However, the Bayesian approach also learns to predict the reliability of each modality on a given instance, and incorporates this information into the Bayes net. While more flexible than the interpolation techniques described in (Shriberg et al., 2000), training modality-specific classifiers separately is s"
P08-1031,P05-1045,0,0.0324168,"awn from a Beta distribution with prior λ0 . We have zd,n ∼ ηd if cd,n = 1, and zd,n ∼ φd otherwise. Finally, the word wd,n is drawn from the multinomial θzd,n , where zd,n indexes a topic-specific language model. Each of the K language models θk is drawn from a symmetric Dirichlet prior θ0 . 5 Posterior Sampling Ultimately, we need to compute the model’s posterior distribution given the training data. Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004). We now present sampling equations for each of the hidden variables in Figure 2. The prior over keyphrase clusters ψ is sampled based on hyperprior ψ0 and keyphrase cluster assignments x. We write p(ψ |. . .) to mean the probability conditioned on all the other variables. p(ψ |. . .) ∝ p(ψ |ψ0 )p(x |ψ)"
P08-1031,P06-1085,0,0.00469492,"ion with prior λ0 . We have zd,n ∼ ηd if cd,n = 1, and zd,n ∼ φd otherwise. Finally, the word wd,n is drawn from the multinomial θzd,n , where zd,n indexes a topic-specific language model. Each of the K language models θk is drawn from a symmetric Dirichlet prior θ0 . 5 Posterior Sampling Ultimately, we need to compute the model’s posterior distribution given the training data. Doing so analytically is intractable due to the complexity of the model, but sampling-based techniques can be used to estimate the posterior. We employ Gibbs sampling, previously used in NLP by Finkel et al. (2005) and Goldwater et al. (2006), among others. This technique repeatedly samples from the conditional distributions of each hidden variable, eventually converging on a Markov chain whose stationary distribution is the posterior distribution of the hidden variables in the model (Gelman et al., 2004). We now present sampling equations for each of the hidden variables in Figure 2. The prior over keyphrase clusters ψ is sampled based on hyperprior ψ0 and keyphrase cluster assignments x. We write p(ψ |. . .) to mean the probability conditioned on all the other variables. p(ψ |. . .) ∝ p(ψ |ψ0 )p(x |ψ), = p(ψ |ψ0 ) L Y p(xℓ |ψ) ℓ"
P08-1031,P06-2063,0,0.0598858,"ments are annotated in a noisy manner. In this work, we apply our method to a collection of reviews in two categories: restaurants and cell phones. The training data consists of review text and the associated pros/cons lists. We then evaluate the ability of our model to predict review properties when the pros/cons list is hidden. Across a variety of evaluation scenarios, our algorithm consistently outperforms alternative strategies by a wide margin. 2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (Popescu et al., 2005; Hu and Liu, 2004; Kim and Hovy, 2006). These methods extract lists of phrases, which are analogous to the keyphrases we use as input to our algorithm. However, our approach is distinguished in two ways: first, we are able to predict keyphrases beyond those that appear verbatim in the text. Second, our approach learns the relationships between keyphrases, allowing us to draw direct comparisons between reviews. Bayesian Topic Modeling One aspect of our model views properties as distributions over words in the document. This approach is inspired by methods in the topic modeling literature, such as Latent Dirichlet Allocation (LDA) ("
P08-1031,H05-2017,0,0.410414,"applicable to many scenarios where documents are annotated in a noisy manner. In this work, we apply our method to a collection of reviews in two categories: restaurants and cell phones. The training data consists of review text and the associated pros/cons lists. We then evaluate the ability of our model to predict review properties when the pros/cons list is hidden. Across a variety of evaluation scenarios, our algorithm consistently outperforms alternative strategies by a wide margin. 2 Related Work Review Analysis Our approach relates to previous work on property extraction from reviews (Popescu et al., 2005; Hu and Liu, 2004; Kim and Hovy, 2006). These methods extract lists of phrases, which are analogous to the keyphrases we use as input to our algorithm. However, our approach is distinguished in two ways: first, we are able to predict keyphrases beyond those that appear verbatim in the text. Second, our approach learns the relationships between keyphrases, allowing us to draw direct comparisons between reviews. Bayesian Topic Modeling One aspect of our model views properties as distributions over words in the document. This approach is inspired by methods in the topic modeling literature, such"
P08-1031,P08-1036,0,0.187105,"topic modeling literature, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), where topics are treated as hidden variables that govern the distribution of words in a text. Our algorithm extends this notion by biasing the induced hidden topics toward a clustering of known keyphrases. Tying these two information sources together enhances the robustness of the hidden topics, thereby increasing 264 the chance that the induced structure corresponds to semantically meaningful properties. Recent work has examined coupling topic models with explicit supervision (Blei and McAuliffe, 2007; Titov and McDonald, 2008). However, such approaches assume that the documents are labeled within a predefined annotation structure, e.g., the properties of food, ambiance, and service for restaurants. In contrast, we address free-text annotations created by end users, without known semantic properties. Rather than requiring a predefined annotation structure, our model infers one from the data. 3 Problem Formulation We formulate our problem as follows. We assume a dataset composed of documents with associated keyphrases. Each document may be marked with multiple keyphrases that express unseen semantic properties. Acros"
P08-1031,J98-3005,0,\N,Missing
P08-1031,W05-1612,0,\N,Missing
P08-1031,H01-1054,0,\N,Missing
P08-1031,C00-2137,0,\N,Missing
P08-1031,D07-1109,0,\N,Missing
P08-1031,P01-1008,1,\N,Missing
P08-1031,P06-1003,0,\N,Missing
P08-1031,W00-0403,0,\N,Missing
P08-1031,J93-3001,0,\N,Missing
P08-1031,W05-0908,0,\N,Missing
P08-1031,N07-1038,1,\N,Missing
P08-1031,J06-4012,0,\N,Missing
P08-1097,P01-1016,0,0.0270445,"t, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriented dialogue (Cassell et al., 2001). These findings are exploited to generate realistic conversational “grounding” behavior in an animated agent. The semantic content of gesture was leveraged – again, for gesture generation – in (Kopp et al., 2007), which presents an animated agent that is capable of augmenting navigation directions with gestures that describe the physical properties of landmarks along the route. Both systems generate plausible and human-like gestural behavior; we address the converse problem of interpreting such gestures. In this vein, hand-coded gesture features have been used to improve sentence segmentation"
P08-1097,P07-1045,1,0.363629,"ures. In this vein, hand-coded gesture features have been used to improve sentence segmentation, show853 ing that sentence boundaries are unlikely to overlap gestures that are in progress (Chen et al., 2006). Features that capture the start and end of gestures are shown to improve sentence segmentation beyond lexical and prosodic features alone. This idea of gestural features as a sort of visual punctuation has parallels in the literature on prosody, which we discuss in the next subsection. Finally, ambiguous noun phrases can be resolved by examining the similarity of co-articulated gestures (Eisenstein and Davis, 2007). While noun phrase coreference can be viewed as a discourse processing task, we address the higher-level discourse phenomenon of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation ha"
P08-1097,J86-3001,0,0.625896,"Missing"
P08-1097,P94-1002,0,0.100677,"tein, Regina Barzilay and Randall Davis Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology 77 Massachusetts Ave., Cambridge MA 02139 {jacobe, regina, davis}@csail.mit.edu Abstract method builds on the idea that coherent discourse segments are characterized by gestural cohesion; in other words, that such segments exhibit homogeneous gestural patterns. Lexical cohesion (Halliday and Hasan, 1976) forms the backbone of many verbal segmentation algorithms, on the theory that segmentation boundaries should be placed where the distribution of words changes (Hearst, 1994). With gestural cohesion, we explore whether the same idea holds for gesture features. This paper explores the relationship between discourse segmentation and coverbal gesture. Introducing the idea of gestural cohesion, we show that coherent topic segments are characterized by homogeneous gestural forms and that changes in the distribution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manua"
P08-1097,H93-1016,0,0.168793,"Missing"
P08-1097,P98-1102,0,0.0451873,"structure. Applying our algorithm to a dataset of faceto-face dialogues, we find that gesture communicates unique information, improving segmentation performance over lexical features alone. The positive impact of gesture is most pronounced when automatically-recognized speech transcripts are used, but gestures improve performance by a significant margin even in combination with manual transcripts. 2 Related Work Gesture and discourse Much of the work on gesture in natural language processing has focused on multimodal dialogue systems in which the gestures and speech may be constrained, e.g. (Johnston, 1998). In contrast, we focus on improving discourse processing on unconstrained natural language between humans. This effort follows basic psychological and linguistic research on the communicative role of gesture (McNeill, 1992; Kendon, 2004), including some efforts that made use of automatically acquired visual features (Quek, 2003). We extend these empirical studies with a statistical model of the relationship between gesture and discourse segmentation. Hand-coded descriptions of body posture shifts and eye gaze behavior have been shown to correlate with topic and turn boundaries in task-oriente"
P08-1097,P06-1004,1,0.866223,"setting for ψ is found via a gradient-based search. This setting is then used to generate another segmentation, and the process is iterated until convergence, as in hard expectationmaximization. The Dirichlet priors on the language models are symmetric, and are chosen via crossvalidation. Sampling or gradient-based techniques may be used to estimate these parameters, but this is left for future work. Relation to other segmentation models Other cohesion-based techniques have typically focused on hand-crafted similarity metrics between sentences, such as cosine similarity (Galley et al., 2003; Malioutov and Barzilay, 2006). In contrast, the model described here is probabilistically motivated, maximizing the joint probability of the segmentation with the observed words and gestures. Our objective criterion is similar in form to that of Utiyama and Isahara (2001); however, in contrast to this prior work, our criterion is justified by a Bayesian approach. Also, while the smoothing in our approach arises naturally from the symmetric Dirichlet prior, Utiyama and Isahara apply Laplace’s rule and add pseudo-counts of one in all cases. Such an approach would be incapable of flexibly balancing the contributions of each"
P08-1097,J97-1005,0,0.0545908,"bution of gestural features predict segment boundaries. Gestural features are extracted automatically from video, and are combined with lexical features in a Bayesian generative model. The resulting multimodal system outperforms text-only segmentation on both manual and automaticallyrecognized speech transcripts. 1 Introduction When people communicate face-to-face, discourse cues are expressed simultaneously through multiple channels. Previous research has extensively studied how discourse cues correlate with lexico-syntactic and prosodic features (Hearst, 1994; Hirschberg and Nakatani, 1998; Passonneau and Litman, 1997); this work informs various text and speech processing applications, such as automatic summarization and segmentation. Gesture is another communicative modality that frequently accompanies speech, yet it has not been exploited for computational discourse analysis. This paper empirically demonstrates that gesture correlates with discourse structure. In particular, we show that automatically-extracted visual features can be combined with lexical cues in a statistical model to predict topic segmentation, a frequently studied form of discourse structure. Our The motivation for this approach comes"
P08-1097,J02-1002,0,0.025458,"lion words. The entire ICSI meeting corpus contains roughly 600,000 words, although only one third of this dataset was annotated for segmentation (Galley et al., 2003). The physics lecture corpus that was mentioned above contains 232,000 words (Malioutov and Barzilay, 2006). The task considered in this section is thus more difficult than much of the previous discourse segmentation work on two dimensions: there is less training data, and a finer-grained segmentation is required. Metrics All experiments are evaluated in terms of the commonly-used Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002) scores. These metrics are penalties, so lower values indicate better segmentations. The Pk metric expresses the probability that any randomly chosen pair of sentences is incorrectly segmented, if they are k sentences apart (Beeferman et al., 1999). Following tradition, k is set to half of the mean segMethod 1. gesture only 2. ASR only 3. ASR + gesture 4. transcript only 5. transcript + gesture 6. random 7. equal-width Pk .486 .462 .388 .382 .332 .473 .508 nificant for both Pk (t(14) = 2.00, p < .05) and WD (t(14) = 1.94, p < .05). WD .502 .476 .401 .397 .349 .526 .515 Table 1: For each method"
P08-1097,P90-1002,0,0.169566,"of topic segmentation. In addition, this prior work focused primarily on pointing gestures directed at pre-printed visual aids. The current paper presents a new domain, in which speakers do not have access to visual aids. Thus pointing gestures are less frequent than “iconic” gestures, in which the form of motion is the principle communicative feature (McNeill, 1992). Non-textual features for topic segmentation Research on non-textual features for topic segmentation has primarily focused on prosody, under the assumption that a key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between ges"
P08-1097,J01-1002,0,0.0204685,"key prosodic function is to mark structure at the discourse level (Steedman, 1990; Grosz and Hirshberg, 1992; Swerts, 1997). The ultimate goal of this research is to find correlates of hierarchical discourse structure in phonetic features. Today, research on prosody has converged on prosodic cues which correlate with discourse structure. Such markers include pause duration, fundamental frequency, and pitch range manipulations (Grosz and Hirshberg, 1992; Hirschberg and Nakatani, 1998). These studies informed the development of applications such as segmentation tools for meeting analysis, e.g. (Tur et al., 2001; Galley et al., 2003). In comparison, the connection between gesture and discourse structure is a relatively unexplored area, at least with respect to computational approaches. One conclusion that emerges from our analysis is that gesture may signal discourse structure in a different way than prosody does: while specific prosodic markers characterize segment boundaries, gesture predicts segmentation through intrasegmental cohesion. The combination of these two modalities is an exciting direction for future research. 3 Visual Features for Discourse Analysis This section describes the process o"
P08-1097,P01-1064,0,0.238092,"ferent priors for the verbal and gestural language models allows us to weight these modalities in a Bayesian framework. Finally, we model the probability of the segmentation z by Q considering the durations of each segment: p(z) = i p(dur(i)|ψ). A negativebinomial distribution with parameter ψ is applied to discourage extremely short or long segments. Inference Crucially, both the likelihood (equation 1) and the prior (equation 2) factor into a product across the segments. This factorization enables the optimal segmentation to be found using a dynamic program, similar to those demonstrated by Utiyama and Isahara (2001) and Malioutov and 856 Barzilay (2006). For each set of segmentation points z, the associated language models are set to their posterior expectations, e.g., θi = E[θ|{xt : zt = i}, α]. The Dirichlet prior is conjugate to the multinomial, so this expectation can be computed in closed form: θi,j = n(i, j) + α , N (i) + W α (3) where n(i, j) is the count of word j in segment i and N (i) is the total number of words in segment i (Bernardo and Smith, 2000). The symmetric Dirichlet prior α acts as a smoothing pseudo-count. In the multimodal context, the priors act to control the weight of each modal"
P08-1097,P03-1071,0,\N,Missing
P08-1097,P06-1003,0,\N,Missing
P08-1097,C98-1099,0,\N,Missing
P08-1097,P07-1094,0,\N,Missing
P11-1137,W10-1757,0,0.0335859,"Missing"
P11-1137,D10-1124,1,0.267148,"Missing"
P11-1137,N04-1039,0,0.241069,"Missing"
P11-1137,W03-1018,0,0.0271586,"Missing"
P11-1137,W04-3223,0,0.0219742,"Missing"
P11-2008,C10-2005,0,0.591162,"toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T"
P11-2008,W10-0713,0,0.446157,"enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate t"
P11-2008,J93-2004,0,0.0664899,"erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. In this paper, we produce an English POS tagger that is designed especially for Twitter data. Our contributions are as follows: • we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we prov"
P11-2008,petrov-etal-2012-universal,1,0.297019,"Missing"
P11-2008,N10-1020,0,0.140487,"yD licenseN and& #2$ notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a"
P11-2008,N10-1100,0,0.180483,"notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour"
P11-2008,N03-1033,0,0.210842,"Missing"
P11-2008,P10-1040,0,0.263127,"butional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ USVT , where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. M ETAPH : Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7 1 α = 100 , C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8 Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (War"
P12-1020,E09-3001,0,0.0289407,"the speech stream, but they are tested on an artificial corpus with only 80 vocabulary items that was constructed so as to “avoid strong word-to-word dependencies” (R¨as¨anen, 2011). Here, we use a naturalistic corpus, demonstrating that lexical-phonetic learning is possible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic in"
P12-1020,W11-0601,0,0.0951404,"Missing"
P12-1020,D08-1113,0,0.214064,"Missing"
P12-1020,P08-1016,0,0.0608466,"sen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these models even maintains an explicit lexicon), nor do they model or learn from phonetic variation in the input. 3 Lexical-phonetic model Our lexical-phonetic model is defined using the standard noisy channel framework: first a sequence of intended word tokens is generated using a language model, and then each token is transformed by a pr"
P12-1020,N06-1041,0,0.0331693,"Missing"
P12-1020,H94-1050,0,0.0377277,"int probability (l, x, r) as p(x)p(l|x)p(r|x) rather than as a leftto-right chain p(l)p(x|l)p(r|x). Given our independence assumption above, these two quantities are mathematically equivalent, so the difference matters only because we are using smoothed estimates. Our factorization leads to a symmetric treatment of left and right contexts, which simplifies implementation: we can store all the context parameters locally as PL (·|x) rather than distributed over various P (x|·). Next, we explain our transducer T . A weighted finite-state transducer (WFST) is a variant of a finitestate automaton (Pereira et al., 1994) that reads an input string symbol-by-symbol and probabilistically produces an output string; thus it can be used to specify a conditional probability on output strings given an input. Our WFST (Figure 3) computes a weighted edit distance, and is implemented using OpenFST (Allauzen et al., 2007). It contains a state for each triplet of (previous, current, next) phones; conditioned on this state, it emits a character output which can be thought of as a possible surface realization of current in its particular environment. The output can be the empty string , in which case current is deleted. T"
P12-1020,W10-2902,0,0.0286415,"nd five manner values (stop, nasal stop, fricative, vowel, other). Empty segments like  and • are assigned a special value “no-value” for all features. 187 Figure 4: Some features generated for (•, D, i) → d. Each black factor node corresponds to a positional template. The features instantiated for the (curr)→out and →out template are shown in full, and we show some of the features for the (curr,next)→out template. tic Optimality Theory models (Goldwater and Johnson, 2003; Hayes and Wilson, 2008). 4 Inference Global optimization of the model posterior is difficult; instead we use Viterbi EM (Spitkovsky et al., 2010; Allahverdyan and Galstyan, 2011). We begin with a simple initial transducer and alternate between two phases: clustering together surface forms, and reestimating the transducer parameters. We iterate this procedure until convergence (when successive clustering phases find nearly the same set of merges); this tends to take about 5 or 6 iterations. In our clustering phase, we improve the model posterior as much as possible by greedily making type merges, where, for a pair of intended word forms u and v, we replace all instances of xi = u with xi = v. We maintain the invariant that each intende"
P12-1020,P06-1124,0,0.323419,"x1 . . . xn . As shown in Figure 2, si is produced from xi by a transducer T : si ∼ T (xi ), which models phonetic changes. Each xi is sampled from a distribution θ which represents word frequencies, and its left and right context words, li and ri , are drawn from distributions conditioned on xi , in order to capture information about the environments in which xi appears: li ∼ PL (xi ), ri ∼ PR (xi ). Because the number of word types is not known in advance, θ is drawn from a Dirichlet process DP (α), and PL (x) and PR (x) have Pitman-Yor priors with concentration parameter 0 and discount d (Teh, 2006). 186 Our generative model of xi is unusual for two reasons. First, we treat each xi independently rather than linking them via a Markov chain. This makes the model deficient, since li overlaps with xi−1 and so forth, generating each token twice. During inference, however, we will never compute the joint probability of all the data at once, only the probabilities of subsets of the variables with particular intended word forms u and v. As long as no two of these words are adjacent, the deficiency will have no effect. We make this independence assumption for computational reasons—when deciding w"
P12-1020,P08-2042,0,0.121566,"ible in this more general setting and that word-level context information is important for doing so. Several other related systems work directly from the acoustic signal and many of these do use naturalistic corpora. However, they do not learn at both the lexical and phonetic/acoustic level. For example, Park and Glass (2008), Aimetti (2009), Jansen et al. (2010), and McInnes and Goldwater (2011) present lexicon-building systems that use hard-coded acoustic similarity measures rather than learning about variability, and they only extract and cluster a few frequent words. On the phonetic side, Varadarajan et al. (2008) and Dupoux et al. (2011) describe systems that learn phone-like units but without the benefit of top-down information. A final line of related work is on word segmentation. In addition to the models mentioned in Section 1, which use phonemic input, a few models of word segmentation have been tested using phonetic input (Fleck, 2008; Rytting, 2007; Daland and Pierrehumbert, 2010). However, they do not cluster segmented Figure 2: Our generative model of the surface tokens s from intended tokens x, which occur with left and right contexts l and r. word tokens into lexical items (none of these mo"
P12-1020,J01-3002,0,0.178409,"iation (many English vowels are reduced to [@] in unstressed positions) and the context—if the next word is “want”, “you” is a plausible choice. To date, most models of infant language learning have focused on either lexicon-building or phonetic learning in isolation. For example, many models of word segmentation implicitly or explicitly build a lexicon while segmenting the input stream of phonemes into word tokens; in nearly all cases the phonemic input is created from an orthographic transcription using a phonemic dictionary, thus abstracting away from any phonetic variability (Brent, 1999; Venkataraman, 2001; Swingley, 2005; Goldwater et al., 2009, among others). As illustrated in Figure 1, these models attempt to infer line (a) from line (d). However, (d) is an idealization: real speech has variability, and behavioral evidence suggests that infants are still learning about the phonetics and phonology of their language even after beginning to segment words, rather than learning to neutralize 184 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184–193, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics the variat"
P14-1002,W05-0613,0,0.124985,"Concatenation DPLP General DPLP TSP 1-1 (Joty, et al., 2013) HILDA (Hernault, et al., 2010) 60 90 K (b) Nuclearity 58 56 54 52 50 150 30 Concatenation DPLP General DPLP TSP 1-1 (Joty, et al., 2013) HILDA (Hernault, et al., 2010) 60 90 K 150 (c) Relation Figure 3: The performance of our parser over different latent dimension K. Results for D PLP include the additional features from Table 3 tences near the root. Learning-based approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence relations at the document level (Baldridge and Lascarides, 2005). Of particular relevance to our inference technique are incremental discourse parsing approaches, such as shift-reduce (Sagae, 2009) and A* (Muller et al., 2012). Prior learning-based work has largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantics (Forbes-Riley et al., 2006) suggests that shallow feature sets may struggle to capture the long tail of alternative lexicalizations that can be used to realize discourse relations (Prasad et al., 2010; Marcu and Echihabi, 2002). Only Subba and Di Eugenio (2009) incorporate"
P14-1002,P12-1091,0,0.0105643,"ntification), more accurate decoding, and special treatment of intra-sentence relations; this is a direction for future research. Discriminative learning of latent features for discourse processing can be viewed as a form of representation learning (Bengio et al., 2013). Also called Deep Learning, such approaches have recently been applied in a number of NLP tasks (Collobert et al., 2011; Socher et al., 2012). Of particular relevance are applications to the detection of semantic or discourse relations, such as paraphrase, by comparing sentences in an induced latent space (Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013). In this work, we show how discourse structure annotations can function as a supervision signal to discriminatively learn a transformation from lexical features to a latent space that is well-suited for discourse parsing. Unlike much of the prior work on representation learning, we induce a simple linear transformation. Extension of our approach by incorporating a non-linear activation function is a natural topic for future research. 7 Acknowledgments We thank the reviewers for their helpful feedback, particularly for the connection to multitask learning. We also wan"
P14-1002,H91-1060,0,0.253786,"Missing"
P14-1002,D13-1090,1,0.381497,"ccurate decoding, and special treatment of intra-sentence relations; this is a direction for future research. Discriminative learning of latent features for discourse processing can be viewed as a form of representation learning (Bengio et al., 2013). Also called Deep Learning, such approaches have recently been applied in a number of NLP tasks (Collobert et al., 2011; Socher et al., 2012). Of particular relevance are applications to the detection of semantic or discourse relations, such as paraphrase, by comparing sentences in an induced latent space (Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013). In this work, we show how discourse structure annotations can function as a supervision signal to discriminatively learn a transformation from lexical features to a latent space that is well-suited for discourse parsing. Unlike much of the prior work on representation learning, we induce a simple linear transformation. Extension of our approach by incorporating a non-linear activation function is a natural topic for future research. 7 Acknowledgments We thank the reviewers for their helpful feedback, particularly for the connection to multitask learning. We also want to thank Kenji Sagae and"
P14-1002,P13-1048,0,0.366599,"discourse-driven projection of surface features. The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank. 1 The projections are in the neighborhood of 50 cents a share to 75 cents, CIRCUMSTANCE compared with a restated $1.65 a share a year earlier, when profit was $107.8 million on sales of $435.5 million. Figure 1: An example of RST discourse structure. et al., 2013). While recent work has introduced increasingly powerful features (Feng and Hirst, 2012) and inference techniques (Joty et al., 2013), discourse relations remain hard to detect, due in part to a long tail of “alternative lexicalizations” that can be used to realize each relation (Prasad et al., 2010). Surface and syntactic features are not capable of capturing what are fundamentally semantic distinctions, particularly in the face of relatively small annotated training sets. In this paper, we present a representation learning approach to discourse parsing. The core idea of our work is to learn a transformation from a bag-of-words surface representation into a latent space in which discourse relations are easily identifiable."
P14-1002,W01-1605,0,0.909268,"tained from MALTParser (Nivre et al., 2007). (11) m where αt is a learning rate. In iteration t, we choose αt = 1t . After convergence, we obtain the weights w by applying the SVM over the entire dataset, using the final A. The algorithm is summarized in Algorithm 1 and more details about implementation will be clarified in Section 4. While minibatch learning requires more iterations, the SVM training is much faster in each batch, and the overall algorithm is several times faster than using the entire training set for each update. 5 Experiments We evaluate D PLP on the RST Discourse Treebank (Carlson et al., 2001), comparing against state-of-the-art results. We also investigate the information encoded by the projection matrix. 5.1 Experimental Setup Dataset The RST Discourse Treebank (RSTDT) consists of 385 documents, with 347 for train17 Feature Examples hB EGIN - WORD -S TACK 1 = buti hB EGIN - WORD -S TACK 1-Q UEUE 1 = but, thei hB EGIN - TAG -S TACK 1 = CCi hB EGIN - TAG -S TACK 1-Q UEUE 1 = CC, DTi hH EAD - WORDS -S TACK 2 = workingi Words at beginning and end of the EDU POS tag at beginning and end of the EDU Head word set from each EDU. The set includes words whose parent in the depenency graph"
P14-1002,W13-3214,0,0.0327677,"lso captures basic intuitions about discourse connectives and verbs, as shown in Figure 4(a). Deep learning approaches typically apply a non-linear transformation such as the sigmoid function (Bengio et al., 2013). We have conducted a few unsuccessful experiments with the “hard tanh” function proposed by Collobert and Weston (2008), but a more complete exploration of non-linear transformations must wait for future work. Another direction would be more sophisticated composition of the surface features within each elementary discourse unit, such as the hierarchical convolutional neural network (Kalchbrenner and Blunsom, 2013) or the recursive tensor network (Socher et al., 2013). It seems likely that a better accounting for syntax could improve the latent representations that our method induces. forming the lexical representation of discourse units into a latent space to facilitate learning. As shown in Figure 4(a), this projection succeeds at grouping words with similar discourse functions. We might expect to obtain further improvements by augmenting this representation learning approach with rich syntactic features (particularly for span identification), more accurate decoding, and special treatment of intra-sen"
P14-1002,P04-1015,0,0.0204899,"en choose either to shift the front of the queue onto the top of the stack, or to reduce the top two elements on the stack in a discourse relation. The reduction operation must choose both the type of relation and which element will be the nucleus. So, overall there are multiple reduce operations with specific relation types and nucleus positions. Shift-reduce parsing can be learned as a classification task, where the classifier uses features of the elements in the stack and queue to decide what move to take. Previous work has employed decision trees (Marcu, 1999) and the averaged perceptron (Collins and Roark, 2004; Sagae, 2009) for this purpose. Instead, we employ a large-margin classifier, because we can compute derivatives of the margin-based objective function with respect to both the classifier weights as well as the projection matrix. 2.2 Discourse parsing with projected features More formally, we denote the surface feature vocabulary V, and represent each EDU as the numeric vector v ∈ NV , where V = #|V |and the nth element of v is the count of the n-th surface feature in this EDU (see Table 1 for a summary of notation). During shift-reduce parsing, we consider features of three EDUs:2 the top tw"
P14-1002,P08-1068,0,0.00707223,"ity. In addition, we show that the latent representation coheres well with the characterization of discourse connectives in the Penn Discourse Treebank (Prasad et al., 2008). Notation V V K wm C 2 vi v A Model The core idea of this paper is to project lexical features into a latent space that facilitates discourse parsing. In this way, we can capture the meaning of each discourse unit, without suffering from the very high dimensionality of a lexical representation. While such feature learning approaches have proven to increase robustness for parsing, POS tagging, and NER (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010), they would seem to have an especially promising role for discourse, where training data is relatively sparse and ambiguity is considerable. Prasad et al. (2010) show that there is a long tail of alternative lexicalizations for discourse relations in the Penn Discourse Treebank, posing obvious challenges for approaches based on directly matching lexical features observed in the training data. Based on this observation, our goal is to learn a function that transforms lexical features into a much lower-dimensional latent representation, while simultaneously learning to pre"
P14-1002,krestel-etal-2008-minding,0,0.00680928,"frequent unigrams in the RST-DT training set. For comparison, we also apply t-SNE to the projection matrix Bnmf recovered from nonnegative matrix factorization. Figure 4 highlights words that are related to discourse analysis. Among the top 1000 words, we highlight the words from 5 major discourse connective categories provided in Appendix B of the PDTB annotation manual (Prasad et al., 2008): C ONJUNCTION, C ONTRAST, P RECEDENCE, R E SULT , and S UCCESSION . In addition, we also highlighted two verb categories from the top 1000 words: modal verbs and reporting verbs, with their inflections (Krestel et al., 2008). From the figure, it is clear D PLP has learned a projection matrix that successfully groups several major discourse-related word classes: particularly modal and reporting verbs; it has also grouped succession and precedence connectives with some success. In contrast, while NMF does obtain compact clusters of words, these clusters appear to be completely unrelated to discourse function of the words that they include. This demonstrates the value of using discriminative training to obtain the transformed representation of the discourse units. 6 Recent work has succeeded in pushing the stateof-t"
P14-1002,D09-1036,0,0.0511698,"ouped succession and precedence connectives with some success. In contrast, while NMF does obtain compact clusters of words, these clusters appear to be completely unrelated to discourse function of the words that they include. This demonstrates the value of using discriminative training to obtain the transformed representation of the discourse units. 6 Recent work has succeeded in pushing the stateof-the-art in RST parsing by innovating on several fronts. Feng and Hirst (2012) explore rich linguistic linguistic features, including lexical semantics and discourse production rules suggested by Lin et al. (2009) in the context of the Penn Discourse Treebank (Prasad et al., 2008). Muller et al. (2012) show that A* decoding can outperform both greedy and graph-based decoding algorithms. Joty et al. (2013) achieve the best prior results on RST relation detection by (i) jointly performing relation detection and classification, (ii) performing bottom-up rather than greedy decoding, and (iii) distinguishing between intra-sentence and inter-sentence relations. Our approach is largely orthogonal to this prior work: we focus on transRelated Work Early work on document-level discourse parsing applied hand-craf"
P14-1002,D10-1113,0,0.0150521,"Missing"
P14-1002,W10-4327,0,0.230176,"Missing"
P14-1002,P12-1007,0,0.0696644,"ection matrix that successfully groups several major discourse-related word classes: particularly modal and reporting verbs; it has also grouped succession and precedence connectives with some success. In contrast, while NMF does obtain compact clusters of words, these clusters appear to be completely unrelated to discourse function of the words that they include. This demonstrates the value of using discriminative training to obtain the transformed representation of the discourse units. 6 Recent work has succeeded in pushing the stateof-the-art in RST parsing by innovating on several fronts. Feng and Hirst (2012) explore rich linguistic linguistic features, including lexical semantics and discourse production rules suggested by Lin et al. (2009) in the context of the Penn Discourse Treebank (Prasad et al., 2008). Muller et al. (2012) show that A* decoding can outperform both greedy and graph-based decoding algorithms. Joty et al. (2013) achieve the best prior results on RST relation detection by (i) jointly performing relation detection and classification, (ii) performing bottom-up rather than greedy decoding, and (iii) distinguishing between intra-sentence and inter-sentence relations. Our approach i"
P14-1002,W09-3813,0,0.0857687,"ourse relations are easily identifiable. The latent representation for each discourse unit can be viewed as a discriminativelytrained vector-space representation of its meaning. Alternatively, our approach can be seen as a nonlinear learning algorithm for incremental structure prediction, which overcomes feature sparsity through effective parameter tying. We consider several alternative methods for transforming the original features, corresponding to different ideas of the meaning and role of the latent representation. Our method is implemented as a shift-reduce discourse parser (Marcu, 1999; Sagae, 2009). Learning is performed as large-margin transitionbased structure prediction (Taskar et al., 2003), while at the same time jointly learning to project the surface representation into latent space. The Introduction Discourse structure describes the high-level organization of text or speech. It is central to a number of high-impact applications, such as text summarization (Louis et al., 2010), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009), question answering (Ferrucci et al., 2010), and automatic evaluation of student writing (Miltsakaki and Kukich, 2004; Burstein et al."
P14-1002,P02-1047,0,0.050602,"ntence relations at the document level (Baldridge and Lascarides, 2005). Of particular relevance to our inference technique are incremental discourse parsing approaches, such as shift-reduce (Sagae, 2009) and A* (Muller et al., 2012). Prior learning-based work has largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantics (Forbes-Riley et al., 2006) suggests that shallow feature sets may struggle to capture the long tail of alternative lexicalizations that can be used to realize discourse relations (Prasad et al., 2010; Marcu and Echihabi, 2002). Only Subba and Di Eugenio (2009) incorporate rich compositional semantics into discourse parsing, but due to the ambiguity of their semantic parser, they must manually select the correct semantic parse from a forest of possiblities. fication for visualization is we consider only the top 1000 frequent unigrams in the RST-DT training set. For comparison, we also apply t-SNE to the projection matrix Bnmf recovered from nonnegative matrix factorization. Figure 4 highlights words that are related to discourse analysis. Among the top 1000 words, we highlight the words from 5 major discourse connec"
P14-1002,P99-1047,0,0.597726,"in which discourse relations are easily identifiable. The latent representation for each discourse unit can be viewed as a discriminativelytrained vector-space representation of its meaning. Alternatively, our approach can be seen as a nonlinear learning algorithm for incremental structure prediction, which overcomes feature sparsity through effective parameter tying. We consider several alternative methods for transforming the original features, corresponding to different ideas of the meaning and role of the latent representation. Our method is implemented as a shift-reduce discourse parser (Marcu, 1999; Sagae, 2009). Learning is performed as large-margin transitionbased structure prediction (Taskar et al., 2003), while at the same time jointly learning to project the surface representation into latent space. The Introduction Discourse structure describes the high-level organization of text or speech. It is central to a number of high-impact applications, such as text summarization (Louis et al., 2010), sentiment analysis (Voll and Taboada, 2007; Somasundaran et al., 2009), question answering (Ferrucci et al., 2010), and automatic evaluation of student writing (Miltsakaki and Kukich, 2004; B"
P14-1002,J00-3005,0,0.0920094,"greedy and graph-based decoding algorithms. Joty et al. (2013) achieve the best prior results on RST relation detection by (i) jointly performing relation detection and classification, (ii) performing bottom-up rather than greedy decoding, and (iii) distinguishing between intra-sentence and inter-sentence relations. Our approach is largely orthogonal to this prior work: we focus on transRelated Work Early work on document-level discourse parsing applied hand-crafted rules and heuristics to build trees in the framework of Rhetorical Structure Theory (Sumita et al., 1992; Corston-Oliver, 1998; Marcu, 2000a). An early data-driven approach was offered by Schilder (2002), who used distributional techniques to rate the topicality of each discourse unit, and then chose among underspecified discourse structures by placing more topical sen20 but although once can will might could would should and when after may report before then later but may when would also can report says said although said reported says saying say until believe think however also Conjunction asked Contrast until Precedence will Result Succession before Modal verb Reporting verb so might and say thus though must asked must then th"
P14-1002,D12-1110,0,0.0431543,"s at grouping words with similar discourse functions. We might expect to obtain further improvements by augmenting this representation learning approach with rich syntactic features (particularly for span identification), more accurate decoding, and special treatment of intra-sentence relations; this is a direction for future research. Discriminative learning of latent features for discourse processing can be viewed as a form of representation learning (Bengio et al., 2013). Also called Deep Learning, such approaches have recently been applied in a number of NLP tasks (Collobert et al., 2011; Socher et al., 2012). Of particular relevance are applications to the detection of semantic or discourse relations, such as paraphrase, by comparing sentences in an induced latent space (Socher et al., 2011; Guo and Diab, 2012; Ji and Eisenstein, 2013). In this work, we show how discourse structure annotations can function as a supervision signal to discriminatively learn a transformation from lexical features to a latent space that is well-suited for discourse parsing. Unlike much of the prior work on representation learning, we induce a simple linear transformation. Extension of our approach by incorporating a"
P14-1002,D13-1170,0,0.00721345,"bs, as shown in Figure 4(a). Deep learning approaches typically apply a non-linear transformation such as the sigmoid function (Bengio et al., 2013). We have conducted a few unsuccessful experiments with the “hard tanh” function proposed by Collobert and Weston (2008), but a more complete exploration of non-linear transformations must wait for future work. Another direction would be more sophisticated composition of the surface features within each elementary discourse unit, such as the hierarchical convolutional neural network (Kalchbrenner and Blunsom, 2013) or the recursive tensor network (Socher et al., 2013). It seems likely that a better accounting for syntax could improve the latent representations that our method induces. forming the lexical representation of discourse units into a latent space to facilitate learning. As shown in Figure 4(a), this projection succeeds at grouping words with similar discourse functions. We might expect to obtain further improvements by augmenting this representation learning approach with rich syntactic features (particularly for span identification), more accurate decoding, and special treatment of intra-sentence relations; this is a direction for future resear"
P14-1002,N04-1043,0,0.0119899,"Missing"
P14-1002,D09-1018,0,0.0112986,"Missing"
P14-1002,N03-1030,0,0.0755581,"0 90 K (a) Span 69 68 67 66 150 F-score F-score F-score 84 83 82 81 80 79 78 77 76 30 65 30 Concatenation DPLP General DPLP TSP 1-1 (Joty, et al., 2013) HILDA (Hernault, et al., 2010) 60 90 K (b) Nuclearity 58 56 54 52 50 150 30 Concatenation DPLP General DPLP TSP 1-1 (Joty, et al., 2013) HILDA (Hernault, et al., 2010) 60 90 K 150 (c) Relation Figure 3: The performance of our parser over different latent dimension K. Results for D PLP include the additional features from Table 3 tences near the root. Learning-based approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence relations at the document level (Baldridge and Lascarides, 2005). Of particular relevance to our inference technique are incremental discourse parsing approaches, such as shift-reduce (Sagae, 2009) and A* (Muller et al., 2012). Prior learning-based work has largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantics (Forbes-Riley et al., 2006) suggests that shallow feature sets may struggle to capture the long tail of alternative lexicalizations that can be used to realize discourse rela"
P14-1002,C12-1115,0,0.105375,"1-1 (Joty, et al., 2013) HILDA (Hernault, et al., 2010) 60 90 K 150 (c) Relation Figure 3: The performance of our parser over different latent dimension K. Results for D PLP include the additional features from Table 3 tences near the root. Learning-based approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence relations at the document level (Baldridge and Lascarides, 2005). Of particular relevance to our inference technique are incremental discourse parsing approaches, such as shift-reduce (Sagae, 2009) and A* (Muller et al., 2012). Prior learning-based work has largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantics (Forbes-Riley et al., 2006) suggests that shallow feature sets may struggle to capture the long tail of alternative lexicalizations that can be used to realize discourse relations (Prasad et al., 2010; Marcu and Echihabi, 2002). Only Subba and Di Eugenio (2009) incorporate rich compositional semantics into discourse parsing, but due to the ambiguity of their semantic parser, they must manually select the correct semantic parse from"
P14-1002,N09-1064,0,0.0632919,"Missing"
P14-1002,D13-1024,0,0.0131035,"e parsing algorithm changes the status of the stack and the queue according the selected transition, then creates the next sample with the updated status. 4.1 Solving the quadratic programming defined by the dual form of the SVM is time-consuming, especially on a large-scale dataset. But if we focus on learning the projection matrix A, we can speed up learning by sampling only a small proportion of the training data to compute an approximate optimum for {w1:C , η1:l,1:C }, before each update of A. This idea is similar to the mini-batch learning, which has been used in large-scale SVM problem (Nelakanti et al., 2013) and deep learning models (Le et al., 2011). Specifically, in iteration t, the algorithm randomly chooses a subset of training samples Dt to train the model. We cannot make a closed-form update to A based on this small sample, but we can take an approximate gradient step, vi ∈SV(Dt ) Parameters and Initialization There are three free parameters in our approach: the latent dimension K, and regularization parameters λ and τ . We consider the values K ∈ {30, 60, 90, 150}, λ ∈ {1, 10, 50, 100} and τ ∈ {1.0, 0.1, 0.01, 0.001}, and search over this space using a development set of thirty document ra"
P14-1002,prasad-etal-2008-penn,0,0.244918,"ntics into discourse parsing, but due to the ambiguity of their semantic parser, they must manually select the correct semantic parse from a forest of possiblities. fication for visualization is we consider only the top 1000 frequent unigrams in the RST-DT training set. For comparison, we also apply t-SNE to the projection matrix Bnmf recovered from nonnegative matrix factorization. Figure 4 highlights words that are related to discourse analysis. Among the top 1000 words, we highlight the words from 5 major discourse connective categories provided in Appendix B of the PDTB annotation manual (Prasad et al., 2008): C ONJUNCTION, C ONTRAST, P RECEDENCE, R E SULT , and S UCCESSION . In addition, we also highlighted two verb categories from the top 1000 words: modal verbs and reporting verbs, with their inflections (Krestel et al., 2008). From the figure, it is clear D PLP has learned a projection matrix that successfully groups several major discourse-related word classes: particularly modal and reporting verbs; it has also grouped succession and precedence connectives with some success. In contrast, while NMF does obtain compact clusters of words, these clusters appear to be completely unrelated to disc"
P14-1002,P10-1040,0,0.0548431,"we show that the latent representation coheres well with the characterization of discourse connectives in the Penn Discourse Treebank (Prasad et al., 2008). Notation V V K wm C 2 vi v A Model The core idea of this paper is to project lexical features into a latent space that facilitates discourse parsing. In this way, we can capture the meaning of each discourse unit, without suffering from the very high dimensionality of a lexical representation. While such feature learning approaches have proven to increase robustness for parsing, POS tagging, and NER (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010), they would seem to have an especially promising role for discourse, where training data is relatively sparse and ambiguity is considerable. Prasad et al. (2010) show that there is a long tail of alternative lexicalizations for discourse relations in the Penn Discourse Treebank, posing obvious challenges for approaches based on directly matching lexical features observed in the training data. Based on this observation, our goal is to learn a function that transforms lexical features into a much lower-dimensional latent representation, while simultaneously learning to predict discourse structu"
P14-1002,C10-2118,0,0.0494507,"edicting relations and nuclearity on the RST Treebank. 1 The projections are in the neighborhood of 50 cents a share to 75 cents, CIRCUMSTANCE compared with a restated $1.65 a share a year earlier, when profit was $107.8 million on sales of $435.5 million. Figure 1: An example of RST discourse structure. et al., 2013). While recent work has introduced increasingly powerful features (Feng and Hirst, 2012) and inference techniques (Joty et al., 2013), discourse relations remain hard to detect, due in part to a long tail of “alternative lexicalizations” that can be used to realize each relation (Prasad et al., 2010). Surface and syntactic features are not capable of capturing what are fundamentally semantic distinctions, particularly in the face of relatively small annotated training sets. In this paper, we present a representation learning approach to discourse parsing. The core idea of our work is to learn a transformation from a bag-of-words surface representation into a latent space in which discourse relations are easily identifiable. The latent representation for each discourse unit can be viewed as a discriminativelytrained vector-space representation of its meaning. Alternatively, our approach ca"
P14-1002,P11-1148,0,0.0230686,"Missing"
P14-1002,W12-1623,0,0.124785,"Missing"
P14-1002,miltsakaki-etal-2004-penn,0,\N,Missing
P14-2044,P10-1132,0,0.0174697,". A review and comparison of older systems is provided by Christodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features."
P14-2044,W13-3520,0,0.0466473,"Missing"
P14-2044,N10-1083,0,0.135339,"Missing"
P14-2044,P06-3002,0,0.0580063,"Missing"
P14-2044,P11-1087,0,0.129669,"Missing"
P14-2044,N06-1041,0,0.0466985,"Missing"
P14-2044,D13-1169,0,0.0235492,"Missing"
P14-2044,D10-1083,0,0.0183087,"hristodoulopoulos et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linea"
P14-2044,W13-3512,0,0.0246285,"be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distanc"
P14-2044,N13-1090,0,0.0350707,"of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model"
P14-2044,petrov-etal-2012-universal,0,0.0558409,"Missing"
P14-2044,D10-1056,1,0.856919,"unction, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear,"
P14-2044,W09-1121,0,0.0417273,"Missing"
P14-2044,D11-1059,1,0.889971,"et al. (2010), who found that imposing a one-tag-per-word-type constraint to reduce model flexibility tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS indu"
P14-2044,D07-1043,0,0.363702,"Missing"
P14-2044,E03-1009,0,0.0278881,"h syntactic function, so that words with similar syntactic functions should have similar distributional properties. In contrast, there can be multiple paradigms for a single morphological inflection (such as past tense in English). But accurate computation of distributional similarity requires large amounts of data, which may not be available for rare words; morphological rules can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and mor"
P14-2044,P12-1020,1,0.853847,"following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from amongst those with the longest"
P14-2044,erjavec-2004-multext,0,0.126496,"clusters. The probability of word w1 following w2 depends on two factors: 1) the distributional similarity between all words in the proposed partition containing w1 and w2 , which is encoded using a Gaussian likelihood function over the word embeddings; and 2) the morphological similarity between w1 and w2 , which acts as a prior distribution on the induced clustering. We use a log-linear model to capture suffix similarities between words, and learn the feature weights by iterating between sampling and weight learning. We apply our model to the English section of the the Multext-East corpus (Erjavec, 2004) in order to evaluate both against the coarse-grained and 265 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 265–271, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics fine-grained tags, where the fine-grained tags encode detailed morphological classes. We find that our model effectively combines morphological features with distributional similarity, outperforming comparable alternative approaches. 2 Related work Unsupervised POS tagging has a long history in NLP. This paper focuses on the"
P14-2044,N12-1045,1,0.820633,"Missing"
P14-2044,E12-1003,0,0.0244364,", the probability of customer i following j is P (ci = j|X, Θ, w, α) ∝ P (fol(i)|Xj , Θ)P (ci = j|w, α). (8) Our non-sequential ddCRP introduces cycles into the follower structure, which are handled in the sampler as described by Socher et al. (2011). Also, the block of customers being moved around can potentially be very large, which makes it easy for the likelihood term to swamp the prior. In practice we found that introducing an additional parameter a (used to exponentiate the prior) improved results— although we report results without this exponent as well. This technique was also used by Titov and Klementiev (2012) and Elsner et al. (2012). Inference also includes optimizing the feature weights for the log-linear model in the ddCRP prior (Titov and Klementiev, 2012). We interleave L-BFGS optimization within sampling, as in Monte Carlo Expectation-Maximization (Wei and Tanner, 1990). We do not apply the exponentiation parameter a when training the weights because this procedure affects the follower structure only, and we do not have to worry about the magnitude of the likelihood. Before the first iteration we initialize the follower structure: for each word, we choose randomly a word to follow from among"
P14-2044,P10-1040,0,0.00921189,"es can be applied to any word regardless of how often it appears. These observations suggest that a general approach to the induction of syntactic categories should leverage both distributional and morphological features (Clark, 2003; Christodoulopoulos et al., 2010). But these features are difficult to combine because of their disparate representations. Distributional information is typically represented in numerical vectors, and recent work has demonstrated the utility of continuous vector representations, or “embeddings” (Mikolov et al., 2013; Luong et al., 2013; Kim and de Marneffe, 2013; Turian et al., 2010). In contrast, morphology is often represented in terms of sparse, discrete features (such as morphemes), or via pairwise measures such as string edit distance. Moreover, the mapping between a surface form and morphology is complex and nonlinear, so that simple metrics such as edit distance will only weakly approximate morphological similarity. In this paper we present a new approach for inducing part-of-speech (POS) classes, combining morphological and distributional information in a non-parametric Bayesian generative model based on the distance-dependent Chinese restaurant process (ddCRP; Bl"
P14-2044,D09-1071,0,0.0418626,"Missing"
P14-2044,D12-1086,0,0.019225,"ty tended to improve system performance; like other recent systems, we impose that constraint here. Recent work also shows that the combination of morphological and distributional information yields the best results, especially cross-linguistically (Clark, 2003; Berg-Kirkpatrick et al., 2010). Since then, most systems have incorporated morphology in some way, whether as an initial step to obtain prototypes for clusters (Abend et al., 2010), or as features in a generative model (Lee et al., 2010; Christodoulopoulos et al., 2011; Sirts and Alum¨ae, 2012), or a representation-learning algorithm (Yatbaz et al., 2012). Several of these systems use a small fixed set of orthographic and/or suffix features, sometimes obtained from an unsupervised morphological segmentation system (Abend et al., 2010; Lee et al., 2010; Christodoulopoulos et al., 2011; Yatbaz et al., 2012). Blunsom and Cohn’s (2011) model learns an n-gram character model over the words in each cluster; we learn a loglinear model, which can incorporate arbitrary features. Berg-Kirkpatrick et al. (2010) also include a log-linear model of morphology in POS induction, but they use morphology in the likelihood term of a parametric sequence model, th"
P14-2068,D12-1072,0,0.0386889,"Missing"
P14-2068,W10-0701,0,0.0251364,"Missing"
P14-2068,N13-1039,0,0.0149382,"insist hear wonder feel state discov forget assert guess observ maintain doubt 0 Table 1: Lemmas of source-introducing predicates (cues) and groups (Saur´ı, 2008). Cue Words Figure 2: Count of cue words in our dataset. Each word is patterned according to its group, as shown in Figure 3. but with reference also to Saur´ı’s (2008) dissertation for cues that are common in Twitter. The Porter Stemmer is applied to match inflections, e.g. denies/denied; for irregular cases not handled by the Porter Stemmer (e.g., forget/forgot), we include both forms. We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al., 2013) to select only instances in the verb sense. Figure 2 shows the distribution of the cues and Figure 3 shows the distribution of the cue groups. For cues that appear in multiple groups, we chose the most common group. • We run the Stanford Dependency parser to obtain labeled dependencies (De Marneffe et al., 2006), requiring that the cue has outgoing edges of the type NSUBJ (noun subject) and CCOMP (clausal complement). The subtree headed by the modifier of the CCOMP relation is considered the claim; the subtree headed by the modifier of the NSUBJ relation is considered the source. See Figure 4"
P14-2068,de-marneffe-etal-2006-generating,0,0.0173134,"Missing"
P14-2068,pareti-2012-database,0,0.0271767,"dataset 2 Text data We gathered a dataset of Twitter messages from 103 professional journalists and bloggers who work in the field of American Politics.1 Tweets were gathered using Twitter’s streaming API, extracting the complete permissible timeline up to February 23, 2014. A total of 959,754 tweets were gathered, and most were written in early 2014. Our interest in this text is specifically in quoted content — including “indirect” quotes, which may include paraphrased quotations, as in the examples in Figure 1. While labeled datasets for such quotes have been created (O’Keefe et al., 2012; Pareti, 2012), these are not freely available at present. In any case, the relevance of these datasets to Twitter text is currently unproven. Therefore, rather than train a supervised model to detect quotations, we apply a simple dependency-based heuristic. • We focus on tweets that contain any member of a list of source-introducing predicates (we borrow the terminology of Pareti (2012) and call this the CUE). Our complete list — shown in Table 1 — was selected mainly from the examples presented by Saur´ı and Pustejovsky (2012), 1 say, report, tell, told, observe, state, accord, insist, assert, claim, main"
P14-2068,D11-1147,0,0.0489767,"use of the word claims in Figure 1, which conveys the author’s doubt about the indirectly quoted content. Detecting and reasoning about the certainty of propositional content has been identified as a key task for information extraction, and is now supported by the FactBank corpus of annotations for newstext (Saur´ı and Pustejovsky, 2009). However, less is known about this phenomenon in social media — a domain whose endemic uncertainty makes proper treatment of factuality even more crucial (Morris et al., 2012). Successful automation of factuality judgments could help to detect online rumors (Qazvinian et al., 2011), and might enable new applications, such as the computation of reliability ratings for ongoing stories. This paper investigates how linguistic resources and extra-linguistic factors affect perceptions of the certainty of quoted information in Twitter. We present a new dataset of Twitter messages that use FactBank predicates (e.g., claim, say, insist) to scope the claims of named entity sources. This dataset was annotated by Mechanical Turk workers who gave ratings for the factuality of the scoped claims in each Twitter message. This enables us to build a predictive model of the factuality ann"
P14-2068,J12-2003,0,0.538242,"Missing"
P14-2068,N09-1057,0,0.0311897,"Missing"
P14-2068,J12-2002,0,0.0598043,"Missing"
P14-2068,D08-1027,0,0.0399424,"Missing"
P14-2068,W06-2915,0,0.098576,"Missing"
P14-2088,W06-1615,0,0.871234,"248194 295154 148061 182208 91582 57477 0 83938 117515 148061 126516 Overall 1480528 625089 34137 84465 130327 115062 115252 0 0 0 0 148519 49194 62387 0 55692 0 60404 0 0 0 0 0 479243 315792 60404 Table 1: Statistics of the Tycho Brahe Corpus CRF tagger We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features (Okazaki, 2007), with SGD optimization. Following the work of Nogueira Dos Santos et al. (2008) on this dataset, we apply the feature set of Ratnaparkhi (1996). There are 16 feature templates and 372, 902 features in total. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. Results Table 2 presents results for different domain adaptation tasks. We also compute the transfer raaccuracy tio, which is defined as adaptation baseline accuracy , shown in Figure 1. The generally positive trend of these graphs indicates that adaptation becomes progressively more important as we select test sets that are more temporally remote from the training data. In general, mDA outperforms SCL and PCA, the latter of which shows little improve"
P14-2088,P07-1056,0,0.697883,"red feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain 538 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 2 High-dimensional setting Structured prediction tasks often have much more features than simple bag-of-words representation, and performance relies on the rare features. In a naive implementation of the denoising approach, both P and Q will be dense matrices with dimensionality d × d, which would be roughly 1011 elements in our experiments. To solve this problem, Chen et"
P14-2088,P09-1056,0,0.0228206,"consider the domain adaptation problem of training on recent data and testing on older historical text. on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. 5 Conclusion and Future Work Denoising autoencoders provide an intuitive solution for domain adaptation: transform the features into a representation that is resistant to the noise that may characterize the domain adaptation process. The original implementation of this idea produced this noise directly (Glorot et al., 2011b); later work showed that dropout noise"
P14-2088,D12-1120,0,0.0961259,"Missing"
P14-2088,P07-1034,0,0.870946,"ject the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspondence Learning (SCL; Blitzer et al., 2006), another feature learning algorithm. In all cases, we include the entire dataset to compute the feature projections; we also conducted experiments using only the test and training data for feature projections, with very similar results. Related Work Domain adaptation Most previous work on domain adaptation focused on the supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused P"
P14-2088,P07-1033,0,0.906734,"Missing"
P14-2088,D07-1041,0,0.198289,"Missing"
P14-2088,N13-1037,1,0.800888,"imple and efficient feature projection. Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial effic"
P14-2088,N09-1068,0,0.174542,"Missing"
P14-2088,N03-1033,0,0.0461648,"et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is sub"
P14-2088,D13-1117,0,0.151336,"ct adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands"
P14-2088,W96-0213,0,\N,Missing
P17-1082,P13-1025,0,0.0611252,"individuals with respect to their interlocutors and readers. We attempt the first large-scale operationalization of stancetaking through computational methods. Du Bois (2007) formalizes stancetaking as a multi-dimensional construct, reflecting the relationship of discourse participants to (a) the audience or interlocutor; (b) the topic of discourse; (c) the talk or text itself. However, the multidimensional nature of stancetaking poses problems for traditional computational approaches, in which labeled data is obtained by relying on annotator intuitions about scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016). Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features. Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009). We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998). We then extend this lexicon to the social media d"
P17-1082,D16-1120,0,0.027628,"ate with social phenomena.2 2 Related Work From a theoretical perspective, we build on prior work on interactional meaning in language. Methodologically, our paper relates to prior work on lexicon-based analysis and contrastive studies of social media communities. 2.1 Linguistic Variation and Social Meaning In computational sociolinguistics (Nguyen et al., 2016), language variation has been studied primarily in connection with macro-scale social variables, such as age (Argamon et al., 2007; Nguyen et al., 2013), gender (Burger et al., 2011; Bamman et al., 2014), race (Eisenstein et al., 2011; Blodgett et al., 2016), and geography (Eisenstein et al., 2010). This parallels what Eckert (2012) has called the “first wave” of language variation studies in sociolinguistics, which also focused on macro-scale variables. More recently, sociolinguists have dedicated increased attention to situational and stylistic variation, and the interactional meaning that such variation can convey (Eckert and Rickford, 2001). This linguistic research can be aligned with computational efforts to quantify phenomena such 2.2 Lexicon-based Analysis Our operationalization of stancetaking is based on the induction of lexicons of sta"
P17-1082,N13-1037,1,0.777232,"tudies on stance, and texts in several genres of English. This list includes certainty adverbs (e.g., actually, of course, in fact), affect markers (e.g., amazing, thankful, sadly), and hedges (e.g., kind of, maybe, something like) among other adverbial, adjectival, verbal, and modal markers of stance. In total, this list consists of 448 stance markers. The Biber and Finegan (1989) lexicon is primarily based on written genres from the pre-social media era. Our dataset — like much of the recent work in this domain — consists of online discussions, which differ significantly from printed texts (Eisenstein, 2013). One difference is that online discussions contain a number of dialog act markers that are characteristic of spoken language, such as oh yeah, nah, wow. We accounted for this by adding 74 dialog act markers from the Switchboard Dialog Act Corpus (Jurafsky et al., 1998). The final seed lexicon consists of 517 unique markers, from these two sources. Note that the seed lexicon also includes markers that contain multiple tokens (e.g. kind of, I know). 4.2 Seed term Expanded terms (Example seeds from Biber and Finegan (1989)) significantly certainly incredibly considerably, substantially, dramatic"
P17-1082,D10-1124,1,0.80503,"Missing"
P17-1082,W10-0701,0,0.0952993,"Missing"
P17-1082,D16-1057,0,0.0337244,"iber and Finegan (1989), and 112 additional markers based on the seed list of dialog acts. In total, our stance lexicon contains 812 unique markers. Lexicon expansion Online discussions differ not only from written texts, but also from spoken discussions, due to their use of non-standard vocabulary and spellings. To measure stance accurately, these genre differences must be accounted for. We therefore expanded the seed lexicon using automated techniques based on distributional statistics. This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016). Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set. We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD 2 VEC (Mikolov et al., 2013) and WANG 2 VEC (Ling et al., 2015) with default parameters. The WANG 2 VEC method augments WORD 2 VEC by accounting for word order information. We found the similarity judgments obtained from WANG 2 VEC to be qualitatively more meaningful, so we used these embeddings"
P17-1082,P97-1023,0,0.594042,"ity to items in the seed list from Biber and Finegan (1989), and 112 additional markers based on the seed list of dialog acts. In total, our stance lexicon contains 812 unique markers. Lexicon expansion Online discussions differ not only from written texts, but also from spoken discussions, due to their use of non-standard vocabulary and spellings. To measure stance accurately, these genre differences must be accounted for. We therefore expanded the seed lexicon using automated techniques based on distributional statistics. This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016). Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set. We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD 2 VEC (Mikolov et al., 2013) and WANG 2 VEC (Ling et al., 2015) with default parameters. The WANG 2 VEC method augments WORD 2 VEC by accounting for word order information. We found the similarity judgments obtained from WANG 2 VEC to be qualitatively more meaningful, so w"
P17-1082,C94-1049,0,0.0303205,"vidual authors, and the resulting dimensions appeared to be less stylistically coherent. Singular value decomposition is often used in combination with a transformation of the cooccurrence counts by pointwise mutual information (Bullinaria and Levy, 2007). This transformation ensures that each cell in the matrix indicates how much more likely a stance marker is to cooccur with a given subreddit than would happen by chance under an independence assumption. Because negative PMI values tend to be unreliable, we use positive PMI (PPMI), which involves replacing all negative PMI values with zeros (Niwa and Nitta, 1994). Therefore, we obtain stance dimensions by applying singular value decomposition to the matrix constructed as follows:   Pr(marker = m, subreddit = s) Xm,s = log . Pr(marker = m) Pr(subreddit = s) + tre e s 0.00 food photos hopb attle s his tory politic s 0.01 g one wild worldne ws (-) 5.1 0.02 ns fw fac e palm athe is m 0.04 0.03 (-) 0.02 aww 4c han 0.01 Dim-2 funny 0.00 0.01 0.02 0.03 (+) Figure 1: Mapping of subreddits in dimension two and dimension three, highlighting especially popular subreddits. Picture-oriented subreddits r/gonewild and r/aww map high on dimension two and low on dim"
P17-1082,W98-0319,0,0.725698,"ut scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016). Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features. Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009). We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998). We then extend this lexicon to the social media domain using word embeddings. Finally, we apply multi-dimensional analysis of co-occurrence patterns to identify a small set of stance dimensions. To measure the internal coherence (construct validity) of the stance dimensions, we use a word The sociolinguistic construct of stancetaking describes the activities through which discourse participants create and signal relationships to their interlocutors, to the topic of discussion, and to the talk itself. Stancetaking underlies a wide range of interactional phenomena, relating to formality, polit"
P17-1082,Q16-1005,0,0.407053,"readers. We attempt the first large-scale operationalization of stancetaking through computational methods. Du Bois (2007) formalizes stancetaking as a multi-dimensional construct, reflecting the relationship of discourse participants to (a) the audience or interlocutor; (b) the topic of discourse; (c) the talk or text itself. However, the multidimensional nature of stancetaking poses problems for traditional computational approaches, in which labeled data is obtained by relying on annotator intuitions about scalar concepts such politeness (Danescu-Niculescu-Mizil et al., 2013) and formality (Pavlick and Tetreault, 2016). Instead, our approach is based on a theoretically-guided application of unsupervised learning, in the form of factor analysis, applied to lexical features. Stancetaking is characterized in large part by an array of linguistic features ranging from discourse markers such as actually to backchannels such as yep (Kiesling, 2009). We therefore first compile a lexicon of stance markers, combining prior lexicons from Biber and Finegan (1989) and the Switchboard Dialogue Act Corpus (Jurafsky et al., 1998). We then extend this lexicon to the social media domain using word embeddings. Finally, we app"
P17-1082,N12-1057,0,0.100599,"al., 2009) and a set of preregistered hypotheses. To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations. A predictive evaluation shows that the membership of online communities is determined in part by the interactional stances that predominate in those communities. Furthermore, the induced stance dimensions are shown to align with annotations of politeness and formality. as subjectivity (Riloff and Wiebe, 2003), sentiment (Wiebe et al., 2005), politeness (DanescuNiculescu-Mizil et al., 2013), formality (Pavlick and Tetreault, 2016), and power dynamics (Prabhakaran et al., 2012). While linguistic research on interactional meaning has focused largely on qualitative methodologies such as discourse analysis (e.g., Bucholtz and Hall, 2005), these computational efforts have made use of crowdsourced annotations to build large datasets of, for example, polite and impolite text. These annotation efforts draw on the annotators’ intuitions about the meaning of these sociolinguistic constructs. Interpersonal stancetaking represents an attempt to unify concepts such as sentiment, politeness, formality, and subjectivity under a single theoretical framework (Jaffe, 2009; Kiesling,"
P17-1082,N15-1142,0,0.0125125,"ust be accounted for. We therefore expanded the seed lexicon using automated techniques based on distributional statistics. This is similar to prior work on the expansion of sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Hamilton et al., 2016). Our lexicon expansion approach used word embeddings to find words that are distributionally similar to those in the seed set. We trained word embeddings on a corpus of 25 million Reddit comments and a vocabulary of 100K most frequent words on Reddit using the structured skip-gram models of both WORD 2 VEC (Mikolov et al., 2013) and WANG 2 VEC (Ling et al., 2015) with default parameters. The WANG 2 VEC method augments WORD 2 VEC by accounting for word order information. We found the similarity judgments obtained from WANG 2 VEC to be qualitatively more meaningful, so we used these embeddings to construct the expanded lexicon.5 5 Linguistic Dimensions of Stancetaking To summarize the main axes of variation across the lexicon of stance markers, we apply a multidimensional analysis (Biber, 1992) to the distributional statistics of stance markers across subreddit communities. Each dimension of variation can then be viewed as a spectrum, characterized by t"
P17-1082,W03-1014,0,0.0624036,"ges 884–895 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1082 intrusion task (Chang et al., 2009) and a set of preregistered hypotheses. To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations. A predictive evaluation shows that the membership of online communities is determined in part by the interactional stances that predominate in those communities. Furthermore, the induced stance dimensions are shown to align with annotations of politeness and formality. as subjectivity (Riloff and Wiebe, 2003), sentiment (Wiebe et al., 2005), politeness (DanescuNiculescu-Mizil et al., 2013), formality (Pavlick and Tetreault, 2016), and power dynamics (Prabhakaran et al., 2012). While linguistic research on interactional meaning has focused largely on qualitative methodologies such as discourse analysis (e.g., Bucholtz and Hall, 2005), these computational efforts have made use of crowdsourced annotations to build large datasets of, for example, polite and impolite text. These annotation efforts draw on the annotators’ intuitions about the meaning of these sociolinguistic constructs. Interpersonal st"
P17-1082,D13-1010,0,0.15238,"appropriate when there is some notion of a correct answer. As stancetaking is a multidimensional concept, we have taken an unsupervised approach. Therefore, we use evaluation techniques based on the notion of validity, which is the extent to which the operationalization of a construct truly captures the intended quantity or concept. Validation techniques for unsupervised content analysis are widely found in the social science literature (Weber, 1990; Quinn et al., 2010) and have also been recently used in the NLP and machine learning communities (e.g., Chang et al., 2009; Murphy et al., 2012; Sim et al., 2013). We used several methods to validate the stance dimensions extracted from the corpus of Reddit comments. This section describes intrinsic evaluations, which test whether the extracted stance dimensions are linguistically coherent and meanResults: Stance Dimensions From the SVD analysis, we extracted the six principal latent dimensions that explain the most variation in our dataset.7 The decision to include only the first six dimensions was based on the strength of the singular values corresponding to the dimensions. Table 3 shows the top five stance markers for each extreme of the six dimensi"
P17-1082,C12-1118,0,0.0654126,"ndard annotations is appropriate when there is some notion of a correct answer. As stancetaking is a multidimensional concept, we have taken an unsupervised approach. Therefore, we use evaluation techniques based on the notion of validity, which is the extent to which the operationalization of a construct truly captures the intended quantity or concept. Validation techniques for unsupervised content analysis are widely found in the social science literature (Weber, 1990; Quinn et al., 2010) and have also been recently used in the NLP and machine learning communities (e.g., Chang et al., 2009; Murphy et al., 2012; Sim et al., 2013). We used several methods to validate the stance dimensions extracted from the corpus of Reddit comments. This section describes intrinsic evaluations, which test whether the extracted stance dimensions are linguistically coherent and meanResults: Stance Dimensions From the SVD analysis, we extracted the six principal latent dimensions that explain the most variation in our dataset.7 The decision to include only the first six dimensions was based on the strength of the singular values corresponding to the dimensions. Table 3 shows the top five stance markers for each extreme"
P17-1082,J16-3007,0,0.147488,"Missing"
P17-1082,D16-1108,0,0.0274653,"ia platforms such as Reddit, Stack Exchange, and Wikia can be considered multicommunity environments, in that they host multiple subcommunities with distinct social and linguistic properties. Such subcommunities can be contrasted in terms of topics (Adamic et al., 2008; Hessel et al., 2014) and social networks (Backstrom et al., 2006). Our work focuses on Reddit, emphasizing community-wide differences in norms for interpersonal interaction. In the same vein, Tan and Lee (2015) attempt to characterize stylistic differences across subreddits by focusing on very common words and parts-of-speech; Tran and Ostendorf (2016) use language models and topic models to measure similarity across threads within a subreddit. One distinction of our approach is that the use of multidimensional analysis gives us interpretable dimensions of variation. This makes it possible to identify the specific interpersonal features that vary across communities. 3 U2 : “Definitely the beard. But keep it trimmed.” The phrases in bold face are markers of stance, indicating a evaluative stance. The following example is a part of a thread in the subreddit r/photoshopbattles where users discuss an edited image posted by the original poster O"
P17-1082,N12-1072,0,0.050961,"uction What does it mean to be welcoming or standoffish, light-hearted or cynical? Such interactional styles are performed primarily with language, yet little is known about how linguistic resources are arrayed to create these social impressions. The sociolinguistic concept of interpersonal stancetaking attempts to answer this question, by providing a conceptual framework that accounts for a range of interpersonal phenomena, subsuming formality, politeness, and subjectivity (Du Bois, 2007).1 This 1 Stancetaking is distinct from the notion of stance which corresponds to a position in a debate (Walker et al., 2012). Similarly, Freeman et al. (2014) correlate phonetic features with the strength of such argumentative stances. 884 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 884–895 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1082 intrusion task (Chang et al., 2009) and a set of preregistered hypotheses. To measure the utility of the stance dimensions, we perform a series of extrinsic evaluations. A predictive evaluation shows that the membership of online communities is deter"
P17-1082,D11-1120,0,\N,Missing
P19-1593,D16-1053,0,0.0312085,"ison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a core"
P19-1593,P15-1136,0,0.179541,"Missing"
P19-1593,N19-1423,0,0.0266509,"ellelism+URL). Language model pretraining yields an absolute gain of 3.2 in F1 . This demonstrates the ability of RefReader to leverage unlabeled text, which is a distinctive feature in comparison with prior work. When training is carried out in the unsupervised setting (with the language modeling objective only), the model is still capable of learning the latent coreferential structure between pronouns and names to some extent, outperforming a supervised coreference system that gives competitive results on OntoNotes (Clark and Manning, 2015). We also test a combination of RefReader and BERT (Devlin et al., 2019), using BERT’s contextualized word embeddings as base features xt (concatenation of the top 4 layers), which yields substantial improvements in accuracy. While this model still resolves references incrementally, it cannot be said to be purely incremental, because BERT uses “future” information to build its contextualized embeddings.5 Note that the gender 5 Future work may explore the combination of RefReader bias increases slightly, possibly due to bias in the data used to train BERT. GAP examples are short, containing just a few entity mentions. To test the applicability of our method to long"
P19-1593,N18-2007,0,0.0650583,"is a learnable vector. Figure 2: Overview of the model architecture. (i) (i) value vt ∈ RDv , and a salience st ∈ [0, 1]. There are two components to the model: the memory unit, which stores and tracks the states of the entities in the text; and the recurrent unit, which controls the memory via a set of gates. An overview is presented in Figure 2. 2.1 Recurrent Unit The recurrent unit is inspired by the CoreferentialGRU, in which the current hidden state of a gated recurrent unit (GRU; Chung et al., 2014) is combined with the state at the time of the most recent mention of the current entity (Dhingra et al., 2018). However, instead of relying on the coreferential structure to construct a dynamic computational graph, we use an external memory unit to keep track of previously mentioned entities and let the model learn to decide what to store in each cell. The memory state is summarized by the P (i) (i) weighted sum over values: mt = N i=1 s vt . The current hidden state and the input are ˜t = combined into a pre-recurrent state h tanh(W ht−1 + U xt ), which is used to control the memory operations; the matrices W and U are trained parameters. To compute the next hidden state ht , we perform a recurrent u"
P19-1593,J95-2003,0,0.139821,"update (indicating coreference with Padbury), and a weaker update to m1. If the update to m0 is above the threshold, then the reader may receive credit for this coreference edge, which would otherwise be scored as a false negative. The reader ignores the names Braylon Edwards, Piers Haggard, and Cathy Vespers, leaving them out of the memory. Edwards and Vespers appear in prepositional phrases, while Haggard is a possessive determiner of the object of a prepositional phrase. Centering theory argues that these syntactic positions have low salience in comparison with subject and object position (Grosz et al., 1995). It is possible that the reader has learned this principle, and that this is why it chooses not to store these names in memory. However, the reader also learns from the GAP supervision that pronouns are important, and therefore stores the pronoun his even though it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to"
P19-1593,D17-1195,0,0.0335153,"t, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This paper demonstrates the viability of incremental reference resolution, using an end-to-end differentiable memory network. This enables semisupervised learning from a language modeling objective, which substantially improves performance. A key question for future work is the performance on longer texts, such as the full-length news articles encountered in"
P19-1593,I17-1048,0,0.0201279,"sive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabeled data. 5 Conclusion This"
P19-1593,D17-1018,1,0.869104,"o3 Ismael told (2) u4 Captain Ahab (2) o7 he saw Moby-Dick self link coreferential not coreferential Figure 1: A referential reader with two memory cells. (i) (i) Overwrite and update are indicated by ot and ut ; in practice, these operations are continuous gates. Thickness and color intensity of edges between memory cells at neighboring steps indicate memory salience; 7 indicates an overwrite. Introduction Reference resolution is fundamental to language understanding. Current state-of-the-art systems employ the mention-pair model, in which a classifier is applied to all pairs of spans (e.g., Lee et al., 2017). This approach is expensive in both computation and labeled data, and it is also cognitively implausible: human readers interpret text in a nearly online fashion (Tanenhaus et al., 1995). We present a new method for reference resolution, which reads the text left-to-right while storing entities in a fixed-size working memory (Figure 1). As each token is encountered, the reader must decide whether to: (a) link the token to an existing memory, thereby creating a coreference link, (b) overwrite an existing memory and store a new entity, or (c) disregard the token and move ahead. As memories are"
P19-1593,D14-1162,0,0.0808162,"Missing"
P19-1593,W09-3905,0,0.0353313,"et of pronoun-name anaphora demonstrates strong performance with purely incremental text processing. Work carried out as an intern at Facebook AI Research ston et al., 2015), in which memory operations are differentiable, enabling end-to-end training from gold anaphora resolution data. Furthermore, the memory can be combined with a recurrent hidden state, enabling prediction of the next word. This makes it possible to train the model from unlabeled data using a language modeling objective. To summarize, we present a model that processes the text incrementally, resolving references on the fly (Schlangen et al., 2009). The model yields promising results on the GAP dataset of pronoun-name references.1 2 Model For a given document consisting of a sequence of tokens {wt }Tt=1 , we represent text at two levels: • Tokens: represented as {xt }Tt=1 , where the vector xt ∈ RDx is computed from any token-level encoder. • Entities: represented by a fixed-length mem(i) (i) (i) ory Mt = {(kt , vt , st )}N i=1 , where each (i) memory is a tuple of a key kt ∈ RDk , a 1 Code available at: liufly/refreader https://github.com/ 5918 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pag"
P19-1593,Q18-1042,0,0.267101,"f links). P The Pcoreference loss is then the cross-entropy Ti=1 Tj=i+1 H(ψˆi,j , yi,j ). Because the hidden state ht is computed recurrently from w1:t , the reader can also be trained from a language modeling objective, even when coreference annotations are unavailable. Word probabilities P (wt+1 |ht ) are computed by projecting the hidden state ht by a matrix of output embeddings, and applying the softmax operation. 3 Experiments As an evaluation of the ability of the referential reader to correctly track entity references in text, we evaluate against the GAP dataset, recently introduced by Webster et al. (2018). Each instance consists of: (1) a sequence of tokens w1 , . . . , wT extracted from Wikipedia biographical pages; (2) two person names (A and B, whose token index spans are denoted sA and sB ); (3) a single-token pronoun (P with the token index sP ); and (4) two binary labels (yA and yB ) indicating whether P is referring to A or B. Language modeling. Given the limited size of GAP, it is difficult to learn a strong recurrent model. We therefore consider the task of language modeling as a pre-training step. We make use of the page text of the original Wikipedia articles from GAP, the URLs to w"
P19-1593,D17-1197,0,0.0309738,"it is also a possessive determiner. 4 Related Work Memory networks provide a general architecture for online updates to a set of distinct memories (Weston et al., 2015; Sukhbaatar et al., 2015). The link between memory networks and incremental text processing was emphasized by Cheng et al. (2016). Henaff et al. (2017) used memories to track the states of multiple entities in a text, but they predefined the alignment of entities to memories, rather than learning to align entities with memories using gates. The incorporation of entities into language models has also been explored in prior work (Yang et al., 2017; Kobayashi et al., 2017); similarly, Dhingra et al. (2018) augment the gated recurrent unit (GRU) architecture with additional edges between coreferent mentions. In general, this line of prior work assumes that coreference information is available at test time (e.g., from a coreference resolution system), rather than determining coreference in an online fashion. Ji et al. (2017) propose a generative entity-aware language model that incorporates coreference as a discrete latent variable. For this reason, importance sampling is required for inference, and the model cannot be trained on unlabele"
Q15-1024,P14-1023,0,0.00516548,"ics from throughout the sentence into the entity span, using our up-down compositional procedure. In recent work, Rutherford and Xue (2014) take an alternative approach, using features that represent whether coreferent mentions are arguDistributional semantics begins with the hypothesis that words and phrases that tend to appear in the same contexts have the same meaning (Firth, 1957). The current renaissance of interest in distributional and distributed semantics can be attributed in part to the application of discriminative techniques, which emphasize predictive models (Bengio et al., 2006; Baroni et al., 2014b), rather than context-counting and matrix factorization (Landauer et al., 1998; Turney and Pantel, 2010). Recent work has made practical the idea of propagating distributed information through linguistic structures (Smolensky, 1990; Collobert et al., 2011). In such models, the distributed representations and compositional operators can be fine-tuned by backpropagating supervision from task-specific labels, enabling accurate and fast models for a wide range of language technologies (Socher et al., 2011; Socher et al., 2013; Chen and Manning, 2014). Of particular relevance is recent work on tw"
Q15-1024,J08-1001,0,0.0778525,"Missing"
Q15-1024,P13-2013,0,0.757246,"es. A third level of subtypes is defined for only some types, specifying the semantic contribution of each argument. There are two main approaches to evaluating implicit discourse relation classification. Multiclass classification requires identifying the discourse relation from all possible choices. This task was explored by Lin et al. (2009), who focus on second-level discourse relations. More recent work has emphasized binary classification, where the goal is to build and evaluate separate “one-versus-all” classifiers for each discourse relation (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013). We primarily focus on multiclass classification, because it is more relevant for the ultimate goal of building a PDTB parser; however, to compare with recent prior work, we also evaluate on binary relation classification. 335 6.1 Multiclass classification Our main evaluation involves predicting the correct discourse relation for each argument pair, from among the second-level relation types. The training and test set construction follows Lin et al. (2009) with a few changes: • We use sections 2-20 of the PDTB as a training set, sections 0-1 as a development set for parameter tuning, and sect"
Q15-1024,D12-1050,0,0.0575963,"Missing"
Q15-1024,D14-1082,0,0.0109152,"Missing"
Q15-1024,W01-0514,0,0.0252928,"e different: rather than inducing distributed representations of entity mentions, the goal of this work is to support an infinite-order generative model of dependency parsing. While Le and Zuidema apply this idea as a generative reranker within a supervised dependency parsing framework, we are interested to explore whether it could be employed to do unsupervised syntactic analysis, which could substitute for the supervised syntactic parser in our system. The application of distributional and distributed semantics to discourse includes the use of latent semantic analysis for text segmentation (Choi et al., 2001) and coherence assessment (Foltz et al., 1998), as well as paraphrase detection by the factorization of matrices of distributional counts (Kauchak and Barzilay, 2006; Mihalcea et al., 2006). These approaches essentially compute a distributional representation in advance, and then use it alongside other features. In contrast, our approach follows more recent work in which the distributed representation is driven by supervision from discourse annotations. For example, Ji and Eisenstein (2014) show that RST parsing can be performed by learning task-specific word representations, which perform con"
Q15-1024,P98-1044,0,0.294551,"Missing"
Q15-1024,D13-1203,0,0.0345972,"Missing"
Q15-1024,P03-1054,0,0.0128828,"0. For the composition parameters, we follow Bengio (2012) and initialize U and V with p uniform random values drawn from p the range [− 6/2K, 6/2K]. Word representations We trained a word2vec model (Mikolov et al., 2013) on the PDTB corpus, standardizing the induced representations to zeromean, unit-variance (LeCun et al., 2012). Experiments with pre-trained GloVe word vector representations (Pennington et al., 2014) gave broadly similar results. Syntactic structure Our model requires that the syntactic structure for each argument is represented as a binary tree. We run the Stanford parser (Klein and Manning, 2003) to obtain constituent parse trees of each sentence in the PDTB, and binarize all resulting parse trees. Argument spans in the Penn Discourse Treebank need not be sentences or syntactic constituents: they can include multiple sentences, non-constituent spans, and even discontinuous spans (Prasad et al., 2008). In all cases, we identify the syntactic subtrees within the argument span, and unify them in a right branching superstructure. Coreference The impact of entity semantics on discourse relation detection is inherently limited by two factors: (1) the frequency with which the 334 Dataset Ann"
Q15-1024,D14-1081,0,0.0123669,"its parent, and from its own upward representation. A key difference in our approach is that the siblings in a production are more directly connected: the upward representation of a given node is used to compute the downward representation of its sibling, similar to the inside-outside algorithm. In the models of Paulus et al. (2014) and ˙Irsoy and Cardie (2013), the connection between siblings nodes is less direct, as it is channeled through the representation of the parent node. From this perspective, the most closely related prior work is the Inside-Outside Recursive Neural Network (Le and Zuidema, 2014), published shortly before this paper was submitted. The compositional procedure in this paper is identical, although the application is quite different: rather than inducing distributed representations of entity mentions, the goal of this work is to support an infinite-order generative model of dependency parsing. While Le and Zuidema apply this idea as a generative reranker within a supervised dependency parsing framework, we are interested to explore whether it could be employed to do unsupervised syntactic analysis, which could substitute for the supervised syntactic parser in our system."
Q15-1024,S13-1001,0,0.0246059,"representations, and towards more traditional logical representations of meaning. In this sense, our approach is “bottom-up”, as we try to add a small amount of logical formalism to distributed representations; other approaches are “top-down”, softening purely logical representations by using distributional clustering (Poon and Domingos, 2009; Lewis and Steedman, 2013) or Bayesian non-parametrics (Titov and Klementiev, 2011) to obtain types for entities and relations. Still more ambitious would be to implement logical semantics within a distributed compositional framework (Clark et al., 2011; Grefenstette, 2013). At present, these combinations of logical and distributed semantics have been explored only at the sentence level. In generalizing such approaches to multi-sentence discourse, we argue that it will not be sufficient to compute distributed representations of sentences: a multitude of other elements, such as entities, will also have to represented. 8 Conclusion Discourse relations are determined by the meaning of their arguments, and progress on discourse parsing therefore requires computing representations of the argument semantics. We present a compositional method for inducing distributed r"
Q15-1024,J95-2003,0,0.832335,"relations p  .001, not significantly more accurate on the E XPANSION relation, and significantly less accurate than the Park and Cardie (2012) system on the C OMPARISON relation at p  .001. 7 Related Work This paper draws on previous work in discourse relation detection and compositional distributed semantics. 7.1 Discourse relations Many models of discourse structure focus on relations between spans of text (Knott, 1996), including rhetorical structure theory (RST; Mann and Thompson, 1988), lexicalized tree-adjoining grammar for discourse (D-LTAG; Webber, 2004), and even centering theory (Grosz et al., 1995), which posits relations such as CONTINUATION and SMOOTH SHIFT between adjacent spans. Consequently, the automatic identification of discourse relations has long been considered a key component of discourse parsing (Marcu, 1999). We work within the D-LTAG framework, as annotated in the Penn Discourse Treebank (PDTB; Prasad et al., 2008), with the task of identifying implicit discourse relations. The seminal work in this task is from Pitler et al. (2009) and Lin et al. (2009). Pitler et al. (2009) focus on lexical features, including linguistically motivated word groupings such as Levin verb cl"
Q15-1024,P14-1092,0,0.190409,"Missing"
Q15-1024,P14-1002,1,0.8724,"unction is modified as, (m) &gt; ψ(y) = (u0 (n) ) Ay u0 + X (m) &gt; (di (n) ) By dj i,j∈A(m,n) + β&gt; y φ(m,n) + by , (5) where β y is the classification weight on surface features for relation y. We describe these features in Section 5. 4 Large-margin learning framework There are two sets of parameters to be learned: the classification parameters θ class = {Ay , By , β y , by }y∈Y , and the composition parameters θ comp = {U, V}. We use pre-trained word representations, and do not update them. While prior work shows that it can be advantageous to retrain word representations for discourse analysis (Ji and Eisenstein, 2014), our preliminary experiments found that updating the word representations led to serious overfitting in this model. Following Socher et al. (2011), we define a large margin objective, and use backpropagation to learn all parameters of the network jointly (Goller and Kuchler, 1996). Learning is performed using stochastic gradient descent (Bottou, 1998), so we present the learning problem for a single argument pair (m, n) with the gold discourse relation y ∗ . The objective function for this training example is a regularized hinge loss, L(θ) = X y 0 :y 0 6=y ∗   max 0, 1 − ψ(y ∗ ) + ψ(y 0 ) +"
Q15-1024,W13-3214,0,0.0364958,"ially compute a distributional representation in advance, and then use it alongside other features. In contrast, our approach follows more recent work in which the distributed representation is driven by supervision from discourse annotations. For example, Ji and Eisenstein (2014) show that RST parsing can be performed by learning task-specific word representations, which perform considerably better than generic word2vec representations (Mikolov et al., 340 2013). Li et al. (2014) propose a recursive neural network approach to RST parsing, which is similar to the upward pass in our model, and Kalchbrenner and Blunsom (2013) show how a recurrent neural network can be used to identify dialogue acts. However, prior work has not applied these ideas to the classification of implicit relations in the PDTB, and does not consider the role of entities. As we argue in the introduction, a single vector representation is insufficiently expressive, because it obliterates the entity chains that help to tie discourse together. More generally, our entity-augmented distributed representation can be viewed in the context of recent literature on combining distributed and formal semantics: by representing entities, we are taking a"
Q15-1024,N06-1058,0,0.0256203,"ependency parsing. While Le and Zuidema apply this idea as a generative reranker within a supervised dependency parsing framework, we are interested to explore whether it could be employed to do unsupervised syntactic analysis, which could substitute for the supervised syntactic parser in our system. The application of distributional and distributed semantics to discourse includes the use of latent semantic analysis for text segmentation (Choi et al., 2001) and coherence assessment (Foltz et al., 1998), as well as paraphrase detection by the factorization of matrices of distributional counts (Kauchak and Barzilay, 2006; Mihalcea et al., 2006). These approaches essentially compute a distributional representation in advance, and then use it alongside other features. In contrast, our approach follows more recent work in which the distributed representation is driven by supervision from discourse annotations. For example, Ji and Eisenstein (2014) show that RST parsing can be performed by learning task-specific word representations, which perform considerably better than generic word2vec representations (Mikolov et al., 340 2013). Li et al. (2014) propose a recursive neural network approach to RST parsing, which"
Q15-1024,Q13-1015,0,0.0229312,"se together. More generally, our entity-augmented distributed representation can be viewed in the context of recent literature on combining distributed and formal semantics: by representing entities, we are taking a small step away from purely vectorial representations, and towards more traditional logical representations of meaning. In this sense, our approach is “bottom-up”, as we try to add a small amount of logical formalism to distributed representations; other approaches are “top-down”, softening purely logical representations by using distributional clustering (Poon and Domingos, 2009; Lewis and Steedman, 2013) or Bayesian non-parametrics (Titov and Klementiev, 2011) to obtain types for entities and relations. Still more ambitious would be to implement logical semantics within a distributed compositional framework (Clark et al., 2011; Grefenstette, 2013). At present, these combinations of logical and distributed semantics have been explored only at the sentence level. In generalizing such approaches to multi-sentence discourse, we argue that it will not be sufficient to compute distributed representations of sentences: a multitude of other elements, such as entities, will also have to represented. 8"
Q15-1024,W14-4327,0,0.239256,"K, λ, η separately for each classifier (see Section 5 for details), by performing a grid search to optimize the F-measure on the development data. Following Pitler et al. (2009), we obtain a balanced training set by resampling training instances in each class until the number of positive and negative instances are equal. 6.2.2 Competitive systems We compare against the published results from several competitive systems, focusing on systems which use the predominant training / test split, with sections 2-20 for training and 21-22 for testing. This means we cannot compare with recent work from Li and Nenkova (2014), who use sections 20-24 for testing. Pitler et al. (2009) present a classification model using linguistically-informed features, such as polarity tags and Levin verb classes. Zhou et al. (2010) predict discourse connective words, and then use these predicted connectives as features in a downstream model to predict relations. Park and Cardie (2012) showed that the performance on each relation can be improved by selecting a locally-optimal feature set. Biran and McKeown (2013) reweight word pair features using distributional statistics from the Gigaword corpus, obtaining denser aggregated score"
Q15-1024,D14-1220,0,0.0294126,"the factorization of matrices of distributional counts (Kauchak and Barzilay, 2006; Mihalcea et al., 2006). These approaches essentially compute a distributional representation in advance, and then use it alongside other features. In contrast, our approach follows more recent work in which the distributed representation is driven by supervision from discourse annotations. For example, Ji and Eisenstein (2014) show that RST parsing can be performed by learning task-specific word representations, which perform considerably better than generic word2vec representations (Mikolov et al., 340 2013). Li et al. (2014) propose a recursive neural network approach to RST parsing, which is similar to the upward pass in our model, and Kalchbrenner and Blunsom (2013) show how a recurrent neural network can be used to identify dialogue acts. However, prior work has not applied these ideas to the classification of implicit relations in the PDTB, and does not consider the role of entities. As we argue in the introduction, a single vector representation is insufficiently expressive, because it obliterates the entity chains that help to tie discourse together. More generally, our entity-augmented distributed represen"
Q15-1024,D09-1036,0,0.67511,"t, u0 and u0 , as well as on the downward vectors for each pair of aligned entity mentions. For the cases where there are no coreferent entity mentions between two sentences, A(m, n) = ∅, the classification model considers only the upward vectors at the root. 332 To avoid overfitting, we apply a lowdimensional approximation to each Ay , Ay = ay,1 a&gt; y,2 + diag(ay,3 ). (4) The same approximation is also applied to each By , reducing the number of classification parameters from 2 × #|Y |× K 2 to 2 × #|Y |× 3K. Surface features Prior work has identified a number of useful surface-level features (Lin et al., 2009), and the classification model can easily be extended to include them. Defining φ(m,n) as the vector of surface features extracted from the argument pair (m, n), the corresponding decision function is modified as, (m) &gt; ψ(y) = (u0 (n) ) Ay u0 + X (m) &gt; (di (n) ) By dj i,j∈A(m,n) + β&gt; y φ(m,n) + by , (5) where β y is the classification weight on surface features for relation y. We describe these features in Section 5. 4 Large-margin learning framework There are two sets of parameters to be learned: the classification parameters θ class = {Ay , By , β y , by }y∈Y , and the composition parameters"
Q15-1024,P11-1100,0,0.026688,"Missing"
Q15-1024,W10-4327,0,0.0419308,"Missing"
Q15-1024,W10-4310,0,0.22812,"Missing"
Q15-1024,P84-1076,0,0.226149,"Missing"
Q15-1024,P99-1047,0,0.113216,"s work in discourse relation detection and compositional distributed semantics. 7.1 Discourse relations Many models of discourse structure focus on relations between spans of text (Knott, 1996), including rhetorical structure theory (RST; Mann and Thompson, 1988), lexicalized tree-adjoining grammar for discourse (D-LTAG; Webber, 2004), and even centering theory (Grosz et al., 1995), which posits relations such as CONTINUATION and SMOOTH SHIFT between adjacent spans. Consequently, the automatic identification of discourse relations has long been considered a key component of discourse parsing (Marcu, 1999). We work within the D-LTAG framework, as annotated in the Penn Discourse Treebank (PDTB; Prasad et al., 2008), with the task of identifying implicit discourse relations. The seminal work in this task is from Pitler et al. (2009) and Lin et al. (2009). Pitler et al. (2009) focus on lexical features, including linguistically motivated word groupings such as Levin verb classes and polarity tags. Lin et al. (2009) identify four different feature categories, based on the raw text, the context, and syntactic parse trees; the same feature sets are used in later work on end-to-end discourse parsing ("
Q15-1024,N13-1090,0,0.0156973,"nsionality, λ ∈ {0.0002, 0.002, 0.02, 0.2} for the regularization (on each training instance), and η ∈ {0.01, 0.03, 0.05, 0.09} for the learning rate. We assign separate regularizers and learning rates to the upward composition model, downward composition model, feature model and the classification model with composition vectors. Initialization All the classification parameters are initialized to 0. For the composition parameters, we follow Bengio (2012) and initialize U and V with p uniform random values drawn from p the range [− 6/2K, 6/2K]. Word representations We trained a word2vec model (Mikolov et al., 2013) on the PDTB corpus, standardizing the induced representations to zeromean, unit-variance (LeCun et al., 2012). Experiments with pre-trained GloVe word vector representations (Pennington et al., 2014) gave broadly similar results. Syntactic structure Our model requires that the syntactic structure for each argument is represented as a binary tree. We run the Stanford parser (Klein and Manning, 2003) to obtain constituent parse trees of each sentence in the PDTB, and binarize all resulting parse trees. Argument spans in the Penn Discourse Treebank need not be sentences or syntactic constituents"
Q15-1024,W12-1614,0,0.813812,"xteen such relation types. A third level of subtypes is defined for only some types, specifying the semantic contribution of each argument. There are two main approaches to evaluating implicit discourse relation classification. Multiclass classification requires identifying the discourse relation from all possible choices. This task was explored by Lin et al. (2009), who focus on second-level discourse relations. More recent work has emphasized binary classification, where the goal is to build and evaluate separate “one-versus-all” classifiers for each discourse relation (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013). We primarily focus on multiclass classification, because it is more relevant for the ultimate goal of building a PDTB parser; however, to compare with recent prior work, we also evaluate on binary relation classification. 335 6.1 Multiclass classification Our main evaluation involves predicting the correct discourse relation for each argument pair, from among the second-level relation types. The training and test set construction follows Lin et al. (2009) with a few changes: • We use sections 2-20 of the PDTB as a training set, sections 0-1 as a development set for"
Q15-1024,D14-1162,0,0.0943666,"ates to the upward composition model, downward composition model, feature model and the classification model with composition vectors. Initialization All the classification parameters are initialized to 0. For the composition parameters, we follow Bengio (2012) and initialize U and V with p uniform random values drawn from p the range [− 6/2K, 6/2K]. Word representations We trained a word2vec model (Mikolov et al., 2013) on the PDTB corpus, standardizing the induced representations to zeromean, unit-variance (LeCun et al., 2012). Experiments with pre-trained GloVe word vector representations (Pennington et al., 2014) gave broadly similar results. Syntactic structure Our model requires that the syntactic structure for each argument is represented as a binary tree. We run the Stanford parser (Klein and Manning, 2003) to obtain constituent parse trees of each sentence in the PDTB, and binarize all resulting parse trees. Argument spans in the Penn Discourse Treebank need not be sentences or syntactic constituents: they can include multiple sentences, non-constituent spans, and even discontinuous spans (Prasad et al., 2008). In all cases, we identify the syntactic subtrees within the argument span, and unify t"
Q15-1024,P06-1055,0,0.0185912,"ide algorithm for computing marginal probabilities in a probabilistic contextfree grammar (Lari and Young, 1990), the inside scores are constructed in a bottom-up fashion, like our upward nodes; the outside score for node i is constructed from a product of the outside score of the parent ρ(i) and the inside score of the sibling s(i), like our downward nodes. The standard inside-outside algorithm sums over all possible parse trees, but since the parse tree is observed in our case, a closer analogy would be to the constrained version of the inside-outside algorithm for latent variable grammars (Petrov et al., 2006). Cohen et al. (2014) describe a tensor formulation of the constrained inside-outside algorithm; similarly, we could compute the downward vectors by a tensor contraction of the parent and sibling vectors (Smolensky, 1990; Socher et al., 2014). However, this would involve K 3 parameters, rather than the K 2 parameters in our matrixvector composition. 3 Predicting discourse relations To predict the discourse relation between an argument pair (m, n), the decision function is a sum of bilinear products, (m) (n) ψ(y) = (u0 )&gt; Ay u0 X (m) (n) + (di )&gt; By dj + by , (3) i,j∈A(m,n) where Ay ∈ RK×K and"
Q15-1024,C08-2022,0,0.0214432,"the Penn Discourse Treebank (PDTB; Prasad et al., 2008), which provides a discourse level annotation over the Wall Street Journal corpus. In the PDTB, each discourse relation is annotated between two argument spans. Identifying the argument spans of discourse relations is a challenging task (Lin et al., 2012), which we do not attempt here; instead, we use gold argument spans, as in most of the relevant prior work. PDTB relations may be explicit, meaning that they are signaled by discourse connectives (e.g., because); alternatively, they may be implicit, meaning that the connective is absent. Pitler et al. (2008) show that most explicit connectives are unambiguous, so we focus on the problem of classifying implicit discourse relations. The PDTB provides a three-level hierarchy of discourse relations. The first level consists of four major relation classes: T EMPORAL , C ON TINGENCY, C OMPARISON and E XPANSION . For each class, a second level of types is defined to provide finer semantic or pragmatic distinctions; there are sixteen such relation types. A third level of subtypes is defined for only some types, specifying the semantic contribution of each argument. There are two main approaches to evalua"
Q15-1024,P09-1077,0,0.501541,"nctions; there are sixteen such relation types. A third level of subtypes is defined for only some types, specifying the semantic contribution of each argument. There are two main approaches to evaluating implicit discourse relation classification. Multiclass classification requires identifying the discourse relation from all possible choices. This task was explored by Lin et al. (2009), who focus on second-level discourse relations. More recent work has emphasized binary classification, where the goal is to build and evaluate separate “one-versus-all” classifiers for each discourse relation (Pitler et al., 2009; Park and Cardie, 2012; Biran and McKeown, 2013). We primarily focus on multiclass classification, because it is more relevant for the ultimate goal of building a PDTB parser; however, to compare with recent prior work, we also evaluate on binary relation classification. 335 6.1 Multiclass classification Our main evaluation involves predicting the correct discourse relation for each argument pair, from among the second-level relation types. The training and test set construction follows Lin et al. (2009) with a few changes: • We use sections 2-20 of the PDTB as a training set, sections 0-1 as"
Q15-1024,D09-1001,0,0.0111229,"that help to tie discourse together. More generally, our entity-augmented distributed representation can be viewed in the context of recent literature on combining distributed and formal semantics: by representing entities, we are taking a small step away from purely vectorial representations, and towards more traditional logical representations of meaning. In this sense, our approach is “bottom-up”, as we try to add a small amount of logical formalism to distributed representations; other approaches are “top-down”, softening purely logical representations by using distributional clustering (Poon and Domingos, 2009; Lewis and Steedman, 2013) or Bayesian non-parametrics (Titov and Klementiev, 2011) to obtain types for entities and relations. Still more ambitious would be to implement logical semantics within a distributed compositional framework (Clark et al., 2011; Grefenstette, 2013). At present, these combinations of logical and distributed semantics have been explored only at the sentence level. In generalizing such approaches to multi-sentence discourse, we argue that it will not be sufficient to compute distributed representations of sentences: a multitude of other elements, such as entities, will"
Q15-1024,prasad-etal-2008-penn,0,0.947203,"tegories, obtaining 500 word pair features, 100 constituent features, and 100 dependency features. In addition, Rutherford and Xue (2014) discovered that replacing word pair with their Brown cluster assignments could give further improvements. In our implementation, we used the Brown word clusters provided by Turian et al. (2010), in which words from the Reuters Corpus (RCV1) are grouped into 3,200 clusters. The feature selection method of Lin et al. (2009) was then used to obtain a set of 600 Brown cluster features. 6 Experiments We evaluate our approach on the Penn Discourse Treebank (PDTB; Prasad et al., 2008), which provides a discourse level annotation over the Wall Street Journal corpus. In the PDTB, each discourse relation is annotated between two argument spans. Identifying the argument spans of discourse relations is a challenging task (Lin et al., 2012), which we do not attempt here; instead, we use gold argument spans, as in most of the relevant prior work. PDTB relations may be explicit, meaning that they are signaled by discourse connectives (e.g., because); alternatively, they may be implicit, meaning that the connective is absent. Pitler et al. (2008) show that most explicit connectives"
Q15-1024,E14-1068,0,0.686137,"ution can be expected to yield further improvements in the effectiveness of distributed entity semantics for discourse relation detection. Additional features We supplement our classification model using additional surface features proposed by Lin et al. (2009). These include four categories: word pair features, constituent parse features, dependency parse features, and contextual features. As done in this prior work, we use mutual information to select features in the first three categories, obtaining 500 word pair features, 100 constituent features, and 100 dependency features. In addition, Rutherford and Xue (2014) discovered that replacing word pair with their Brown cluster assignments could give further improvements. In our implementation, we used the Brown word clusters provided by Turian et al. (2010), in which words from the Reuters Corpus (RCV1) are grouped into 3,200 clusters. The feature selection method of Lin et al. (2009) was then used to obtain a set of 600 Brown cluster features. 6 Experiments We evaluate our approach on the Penn Discourse Treebank (PDTB; Prasad et al., 2008), which provides a discourse level annotation over the Wall Street Journal corpus. In the PDTB, each discourse relati"
Q15-1024,D13-1170,0,0.00283268,"ve techniques, which emphasize predictive models (Bengio et al., 2006; Baroni et al., 2014b), rather than context-counting and matrix factorization (Landauer et al., 1998; Turney and Pantel, 2010). Recent work has made practical the idea of propagating distributed information through linguistic structures (Smolensky, 1990; Collobert et al., 2011). In such models, the distributed representations and compositional operators can be fine-tuned by backpropagating supervision from task-specific labels, enabling accurate and fast models for a wide range of language technologies (Socher et al., 2011; Socher et al., 2013; Chen and Manning, 2014). Of particular relevance is recent work on twopass procedures for distributed compositional semantics. Paulus et al. (2014) perform targeted sentiment analysis by propagating information from 339 7.2 Compositional distributed semantics the sentence level back to child non-terminals in the parse tree. Their compositional procedure is different from ours: in their work, the “downward” meaning of each non-terminal is reconstructed from the upward and downward meanings of its parents. ˙Irsoy and Cardie (2013) propose an alternative two-pass procedure, where the downward r"
Q15-1024,D09-1018,0,0.0254125,"Missing"
Q15-1024,P11-1145,0,0.013381,"ibuted representation can be viewed in the context of recent literature on combining distributed and formal semantics: by representing entities, we are taking a small step away from purely vectorial representations, and towards more traditional logical representations of meaning. In this sense, our approach is “bottom-up”, as we try to add a small amount of logical formalism to distributed representations; other approaches are “top-down”, softening purely logical representations by using distributional clustering (Poon and Domingos, 2009; Lewis and Steedman, 2013) or Bayesian non-parametrics (Titov and Klementiev, 2011) to obtain types for entities and relations. Still more ambitious would be to implement logical semantics within a distributed compositional framework (Clark et al., 2011; Grefenstette, 2013). At present, these combinations of logical and distributed semantics have been explored only at the sentence level. In generalizing such approaches to multi-sentence discourse, we argue that it will not be sufficient to compute distributed representations of sentences: a multitude of other elements, such as entities, will also have to represented. 8 Conclusion Discourse relations are determined by the mea"
Q15-1024,P10-1040,0,0.0206371,"Missing"
Q15-1024,P99-1006,0,0.206634,"Missing"
Q15-1024,D14-1196,0,0.22765,"Missing"
Q15-1024,C10-2172,0,0.786986,"ced training set by resampling training instances in each class until the number of positive and negative instances are equal. 6.2.2 Competitive systems We compare against the published results from several competitive systems, focusing on systems which use the predominant training / test split, with sections 2-20 for training and 21-22 for testing. This means we cannot compare with recent work from Li and Nenkova (2014), who use sections 20-24 for testing. Pitler et al. (2009) present a classification model using linguistically-informed features, such as polarity tags and Levin verb classes. Zhou et al. (2010) predict discourse connective words, and then use these predicted connectives as features in a downstream model to predict relations. Park and Cardie (2012) showed that the performance on each relation can be improved by selecting a locally-optimal feature set. Biran and McKeown (2013) reweight word pair features using distributional statistics from the Gigaword corpus, obtaining denser aggregated score features. 338 6.2.3 Experimental results Table 4 presents the performance of the DISCO 2 model and the published results of competitive systems. DISCO 2 achieves the best results on most metric"
Q15-1024,C98-1044,0,\N,Missing
Q15-1024,2014.lilt-9.5,0,\N,Missing
Q17-1021,P15-2126,0,0.0241596,"ets, and offered minimal performance improvements. In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space. Personalization has been an active research topic in areas such as speech recognition and information retrieval. Standard techniques for these tasks include linear transformation of model parameters (Leggetter and Woodland, 1995) and collaborative filtering (Breese et al., 1998). These methods have recently been adapted to personalized sentiment analysis (Tang et al., 2015a; Al Boni et al., 2015). Supervised personalization typically requires labeled training examples for every individual user. In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors. Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterog"
Q17-1021,P15-1104,0,0.0542016,"th a large amount of isolated author nodes. To improve the quality of the author embeddings, we expand the set of author nodes by adding nodes that do the most to densify the author networks: for the follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set. The statistics of the resulting networks are presented in Table 2. 5.2 Experimental Settings We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task. The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015). We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods. Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of S OCIAL ATTENTION. Mixture"
Q17-1021,P06-2005,0,0.0446254,"Missing"
Q17-1021,W06-1615,0,0.0332027,"1 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010). However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements. In this paper, we convert social network positions into node embeddings, and use an attentional component to smooth the classification rule across the embedding space. Personalization has been an active research topic in"
Q17-1021,D16-1120,0,0.08253,"Missing"
Q17-1021,P07-1033,0,0.0658963,"Missing"
Q17-1021,D10-1124,1,0.876353,"Missing"
Q17-1021,N13-1037,1,0.853875,"perts and concatenation obtain slightly worse F1 scores than the baseline CNN system, but random attention performs significantly better. In contrast to the SemEval datasets, individual users often contribute multiple reviews in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7). As an author tends to express similar opinions toward related products, random attention 11 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author so"
Q17-1021,N09-1068,0,0.0298879,"views in the Ciao datasets (the average number of reviews from an author is 10.8; Table 7). As an author tends to express similar opinions toward related products, random attention 11 https://code.google.com/archive/p/ word2vec 303 Related Work Domain adaptation and personalization Domain adaptation is a classic approach to handling the variation inherent in social media data (Eisenstein, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across domains, using enhanced feature spaces (Daum´e III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009). Recent work focuses on unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006). However, in many cases, the data has no natural partitioning into domains. In preliminary work, we constructed social network domains by running community detection algorithms on the author social network (Fortunato, 2010). However, these algorithms proved to be unstable on the sparse networks obtained from social media datasets, and offered minimal performance improvements. In this paper, we convert social network p"
Q17-1021,S15-2097,0,0.0646927,"Missing"
Q17-1021,S15-2095,0,0.0125221,"ce the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we compare a range of numbers of basis models, {3, 5, 10, 15}. We found that a relatively small number of bas"
Q17-1021,P15-1073,0,0.0485176,"w data. 1 Introduction Words can mean different things to different people. Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963). In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for characOne exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender. However, such demographic information is not directly available in most datasets, and it is not yet clear whether predicted age and gender offer any improvements. On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), and language modeling (Federico, 1996). But personali"
Q17-1021,P07-1034,0,0.0426865,"training instance: models are inactive. This can occur because some basis models may be initialized with parameters that are globally superior. As a result, the “dead” basis models will receive near-zero gradient updates, and therefore can never improve. The true model capacity can thereby be substantially lower than the K assigned experts. Ideally, dead basis models will be avoided because each basis model should focus on a unique region of the social network. To ensure that this happens, we pretrain the basis models using an instance weighting approach from the domain adaptation literature (Jiang and Zhai, 2007). For each basis model k, each author a has an instance weight αa,k . These instance weights are based on the author’s social network node embedding, so that socially proximate authors will have high weights for the same basis models. This is ensured by endowing each basis model with a random vector γk ∼ N (0, σ 2 I), and setting the instance weights as, αa,k = sigmoid(γk> va ). (7) The simple design results in similar instance weights for socially proximate authors. During pretraining, we train the k-th basis model by optimizing the following loss function for every instance: `k = −αa,k T X t"
Q17-1021,P14-1062,0,0.011628,"combining all of these relation types into a unified multi-relational network. It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014). 4.2 Sentiment Classification with Convolutional Neural Networks We next describe the basis models, p(y |x, Z = k). Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features. CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information. Let x = [h1 , h2 , · · · , hn ] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence. We use one convolutional layer and one max pooling layer to generate the sentence representation of x. The convolutional layer involves filters that are applied to bigrams to produce feature maps. Formally, given the bigr"
Q17-1021,D14-1181,0,0.00450147,"tion types into a unified multi-relational network. It is possible that embeddings in such a network could be estimated using techniques borrowed from multirelational knowledge networks (Bordes et al., 2014; Wang et al., 2014). 4.2 Sentiment Classification with Convolutional Neural Networks We next describe the basis models, p(y |x, Z = k). Because our target task is classification on microtext documents, we model this distribution using convolutional neural networks (CNNs; Lecun et al., 1989), which have been proven to perform well on sentence classification tasks (Kalchbrenner et al., 2014; Kim, 2014). CNNs apply layers of convolving filters to n-grams, thereby generating a vector of dense local features. CNNs improve upon traditional bagof-words models because of their ability to capture word ordering information. Let x = [h1 , h2 , · · · , hn ] be the input sentence, where hi is the D(w) dimensional word vector corresponding to the i-th word in the sentence. We use one convolutional layer and one max pooling layer to generate the sentence representation of x. The convolutional layer involves filters that are applied to bigrams to produce feature maps. Formally, given the bigram word vect"
Q17-1021,N15-1142,0,0.0125705,"follower network, we add additional individuals that are followed by at least a hundred authors in the original set; for the mention and retweet networks, we add all users that have been mentioned or retweeted by at least twenty authors in the original set. The statistics of the resulting networks are presented in Table 2. 5.2 Experimental Settings We employ the pretrained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets, and have been shown to perform very well on this task. The embeddings are learned using the structured skip-gram model (Ling et al., 2015), and the embedding dimension is set at 600, following Astudillo et al. (2015). We report the same evaluation metric as the SemEval challenge: the average F1 score of positive and negative classes.8 Competitive systems We consider five competitive Twitter sentiment classification methods. Convolutional neural network (CNN) has been described in § 4.2, and is the basis model of S OCIAL ATTENTION. Mixture of experts employs the same CNN model as an expert, but the mixture densi8 Regarding the neutral class: systems are penalized with false positives when neutral tweets are incorrectly classified"
Q17-1021,S13-2052,0,0.103279,"Missing"
Q17-1021,P11-1077,0,0.0796886,"n is divided among several basis models, depending on the author’s position in the social network. This has the effect of smoothing the classification function across the social network, and makes it possible to induce personalized classifiers even for authors for whom there is no labeled data or demographic metadata. This model significantly improves the accuracies of sentiment analysis on Twitter and on review data. 1 Introduction Words can mean different things to different people. Fortunately, these differences are rarely idiosyncratic, but are often linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963). In natural language processing (NLP), social media data has brought variation to the fore, spurring the development of new computational techniques for characOne exception is the work of Hovy (2015), who shows that the accuracies of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender. However, such demographic inf"
Q17-1021,S15-2078,0,0.0342528,"cture of random attention is nearly identical to S OCIAL ATTENTION: the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we comp"
Q17-1021,S15-2079,0,0.0132732,"TION: the only distinction is that we replace the pretrained author embeddings with random embedding vectors, drawing uniformly from the interval (−0.25, 0.25). Concatenation concatenates the author embedding with the sentence representation obtained from CNN, and then feeds the new representation to a softmax classifier. Finally, we include S OCIAL ATTENTION, the attention-based neural network method described in § 4. We also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): W E BIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015), and LSISLIF (Hamdan et al., 2015). UNITN achieves the best average F1 score on Test 2013–2015 sets among all the submitted systems. Finally, we republish results of N LSE (Astudillo et al., 2015), a non-linear subspace embedding model. Parameter tuning We tune all the hyperparameters on the SemEval 2013 development set. We choose the number of bigram filters for the CNN models from {50, 100, 150}. The size of author embeddings is selected from {50, 100}. For mixture of experts, random attention and S OCIAL ATTENTION , we compare a range of numbers of basis models, {3, 5, 10, 15}. We found th"
Q17-1021,W11-2207,0,0.102353,"(Tang et al., 2015a; Al Boni et al., 2015). Supervised personalization typically requires labeled training examples for every individual user. In contrast, by leveraging the social network structure, we can obtain personalization even when labeled data is unavailable for many authors. Sentiment analysis with social relations Previous work on incorporating social relations into sentiment classification has relied on the label consistency assumption, where the existence of social connections between users is taken as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes. Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph. Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach. A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic"
Q17-1021,C14-1018,0,0.0309121,"ON outperforms prior work regardless of which network is selected. Twitter’s “following” relation is a relatively low-cost form of social engagement, and it is less public than retweeting or mentioning another user. Thus it is unsurprising that the follower network is least useful for socially-informed personalization. The R ETWEET + network has denser social connections than M ENTION +, which could lead to better author embeddings. 5.4 Analysis We now investigate whether language variation in sentiment meaning has been captured by different basis models. We focus on the same sentiment words (Tang et al., 2014) that we used to test linguistic homophily in our analysis. We are interested to discover sentiment words that are used with the opposite sentiment meanings by some authors. To measure the level of model-specificity for each Basis model More positive More negative 1 2 3 4 5 banging loss fever broken fucking chilling cold ill sick suck ass damn piss bitch shit insane bawling fever weird cry ruin silly bad boring dreadful dear like god yeah wow satisfy trust wealth strong lmao talent honestly voting win clever lmao super lol haha hahaha lovatics wish beliebers arianators kendall Table 5: Top 5 m"
Q17-1021,P15-1098,0,0.365126,"k’, speakers like Taylor Swift may employ either the positive and negative meanings, while speakers like Charles Rangel employ only the negative meaning. In other cases, communities may maintain completely distinct semantics for a word, such as the term ‘pants’ in American and British English. Thanks to Christopher Potts for suggesting this distinction and this example. 2 296 # Positive # Negative # Neutral # Tweet 3,230 477 1,572 982 1,038 1,265 273 601 202 365 4,109 614 1,640 669 987 8,604 1,364 3,813 1,853 2,390 Table 1: Statistics of the SemEval Twitter sentiment datasets. scale networks (Tang et al., 2015b). Applying the algorithm to Figure 1, the authors within each triad would likely be closer to each other than to authors in the opposite triad. We then incorporate these embeddings into an attention-based neural network model, called S OCIAL ATTENTION, which employs multiple basis models to focus on different regions of the social network. We apply S OCIAL ATTENTION to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks (Nakov et al., 2013). We further adopt the system to Ciao product reviews (Tang et al., 2012"
Q17-1021,W06-1639,0,0.0413497,"omputational Linguistics, vol. 5, pp. 295–307, 2017. Action Editor: Christopher Potts. Submission batch: 10/2016; Revision batch: 12/2016; Published 8/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Dataset Train 2013 Dev 2013 Test 2013 Test 2014 Test 2015 Figure 1: Words such as ‘sick’ can express opposite sentiment polarities depending on the author. We account for this variation by generalizing across the social network. network information is available in a wide range of contexts, from social media (Huberman et al., 2008) to political speech (Thomas et al., 2006) to historical texts (Winterer, 2012). Thus, social network homophily has the potential to provide a more general way to account for linguistic variation in NLP. Figure 1 gives a schematic of the motivation for our approach. The word ‘sick’ typically has a negative sentiment, e.g., ‘I would like to believe he’s sick rather than just mean and evil.’1 However, in some communities the word can have a positive sentiment, e.g., the lyric ‘this sick beat’, recently trademarked by the musician Taylor Swift.2 Given labeled examples of ‘sick’ in use by individuals in a social network, we assume that th"
Q17-1021,Q14-1024,0,0.0271447,"en as a clue that the sentiment polarities of the users’ messages should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes. Each node is then associated with a sentiment label distribution, and these label distributions are smoothed by label propagation over the graph. Similar approaches are explored by Hu et al. (2013), who employ the graph Laplacian as a source of regularization, and by Tan et al. (2011) who take a factor graph approach. A related idea is to label the sentiment of individuals in a social network towards each other: West et al. (2014) exploit the sociological theory of structural balance to improve the accuracy of dyadic sentiment labels in this setting. All of these efforts are based on the intuition that individual predictions p(y) should be smooth across the network. In contrast, our work is based on the intuition that social neighbors use language similarly, so they should have a similar conditional distribution p(y |x). These intuitions are complementary: if both hold for a specific setting, then label consistency and linguistic consistency could in principle be combined to improve performance. Social relations can al"
W11-2202,S07-1012,0,0.0908542,"Missing"
W11-2202,D08-1031,0,0.0174584,"rence resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 10 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when availab"
W11-2202,W98-1118,0,0.0439867,"rk. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches be"
W11-2202,P11-1098,0,0.0389599,"text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not"
W11-2202,N01-1007,0,0.441624,"m of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem tha"
W11-2202,doddington-etal-2004-automatic,0,0.0381729,"ave many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a non-temporal version of our model (as described in Sections 3 and 4), and a temporal version with 5 epochs. For"
W11-2202,S07-1058,0,0.0623979,"that appear in names (such as titles and first names). We are aware of no existing system that performs all three of these tasks jointly. We evaluate on a dataset of political blogs, measuring our system’s ability to discover a set of reference entities (recall) while maintaining a compact number of rows and columns (precision). With as few as five partially-complete prototype examples, our approach gives accurate tables that match well against a manually-annotated reference list. Our method outperforms a baseline singlelink clustering approach inspired by one of the most successful entries (Elmacioglu et al., 2007) in the SEMEVAL “Web People Search” shared task (Artiles et al., 2007). 2 Task Definition In this work, we assume that a bag of M mentions in text have been identified. The mth mention wm is a sequence of contiguous word tokens (its length is denoted Nm ) understood to refer to a real-world entity. The entities (and the mapping of mentions to entities) are not known in advance. While our focus in this paper is names of people, the task is defined in a more generic way. Formally, the task is to construct a table x where rows correspond to entities and columns to functional fields. The number of"
W11-2202,N09-1019,0,0.0814631,"d entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. Resolving Mentions to Entities The problem of resolving mentions to entities has been approach from a var"
W11-2202,P05-1045,0,0.0716754,"the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging strings that reference the same entity. We also manually discarded strings from"
W11-2202,P07-1107,0,0.0705131,"ns and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi a"
W11-2202,P10-2054,0,0.24949,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,N10-1061,0,0.177867,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,D08-1068,0,0.0585186,"to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future"
W11-2202,P11-1080,0,0.346535,"for the 100 entities. Most entities only include first and last names, though the most frequent entities have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a"
W11-2202,W02-2024,0,0.109779,"ng over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametri"
W11-2202,N10-1082,0,\N,Missing
W13-1102,D10-1124,1,0.369793,"Missing"
W13-1102,P11-1137,1,0.0982633,"lexical items into social media text, but not the underlying structural rules. Introduction The differences between social media text and other forms of written language are a subject of increasing interest for both language technology (Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011) and linguistics (Androutsopoulos, 2011; Dresner and Herring, 2010; Paolillo, 1996). Many words that are endogenous to social media have been linked with specific geographical regions (Eisenstein et al., 2010; Wing and Baldridge, 2011) and demographic groups (Argamon et al., 2007; Rao et al., 2010; Eisenstein et al., 2011), raising the question of whether this variation is related to spoken language dialects. Dialect variation encompasses differences at multiple linguistic levels, including the lexicon, morphology, syntax, and phonology. While previous work on group differences in social media language has generally focused on lexical differences, this paper considers the most purely “spoken” aspect of dialect: phonology. These hypotheses are examined in the context of the phonological variable of consonant cluster reduction (also known as consonant cluster simplification, or more specifically, -/t,d/ deletion)"
W13-1102,I11-1100,0,0.00957302,"Missing"
W13-1102,P11-2008,1,0.045835,"Missing"
W13-1102,W11-2210,0,0.0388779,"ive, a characteristic of New York English (Gordon, 2004), rural Southern English (Thomas, 2004), as well as AAE (Green, 2002). The next two pairs represent “g-dropping”, the replacement of the velar nasal with the coronal nasal, which has been associated with informal speech in many parts of the English-speaking world.1 The final word pair know/kno does not differ in pronunciation, and is included as a control. These pairs were selected because they are all frequently-used words, and because they cover a range of typical “shortenings” in social media and other computer mediated communication (Gouws et al., 2011). Another criterion is that each shortened form can be recognized relatively unambiguously. Although wit and wan are standard English words, close examination of the data did not reveal any examples in which the surface forms could be construed to indicate these words. Other words were rejected for this reason: for example, best may be reduced to bes, but this surface form is frequently used as an acronym for Blackberry Enterprise Server. Consonant cluster reduction may be combined with morphosyntactic variation, particularly in African American English. Thompson et al. (2004) describe several"
W13-1102,D11-1141,0,0.00682904,"social media writing transcribes phonological properties of speech, it is not merely a case of inventing orthographic transcriptions. Rather, social media displays influence from structural properties of the phonological system. 1 • H0: Phonological variation does not impact social media text. • H1: Phonological variation may introduce new lexical items into social media text, but not the underlying structural rules. Introduction The differences between social media text and other forms of written language are a subject of increasing interest for both language technology (Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011) and linguistics (Androutsopoulos, 2011; Dresner and Herring, 2010; Paolillo, 1996). Many words that are endogenous to social media have been linked with specific geographical regions (Eisenstein et al., 2010; Wing and Baldridge, 2011) and demographic groups (Argamon et al., 2007; Rao et al., 2010; Eisenstein et al., 2011), raising the question of whether this variation is related to spoken language dialects. Dialect variation encompasses differences at multiple linguistic levels, including the lexicon, morphology, syntax, and phonology. While previous work on group diffe"
W13-1102,P11-1096,0,0.023687,"ariation does not impact social media text. • H1: Phonological variation may introduce new lexical items into social media text, but not the underlying structural rules. Introduction The differences between social media text and other forms of written language are a subject of increasing interest for both language technology (Gimpel et al., 2011; Ritter et al., 2011; Foster et al., 2011) and linguistics (Androutsopoulos, 2011; Dresner and Herring, 2010; Paolillo, 1996). Many words that are endogenous to social media have been linked with specific geographical regions (Eisenstein et al., 2010; Wing and Baldridge, 2011) and demographic groups (Argamon et al., 2007; Rao et al., 2010; Eisenstein et al., 2011), raising the question of whether this variation is related to spoken language dialects. Dialect variation encompasses differences at multiple linguistic levels, including the lexicon, morphology, syntax, and phonology. While previous work on group differences in social media language has generally focused on lexical differences, this paper considers the most purely “spoken” aspect of dialect: phonology. These hypotheses are examined in the context of the phonological variable of consonant cluster reductio"
W14-3212,R11-1019,0,0.0223819,"For example, Twitter has been used both for mining both public health information (Paul and Dredze, 2011) and for estimating individual health status (Sokolova et al., 2013; Teodoro and Naaman, 2013). Domain-specific online communities, such Aspies Central, have their own advantages, targeting specific issues and featuring more closeknit and long-term relationships among members (Newton et al., 2009). Previous studies on mining health information show that technical models and tools from computational linguistics are helpful for both understanding contents and providing informative features. Sokolova and Bobicev (2011) use sentiment analysis to analyze opinions expressed in healthrelated Web messages; Hong et al. (2012) focus on lexical differences to automatically distinguish schizophrenic patients from healthy individuals. Topic models have previously been used to mine health information: Resnik et al. (2013) use LDA to improve the prediction for neuroticism and depression on college students, while Paul and Dredze (2013) customize their factorial LDA to model the joint effect of drug, aspect, and route of administration. Most relevantly for the current paper, Nguyen et al. (2013) use LDA to discover auti"
W14-3212,D12-1004,0,0.0288573,"or estimating individual health status (Sokolova et al., 2013; Teodoro and Naaman, 2013). Domain-specific online communities, such Aspies Central, have their own advantages, targeting specific issues and featuring more closeknit and long-term relationships among members (Newton et al., 2009). Previous studies on mining health information show that technical models and tools from computational linguistics are helpful for both understanding contents and providing informative features. Sokolova and Bobicev (2011) use sentiment analysis to analyze opinions expressed in healthrelated Web messages; Hong et al. (2012) focus on lexical differences to automatically distinguish schizophrenic patients from healthy individuals. Topic models have previously been used to mine health information: Resnik et al. (2013) use LDA to improve the prediction for neuroticism and depression on college students, while Paul and Dredze (2013) customize their factorial LDA to model the joint effect of drug, aspect, and route of administration. Most relevantly for the current paper, Nguyen et al. (2013) use LDA to discover autism-related topics, using a dataset of 10,000 posts from ten different autism commnities. However, their"
W14-3212,R13-1082,0,0.0115581,"e found most answer posts of user U SER 15 are from the board “love104 sume that each document is generated by a mixture of its authors’ topic distributions. Our model can be viewed as one further extension of topic models by incorporating more metadata information (authorship, thread structure) in online forums. relationships-and-dating”. 7 Related Work Social media has become an important source of health information (Choudhury et al., 2014). For example, Twitter has been used both for mining both public health information (Paul and Dredze, 2011) and for estimating individual health status (Sokolova et al., 2013; Teodoro and Naaman, 2013). Domain-specific online communities, such Aspies Central, have their own advantages, targeting specific issues and featuring more closeknit and long-term relationships among members (Newton et al., 2009). Previous studies on mining health information show that technical models and tools from computational linguistics are helpful for both understanding contents and providing informative features. Sokolova and Bobicev (2011) use sentiment analysis to analyze opinions expressed in healthrelated Web messages; Hong et al. (2012) focus on lexical differences to automatica"
W14-3212,N13-1017,0,0.0274623,"dies on mining health information show that technical models and tools from computational linguistics are helpful for both understanding contents and providing informative features. Sokolova and Bobicev (2011) use sentiment analysis to analyze opinions expressed in healthrelated Web messages; Hong et al. (2012) focus on lexical differences to automatically distinguish schizophrenic patients from healthy individuals. Topic models have previously been used to mine health information: Resnik et al. (2013) use LDA to improve the prediction for neuroticism and depression on college students, while Paul and Dredze (2013) customize their factorial LDA to model the joint effect of drug, aspect, and route of administration. Most relevantly for the current paper, Nguyen et al. (2013) use LDA to discover autism-related topics, using a dataset of 10,000 posts from ten different autism commnities. However, their focus was on automated classification of communities as autism-related or not, rather than on analysis and on providing support for qualitative autism researchers. The applicability of the model developed in our paper towards classification tasks is a potential direction for future research. In general, topi"
W14-3212,D13-1133,0,0.0312785,"issues and featuring more closeknit and long-term relationships among members (Newton et al., 2009). Previous studies on mining health information show that technical models and tools from computational linguistics are helpful for both understanding contents and providing informative features. Sokolova and Bobicev (2011) use sentiment analysis to analyze opinions expressed in healthrelated Web messages; Hong et al. (2012) focus on lexical differences to automatically distinguish schizophrenic patients from healthy individuals. Topic models have previously been used to mine health information: Resnik et al. (2013) use LDA to improve the prediction for neuroticism and depression on college students, while Paul and Dredze (2013) customize their factorial LDA to model the joint effect of drug, aspect, and route of administration. Most relevantly for the current paper, Nguyen et al. (2013) use LDA to discover autism-related topics, using a dataset of 10,000 posts from ten different autism commnities. However, their focus was on automated classification of communities as autism-related or not, rather than on analysis and on providing support for qualitative autism researchers. The applicability of the model"
W14-3212,N10-1012,0,\N,Missing
W16-5704,D13-1192,0,0.0199611,". OFFLINE 3. ONLINE 0.14 0.32 0.34 0.27 0.47 0.55 0.33 0.27 0.26 0.20 0.29 0.29 0.30 0.34 0.35 Top systems from Trec-2014 TTG 4. TTGPKUICST2 (Lv et al., 2014) 5. EM50 (Magdy et al., 2014) 6. hltcoeTTG1 (Xu et al., 2014) 0.37 0.29 0.40 0.58 0.48 0.59 0.46 0.42 0.34 0.35 0.25 0.28 0.46 0.38 0.37 Table 1: Performance of Models in the TREC 2014 TTG Task. Weighted recall and F1 are indicated as Rec.w and F1w . Monte Carlo and Metropolis Hastings, and a custom data structure; this inference procedure was complex enough to warrant a companion paper (Ahmed et al., 2011b). The rCRP is also employed by Diao and Jiang (2013, 2014). In contrast, the dd-CRP makes no Markovian assumptions, and efficient inference is possible through relatively straightforward Gibbs sampling in a fixed window. 6 Conclusion We present a simple non-parametric model for clustering short documents (such as tweets) into storylines, which are conceptually coherent and temporally focused. Future work may consider learning more flexible temporal distance functions, which could potentially represent temporal periodicity or parametric models of content popularity. Acknowledgments We thank the reviewers for their helpful feedback. This researc"
W16-5704,P12-1056,0,0.0242903,"t see (Allan, 2002) for a summary of “first generation” research. More recent non-Bayesian approaches have focused on string overlap (Suen et al., 2013), submodular optimization (Shahaf et al., 2012), and locality-sensitive hashing (Petrovi´c et al., 2010). In Bayesian storyline analysis, the seminal models are Topics-OverTime (Wang and McCallum, 2006), which associates a parametric distribution over time with each topic (Ihler et al., 2006), and the Dynamic Topic Model (Blei and Lafferty, 2006), which models topic evolution as a linear dynamical system (Nallapati et al., 2007). Later work by Diao et al. (2012) offers a model for identifying “bursty” topics, with inference requiring dynamic programming. All these approaches require the number of topics to be identified in advance. Kim and Oh (2011) apply a distance-dependent Chinese Restaurant Franchise for temporal topic modeling; they evaluate using predictive likelihood rather than comparing against ground truth, and do not consider online inference. The Infinite Topic-Cluster model (Ahmed et al., 2011a) is non-parametric over the number of storylines, through the use of the recurrent Chinese Restaurant Process (rCRP). The model is substantially"
W16-5704,N10-1021,0,0.0561629,"Missing"
W16-5704,P14-2044,1,0.826614,"neat separation that it draws between textual content, which is treated as a stochastic emission from an unknown Multinomial distribution, and time, which is modeled as a prior on graphs over documents, through an arbitrary distance function. However, straightforward implementations of the dd-CRP are insufficiently scalable, and so the model 30 Proceedings of 2nd Workshop on Computing News Storylines, pages 30–35, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics has been relatively underutilized in the NLP literature (Titov and Klementiev, 2011; Kim and Oh, 2011; Sirts et al., 2014). We describe improvements to Bayesian inference that make the application of this model feasible, and present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014 (Lin et al., 2014). 2 Model The basic task that we address is to group short text documents into an unknown number of storylines, based on their textual content and their temporal signature. The textual content may be extremely sparse — the typical Tweet is on the order of ten words long — so leveraging temporal information is crucial. Moreover, the temporal signal is multiscale: in the 24-hour news cyc"
W16-5704,P11-1145,0,0.017032,"er, 2011). This model is distinguished by the neat separation that it draws between textual content, which is treated as a stochastic emission from an unknown Multinomial distribution, and time, which is modeled as a prior on graphs over documents, through an arbitrary distance function. However, straightforward implementations of the dd-CRP are insufficiently scalable, and so the model 30 Proceedings of 2nd Workshop on Computing News Storylines, pages 30–35, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics has been relatively underutilized in the NLP literature (Titov and Klementiev, 2011; Kim and Oh, 2011; Sirts et al., 2014). We describe improvements to Bayesian inference that make the application of this model feasible, and present encouraging empirical results on the Tweet Timeline Generation task from TREC 2014 (Lin et al., 2014). 2 Model The basic task that we address is to group short text documents into an unknown number of storylines, based on their textual content and their temporal signature. The textual content may be extremely sparse — the typical Tweet is on the order of ten words long — so leveraging temporal information is crucial. Moreover, the temporal signal"
W16-5903,P11-1049,0,0.199236,"ring sentences that cover 30 ψi = N X i PV si j xi,j PN i0 xi0 ,j Depth(i) , (8) where si is an indicator of whether EDU i appears in the summary, V is the vocabulary size, xi,j is the count of word j in EDU i, and PN i0 xi0 ,j counts the term frequency over the entire document. Summary EDU position Previous summarization research shows that the position of each sentence is an important factor in extractive summarization. We employ three positioned-based features: the minimum, maximum and average position of all EDUs appearing in the summary. Many more summarization features are considered by Berg-Kirkpatrick et al. (2011), and these may be incorporated in the model in future work. 3.3 Summary proposal distribution To use SampleRank to train from indirect supervision, we must augment the sample state to the tuple (s, d), where s is the summary and d is the discourse structure. The proposal distribution must therefore modify the summary as well as the discourse structure. Our proposal takes a stage-wise approach, first sampling a discourse structure d ∼ qd (dold ), and then sampling a summary conditioned on the discourse structure, s ∼ qs|d (d), such that s is guaranteed to obey the constraints described above."
W16-5903,D15-1263,1,0.840671,"nt model of discourse structure and summarization. 1 NON - VOLITIONAL CAUSE 1A 1C R 1B [The more people you love,]1A [the weaker you are.]1B [You’ll do things for them that you know you shouldn’t do.]1C Figure 1: An example Rhetorical Structure Theory parse of a small segment of text. with 1A, and then the combined unit 1A:B at the nucleus of its relationship with 1C. In any given discourse relation, the nucleus is more likely to relevant to a summary of the document (Marcu, 1999c), and its sentiment is more likely to be relevant to the overall document-level polarity (Heerschop et al., 2011; Bhatia et al., 2015). Thus, recovering this nuclearity structure is a key task for discourse parsing, with important practical applications. Introduction Rhetorical structure theory (RST) is a hierarchical model of document-level organization, in which segments of text are linked in binary or multi-way discourse relations (Mann and Thompson, 1988). Many RST relations are asymmetric, containing a nucleus and a satellite. An example is shown in Figure 1, with unit 1B as the nucleus of its relationship All known RST discourse parsers take one of two approximations, which are well known in structure prediction. In dy"
W16-5903,C14-1160,0,0.014533,"parsing from downstream tasks. A somewhat related line of work has used explicitly labeled discourse relations as a source of supervision for the classification of implicit discourse relations. Marcu and Echihabi (2002) were the first to explore this approach, working in the context of RST. Sporleder and Lascarides (2008) suggest that informational differences between explicit and implicit discourse relations limit the possible efficacy of this approach. More recent work has treated these two relation types as separate domains, obtaining good results by applying domain adaptation techniques (Braud and Denis, 2014; Ji et al., 2015). Recent work has applied a number of machine learning techniques to summarization, with particularly relevant work focusing on syntacticallymotivated sentence compression (Berg-Kirkpatrick et al., 2011). The combination of the proposed approach with abstractive summarization via sentence compression might yield better results on summarization metrics. Discourse structure has also been linked to sentence compression (Sporleder and Lapata, 2005), suggesting another intriguing direction for future work. Other recent machine learning approaches have employed neural attentional m"
W16-5903,J92-4003,0,0.121425,"epted with probability `(d `(d) . When the probability P (d |x) and scoring function ω(d) disagree, an update is made to θ to try to align the probability with the scoring metric. For more on the details of the algorithm, see the original paper (Wick et al., 2011). 2.1 Features We employ the following features for every internal node (discourse unit) of an RST tree: Lexical Features These features capture the first word and last word of both the left and right EDU of internal node. We also add lexical features combined with nuclearity of the EDU. 27 Cluster Features These features include the Brown et al. (1992) cluster prefix for last and first word of both left and right EDU of internal node. Syntactic Features These set of features employ POS tags for last and first word of both left and right EDU of internal node. Sentence-Paragraph Features We also add two features if left and right EDU are in same sentence and if they are in same paragraph. Text Organizational Features Each sample contains a complete nuclearity structure for the document, and we can compute global features of this structure. Specifically, we compute: whether the full RST tree is left sided, right sided or fully balanced; the se"
W16-5903,P14-1048,0,0.312954,"sists of 385 documents, with 347 for train and 38 for testing in the standard split. We only focus on nuclearity and span prediction tasks. We use the same F1 score on span and nuclearity as our evaluation metrics defined in the section 2.3. We compare our SampleRank approach with several competitive parsers from the literature: HILDA (Hernault et al., 2010), a bottom-up classification-driven parser; DPLP (Ji and Eisenstein, 2014), a shift-reduce parser that uses representation learning; and a condition random field (CRF) based parser with post-editing operations and a rich array of features (Feng and Hirst, 2014). SampleRank is competitive on the span metric, outperforming all systems except for the CRF approach, which employs rich linguistic features including syntax and entity transitions. On the nuclearity metric, the SampleRank-based parser does somewhat worse than these prior efforts. 4.2 3.4 Scoring function Evaluation Indirect supervision We evaluate our indirectly supervised model on the task of summarization for CNN news document and summaries, using the data. The data is obtained by crawling the CNN news website for news articles and the summaries are obtained by the bullet sections. We coll"
W16-5903,D13-1158,0,0.420828,"ution, it is capable of exploring the entire space of output configurations (in the limit). Furthermore, SampleRank can be trained using indirect supervision, which provides a potential solution to the problem of limited training data for discourse parsing. Because discourse nuclearity structures are closely linked to other documentlabeling tasks — such as summarization and sentiment analysis — it is in principle possible to use labels from those tasks as a supervision signal for discourse parsing itself. To do this, we link discourse structure and summarization using a constraint proposed by Hirao et al. (2013). SampleRank then explores the joint space of extractive summaries and discourse parses, scoring the summaries against automatically-obtained reference summaries, while simultaneously learning to produce discourse parses 26 that are compatible with high-scoring summaries. At this stage, we have obtained only mixed empirical results with the application of SampleRank to RST discourse parsing: SampleRank offers improvements on one metric for RST parsing in the supervised learning scenario, but it does not improve over a summarization baseline in the indirect supervision scenario. Nonetheless, we"
W16-5903,P14-1002,1,0.951888,"ost of ignoring aspects of document structure that may be relevant for identify25 Proceedings of the Workshop on Structured Prediction for NLP, pages 25–34, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics ing the correct parse. For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features. Alternatively, transition-based methods construct the discourse parse through a series of local decisions, typically driven by a classifier (Marcu, 1999b; Sagae, 2009; Ji and Eisenstein, 2014). While the classifier is free to examine any aspect of the document or the existing partial parse, the accuracy of such methods may be limited by search errors. A second limitation of existing discourse parsers relates to the amount of available training data. Because discourse is a high-level linguistic phenomenon, relatively large amounts of text must be annotated to produce each training instance. In RST, the smallest possible components of each discourse relation are elementary discourse units (EDUs), which correspond roughly to clauses. A relatively long news article might feature only a"
W16-5903,D15-1264,1,0.851639,"m tasks. A somewhat related line of work has used explicitly labeled discourse relations as a source of supervision for the classification of implicit discourse relations. Marcu and Echihabi (2002) were the first to explore this approach, working in the context of RST. Sporleder and Lascarides (2008) suggest that informational differences between explicit and implicit discourse relations limit the possible efficacy of this approach. More recent work has treated these two relation types as separate domains, obtaining good results by applying domain adaptation techniques (Braud and Denis, 2014; Ji et al., 2015). Recent work has applied a number of machine learning techniques to summarization, with particularly relevant work focusing on syntacticallymotivated sentence compression (Berg-Kirkpatrick et al., 2011). The combination of the proposed approach with abstractive summarization via sentence compression might yield better results on summarization metrics. Discourse structure has also been linked to sentence compression (Sporleder and Lapata, 2005), suggesting another intriguing direction for future work. Other recent machine learning approaches have employed neural attentional mechanisms for sent"
W16-5903,J15-3002,0,0.0954715,") is a hierarchical model of document-level organization, in which segments of text are linked in binary or multi-way discourse relations (Mann and Thompson, 1988). Many RST relations are asymmetric, containing a nucleus and a satellite. An example is shown in Figure 1, with unit 1B as the nucleus of its relationship All known RST discourse parsers take one of two approximations, which are well known in structure prediction. In dynamic programming approaches to discourse parsing, the feature space is locally restricted, allowing only features of discourse units that are sequentially adjacent (Joty et al., 2015), or adjacent in the discourse parse (Yoshida et al., 2014; Li et al., 2014). This makes exact inference possible, but at the cost of ignoring aspects of document structure that may be relevant for identify25 Proceedings of the Workshop on Structured Prediction for NLP, pages 25–34, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics ing the correct parse. For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features. Alternatively, transition-based methods c"
W16-5903,D14-1220,0,0.221113,"text are linked in binary or multi-way discourse relations (Mann and Thompson, 1988). Many RST relations are asymmetric, containing a nucleus and a satellite. An example is shown in Figure 1, with unit 1B as the nucleus of its relationship All known RST discourse parsers take one of two approximations, which are well known in structure prediction. In dynamic programming approaches to discourse parsing, the feature space is locally restricted, allowing only features of discourse units that are sequentially adjacent (Joty et al., 2015), or adjacent in the discourse parse (Yoshida et al., 2014; Li et al., 2014). This makes exact inference possible, but at the cost of ignoring aspects of document structure that may be relevant for identify25 Proceedings of the Workshop on Structured Prediction for NLP, pages 25–34, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics ing the correct parse. For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features. Alternatively, transition-based methods construct the discourse parse through a series of local decisions, typically"
W16-5903,W04-1013,0,0.0188861,"frontier size. Algorithm 1 Sample Rank algorithm for learning discourse parsing and extract summarization from indirect supervision 1: for e = 1 to #epochs do 2: for i = 1 to N do 0 3: d ∼ qd (· |xi , di ) 0 0 4: s ∼ qs|d (· |xi , d ) 0 0 0 5: y ← {d , s } 6: y + ← arg maxy∈{yi ,y0 } ω(y) 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: y − ← arg miny∈{yi ,y0 } ω(y) 0 yi ← acceptOrReject(y , yi ; θ t , ω, q) Of ← f (xi , y + ) − f (xi , y − ) ∆ω = ω(y + ) − ω(y − ) if ∆ω 6= 0 and θ &gt; t Of &lt; ∆ω then θ t+1 ← update(Of, ∆ω, θ t ) t←t+1 end if end for end for mary quality, which we do using the ROUGE metric (Lin, 2004). For completeness, Algorithm 1 presents our specialization of the SampleRank algorithm to learning joint discourse parsing and summarization from indirect summary-based supervision. 4 We evaluate the supervised model from § 2 on the RST parsing task, and the indirectly-supervised model § 3 on summarization. 4.1 In this setting, we receive no supervision on the discourse structure, only on the summary s. Our scoring function therefore can only quantify the sum31 Supervised evaluation The supervised model is evaluated on supervised task of discourse parsing on RST-DT dataset (Carlson et al., 20"
W16-5903,P02-1047,0,0.0948841,"algorithms for transition-based discourse parsing in the framework of Segmented Dis32 course Representation Theory (SDRT). Our proposed approach has the advantage of allowing arbitrary features, and avoiding local search errors; however, stochastic search is not guaranteed to fully explore the search space in any finite amount of time. We are unaware of prior work on indirect supervision for discourse parsing from downstream tasks. A somewhat related line of work has used explicitly labeled discourse relations as a source of supervision for the classification of implicit discourse relations. Marcu and Echihabi (2002) were the first to explore this approach, working in the context of RST. Sporleder and Lascarides (2008) suggest that informational differences between explicit and implicit discourse relations limit the possible efficacy of this approach. More recent work has treated these two relation types as separate domains, obtaining good results by applying domain adaptation techniques (Braud and Denis, 2014; Ji et al., 2015). Recent work has applied a number of machine learning techniques to summarization, with particularly relevant work focusing on syntacticallymotivated sentence compression (Berg-Kir"
W16-5903,P99-1047,0,0.518957,"ne, without discourse-level supervision. We obtain mixed results in the fully supervised case, and negative results for the joint model of discourse structure and summarization. 1 NON - VOLITIONAL CAUSE 1A 1C R 1B [The more people you love,]1A [the weaker you are.]1B [You’ll do things for them that you know you shouldn’t do.]1C Figure 1: An example Rhetorical Structure Theory parse of a small segment of text. with 1A, and then the combined unit 1A:B at the nucleus of its relationship with 1C. In any given discourse relation, the nucleus is more likely to relevant to a summary of the document (Marcu, 1999c), and its sentiment is more likely to be relevant to the overall document-level polarity (Heerschop et al., 2011; Bhatia et al., 2015). Thus, recovering this nuclearity structure is a key task for discourse parsing, with important practical applications. Introduction Rhetorical structure theory (RST) is a hierarchical model of document-level organization, in which segments of text are linked in binary or multi-way discourse relations (Mann and Thompson, 1988). Many RST relations are asymmetric, containing a nucleus and a satellite. An example is shown in Figure 1, with unit 1B as the nucleus"
W16-5903,C12-1115,0,0.0149475,"le to discourse-driven summarization than the RST data, or the difference may be explained HILDA’s superior performance on nuclearity metric. 5 Related Work Early work on RST discourse parsing focused on local classifiers (Marcu, 1999b; Hernault et al., 2010), with more recent work exploring structure prediction techniques such as sequence labeling (Joty et al., 2015), chart parsing (Li et al., 2014), and minimum spanning tree (Feng and Hirst, 2014). A parallel line of research has considered incremental discourse parsing techniques such as shift-reduce (Sagae, 2009; Ji and Eisenstein, 2014). Muller et al. (2012) apply more advanced search-based algorithms for transition-based discourse parsing in the framework of Segmented Dis32 course Representation Theory (SDRT). Our proposed approach has the advantage of allowing arbitrary features, and avoiding local search errors; however, stochastic search is not guaranteed to fully explore the search space in any finite amount of time. We are unaware of prior work on indirect supervision for discourse parsing from downstream tasks. A somewhat related line of work has used explicitly labeled discourse relations as a source of supervision for the classification"
W16-5903,D15-1044,0,0.0528258,"plied a number of machine learning techniques to summarization, with particularly relevant work focusing on syntacticallymotivated sentence compression (Berg-Kirkpatrick et al., 2011). The combination of the proposed approach with abstractive summarization via sentence compression might yield better results on summarization metrics. Discourse structure has also been linked to sentence compression (Sporleder and Lapata, 2005), suggesting another intriguing direction for future work. Other recent machine learning approaches have employed neural attentional mechanisms for sentence summarization (Rush et al., 2015), but to our knowledge such structure-free discriminatively trained approaches have not been applied on the document level. LEAD SampleRank, trained on CNN summaries (this work) TKP+SampleRank trained on RST treebank ROUGE-1 F score Recall ROUGE-2 F score Recall 0.2818 0.2317 0.2731 0.1154 0.0858 0.0967 0.2569 0.2304 0.2730 0.1042 0.0851 0.0963 Table 2: Evaluation of joint summarization and discourse parsing algorithm 6 Discussion This paper proposes a new structure learning approach for discourse parsing, based on the SampleRank algorithm. This approach has the potential to address two major"
W16-5903,W09-3813,0,0.142602,"but at the cost of ignoring aspects of document structure that may be relevant for identify25 Proceedings of the Workshop on Structured Prediction for NLP, pages 25–34, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics ing the correct parse. For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features. Alternatively, transition-based methods construct the discourse parse through a series of local decisions, typically driven by a classifier (Marcu, 1999b; Sagae, 2009; Ji and Eisenstein, 2014). While the classifier is free to examine any aspect of the document or the existing partial parse, the accuracy of such methods may be limited by search errors. A second limitation of existing discourse parsers relates to the amount of available training data. Because discourse is a high-level linguistic phenomenon, relatively large amounts of text must be annotated to produce each training instance. In RST, the smallest possible components of each discourse relation are elementary discourse units (EDUs), which correspond roughly to clauses. A relatively long news ar"
W16-5903,H05-1033,0,0.0439368,"More recent work has treated these two relation types as separate domains, obtaining good results by applying domain adaptation techniques (Braud and Denis, 2014; Ji et al., 2015). Recent work has applied a number of machine learning techniques to summarization, with particularly relevant work focusing on syntacticallymotivated sentence compression (Berg-Kirkpatrick et al., 2011). The combination of the proposed approach with abstractive summarization via sentence compression might yield better results on summarization metrics. Discourse structure has also been linked to sentence compression (Sporleder and Lapata, 2005), suggesting another intriguing direction for future work. Other recent machine learning approaches have employed neural attentional mechanisms for sentence summarization (Rush et al., 2015), but to our knowledge such structure-free discriminatively trained approaches have not been applied on the document level. LEAD SampleRank, trained on CNN summaries (this work) TKP+SampleRank trained on RST treebank ROUGE-1 F score Recall ROUGE-2 F score Recall 0.2818 0.2317 0.2731 0.1154 0.0858 0.0967 0.2569 0.2304 0.2730 0.1042 0.0851 0.0963 Table 2: Evaluation of joint summarization and discourse parsin"
W16-5903,D07-1047,0,0.0197308,"s only a few hundred documents. Prior work has frequently noted the connection between discourse nuclearity structure and summarization: for example, Marcu (1999c) shows that the nuclearity of a segment predicts its overall importance in the discourse, and Hirao et al. (2013) show that RST nuclearity trees can be exploited for single-document summarization in a constraintbased optimization framework. Summarization annotations are considerably easier to obtain than discourse parses, since they are often available “for free”, in the form of bullet-point summaries of news articles (Marcu, 1999a; Svore et al., 2007). We propose to exploit these annotations to train a discourse parser. We scrape a corpus of newspaper 29 articles and summaries from the CNN website. We then introduce the summary s as an additional variable, while using the discourse parse d to constrain the space of possible summaries: specifically, the elements of the text that align with the summary must be close to the root of the RST tree. By training a model to produce a good summary, we simultaneously train a discourse parser to produce nuclearity structures that are compatible with the ground truth summaries. In this way, a discourse"
W16-5903,W12-1623,0,0.037126,"Missing"
W16-5903,D14-1196,0,0.021513,", in which segments of text are linked in binary or multi-way discourse relations (Mann and Thompson, 1988). Many RST relations are asymmetric, containing a nucleus and a satellite. An example is shown in Figure 1, with unit 1B as the nucleus of its relationship All known RST discourse parsers take one of two approximations, which are well known in structure prediction. In dynamic programming approaches to discourse parsing, the feature space is locally restricted, allowing only features of discourse units that are sequentially adjacent (Joty et al., 2015), or adjacent in the discourse parse (Yoshida et al., 2014; Li et al., 2014). This makes exact inference possible, but at the cost of ignoring aspects of document structure that may be relevant for identify25 Proceedings of the Workshop on Structured Prediction for NLP, pages 25–34, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics ing the correct parse. For example, we may prefer balanced nuclearity structures, or we may prefer to avoid left-branching structures, but these properties cannot be captured with local features. Alternatively, transition-based methods construct the discourse parse through a series of local dec"
W16-5903,P14-1019,0,0.0283439,"esents a vector of features and θ represents a vector of weights. As noted above, prior work has largely focused on two restrictions to this model: either constraining the feature function f (·) to consider only local phenomena, or using a local, transition-based approach to incrementally construct the discourse nuclearity structure. Instead, we use stochastic search to identify the top-scoring discourse structure for any document. This enables the use of arbitrary features, while avoiding making premature commitments to local discourse structures. The SampleRank algorithm (Wick et al., 2011; Zhang et al., 2014) enables us to learn the weight vector θ in the context of this stochastic inference algorithm. To use SampleRank, we must define three things: • a feature function f (·); • a sampling distribution q(·); • a scoring function ω(·). At each step in the algorithm, we sample a discourse structure d0 ∼ q(d), where d is the previous discourse structure. This sample is then stochastically accepted or rejected, according to the MetropolisHastings algorithm: if the sample d0 achieves higher likelihood `(d0 ) than the previous sample `(d), then it is accepted; if not, the sample may still be ac0) cepted"
W18-1602,I13-1041,0,0.0144577,"lar approaches yield performance improvements in sentiment analysis and entity linking, we were unable to obtain performance improvements in part-of-speech tagging, despite strong evidence for the link between part-of-speech error rates and social network structure. 1 Introduction Social media feature greater diversity than the formal genres that constitute classic datasets such as the Penn Treebank (Marcus et al., 1993) and the Brown Corpus (Francis and Kucera, 1982): there are more authors, more kinds of authors, more varied communicative settings, fewer rules, and more stylistic variation (Baldwin et al., 2013; Eisenstein, 2013). Previous work has demonstrated precipitous declines in the performance of stateof-the-art systems for core tasks such as part-ofspeech tagging (Gimpel et al., 2011) and namedentity recognition (Ritter et al., 2010) when these 11 Proceedings of the 2nd Workshop on Stylistic Variation, pages 11–19 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics To measure the impact of socially-linked language variation, we focus on part-of-speech tagging, a fundamental task for syntactic analysis. First, we measure the extent to which tagger performanc"
W18-1602,J92-4003,0,0.168353,"e drawn in a manner that is agnostic to network structure, the training and test sets are expected to be more linguistically similar, and therefore, test set performance should be better. As shown in Table 3, the results support the theory: predictive accuracy is higher when the test and training sets are not drawn from different parts of the network. 4 Network p(y |x, a) = K X k=1 πa,k × pk (y |x) (1) The basis taggers pk (y |x) can be arbitrary conditional distributions. We use a hierarchical recurrent neural network model, in addition to a tag dictionary and Brown cluster surface features (Brown et al., 1992), which we describe in more detail in § 4.2. The component weighting distribution πa,k is conditioned on the social network G, and functions as an attentional mechanism, described in § 4.1. The main idea is that for a pair of authors ai and aj who are nearby in the social network G, the prediction rules should behave similarly if the attentional distributions are similar, i.e., πai ,k ≈ πaj ,k . If we have labeled training data for ai and wish to make predictions on author aj , some of the personalization from ai will be shared by aj . The overall classification approach can be viewed as a mix"
W18-1602,D15-1176,0,0.0364878,"Missing"
W18-1602,N13-1037,1,0.81014,"erformance improvements in sentiment analysis and entity linking, we were unable to obtain performance improvements in part-of-speech tagging, despite strong evidence for the link between part-of-speech error rates and social network structure. 1 Introduction Social media feature greater diversity than the formal genres that constitute classic datasets such as the Penn Treebank (Marcus et al., 1993) and the Brown Corpus (Francis and Kucera, 1982): there are more authors, more kinds of authors, more varied communicative settings, fewer rules, and more stylistic variation (Baldwin et al., 2013; Eisenstein, 2013). Previous work has demonstrated precipitous declines in the performance of stateof-the-art systems for core tasks such as part-ofspeech tagging (Gimpel et al., 2011) and namedentity recognition (Ritter et al., 2010) when these 11 Proceedings of the 2nd Workshop on Stylistic Variation, pages 11–19 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics To measure the impact of socially-linked language variation, we focus on part-of-speech tagging, a fundamental task for syntactic analysis. First, we measure the extent to which tagger performance is correlated wit"
W18-1602,D10-1124,1,0.813823,"Missing"
W18-1602,J93-2004,0,0.0623821,"r attempts to add robustness to stylistic variation, by building a mixture-of-experts model in which each expert is associated with a region of the social network. While prior work found that similar approaches yield performance improvements in sentiment analysis and entity linking, we were unable to obtain performance improvements in part-of-speech tagging, despite strong evidence for the link between part-of-speech error rates and social network structure. 1 Introduction Social media feature greater diversity than the formal genres that constitute classic datasets such as the Penn Treebank (Marcus et al., 1993) and the Brown Corpus (Francis and Kucera, 1982): there are more authors, more kinds of authors, more varied communicative settings, fewer rules, and more stylistic variation (Baldwin et al., 2013; Eisenstein, 2013). Previous work has demonstrated precipitous declines in the performance of stateof-the-art systems for core tasks such as part-ofspeech tagging (Gimpel et al., 2011) and namedentity recognition (Ritter et al., 2010) when these 11 Proceedings of the 2nd Workshop on Stylistic Variation, pages 11–19 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics"
W18-1602,P11-2008,1,0.812646,"Missing"
W18-1602,N13-1039,0,0.0703267,"Missing"
W18-1602,D14-1162,0,0.0811214,", the “dead” basis models will receive near-zero gradient updates, and therefore can never improve. Careful initialization of the parameters φk and bk and using L2-regularization parameters of the model helped mitigate the issue to some extent. Using the attentional weights computed using the sigmoid function as described in Equation 3 does not have this problem, but the final evaluation results were quite similar to the model with attentional weights computed using softmax as mentioned in Equation 2. 5 Accuracy Experimental Settings We use 100-dimensional pretrained Twitter GloVe embeddings (Pennington et al., 2014) which are 16 Network Actual Network Random Follow Mention Retweet 0.90 0.38 0.36 1.10 1.06 0.68 tity linking, where the existence of social relations between users is considered as a clue that the sentiment polarities in the messages from the users should be similar or the entities that they refer to in their messages are the same. Speriosu et al. (2011) constructs a heterogeneous network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2"
W18-1602,W10-0510,1,0.864899,"Missing"
W18-1602,N10-1020,0,0.0171356,"s and social network structure. 1 Introduction Social media feature greater diversity than the formal genres that constitute classic datasets such as the Penn Treebank (Marcus et al., 1993) and the Brown Corpus (Francis and Kucera, 1982): there are more authors, more kinds of authors, more varied communicative settings, fewer rules, and more stylistic variation (Baldwin et al., 2013; Eisenstein, 2013). Previous work has demonstrated precipitous declines in the performance of stateof-the-art systems for core tasks such as part-ofspeech tagging (Gimpel et al., 2011) and namedentity recognition (Ritter et al., 2010) when these 11 Proceedings of the 2nd Workshop on Stylistic Variation, pages 11–19 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics To measure the impact of socially-linked language variation, we focus on part-of-speech tagging, a fundamental task for syntactic analysis. First, we measure the extent to which tagger performance is correlated with network structure, finding that tagger performance on friends is significantly more correlated than would be expected by chance. We then design alternative training and test splits that are aligned with network str"
W18-1602,P11-1077,0,0.0182269,"is aligned with one or more social networks among authors on the microblogging platform Twitter. We choose Twitter because language styles in this platform are particularly diverse (Eisenstein et al., 2010), and because moderately large labeled datasets are available (Gimpel et al., 2011; Owoputi et al., 2013). We choose social networks for several reasons. First, they can readily be obtained from both metadata and behavioral traces on multiple social media platforms (Huberman et al., 2008). Second, social networks are strongly correlated with “demographic” author-level variables such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnellGinet, 2003), race (Green, 2002), and geography (Trudgill, 1974), thanks to the phenomenon of homophily, also known as assortative mixing (McPherson et al., 2001; Al Zamal et al., 2012). These demographic variables are in turn closely linked to language variation in American English (Wolfram and Schilling-Estes, 2005), and have been shown to improve some document classification tasks (Hovy, 2015). Third, there is growing evidence of the strong relationship between social network structures and language variation, even beyond the extent to which the social network"
W18-1602,W11-2207,0,0.0267871,"oblem, but the final evaluation results were quite similar to the model with attentional weights computed using softmax as mentioned in Equation 2. 5 Accuracy Experimental Settings We use 100-dimensional pretrained Twitter GloVe embeddings (Pennington et al., 2014) which are 16 Network Actual Network Random Follow Mention Retweet 0.90 0.38 0.36 1.10 1.06 0.68 tity linking, where the existence of social relations between users is considered as a clue that the sentiment polarities in the messages from the users should be similar or the entities that they refer to in their messages are the same. Speriosu et al. (2011) constructs a heterogeneous network with tweets, users, and n-grams as nodes, and the sentiment label distributions associated with the nodes are refined by performing label propagation over social relations. Tan et al. (2011) and Hu et al. (2013) leverage social relations for sentiment analysis by exploiting a factor graph model and the graph Laplacian technique respectively, so that the tweets belonging to social connected users share similar label distributions. Yang et al. (2016) proposed a neural based structured learning architecture for tweet entity linking, leveraging the tendency of s"
W18-1602,D16-1152,1,0.927218,"hich corresponds to domain adaptation across social network communities. This speaks to the importance of covering all relevant social network communities in training data. We then consider how to address the problem of language variation, by building social awareness into a recurrent neural tagging model. Our modeling approach is inspired by Yang and Eisenstein (2017), who train a mixture-of-experts for sentiment analysis, where the expert weights are computed from social network node embeddings. But while prior work demonstrated improvements in sentiment analysis and information extraction (Yang et al., 2016), this approach does not yield any gains on part-of-speech tagging. We conclude the paper by briefly considering possible reasons for this discrepancy, and propose approaches for future work in social adaptation of syntactic analysis.1 2 Dataset #Msg. #Tok. OCT27 DAILY547 1,827 547 26,594 7,707 Table 1: Annotated datasets: number of messages and tokens Network #Authors #Nodes #Edges Follow Mention Retweet 1,280 1,217 1,154 905,751 384,190 182,390 1,239,358 623,754 314,381 Table 2: Statistics for each social network dataset, which we refer to as follow, mention and retweet networks in Table 2."
W18-1602,Q17-1021,1,0.925847,"tagger performance on friends is significantly more correlated than would be expected by chance. We then design alternative training and test splits that are aligned with network structure, and find that test set performance decreases in this scenario, which corresponds to domain adaptation across social network communities. This speaks to the importance of covering all relevant social network communities in training data. We then consider how to address the problem of language variation, by building social awareness into a recurrent neural tagging model. Our modeling approach is inspired by Yang and Eisenstein (2017), who train a mixture-of-experts for sentiment analysis, where the expert weights are computed from social network node embeddings. But while prior work demonstrated improvements in sentiment analysis and information extraction (Yang et al., 2016), this approach does not yield any gains on part-of-speech tagging. We conclude the paper by briefly considering possible reasons for this discrepancy, and propose approaches for future work in social adaptation of syntactic analysis.1 2 Dataset #Msg. #Tok. OCT27 DAILY547 1,827 547 26,594 7,707 Table 1: Annotated datasets: number of messages and token"
W19-2513,P18-1220,0,0.117369,"s interest in improving the accuracy of OCR itself (e.g., Berg-Kirkpatrick et al., 2013). Several papers tackle the more general problem of OCR post-correction. An early example is the work of Tong and Evans (1996), who employ bigram word counts and character transduction probabilities to score corrections by their logprobability. However, their approach cannot handle whitespace erorrs (which they refer to as “runon” and “split-word” errors). Another approach is to train a supervised system from synthetic training data, using features such as proposed spelling corrections (Lund et al., 2011). Dong and Smith (2018) propose an alternative unsupervised training technique for OCR post-correction, which builds on character-level LSTMs. In their method, which they call seq2seq-noisy, they build an ensemble of post-processing systems. On each exTrue negative: A segmentation was not annotated, and the system does not propose one. The recall is computed as TP/(TP + FN), and the false positive rate is computed as FP/(FP + TN). 4 Related Work Results Results are shown in Figure 2 and in Table 1. The contextualized likelihood ratio obtains a recall of 0.768 at a false positive rate of 0.008, and a recall of 0.909"
W19-2513,P13-1021,0,0.0304217,"gmented. 5 False negative: A segmentation was annotated, and the system does not propose it. Dataset “cleanliness” is an increasingly salient issue for digital humanities research. Difficulties with optical character recognition (OCR) were highlighted in a 2018 report to the Mellon Foundation (Smith and Cordell, 2018), which outlines an agenda for research and infrastructure development in handling such texts. A key point from this report is that postprocessing of noisily digitized texts will continue to be important, despite the obvious interest in improving the accuracy of OCR itself (e.g., Berg-Kirkpatrick et al., 2013). Several papers tackle the more general problem of OCR post-correction. An early example is the work of Tong and Evans (1996), who employ bigram word counts and character transduction probabilities to score corrections by their logprobability. However, their approach cannot handle whitespace erorrs (which they refer to as “runon” and “split-word” errors). Another approach is to train a supervised system from synthetic training data, using features such as proposed spelling corrections (Lund et al., 2011). Dong and Smith (2018) propose an alternative unsupervised training technique for OCR pos"
W19-2513,P12-3029,0,0.0925743,"Missing"
W19-2513,W96-0108,0,0.799194,"alient issue for digital humanities research. Difficulties with optical character recognition (OCR) were highlighted in a 2018 report to the Mellon Foundation (Smith and Cordell, 2018), which outlines an agenda for research and infrastructure development in handling such texts. A key point from this report is that postprocessing of noisily digitized texts will continue to be important, despite the obvious interest in improving the accuracy of OCR itself (e.g., Berg-Kirkpatrick et al., 2013). Several papers tackle the more general problem of OCR post-correction. An early example is the work of Tong and Evans (1996), who employ bigram word counts and character transduction probabilities to score corrections by their logprobability. However, their approach cannot handle whitespace erorrs (which they refer to as “runon” and “split-word” errors). Another approach is to train a supervised system from synthetic training data, using features such as proposed spelling corrections (Lund et al., 2011). Dong and Smith (2018) propose an alternative unsupervised training technique for OCR post-correction, which builds on character-level LSTMs. In their method, which they call seq2seq-noisy, they build an ensemble of"
W19-4811,W18-5417,0,0.148525,"age models, evaluated their performance on grammatical agreement detection, and analyzed activation patterns within specific hidden units. We build on this analysis strategy as we aggregate (character-) sequence activation patterns across all hidden units in a model into quantitative measures. Language Arabic Bulgarian Coptic Danish Greek English Spanish Basque Persian Irish Hebrew Hindi Hungarian Indonesian Italian Latvian Russian Swedish Tamil Thai Turkish Ukrainian Vietnamese Chinese Substantial prior work exists on the character level as well (Karpathy et al., 2015; Vania and Lopez, 2017; Kementchedjhieva and Lopez, 2018; Gerz et al., 2018). Smith et al. (2018) examined the character component in multilingual parsing models empirically, comparing it to the contribution of POS embeddings and pre-trained embeddings. Chaudhary et al. (2018) leveraged crosslingual character-level correspondence to train NER models for low-resource languages. Godin et al. (2018), compared CNN and LSTM character models on a type-level prediction task on three languages, using the post-network softmax values to see which models identify useful character sequences. Unlike their analysis, we examine a more applied token-level task (PO"
W19-4811,N16-1030,0,0.0189038,"tify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages. 1 Introduction Subword vector representations are now a standard part of neural architectures for natural language processing (e.g., Bojanowski et al., 2017; Peters et al., 2018). In particular, character representations have been shown to handle out-of-vocabulary words in supervised tagging tasks (Ling et al., 2015; Lample et al., 2016). These advantages generalize across multiple languages, where morphological formation may differ greatly but the character composition of words remains a relatively reliable primitive (Plank et al., 2016). While the advantages of character-level models are readily apparent, existing evaluation methods fail to explain the mechanism by which these models encode linguistic knowledge about morphology and orthography. Different languages exhibit ∗ 2 Related Work Several recent papers attempt to explain neural network performance by investigating hidden state activation patterns on auxiliary or dow"
W19-4811,Q17-1010,0,0.0331147,"LSTM. We aggregate the behavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages. 1 Introduction Subword vector representations are now a standard part of neural architectures for natural language processing (e.g., Bojanowski et al., 2017; Peters et al., 2018). In particular, character representations have been shown to handle out-of-vocabulary words in supervised tagging tasks (Ling et al., 2015; Lample et al., 2016). These advantages generalize across multiple languages, where morphological formation may differ greatly but the character composition of words remains a relatively reliable primitive (Plank et al., 2016). While the advantages of character-level models are readily apparent, existing evaluation methods fail to explain the mechanism by which these models encode linguistic knowledge about morphology and orthography."
W19-4811,D15-1176,0,0.0719382,"Missing"
W19-4811,D18-1366,0,0.088262,"s across all hidden units in a model into quantitative measures. Language Arabic Bulgarian Coptic Danish Greek English Spanish Basque Persian Irish Hebrew Hindi Hungarian Indonesian Italian Latvian Russian Swedish Tamil Thai Turkish Ukrainian Vietnamese Chinese Substantial prior work exists on the character level as well (Karpathy et al., 2015; Vania and Lopez, 2017; Kementchedjhieva and Lopez, 2018; Gerz et al., 2018). Smith et al. (2018) examined the character component in multilingual parsing models empirically, comparing it to the contribution of POS embeddings and pre-trained embeddings. Chaudhary et al. (2018) leveraged crosslingual character-level correspondence to train NER models for low-resource languages. Godin et al. (2018), compared CNN and LSTM character models on a type-level prediction task on three languages, using the post-network softmax values to see which models identify useful character sequences. Unlike their analysis, we examine a more applied token-level task (POS tagging), and focus on the hidden states within the LSTM model in order to analyze its raw view of word composition. Affix† Morph synth‡ S S p S S S S = s = s S S S S s S S S ∅ S S ∅ S int fus agg fus fus fus fus agg fu"
W19-4811,Q16-1037,0,0.215101,"by which these models encode linguistic knowledge about morphology and orthography. Different languages exhibit ∗ 2 Related Work Several recent papers attempt to explain neural network performance by investigating hidden state activation patterns on auxiliary or downstream 1 https://github.com/ruyimarone/ character-eyes Work done while at Georgia Institute of Technology. 95 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 95–102 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics tasks. On the word level, Linzen et al. (2016) trained LSTM language models, evaluated their performance on grammatical agreement detection, and analyzed activation patterns within specific hidden units. We build on this analysis strategy as we aggregate (character-) sequence activation patterns across all hidden units in a model into quantitative measures. Language Arabic Bulgarian Coptic Danish Greek English Spanish Basque Persian Irish Hebrew Hindi Hungarian Indonesian Italian Latvian Russian Swedish Tamil Thai Turkish Ukrainian Vietnamese Chinese Substantial prior work exists on the character level as well (Karpathy et al., 2015; Vani"
W19-4811,Q18-1032,0,0.0400131,"Missing"
W19-4811,N18-1202,0,0.0460053,"ehavior of these units into language-level metrics which quantify the challenges that taggers face on languages with different morphological properties, and identify links between synthesis and affixation preference and emergent behavior of the hidden tagger layer. In a comparative experiment, we show how modifying the balance between forward and backward hidden units affects model arrangement and performance in these types of languages. 1 Introduction Subword vector representations are now a standard part of neural architectures for natural language processing (e.g., Bojanowski et al., 2017; Peters et al., 2018). In particular, character representations have been shown to handle out-of-vocabulary words in supervised tagging tasks (Ling et al., 2015; Lample et al., 2016). These advantages generalize across multiple languages, where morphological formation may differ greatly but the character composition of words remains a relatively reliable primitive (Plank et al., 2016). While the advantages of character-level models are readily apparent, existing evaluation methods fail to explain the mechanism by which these models encode linguistic knowledge about morphology and orthography. Different languages e"
W19-4811,D17-1010,1,0.933565,"g; = is equally prefixing/suffixing; ∅ is little affixation. ‡ Morphological synthesis: agglutinative, fusional, introflexive, isolating. 3 Tagging Task We train a set of LSTM tagging models, following the setup of Ling et al. (2015). A word representation trained from a character-level LSTM submodule is fed into a word-level bidirectional LSTM, with each word’s hidden state subsequently fed into a two-layer perceptron producing tag scores, which are then softmaxed to produce a tagging distribution. For languages with additional morphosyntactic attribute tagging, we follow the architecture in Pinter et al. (2017) where the same word-level Bi-LSTM states are used to predict each attribute’s value using its own perceptron+softmax scaffolding. In order to produce character models which would be as informative as possible to our subsequent analysis, we do not include word-level embeddings, pre-trained or otherwise, in our setup. Our analysis assumes a characterization of unit roles, where each hidden unit is observed to have some specific function. Findings from Linzen et al. (2016) and others suggest that a single hidden unit can learn to track complex syntactic rules. Radford et al. (2017) found that a"
W19-4811,D18-1365,0,0.0369365,"Missing"
W19-4811,P16-2067,0,0.118283,"its affects model arrangement and performance in these types of languages. 1 Introduction Subword vector representations are now a standard part of neural architectures for natural language processing (e.g., Bojanowski et al., 2017; Peters et al., 2018). In particular, character representations have been shown to handle out-of-vocabulary words in supervised tagging tasks (Ling et al., 2015; Lample et al., 2016). These advantages generalize across multiple languages, where morphological formation may differ greatly but the character composition of words remains a relatively reliable primitive (Plank et al., 2016). While the advantages of character-level models are readily apparent, existing evaluation methods fail to explain the mechanism by which these models encode linguistic knowledge about morphology and orthography. Different languages exhibit ∗ 2 Related Work Several recent papers attempt to explain neural network performance by investigating hidden state activation patterns on auxiliary or downstream 1 https://github.com/ruyimarone/ character-eyes Work done while at Georgia Institute of Technology. 95 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks f"
W19-4811,D18-1291,0,0.0487874,"Missing"
W19-4811,P17-1184,0,0.0210352,"016) trained LSTM language models, evaluated their performance on grammatical agreement detection, and analyzed activation patterns within specific hidden units. We build on this analysis strategy as we aggregate (character-) sequence activation patterns across all hidden units in a model into quantitative measures. Language Arabic Bulgarian Coptic Danish Greek English Spanish Basque Persian Irish Hebrew Hindi Hungarian Indonesian Italian Latvian Russian Swedish Tamil Thai Turkish Ukrainian Vietnamese Chinese Substantial prior work exists on the character level as well (Karpathy et al., 2015; Vania and Lopez, 2017; Kementchedjhieva and Lopez, 2018; Gerz et al., 2018). Smith et al. (2018) examined the character component in multilingual parsing models empirically, comparing it to the contribution of POS embeddings and pre-trained embeddings. Chaudhary et al. (2018) leveraged crosslingual character-level correspondence to train NER models for low-resource languages. Godin et al. (2018), compared CNN and LSTM character models on a type-level prediction task on three languages, using the post-network softmax values to see which models identify useful character sequences. Unlike their analysis, we examine a"
W19-5028,D18-1412,0,0.0697144,"in Section 4. Results Results are presented in Tables 3 and 4. Results are consistent with previous experiments in that augmentation with concept annotations does not improve performance. For both ontologies, neither the Dummy Concepts nor the Concepts Only models outperform the full-text Approach 2: Multi-task Learning We present an alternative application of cTAKES as a form of distant supervision. Our approach is inspired by recent successes in multi-task learning for NLP which demonstrate that cheaply-obtained labels framed as an auxiliary task can improve performance on downstream tasks (Swayamdipta et al., 2018; Ruder, 2017; Zhang and Weiss, 2016). We propose to predict clinical information extraction system annotations as an auxiliary task, and share lower-level representations with the clinical coding task through a jointly-trained model architecture. We hypothesize that domain-knowledge embedded in cTAKES will guide the shared layers of the model architecture towards a more optimal representation for the clinical coding task. We formulate the auxiliary task as follows: given each word-embedding or word-embedding span in the input which cTAKES has assigned a code, can the model predict the code as"
W19-5028,D18-1352,0,\N,Missing
W19-5028,D18-1306,0,\N,Missing
W19-5028,N18-1189,0,\N,Missing
