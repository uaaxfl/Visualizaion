2007.mtsummit-ucnlg.19,W06-1413,0,\N,Missing
2007.mtsummit-ucnlg.19,W06-1410,1,\N,Missing
2007.mtsummit-ucnlg.19,W07-2307,0,\N,Missing
2007.mtsummit-ucnlg.19,J03-1003,1,\N,Missing
2020.evalnlgeval-1.3,W05-0909,0,0.0826544,"skewed distribution of predicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does no"
2020.evalnlgeval-1.3,W00-1401,0,0.387257,"ndly or pleasant to use. Some high-level properties are fairly subjective, and may best be evaluated using human ratings. Still one could argue that these properties may be decomposed into a set of different abilities. For example: using the correct register, being able to translate jargon into layman’s terms, generating unambiguous descriptions. a N OUN P HRASE M OD M OD H EIGHT M OOD tall happy H EAD N OUN AGE N OUN toddler Figure 1: Parse tree for the phrase: a tall happy toddler. an overview of HPSG grammars (Pollard and Sag, 1994) that are available for other languages.6 In related work, Bangalore et al. (2000) use automated parsers for evaluation, but they compare the parse trees for the system outputs with those of the reference data, and compute an accuracy metric. At this point, it is fair to say that not all languages are as well-resourced as English. Of course we can evaluate grammaticality if there is a relatively complete description of a language. But not everyone has that luxury. Moreover, as NLG researchers, we aren’t just interested in grammaticality. Why should we care about grammars, then? There are two ways to respond to this criticism. First, if you accept that NLG evaluation is a go"
2020.evalnlgeval-1.3,W11-2817,0,0.0261847,"predicates and entities, or we can select only specific templates/realisations to have a particular distribution of the data. Here are the aspects that we imagine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the numb"
2020.evalnlgeval-1.3,W19-8651,0,0.0120485,"ntities, or we can select only specific templates/realisations to have a particular distribution of the data. Here are the aspects that we imagine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the number of different order"
2020.evalnlgeval-1.3,W18-6507,0,0.0125555,"have a particular distribution of the data. Here are the aspects that we imagine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the number of different orders in which triples are realised in the output texts. (E.g. maintaining the inp"
2020.evalnlgeval-1.3,W18-6521,1,0.915613,"r example, is a state-of-the-art dataset. It offers an excellent overview table (Table 1 in Gardent et al. 2017) describing properties of the input (e.g. number of different predicates, number of combinations of RDF triples, relations between the different triples) and output (e.g. number of sentences verbalising different amounts of triples). Still missing from the description of the WebNLG corpus is the distribution of different predicates. Figure 2 shows the frequency distribution of different labels in the training set (computed using the XML files from the enriched WebNLG dataset; Castro Ferreira et al. 2018). The plot reveals that the data is heavily skewed, with 76 predicates (out of 246) occurring fewer than 10 times, while the most frequent predicate (‘country’) occurs 2150 times. End-to-end systems will probably perform worse on the tail of the distribution (where example outputs are scarce) than on the head (where examples are plentiful). On the output side, it is not clear from the original WebNLG corpus how many different possible lexicalisations there are for each predicate.8 This is difficult to study with unstructured text output, but luckily the enriched WebNLG dataset converted the ou"
2020.evalnlgeval-1.3,W14-3348,0,0.0139143,"edicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the"
2020.evalnlgeval-1.3,W19-8652,0,0.0419252,"Missing"
2020.evalnlgeval-1.3,P14-2074,0,0.029397,"hese systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknesses of a particular model, or model architecture. This paper argues that we need alternative (or at least additional) metrics to provide this kind of insight. We believe that rule-based approaches are well-suited for this task. 1.1 Not just BLEU; also uncontrolled data BLEU is an easy target; it’s a quick-and-dirty solution that ignores paraphrases and different-butvalid perspectives on the input data. But if we only look at the metr"
2020.evalnlgeval-1.3,2020.emnlp-main.393,0,0.0206675,"have the same weather but different place names. NLG models trained on such a corpus would only learn to produce a fixed weather template, where they should copy in the name from the input. An evaluation is only meaningful if there are clear differences in all (combinations of) variables, between training and test set. At the same time, the training data should also not contain so much variation that it’s impossible to detect any pattern. It is an open question 7 For further discussion of shared tasks and leaderboards in NLP, see: Parra Escart´ın et al. 2017; Nissim et al. 2017; Rogers 2019; Ethayarajh and Jurafsky 2020. how much systematicity (and redundancy) there should be in the training data for NLG systems to learn how to perform any language generation task. Finally, it is important to have specific information about the output. For example: how many different ways are there to verbalise the same predicates, entities, numbers, dates and times? Without this information, it is impossible to say anything about the complexity of the task. 4.2 Are current datasets sufficient? We don’t believe current datasets are sufficient to measure the extent to which systems are able to generalise from the training dat"
2020.evalnlgeval-1.3,W19-5204,0,0.0251958,"roach to produce NLG test benches. We believe 1 Although there are also benefits to having a more uncontrolled elicitation task. For example, having fewer constraints means that the resulting data will be more diverse. 2 This is not just a problem in NLG. Freitag et al. (2020, and references therein) describe how human translators tend to produce translationese: translations that overly rely on the source text, resulting in less natural-sounding texts. This reduces the diversity of the evaluation data for Machine Translation (MT), which has strong effects on the evaluation metrics used in MT (Freitag et al., 2019). The authors go on to show that we can improve the correlation between modern evaluation metrics and human ratings, by improving the reference data (in this case: asking linguists to generate more fluent and diverse translations). But of course, this kind of exercise is expensive and time-consuming. 17 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 17–27, Online (Dublin, Ireland), December 2020. that a rule-based approach (combined with new or existing NLG data) would again be ideal. D ET 1.2 The downside of end-to-end systems; opportunities for rule-based approaches Ther"
2020.evalnlgeval-1.3,2020.emnlp-main.5,0,0.0199483,"corpora, is that quality control is difficult. And even if we can control the quality of the data, it is very hard to control the diversity of the generated texts.2 This makes it harder to study the ability of NLG systems to generalise from the training data to unseen instances. We will argue (in Section 4) that we need a more systematic approach to produce NLG test benches. We believe 1 Although there are also benefits to having a more uncontrolled elicitation task. For example, having fewer constraints means that the resulting data will be more diverse. 2 This is not just a problem in NLG. Freitag et al. (2020, and references therein) describe how human translators tend to produce translationese: translations that overly rely on the source text, resulting in less natural-sounding texts. This reduces the diversity of the evaluation data for Machine Translation (MT), which has strong effects on the evaluation metrics used in MT (Freitag et al., 2019). The authors go on to show that we can improve the correlation between modern evaluation metrics and human ratings, by improving the reference data (in this case: asking linguists to generate more fluent and diverse translations). But of course, this kin"
2020.evalnlgeval-1.3,W17-3518,0,0.0995736,"oroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknesses of a particular model, or model architecture. This paper argues that we need alternative (or at least additional) metrics t"
2020.evalnlgeval-1.3,W09-0613,0,0.0325752,"ool that incorporates all different systems. To find these systems and assess the feasibility of our proposal, we used the Publish or Perish software9 to retrieve all pub9 Search carried out on the 17th of August, 2020, using the macOS GUI edition, version 7.25.2877.7516. Software Triple: hSAGE Publications, founder, Sara Miller McCunei Text: Sara Miller McCune founded SAGE Publications. Template: ENT-1 founded ENT-2. Mapping: ENT-1: Sara Miller McCune ENT-2: SAGE Publications Table 2: Example from the extended WebNLG corpus. lications on Google Scholar that cite the original SimpleNLG paper (Gatt and Reiter, 2009).10 We found 361 publications referring to SimpleNLG on Google Scholar, coming from a wide array of different venues. We are still in the process of analysing the results, but our impression is that only a small proportion of the reported systems is useful. Many are either unavailable, form part of a larger pipeline, or use proprietary/personal data (e.g. BT-Nurse; Hunter et al. 2012). 5.2 From datasets to rules, and back again Another approach is to construct our own templatedriven corpus generator, based on existing datasets. Table 2 shows part of an entry from the extended WebNLG corpus. Th"
2020.evalnlgeval-1.3,P16-2043,0,0.0135804,"datasets are used to determine whether systems are able to generalise from experience to unseen situations (Mitchell, 1997). To test this, researchers typically use separate training, development, and test sets. Different models are trained using the training set, the best model is selected using the development set, and then we evaluate its performance on the test set. 4.1 Requirements to measure generalisation Using different splits is necessary, but not sufficient for NLG tasks. We can see this when we look at the generation of weather forecasts, a popular topic in the NLG community (e.g. Gkatzia et al. 2016 and references therein). It is not good enough to only have a corpus where all inputs have the same weather but different place names. NLG models trained on such a corpus would only learn to produce a fixed weather template, where they should copy in the name from the input. An evaluation is only meaningful if there are clear differences in all (combinations of) variables, between training and test set. At the same time, the training data should also not contain so much variation that it’s impossible to detect any pattern. It is an open question 7 For further discussion of shared tasks and le"
2020.evalnlgeval-1.3,W04-1013,0,0.0244245,"ution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknes"
2020.evalnlgeval-1.3,2020.acl-main.448,0,0.0402381,"ineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknesses of a particular model, or model architecture. This paper argues that we need alternative (or at least additional) metrics to provide this kind of insight. We believe that rule-based approaches are well-suited for this task. 1.1 Not just BLEU; also uncontrolled data BLEU is an easy target; it’s a quick-and-dirty solution that ignores paraphrases and different-butvalid perspectives on the input data. But if we only look at the metrics, we miss the elephant in the room: the corpora we use to train NLG systems a"
2020.evalnlgeval-1.3,W16-6630,0,0.0152345,". Here are the aspects that we imagine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the number of different orders in which triples are realised in the output texts. (E.g. maintaining the input order, ordering triples alph"
2020.evalnlgeval-1.3,2020.lantern-1.4,1,0.833313,"Missing"
2020.evalnlgeval-1.3,W17-3503,1,0.878332,"Missing"
2020.evalnlgeval-1.3,W18-6550,1,0.885032,"Missing"
2020.evalnlgeval-1.3,J17-4007,0,0.0352085,"Missing"
2020.evalnlgeval-1.3,W17-5525,0,0.0370535,"Missing"
2020.evalnlgeval-1.3,W16-6644,0,0.0202933,"a. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknesses of a particular model, or model architecture. This paper argues that we need alter"
2020.evalnlgeval-1.3,W14-4412,0,0.0586578,"Missing"
2020.evalnlgeval-1.3,W18-5019,0,0.0211321,"three major advantages, compared to the use of crowd-sourced data: 1. It is efficient and cost-effective to produce large corpora, since no human annotators are needed. 2. We can easily create different sub-corpora with very specific distributions of the input data, which would allow us to estimate the extent to which systems are able to generalise from low-frequent training examples. 3. It allows us to automatically evaluate the quality of the output in ways that are not possible (or very labor-intensive) with humangenerated data. The generate-and-train approach has recently been applied by Oraby et al. (2018, using the PER SONAGE system; Mairesse and Walker 2010) to create a synthetic corpus of utterances in the RESTAU RANT domain, where the authors controlled the personality of the utterances. Oraby et al. showed that it is possible for neural NLG models to distinguish style and content, and that models trained on their data were able to generate meaningful output with the desired personality traits. The idea of training NLG-systems based on the output of other NLG systems is controversial. Ehud Reiter argues on his blog that this is just reverseengineering existing systems (Reiter, 2017). This"
2020.evalnlgeval-1.3,P02-1040,0,0.110465,"set, which is revealed to have a skewed distribution of predicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested (without any confounds) once we are able to systematically manipulate the skewness of the data, using a rule-based approach. 1 Introduction Recent years have seen many Natural Language Generation (NLG) researchers move away from rule-based systems, and towards neural end-to-end systems. These systems are typically evaluated using textual similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020"
2020.evalnlgeval-1.3,W17-1608,0,0.0500517,"Missing"
2020.evalnlgeval-1.3,C16-1141,0,0.0677096,"Missing"
2020.evalnlgeval-1.3,W17-3521,0,0.0239379,"gine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the number of different orders in which triples are realised in the output texts. (E.g. maintaining the input order, ordering triples alphabetically or based on their content.)"
2020.evalnlgeval-1.3,J18-3002,0,0.015019,"similarity metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), or ROUGE (Lin, 2004), on large corpora of crowd-sourced texts (e.g., the E2E dataset, Novikova et al. 2016; the WebNLG dataset, Gardent et al. 2017; or MS COCO, Lin et al. 2014). This evaluation strategy tells us to what extent the generated texts are similar to the reference data, but it is often difficult to determine exactly what that resemblance buys us. By now it is well-known that BLEU correlates poorly with human ratings (Elliott and Keller, 2014; Kilickaya et al., 2017; Reiter, 2018; Sulem et al., 2018; Mathur et al., 2020), but BLEU by itself also does not tell us anything about the strengths and weaknesses of a particular model, or model architecture. This paper argues that we need alternative (or at least additional) metrics to provide this kind of insight. We believe that rule-based approaches are well-suited for this task. 1.1 Not just BLEU; also uncontrolled data BLEU is an easy target; it’s a quick-and-dirty solution that ignores paraphrases and different-butvalid perspectives on the input data. But if we only look at the metrics, we miss the elephant in the room:"
2020.evalnlgeval-1.3,P19-1355,0,0.0175899,"did not address yet is how to parse imperfect outputs. There are no guarantees that the output of end-to-end systems will conform to any of the rules through which the corpus was generated. Using a strict approach, we could say that faulty output just doesn’t count; if it is flawed, the system simply did not fully learn the relevant rules. But perhaps we would also like to give partial credit to systems that almost learned how to perform the generation task. We leave this as a question for future research.13 12 So next to environmental issues caused by computationally heavy approaches to NLP (Strubell et al., 2019), we can also say that such approaches are an obstacle to properly evaluate new systems. 13 But note that the texts (and probably system outputs as well) are very predictable. This makes it interesting to explore whether metrics based on edit distance could work here, even though they have been shown to be inadequate ‘in the wild.’ Predictions 2. The number of examples needed to successfully learn a template, depends on the amount of alternative templates that could also verbalise the same predicate, the amount of predicates in the corpus, and the size of the corpus. 3. It is easier to learn h"
2020.evalnlgeval-1.3,D18-1081,0,0.0445454,"Missing"
2020.evalnlgeval-1.3,W13-2125,0,0.0203994,"y specific templates/realisations to have a particular distribution of the data. Here are the aspects that we imagine may be interesting to manipulate: • The number of different templates/entities in the train, validation, and test sets. (Note that templates and entities may be manipulated separately from each other.) available from: https://harzing.com/resources/ publish-or-perish 10 This approach excludes many systems using SimpleNLG in a different language, e.g. Brazilian Portuguese (de Oliveira and Sripada, 2014), Dutch (de Jong, 2018), German (Bollmann, 2011; Braun et al., 2019), French (Vaudry and Lapalme, 2013), Galician (Cascallar-Fuentes et al., 2018), Italian (Mazzei et al., 2016), or Spanish (Ramos-Soto et al., 2017). 21 • The frequency with which those different templates/entities each occur. Is there a uniform distribution, or do some templates/entities occur more than others? • The overlap in terms of templates/entities between the train, validation, and test sets. Here we may also choose to generate multiple different test sets to accompany the same training set, to make evaluation more efficient. • The ordering principles, and the number of different orders in which triples are realised in"
2020.inlg-1.10,P14-5010,0,0.00455171,"i and Gardent (2017), we employ the following size metrics to compare the Enriched WebNLG dataset (Castro Ferreira et al., 2018) to our dataset (see Table 1): • Number of instances: Absolute number of texts in the dataset (single sentences for CACAPO, single sentences and multi-sentence phrases for WebNLG). This gives a direct indication of the dataset size. Delexicalization was done by a script that matches the annotated data with the original string, using the string location information provided by Prodigy. The annotation of syntactical information and lemmatization was done using CoreNLP (Manning et al., 2014) for English, and DeepFrog10 for Dutch. • Number of unique MRs: Number of different MRs appearing in the dataset (set of attribute-value paired data for CACAPO, set of RDF-triple data for WebNLG aligned to a text). Besides dataset size, this also gives an indication of training difficulty: more unique MRs means a greater challenge to train models on the data. 4. Referring Expression Generation is the task of generating the correct entities in a text (Krahmer and van Deemter, 2012). In this step, a system can be trained to fill the ENTITY-[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] placeholders found in the"
2020.inlg-1.10,D19-6301,0,0.0161395,"nt work The current work introduces the CACAPO dataset which addresses the aforementioned limitations of the existing datasets: it contains intermediate representations for discourse ordering, text structuring, lexicalization, referring expression generation, and textual realization for pipeline approaches • Dutch weather domain texts cover severaldaily short-term weather forecasts for The 3 2 See https://github.com/TallChris91/ CACAPO-Dataset for the collection tools. 4 http://www.nexisuni.com/ At least, datasets that start from data. Surface realization datasets such as the one employed in (Mille et al., 2019) can be seen as facilitating the pipeline approach. 70 Netherlands from the Royal Netherlands Meteorological Institute (KNMI); the Dutch national weather service. These texts originate from the “complete weather report” prognosis, found on the KNMI website5 . The weather reports were obtained for all of 2019, totalling 5,897 texts (1,099,556 tokens; 1,076 types). reports from 596 websites ranging from 2012 to 2019 (1,105,567 tokens; 26,968 types). Thus, in total 51,575 texts were collected via these different methods. For the CACAPO dataset, all texts above 325 words were discarded as most bas"
2020.inlg-1.10,W17-3537,0,0.018841,"ently the only other dataset viable for both end-to-end, as well as neural pipeline architectures. The present paper thus presents a new automatically scraped dataset that can be used for end-toend, as well as neural pipeline architectures. Furthermore, it describes a collection process inspired by Oraby et al. (2019), where collection starts with the news reports and attribute-value datapoints are constructed from them, which also enables relatively low-effort extension and adaptation of the current dataset (Section 3). Characteristics of the dataset are described based on the methodology by Perez-Beltrachini and Gardent (2017) (Section 4). Finally, a baseline is developed for the dataset using TGen (Duˇsek and Jurˇc´ıcˇ ek, 2015) (Section 5). The full dataset is freely available for research purposes upon request, licensed under AusGoal Restrictive Licence. A ‘thin’ version of the dataset that contains the annotated data in combination with the URLs of the scraped texts and the scraping tools is publicly available via https://github. com/TallChris91/CACAPO-Dataset, licensed under CC BY-NC-SA. This paper describes the CACAPO dataset, built for training both neural pipeline and endto-end data-to-text language generat"
2020.inlg-1.10,W19-8645,0,0.0202586,"exts from the sports, weather, stocks, and incidents domain for both Dutch and English. 2.2 3 Control bottleneck Furthermore, most existing datasets are constructed for end-to-end architectures, where the non-lingustic input is converted into natural language without explicit intermediate representations in between (Castro Ferreira et al., 2019). By contrast, researchers have started to experiment with neural pipeline methods, in which the data conversion process happens via one or more explicit intermediate transformations (see, for instance, Castro Ferreira et al., 2019; Jiang et al., 2020; Moryossef et al., 2019a,b). These methods enable the control over parts of the data-to-text conversion process, making it possible to develop hybrid (e.g. rule-based and neural) systems. Additionally, a direct comparison between end-to-end and pipeline approaches suggests that pipeline approaches lead to improved output quality, and decreases data hallucination and data omission; two challenges for datasets compiled using unedited texts from publicly available sources (Castro Ferreira et al., 2019). However, pipeline architectures require a training dataset containing the intermediate representations in order to be"
2020.inlg-1.10,W18-6319,0,0.0134508,"s. Furthermore, the fact that these texts were derived from ‘naturally occurring’ Baseline system performance TGen, a sequence-to-sequence model using Attention (Duˇsek and Jurˇc´ıcˇ ek, 2015), was used to establish a baseline on the CACAPO dataset.11 The performance of TGen was evaluated on the test data of the CACAPO dataset using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015) and 12 METEOR and BertScore were calculated using the authors’ provided scripts, while BLEU was calculated using SacreBLEU (Post, 2018), NIST using NLTK (Bird et al., 2009), and ROUGE-L and CIDEr using nlg-eval (Sharma et al., 2017). 11 Parameters are provided in Appendix B. It should be noted that the system is only trained in an end-to-end fashion. 75 Domain Incidents (Dutch) Stocks (Dutch) Sports (Dutch) Weather (Dutch) Incidents (English) Stocks (English) Sports (English) Weather (English) BLEU NIST BertScore METEOR ROUGE-L CIDEr 4.65 17.46 1.92 1.66 0.68 0.41 1.27 6.80 1.13 2.44 0.86 0.15 0.36 0.26 0.64 1.20 70.29 74.13 68.39 64.11 82.37 80.08 82.50 86.24 10.93 21.84 7.34 7.11 5.92 3.19 5.66 8.74 20.02 27.95 13.85 11.71"
2020.inlg-1.10,W16-2607,0,0.0573518,"Missing"
2020.inlg-1.10,N19-1236,0,0.0224508,"exts from the sports, weather, stocks, and incidents domain for both Dutch and English. 2.2 3 Control bottleneck Furthermore, most existing datasets are constructed for end-to-end architectures, where the non-lingustic input is converted into natural language without explicit intermediate representations in between (Castro Ferreira et al., 2019). By contrast, researchers have started to experiment with neural pipeline methods, in which the data conversion process happens via one or more explicit intermediate transformations (see, for instance, Castro Ferreira et al., 2019; Jiang et al., 2020; Moryossef et al., 2019a,b). These methods enable the control over parts of the data-to-text conversion process, making it possible to develop hybrid (e.g. rule-based and neural) systems. Additionally, a direct comparison between end-to-end and pipeline approaches suggests that pipeline approaches lead to improved output quality, and decreases data hallucination and data omission; two challenges for datasets compiled using unedited texts from publicly available sources (Castro Ferreira et al., 2019). However, pipeline architectures require a training dataset containing the intermediate representations in order to be"
2020.inlg-1.10,2021.wnut-1.39,0,0.0894948,"Missing"
2020.inlg-1.10,W19-8639,0,0.0239619,"ystems may be especially relevant (i.e., press agencies, publishers, weather institutes, etc.), oftentimes have an extensive archive of historical data and human-written texts, that would contain similar types of ‘noise’ in their data representation. Therefore, it seems imperative to also pursue other dataset collection techniques—such as text and data collection—by scraping publicly available sources. Datasets that were compiled via this method have also seen a surge recently, with YelpNLG (Oraby et al., 2019), RotoWire (Wiseman et al., 2017), and RotoWire-inspired datasets like RotoWire-FG (Wang, 2019) and MLB (Puduppully et al., 2019). Using this method Neural data-to-text NLG models have the ability to produce texts without requiring handwritten rules and templates, generating texts in a completely data-driven way. However, neural data-to-text NLG is struggling to overcome two critical bottlenecks, identified by Oraby et al. (2019), that hamper the performance of the models: (1) a data bottleneck, a lack of (high quality, large scale) parallel data-text datasets; and (2) a control bottleneck, which they describe as an inability to control stylistic variation, but can be more broadly descr"
2020.inlg-1.10,D19-1053,0,0.0188019,"direct descriptions of the data in the text. This is challenging for NLG systems, as shown by the system performance scores when performing an end-to-end data-to-text task on the dataset using TGen (Duˇsek and Jurˇc´ıcˇ ek, 2015). However, the dataset closely mirrors real-world scenarios in which companies oftentimes have large amounts of human-written texts that are not purposefully written for NLG applications, accompanied by corresponding data. texts to be of reasonable quality. Recent learningbased metrics, such as RUSE (Shimanaka et al., 2018), BertScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020) might be more viable options, since they claim to capture semantic similarity. However, we discourage using this dataset as a leaderboard chasing game and recommend using various types of evaluation methods to evaluate systems trained on the CACAPO dataset (e.g., evaluating the results on the dataset using human and automatic metrics, and qualitative and quantitative research methods). Variety in evaluation methods ensures that the results obtained on this dataset are put into a broad perspective. This will give valuable insights into the systems trained on t"
2020.inlg-1.10,2020.acl-main.704,0,0.0219641,"in the text. This is challenging for NLG systems, as shown by the system performance scores when performing an end-to-end data-to-text task on the dataset using TGen (Duˇsek and Jurˇc´ıcˇ ek, 2015). However, the dataset closely mirrors real-world scenarios in which companies oftentimes have large amounts of human-written texts that are not purposefully written for NLG applications, accompanied by corresponding data. texts to be of reasonable quality. Recent learningbased metrics, such as RUSE (Shimanaka et al., 2018), BertScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020) might be more viable options, since they claim to capture semantic similarity. However, we discourage using this dataset as a leaderboard chasing game and recommend using various types of evaluation methods to evaluate systems trained on the CACAPO dataset (e.g., evaluating the results on the dataset using human and automatic metrics, and qualitative and quantitative research methods). Variety in evaluation methods ensures that the results obtained on this dataset are put into a broad perspective. This will give valuable insights into the systems trained on the dataset, as well as the charact"
2020.inlg-1.10,W18-6456,0,0.0257489,"texts means that there may be superfluous information, as well as indirect descriptions of the data in the text. This is challenging for NLG systems, as shown by the system performance scores when performing an end-to-end data-to-text task on the dataset using TGen (Duˇsek and Jurˇc´ıcˇ ek, 2015). However, the dataset closely mirrors real-world scenarios in which companies oftentimes have large amounts of human-written texts that are not purposefully written for NLG applications, accompanied by corresponding data. texts to be of reasonable quality. Recent learningbased metrics, such as RUSE (Shimanaka et al., 2018), BertScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020) might be more viable options, since they claim to capture semantic similarity. However, we discourage using this dataset as a leaderboard chasing game and recommend using various types of evaluation methods to evaluate systems trained on the CACAPO dataset (e.g., evaluating the results on the dataset using human and automatic metrics, and qualitative and quantitative research methods). Variety in evaluation methods ensures that the results obtained on this dataset are put into a broad perspective"
2020.inlg-1.10,W19-3706,0,0.0237335,"l, and baseball, for YelpNLG, RotoWire, and MLB respectively), and one language (English). This makes it difficult to train domain-invariant systems. such as the one by Castro Ferreira et al. (2019). Furthermore, it is a sentence-level dataset containing unedited sentences from news articles written by professional journalists and meteorologists (see Section 3 for details). Finally, many of the datasets that are commonly used currently lack domain diversity (Radev et al., 2020) and are solely constructed for the English language (with the exception of WebNLG, see Castro Ferreira et al., 2018; Shimorina et al., 2019). The CACAPO dataset contains texts from the sports, weather, stocks, and incidents domain for both Dutch and English. 2.2 3 Control bottleneck Furthermore, most existing datasets are constructed for end-to-end architectures, where the non-lingustic input is converted into natural language without explicit intermediate representations in between (Castro Ferreira et al., 2019). By contrast, researchers have started to experiment with neural pipeline methods, in which the data conversion process happens via one or more explicit intermediate transformations (see, for instance, Castro Ferreira et"
2020.inlg-1.45,P02-1040,0,0.117646,"w that different kinds of errors elicit significantly different evaluation scores, even though all erroneous descriptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. Th"
2020.inlg-1.45,J18-3002,0,0.250965,"riptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of"
2020.inlg-1.45,2020.acl-main.704,0,0.0241616,"n as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of mistakes, and these mistakes are likely to be of different importance for a ‘correct’ interpretation of the relevant image. Consider Figure 1, which shows multiple human reference descriptions, and a description generated by Li et al.’s (2018) system (all in Chinese, with English glosses). This system makes three different mistakes, which are shown separatel"
2020.inlg-1.45,P17-1024,0,0.0222989,"hese studies show that while some errors may make users abandon a product, other errors may not be judged as harshly. In fact, Mirnig et al. found that people may even like a robot more if it occasionally makes a mistake. But, as Abdolrahmani et al. note: this all depends on the context of use. 399 Our study asks how we can systematically study the impact of different kinds of errors in automatic image descriptions. Several studies have proposed different categorizations of these errors. We will discuss those studies below. 2.2 Weaknesses in system competence Hodosh and Hockenmaier (2016) and Shekhar et al. (2017) both manipulate existing image descriptions to generate flawed descriptions, which they use to see if automatic image description systems can recognize those flaws. For example, given a sentence like (2), Hodosh and Hockenmaier swap the existing scene description for another one (2→2a), and ask systems to identify the correct description. Shekhar et al. change an entity with another entity falling under the same supercategory (e.g. VEHI CLE , 2→2b), and ask systems to identify the flaw in the description. (2) Ref: A man is riding a bicycle down the street. a. A man is riding a bicycle on the"
2021.naacl-main.51,L18-1361,0,0.0192145,"or confirmatory, it is easier to understand the status of your results. Compared to the work on reporting quality, there has been little talk of preregistration in the NLP literature; the terms ‘preregister’ or ‘preregistration’ are hardly used in the ACL Anthology.2 For this reason, we will focus on preregistration and its application in NLP research. The next sections discuss how preregistration works (§2), propose preregistration questions for NLP research (§3), discuss the idea of ‘registered reports’ as an alter2 Looking for these terms, we found four papers that mention preregistration: Cao et al. (2018) and van der Lee et al. (2019) mention it, and van Miltenburg et al. (2018) and Futrell and Levy (2019) share their own preregistration. native pathway to publication (§4) and the overall feasibility of preregistrations in NLP (§5). 2 How does preregistration work? Before you begin, you enter the hypotheses, design, and analysis plan of your study on a website like the Open Science Framework, AsPredicted, or ResearchBox. These sites provide a time stamp; evidence that you indeed made all the relevant decisions before carrying out the study. During your study, you follow the preregistered plans"
2021.naacl-main.51,2020.acl-tutorials.4,0,0.0411718,"the infrastructure is in place. Munafò et al. also recommend to diversify peer review. Instead of only having journals, that are responsible for both the evaluation and dissemination of research, we can now also solicit peer feedback after publishing our work on a platform like ArXiv or OpenReview. The NLP community is clearly ahead of the curve in terms of the adoption of preprints, and actively discussing ways to improve peer review (ACL Reviewing Committee 2020a,b; Rogers and Augenstein 2020). To improve the quality of the reviews themselves, ACL2020 featured a tutorial on peer reviewing (Cohen et al., 2020). Another advice from Munafò et al. is to adopt reporting guidelines, so that papers include all relevant details for others to reproduce the results. The NLP community is rapidly adopting such guidelines, in the form of Dodge et al.’s (2019) reproducibility checklist that authors for EMNLP2020 need to fill in. Beyond reproducibility, we are also seeing more and more researchers adopting Data statements (Bender and Friedman, 2018), Model cards (Mitchell et al., 2019), and Datasheets (Gebru et al., 2018) for ethical reasons. Munafò et al.’s final recommendation, preregistration, means that auth"
2021.naacl-main.51,D19-1224,0,0.0608082,"Missing"
2021.naacl-main.51,P18-1128,0,0.0292196,"This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research. 1 Introduction applied correctly. In NLP, we see different researchers picking up the gauntlet to teach others about statistics (Dror et al., 2018, 2020), achieving language-independence (Bender, 2011), or best practices in human evaluation (van der Lee et al., 2019, 2021). Moreover, every *ACL conference offers tutorials on a wide range of different topics. While efforts to improve methodology could be more systematic (e.g. by actively encouraging methodology tutorials, and working towards community standards),1 the infrastructure is in place. Munafò et al. also recommend to diversify peer review. Instead of only having journals, that are responsible for both the evaluation and dissemination of research, we can now also solicit peer fe"
2021.naacl-main.51,P13-1166,0,0.0311594,"Missing"
2021.naacl-main.51,2020.inlg-1.23,1,0.754724,"Missing"
2021.naacl-main.51,C18-1328,0,0.0125658,"ry-picked examples showing good performance. The main benefit of asking these questions beforehand is that they force researchers to carefully consider their methodology, and they make researchers’ expectations explicit. This also helps to identify unexpected findings, or changes that 5 https://coling2018.org/paper-types/ Taking the best papers from COLING 2018 as an example, Ruppenhofer et al. (2018, analysis) test assumptions from the linguistics literature about affixoids, Thompson and Mimno (2018, experiment) test which subsampling methods improve the output generated by topic models, and Lan and Xu (2018, reproduction) test whether the reported performance for different neural network models generalises to other tasks. 615 6 were made to the research design during the study. Resource papers are on the qualitative side of the spectrum, and as such the questions from Haven and Grootel (2019), presented at the bottom of Table 1, are generally appropriate for these kinds of papers as well. Particularly 1) the original purpose for collecting the data, 2) sampling decisions (what documents to include), and 3) annotation (what framework/perspective to use) are important. Because the former typically"
2021.naacl-main.51,C18-1097,0,0.0238321,"dies (Nosek et al., Scientific results are only as reliable as the methods that we use to obtain those results. Recent years have seen growing concerns about the reproducibility of scientific research, leading some to speak of a ‘reproducibility crisis’ (see Fidler and Wilcox 2018 for an overview of the debate). Although the main focus of the debate has been on psychology (e.g. through Open Science Collaboration 2015) and medicine (Macleod et al., 2014), there are worries about the reproducibility of Natural Language Processing (NLP) research as well (Fokkens et al., 2013; Cohen et al., 2018; Moore and Rayson, 2018; Branco et al., 2020). The reproducibility debate has led to Munafò et al.’s (2017) Manifesto for reproducible science, where the authors discuss the different threats to reproducible science, and different ways to address these threats. We will first highlight some of their proposals, and discuss their adoption rate in NLP. Our main observation is that preregistration is rarely used. We believe this is an undesirable situation, and devote the rest of this paper to argue for preregistration of NLP research. 1 A more radical proposal would be to always host Munafò et al. recommend more methodo"
2021.naacl-main.51,J18-3002,0,0.0201352,"another goal. Survey papers should follow the PRISMA guidelines for structured reviews (Moher et al., 2009; Liberati et al., 2009). According to these guidelines, researchers should state exactly where they searched for existing literature, what search terms they used, and what criteria they used to select relevant papers. This increases reproducibility, allows readers to find any gaps in the survey, and avoids a biased presentation of the literature (i.e. only citing researchers you know, or work that fits your preferred narrative). A recent NLP example of a structured review is provided by Reiter (2018). 4 Registered reports that offer registered reports, but strongly encourage the NLP community to take steps in this direction.7 5 Feasibility Gelman and Loken (2013, 2014) touch upon the feasibility of preregistration, noting that: “[f]or most of our own research projects this strategy hardly seems possible: in our many applied research projects, we have learned so much by looking at the data. Our most important hypotheses could never have been formulated ahead of time.” This certainly rings true for NLP as well. However, we should be careful about conclusions that are drawn on the basis of p"
2021.naacl-main.51,C18-1329,0,0.0130027,"be found. The questions also stimulate researchers to go beyond the practice of providing some ‘lemons’ alongside cherry-picked examples showing good performance. The main benefit of asking these questions beforehand is that they force researchers to carefully consider their methodology, and they make researchers’ expectations explicit. This also helps to identify unexpected findings, or changes that 5 https://coling2018.org/paper-types/ Taking the best papers from COLING 2018 as an example, Ruppenhofer et al. (2018, analysis) test assumptions from the linguistics literature about affixoids, Thompson and Mimno (2018, experiment) test which subsampling methods improve the output generated by topic models, and Lan and Xu (2018, reproduction) test whether the reported performance for different neural network models generalises to other tasks. 615 6 were made to the research design during the study. Resource papers are on the qualitative side of the spectrum, and as such the questions from Haven and Grootel (2019), presented at the bottom of Table 1, are generally appropriate for these kinds of papers as well. Particularly 1) the original purpose for collecting the data, 2) sampling decisions (what documents"
C10-1085,J05-3002,0,0.0317339,"that of another, the meaning of one sentence can overlap only partly with that of another, etc. This requires an analysis of the semantic similarity between a pair of expressions. Like detection, automatic analysis of semantic similarity can play an important role in NLP applications. To return to the case of multi-document summarization, analysing the semantic similarity between sentences extracted from different documents provides the basis for sentence fusion, a process where a new sentence is generated that conveys all common information from both sentences without introducing redundancy (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005b). 752 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752–760, Beijing, August 2010 Analysis of semantic similarity can be approached from different angles. A basic approach is to use string similarity measures such as the Levenshtein distance or the Jaccard similarity coefficient. Although cheap and fast, this fails to account for less obvious cases such as synonyms or syntactic paraphrasing. At the other extreme, we can perform a deep semantic analysis of two expressions and rely on formal reasoning to derive a log"
C10-1085,C04-1154,0,0.0262968,"labeled syntactic trees. For expository reasons the alignment is not exhaustive. over restates, etc. Furthermore, equals, restates and intersects are symmetrical, whereas generalizes is the inverse of specifies. Finally, nodes containing unique information, such as Alzheimer and Parkinson, remain unaligned. 3 Related work Many syntax-based approaches to machine translation rely on bilingual treebanks to extract transfer rules or train statistical translation models. In order to build bilingual treebanks a number of methods for automatic tree alignment have been developed, e.g., (Gildea, 2003; Groves et al., 2004; Tinsley et al., 2007; Lavie et al., 2008). Most related to our approach is the work on discriminative tree alignment by Tiedemann & Kotz´e (2009). However, these algorithms assume that source and target sentences express the same information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned. See (MacCartney et al., 2008) for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text. MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supe"
C10-1085,C08-1066,0,0.0161322,"me information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned. See (MacCartney et al., 2008) for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text. MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supervised learning which also exploits external resources for knowledge of semantic relatedness. In contrast to our work, they do not use syntactic trees or similarity relation labels. Partly similar semantic relations are used in (MacCartney and Manning, 2008) for modeling semantic containment and exclusion in natural language inference. Marsi & Krahmer (2005a) is closely related to our work, but follows a more complicated method: first a dynamic programmingbased tree alignment algorithm is applied, followed by a classification of similarity relations using a supervised-classifier. Other differences are that their data set is much smaller and consists of parallel rather than comparable text. A major drawback of this algorithmic approach it that it cannot cope with crossing alignments. We are not aware of other work that combines alignment with sema"
C10-1085,D08-1084,0,0.0267609,"ine translation rely on bilingual treebanks to extract transfer rules or train statistical translation models. In order to build bilingual treebanks a number of methods for automatic tree alignment have been developed, e.g., (Gildea, 2003; Groves et al., 2004; Tinsley et al., 2007; Lavie et al., 2008). Most related to our approach is the work on discriminative tree alignment by Tiedemann & Kotz´e (2009). However, these algorithms assume that source and target sentences express the same information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned. See (MacCartney et al., 2008) for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text. MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supervised learning which also exploits external resources for knowledge of semantic relatedness. In contrast to our work, they do not use syntactic trees or similarity relation labels. Partly similar semantic relations are used in (MacCartney and Manning, 2008) for modeling semantic containment and exclusion in natural language inference. Marsi & Krahmer (2005a) is"
C10-1085,W05-1201,1,0.933455,"of one sentence can overlap only partly with that of another, etc. This requires an analysis of the semantic similarity between a pair of expressions. Like detection, automatic analysis of semantic similarity can play an important role in NLP applications. To return to the case of multi-document summarization, analysing the semantic similarity between sentences extracted from different documents provides the basis for sentence fusion, a process where a new sentence is generated that conveys all common information from both sentences without introducing redundancy (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005b). 752 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752–760, Beijing, August 2010 Analysis of semantic similarity can be approached from different angles. A basic approach is to use string similarity measures such as the Levenshtein distance or the Jaccard similarity coefficient. Although cheap and fast, this fails to account for less obvious cases such as synonyms or syntactic paraphrasing. At the other extreme, we can perform a deep semantic analysis of two expressions and rely on formal reasoning to derive a logical relation between the"
C10-1085,W05-1612,1,0.942749,"of one sentence can overlap only partly with that of another, etc. This requires an analysis of the semantic similarity between a pair of expressions. Like detection, automatic analysis of semantic similarity can play an important role in NLP applications. To return to the case of multi-document summarization, analysing the semantic similarity between sentences extracted from different documents provides the basis for sentence fusion, a process where a new sentence is generated that conveys all common information from both sentences without introducing redundancy (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005b). 752 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752–760, Beijing, August 2010 Analysis of semantic similarity can be approached from different angles. A basic approach is to use string similarity measures such as the Levenshtein distance or the Jaccard similarity coefficient. Although cheap and fast, this fails to account for less obvious cases such as synonyms or syntactic paraphrasing. At the other extreme, we can perform a deep semantic analysis of two expressions and rely on formal reasoning to derive a logical relation between the"
C10-1085,J98-3005,0,0.0561914,"Missing"
C10-1085,2007.mtsummit-papers.62,0,0.043763,"es. For expository reasons the alignment is not exhaustive. over restates, etc. Furthermore, equals, restates and intersects are symmetrical, whereas generalizes is the inverse of specifies. Finally, nodes containing unique information, such as Alzheimer and Parkinson, remain unaligned. 3 Related work Many syntax-based approaches to machine translation rely on bilingual treebanks to extract transfer rules or train statistical translation models. In order to build bilingual treebanks a number of methods for automatic tree alignment have been developed, e.g., (Gildea, 2003; Groves et al., 2004; Tinsley et al., 2007; Lavie et al., 2008). Most related to our approach is the work on discriminative tree alignment by Tiedemann & Kotz´e (2009). However, these algorithms assume that source and target sentences express the same information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned. See (MacCartney et al., 2008) for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text. MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supervised learning which"
C10-1085,vossen-etal-2008-integrating,0,0.0273872,"Missing"
C10-1085,P08-2049,1,0.889766,"Missing"
C10-1085,W08-0411,0,0.0229444,"sons the alignment is not exhaustive. over restates, etc. Furthermore, equals, restates and intersects are symmetrical, whereas generalizes is the inverse of specifies. Finally, nodes containing unique information, such as Alzheimer and Parkinson, remain unaligned. 3 Related work Many syntax-based approaches to machine translation rely on bilingual treebanks to extract transfer rules or train statistical translation models. In order to build bilingual treebanks a number of methods for automatic tree alignment have been developed, e.g., (Gildea, 2003; Groves et al., 2004; Tinsley et al., 2007; Lavie et al., 2008). Most related to our approach is the work on discriminative tree alignment by Tiedemann & Kotz´e (2009). However, these algorithms assume that source and target sentences express the same information (i.e. parallel text) and cannot cope with comparable text where parts may remain unaligned. See (MacCartney et al., 2008) for further arguments and empirical evidence that MT alignment algorithms are not suitable for aligning parallel monolingual text. MacCartney, Galley, and Manning (2008) describe a system for monolingual phrase alignment based on supervised learning which also exploits externa"
C10-1085,P03-1011,0,\N,Missing
C10-1085,W07-1401,0,\N,Missing
C18-1082,W05-0909,0,0.0102598,"n and computergenerated texts were also investigated, because these preconceptions could moderate the effectiveness of tailoring. 2 2.1 Background Evaluation in NLG Evaluation has become an increasingly important topic within the NLP domain as a whole, but also within the NLG domain more specifically. While the NLG domain has a strong evaluation tradition (Gatt and Belz, 2010), there is an ongoing discussion regarding the type of evaluation that should be used. One popular evaluation method is the use of metrics that can be computed automatically, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and NIST (Doddington, 2002). However, previous literature suggests that the use of such metrics should be done with caution. These metrics are attractive because they are quick, fast and repeatable (Reiter and Belz, 2009), and have in some cases been found to correlate with human judgments (Gkatzia and Mahamood, 2015). However, most if not all of the current automated metrics that are used in the NLG domain are based on overlap with a certain reference text. Therefore, it can be derived that such metrics are only useful if an aligned NLG output-reference text corpus can be constructed and if"
C18-1082,E06-1040,0,0.250714,"ated if tailoring affects perceived text quality, for which no results were garnered. This lack of results might be due to negative preconceptions about computer-generated texts which were found in the first study. 1 Introduction Evaluation of end-to-end Natural Language Generation (NLG) systems is important to assess whether the system has properly expressed certain properties (e.g. quality, speed), or whether the designed properties work as intended (Gkatzia and Mahamood, 2015). Traditional NLG evaluation approaches can typically be assigned to one of two categories: intrinsic or extrinsic (Belz and Reiter, 2006). Intrinsic approaches seek to evaluate properties of the system itself. This can be done using automatic measures such as BLEU, NIST, ROUGE, etc. or by asking human participants to rate the systems output with e.g. Likert or rating scales. Extrinsic approaches aim to assess the impact of the system, by measuring if the system can fulfill its purpose or what the user gains from the systems output. While scholars have been positive about the effort the NLG community has put into their evaluations (Gatt and Belz, 2010, for instance), it is often the case that an extensive evaluation does not tak"
C18-1082,W16-6612,1,0.823617,"tional aspects of the intended reader (Mahamood and Reiter, 2011; Ghosh et al., 2017, for instance). The input data is scraped from Goal.com and contains various types of data related to a soccer match (e.g. date played, goals scored, information about the players). Based on this data, a short Dutch match summary of about 50 to 150 words is produced, which is inspired by the reports of the GoalGetter system (Theune et al., 2001). However, although inspired by previous work, texts by PASS are novel in the sense that the templates have been directly derived from sentences in the MeMo FC corpus (Braun et al., 2016). This corpus contains match reports directly taken from the clubs that participated in the match. This often means that the tone of voice in these reports is emotional, while still maintaining a relatively professional style. Using these reports makes it possible to produce tailored match reports with PASS. This means that the tone of a generated report should appear to be more disappointed or frustrated in case the team of the target audience lost, and more upbeat in case of a win for the team of the target audience (van der Lee et al., 2017, for examples). Empirical evaluation of ANLG syste"
C18-1082,P17-1059,0,0.0134109,"ations of texts written by computers compared to human-written texts (Graefe et al., 2016). 2.2 PASS and ‘Affective’ Natural Language Generation While an increase in NLG systems with a tailoring component might be expected, one of the few systems in this category is PASS (van der Lee et al., 2017). PASS generates soccer match summaries aimed at fans of one of the teams that participated in the match. Thus, the system can be seen as part of the ‘Affective’ NLG (ANLG) tradition, which aims to produce texts tailored towards the emotional aspects of the intended reader (Mahamood and Reiter, 2011; Ghosh et al., 2017, for instance). The input data is scraped from Goal.com and contains various types of data related to a soccer match (e.g. date played, goals scored, information about the players). Based on this data, a short Dutch match summary of about 50 to 150 words is produced, which is inspired by the reports of the GoalGetter system (Theune et al., 2001). However, although inspired by previous work, texts by PASS are novel in the sense that the templates have been directly derived from sentences in the MeMo FC corpus (Braun et al., 2016). This corpus contains match reports directly taken from the club"
C18-1082,W15-4708,0,0.733658,"cases, and that participants struggled with correctly identifying whether a text was written by a human or computer. The second study investigated if tailoring affects perceived text quality, for which no results were garnered. This lack of results might be due to negative preconceptions about computer-generated texts which were found in the first study. 1 Introduction Evaluation of end-to-end Natural Language Generation (NLG) systems is important to assess whether the system has properly expressed certain properties (e.g. quality, speed), or whether the designed properties work as intended (Gkatzia and Mahamood, 2015). Traditional NLG evaluation approaches can typically be assigned to one of two categories: intrinsic or extrinsic (Belz and Reiter, 2006). Intrinsic approaches seek to evaluate properties of the system itself. This can be done using automatic measures such as BLEU, NIST, ROUGE, etc. or by asking human participants to rate the systems output with e.g. Likert or rating scales. Extrinsic approaches aim to assess the impact of the system, by measuring if the system can fulfill its purpose or what the user gains from the systems output. While scholars have been positive about the effort the NLG co"
C18-1082,hastie-belz-2014-comparative,0,0.130779,"rovide a good assessment of many linguistic properties such as content selection, information structure, appropriateness, etcetera (Scott and Moore, 2007). While use of automatic metrics is increasing (Gkatzia and Mahamood, 2015), the above concerns might contribute to the fact that evaluation using human input is still the most popular evaluation method in the NLG domain. Most of these evaluations have been quantitative (Sambaraju et al., 2011), but there can be sizable differences between these quantitative evaluations. A main distinction can be made between intrinsic and extrinsic methods (Hastie and Belz, 2014). Intrinsic methods evaluate the output of the system itself, for example by having humans read and rate text output of the NLG system and comparing these ratings against human written texts on metrics such as fluency, correctness, understandability, etcetera. Extrinsic measures aim to evaluate the impact of the system, for instance by measuring whether a system can fulfill the purpose it was built for (Hastie and Belz, 2014). A corpus analysis by Gkatzia and Mahamood (2015) shows that intrinsic human-based measures are used to a much greater degree compared to extrinsic human-based evaluation"
C18-1082,W11-2803,0,0.435462,"they have different expectations of texts written by computers compared to human-written texts (Graefe et al., 2016). 2.2 PASS and ‘Affective’ Natural Language Generation While an increase in NLG systems with a tailoring component might be expected, one of the few systems in this category is PASS (van der Lee et al., 2017). PASS generates soccer match summaries aimed at fans of one of the teams that participated in the match. Thus, the system can be seen as part of the ‘Affective’ NLG (ANLG) tradition, which aims to produce texts tailored towards the emotional aspects of the intended reader (Mahamood and Reiter, 2011; Ghosh et al., 2017, for instance). The input data is scraped from Goal.com and contains various types of data related to a soccer match (e.g. date played, goals scored, information about the players). Based on this data, a short Dutch match summary of about 50 to 150 words is produced, which is inspired by the reports of the GoalGetter system (Theune et al., 2001). However, although inspired by previous work, texts by PASS are novel in the sense that the templates have been directly derived from sentences in the MeMo FC corpus (Braun et al., 2016). This corpus contains match reports directly"
C18-1082,D17-1238,0,0.104685,"Missing"
C18-1082,P02-1040,0,0.104482,"reconceptions about human-written and computergenerated texts were also investigated, because these preconceptions could moderate the effectiveness of tailoring. 2 2.1 Background Evaluation in NLG Evaluation has become an increasingly important topic within the NLP domain as a whole, but also within the NLG domain more specifically. While the NLG domain has a strong evaluation tradition (Gatt and Belz, 2010), there is an ongoing discussion regarding the type of evaluation that should be used. One popular evaluation method is the use of metrics that can be computed automatically, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and NIST (Doddington, 2002). However, previous literature suggests that the use of such metrics should be done with caution. These metrics are attractive because they are quick, fast and repeatable (Reiter and Belz, 2009), and have in some cases been found to correlate with human judgments (Gkatzia and Mahamood, 2015). However, most if not all of the current automated metrics that are used in the NLG domain are based on overlap with a certain reference text. Therefore, it can be derived that such metrics are only useful if an aligned NLG output-reference te"
C18-1082,J09-4008,0,0.45069,"rate the systems output with e.g. Likert or rating scales. Extrinsic approaches aim to assess the impact of the system, by measuring if the system can fulfill its purpose or what the user gains from the systems output. While scholars have been positive about the effort the NLG community has put into their evaluations (Gatt and Belz, 2010, for instance), it is often the case that an extensive evaluation does not take much priority after a system is built. The usage of automatic measures is gaining traction due to its quickness and low costs, but they are still considered controversial by many (Reiter and Belz, 2009; Novikova et al., 2017, for instance). Furthermore, the intrinsic approaches with human ratings are often limited in scope, using a relatively small sample of participants, using relatively short questionnaires that only shine light on a small aspect of text quality, and/or comparing the computer-generated texts against nonrepresentative human texts. Similarly, the amount of extrinsic evaluations that have been performed up until now is low. While they are considered the most useful type of evaluation by some (Reiter and Belz, 2009), they are not carried out as often as other types of evaluat"
C18-1082,W02-2113,0,0.127505,"(Gkatzia and Mahamood, 2015). However, most if not all of the current automated metrics that are used in the NLG domain are based on overlap with a certain reference text. Therefore, it can be derived that such metrics are only useful if an aligned NLG output-reference text corpus can be constructed and if it can be assumed that qualitatively good output has a strong overlap with this standard. This will not be the case in many situations. Furthermore, these metrics also rely on the assumption that the reference text is a good representation of the ‘best case scenario, which it often is not (Reiter and Sripada, 2002). Finally, it has been argued that such metrics do not provide a good assessment of many linguistic properties such as content selection, information structure, appropriateness, etcetera (Scott and Moore, 2007). While use of automatic metrics is increasing (Gkatzia and Mahamood, 2015), the above concerns might contribute to the fact that evaluation using human input is still the most popular evaluation method in the NLG domain. Most of these evaluations have been quantitative (Sambaraju et al., 2011), but there can be sizable differences between these quantitative evaluations. A main distincti"
C18-1082,W11-2804,0,0.320603,"e reference text is a good representation of the ‘best case scenario, which it often is not (Reiter and Sripada, 2002). Finally, it has been argued that such metrics do not provide a good assessment of many linguistic properties such as content selection, information structure, appropriateness, etcetera (Scott and Moore, 2007). While use of automatic metrics is increasing (Gkatzia and Mahamood, 2015), the above concerns might contribute to the fact that evaluation using human input is still the most popular evaluation method in the NLG domain. Most of these evaluations have been quantitative (Sambaraju et al., 2011), but there can be sizable differences between these quantitative evaluations. A main distinction can be made between intrinsic and extrinsic methods (Hastie and Belz, 2014). Intrinsic methods evaluate the output of the system itself, for example by having humans read and rate text output of the NLG system and comparing these ratings against human written texts on metrics such as fluency, correctness, understandability, etcetera. Extrinsic measures aim to evaluate the impact of the system, for instance by measuring whether a system can fulfill the purpose it was built for (Hastie and Belz, 201"
C18-1082,W17-3513,1,0.533337,"Missing"
C18-1188,E09-1046,0,0.021524,"e extracted from each review text by means of Frog5 , an off-the-shelf NLP system for Dutch (Van den Bosch et al., 2007). We matched the automatic analyses with a set of syntactic patterns of which the lexical realizations (the words associated with the PoS-tags) may reflect statements on the valence of an aspect. Examples of patterns and example lexical realizations are given in Table 1. To assess the presence and direction of the valence in the lexical realizations (henceforth referred to as phrases) found with matching syntactic patterns, we matched them to the Duoman subjectivity lexicon (Jijkoun and Hofmann, 2009). This a lexicon of nouns and adjectives rated by human annotators as ‘Very negative’, ‘Negative’, ‘Neutral’, ‘Positive’ and ‘Very positive’. We labeled each phrase with a positive or very positive word as ‘pro’, and each phrase with a negative or very negative word as ‘con’. Phrases in which no word has a positive or negative valence in Duoman lexicon were discarded, as well as phrases that matched an equal amount of positive and negative words. A phrase that carries a combination of a ‘positive’ and ‘very negative’ is labeled as ‘con’, while phrases with a ‘very positive’ and ‘negative’ word"
C18-1188,P06-2063,0,0.0613142,"Missing"
C18-1188,W14-5905,0,0.0335956,"standard is compared to system output through human evaluation. After discussing related work, this paper will proceed with a description of the methods. Subsequently, the experimental set-up will be outlined, followed by the results. The paper ends with a conclusion and discussion. 2 Related Work Studies in aspect-based sentiment analysis can roughly be divided into rule-based and machine learningbased approaches. Rule-based approaches implement searching for combinations of aspects and evaluations on the sentence level. Such combinations are commonly identified based on syntactic patterns (Poria et al., 2014; Zhang et al., 2014; Chinsha and Joseph, 2015; Rana and Cheah, 2015; Yan et al., 2015) while the evaluation is scored based on a sentiment lexicon (Chinsha and Joseph, 2015; Rana and Cheah, 2015; Poria et al., 2014), a PageRank-based procedure (Yan et al., 2015), or optimization based on review labels (Zhang et al., 2014). We apply a knowledge-driven approach to aspect-based sentiment analysis, comparable to the approach by Rana and Cheah (2015). Candidate phrases are identified based on predefined syntactic patterns and a sentiment lexicon is used to identify patterns that likely signify a p"
C18-1188,D13-1170,0,0.0102213,"Missing"
C18-1188,L16-1652,0,0.0194187,"e, the labels are the human generated aspect sentences. We treat each aspect as a potential class to predict. The chosen architecture is similar to Joulin et al. (2016) which is a modification of the cbow model by Mikolov et al. (2013), but instead of predicting a hidden word, a hidden class is predicted. We use softmax to compute the probability distribution over the aspect classes. The model is trained using stochastic gradient descent and a linearly decaying learning rate. To pretrain the model we use 320-dimensional word embeddings derived from the corpora from the web (COW) introduced in Tulkens et al. (2016). 3.3 System 3: Shallow neural classification with clustering Because users can state positive and negative aspects in any way they want, we end up with a large number of classes in a long-tail distribution. From inspection of the development data we observe that many of the different classes are in fact paraphrases of the same semantic content (’great coffee’, ’coffee tastes great’, etc.) To try to alleviate this problem, we add a preprocessing step where we cluster the aspect summaries based on averaging over word embedding vectors. Using k-means clustering we then cluster the summaries into"
C18-1310,W16-6615,0,0.104643,"Missing"
C18-1310,Q17-1010,0,0.0148489,"Missing"
C18-1310,W16-3210,0,0.0661468,"Missing"
C18-1310,W97-0802,0,0.0553999,"his approach, we find an even lower correlation of 0.20. We conclude that image content only has a limited influence on description diversity. 5.4 Predicting image specificity from eye-tracking data: a negative result We now turn to look at whether the image specificity scores are correlated with our eye-tracking data. As noted above, this experiment builds on Coco and Keller’s (2012) work, which shows that scan patterns can be used to predict what people will say about an image. Rather than taking Coco and Keller’s more 9 Even though wordnets exist for Dutch (Postma et al., 2016) and German (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010), we did not use them because they have lower coverage, and we do not need to worry about lemmatization. 10 Our preregistration can be found at: https://osf.io/6pc2t/register/565fb3678c5e4a66b5582f67 We deviated from our preregistration in two areas: (1) the experiment took more time than expected, so we split the experiment in two parts; (2) we had to discard more data than expected, so we collected data from more participants to compensate. 3666 advanced approach (involving image segmentation), we use a more naive strategy: (1) We first compute the average pairwi"
C18-1310,henrich-hinrichs-2010-gernedit,0,0.0345662,"even lower correlation of 0.20. We conclude that image content only has a limited influence on description diversity. 5.4 Predicting image specificity from eye-tracking data: a negative result We now turn to look at whether the image specificity scores are correlated with our eye-tracking data. As noted above, this experiment builds on Coco and Keller’s (2012) work, which shows that scan patterns can be used to predict what people will say about an image. Rather than taking Coco and Keller’s more 9 Even though wordnets exist for Dutch (Postma et al., 2016) and German (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010), we did not use them because they have lower coverage, and we do not need to worry about lemmatization. 10 Our preregistration can be found at: https://osf.io/6pc2t/register/565fb3678c5e4a66b5582f67 We deviated from our preregistration in two areas: (1) the experiment took more time than expected, so we split the experiment in two parts; (2) we had to discard more data than expected, so we collected data from more participants to compensate. 3666 advanced approach (involving image segmentation), we use a more naive strategy: (1) We first compute the average pairwise similarity scores between"
C18-1310,2016.gwc-1.43,1,0.894287,"Missing"
C18-1310,W16-3207,1,0.714733,"Missing"
C18-1310,W17-3503,1,0.831061,"Missing"
C18-1310,W18-3910,1,0.813513,"Missing"
C18-1310,Q14-1006,0,0.573933,"ijpen hoe mensen afbeeldingen beschrijven, en in het bijzonder wat de rol is van visuele aandacht op het beschrijvingsproces. 1 Introduction Automatic image description is a task at the intersection of Computer Vision (CV) and Natural Language Processing (NLP). The goal is for machines to automatically produce natural language descriptions for any image (Bernardi et al., 2016). The field of automatic image description saw an explosive growth in 2014 with the release of the Flickr30K and MS COCO datasets: two corpora of images collected from Flickr, with 5 crowd-sourced descriptions per image (Young et al., 2014; Lin et al., 2014). These resources enabled researchers to train end-to-end systems that automatically learn a mapping between images and text (Vinyals et al., 2015), but also to better understand how humans describe images (e.g., This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3658 Proceedings of the 27th International Conference on Computational Linguistics, pages 3658–3669 Santa Fe, New Mexico, USA, August 20-26, 2018. Raw Translation Een hele kudde schapen &lt;uh&gt; met een man &lt;corr&gt; met een he"
D19-1052,W18-6531,0,0.0293757,"ation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.1 1 Introduction Data-to-text Natural Language Generation (NLG) is the computational process of generating meaningful and coherent natural language text to describe non-linguistic input data (Gatt and Krahmer, 2018). Practical applications can be found in domains such as weather forecasts (Mei et al., 2016), health care (Portet et al., 2009), feedback for car drivers (Braun et al., 2018), diet management (Anselma and Mazzei, 2018), election results (Lepp¨anen et al., 2017) and sportscasting news (van der Lee et al., 2017). 1 https://github.com/ThiagoCF05/ DeepNLG/ 552 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 552–562, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and more reusable, output results. In fact, this has never been systematically studied, and this is the main goal of the current paper. We present a systematic comparison between neural pipeline"
D19-1052,W11-2832,0,0.0627841,"ing the order in which the communicative goals should be verbalized in the target text. In our case, the communicative goals are the RDF triples received as input by the model. Given a set of linearized triples, this step determines the order in which they should be verbalized. For example, given the triple set in Figure 1 in the linearized format: Data The experiments presented in this work were conducted on the WebNLG corpus (Gardent et al., 2017a,b), which consists of sets of h Subject, Predicate, Object i RDF triples and their target texts. In comparison with other popular NLG benchmarks (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018), WebNLG is the most semantically varied corpus, consisting of 25,298 texts describing 9,674 sets of up to 7 RDF triples in 15 domains. Out of these domains, 5 are exclusively present in the test set, being unseen during the training and validation processes. Figure 1 depicts an example of a set of 3 RDF triples and its related <TRIPLE> A.C. Cesena manager Massimo Drago </TRIPLE> <TRIPLE> Massimo Drago club S.S.D. Potenza Calcio </TRIPLE> <TRIPLE> Massimo Drago club Calcio Catania </TRIPLE> Our discourse ordering model would ideally return the set cl"
D19-1052,W18-6505,0,0.309902,"o complex in general (see Gatt and Krahmer 2018 for a discussion of different architectures). The emergence of neural methods changed this: provided there is enough training data, it does become possible to learn a direct mapping from input to output, as has also been shown in, for example, neural machine translation. As a result, in NLG more recently, neural end-to-end data-totext models have been proposed, which directly learn input-output mappings and rely much less on explicit intermediate representations (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Lebret et al., 2016; Gehrmann et al., 2018). However, the fact that neural end-to-end approaches are possible does not necessarily entail that they are better than (neural) pipeline models. On the one hand, cascading of errors is a known problem of pipeline models in general (an error in an early module will impact all later modules in the pipeline), which (almost by definition) does not apply to end-to-end models. On the other hand, it is also conceivable that developing dedicated neural modules for specific tasks leads to better performance on each of these successive tasks, and combining them might lead to better, Traditionally, mos"
D19-1052,P18-1182,1,0.889919,"Missing"
D19-1052,J12-1006,1,0.890177,"Missing"
D19-1052,W18-6521,1,0.167192,"output results. In fact, this has never been systematically studied, and this is the main goal of the current paper. We present a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of output text from RDF input triples, relying on an augmented version of the WebNLG corpus (Gardent et al., 2017b). Using two state-of-the-art deep learning techniques, GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017), we develop both a neural pipeline and an end-to-end architecture. The former, which also makes use of the NeuralREG approach (Castro Ferreira et al., 2018a), tackles standard NLG tasks (discourse ordering, text structuring, lexicalization, referring expression generation and textual realization) in sequence, while the latter does not address these individual tasks, but directly tries to learn how to map RDF triples into corresponding output text. Using a range of evaluation techniques, including both automatic and human measures, combined with a qualitative analysis, we provide answers to our two main research questions: (RQ1) How well do deep learning methods perform as individual modules in a data-to-text pipeline architecture? And (RQ2) How"
D19-1052,W07-0734,0,0.0235076,"the output of the previous step is fed into the next one. We call these implementations Random, Majority, GRU and Transformer, where each one has its steps solved by one of the proposed baselines or deep learning implementations. In Random and Majority, the referring expressions were generated by the OnlyNames baseline, whereas for GRU and Transformer, NeuralREG was used for the seen entities, OnlyNames for the unseen ones and special rules to realize dates and numbers. 7.3 Automatic Evaluation We evaluated the textual outputs of each system using the BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) metrics. The evaluation was done on the entire test data, as well as only in their seen and unseen domains. Human Evaluation We conducted a human evaluation, selecting the same 223 samples used in the evaluation of the WebNLG challenge (Gardent et al., 2017b). For each sample, we used the original texts and the ones generated by our 6 approaches and by the Melbourne and UPF-FORGe reference systems, totaling 2,007 trials. Each trial displayed the triple set and the respective text. The goal of the participants was to rate the trials based on the fluency (i.e., does the text flow in a natural,"
D19-1052,W14-4012,0,0.164738,"Missing"
D19-1052,D16-1128,0,0.117162,"Missing"
D19-1052,P16-2008,0,0.0995305,"Missing"
D19-1052,W17-3513,1,0.894325,"Missing"
D19-1052,W19-2308,0,0.0660415,"Missing"
D19-1052,W17-3528,0,0.0827802,"Missing"
D19-1052,P17-1017,0,0.501998,"s of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 552–562, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and more reusable, output results. In fact, this has never been systematically studied, and this is the main goal of the current paper. We present a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of output text from RDF input triples, relying on an augmented version of the WebNLG corpus (Gardent et al., 2017b). Using two state-of-the-art deep learning techniques, GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017), we develop both a neural pipeline and an end-to-end architecture. The former, which also makes use of the NeuralREG approach (Castro Ferreira et al., 2018a), tackles standard NLG tasks (discourse ordering, text structuring, lexicalization, referring expression generation and textual realization) in sequence, while the latter does not address these individual tasks, but directly tries to learn how to map RDF triples into corresponding output text. Using a range of evaluation t"
D19-1052,W18-6501,0,0.165623,"proaches managed to generate a semantic text based on the two seen predicates, whereas the end-to-end approaches hallucinated texts which have no semantic relation with the non-linguistic input. Discussion This study introduced a systematic comparison between pipeline and end-to-end architectures for data-to-text generation, exploring the role of deep neural networks in the process. In this section we answer the two introduced research questions and additional topics based on our findings. Related Work We compared the proposed approaches with 4 state-of-the-art RDF-to-text systems. Except for Marcheggiani and Perez (2018), all the others are not end-to-end approaches, already directing the field to pipeline architectures. UPF-FORGe is a proper pipeline system with sevHow well do deep learning methods perform as individual modules in a data-to-text pipeline? In comparison with Random and Majority baselines, we observed that our deep learning implementations registered a higher performance in the 559 Ace Ace Ace Ace Wilder Wilder Wilder Wilder background birthPlace birthYear occupation “solo singer” Sweden 1982 Songwriter ↓ GRU Ace Wilder, born in Sweden, performs as Songwriter. Transformer Ace Wilder (born in S"
D19-1052,N16-1086,0,0.334707,"put to output using rules was simply too complex in general (see Gatt and Krahmer 2018 for a discussion of different architectures). The emergence of neural methods changed this: provided there is enough training data, it does become possible to learn a direct mapping from input to output, as has also been shown in, for example, neural machine translation. As a result, in NLG more recently, neural end-to-end data-totext models have been proposed, which directly learn input-output mappings and rely much less on explicit intermediate representations (Wen et al., 2015; Duˇsek and Jurcicek, 2016; Mei et al., 2016; Lebret et al., 2016; Gehrmann et al., 2018). However, the fact that neural end-to-end approaches are possible does not necessarily entail that they are better than (neural) pipeline models. On the one hand, cascading of errors is a known problem of pipeline models in general (an error in an early module will impact all later modules in the pipeline), which (almost by definition) does not apply to end-to-end models. On the other hand, it is also conceivable that developing dedicated neural modules for specific tasks leads to better performance on each of these successive tasks, and combining"
D19-1052,W17-3518,0,0.365417,"s of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 552–562, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and more reusable, output results. In fact, this has never been systematically studied, and this is the main goal of the current paper. We present a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of output text from RDF input triples, relying on an augmented version of the WebNLG corpus (Gardent et al., 2017b). Using two state-of-the-art deep learning techniques, GRU (Cho et al., 2014) and Transformer (Vaswani et al., 2017), we develop both a neural pipeline and an end-to-end architecture. The former, which also makes use of the NeuralREG approach (Castro Ferreira et al., 2018a), tackles standard NLG tasks (discourse ordering, text structuring, lexicalization, referring expression generation and textual realization) in sequence, while the latter does not address these individual tasks, but directly tries to learn how to map RDF triples into corresponding output text. Using a range of evaluation t"
D19-1052,W18-3601,0,0.0368645,"oals should be verbalized in the target text. In our case, the communicative goals are the RDF triples received as input by the model. Given a set of linearized triples, this step determines the order in which they should be verbalized. For example, given the triple set in Figure 1 in the linearized format: Data The experiments presented in this work were conducted on the WebNLG corpus (Gardent et al., 2017a,b), which consists of sets of h Subject, Predicate, Object i RDF triples and their target texts. In comparison with other popular NLG benchmarks (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018), WebNLG is the most semantically varied corpus, consisting of 25,298 texts describing 9,674 sets of up to 7 RDF triples in 15 domains. Out of these domains, 5 are exclusively present in the test set, being unseen during the training and validation processes. Figure 1 depicts an example of a set of 3 RDF triples and its related <TRIPLE> A.C. Cesena manager Massimo Drago </TRIPLE> <TRIPLE> Massimo Drago club S.S.D. Potenza Calcio </TRIPLE> <TRIPLE> Massimo Drago club Calcio Catania </TRIPLE> Our discourse ordering model would ideally return the set club club manager, which later is used to retr"
D19-1052,N19-1236,0,0.0808169,"Missing"
D19-1052,W17-5525,0,0.471746,"ich the communicative goals should be verbalized in the target text. In our case, the communicative goals are the RDF triples received as input by the model. Given a set of linearized triples, this step determines the order in which they should be verbalized. For example, given the triple set in Figure 1 in the linearized format: Data The experiments presented in this work were conducted on the WebNLG corpus (Gardent et al., 2017a,b), which consists of sets of h Subject, Predicate, Object i RDF triples and their target texts. In comparison with other popular NLG benchmarks (Belz et al., 2011; Novikova et al., 2017; Mille et al., 2018), WebNLG is the most semantically varied corpus, consisting of 25,298 texts describing 9,674 sets of up to 7 RDF triples in 15 domains. Out of these domains, 5 are exclusively present in the test set, being unseen during the training and validation processes. Figure 1 depicts an example of a set of 3 RDF triples and its related <TRIPLE> A.C. Cesena manager Massimo Drago </TRIPLE> <TRIPLE> Massimo Drago club S.S.D. Potenza Calcio </TRIPLE> <TRIPLE> Massimo Drago club Calcio Catania </TRIPLE> Our discourse ordering model would ideally return the set club club manager, which"
D19-1052,P02-1040,0,0.104405,"of our pipeline architecture, where the output of the previous step is fed into the next one. We call these implementations Random, Majority, GRU and Transformer, where each one has its steps solved by one of the proposed baselines or deep learning implementations. In Random and Majority, the referring expressions were generated by the OnlyNames baseline, whereas for GRU and Transformer, NeuralREG was used for the seen entities, OnlyNames for the unseen ones and special rules to realize dates and numbers. 7.3 Automatic Evaluation We evaluated the textual outputs of each system using the BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) metrics. The evaluation was done on the entire test data, as well as only in their seen and unseen domains. Human Evaluation We conducted a human evaluation, selecting the same 223 samples used in the evaluation of the WebNLG challenge (Gardent et al., 2017b). For each sample, we used the original texts and the ones generated by our 6 approaches and by the Melbourne and UPF-FORGe reference systems, totaling 2,007 trials. Each trial displayed the triple set and the respective text. The goal of the participants was to rate the trials based on the fluency (i."
D19-1052,D18-1437,0,0.0327238,"neural pipeline approaches were superior to the end-to-end ones in most tested circumstances: the former generates more fluent texts which better describes data on all domains of the corpus. The difference is most noticeable for unseen domains, where the performance of end-to-end approaches drops considerably. This shows that endto-end approaches do not generalize as well as the pipeline ones. In the qualitative analysis, we also found that end-to-end generated texts have the problem of describing non-linguistic representations which are not present in the input, also known as Hallucination (Rohrbach et al., 2018). The example in Figure 2 shows the advantage of our pipeline approaches in comparison with the end-to-end ones. It depicts the texts produced by the proposed approaches for an unseen set during training of 4 triples, where 2 out of the 4 predicates are present in the WebNLG training set (e.g., birthPlace and occupation). In this context, the pipeline approaches managed to generate a semantic text based on the two seen predicates, whereas the end-to-end approaches hallucinated texts which have no semantic relation with the non-linguistic input. Discussion This study introduced a systematic com"
D19-5512,D18-1066,0,0.0959987,"). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) L"
D19-5512,P82-1020,0,0.735919,"Missing"
D19-5512,P18-1067,0,0.0291422,"013). This is something that machine learning methods might be better suited for as they can be trained on specific content, thus are able to analyze more complex language. Yet, not much is known about the effectiveness of lexiconbased compared to machine learning methods or a ground truth: comparative research is scarce, with few exceptions like Hartmann et al. (2019). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized b"
D19-5512,N19-1423,0,0.00609203,"cation model. Results from the qualitative analysis show that the categories in LIWC might not be sufficient to cover the full range of categorical linguistic differences between the two groups. These shortcomings might be addressed by novel approaches that aim to combine dictionaries with neural text analysis methods, such as Empath (Fast et al., 2016). Or by extending neural Emotion Classification and Emotion Cause Detection systems like (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018) to cover more psychology-relevant categories. Using novel pre-trained word-embeddings such as BERT (Devlin et al., 2019) could also boost the results for the current approach, as this has improved results for many tasks. Table 5: Translated words not in LIWC’s lexicon ordered by importance for relationship goal identification. Blue is indicative of long-term, red is indicative of date 4 Discussion In this study, a lexicon-based text analysis method (LIWC) was compared to machine learning approaches (regression, classification model), with human judgment scores as a baseline. Lexiconbased methods are criticized because they may not capture complex elements of language and do not discriminate between domains. Sti"
D19-5512,W05-1201,1,0.459163,"e similarity of labels from lexicon-based and machine learning approaches compared to a human baseline. The output of LIWC was limited to the six labels discussed in Section 2.3. The 300 dating profiles evaluated in the human evaluation task were rated by LIWC for fair comparison. The same 300 texts were also used (with random ten-fold cross-validation) for the regression model. This model was trained to give continuous scores on the six text labels. Word features were chosen for fair comparison, since LIWC is wordbased and humans also tend to analyze texts at word, phrase, or sentence level (Marsi and Krahmer, 2005). TheilSenRegressor was the regression algorithm used (see Appendix B for details). 2.5 LIWC Table 3: Accuracy scores by humans and classification models. Bold indicates highest score on metric. = 0.24). These predictions were used as a baseline for the relationship goal classification task. Additionally, participants were asked to highlight the words in the text on which they based their longterm or date prediction. All marked words were then collected and counted for the qualitative analysis. 2.4 Human 2.6 Qualitative analysis A qualitative analysis of the output on the relationship goal ide"
D19-5512,Y15-2017,0,0.0147442,"n specific content, thus are able to analyze more complex language. Yet, not much is known about the effectiveness of lexiconbased compared to machine learning methods or a ground truth: comparative research is scarce, with few exceptions like Hartmann et al. (2019). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systema"
D19-5512,S18-1106,0,0.0231313,"to machine learning methods or a ground truth: comparative research is scarce, with few exceptions like Hartmann et al. (2019). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and pr"
D19-5512,J93-1003,0,0.545931,"text on which they based their longterm or date prediction. All marked words were then collected and counted for the qualitative analysis. 2.4 Human 2.6 Qualitative analysis A qualitative analysis of the output on the relationship goal identification task was performed to analyze possible shortcomings of LIWC’s lexicon. Indicative words for identification according to Gini Importance scores obtained with XGBoost were compared to LIWC’s lexicon (Breiman et al., 1984). Furthermore, LIWC’s lexicon was compared to indicative words according to humans and according to log-likelihood ratio scores (Dunning, 1993). Labels from LIWC were also compared with topics obtained by topic modeling (see Appendix B). Relationship Goal Identification Task With this task, the meaningfulness of the lexicons used by LIWC to capture writers’ relationship goals was investigated and compared to the feature sets that humans and machine learning approaches use. To do so, three classification models were used. One classification model used LIWC’s label scores on the aforementioned six labels as features. The second classification model used word features. Furthermore, a meta-classifier was trained on the probability scores"
D19-5512,D18-1137,0,0.0338046,"f lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared the outcomes of (1) LIWC, (2) machine l"
D19-5512,P18-3013,0,0.0236302,"lexiconbased compared to machine learning methods or a ground truth: comparative research is scarce, with few exceptions like Hartmann et al. (2019). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to ass"
D19-5512,S13-1042,0,0.0549387,"Missing"
D19-5512,C18-1329,0,0.0667151,"Missing"
D19-5512,L16-1652,0,0.0362368,"Missing"
D19-5512,D18-1379,0,0.0642104,"rtmann et al. (2019). Thus, outcomes of lexicon-based approaches are often taken at face value, without knowing how they compare to human attributions. While some researchers dispute the effectiveness of lexiconbased approaches (Kross et al., 2019; Johnson and Goldwasser, 2018), there are others who found that such approaches are helpful on their own (Do and Choi, 2015), or that classification performance increases with the addition of features from such approaches (Sawhney et al., 2018; Pamungkas and Patti, 2018). Additionally, most work on writer’s intentions focuses on basic emotions only (Yang et al., 2018; Chen et al., 2018; Yu et al., 2018). Thus, LIWC’s wide range of psychology-related label detection is presently not matched by others. Psychologically motivated, lexicon-based text analysis methods such as LIWC (Pennebaker et al., 2015) have been criticized by computational linguists for their lack of adaptability, but they have not often been systematically compared with either human evaluations or machine learning approaches. The goal of the current study was to assess the effectiveness and predictive ability of LIWC on a relationship goal classification task. In this paper, we compared th"
D19-6307,P07-2045,0,0.00522109,"09 61.90 52.55 0.14 0.08 0.00 0.00 0.00 46.31 35.43 47.85 6.65 29.59 15.54 54.64 49.00 55.04 DIST 6.40 0.05 10.88 10.41 10.09 9.19 11.04 10.89 10.83 9.20 8.55 9.47 12.26 12.37 10.51 0.01 0.01 0.00 0.00 0.00 9.37 9.00 9.60 4.50 10.07 6.41 11.73 9.86 11.73 NIST 53.82 58.78 74.39 70.54 67.88 67.08 71.75 73.33 67.99 63.46 72.35 66.37 65.73 66.21 71.77 55.37 52.86 31.35 31.50 34.07 63.79 59.89 64.76 50.58 57.28 59.11 63.27 62.70 63.5 Table 1: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets. 3 final realized text. The SMT model was built using the Moses toolkit (Koehn et al., 2007). Results and Discussion Concerning the languages covered in the previous version of the shared-task, our approach introduced promising results for English, French, Portuguese and Spanish, with BLEU scores higher than 40. For the newly covered languages, results were promising for the realization of Hindi and Indonesian texts, with BLEU scores higher than 50. On the other hand, our approach obtained low results for Arabic and Russian, and had problems to generate texts in the Asian languages Chinese, Japanese and Korean. For Chinese and Japanese, the problem arose from the fact we did not mana"
D19-6307,N12-1047,0,0.0268211,"nization process well, which had a drastic negative influence on the final results. The settings were copied from the Statistical MT system introduced in Ferreira et al. (2017). At training time, we extract and score phrases up to the size of nine tokens. As feature functions, we used direct and inverse phrase translation probabilities and lexical weighting, as well as word, unknown word and phrase penalties. These feature functions were trained using alignments from the training set obtained by MGIZA (Gao and Vogel, 2008). Model weights were tuned on the development data using 60-batch MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrase-msd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Conclusion This study described a shallow surface realizer for the 11 target languages in the Surface Realization Shared"
D19-6307,D19-6301,0,0.0744659,"than 50. However, the approach appeared to work poorly for Arabic and Russian, and had problems to generate texts in the Asian languages Chinese, Japanese and Korean. In the remainder of this paper, we better describe our method: Section 2 describes the general approach, Section 3 describes the results and discussion of our approach and Section 4 concludes the study, also describing future work which can be done to improve the model. This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rulebased and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available1 . 1 Introduction This study presents the approach developed by the Tilburg University team for the"
D19-6307,W17-3501,1,0.905282,"appeared to work poorly for Arabic and Russian, and had problems to generate texts in the Asian languages Chinese, Japanese and Korean. In the remainder of this paper, we better describe our method: Section 2 describes the general approach, Section 3 describes the results and discussion of our approach and Section 4 concludes the study, also describing future work which can be done to improve the model. This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rulebased and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available1 . 1 Introduction This study presents the approach developed by the Tilburg University team for the shallow track of the Multilingua"
D19-6307,D19-1052,1,0.882314,"Missing"
D19-6307,W18-3604,1,0.390485,"nd had problems to generate texts in the Asian languages Chinese, Japanese and Korean. In the remainder of this paper, we better describe our method: Section 2 describes the general approach, Section 3 describes the results and discussion of our approach and Section 4 concludes the study, also describing future work which can be done to improve the model. This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR’19) (Mille et al., 2019). Based on Ferreira et al. (2017) and on our 2018 submission Ferreira et al. (2018), the approach generates texts by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a rulebased and a statistical machine translation (SMT) model. This year our submission is able to realize texts in the 11 languages proposed for the task, different from our last year submission, which covered only 6 Indo-European languages. The model is publicly available1 . 1 Introduction This study presents the approach developed by the Tilburg University team for the shallow track of the Multilingual Surface Realization Shared Task 2019 (SR’19) (Mi"
D19-6307,D08-1089,0,0.0157895,"ect and inverse phrase translation probabilities and lexical weighting, as well as word, unknown word and phrase penalties. These feature functions were trained using alignments from the training set obtained by MGIZA (Gao and Vogel, 2008). Model weights were tuned on the development data using 60-batch MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrase-msd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Conclusion This study described a shallow surface realizer for the 11 target languages in the Surface Realization Shared Task 2019 (SR’19). In future work, we aim to fix the problems for the Asian languages Chinese, Japanese and Korean. Specifically, for Chinese and Japanese, we require a proper method to tokenize/detokenize the results produced by our 61 approach. Moreover, we aim to design the task based o"
D19-6307,W08-0509,0,0.0141523,"nese and Japanese, the problem arose from the fact we did not manage the tokenization/detokenization process well, which had a drastic negative influence on the final results. The settings were copied from the Statistical MT system introduced in Ferreira et al. (2017). At training time, we extract and score phrases up to the size of nine tokens. As feature functions, we used direct and inverse phrase translation probabilities and lexical weighting, as well as word, unknown word and phrase penalties. These feature functions were trained using alignments from the training set obtained by MGIZA (Gao and Vogel, 2008). Model weights were tuned on the development data using 60-batch MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrase-msd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Conclusion This study des"
D19-6307,W11-2123,0,0.0374484,"Missing"
E17-1062,P16-1054,1,0.554453,"e full name for discourse-new references and only the surname for discourse-old references). It is not specified how the full name should be realised (remember the Henry Charles Bukowski-example), and neither can the approach deal with exceptions to the surname-only rule (remember the Madonna Ciccone-example) or with intratext variation. that human writers would adhere to such a strict rule. Rather, one might expect writers to vary in their choices of which name to use, depending on stylistic and discourse factors, much like the choice of referential form varies as a function of such factors (Ferreira et al., 2016a; Ferreira et al., 2016b). In general, we know very little about how proper names should be generated in text – as far as we know, there have been hardly any systematic corpus studies and only very little concrete proposals on how to automatically generate proper name references. In this paper, we therefore present a large scale corpus analysis, and, based on this, two versions of a new probabilistic model of proper name generation: one that always chooses the most likely proper name form and one that relies on a ‘roulettewheel’ selection model and hence will generate more varied references."
E17-1062,W16-6636,1,0.70436,"e full name for discourse-new references and only the surname for discourse-old references). It is not specified how the full name should be realised (remember the Henry Charles Bukowski-example), and neither can the approach deal with exceptions to the surname-only rule (remember the Madonna Ciccone-example) or with intratext variation. that human writers would adhere to such a strict rule. Rather, one might expect writers to vary in their choices of which name to use, depending on stylistic and discourse factors, much like the choice of referential form varies as a function of such factors (Ferreira et al., 2016a; Ferreira et al., 2016b). In general, we know very little about how proper names should be generated in text – as far as we know, there have been hardly any systematic corpus studies and only very little concrete proposals on how to automatically generate proper name references. In this paper, we therefore present a large scale corpus analysis, and, based on this, two versions of a new probabilistic model of proper name generation: one that always chooses the most likely proper name form and one that relies on a ‘roulettewheel’ selection model and hence will generate more varied references."
E17-1062,P14-5010,0,0.00919792,"l aims to choose the form of a proper name reference (which kind(s) of name and modifier(s) are part of the proper name reference). To build the REGnames corpus, Ferreira et al. (2016c) selected the 1,000 most frequently mentioned people in the Wikilinks corpus. Then for each person, they selected random webpages from Wikilinks which mention the person at least once. On all selected webpages, part-of-speech tagging, lemmatization, named entity recognition, dependency parsing, syntactic parsing, sentiment analysis and coreference resolution was performed by using the Stanford CoreNLP software (Manning et al., 2014). Features By analysing the REGnames corpus, Ferreira et al. (2016c) observed that proper names vary in their forms throughout a text. Moreover, as discussed in the Introduction (Section 1), a proper name form can also be influenced by the person to be mentioned. Thus, we conditioned the choice of a specific proper name form by a set of discourse features that describe the reference as well as to the person to be mentioned. Table 1 depicts the discourse features used to describe the proper name references. We choose them based on the analysis of the REGnames corpus (Section 3). All extracted p"
E17-1062,J11-4007,0,0.181265,"arles rule out other people not named Charles. A standard REG algorithm, such as, for example, the Incremental Algorithm (Dale and Reiter, 1995) can then be used to compute when a name should be used and in which form. Van Deemter’s work is of a theoretical nature; he has not implemented or tested this idea, so we cannot tell how well it can account for proper name references in text. In addition, in this form, his proposal cannot account for possible variations in proper name form throughout a text. The most detailed study of proper name generation, as far as we know, is the seminal study by Siddharthan et al. (2011), which (re-)generates references to people in news summaries. For their algorithm(s), the authors present two manually constructed rules, based on earlier theories of reference, one for discourse-new references (including the full name) and one for discourse-old references (which in full says: “Use surname only, remove all pre- and post-modifiers.”). They discuss, based on corpus analyses, how notions like discourse-new and discourse-old can be learned without manual annotation, and how they codetermine whether additional attributes such as role and affiliation should be included. Finally, th"
E17-1062,W08-1132,0,0.146927,"(nt | p)). Regarding the cases in which the original proper name form indicates the presence of an appositive, we add a description - obtained from Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) - at the end of the generated proper name reference. Linguistic Realization Once we select the form of a proper name reference to a person in a particular discourse context, we linguistically realize this reference by choosing the most likely words - including titles and proper nouns - to be part of it. The process is analogous to the linguistic realization of a set of attributevalues into a description (Bohnet, 2008; Zarriess and Kuhn, 2013). Equation 4 summarizes it. 5 Baselines In order to evaluate the performance of our model, we developed three baseline models. All the models have their outputs constrained to three choices: given name, surname and full name of a person. 658 test our model properly. In total, we used 43,655 proper names references to 432 people as our evaluation data. In order to investigate the influence of the text domain in the generation of proper names, we classified the webpages from where our evaluation data were extracted according to 3 domains: Blog, News and Wiki. All the we"
E17-1062,E91-1028,0,0.345884,"nces annotated in Wikilinks were grouped according to the Wikipedia page of the entity. This procedure enables easy identification of the mentioned entity and facilitates the extraction of more information about it. A model for proper name generation Similarly to the generation of definite descriptions, our model produces a proper name reference in two sequential steps: content selection and linguistic realization. 4.1 Content Selection The content selection discussed here is analogous to the selection of semantic attributes (type, color, size, etc) when generating a description of an entity (Dale and Haddock, 1991; Dale and Reiter, 1995). However, instead of attributes, the content selection step in our model aims to choose the form of a proper name reference (which kind(s) of name and modifier(s) are part of the proper name reference). To build the REGnames corpus, Ferreira et al. (2016c) selected the 1,000 most frequently mentioned people in the Wikilinks corpus. Then for each person, they selected random webpages from Wikilinks which mention the person at least once. On all selected webpages, part-of-speech tagging, lemmatization, named entity recognition, dependency parsing, syntactic parsing, sent"
E17-1062,W16-6605,0,0.123179,"Missing"
E17-1062,P13-1152,0,0.0306962,"garding the cases in which the original proper name form indicates the presence of an appositive, we add a description - obtained from Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) - at the end of the generated proper name reference. Linguistic Realization Once we select the form of a proper name reference to a person in a particular discourse context, we linguistically realize this reference by choosing the most likely words - including titles and proper nouns - to be part of it. The process is analogous to the linguistic realization of a set of attributevalues into a description (Bohnet, 2008; Zarriess and Kuhn, 2013). Equation 4 summarizes it. 5 Baselines In order to evaluate the performance of our model, we developed three baseline models. All the models have their outputs constrained to three choices: given name, surname and full name of a person. 658 test our model properly. In total, we used 43,655 proper names references to 432 people as our evaluation data. In order to investigate the influence of the text domain in the generation of proper names, we classified the webpages from where our evaluation data were extracted according to 3 domains: Blog, News and Wiki. All the webpages whose the url conta"
E17-1062,N16-1048,1,\N,Missing
J03-1003,P99-1017,0,0.217578,"e plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph structures (see, e.g., Gibbons [1985], Cormen, Lei"
J03-1003,E91-1028,0,0.967427,"nhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) The"
J03-1003,J95-2003,0,0.0498947,"Missing"
J03-1003,P97-1027,0,0.417517,"Missing"
J03-1003,W01-0805,1,0.347502,"Missing"
J03-1003,W98-1426,0,0.00488018,"lts from one algorithm to another. (3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms. For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph. (4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm. The outline of this article is as follows. In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed. In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end"
J03-1003,P00-1012,0,0.102768,"r. (3) The graph perspective provides a clean solution for some problems that have plagued earlier algorithms. For instance, the generation of relational expressions (i.e., referring expressions that include references to other objects) is enhanced by the fact that both properties and relations are formalized in the same way, namely, as edges in a graph. (4) The combined use of graphs and cost functions paves the way for a natural integration of traditional rule-based approaches to generating referring expressions and more recent statistical approaches, such as Langkilde and Knight (1998) and Malouf (2000), in a single algorithm. The outline of this article is as follows. In Section 2 the content selection problem for generating referring expressions is explained, and some well-known solutions to the problem are discussed. In Section 3, we describe how scenes can be modeled as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 4 contains a sketch of the basic generation algorithm, which is illustrated with a worked example. In Section 5 various ways to formalize cost functions are discussed and compared. We end with some concludi"
J03-1003,P90-1013,0,0.777333,"phs, and this enhances comparison and integration of the various approaches; (3) the graph perspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four"
J03-1003,W98-1419,0,0.177722,"rspective allows us to solve a number of problems that have plagued earlier algorithms for the generation of referring expressions; and (4) the combined use of graphs and cost functions paves the way for an integration of rule-based generation techniques with more recent stochastic approaches. 1. Introduction The generation of referring expressions is one of the most common tasks in natural language generation and has been addressed by many researchers in the past two decades, including Appelt (1985), Reiter (1990), Dale and Haddock (1991), Dale (1992), Dale and Reiter (1995), Horacek (1997), Stone and Webber (1998), Krahmer and Theune (1998, 2002), Bateman (1999), and van Deemter (2000, 2002). In this article, we present a general, graph-theoretic approach to the generation of referring expressions. We propose to formalize a scene (i.e., a domain of objects and their properties and relations) as a labeled directed graph and describe the content selection problem (which properties and relations to include in a description for an object?) as a subgraph construction problem. The graph perspective has four main advantages. (1) There are many attractive and well-understood algorithms for dealing with graph s"
J03-1003,W00-1424,0,0.0587624,"Missing"
J03-1003,J02-1003,0,0.604587,"Missing"
J03-1003,P89-1009,0,\N,Missing
J05-1002,W98-1425,0,0.0589033,"particular, many systems will contain more intermediate representations. Template-based and standard NLG systems are said to be ‘‘Turing equivalent’’ (Reiter and Dale 1997); that is, each of them can generate all recursively enumerable languages. However, template-based systems have been claimed to be inferior with respect to maintainability, output quality and variation, and well-foundedness. Reiter and Dale (1997) state that template-based systems are more difficult to maintain and update (page 61) and that they produce poorer and less varied output (pages 60, 84) than standard NLG systems. Busemann and Horacek (1998) go even further by suggesting that template-based systems do not embody generic linguistic insights (page 238). Consistent with this view, template-based systems are sometimes overlooked. In fact, the only current textbook on NLG (Reiter and Dale 2000) does not pay any attention to template-based generation, except for a passing mention of the ECRAN system (Geldof and van de Velde 1997). Another example is a recent overview of NLG systems in the RAGS project (Cahill et al. 1999). The selection criteria employed by the authors were that the systems had to be fully implemented, complete (i.e.,"
J05-1002,A94-1047,0,0.0437963,"Missing"
J05-1002,P98-1116,0,0.0120019,"lly, template-based systems do not have to use shortcuts any more than standard NLG systems: Where linguistic rules are available, both types of systems can use them, as we have seen. Another response to the absence of linguistic rules is the use of statistical information derived from corpora, as is increasingly more common in realization, but also for instance in aggregation (e.g., Walker, Rambow, and Rogati 2002). The point we want to make here, however, is that ‘‘template-based’’ systems may profit from such corpus-based approaches just as much as ‘‘standard’’ NLG systems. The approach of Langkilde and Knight (1998), for example, in which corpus-derived n-grams are used for selecting the best ones from among a set of candidates produced by overgeneration, can also be applied to template-based systems (witness the mixed template/ stochastic system of Galley, Fosler-Lussier, and Potamianos [2001]). We have argued that systems that call themselves template based can, in principle, perform all NLG tasks in a linguistically well-founded way and that more and more actually implemented systems of this kind deviate dramatically from the stereotypical systems that are often associated with the term template. Conv"
J05-1002,E03-1019,0,0.0197257,"to be underspecified for number and person, while using attribute grammar rules to complete the specification: Returning to the example above, the number attribute of John and Mary is inferred to be plural (unlike, e.g., John and I); a subject-verb agreement rule makes the further inference that the verb must be realized as walk, rather than walks. 5. Templates: An Updated View A new generation of systems that call themselves template-based have blurred the line between template-based and standard NLG. This is not only because some systems combine standard NLG with templates and canned text (Piwek 2003), but also because modern template-based systems tend to use syntactically structured templates and allow the gaps in them to be filled recursively (i.e., by filling a gap, a new gap may result). Some ‘‘template-based’’ systems, finally, use grammars to aid linguistic realization. These developments call into question the very definition of ‘‘template based’’ (section 2), since the systems that call themselves template-based have come to express their nonlinguistic input with varying degrees of directness. ‘‘Template-based’’ systems vary in terms of linguistic coverage, the amount of syntactic"
J05-1002,W02-1713,0,0.130969,"types of systems have more in common than is generally thought and that it is counterproductive to treat them as distant cousins instead of close siblings. In fact, we argue that there is no crisp distinction between the two. 16 van Deemter, Krahmer, and Theune Real versus Template-Based NLG 3. Template-Based NLG Systems in Practice In recent years, a number of new template-based systems have seen the light, including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997; Theune et al. 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul, and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a substantial research effort, achieving generative capabilities beyond what is usually expected from template-based systems, yet they call themselves template-based, and they clearly fall within the characterization of template-based systems offered above. In this article we draw on our own experiences with a data-to-speech method called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S consists of two modules: (1) a language generation module (LGM)"
J05-1002,W98-1428,0,0.0272105,"stems investigated were template-based. In what follows, we claim that the two types of systems have more in common than is generally thought and that it is counterproductive to treat them as distant cousins instead of close siblings. In fact, we argue that there is no crisp distinction between the two. 16 van Deemter, Krahmer, and Theune Real versus Template-Based NLG 3. Template-Based NLG Systems in Practice In recent years, a number of new template-based systems have seen the light, including TG/2 (Busemann and Horacek 1998), D2S (van Deemter and Odijk 1997; Theune et al. 2001), EXEMPLARS (White and Caldwell 1998), YAG (McRoy, Channarukul, and Ali 2003), and XTRAGEN (Stenzhorn 2002). Each of these systems represents a substantial research effort, achieving generative capabilities beyond what is usually expected from template-based systems, yet they call themselves template-based, and they clearly fall within the characterization of template-based systems offered above. In this article we draw on our own experiences with a data-to-speech method called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch"
J05-1002,C98-1112,0,\N,Missing
J05-1002,E03-1062,0,\N,Missing
J05-1002,W02-2211,0,\N,Missing
J10-2007,W09-0629,0,0.0580008,"Missing"
J10-2007,J07-2004,0,\N,Missing
J10-2007,J07-2013,0,\N,Missing
J10-2007,N10-4007,0,\N,Missing
J12-1006,W08-1107,0,0.322048,"Missing"
J12-1006,W09-0612,0,0.0272606,"Missing"
J12-1006,P02-1012,0,0.321477,"and even harder to choose between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue"
J12-1006,P02-1013,0,0.668288,"xample, that Step 2 selects the properties P ∪ S and P ∪ R, ruling out all distractors. L now takes the form (P ∪ S) ∩ (P ∪ R) (e.g., “the things that are both (women or men) and (women or wearing suits)”). The second phase uses logic optimization techniques, originally designed for the minimization of digital circuits (McCluskey 1965), to simplify this to P ∪ (S ∩ R) (“the women, and the men wearing suits”). Figure 3 Outline of the ﬁrst stage of van Deemter’s (2002) Boolean REG algorithm. 182 Krahmer and van Deemter Computational Generation of Referring Expressions Variations and Extensions. Gardent (2002) drew attention to situations where this proposal produces unacceptably lengthy descriptions; suppose, for example, the algorithm accumulates numerous properties during Steps 1 and 2, before ﬁnding one complex property (a union of three properties) during Step 3 which, on its own would have sufﬁced to identify the referent. This will make the description generated much lengthier than necessary, because the properties from Steps 1 and 2 are now superﬂuous. Gardent’s take on this problem amounts to a reinstatement of Full Brevity embedded in a reformulation of REG as a constraint satisfaction pr"
J12-1006,W11-2815,0,0.0233073,"thms. But, of course, human-likeness is not the only yardstick that can be used. In NLG systems whose main aim is to be practically useful, for example, it may be more important for referring expressions to be clear than to be human-like in all respects. The difference is important because psycholinguists have shown that human speakers have only limited capabilities for taking the addressee into account, frequently producing expressions that cannot be interpreted correctly by an addressee—for example, when they are under time pressure (Horton and Keysar 1996). If usefulness or successfulness (Garouﬁ and Koller 2011), rather than human-likeness, is the yardstick for success then a different type of evaluation test needs to be used. Possible tests include, for example, speed and accuracy of task completion (i.e., how often and how fast do readers ﬁnd the referent?). A variety of hearer-oriented tests is starting to be used in recent REG research (Paraboni, van Deemter, and Masthoff 2007; Khan, van Deemter, and Ritchie 2008), but evaluation of REG algorithms (and of NLG in general) remains difﬁcult (see, e.g., Oberlander 1998; Belz 2009; and Gatt and Belz 2010). Arguably, a central problem is that many diff"
J12-1006,W08-1131,0,0.324755,"Missing"
J12-1006,W09-0629,0,0.211729,"Missing"
J12-1006,W07-2307,1,0.934054,"Missing"
J12-1006,W10-4207,0,0.0217055,"ally all of them, regardless of their purpose, contain an REG module of some sort (Mellish et al. 2006). This is hardly surprising in view of the central role that reference plays in communication. A system providing advice about air travel (White, Clark, and Moore 2010) needs to refer to ﬂights (“the cheapest ﬂight,” “the KLM direct ﬂight”), a pollen forecast system (Turner et al. 2008) needs to generate spatial descriptions for areas with low or high pollen levels (“the central belt and further North”), and a robot dialogue system that assembles construction toys together with a human user (Giuliani et al. 2010) needs to refer to the components (“insert the green bolt through the end of this red cube”). REG “is concerned with how we produce a description of an entity that enables the hearer to identify that entity in a given context” (Reiter and Dale 2000, page 55). Because this can often be done in many different ways, a REG algorithm needs to make a number of choices. According to Reiter and Dale (2000), the ﬁrst choice concerns what form of referring expression is to be used; should the target be referred to, for instance, using its proper name, a pronoun (“he”), or a description (“the man with th"
J12-1006,P10-2011,1,0.850491,"owing feedback from the addressee. Others have argued that conversation partners automatically “align” with each other during interaction (Pickering and Garrod 2004). For instance, Branigan et al. (2010) report on a study showing that if a computer uses the word “seat” instead of the more common “bench” in a referring expression, the user is subsequently more likely to use “seat” instead of “bench” as well. This kind of lexical alignment takes place at the level of linguistic realization, and there is at least one NLG realizer that can mimic this process (Buschmeier, Bergmann, and Kopp 2009). Goudbeek and Krahmer (2010) found that speakers in an interactive setting also align at the level of content selection; they present experimental data showing that human speakers may opt for a “dispreferred” attribute (even when a preferred attribute would be distinguishing) when these were salient in a preceding interaction. The reader may want to consult Arnold (2008) for an overview of studies on reference choice in context, Clark and Bangerter (2004) for a discussion of studies on collaborative references, or Krahmer (2010) for a confrontation of some recent psycholinguistic ﬁndings with REG algorithms. Psycholingui"
J12-1006,J95-2003,0,0.583506,"Missing"
J12-1006,J86-3001,0,0.285574,"r and van Deemter Computational Generation of Referring Expressions think of salience—just like height or age—as coming in degrees. Existing theories of linguistic salience do not merely separate what is salient from what is not. They assign referents to different salience bands, based on factors such as recency of mention and syntactic structure (Gundel, Hedberg, and Zacharski 1993; Haji˘cov´a 1993; Grosz, Joshi, and Weinstein 1995). Salience and Context-Sensitive REG. Early REG algorithms (Kronfeld 1990; Dale and Reiter 1995) assumed that salience could be modeled by means of a focus stack (Grosz and Sidner 1986): A referring expression is taken to refer to the highest element on the stack that matches its description (see also DeVault, Rich, and Sidner 2004). Krahmer and Theune (2002) argue that the focus stack approach is not ﬂexible enough for context-sensitive generation of descriptions. They propose to assign individual salience weights (sws) to the objects in the domain, and to reinterpret referring expressions like “the man” as referring to the currently most salient man. Once such a gradable notion of salience is adopted, we are back in the territory of Section 3.3. One simple way to generate"
J12-1006,J95-3003,0,0.637708,"contains the seeds of much later work in REG, given its skepticism about the naturalness of minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Da"
J12-1006,W08-1129,0,0.0958485,"Missing"
J12-1006,P97-1027,0,0.0460623,"t Determination, in the style of a pipeline, with most of the actual research focusing predominantly on Content Determination. One might have thought that good results are easy to achieve by sending the output of the Content Determination module to 188 Krahmer and van Deemter Computational Generation of Referring Expressions a generic realizer (i.e., a program converting meaning representations into natural language). With hindsight, any such expectations must probably count as naive. Some REG studies have taken a different approach, interleaving Content Determination and Surface Realization (Horacek 1997; Stone and Webber 1998; Krahmer and Theune 2002; Siddharthan and Copestake 2004), running counter to the pipeline architecture (Mellish et al. 2006). In this type of approach, syntactic structures are built up in tandem with semantic descriptions: when type, man has been added to the semantic description, a partial syntactic tree is constructed for a noun phrase, whose head noun is man. As more properties are added to the semantic description, appropriate modiﬁers are slotted into the syntax tree; ﬁnally, the noun phrase is completed by choosing an appropriate determiner. Even in these inte"
J12-1006,W05-1606,0,0.0754419,"he greatest height of all men in this KB, the set of properties can be converted into {man, height = maximum}, where the exact height has been pruned away. The new description can be realized as “the tallest man” or simply as “the tall man” (provided the referent’s height exceeds a certain minimum value). The algorithm becomes more complicated when sets are referred to (because the elements of the target set may not all have the same heights), or when two or more gradable properties are combined (as in “the strong, tall man in the expensive car”) (van Deemter 2006). Variations and Extensions. Horacek (2005) integrates vagueness with other types of uncertainty. Horacek could be said to depict an REG algorithm as essentially a gambler who wants to maximize the chance of the referent being identiﬁed on the basis of 185 Computational Linguistics Volume 38, Number 1 the generated expression. Other things being equal, for example, it may be safer to identify a dog as being “owned by John,” than as being “tall,” because the latter involves borderline cases. A similar approach can be applied to perceptual uncertainty (as when it is uncertain whether the hearer will be able to observe a certain property)"
J12-1006,C08-1055,1,0.929481,"Missing"
J12-1006,J04-4001,0,0.0233588,"between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as “several"
J12-1006,P07-1043,0,0.0461562,"minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link between the Gricean maxims and the generation of r"
J12-1006,J10-2007,1,0.854011,"k from the addressee. Others have argued that conversation partners automatically “align” with each other during interaction (Pickering and Garrod 2004). For instance, Branigan et al. (2010) report on a study showing that if a computer uses the word “seat” instead of the more common “bench” in a referring expression, the user is subsequently more likely to use “seat” instead of “bench” as well. This kind of lexical alignment takes place at the level of linguistic realization, and there is at least one NLG realizer that can mimic this process (Buschmeier, Bergmann, and Kopp 2009). Goudbeek and Krahmer (2010) found that speakers in an interactive setting also align at the level of content selection; they present experimental data showing that human speakers may opt for a “dispreferred” attribute (even when a preferred attribute would be distinguishing) when these were salient in a preceding interaction. The reader may want to consult Arnold (2008) for an overview of studies on reference choice in context, Clark and Bangerter (2004) for a discussion of studies on collaborative references, or Krahmer (2010) for a confrontation of some recent psycholinguistic ﬁndings with REG algorithms. Psycholingui"
J12-1006,W08-1138,1,0.918692,"Alternatively, one could assign costs in accordance with the list of preferred attributes in the IA, making more preferred properties cheaper than less preferred ones. A third possibility is to compute the costs of an edge e in terms of the probability P(e) that e occurs in a distinguishing description (which can be estimated by counting occurrences in a corpus), making frequent properties cheap and rare ones expensive: cost(e) = −log2 (P(e)) Experiments with stochastic cost functions have shown that these enable the graphbased algorithm to capture a lot of the ﬂexibility of human references (Krahmer et al. 2008; Viethen et al. 2008). In the graph-based perspective, relations are treated in the same way as individual properties, and there is no risk of running into inﬁnite loops (“the cup to the left of the saucer to the right of the cup . . . ”). Unlike Dale and Haddock (1991) and Kelleher and Kruijff (2006), no special measures are required, because a relational edge is either included in a referring graph or not: including it twice is not possible. Van Deemter and Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in terms of graphs. They argue, however, that the g"
J12-1006,J03-1003,1,0.952883,"Missing"
J12-1006,N03-1020,0,0.0387736,"e, Rambow, and Whittaker 2000). The BLEU (Papineni et al. 2002) and NIST (Doddington 2002) metrics, which have their origin in machine translation evaluation, have also been proposed for REG evaluation. BLEU measures n-gram overlap between strings; for machine translation n is often set to 4, but given that referring expressions tend to be short, n = 3 seems a better option for REG evaluation (Gatt, Belz, and Kow 2009). NIST is a BLEU variant giving more importance to less frequent (and hence more informative) n-grams. Finally, Belz and Gatt (2008) also use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for evaluating automatically generated summaries. An obvious beneﬁt of these string metrics is that they are easy to compute automatically, whereas property-based evaluation measures such as Dice require an extensive manual annotation of selected properties. However, the added value of string-based metrics for REG is relatively unclear. It is not obvious, for instance, that a smaller Levenshtein distance is always to be preferred over a longer one; the expressions “the man wearing a t-shirt” and “the woman wearing a t-shirt” are at a mere Levenshtein distance of 2 from ea"
J12-1006,P00-1012,0,0.0587648,"he performance of an REG algorithm on a given data set. We shall see that although much work has been done in recent years, there are still signiﬁcant open questions, particularly regarding the relation between automatic metrics and human judgments. 195 Computational Linguistics Volume 38, Number 1 5.1 Corpora for REG Evaluation Text corpora are full of referring expressions. For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modiﬁer orderings (Shaw and Hatzivassiloglou 1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of anaphoric links between referring expressions. The texts that make up the GNOME corpus (Poesio et al. 2004), for instance, contain descriptions of museum objects and medical patient information leaﬂets, with each of the two subcorpora containing some 6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms. For that, one needs a corpus that is fully “semantically transparent” (van Deemter, van der Sluis, a"
J12-1006,W99-0108,0,0.0305107,"remarkably difﬁcult to decide when these should be used, and even harder to choose between the different types of demonstratives (Piwek 2008). Concerning pronouns, Krahmer and Theune suggested that “he” abbreviates “the (most salient) 187 Computational Linguistics Volume 38, Number 1 man,” and “she” “the (most salient) woman.” In this way, algorithms for generating distinguishing descriptions might also become algorithms for pronoun generation. Such an approach to pronoun generation is too simple, however, because additional factors are known to determine whether a pronoun is suitable or not (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004). Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example, emphasized the role of topics and discourse structure for pronoun generation, and pointed out that the changes in time scale are a reliable cue for this. In particular, they found that in certain places a deﬁnite description was used where a pronoun would have been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as"
J12-1006,W09-0608,0,0.021117,"e of an REG algorithm on a given data set. We shall see that although much work has been done in recent years, there are still signiﬁcant open questions, particularly regarding the relation between automatic metrics and human judgments. 195 Computational Linguistics Volume 38, Number 1 5.1 Corpora for REG Evaluation Text corpora are full of referring expressions. For evaluating the realization of referring expressions, such corpora are very suitable, and various researchers have used them, for instance, to evaluate algorithms for modiﬁer orderings (Shaw and Hatzivassiloglou 1999; Malouf 2000; Mitchell 2009). Text corpora are also important for the study of anaphoric links between referring expressions. The texts that make up the GNOME corpus (Poesio et al. 2004), for instance, contain descriptions of museum objects and medical patient information leaﬂets, with each of the two subcorpora containing some 6,000 NPs. Much information is marked up, including anaphoric links. Yet, text corpora of this kind are of limited value for evaluating the content selection part of REG algorithms. For that, one needs a corpus that is fully “semantically transparent” (van Deemter, van der Sluis, and Gatt 2006): A"
J12-1006,N03-2024,0,0.134978,"Missing"
J12-1006,W98-0607,0,0.151953,"Missing"
J12-1006,P02-1040,0,0.088798,"2008). The measures discussed so far do not take the actual linguistic realization of the referring expressions into account. For these, string distance metrics are obvious candidates, because these have proven their worth in various other areas of computational linguistics. One well-known string distance metric, which has also been proposed for REG evaluation, is the Levenshtein (1966) distance: The minimal number of insertions, deletions, and substitutions needed to convert one string into another, possibly normalized with respect to length (Bangalore, Rambow, and Whittaker 2000). The BLEU (Papineni et al. 2002) and NIST (Doddington 2002) metrics, which have their origin in machine translation evaluation, have also been proposed for REG evaluation. BLEU measures n-gram overlap between strings; for machine translation n is often set to 4, but given that referring expressions tend to be short, n = 3 seems a better option for REG evaluation (Gatt, Belz, and Kow 2009). NIST is a BLEU variant giving more importance to less frequent (and hence more informative) n-grams. Finally, Belz and Gatt (2008) also use the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed for evaluating automati"
J12-1006,J07-2004,1,0.939287,"Missing"
J12-1006,passonneau-2006-measuring,0,0.0973052,"Missing"
J12-1006,J04-3003,0,0.073445,"Missing"
J12-1006,J98-2001,0,0.516966,"been unambiguous. This happened, for example, when the time frame of the current sentence differed from that of the sentence in which the previous mention occurred, as can be signaled, for example, by a change in tense or a cue-phrase such as “several months ago.” Kibble and Power (2004), in an alternative approach, use Centering Theory as their starting point in a constraint-based text generation framework, taking into account constraints such as salience, cohesion, and continuity for the choice of referring expressions. Many studies on contextual reference take text as their starting point (Poesio and Vieira 1998; Belz et al. 2010, among others), unlike the majority of REG research discussed so far, which uses standard knowledge representations of the kind exempliﬁed in Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant is presented by Siddharthan and Copestake (2004), who set themselves the task of generating a referring expression at a speciﬁc point in a discourse, without assuming that a knowledge base (in the normal sense of the word) is available: All their algorithm has to go by is text. For example, a text might start saying “The new president applauded the o"
J12-1006,P90-1013,0,0.5053,"Missing"
J12-1006,C92-1038,0,0.781158,"hich focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concerned with the link between the Gricean maxims and the generation of referring expressions. They discuss the following pair of examples: (1) Sit by the table. (2) Sit by the brown wooden table. In a situation where there is only one table, which happens to be brown and wooden, both the descriptions in (1) and (2) would successfully refer to their target. However, if you hear (2) you might make the additional inference that it is signiﬁcant to know that the table is brown a"
J12-1006,W10-4212,1,0.906985,"Missing"
J12-1006,P88-1003,0,0.520454,"ctive Plurals. Reference to sets is a rich topic, where many issues on the borderline between theoretical, computational, and experimental linguistics are waiting to be explored. Most computational proposals, so far, use properties that apply to individual objects. To refer to a set, in this view, is to say things that are true of each member of the set. Such references may be contrasted with collective ones (e.g., “the lines that run parallel to each other,” “the group of four people”) which are more complicated from a 183 Computational Linguistics Volume 38, Number 1 semantic point of view (Scha and Stallard 1988; Lønning 1997, among others). For initial ideas about the generation of collective plurals, we refer to Stone (2000). 3.2 Relational Descriptions Another important limitation of most early REG algorithms is that they are restricted to one-place predicates (e.g., “being a man”), instead of relations involving two or more arguments. Even a property like “wearing a suit” is modeled as if it were simply a one-place predicate without internal structure (instead of a relation between a person and a piece of clothing). This means that the algorithms in question are unable to identify one object via"
J12-1006,P04-1052,0,0.150792,"4), in an alternative approach, use Centering Theory as their starting point in a constraint-based text generation framework, taking into account constraints such as salience, cohesion, and continuity for the choice of referring expressions. Many studies on contextual reference take text as their starting point (Poesio and Vieira 1998; Belz et al. 2010, among others), unlike the majority of REG research discussed so far, which uses standard knowledge representations of the kind exempliﬁed in Table 1 (or some more sophisticated frameworks, see Section 4). An interesting variant is presented by Siddharthan and Copestake (2004), who set themselves the task of generating a referring expression at a speciﬁc point in a discourse, without assuming that a knowledge base (in the normal sense of the word) is available: All their algorithm has to go by is text. For example, a text might start saying “The new president applauded the old president.” From this alone, the algorithm has to ﬁgure out whether, in the next sentence, it can talk about “the old president” (or some other suitable noun phrase) without risk of misinterpretation by the reader. The authors argue that standard REG methods can achieve reasonable results in"
J12-1006,J11-4007,0,0.0688613,"Missing"
J12-1006,W06-1412,0,0.0869462,"Walker (2005) and Gupta and Stent (2005), who studied references in dialogue corpora discussed in Section 5. They found that in these data sets, traditional algorithms are outperformed by simple strategies that pay attention to the referring expressions produced earlier in the dialogue. A more recent machine learning experiment on a larger scale, using data from the iMap corpus, conﬁrmed the importance of features related to the process of alignment (Viethen, Dale, and Guhe 2011). Other researchers have started exploring the generation of referring expressions in interactive settings as well. Stoia et al. (2006), for example, presented a system that generates references in situated dialogues, taking into account both dialogue history and spatial visual context, deﬁned in terms of which distractors are in the current ﬁeld of vision of the speakers and how distant they are from the target. Janarthanam and Lemon (2009) present a method which automatically adapts to the expertise level of the intended addressee (using “the router” when communicating with an expert user, and “the black block with the lights” while interacting with a novice). This line of research ﬁts in well with another, more general, st"
J12-1006,W00-1416,0,0.136275,"xperimental linguistics are waiting to be explored. Most computational proposals, so far, use properties that apply to individual objects. To refer to a set, in this view, is to say things that are true of each member of the set. Such references may be contrasted with collective ones (e.g., “the lines that run parallel to each other,” “the group of four people”) which are more complicated from a 183 Computational Linguistics Volume 38, Number 1 semantic point of view (Scha and Stallard 1988; Lønning 1997, among others). For initial ideas about the generation of collective plurals, we refer to Stone (2000). 3.2 Relational Descriptions Another important limitation of most early REG algorithms is that they are restricted to one-place predicates (e.g., “being a man”), instead of relations involving two or more arguments. Even a property like “wearing a suit” is modeled as if it were simply a one-place predicate without internal structure (instead of a relation between a person and a piece of clothing). This means that the algorithms in question are unable to identify one object via another, as when we say “the woman next to the man who wears a suit,” and so on. One early paper does discuss relatio"
J12-1006,W98-1419,0,0.337931,"h later work in REG, given its skepticism about the naturalness of minimal descriptions, its use of Rosch (1978)–style basic categories, and its acknowledgment of the role of computational complexity. Broadly speaking, it suggests an incremental generation strategy, compatible with the ones described subsequently, although it is uncertain what exactly was implemented. In recent years, the Appelt–Kronfeld line of research has largely given way to a new research tradition which focused away from the full complexity of human communication, with notable exceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O’Donnell, Cheng and Hitzeman (1998), and Koller and Stone (2007). 2.2 Generating Distinguishing Descriptions In the early 1990s a new approach to REG started gaining currency, when Dale and Reiter re-focused on the problem of determining what properties a referring expression 176 Krahmer and van Deemter Computational Generation of Referring Expressions should use if identiﬁcation of the referent is the central goal (Dale 1989, 1992; Reiter 1990; Reiter and Dale 1992). This line of work culminated in the seminal paper by Dale and Reiter (1995). Like Appelt (1985), Dale and Reiter are concern"
J12-1006,P11-2116,1,0.841007,"the ﬁrst case, and a tie between two properties that have the same discriminatory power in the second. To resolve such ties frequency data would clearly be helpful. Similar questions apply to other generation algorithms. For instance, the graph-based algorithm as described by Krahmer et al. (2008) assigns one of three different costs to properties (they can be free, cheap, or somewhat expensive), and frequency data is used to determine which costs should be assigned to which properties (properties that are almost always used in a particular domain can be for free, etc.). A recent experiment (Theune et al. 2011) suggests that training the graph-based algorithm on a corpus with a few dozen items may already lead to a good performance. In general, knowing how much data is required for a new domain to reach a good level of performance is an important open problem for many REG algorithms. 6.2 How Do We Move beyond the “Paradigms” of Reference? A substantial amount of REG research focuses on what we referred to in the Introduction as the “paradigms” of reference: “ﬁrst-mention” distinguishing descriptions consisting of a noun phrase starting with “the” that serve to identify some target, and that do so wi"
J12-1006,W09-0607,0,0.100674,"Missing"
J12-1006,W08-1104,0,0.0894647,"cal information (Reiter and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is among those that have received most scholarly attention. A survey of implemented, practical NLG systems shows that virtually all of them, regardless of their purpose, contain an REG module of some sort (Mellish et al. 2006). This is hardly surprising in view of the central role that reference plays in communication. A system providing advice about air travel (White, Clark, and Moore 2010) needs to refer to ﬂights (“the cheapest ﬂight,” “the KLM direct ﬂight”), a pollen forecast system (Turner et al. 2008) needs to generate spatial descriptions for areas with low or high pollen levels (“the central belt and further North”), and a robot dialogue system that assembles construction toys together with a human user (Giuliani et al. 2010) needs to refer to the components (“insert the green bolt through the end of this red cube”). REG “is concerned with how we produce a description of an entity that enables the hearer to identify that entity in a given context” (Reiter and Dale 2000, page 55). Because this can often be done in many different ways, a REG algorithm needs to make a number of choices. Acc"
J12-1006,J02-1003,1,0.886397,"Missing"
J12-1006,J06-2002,1,0.925449,"Missing"
J12-1006,W06-1420,1,0.92113,"Missing"
J12-1006,W06-1410,0,0.414977,"or this data set, participants were asked to describe objects in various computer generated scenes. Each of these scenes contained up to 30 objects (“cones”) randomly positioned on a virtual surface. All objects had the same shape and size, and hence targets could only be distinguished using their color (either green or purple) and their location on the surface (“the green cone at the left bottom”). Each participant was asked to identify targets in one shot, and for the beneﬁt of an addressee who was physically present but did not interact with the participant. The Drawer corpus, collected by Viethen and Dale (2006), has a similar objective, but here targets are real, being one of 16 colored drawers in a ﬁling cabinet. On different occasions, participants were given a random number between 1 and 16 and asked to refer to the corresponding drawer for an onlooker. Naturally, they were asked not to use the number; instead they could refer to the target drawers using color, row, and column, or some combination of these. In this corpus, referring expressions (“the pink drawer in the ﬁrst row, third column”) once again solely serve an identiﬁcation purpose. Viethen and Dale (2008) also collected another corpus"
J12-1006,D11-1107,0,0.117858,"Missing"
J12-1006,viethen-etal-2008-controlling,1,0.891802,"uld assign costs in accordance with the list of preferred attributes in the IA, making more preferred properties cheaper than less preferred ones. A third possibility is to compute the costs of an edge e in terms of the probability P(e) that e occurs in a distinguishing description (which can be estimated by counting occurrences in a corpus), making frequent properties cheap and rare ones expensive: cost(e) = −log2 (P(e)) Experiments with stochastic cost functions have shown that these enable the graphbased algorithm to capture a lot of the ﬂexibility of human references (Krahmer et al. 2008; Viethen et al. 2008). In the graph-based perspective, relations are treated in the same way as individual properties, and there is no risk of running into inﬁnite loops (“the cup to the left of the saucer to the right of the cup . . . ”). Unlike Dale and Haddock (1991) and Kelleher and Kruijff (2006), no special measures are required, because a relational edge is either included in a referring graph or not: including it twice is not possible. Van Deemter and Krahmer (2007) show that many of the proposals discussed in Section 3 can be recast in terms of graphs. They argue, however, that the graph-based approach is"
J12-1006,viethen-etal-2010-dialogue,0,0.0261134,"ess suitable for studying content determination. To facilitate the study of reference, the iMap corpus was created (Guhe and Bard 2008), a modiﬁed version of the Map Task corpus where landmarks are not labelled, and systematically differ along a number of dimensions, including type (owl, penguin, etc.), 196 Krahmer and van Deemter Computational Generation of Referring Expressions number (singular, plural) and color; a target may thus be referred to as “the two purple owls.” Because participants may refer to targets more than once, it becomes possible to study initial and subsequent reference (Viethen et al. 2010). Yet another example is the Coconut corpus (Di Eugenio et al. 2000), a set of taskoriented dialogues in which participants negotiate which furniture items they want to buy on a ﬁxed, shared budget. Referring expressions in this corpus (“a yellow rug for 150 dollars”) do not only contain information to identify a particular piece of furniture, but also include properties which directly refer to the task at hand (e.g., how much money is still available for a particular furniture item and what the state of agreement between the negotiators is). An attractive aspect of these corpora is that they"
J12-1006,J10-2001,0,0.0303998,"Missing"
J12-1006,H89-1033,0,0.222391,"eworks, such as Graph Theory and Description Logic (Section 4), and collection of data and evaluation of REG algorithms (Section 5). Section 6 highlights open questions and avenues for future work. Section 7 summarizes our ﬁndings. 2. A Very Short History of Pre-2000 REG Research The current survey focuses primarily on the progress in REG research in the 21st century, but it is important to have a basic insight into pre-2000 REG research and how it laid the foundation for much of the current work. 2.1 First Beginnings REG can be traced back to the earliest days of Natural Language Processing; Winograd (1972) (Section 8.3.3, Naming Objects and Events), for example, sketches a primitive “incremental” REG algorithm, used in his SHRDLU program. In the 1980s, researchers such as Appelt and Kronfeld set themselves the ambitious task of modeling the human capacity for producing and understanding referring expressions in programs such as KAMP and BERTRAND (Appelt 1985; Appelt and Kronfeld 1987; Kronfeld 1990). They argued that referring expressions should be studied as part of a larger speech act. KAMP (Appelt 1985), for example, was conceived as a general utterance planning system, building on Cohen and"
koolen-krahmer-2010-tuna,W07-2307,0,\N,Missing
koolen-krahmer-2010-tuna,W09-0629,0,\N,Missing
koolen-krahmer-2010-tuna,J03-1003,1,\N,Missing
koolen-krahmer-2010-tuna,J10-2007,1,\N,Missing
koolen-krahmer-2010-tuna,J02-1003,0,\N,Missing
koolen-krahmer-2010-tuna,W10-4221,1,\N,Missing
koolen-krahmer-2010-tuna,P89-1009,0,\N,Missing
N16-1048,P02-1012,0,0.285859,"994) to neonatal intensive care reports (Portet et al., 2009). One important way to achieve coherence in texts is by generating appropriate referring expressions throughout the text (Krahmer and van Deemter, 2012). In this generation process, the choice of referential form is a crucial task (Reiter and Dale, 2000): when referring to a person or object in a text, should the system use a proper name (“Phillip Anschutz”), a definite description (“the American entrepreneur”) or a pronoun (“he”)? Despite the large amount of algorithms developed for deciding upon the form of a referring expression (Callaway and Lester, 2002; Greenbacker and McCoy, 2009; Gupta and Bandopadhyay, 2009; Or˘asan and Dornescu, 2009; Greenbacker et al., 2010), it is difficult to know how well these algorithms actually perform. Typically, such algorithms are evalIn general, corpora of referring expressions have only one gold standard referential form for each situation, while different writers may conceivably vary in the referential form they would use. This complicates the development and evaluation of text generation algorithms, since these will typically attempt to predict the corpus gold standard, which may not always be representat"
N16-1048,W10-4231,0,0.0244126,"Missing"
N16-1048,J95-2003,0,0.266045,"dual variation, we analysed to what extent three linguistic factors had an impact on the entropy scores: syntactic position, referential status and recency. We found a higher amount of individual variation when writers had to choose referential forms in the direct object position, referring to previously mentioned topics in the text and first mentioned ones in the sentence, and references that were relatively distant from the most recent antecedent 426 reference to the same topic. These findings can be related to theories of reference involving the salience of a referent (Gundel et al., 1993; Grosz et al., 1995, among others). Brennan (1995), for example, argued that references in the role of the subject of a sentence are more likely to be salient than references in the role of the object. Chafe (1994), to give a second example, pointed out that references to previously mentioned referents in the discourse and ones that are close to their antecedent are more likely to be salient than references to new referents or ones that are distant from their antecedents. Note, incidentally, that none of these earlier studies address the issue of individual variation in referential form. Arguably, the amount of"
N16-1048,W09-2820,0,0.401987,"2009). One important way to achieve coherence in texts is by generating appropriate referring expressions throughout the text (Krahmer and van Deemter, 2012). In this generation process, the choice of referential form is a crucial task (Reiter and Dale, 2000): when referring to a person or object in a text, should the system use a proper name (“Phillip Anschutz”), a definite description (“the American entrepreneur”) or a pronoun (“he”)? Despite the large amount of algorithms developed for deciding upon the form of a referring expression (Callaway and Lester, 2002; Greenbacker and McCoy, 2009; Gupta and Bandopadhyay, 2009; Or˘asan and Dornescu, 2009; Greenbacker et al., 2010), it is difficult to know how well these algorithms actually perform. Typically, such algorithms are evalIn general, corpora of referring expressions have only one gold standard referential form for each situation, while different writers may conceivably vary in the referential form they would use. This complicates the development and evaluation of text generation algorithms, since these will typically attempt to predict the corpus gold standard, which may not always be representative of the choices of different writers. Although recent wo"
N16-1048,konstantinova-etal-2012-review,0,0.0221365,"Missing"
N16-1048,J12-1006,1,0.927171,"Missing"
N16-1048,W09-2822,0,0.0331059,"Missing"
N16-1048,U10-1013,0,0.0265515,"ally perform. Typically, such algorithms are evalIn general, corpora of referring expressions have only one gold standard referential form for each situation, while different writers may conceivably vary in the referential form they would use. This complicates the development and evaluation of text generation algorithms, since these will typically attempt to predict the corpus gold standard, which may not always be representative of the choices of different writers. Although recent work in text generation has explored individual variation in the content determination of definite descriptions (Viethen and Dale, 2010; Ferreira and Paraboni, 2014), to the best of our knowledge this has not been systematically explored for choosing referential forms. In this paper, we collect and analyze a new corpus to address this issue. In the collection, we presented different writers with texts in which all references to the main topic of the text have been replaced with gaps. The task of the participants was to fill each of those gaps with a reference to the topic. In the analysis, we estimated to what extent different writers agree with each other in terms of normalized entropy. In addition, we study whether this var"
P01-1012,P99-1040,0,0.0605618,"Missing"
P01-1012,A00-2028,0,0.0274759,"Missing"
P08-2049,J05-3002,0,0.415139,"sentence like (3). We show that question-based sentence fusion is a better defined task than generic sentence fusion (Q-based fusions are shorter, display less variety in length, yield more identical results and have higher normalized Rouge scores). Moreover, we show that in a QA setting, participants strongly prefer Q-based fusions over generic ones, and have a preference for union over intersection fusions. 1 (3) Sentence fusion is a text-to-text generation application, which given two related sentences, outputs a single sentence expressing the information shared by the two input sentences (Barzilay and McKeown 2005). Consider, for example, the following pair of sentences:1 Posttraumatic stress disorder (PTSD) is a psychological disorder which is classified as an anxiety disorder in the DSM-IV. (2) Posttraumatic stress disorder (PTSD) is a psychological disorder. Barzilay and McKeown (2005) argue convincingly that employing such a fusion strategy in a multidocument summarization system can result in more informative and more coherent summaries. Introduction (1) Paul van Pelt Tilburg University Tilburg, The Netherlands paul.vanpelt@gmail.com Posttraumatic stress disorder (abbrev. PTSD) is a psychological d"
P08-2049,N03-1020,0,0.120967,"Missing"
P08-2049,W05-1612,1,0.416906,"y for discussions on the Rouge metrics and to Carel van Wijk for statistical advice. The dataset described in this paper (2200 fusions of pairs of sentences) is available upon request. This research was carried out within the Deaso project (http://daeso.uvt.nl/). 1 All examples are English translations of Dutch originals. It should be noted, however, that there are multiple ways to fuse two sentences. Besides fusing the shared information present in both sentences, we can conceivably also fuse them such that all information present in either of the sentences is kept, without any redundancies. Marsi and Krahmer (2005) refer to this latter strategy as union fusion (as opposed to intersection fusion, as in (3)). A possible union fusion of (1) and (2) would be: (4) Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that can develop after exposure to a terrifying event. Marsi and Krahmer (2005) propose an algorithm which is capable of producing both fusion types. Which type is more useful is likely to depend on the kind of application and information needs of the user, but this is essen"
P08-2049,W04-1016,0,\N,Missing
P10-2011,W09-0608,0,0.30897,"Expression Generation Martijn Goudbeek University of Tilburg Tilburg, The Netherlands m.b.goudbeek@uvt.nl Emiel Krahmer University of Tilburg Tilburg, The Netherlands e.j.krahmer@uvt.nl Abstract rithms rely on similar distinctions. The Graphbased algorithm (Krahmer et al., 2003), for example, searches for the cheapest description for a target, and distinguishes cheap attributes (such as color) from more expensive ones (orientation). Realization of referring expressions has received less attention, yet recent studies on the ordering of modifiers (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) also work from the assumption that some orderings (large red) are preferred over others (red large). We argue that such preferences are less stable when referring expressions are generated in interactive settings, as would be required for applications such as spoken dialogue systems or interactive virtual characters. In these cases, we hypothesize that, besides domain preferences, also the referring expressions that were produced earlier in the interaction are important. It has been shown that if one dialogue participant refers to a couch as a sofa, the next speaker is more likely to use the"
P10-2011,W09-0612,0,0.0960873,"or “lexical entrainment” (Brennan and Clark, 1996) can be seen as a specific form of “alignment” (Pickering and Garrod, 2004) between speaker and addressee. Pickering and Garrod argue that alignment may take place on all levels of interaction, and indeed it has been shown that participants also align their intonation patterns and syntactic structures. However, as far as we know, experimental evidence for alignment on the level of content planning has never been given, and neither have alignment effects in modifier orderings during realization been shown. With a few notable exceptions, such as Buschmeier et al. (2009) who study alignment in micro-planning, and Janarthanam and Lemon (2009) who study alignment in expertise levels, alignment has received little attention in NLG so far. This paper is organized as follows. Experiment I studies the trade-off between adaptation Current Referring Expression Generation algorithms rely on domain dependent preferences for both content selection and linguistic realization. We present two experiments showing that human speakers may opt for dispreferred properties and dispreferred modifier orderings when these were salient in a preceding interaction (without speakers be"
P10-2011,P99-1018,0,0.0548549,"Missing"
P10-2011,W07-2307,0,0.146874,"Missing"
P10-2011,W09-0611,0,0.0765178,"specific form of “alignment” (Pickering and Garrod, 2004) between speaker and addressee. Pickering and Garrod argue that alignment may take place on all levels of interaction, and indeed it has been shown that participants also align their intonation patterns and syntactic structures. However, as far as we know, experimental evidence for alignment on the level of content planning has never been given, and neither have alignment effects in modifier orderings during realization been shown. With a few notable exceptions, such as Buschmeier et al. (2009) who study alignment in micro-planning, and Janarthanam and Lemon (2009) who study alignment in expertise levels, alignment has received little attention in NLG so far. This paper is organized as follows. Experiment I studies the trade-off between adaptation Current Referring Expression Generation algorithms rely on domain dependent preferences for both content selection and linguistic realization. We present two experiments showing that human speakers may opt for dispreferred properties and dispreferred modifier orderings when these were salient in a preceding interaction (without speakers being consciously aware of this). We discuss the impact of these findings"
P10-2011,J03-1003,1,0.936526,"Missing"
P10-2011,P00-1012,0,0.117207,"ring Referring Expression Generation Martijn Goudbeek University of Tilburg Tilburg, The Netherlands m.b.goudbeek@uvt.nl Emiel Krahmer University of Tilburg Tilburg, The Netherlands e.j.krahmer@uvt.nl Abstract rithms rely on similar distinctions. The Graphbased algorithm (Krahmer et al., 2003), for example, searches for the cheapest description for a target, and distinguishes cheap attributes (such as color) from more expensive ones (orientation). Realization of referring expressions has received less attention, yet recent studies on the ordering of modifiers (Shaw and Hatzivassiloglou, 1999; Malouf, 2000; Mitchell, 2009) also work from the assumption that some orderings (large red) are preferred over others (red large). We argue that such preferences are less stable when referring expressions are generated in interactive settings, as would be required for applications such as spoken dialogue systems or interactive virtual characters. In these cases, we hypothesize that, besides domain preferences, also the referring expressions that were produced earlier in the interaction are important. It has been shown that if one dialogue participant refers to a couch as a sofa, the next speaker is more l"
P11-2116,P08-2050,0,0.177076,"ining sets may strongly influence the attribute selection performance. In the furniture domain, we found clear differences between the results of specific training sets, with “bad sets” pulling the overall performance down. This affected Accuracy but not Dice, possibly because the latter is a less strict metric. Whether the encouraging results found for the graph-based algorithm generalize to other REG approaches is still an open question. We also need to investigate how the use of small training sets affects effectiveness and efficiency of target identification by human subjects; as shown by Belz and Gatt (2008), task-performance measures do not necessarily correlate with similarity measures such as Dice. Finally, it will be interesting to repeat Experiment II with Dutch data. The D-TUNA data are cleaner than the TUNA data (Theune et al., 2010), so the risk of “bad” training data will be smaller, which may lead to more consistent results across training sets. 5 Conclusion Our experiment has shown that with 20 or less training instances, acceptable attribute selection results can be achieved; that is, results that do not significantly differ from those obtained using the entire training set. This is g"
P11-2116,W08-2120,0,0.0908871,"Missing"
P11-2116,W07-2307,0,0.438239,"Missing"
P11-2116,W08-1131,0,0.391783,"30 performed equally well. In the people domain we also found a main effect of set size (Dice: F(5,185) = 21.359, p < .001; Accuracy: F(5,185) = 8.074, p < .001). Post hoc pairwise comparisons showed that the scores of set size 20 (Dice: p = .416; Accuracy: p = .146) and set size 30 (Dice: p = .238; Accuracy: p = .324) did not significantly differ from those of the full set of 136 items. 3 For comparison: in the REG Challenge 2008, (which involved a different test set, but the same type of data), the best systems obtained overall Dice and accuracy scores of around 0.80 and 0.55 respectively (Gatt et al., 2008). These scores may well represent the performance ceiling for speaker and context independent algorithms on this task. 663 4 Discussion Experiment II has shown that when using small data sets to train an attribute selection algorithm, results can be achieved that are not significantly different from those obtained using a much larger training set. Domain complexity appears to be a factor in how much training data is needed: using Dice as an evaluation metric, training sets of 10 sufficed in the simple furniture domain, while in the more complex people domain it took a set size of 20 to achieve"
P11-2116,W09-0629,0,0.664507,"ms of costs, with cheaper properties being more preferred. Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the targe"
P11-2116,2007.mtsummit-ucnlg.17,0,0.176833,"quently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers,"
P11-2116,koolen-krahmer-2010-tuna,1,0.840722,"e discouraged from mentioning the location of the target in the visual scene, whereas in the +LOC condition they could mention any properties they wanted. The TUNA corpus was used for comparative evaluation in the REG Challenges (2007-2009). For training in our current experiment, we used the -LOC data from the training set of the REG Challenge 2009 (Gatt et al., 2009): 165 furniture descriptions and 136 people descriptions. For testing, we used the -LOC data from the TUNA 2009 development set: 38 furniture descriptions and 38 people descriptions. Dutch data were taken from the D-TUNA corpus (Koolen and Krahmer, 2010). This corpus uses the same visual scenes and annotation scheme as the TUNA corpus, but with Dutch instead of English descriptions. D-TUNA does not include locations as object properties at all, hence our restriction to -LOC data for English (to make the Dutch and English data more comparable). As Dutch test data, we used 40 furniture items and 40 people items, randomly selected from the textual descriptions in the D-TUNA corpus. The remaining furniture and people descriptions (160 items each) were used for training. 2.2 Method We first determined the frequency with which each property was men"
P11-2116,J03-1003,1,0.700846,"Missing"
P11-2116,W08-1138,1,0.874861,"om the other objects in the scene (the distractors). Crucially, Dale and Reiter do not specify how the ranking of attributes should be determined. They refer to psycholinguistic research Many other REG algorithms similarly rely on preferences. The graph-based based REG algorithm (Krahmer et al., 2003), for example, models preferences in terms of costs, with cheaper properties being more preferred. Various ways to compute costs are possible; they can be defined, for instance, in terms of log probabilities, which makes frequently encountered properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is"
P11-2116,C08-2029,0,0.114712,"red properties cheap, and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660–664, c Portl"
P11-2116,W10-4221,1,0.53568,"ys included, provided they help distinguish the target. This may lead to overspecified descriptions, mimicking the human tendency to mention redundant properties (Dale and Reiter, 1995). We ran the clustering algorithm on our English and Dutch training data for up to six clusters (k = 2 to k = 6). Then we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between attribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theune et al. (2010) on our test data. These “Free-Na¨ıve” (FN) functions were created using the manual approach sketched in the introduction. The order in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that “free” distinguishing properties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., always examining more frequent properties first.1 2.3 Results For the cluster-based cost functions, the best performance was achieved with k = 2, for both domains and both languages. Interestingly, this is the coarsest"
P11-2116,W06-1420,0,0.456502,"Missing"
P11-2116,U10-1013,0,0.246238,"and infrequent ones more expensive. Krahmer et al. (2008) argue that a less fine-grained cost function might generalize better, and propose to use frequency information to, somewhat ad hoc, define three costs: 0 (free), 1 (cheap) and 2 (expensive). This approach was shown to work well: the graph-based algorithm was the best performing system in the most recent REG Challenge (Gatt et al., 2009). Many other attribute selection algorithms also rely on training data to determine preferences in one form or another (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). Unfortunately, suitable data is hard to come by. It has been argued that determining which properties to include in a referring expression requires a “semantically transparent” corpus (van Deemter et al., 2006): a corpus that contains the actual properties of all domain objects as well as the properties that were selected for inclusion in a given reference to the target. Obviously, text corpora tend not to meet this requirement, which is why 660 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 660–664, c Portland, Oregon, June 19-24,"
P11-2116,viethen-etal-2008-controlling,1,0.495691,"n we evaluated the performance of the resulting cost functions on the test data from the same language, using Dice (overlap between attribute sets) and Accuracy (perfect match between sets) as evaluation metrics. For comparison, we also evaluated the best scoring cost functions from Theune et al. (2010) on our test data. These “Free-Na¨ıve” (FN) functions were created using the manual approach sketched in the introduction. The order in which the graph-based algorithm tries to add attributes to a description is explicitly controlled to ensure that “free” distinguishing properties are included (Viethen et al., 2008). In our tests, we used an order of decreasing frequency; i.e., always examining more frequent properties first.1 2.3 Results For the cluster-based cost functions, the best performance was achieved with k = 2, for both domains and both languages. Interestingly, this is the coarsest possible k-means function: with only two costs (0 and 1) it is even less fine-grained than the FN functions advocated by Krahmer et al. (2008). The results for the k-means costs with k = 2 and the FN costs of Theune et al. (2010) are shown in Table 1. No significant differences were found, which suggests that k-mean"
P11-2116,W08-1134,0,\N,Missing
P12-1107,P05-1074,0,0.065489,"Missing"
P12-1107,D08-1021,0,0.0128255,"Missing"
P12-1107,E99-1042,0,0.91382,"put. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. 1 Introduction Sentence simplification can be defined as the process of producing a simplified version of a sentence by changing some of the lexical material and grammatical structure of that sentence, while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be underst"
P12-1107,C96-2183,0,0.415994,", while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a contr"
P12-1107,H05-1098,0,0.0447449,"Missing"
P12-1107,W11-1601,0,0.584213,"l., 2000; Vickrey and Koller, 2008). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al., 2003). Zhu et al. (2010) examine the use of paired documents in English Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster"
P12-1107,W96-0102,0,0.0146636,"ng to a language model. For each noun, adjective and verb in the sentence this model takes that word and its part-of-speech tag and retrieves from WordNet all synonyms from all synsets the word occurs in. The word is then replaced by all of its synset words, and each replacement is scored by a SRILM language model (Stolcke, 2002) with probabilities that are obtained from training on the Simple Wikipedia data. The alternative that has the highest probability according to the language model is kept. If no relevant alternative is found, the word is left unchanged. We use the Memory-Based Tagger (Daelemans et al., 1996) trained on the Brown corpus to compute the part-ofspeech tags. The WordNet::QueryData2 Perl mod2 2.2 2.3 RevILP Woodsend and Lapata’s (2011) model is based on quasi-synchronous grammar (Smith and Eisner, 2006). Quasi-synchronous grammar generates a loose alignment between parse trees. It operates on individual sentences annotated with syntactic information in the form of phrase structure trees. Quasisynchronous grammar is used to generate all possible rewrite operations, after which integer linear programming is employed to select the most appropriate simplification. Their model is trained on"
P12-1107,D08-1019,0,0.0239748,"standing. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a controlled language with a basic vocabulary consisting of 850 words. According to Ogden, 90 percent of a"
P12-1107,W03-1602,0,0.817529,", while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. 1 Introduction Sentence simplification can be defined as the process of producing a simplified version of a sentence by changing some of the lexical material and grammatical structure of that sentence, while still preserving the semantic content of the original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as th"
P12-1107,W09-0424,0,0.0281928,"Missing"
P12-1107,W07-0716,0,0.0153875,"Missing"
P12-1107,E06-1021,0,0.00949,"ired by syntax-based SMT (Yamada and Knight, 2001) and consists of a language model, a translation model and a decoder. The four mentioned simplification operations together form the translation model. Their model is trained on a corpus containing aligned sentences from English Wikipedia and English Simple Wikipedia called PWKP. The PWKP dataset consists of 108,016 pairs of aligned lines from 65,133 Wikipedia and Simple Wikipedia articles. These articles were paired by following the “interlanguage link”3 . TF*IDF at the sentence level was used to align the sentences in the different articles (Nelken and Shieber, 2006). Zhu et al. (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. Although their system outperforms several baselines at the level of these readability metrics, they do not achieve better when evaluated with BLEU or NIST. In this work we aim to investigate the use of phrasebased machine translation modified with a dissimilarity component for the task of sentence simplification. While Zhu et al. (2010) have demonstrated that their"
P12-1107,J03-1002,0,0.00859772,"R We use the Moses software to train a PBMT model (Koehn et al., 2007). The data we use is the PWKP dataset created by Zhu et al. (2010). In general, a statistical machine translation model finds a best translation e˜ of a text in language f to a text in language e by combining a translation model that 1018 finds the most likely translation p(f |e) with a language model that outputs the most likely sentence p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) to build the sentence simplification model. GIZA++ utilizes IBM Models 1 to 5 and an HMM word alignment model to find statistically motivated alignments between words. We first tokenize and lowercase all data and use all unique sentences from the Simple Wikipedia part of the PWKP training set to train an n-gram language model with the SRILM toolkit to learn the probabilities of different n-grams. Then we invoke the GIZA++ aligner using the training simplification pairs. We run GIZA++ with standard settings and we perform no optimization. This results in a phrase table containing phrase pairs"
P12-1107,W04-3219,0,0.0390606,"Missing"
P12-1107,W06-3104,0,0.010711,"ord is then replaced by all of its synset words, and each replacement is scored by a SRILM language model (Stolcke, 2002) with probabilities that are obtained from training on the Simple Wikipedia data. The alternative that has the highest probability according to the language model is kept. If no relevant alternative is found, the word is left unchanged. We use the Memory-Based Tagger (Daelemans et al., 1996) trained on the Brown corpus to compute the part-ofspeech tags. The WordNet::QueryData2 Perl mod2 2.2 2.3 RevILP Woodsend and Lapata’s (2011) model is based on quasi-synchronous grammar (Smith and Eisner, 2006). Quasi-synchronous grammar generates a loose alignment between parse trees. It operates on individual sentences annotated with syntactic information in the form of phrase structure trees. Quasisynchronous grammar is used to generate all possible rewrite operations, after which integer linear programming is employed to select the most appropriate simplification. Their model is trained on two different datasets: one containing alignments between Wikipedia and English Simple Wikipedia (AlignILP), and one containing alignments between edits in the revision history of Simple Wikipedia (RevILP). Re"
P12-1107,P08-1040,0,0.315897,"original sentence, in order to ease its understanding. Particularly language learners (Siddharthan, 2002), people with reading disabilities (Inui et al., 2003) such as aphasia (Carroll et al., 1999), and low-literacy readers (Watanabe et al., 2009) can benefit from this application. It can serve to generate output in a specific limited format, such as subtitles (Daelemans et al., 2004). Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al., 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). The goal of simplification is to achieve an improvement in readability, defined as the ease with which a text can be understood. Some of the factors that are known to help increase the readability of text are the vocabulary used, the length of the sentences, the syntactic structures present in the text, and the usage of discourse markers. One effort to create a simple version of English at the vocabulary level has been the creation of Basic English by Charles Kay Ogden. Basic English is a controlled language with a basic vocabulary consisting o"
P12-1107,D11-1038,0,0.801355,"ple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach of Zhu et al. (2010) in the s"
P12-1107,W10-4223,1,0.848009,"Missing"
P12-1107,P01-1067,0,0.0213427,"scores from the translation model, and the target language model. In principle, all of this should be transportable to a data-driven machine translation account of sentence simplification, provided that a parallel corpus is available that pairs text to simplified versions of that text. ule is used to query WordNet (Fellbaum, 1998). 1.2 This study Zhu et al. (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. Their model is inspired by syntax-based SMT (Yamada and Knight, 2001) and consists of a language model, a translation model and a decoder. The four mentioned simplification operations together form the translation model. Their model is trained on a corpus containing aligned sentences from English Wikipedia and English Simple Wikipedia called PWKP. The PWKP dataset consists of 108,016 pairs of aligned lines from 65,133 Wikipedia and Simple Wikipedia articles. These articles were paired by following the “interlanguage link”3 . TF*IDF at the sentence level was used to align the sentences in the different articles (Nelken and Shieber, 2006). Zhu et al. (2010) evalu"
P12-1107,N10-1056,0,0.0432683,"glish Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al., 2010). Woodsend and Lapata (2011) investigate the use of Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar. They select the most appropriate simplification by using integer linear programming. We follow Zhu et al. (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. We differ from the approach o"
P12-1107,P09-1094,0,0.0435845,"Missing"
P12-1107,C10-1152,0,0.650897,"d to reduce overall sentence length, splits long sentences on the basis of syntactic 1015 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1015–1024, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics information (Chandrasekar and Srinivas, 1997; Carroll et al., 1998; Canning et al., 2000; Vickrey and Koller, 2008). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al., 2003). Zhu et al. (2010) examine the use of paired documents in English Wikipedia and Simple Wikipedia for a data-driven approach to the sentence simplification task. They propose a probabilistic, syntaxbased machine translation approach to the problem and compare against a baseline of no simplification and a phrase-based machine translation approach. In a similar vein, Coster and Kauchak (2011) use a parallel corpus of paired documents from Simple Wikipedia and Wikipedia to train a phrase-based machine translation model coupled with a deletion model. Another useful resource is the edit history of Simple Wikipedia, f"
P12-1107,P07-2045,0,\N,Missing
P12-1107,daelemans-etal-2004-automatic,0,\N,Missing
P16-1054,N16-1048,1,0.93962,", 2000; Callaway and Lester, 2002; Krahmer and Theune, 2002; Gupta and Bandopadhyay, 2009; Greenbacker and McCoy, 2009). However, all of these are fully deterministic, always choosing the same referential form in the same context. The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the original author) does not help either. When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well (Yeh and Mellish, 1997; Ferreira et al., 2016). Generally, we just don’t know. To counter this problem, a recent corpus, called VaREG, was developed in which 20 different writers were asked to produce references for a particular topic in a variety of texts, giving rise to a distribution over forms per reference (Ferreira et al., 2016). This gives us the possibility to distinguish situations where there is more or less agreement between writers in their choices of referential form. But it also enables a new paradigm for choosing referential forms, In this study, we introduce a nondeterministic method for referring expression generation. We"
P16-1054,W09-2820,0,0.401639,"Missing"
P16-1054,C00-1045,0,0.807393,"this human non-determinacy and show that this enables us to generate varied references in texts, which, in terms of coherence and comprehensibility, did not yield significant differences from human-produced references according to human judges. In particular, in this study we focus on the choice of referential form, which is the first decision to be made by referring expression generation models (Reiter and Dale, 2000) and which determines whether a reference takes the form of a proper name, a pronoun, a definite description, etc. Several such models have been proposed (Reiter and Dale, 2000; Henschel et al., 2000; Callaway and Lester, 2002; Krahmer and Theune, 2002; Gupta and Bandopadhyay, 2009; Greenbacker and McCoy, 2009). However, all of these are fully deterministic, always choosing the same referential form in the same context. The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the original author) does not help either. When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well (Yeh and Mellish, 1997; Ferrei"
P16-1054,P02-1012,0,0.900394,"acy and show that this enables us to generate varied references in texts, which, in terms of coherence and comprehensibility, did not yield significant differences from human-produced references according to human judges. In particular, in this study we focus on the choice of referential form, which is the first decision to be made by referring expression generation models (Reiter and Dale, 2000) and which determines whether a reference takes the form of a proper name, a pronoun, a definite description, etc. Several such models have been proposed (Reiter and Dale, 2000; Henschel et al., 2000; Callaway and Lester, 2002; Krahmer and Theune, 2002; Gupta and Bandopadhyay, 2009; Greenbacker and McCoy, 2009). However, all of these are fully deterministic, always choosing the same referential form in the same context. The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the original author) does not help either. When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well (Yeh and Mellish, 1997; Ferreira et al., 2016). Generally"
P16-1054,J97-1007,0,0.231137,", 2000; Henschel et al., 2000; Callaway and Lester, 2002; Krahmer and Theune, 2002; Gupta and Bandopadhyay, 2009; Greenbacker and McCoy, 2009). However, all of these are fully deterministic, always choosing the same referential form in the same context. The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the original author) does not help either. When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well (Yeh and Mellish, 1997; Ferreira et al., 2016). Generally, we just don’t know. To counter this problem, a recent corpus, called VaREG, was developed in which 20 different writers were asked to produce references for a particular topic in a variety of texts, giving rise to a distribution over forms per reference (Ferreira et al., 2016). This gives us the possibility to distinguish situations where there is more or less agreement between writers in their choices of referential form. But it also enables a new paradigm for choosing referential forms, In this study, we introduce a nondeterministic method for referring e"
P18-1182,P02-1012,0,0.106859,"”). In addition, the REG model must account for the different ways in which a particular referential form can be realized. For example, both “Frida” and 1 https://github.com/ThiagoCF05/ NeuralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any featur"
P18-1182,W17-3501,1,0.911895,"uralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in modern “end-to-end” NLG approaches, which perform"
P18-1182,P16-1054,1,0.752112,"reference at a given point in the text should assume the form of, for example, a proper name (“Frida Kahlo”), a pronoun (“she”) or description (“the Mexican painter”). In addition, the REG model must account for the different ways in which a particular referential form can be realized. For example, both “Frida” and 1 https://github.com/ThiagoCF05/ NeuralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our appr"
P18-1182,E17-1062,1,0.912131,"uralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in modern “end-to-end” NLG approaches, which perform"
P18-1182,P11-2031,0,0.038125,"s in the corpus using accuracy and BLEU score (Papineni et al., 2002) as a measure of fluency. Since our model does not handle referring expressions for constants (dates and numbers), we just copied their source version into the template. Post-hoc McNemar’s and Wilcoxon signed ranked tests adjusted by the Bonferroni method were used to test the statistical significance of the models in terms of accuracy and string edit distance, respectively. To test the statistical significance of the BLEU scores of the models, we used a bootstrap resampling together with an approximate randomization method (Clark et al., 2011)2 . Settings NeuralREG was implemented using Dynet (Neubig et al., 2017). Source and target word embeddings were 300D each and trained jointly with the model, whereas hidden units were 512D for each direction, totaling 1024D in the bidirection layers. All non-recurrent matrices were initialized following the method of Glorot and Bengio (2010). Models were trained using stochastic gradient descent with Adadelta (Zeiler, 2012) and mini-batches of size 40. We ran each model for 60 epochs, applying early stopping for model selection based on accuracy on the development set with patience of 20 epoc"
P18-1182,E91-1028,0,0.722496,"erent ways in which a particular referential form can be realized. For example, both “Frida” and 1 https://github.com/ThiagoCF05/ NeuralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in tradition"
P18-1182,W16-6605,0,0.168386,"Missing"
P18-1182,P17-1017,0,0.478131,"ach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in modern “end-to-end” NLG approaches, which perform the task in a more integrated manner (see e.g. Konstas et al., 2017; Gardent et al., 2017b). Some of these approaches have recently focused on inputs which references to entities are delexicalized to general tags (e.g., ENTITY-1, ENTITY-2) in order to decrease data sparsity. Based on the delexicalized input, the model generates outputs which may be likened to templates in which references to the discourse entities are not realized (as in “The ground of ENTITY-1 is located in ENTITY-2.”). While our approach, dubbed as NeuralREG, is compatible with different applications of REG models, in this paper, we concentrate on the last one, relying on a specifically constructed set of 1959 P"
P18-1182,W17-3518,0,0.399772,"ach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in modern “end-to-end” NLG approaches, which perform the task in a more integrated manner (see e.g. Konstas et al., 2017; Gardent et al., 2017b). Some of these approaches have recently focused on inputs which references to entities are delexicalized to general tags (e.g., ENTITY-1, ENTITY-2) in order to decrease data sparsity. Based on the delexicalized input, the model generates outputs which may be likened to templates in which references to the discourse entities are not realized (as in “The ground of ENTITY-1 is located in ENTITY-2.”). While our approach, dubbed as NeuralREG, is compatible with different applications of REG models, in this paper, we concentrate on the last one, relying on a specifically constructed set of 1959 P"
P18-1182,J95-2003,0,0.833517,"Missing"
P18-1182,C00-1045,0,0.807531,"n (“the Mexican painter”). In addition, the REG model must account for the different ways in which a particular referential form can be realized. For example, both “Frida” and 1 https://github.com/ThiagoCF05/ NeuralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, w"
P18-1182,P17-1014,0,0.164222,"G: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in modern “end-to-end” NLG approaches, which perform the task in a more integrated manner (see e.g. Konstas et al., 2017; Gardent et al., 2017b). Some of these approaches have recently focused on inputs which references to entities are delexicalized to general tags (e.g., ENTITY-1, ENTITY-2) in order to decrease data sparsity. Based on the delexicalized input, the model generates outputs which may be likened to templates in which references to the discourse entities are not realized (as in “The ground of ENTITY-1 is located in ENTITY-2.”). While our approach, dubbed as NeuralREG, is compatible with different applications of REG models, in this paper, we concentrate on the last one, relying on a specifically con"
P18-1182,J12-1006,1,0.927348,"Missing"
P18-1182,D16-1128,0,0.0641195,"Missing"
P18-1182,P17-2031,0,0.02867,"Missing"
P18-1182,P14-5010,0,0.00443809,"Missing"
P18-1182,D14-1074,0,0.0262389,"erring expressions to 1,501 entities in the context of the semantic web, derived from a (delexicalized) version of the WebNLG corpus (Gardent et al., 2017a,b). Both this data set and the model will be made publicly available. We compare NeuralREG against two baselines in an automatic and human evaluation, showing that the integrated neural model is a marked improvement. 2 Related work In recent years, we have seen a surge of interest in using (deep) neural networks for a wide range of NLG-related tasks, as the generation of (first sentences of) Wikipedia entries (Lebret et al., 2016), poetry (Zhang and Lapata, 2014), and texts from abstract meaning representations (e.g., Konstas et al., 2017; Castro Ferreira et al., 2017a). However, the usage of deep neural networks for REG has remained limited and we are not aware of any other integrated, end-to-end model for generating referring expressions in discourse. There is, however, a lot of earlier work on selecting the form and content of referring expressions, both in psycholinguistics and in computational linguistics. In psycholinguistic models of reference, various linguistic factors have been proposed as influencing the form of referential expressions, inc"
P18-1182,P15-1158,0,0.128323,"Missing"
P18-1182,P02-1040,0,0.101947,"evaluated models with the goldstandards ones using accuracy and String Edit Distance (Levenshtein, 1966). Since pronouns are highlighted as the most likely referential form to be used when a referent is salient in the discourse, as argued in the introduction, we also computed pronoun accuracy, precision, recall and F1-score in order to evaluate the performance of the models for capturing discourse salience. Finally, we lexicalized the original templates with the referring expressions produced by the models and compared them with the original texts in the corpus using accuracy and BLEU score (Papineni et al., 2002) as a measure of fluency. Since our model does not handle referring expressions for constants (dates and numbers), we just copied their source version into the template. Post-hoc McNemar’s and Wilcoxon signed ranked tests adjusted by the Bonferroni method were used to test the statistical significance of the models in terms of accuracy and string edit distance, respectively. To test the statistical significance of the BLEU scores of the models, we used a bootstrap resampling together with an approximate randomization method (Clark et al., 2011)2 . Settings NeuralREG was implemented using Dynet"
P18-1182,J11-4007,0,0.641319,"both “Frida” and 1 https://github.com/ThiagoCF05/ NeuralREG “Kahlo” are name-variants that may occur in a text, and she can alternatively also be described as, say, “the famous female painter”. Most of the earlier REG approaches focus either on selecting referential form (Orita et al., 2015; Castro Ferreira et al., 2016), or on selecting referential content, typically zooming in on one specific kind of reference such as a pronoun (e.g., Henschel et al., 2000; Callaway and Lester, 2002), definite description (e.g., Dale and Haddock, 1991; Dale and Reiter, 1995) or proper name generation (e.g., Siddharthan et al., 2011; van Deemter, 2016; Castro Ferreira et al., 2017b). Instead, in this paper, we propose NeuralREG: an end-to-end approach addressing the full REG task, which given a number of entities in a text, produces corresponding referring expressions, simultaneously selecting both form and content. Our approach is based on neural networks which generate referring expressions to discourse entities relying on the surrounding linguistic context, without the use of any feature extraction technique. Besides its use in traditional pipeline NLG systems (Reiter and Dale, 2000), REG has also become relevant in m"
P18-1182,W16-2346,0,0.0254897,"Missing"
R19-1070,S17-2051,0,0.128753,"this work and of most other studies in this field, consists of reranking the most likely candidate questions with a more fine-grained, domain-specific approach. Optionally, the system could also return whether a candidate question is a duplicate of the query. The reranking task has been included as a benchmark task (Task 3 - Subtask B) in SemEval2016/2017 (Nakov et al., 2016, 2017). Using the domain of Qatar Living4 , it consisted of re-ranking ten candidate questions retrieved by Google for a target question. Several promising approaches were proposed for this challenge, most notably SimBOW (Charlet and Damnati, 2017) based on the SoftCosine metric and winner of SemEval-2017, and KeLP (Filice et al., 2016), which is based on Tree Kernels and provided top results for all the subtasks in the challenge. However, little is known about the effects of particular design choices for these models, especially concerning the preprocessing methods and wordsimilarity metrics. Moreover, we know little about Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers h"
R19-1070,D11-1096,0,0.0328394,"sk by many studies. We used the implementation of BM25 provided by gensim5 as a baseline. Translation-Based Language Model (TRLM) is a question similarity ranking function, first introduced by Xue et al. (2008). The method combines a language model with a word translation system technique, and is known to obtain better results on the question similarity task than BM25 and only the language model (Jeon et al., 2005). Equation 1 summarizes the TRLM ranking score between questions Q1 and Q2 : Smoothed Partial Tree Kernels (SPTK) are the basis of KeLP (Filice et al., 2016), a system introduced by Croce et al. (2011). SPTK applies the kernel trick by computing the similarity of question pairs based on the number of common substructures their parse trees share. The difference with Partial Tree Kernels (PTK) (Moschitti, 2006) is that SPTK also considers word relations. Besides the different variations of the model, which are well explained in Moschitti (2006) and 5 https://radimrehurek.com/gensim/ summarization/bm25.html 594 Filice et al. (2016), we designed SPTK in the following form. Equation 3 portrays the notation of the similarity metric among two questions’ constituency trees, i.e. TQ1 and TQ2 . X X n"
R19-1070,S16-1172,0,0.10683,"te questions with a more fine-grained, domain-specific approach. Optionally, the system could also return whether a candidate question is a duplicate of the query. The reranking task has been included as a benchmark task (Task 3 - Subtask B) in SemEval2016/2017 (Nakov et al., 2016, 2017). Using the domain of Qatar Living4 , it consisted of re-ranking ten candidate questions retrieved by Google for a target question. Several promising approaches were proposed for this challenge, most notably SimBOW (Charlet and Damnati, 2017) based on the SoftCosine metric and winner of SemEval-2017, and KeLP (Filice et al., 2016), which is based on Tree Kernels and provided top results for all the subtasks in the challenge. However, little is known about the effects of particular design choices for these models, especially concerning the preprocessing methods and wordsimilarity metrics. Moreover, we know little about Community Question Answering forums are popular among Internet users, and a basic problem they encounter is trying to find out if their question has already been posed before. To address this issue, NLP researchers have developed methods to automatically detect question-similarity, which was one of the sh"
R19-1070,S16-1126,0,0.0310224,"ates than EnsSPTK. The results are different in the test set of Semeval 2017: the latter approach is slightly better than the former on re-ranking similar questions according to the MAP metric, but shows a nonsignificant difference in classifying duplicates according to the F-1 score metric. Although the results between our two best approaches are inconclusive, we argue that the inclusion of SPTK in the ensemble is not beneficial due to the trade-off between efficiency and performance. The SPTK approach, mainly its kernel, In SemEval-2016, the UH-PRHLT model was the winner of the shared-task (Franco-Salvador et al., 2016). This system is based on a range of lexical (cosine similarity, word, noun and ngram overlap) and semantic (word representations, alignments, knowledge graphs and common frames) features. In turn, our best model, Ensemble, with considerably less features, obtains competitive results in terms of MAP. The same pattern is seen for the SemEval-2017 test set: the Ensemble approach obtained competitive results with the winner SimBOW, also based on the SoftCosine metric, in terms of MAP, and outperforms it in FScore. 597 Quora results Based on the previous results, we also evaluated the performance"
R19-1070,N13-1090,0,0.0558413,"n X Xi Mij Yj X tM X = Sof tCos(X, Y ) = √ We compare two traditional and two recent approaches in this study: BM25, TranslationBased Language Model (TRLM), SoftCosine and Smoothed Partial Tree Kernels (SPTK - Syntactic Tree Kernels). (2) i=1 j=1 Mij = max(0, cosine(Vi , Vj ))2 As Sim(w, t) in Equation 1, Mij represents the similarity between the i-th word of question Q1 and the j-th one in question Q2 . cosine is the cosine similarity, and Vi and Vj are originally 300-dimension embedding representations of the words, trained on the unannotated part of the Qatar living corpus using Word2Vec (Mikolov et al., 2013) with a context window size of 10. BM25 is a fast information retrieval technique (Robertson et al., 2009) used as a search engine in the first step of the shared task by many studies. We used the implementation of BM25 provided by gensim5 as a baseline. Translation-Based Language Model (TRLM) is a question similarity ranking function, first introduced by Xue et al. (2008). The method combines a language model with a word translation system technique, and is known to obtain better results on the question similarity task than BM25 and only the language model (Jeon et al., 2005). Equation 1 summ"
R19-1070,P04-3031,0,0.181064,"cased data. The table also shows the results of the best baseline (e.g., Google) and the winners of the SemEval 20162017 challenges. As expected, our best models were the ensemble approaches (e.g., Ensemble and EnsSPTK), which combine the ranking scores of all the other evaluated approaches and outperform Experiment 2: Word-Similarity A central component of all of the evaluated models except BM25 is the use of a word-similarity metric. To evaluate which distribution better captures the similarity between two words for the 8 We used the list of English stopwords provided by the NLTK framework (Bird and Loper, 2004) 596 Preproc. L.S.P. L.S. L.P. S.P. L. S. P. Metric Translation Word2Vec fastText Word2Vec+ELMo fastText+ELMo BM25 68.80 67.31 69.95 66.03 67.07 63.77 65.05 63.52 BM25 - TRLM 68.43 63.25 68.42 68.65 66.42 64.53 64.38 64.95 TRLM 68.43 72.90 70.93 71.41 70.56 SoftCosine 72.75 69.15 65.33 68.56 63.68 67.01 60.04 60.66 SoftCosine 70.75 72.75 71.07 73.89 73.43 SPTK 54.34 54.44 SPTK 48.10 54.44 53.49 54.78 54.77 Ensemble 71.62 69.50 68.70 68.67 67.04 67.85 65.31 63.08 Ensemble 70.80 71.40 71.92 73.90 73.73 EnsSPTK 72.40 71.29 69.16 70.37 67.41 68.36 66.66 64.31 EnsSPTK 70.80 72.64 71.92 74.63 73.73"
R19-1070,S17-2003,0,0.0629111,"Missing"
R19-1070,Q17-1010,0,0.0391958,"s, hyperparameters of the models such as σ and α of TRLM and γ of Ensemble with SPTK were optimized in the development split of the data through Grid Search. Moreover, Support Vector Machines in SPTK and Logistic Regression in Ensemble were implemented based on the Scikit-Learn toolkit (Pedregosa et al., 2011) and had their hyperparameters tuned by crossvalidation on the training set. 3.3 task, we evaluated all the models using the wordtranslation probabilities, plus the cosine similarity measure depicted in Equation 2. In the latter, besides Word2Vec representations, we also tested fastText (Bojanowski et al., 2017), a distribution which takes character-level information and tends to overcome spelling variations, and the top layer of ELMo (Peters et al., 2018). To equalize the trials, the data used by the models were lowercased and stripped of stop words and punctuation. 4 Evaluation The first section of Table 1 lists the MAP of the preprocessing methods in the development part of the corpus for each model. Although the best combination of preprocessing methods differs between models, we see that preprocessing the data is beneficial for the performance of all models, except for SPTK. Between the best res"
R19-1070,H93-1039,0,0.0544812,"ent datasets. Results show that the choice of a preprocessing method and a word-similarity metric have a considerable impact on the final results. We also show that the combination of all the analyzed approaches leads to results competitive with related work in question-similarity. 2 T RLM (Q1 , Q2 ) = Y (1 − σ)Ptr (w|Q2 ) + σPlm (w|C) w∈Q1 X Ptr (w|Q2 ) = α Sim(w, t)Plm (t|Q2 )+ t∈Q2 (1 − α)Plm (w|Q2 ) (1) Sim(w, t) denotes a similarity score among words w and t. In the original study, this similarity metric is the word-translation probability P (w|t) obtained by the IBM Translation Model 1 (Brown et al., 1993). Furthermore, C denotes a background corpus to compute unigram probabilities in order to avoid 0 scores. SoftCosine is the ranking function used by SimBOW (Charlet and Damnati, 2017), the winning system of the question similarity re-ranking task of SemEval 2017 (Nakov et al., 2017). The method is similar to a cosine similarity between the tf-idf bag-of-words of the pair of questions, except that it also takes into account word-level similarities as a matrix M . Given X and Y as the respective tfidf bag-of-words for questions Q1 and Q2 , Equation 2 summarizes the SoftCosine metric. Models X tM"
R19-1070,N18-1202,0,0.0135032,"earch. Moreover, Support Vector Machines in SPTK and Logistic Regression in Ensemble were implemented based on the Scikit-Learn toolkit (Pedregosa et al., 2011) and had their hyperparameters tuned by crossvalidation on the training set. 3.3 task, we evaluated all the models using the wordtranslation probabilities, plus the cosine similarity measure depicted in Equation 2. In the latter, besides Word2Vec representations, we also tested fastText (Bojanowski et al., 2017), a distribution which takes character-level information and tends to overcome spelling variations, and the top layer of ELMo (Peters et al., 2018). To equalize the trials, the data used by the models were lowercased and stripped of stop words and punctuation. 4 Evaluation The first section of Table 1 lists the MAP of the preprocessing methods in the development part of the corpus for each model. Although the best combination of preprocessing methods differs between models, we see that preprocessing the data is beneficial for the performance of all models, except for SPTK. Between the best results, we see that suppression of punctuation is beneficial for all the models, while the removal of stopwords and lowercasing are detrimental to BM"
swerts-krahmer-2000-use,P98-1122,0,\N,Missing
swerts-krahmer-2000-use,C98-1117,0,\N,Missing
swerts-krahmer-2000-use,P97-1035,0,\N,Missing
van-der-sluis-krahmer-2004-evaluating,J03-1003,1,\N,Missing
van-der-sluis-krahmer-2004-evaluating,J95-1003,0,\N,Missing
van-der-sluis-krahmer-2004-evaluating,W03-2307,1,\N,Missing
viethen-etal-2008-controlling,W06-1410,1,\N,Missing
viethen-etal-2008-controlling,W07-2307,0,\N,Missing
viethen-etal-2008-controlling,W07-2318,1,\N,Missing
viethen-etal-2008-controlling,J03-1003,1,\N,Missing
viethen-etal-2008-controlling,2007.mtsummit-ucnlg.14,0,\N,Missing
W01-0805,E91-1028,0,0.651721,"d describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale & Haddock 1991, Dale & Reiter 1995, Horacek 1997, Stone & Webber 1998, Krahmer & Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using p"
W01-0805,W00-1424,0,0.126337,"Missing"
W01-0805,J95-2003,0,0.0257553,"a set of nodes from the scene graph. The algorithm should be reformulated in such a way that it tries to generate a subgraph which can refer to each of the nodes in the set, but not to any of the nodes in the scene graph outside this set. Krahmer & Theune (1999) present an extension of the Incremental Algorithm which takes context into account. They argue that an object which has been mentioned in the recent context is somehow salient, and hence can be referred to using fewer properties. This is modelled by assigning salience weights to objects (basically using a version of Centering Theory (Grosz et al. 1995) augmented with a recency effect), and by defining the set of distractors as the set of objects with a salience weight higher or equal than that of the target object. In terms of the graph-theoretical framework, one can easily imagine assigning salience weights to the nodes in the scene graph, and restricting the distractor set essentially as Krahmer & Theune do. In this way, distinguishing graphs for salient objects will generally be smaller than those of non-salient objects. Acknowledgements Thanks are due to Alexander Koller, Kees van Deemter, Paul Piwek, Mari¨et Theune and two anonymous re"
W01-0805,P97-1027,0,0.848409,"h construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale & Haddock 1991, Dale & Reiter 1995, Horacek 1997, Stone & Webber 1998, Krahmer & Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object. Co"
W01-0805,W98-1426,0,0.0999103,"m, in the sense that by defining the cost function in different ways, we can mimic various well-known algorithms for the generation of referring expressions. A second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due to the fact that properties and relations are formalized in the same way, namely as edges in a graph. The third advantage is that the combined usage of graphs and cost-functions paves the way for a natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., Langkilde & Knight 1998, Malouf 2000) in a single algorithm. The outline of this paper is as follows. In section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 3 contains a sketch of the branch and bound algorithm, which is illustrated with a worked example. In section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions. We end with some concluding remarks in section 5. 2 Graphs Consider the following s"
W01-0805,P00-1012,0,0.166591,"efining the cost function in different ways, we can mimic various well-known algorithms for the generation of referring expressions. A second advantage of the graph-theoretical framework is that it does not run into problems with relational descriptions, due to the fact that properties and relations are formalized in the same way, namely as edges in a graph. The third advantage is that the combined usage of graphs and cost-functions paves the way for a natural integration of traditional rule-based approaches to generation with more recent statistical approaches (e.g., Langkilde & Knight 1998, Malouf 2000) in a single algorithm. The outline of this paper is as follows. In section 2, we describe how scenes can be described as labeled directed graphs and show how content selection can be formalized as a subgraph construction problem. Section 3 contains a sketch of the branch and bound algorithm, which is illustrated with a worked example. In section 4 it is argued that by defining cost functions in different ways, we can mimic various well-known algorithms for the generation of referring expressions. We end with some concluding remarks in section 5. 2 Graphs Consider the following scene:"
W01-0805,P90-1013,0,0.709072,"cted graph and describe content selection as a subgraph construction problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale & Haddock 1991, Dale & Reiter 1995, Horacek 1997, Stone & Webber 1998, Krahmer & Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of desc"
W01-0805,W98-1419,0,0.122427,"problem. Cost functions are used to guide the search process and to give preference to some solutions over others. The resulting graph algorithm can be seen as a meta-algorithm in the sense that defining cost functions in different ways allows us to mimic — and even improve— a number of wellknown algorithms. 1 Introduction The generation of referring expressions is one of the most common tasks in natural language generation, and has been addressed by many researchers in the past two decades (including Appelt 1985, Dale 1992, Reiter 1990, Dale & Haddock 1991, Dale & Reiter 1995, Horacek 1997, Stone & Webber 1998, Krahmer & Theune 1999 and van Deemter 2000). As a result, there are many different algorithms for the generation of referring expressions, each with its own objectives: some aim at producing the shortest possible description, others focus on efficiency or realistic output. The degree of detail in which the various algorithms are described differs considerably, and as a result it is often difficult to compare the various proposals. In addition, most of the algorithms are primarily concerned with the generation of descriptions only using properties of the target object. Consequently, the probl"
W03-2307,J84-2002,0,\N,Missing
W03-2307,J03-1003,1,\N,Missing
W03-2307,J02-1003,0,\N,Missing
W03-2307,J95-1003,0,\N,Missing
W03-2307,P02-1013,0,\N,Missing
W03-2710,N01-1027,0,\N,Missing
W03-2710,P95-1016,0,\N,Missing
W03-2710,P01-1012,1,\N,Missing
W03-2710,P99-1030,0,\N,Missing
W03-2710,J00-3003,0,\N,Missing
W03-2710,W02-0213,0,\N,Missing
W03-2710,P99-1026,0,\N,Missing
W03-2710,P98-2188,0,\N,Missing
W03-2710,C98-2183,0,\N,Missing
W05-1201,P03-1011,0,0.0451991,"gnising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in terms of five ba"
W05-1201,C96-1078,0,0.231278,"between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics in the sentence). Here, following (Herrera et al., 2005) and (Barzilay, 2003), we will align sentences at the level of dependency structures. In addition, we will label the alignments in"
W05-1201,2000.eamt-1.5,0,0.0467518,"(Herrera et al., 2005), (Vanderwende et al., 2005)). Our working hypothesis is that semantic overlap at the word and phrase levels may provide a good basis for deciding the semantic relation between sentences. Recognising semantic relations between sentences then becomes a two-step procedure: first, the words and phrases in the respective sentences need to be aligned, after which the relations between the pairs of aligned words and phrases should be labeled in terms of semantic relations. Various alignment algorithms have been developed for data-driven approaches to machine translation (e.g. (Och and Ney, 2000)). Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al., 1996), (Gildea, 2003). For our purposes, an additional advantage of aligning syntactic structures is that it keeps the alignment feasible (as the number of arbitrary substrings that may be aligned grows exponentially to the number of words 1 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1–6, c Ann Arbor, June 2005. 2005 Association for Computational Lingui"
W05-1612,C00-1007,0,0.0244122,"Missing"
W05-1612,P99-1071,0,0.282335,"Missing"
W05-1612,J96-2004,0,0.0244067,"Missing"
W05-1612,P03-1011,0,0.0718816,"Missing"
W05-1612,P98-1116,0,0.111848,"Missing"
W05-1612,P03-1069,0,0.0433048,"Missing"
W05-1612,C96-1078,0,0.0234688,"Missing"
W05-1612,2000.eamt-1.5,0,0.095013,"Missing"
W05-1612,N03-1024,0,0.0377122,"Missing"
W05-1612,van-der-wouden-etal-2002-syntactic,0,0.0425394,"Missing"
W05-1612,C98-1112,0,\N,Missing
W07-1414,A97-1004,0,\N,Missing
W07-1414,C96-1078,0,\N,Missing
W08-1138,2007.mtsummit-ucnlg.14,0,0.0267791,"s a graph search problem, which outputs the cheapest distinguishing graph (if one exists) given a particular cost function. For the second step, realisation, we use a simple template-based realiser written by Irene Langkilde-Geary from Brighton University that was made available to all REG 2008 participants. A version of the Graph-based algorithm was submitted for the ASGRE 2007 Challenge (Theune et al. 2007). For us, one of the most striking, general outcomes was the observed “trend for the mean DICE score obtained by a system to decrease as the proportion of minimal descriptions increases” (Belz and Gatt 2007).2 Thus, while REG systems have a tendency to produce minimal descriptions, human speakers tend to include redundant properties in their descriptions, which is in line with recent findings in psycholinguistics on the production of referring expressions (e.g., Engelhardt et al. 2006). In principle, the graph-based approach has the potential to deal with redundancy by allowing some attributes to have zero costs. Viethen et al. (2008), however, show that merely assigning zero costs to an attribute is not a sufficient condition for inclusion; if the search terminates before the free properties are"
W08-1138,J03-1003,1,0.772741,"Missing"
W08-1138,viethen-etal-2008-controlling,1,\N,Missing
W08-1138,W09-0629,0,\N,Missing
W09-0604,C04-1051,0,0.105585,"Missing"
W09-0604,W03-1608,0,0.0699754,"Missing"
W09-0604,A00-2024,0,0.110693,"Missing"
W09-0604,N03-1003,0,0.280604,"Missing"
W09-0604,W03-1101,0,0.0331291,"the observed data. We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25"
W09-0604,E06-1040,0,0.0771484,"Missing"
W09-0604,P06-1048,0,0.0449688,"Missing"
W09-0604,P05-1036,0,0.305391,"Missing"
W09-0604,W04-1015,0,0.450146,"crucial, and argue for more elaborate sentence compression models which build on NLG work. 1 Introduction The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). The compressed sentence should retain the most important information and remain grammatical. One of the applications is in automatic summarization in order to compress sentences extracted for the summary (Lin, 2003; Jing and McKeown, 2000). Other applications include automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004) and displaying text on devices with very small screens (CorstonOliver, 2001). A more restricted version defines sentence compression as dropping any subset of words from the input sentence while retaining important information and grammaticality (Knight and Proceedings of the 12th European Workshop on Natural Language Generation, pages 25–32, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 25 Atranos and Musa – on automatic subtitling (Vandeghinste and Tsjong Kim Sang, 2004; Vandeghinste and Pan, 2004; Daelemans et al., 2004). All"
W09-0604,vandeghinste-tjong-kim-sang-2004-using,0,0.0675821,"Missing"
W09-0604,daelemans-etal-2004-automatic,1,\N,Missing
W09-0604,Y03-1033,0,\N,Missing
W09-0621,C04-1051,0,0.116471,"Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based"
W09-0621,N06-1058,0,0.0926057,"Missing"
W09-0621,E06-1021,0,0.526588,"u et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion. For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the"
W09-0621,N06-4007,0,0.0198878,"ntence is viewed as a Clustering Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function V = k X X (xj − µi )2 i=1 xj ∈Si where µi is the centroid of all the points xj ∈ Si . The PK1 cluster-stopping algorithm as proposed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: P K1(k) = Cr(k) − mean(Cr[1...∆K]) std(Cr[1...∆K]) Here, Cr is a criterion function, which measures the ratio of withincluster similarity to betweencluster similarity. As soon as P K1(k) exceeds a threshold, k − 1 is selected as the optimum number of clusters. To find the optimal threshold value for clusterstopping, optimization is performed on the development data. Our optimization function is an F score: Fβ = 1 123 (1 + β 2 ) · (precision · recall) (β 2 · precision + recall) http://glaros.dtc.umn.edu/gkhome/views/cluto/ Type k-means cluste"
W09-0621,P07-1059,0,0.0744359,"Missing"
W09-0621,W03-1004,0,0.769439,"rces for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2 Method We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122–125, c Athens, Greece, 30 – 31 March"
W09-0621,W06-1610,0,0.0901955,"Missing"
W09-0621,N06-1003,0,\N,Missing
W09-0630,W07-2307,0,0.0524491,"Missing"
W09-0630,W08-1131,0,0.209828,"arie University Australia i.h.g.brugman@student.utwente.nl m.theune@utwente.nl e.j.krahmer@uvt.nl jviethen@ics.mq.edu.au Abstract to attributes that are highly frequent in the TUNA corpus, while the other attributes have a cost of either 1 (somewhat infrequent) or 2 (very infrequent). The order in which attributes are added is also controlled: to ensure that the cheapest attributes are added first, they are tried in the order of their frequency in the TUNA (2008) training corpus. Using these settings, last year the GRAPH attribute selection algorithm made the top 3 on all evaluation measures (Gatt et al. 2008, Table 11). We describe a new realiser developed for the TUNA 2009 Challenge, and present its evaluation scores on the development set, showing a clear increase in performance compared to last year’s simple realiser. 1 Introduction The TUNA Challenge 2009 is the last in a series of challenges using the TUNA corpus of referring expressions (Gatt et al. 2007) for comparative evaluation of referring expression generation. The 2009 Challenge is aimed at end-to-end referring expression generation, which encompasses two subtasks: (1) attribute selection, choosing a number of attributes that uniquel"
W09-0630,J03-1003,1,0.688799,"Missing"
W09-0630,W08-1138,1,0.84158,"ed as folAttribute selection We use the Graph-based algorithm of Krahmer et al. (2003) for attribute selection. In this approach, objects and their attributes are represented in a graph as nodes and edges respectively, and attribute selection is seen as a graph search problem that outputs the cheapest distinguishing graph, given a particular cost function that assigns costs to attributes. By assigning zero costs to some attributes, e.g., the type of an object, the human tendency to mention redundant properties can be mimicked. For the TUNA Challenge 2009 we use the same settings as last year (Krahmer et al. 2008). The used cost function assigns a zero cost 1 This corresponds to the ANNOTATED-WORD-STRING nodes already present in the TUNA corpus. Unfortunately, various problems prevented us from automatically deriving our templates from those existing annotations. Proceedings of the 12th European Workshop on Natural Language Generation, pages 183–184, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 183 lows. When a set of attributes is input to the realiser, it checks if there is a template matching this particular attribute combination. If so, the template is select"
W09-0630,W09-0629,0,\N,Missing
W09-2812,J05-3002,0,0.0716088,"Missing"
W09-2812,N03-1020,0,0.181494,"the maximal margin relevance criterion (Carbonell and Goldstein, 1998). MMR models the trade-off between a focused summary and a summary with a wide scope. The novelty-reranker is an extension of the cosine-reranker and boosts sentences occurring after an important sentence by multiplying with 1.2. The reranker tries to mimic human behavior as people tend to pick clusters of sentences when summarizing. 4 For the experiments on the development set, we compare each of the automatically produced extracts with five manually written summaries and report macro-average Rouge-2 and Rouge-SU4 scores (Lin and Hovy, 2003). For the experiments on the test set, we also perform a manual evaluation. We follow the DUC 2006 guidelines for manual evaluation of responsiveness and the linguistic quality of the produced summaries. The responsiveness scores express the information content of the summary with respect to the query. The linguistic quality is evaluated on five different objectives: grammaticality, non-redundancy, coherence, referential clarity and focus. The annotators can choose a value on a five point scale where 1 means ‘very poor’ and 5 means ‘very good’. We use two independent annotators to evaluate the"
W09-2812,radev-etal-2004-mead,0,0.100917,"Missing"
W09-2812,vossen-etal-2008-integrating,0,0.0604442,"Missing"
W10-4221,W07-2307,0,0.27877,"Missing"
W10-4221,W08-1131,0,0.60757,"LOC condition they were discouraged (but not prevented) from mentioning object locations. The resulting object descriptions were annotated using XML and combined with an XML representation of the visual scene, listing all objects and their properties in terms of attribute-value pairs. The TUNA corpus is split into two domains: one with descriptions of furniture and one with descriptions of people. The TUNA corpus was used for the comparative evaluation of REG systems in the TUNA Challenges (2007-2009). For our current experiments, we used the TUNA 2008 Challenge training and development sets (Gatt et al., 2008) to train and evaluate the graph-based algorithm on. 2.2 Dutch: the D-TUNA Corpus For Dutch, we used the D(utch)-TUNA corpus of object descriptions (Koolen and Krahmer, 2010). The collection of this corpus was inspired by the TUNA experiment described above, and was done using the same visual scenes. There were three conditions: text, speech and face-to-face. The text condition was a replication (in Dutch) of the TUNA experiment: participants typed identifying descriptions of target referents, distinguishing them from distractor objects in the scene. In the other two conditions participants pr"
W10-4221,koolen-krahmer-2010-tuna,1,0.845228,"an XML representation of the visual scene, listing all objects and their properties in terms of attribute-value pairs. The TUNA corpus is split into two domains: one with descriptions of furniture and one with descriptions of people. The TUNA corpus was used for the comparative evaluation of REG systems in the TUNA Challenges (2007-2009). For our current experiments, we used the TUNA 2008 Challenge training and development sets (Gatt et al., 2008) to train and evaluate the graph-based algorithm on. 2.2 Dutch: the D-TUNA Corpus For Dutch, we used the D(utch)-TUNA corpus of object descriptions (Koolen and Krahmer, 2010). The collection of this corpus was inspired by the TUNA experiment described above, and was done using the same visual scenes. There were three conditions: text, speech and face-to-face. The text condition was a replication (in Dutch) of the TUNA experiment: participants typed identifying descriptions of target referents, distinguishing them from distractor objects in the scene. In the other two conditions participants produced spoken descriptions for an addressee, who was either visible to the speaker (face-to-face condition) or not (speech condition). The resulting descriptions were annotat"
W10-4221,J03-1003,1,0.90135,"Missing"
W10-4221,W08-1138,1,0.935573,"in the Dutch Free-Na¨ıve costs, do not seem to be caused by differences in expressibility, i.e., the ease with which the attributes can be expressed in the two languages (Koolen et al., 2010); rather, they may be due to the fact that the human descriptions in DTUNA do not include any DIMENSION attributes. People Dice Acc. 0.78 0.28 0.73 0.29 0.75 0.25 0.67 0.24 Table 1: Evaluation results for stochastic costs. Language Training Test Dutch Dutch English English Dutch English English Costs and Order For English, we used the Stochastic and FreeNa¨ıve cost functions and the stochastic order from Krahmer et al. (2008). The Stochastic costs and order were derived from the attribute frequencies in the combined training and development sets of the TUNA 2008 Challenge (Gatt et al., 2008), containing 399 items in the furniture domain and 342 items in the people domain. The Free-Na¨ıve costs are simplified versions of the stochastic costs. “Free” attributes are TYPE in both domains, COLOUR for the furniture domain and HAS B EARD and HAS G LASSES for the people domain. Expensive attributes (cost 2) are X - and Y- DIMENSION in the furniture domain and HAS S UIT, HAS S HIRT and HAS T IE in the people domain. All ot"
W10-4221,W06-1420,0,0.166344,"Missing"
W10-4221,viethen-etal-2008-controlling,1,0.889468,"ng nodes. To select the attributes for a distinguishing description, the algorithm searches for a subgraph of the scene graph that uniquely refers to the target referent. Starting from the node representing the target, it performs a depth-first search over the edges connected to the subgraph found so far. The algorithm’s output is the cheapest distinguishing subgraph, given a particular cost function that assigns costs to attributes. By assigning zero costs to some attributes, e.g., the type of an object, the human tendency to mention redundant attributes can be mimicked. However, as shown by Viethen et al. (2008), merely assigning zero costs to an attribute is not a sufficient condition for inclusion; if the graph search terminates before the free attributes are tried, they will not be included. Therefore, the order in which attributes are tried must be explicitly controlled. Thus, when using the graph-based algorithm for attribute selection, two things must be specified: (1) the cost function, and (2) the order in which the attributes should be searched. Both can be based on corpus data, as described in the next section. 4 Costs and Orders For our experiments, we used the graph-based attribute select"
W10-4221,W09-0629,0,\N,Missing
W10-4223,W05-0909,0,0.0771031,"ollection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method cons"
W10-4223,N03-1003,0,0.0318701,"at in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Aut"
W10-4223,N06-1003,0,0.0317805,"hine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation tas"
W10-4223,C08-1013,0,0.095526,"hen a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of suff"
W10-4223,W96-0102,0,0.0711676,"Missing"
W10-4223,C04-1051,0,0.0443599,"imilarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headlin"
W10-4223,N06-1058,0,0.0525483,"sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make t"
W10-4223,P07-2045,0,0.0348047,"September 2006. Using this method we end up with a corpus of 7,400,144 pairwise alignments of 1,025,605 unique headlines1 . 3 Paraphrasing methods In our approach we use the collection of automatically obtained aligned headlines to train a paraphrase generation model using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1 This list of aligned pairs is http://ilk.uvt.nl/∼swubben/resources.html available at Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f |e) with a language model p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++"
W10-4223,W04-1013,0,0.0132683,"s case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned paraphrase corpus. Our method consists of crawling the h"
W10-4223,E06-1021,0,0.0694444,"aphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadriven MT account of paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned par"
W10-4223,J03-1002,0,0.0366185,"l using a PhraseBased MT framework. We compare this approach to a word substitution baseline. The generated paraphrases along with their source head1 This list of aligned pairs is http://ilk.uvt.nl/∼swubben/resources.html available at Phrase-Based MT We use the MOSES package to train a Phrase-Based Machine Translation model (PBMT) (Koehn et al., 2007). Such a model normally finds a best translation e˜ of a text in language f to a text in language e by combining a translation model p(f |e) with a language model p(e): e˜ = arg max p(f |e)p(e) ∗ e∈e GIZA++ is used to perform the word alignments (Och and Ney, 2003) which are then used in the Moses pipeline to generate phrase alignments in order to build the paraphrase model. We first tokenize our data before training a recaser. We then lowercase all data and use all unique headlines in the training data to train a language model with the SRILM toolkit (Stolcke, 2002). Then we invoke the GIZA++ aligner using the 7M training paraphrase pairs. We run GIZA++ with standard settings and we perform no optimization. Finally, we use the MOSES decoder to generate paraphrases for our test data. Instead of assigning equal weights to language and translation model,"
W10-4223,P02-1040,0,0.0872487,"paraphrasing to work, a large collection of data is required. In this case, this would have to be pairs of sentences that are paraphrases of each other. So far, paraphrasing data sets of sufficient size have been mostly lacking. We argue that the headlines aggregated by Google News offer an attractive avenue. 2 Data Collection Currently not many resources are available for paraphrasing; one example is the Microsoft Paraphrase Corpus (MSR) (Dolan et al., 2004; Nelken and Shieber, 2006), which with its 139,000 aligned lines are presented to human judges, whose ratings are compared to the BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE (Lin, 2004) automatic evaluation metrics. Police investigate Doherty drug pics Doherty under police investigation Police to probe Pete pics Pete Doherty arrested in drug-photo probe 3.1 Rocker photographed injecting unconscious fan Doherty ʼinjected unconscious fan with drugʼ Photos may show Pete Doherty injecting passed-out fan Doherty ʼinjected female fanʼ Figure 1: Part of a sample headline cluster, with aligned paraphrases paraphrases can be considered relatively small. In this study we explore the use of a large, automatically acquired aligned"
W10-4223,W04-3219,0,0.0466146,"machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subtitle generation (Daelemans et al., 2004). We regard SPG as a monolingual machine translation task, where the source and target languages are the same (Quirk et al., 2004). However, there are two problems that have to be dealt with to make this approach work, namely obtaining a sufficient amount of examples, and a proper evaluation methodology. As Callison-Burch et al. (2008) argue, automatic evaluation of paraphrasing is problematic. The essence of SPG is to generate a sentence that is structurally different from the source. Automatic evaluation metrics in related fields such as machine translation operate on a notion of similarity, while paraphrasing centers around achieving dissimilarity. Besides the evaluation issue, another problem is that for an datadrive"
W10-4223,P07-1059,0,0.0181436,"as summarization (Knight and Marcu, 2002), question-answering (Lin and Pantel, 2001), machine translation, and paraphrase generation. Sentential paraphrase generation (SPG) is the process of transforming a source sentence into a target sentence in the same language which differs in form from the source sentence, but approximates its meaning. Paraphrasing is often used as a subtask in more complex NLP applications to allow for more variation in text strings presented as input, for example to generate paraphrases of questions that in their original form cannot be answered (Lin and Pantel, 2001; Riezler et al., 2007), or to generate paraphrases of sentences that failed to translate (Callison-Burch et al., 2006). Paraphrasing has also been used in the evaluation of machine translation system output (Russo-Lassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006). Adding certain constraints to paraphrasing allows for additional useful applications. When a constraint is specified that a paraphrase should be shorter than the input text, paraphrasing can be used for sentence compression (Knight and Marcu, 2002; Barzilay and Lee, 2003) as well as for text simplification for question answering or subt"
W10-4223,W09-0621,1,0.548134,"Missing"
W10-4223,W06-1610,0,0.0581942,"Missing"
W10-4223,daelemans-etal-2004-automatic,0,\N,Missing
W11-1604,P05-1074,0,0.0381435,"The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bos"
W11-1604,N03-1003,0,0.0482554,"ed translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus"
W11-1604,P01-1008,0,0.0653452,"neration methods and the experimental setup. Results are presented in Section 4. In Section 5 we discuss our findings and formulate our conclusions. 2 Corpus The main bottleneck in building SMT systems is the need for a substantial amount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requires large amounts of monolingual parallel text. However, paraphrase corpora are scarce; the situation is more dire than in MT, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora. The use of monolingual parallel text corpora was first suggested by Barzilay and McKeown (2001), who built their corpus using various alternative human-produced translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semant"
W11-1604,D08-1021,0,0.0189038,"e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE"
W11-1604,H05-1098,0,0.0190688,"s such as insertion, deletion and many-to-one, one-to-many or many-to-many translation are all covered in the structure of the phrase table. Phrase-based models have been used most prominently in the past decade, as they have shown to outperform other approaches 29 (Callison-Burch et al., 2009). One issue with the phrase-based approach is that recursion is not handled explicitly. It is generally acknowledged that language contains recursive structures up to certain depths. So-called hierarchical models have introduced the inclusion of nonterminals in the mapping rules, to allow for recursion (Chiang et al., 2005). However, using a generic non-terminal X can introduce many substitutions in translations that do not make sense. By making the non-terminals explicit, using syntactic categories such as N P s and V P s, this phenomenon is constrained, resulting in syntax-based translation. Instead of phrase translations, translation rules in terms of syntactic constituents or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages. Incorporating syntax can guide the translation process and unlike phrase-based MT syntax it enables the modeling of lon"
W11-1604,J08-4005,0,0.0153637,"ases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samples of parallel and comparable text in which similar sentences, phrases and words are aligned. One part of the corpus is manually aligned, whereas another part is automatically aligned using a data-driven aligner trained on the first part. The DAESO corpus is extensively described in (Marsi and Krahmer, 2011); the"
W11-1604,C04-1051,0,0.0434847,"sed a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samples of parallel and comparable text in which similar sentences, phrases and words are aligned. One part of the corpus is manually aligned, wh"
W11-1604,E09-1049,0,0.0160286,"e Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of reasoning would suggest that syntax-based paraphrasin"
W11-1604,N03-1017,0,0.0505385,"Missing"
W11-1604,W07-0734,0,0.0376284,"als to make the word order match the surface word order. 3 http://www.vf.utwente.nl/˜druid/TwNC/ TwNC-main.html 30 average score. This implies that we penalize systems that provide output at Levenshtein distance 0, which are essentially copies of the input, and not paraphrases. Formally, the score is computed as follows: X N ISTweightedLD = α (i ∗ Ni ∗ N ISTi ) i=LD(1..8) X (i ∗ Ni ) i=LD(1..8) where α is the percentage of output phrases that have a sentence Levenshtein Distance higher than 0. Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. 4 Results Figure 1 shows NIST scores per Levenshtein Distance. It can be observed that overall the NIST score decreases as the distance to the input increases, indicating that more distant paraphrases are of less quality. The relaxed syntax-based approach (SAMT) performs mildly better than the standard syntaxbased approach, but performs worse than the phrasebased approach. The distribution of generated paraphrases per Levenshtein Distance is shown in Figure 2. It reveals that the Syntax-based approaches tend to stay closer to the source th"
W11-1604,J10-3003,0,0.0204038,"s better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands Introduction One of the challenging properties of natural language is that the same semantic content can typically be expressed by many different surface forms. As the ability to deal with paraphrases holds great potential for improving the coverage of NLP systems, a substantial body of research addressing recognition, extraction and generation of paraphrases has emerged (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). Paraphrase Generation can be regarded as a translation task in which source and target language are the same. Both Paraphrase Generation and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT h"
W11-1604,W07-0716,0,0.0211701,"antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University"
W11-1604,W02-1018,0,0.0253057,"me. Both Paraphrase Generation and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of rea"
W11-1604,J03-1002,0,0.00539719,"ra are still lacking for paraphrase generation, using more linguistically motivated methods might prove beneficial for paraphrase generation. At the same time, automatic syntactic analysis introduces errors in the parse trees, as no syntactic parser is perfect. Likewise, automatic alignment of syntactic phrases may be prone to errors. The main contribution of this paper is a systematic comparison between phrase-based and syntax-based paraphrase generation using an off-the-shelf statistical machine translation (SMT) decoder, namely Moses (Koehn et al., 2007) and the word-alignment tool GIZA++ (Och and Ney, 2003). Training data derives from a new, large scale (2.1M tokens) paraphrase corpus for Dutch, which has been recently released. The paper is organized as follows. Section 2 reviews the paraphrase corpus from which provides training and test data. Next, Section 3 describes the paraphrase generation methods and the experimental setup. Results are presented in Section 4. In Section 5 we discuss our findings and formulate our conclusions. 2 Corpus The main bottleneck in building SMT systems is the need for a substantial amount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requir"
W11-1604,J04-4002,0,0.17308,"neration and Machine Translation (MT) are instances of Text-To-Text Generation, which involves transforming one text into another, obeying certain restrictions. Here these restrictions are that the generated text must be grammatically well-formed and semantically/translationally equivalent to the source text. Addionally Paraphrase Generation requires that the output should differ from the input to a certain degree. However, since many researchers believe that PBMT has reached a performance ceiling, ongoing research looks into more structural approaches to statistical MT (Marcu and Wong, 2002; Och and Ney, 2004; Khalilov and Fonollosa, 2009). Syntaxbased MT attempts to extract translation rules in terms of syntactic constituents or subtrees rather than arbitrary phrases, presupposing syntactic structures for source, target or both languages. Syntactic information might lead to better results in the area of grammatical well-formedness, and unlike phrasebased MT that uses contiguous n-grams, syntax enables the modeling of long-distance translation patterns. While the verdict on whether or not this approach leads to any significant performance gain is still out, a similar line of reasoning would sugges"
W11-1604,W99-0604,0,0.0978911,"ilburg The Netherlands s.wubben@uvt.nl Erwin Marsi NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Pr"
W11-1604,N03-1024,0,0.0375578,"mount of parallel aligned text. Likewise, exploiting SMT for paraphrasing requires large amounts of monolingual parallel text. However, paraphrase corpora are scarce; the situation is more dire than in MT, and this has caused some studies to focus on the automatic harvesting of paraphrase corpora. The use of monolingual parallel text corpora was first suggested by Barzilay and McKeown (2001), who built their corpus using various alternative human-produced translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus"
W11-1604,W04-3219,0,0.0646921,"Missing"
W11-1604,P06-2096,0,0.019951,"ary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases. In a similar vein, Pang et al. (2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified. So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences 28 exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles. However, it is rather small and contains no subsentential allignments. Cohn et al. (2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level. However, all of these corpora are small from an SMT perspective. Recently a new large-scale paraphrase corpus for Dutch, the DAESO corpus, was released. The corpus contains both samp"
W11-1604,W09-0621,1,0.896169,"Missing"
W11-1604,W10-4223,1,0.888359,"Missing"
W11-1604,2002.tmi-tutorials.2,0,0.0383049,"Erwin Marsi NTNU Sem Saelandsvei 7-9 NO-7491 Trondheim Norway Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands emarsi@idi.ntnu.no antal.vdnbosch@uvt.nl e.j.krahmer@uvt.nl Abstract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phr"
W11-1604,P09-1094,0,0.0134462,"stract The similarity between Paraphrase Generation and MT suggests that methods and tools originally developed for MT could be exploited for Paraphrase Generation. One popular approach – arguably the most successful so far – is Statistical Phrase-based Machine Translation (PBMT), which learns phrase translation rules from aligned bilingual text corpora (Och et al., 1999; Vogel et al., 2000; Zens et al., 2002; Koehn et al., 2003). Prior work has explored the use of PBMT for paraphrase generation (Quirk et al., 2004; Bannard and Callison-Burch, 2005; Madnani et al., 2007; Callison-Burch, 2008; Zhao et al., 2009; Wubben et al., 2010) Paraphrase generation can be regarded as machine translation where source and target language are the same. We use the Moses statistical machine translation toolkit for paraphrasing, comparing phrase-based to syntax-based approaches. Data is derived from a recently released, large scale (2.1M tokens) paraphrase corpus for Dutch. Preliminary results indicate that the phrase-based approach performs better in terms of NIST scores and produces paraphrases at a greater distance from the source. 1 Antal van den Bosch Tilburg University P.O. Box 90135 5000 LE Tilburg The Nether"
W11-1604,W06-3119,0,0.0349578,"or subtrees are extracted, presupposing the availability of syntactic structures for source, target, or both languages. Incorporating syntax can guide the translation process and unlike phrase-based MT syntax it enables the modeling of long-distance translation patterns. Syntax-based systems may parse the data on the target side (string-to-tree), source side (tree-tostring), or both (tree-to-tree). In our experiments we use tree-to-tree syntaxbased MT. We also experiment with relaxing the parses by a method proposed under the label of syntax-augmented machine translation (SAMT), described in (Zollmann and Venugopal, 2006). This method combines any neighboring nodes and labels previously unlabeled nodes, removing the syntactic constraint on the grammar1 . We train all systems on the DAESO data (218,102 lines of aligned sentences) and test on a held-out set consisting of manually aligned headlines that ap1 This method is implemented in the Moses package in the program relax-parse as option SAMT 4 Table 2: Examples of output of the phrase-based and syntax-based systems Source Phrase-based Syntax-based jongen ( 7 ) zwaargewond na aanrijding 7-jarige gewond na botsing jongen ( 7 ) zwaar gewond na aanrijding boy (7)"
W11-1604,C00-2163,0,\N,Missing
W11-1604,W09-0401,0,\N,Missing
W11-1604,P07-2045,0,\N,Missing
W12-1503,P08-2050,0,0.0872478,"rties (X and Y- DIMENSION) its furniture and people domains are slightly less complex than their TUNA counterparts, making the attribute selection task a bit easier. One caveat of our study is that so far we have only used the standard automatic metrics on REG evaluation (albeit in accordance with many other studies in this area). However, it has been found that these do not always correspond to the results of human-based evaluations, so it would be interesting to see whether the same learning curve effects are obtained for extrinsic, task based evaluations involving human subjects. Following Belz and Gatt (2008), this could be done by measuring reading times, identification times or error rates as a function of training set size. Comparing IA with FB and GR We have shown that small set sizes are sufficient to reach ceiling for the IA. But which preference orders (PO’s) do we find with these small set sizes? And how does the IA’s performance with these orders compare to the results obtained by alternative algorithms such as Dale and Reiter’s (1995) classic Full Brevity (FB) and Greedy Algorithm (GR)? – a question explicitly asked by van Deemter et al. (2012). In the furniture domain, all five English"
W12-1503,W08-2120,0,0.0572691,"Missing"
W12-1503,W07-2307,0,0.185848,"Missing"
W12-1503,W09-0629,0,0.587668,"ll description, and may opt for a description with a single, relatively dispreferred property (“the man with the blue eyes”) when the alternative would be to combine many, relatively preferred properties (“the large, balding man with the bow tie and the striped tuxedo”). This flexibility is arguably one of the 3 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 3–11, c Utica, May 2012. 2012 Association for Computational Linguistics reasons why the graph-based REG approach works well: it was the best performing system in the most recent REG Challenge (Gatt et al., 2009). But where do the preferences used in the algorithms come from? Dale and Reiter point out that preferences are domain dependent, and that determining them for a given domain is essentially an empirical question. Unfortunately, they do not specify how this particular empirical question should be answered. The general preference for colour over size is experimentally well-established (Pechmann, 1989), but for most other cases experimental data are not readily available. An alternative would be to look at human data, preferably in a “semantically transparent” corpus (van Deemter et al., 2006), t"
W12-1503,2007.mtsummit-ucnlg.17,0,0.0292845,"criptions in a furniture scenario, and found that speakers can refer to a target in many different ways (“the yellow rug”, “the $150 rug”, etc.). The question, then, is how speakers decide which attributes to include in a description, and how this decision process can be modeled in a REG algorithm. When we focus on the generation of distinguishing descriptions (which is often done in REG), it is Even though the IA is exceptional in that it relies on a complete ordering of attributes, most current REG algorithms make use of preferences in some way (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). The graph-based REG algorithm (Krahmer et al., 2003), for example, models preferences in terms of costs, where cheaper is more preferred. Contrary to the IA, the graph-based algorithm assumes that preferences operate at the level of attribute-value pairs (or properties) rather than at the level of attributes; in this way it becomes possible to prefer a straightforward size (large) over a subtle colour (mauve, taupe). Moreover, the graphbased algorithm looks for the cheapest overall description, and may opt for a description with a single, relati"
W12-1503,koolen-krahmer-2010-tuna,1,0.856849,"rue, false HAS T IE true, false X - DIMENSION 1, 2, 3, 4, 5 Y- DIMENSION 1, 2, 3 data collection in TUNA. Attribute 4 Table 1: Attributes and values in the furniture and people domains. X - and Y- DIMENSION refer to an object’s horizontal and vertical position in a scene grid and only occur in the English TUNA corpus. in Table 1 as the X - and Y- DIMENSION), whereas in the -LOC condition they were discouraged (but not prevented) from mentioning object locations. However, some descriptions in the -LOC condition contained location information anyway. D-TUNA For Dutch, we used the D-TUNA corpus (Koolen and Krahmer, 2010). This corpus uses the same visual scenes and annotation scheme as the TUNA corpus, but consists of Dutch instead of English target descriptions. Since the D-TUNA experiment was performed in laboratory conditions, its data is relatively ’cleaner’ than the TUNA data, which means that it contains fewer descriptions that are not fully distinguishing and that its descriptions do not contain X - and Y- DIMENSION attributes. Although the descriptions in D-TUNA were collected in three different conditions (written, spoken, and face-to-face), we only use the written descriptions in this paper, as this"
W12-1503,J03-1003,1,0.917754,"Missing"
W12-1503,C08-2029,0,0.0176009,"urniture scenario, and found that speakers can refer to a target in many different ways (“the yellow rug”, “the $150 rug”, etc.). The question, then, is how speakers decide which attributes to include in a description, and how this decision process can be modeled in a REG algorithm. When we focus on the generation of distinguishing descriptions (which is often done in REG), it is Even though the IA is exceptional in that it relies on a complete ordering of attributes, most current REG algorithms make use of preferences in some way (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). The graph-based REG algorithm (Krahmer et al., 2003), for example, models preferences in terms of costs, where cheaper is more preferred. Contrary to the IA, the graph-based algorithm assumes that preferences operate at the level of attribute-value pairs (or properties) rather than at the level of attributes; in this way it becomes possible to prefer a straightforward size (large) over a subtle colour (mauve, taupe). Moreover, the graphbased algorithm looks for the cheapest overall description, and may opt for a description with a single, relatively dispreferred prop"
W12-1503,P11-2116,1,0.441093,"st, and, perhaps most importantly, which REG algorithm is considered? In this paper, we address these questions by systematically training two REG algorithms (the Incremental Algorithm and the graph-based REG algorithm) on sets of human-produced descriptions of increasing size and evaluating them on a held-out test set; we do this for two different domains (people and furniture descriptions) and two data sets in two different languages (TUNA and D-TUNA, the Dutch version of TUNA). 4 That size of the training set may have an impact on the performance of a REG algorithm was already suggested by Theune et al. (2011), who used the English TUNA corpus to determine preferences (costs) for the graph-based algorithm using a similar learning curve set-up as we use here. However, the current paper expands on Theune et al. (2011) in three major ways. Firstly, and most importantly, where Theune et al. reported results for only one algorithm (the graph-based one), we directly compare the performance of the graph-based algorithm and the Incremental Algorithm (something which, somewhat surprisingly, has not been done before). Secondly, we test whether these algorithms perform differently in two different languages ("
W12-1503,W08-1104,0,0.0216192,"the other hand, does not distinguish between the values of these attributes. Moreover, the graph-based algorithm is arguably more generic than the Incremental Algorithm, as it can straightforwardly deal with relational properties and lends itself to various extensions (Krahmer et al., 2003). In short, the larger training investment required for Graph in simple domains may be compensated by its versatility and better performance on more complex domains. To test this assumption, our experiment should be repeated using data from a more realistic and complex domain, e.g., geographic descriptions (Turner et al., 2008). Unfortunately, currently no such data sets are available. Finally, we found that the results of both algorithms were better for the Dutch data than for the English ones. We think that this is not so much an effect of the language (as English and Dutch are highly comparable) but rather of the way the TUNA and DTUNA corpora were constructed. The D-TUNA corpus was collected in more controlled conditions than TUNA and as a result, arguably, it contains training data of a higher quality. Also, because the D-TUNA corpus does not contain any location properties (X and Y- DIMENSION) its furniture an"
W12-1503,W06-1420,0,0.179344,"Missing"
W12-1503,U10-1013,0,0.200995,"found that speakers can refer to a target in many different ways (“the yellow rug”, “the $150 rug”, etc.). The question, then, is how speakers decide which attributes to include in a description, and how this decision process can be modeled in a REG algorithm. When we focus on the generation of distinguishing descriptions (which is often done in REG), it is Even though the IA is exceptional in that it relies on a complete ordering of attributes, most current REG algorithms make use of preferences in some way (Fabbrizio et al., 2008; Gerv´as et al., 2008; Kelleher, 2007; Spanger et al., 2008; Viethen and Dale, 2010). The graph-based REG algorithm (Krahmer et al., 2003), for example, models preferences in terms of costs, where cheaper is more preferred. Contrary to the IA, the graph-based algorithm assumes that preferences operate at the level of attribute-value pairs (or properties) rather than at the level of attributes; in this way it becomes possible to prefer a straightforward size (large) over a subtle colour (mauve, taupe). Moreover, the graphbased algorithm looks for the cheapest overall description, and may opt for a description with a single, relatively dispreferred property (“the man with the b"
W12-1503,viethen-etal-2008-controlling,1,0.846347,", 2003), which we refer to as Graph, information about domain objects is represented as a labelled directed graph, and REG is modeled as a graph-search problem. The output of the algorithm is the cheapest distinguishing subgraph, given a particular cost function assigning costs to properties (i.e., attribute-value pairs). By assigning zero costs to some properties Graph is also capable of generating overspecified descriptions, including redundant properties. To ensure that the graph search does not terminate before the free properties are added, the search order must be explicitly controlled (Viethen et al., 2008). To ensure a fair comparison with the IA, we make sure that if the target’s TYPE property was not originally selected by the algorithm, it is added afterwards. In this study, both the costs and orders required by Graph are derived from corpus data. We base Training and test data for our experiment were taken from two corpora of referring expressions, one English (TUNA) and one Dutch (D-TUNA). 5 3 Corpora TUNA The TUNA corpus (Gatt et al., 2007) is a semantically transparent corpus consisting of object descriptions in two domains (furniture and people). The corpus was collected in an on-line p"
W12-1503,W08-1134,0,\N,Missing
W12-1503,J09-4008,0,\N,Missing
W13-2108,2007.mtsummit-ucnlg.14,0,0.433706,"treatment of relations between entities. Relations such as on top of or to the left of fall out naturally from the graph-based representation of the domain, a facet missing in earlier algorithms. We believe that this makes the GBA particularly well-suited for generating language in spatial visual domains. In the years since the inception of the GBA, the REG community has become increasingly interested in evaluating algorithms against humanproduced data in visual domains, aiming to mimic human references to objects. This interest has manifested most prominently in the 2007-2009 REG Challenges (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009) based on the TUNA Corpus (van Deemter et al., 2012). The GBA performed among the best algorithms in all three of these challenges. However, in particular its ability to analyze relational information could not be assessed, because the TUNA Corpus does not contain annotated relational descriptions. We rectify this omission in the current work by testing the GBA on the GRE3D3 Corpus, which was designed to study the use of spatial relations in referring expressions (Viethen and Dale, 2008). We compare against a variant of the GBA that we introduce to build"
W13-2108,W09-0629,0,0.506914,"Relations such as on top of or to the left of fall out naturally from the graph-based representation of the domain, a facet missing in earlier algorithms. We believe that this makes the GBA particularly well-suited for generating language in spatial visual domains. In the years since the inception of the GBA, the REG community has become increasingly interested in evaluating algorithms against humanproduced data in visual domains, aiming to mimic human references to objects. This interest has manifested most prominently in the 2007-2009 REG Challenges (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009) based on the TUNA Corpus (van Deemter et al., 2012). The GBA performed among the best algorithms in all three of these challenges. However, in particular its ability to analyze relational information could not be assessed, because the TUNA Corpus does not contain annotated relational descriptions. We rectify this omission in the current work by testing the GBA on the GRE3D3 Corpus, which was designed to study the use of spatial relations in referring expressions (Viethen and Dale, 2008). We compare against a variant of the GBA that we introduce to build longer referring expresWhen they introd"
W13-2108,W08-1132,0,0.444036,"We find that we can match human data better than the original GBA with the variant that encourages overspecification. With this model, we aim to further advance towards human-like reference by developing a method to capture speaker-specific variation. Speaker variation cannot easily be modeled by the classic input variables of REG algorithms, but a number of authors have shown that system output can be improved by using speaker identity as an additional feature; this has often been accompanied by the observation that commonalities can be found in the reference behaviour of different speakers (Bohnet, 2008; Di Fabbrizio et al., 2008a; Mitchell et al., 2011b), particularly for spatial relations (Viethen and Dale, 2009). In the second experiment reported in this paper, we combine these insights by automatically clustering groups of speakers with similar behaviour and then defining separate cost functions for each group to better guide the algorithms. Before we assess the ability of the GBA and our variant to produce human-like referring expressions containing relations (Sections 5 and 6), we will give an overview of the relevant background to the treatment of relations in REG, a short history of"
W13-2108,W09-0631,0,0.0277901,"aker. In (Viethen and Dale, 2010), the impact of speaker identity as a machine-learning feature is more systematically tested. They show that exact knowledge about which speaker produced a referring expression boosts performance, but also find many commonalities between different speakers’ strategies for content selection. Mitchell et al. (2011b) used participant identity in a machine learner to successfully predict the kind of size modifier to be used in a referring expression. Additionally, various submissions to the REG challenges, particularly by Bohnet and Fabbrizio et al. (Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al., 2008a; Di Fabbrizio et al., 2008b) used speakerspecific POs to increase performance in their adaptations of the IA. All of these systems used the exact speaker identity as input, although many of the authors noted that groups of speakers behave similarly (Viethen and Dale, 2010; Mitchell et al., 2011b). We build off of this idea by clustering similar speakers together before learning parameters, and then generate for speaker-specific clusters. This method results in a significant improvement in performance. 3 redundantly is limited, in particular if relations are involve"
W13-2108,P06-1131,0,0.284955,"most other attributes (Viethen and Dale, 2006). Krahmer and Theune (2002) suggest a similar adjustment for the IA by introducing a recursive loop if a relation to another object is introduced to the referring expression under construction. They treat relations as fundamentally different from other attributes in order to recognize when to enter the recursive loop, however, they fail to address the problem of infinite regress, whereby the objects in a domain might be described in a circular manner by the relations holding between them. Another relational extension to the IA has been proposed by Kelleher and Kruijff (2006), treating relations as a completely different class from other attributes. Both extensions of the IA make the simplifying assumption that relations should only be considered if it is not possible to fully distinguish the target referent from the surrounding objects in any other way, with the idea that it takes less effort to consider and describe only one object (Krahmer and Theune, 2002; Viethen and Dale, 2008). 2.2 A Short History of the GBA A new approach to REG was proposed by Krahmer et al. (2003). In this approach, a scene is represented as a labeled directed graph (see Figure 1(b)), an"
W13-2108,koolen-krahmer-2010-tuna,1,0.856874,"atic method to transfer the FREE NA¨I VE cost function to new domains. They found that using only two clusters (a high frequency and a low frequency group with associated costs of 0 and 1) achieves the best results, with no significant differences to the FREE - NA¨I VE cost function on the TUNA Corpus. Subsequently they showed that on this corpus, a training set of only 20 descriptions suffices to determine a 2-means cost function that performs as well as one based on 165 descriptions. In (Koolen et al., 2012), the same authors extended these experiments to a Dutch version of the TUNA Corpus (Koolen and Krahmer, 2010) and came to a similar conclusion. Neither of the corpora used in these experiments included relations between objects. 2.3 Individual Variation in REG A number of authors have argued that to be able to produce human-like referring expressions, an algorithm must account for speaker variation: Different speakers will refer to the same object in different ways, and modeling this variation can bring us closer to generating the rich variety of ex74 pressions that people produce. Several approaches have been made in this direction. Although this was not explicitly discussed in (Jordan and Walker, 2"
W13-2108,E91-1028,0,0.476717,"rate cost functions for each group to better guide the algorithms. Before we assess the ability of the GBA and our variant to produce human-like referring expressions containing relations (Sections 5 and 6), we will give an overview of the relevant background to the treatment of relations in REG, a short history of the GBA, and the relevance of individual variation (Section 2). We introduce our new variant graph-based algorithm, LongestFirst, in Section 3. 2 2.1 tween objects in their output, but all of them suffer from problems with the knowledge representation not being suited to relations. Dale and Haddock (1991) use a constraint network and a recursive loop to extend the Greedy Algorithm, which uses the discriminatory power of an attribute as the main selection criterion. They treat relations the same as other attributes; but in most cases a certain spatial relation to a particular other object is fully distinguishing, which easily leads to strange chains of relations in the output omitting most other attributes (Viethen and Dale, 2006). Krahmer and Theune (2002) suggest a similar adjustment for the IA by introducing a recursive loop if a relation to another object is introduced to the referring expr"
W13-2108,W12-1503,1,0.906546,"heune et al. (2011) used k-means clustering on the property frequencies in order to provide a more systematic method to transfer the FREE NA¨I VE cost function to new domains. They found that using only two clusters (a high frequency and a low frequency group with associated costs of 0 and 1) achieves the best results, with no significant differences to the FREE - NA¨I VE cost function on the TUNA Corpus. Subsequently they showed that on this corpus, a training set of only 20 descriptions suffices to determine a 2-means cost function that performs as well as one based on 165 descriptions. In (Koolen et al., 2012), the same authors extended these experiments to a Dutch version of the TUNA Corpus (Koolen and Krahmer, 2010) and came to a similar conclusion. Neither of the corpora used in these experiments included relations between objects. 2.3 Individual Variation in REG A number of authors have argued that to be able to produce human-like referring expressions, an algorithm must account for speaker variation: Different speakers will refer to the same object in different ways, and modeling this variation can bring us closer to generating the rich variety of ex74 pressions that people produce. Several ap"
W13-2108,P89-1009,0,0.866694,"niversity Baltimore, USA TiCC University of Tilburg Tilburg, The Netherlands Abstract control for finding the ‘best’ referring expression, encompassing several previous approaches. This control is made possible by defining a desired cost function over object properties to guide the construction of the output expression and using a search mechanism that does not stop at the first solution found. One characteristic of the GBA particularly emphasized by Krahmer et al. (2003), advancing from research on algorithms such as the Incremental Algorithm (Dale and Reiter, 1995) and the Greedy Algorithm (Dale, 1989), was the treatment of relations between entities. Relations such as on top of or to the left of fall out naturally from the graph-based representation of the domain, a facet missing in earlier algorithms. We believe that this makes the GBA particularly well-suited for generating language in spatial visual domains. In the years since the inception of the GBA, the REG community has become increasingly interested in evaluating algorithms against humanproduced data in visual domains, aiming to mimic human references to objects. This interest has manifested most prominently in the 2007-2009 REG Ch"
W13-2108,W08-1133,0,0.0520991,"Missing"
W13-2108,J12-1006,1,0.823271,"Missing"
W13-2108,W03-2307,1,0.821234,"Missing"
W13-2108,J03-1003,1,0.644669,"Missing"
W13-2108,W08-1108,0,0.0172854,"e output as the basic IA (relations are never necessary for identifying the target in GRE3D3). These two baselines are tried with an attribute-based PO and a propertybased one. We do not expect a difference between the attribute- and the property-based PO on the IA, as this difference would only come to the fore in a situation where a choice has to be made between two values of the same attribute. In the IA’s analysis of the GRE3D3 domain, this can only happen with relations, which it will not use in this domain. We use Accuracy and Dice, the two most common metrics for human-likeness in REG (Gatt and Belz, 2008; Gatt et al., 2009), to assess our systems. Accuracy reports the relative frequency with which the generated attribute set and the humanproduced attribute set match exactly. Dice measures the overlap between the two attribute sets. For details, see, for example, Krahmer and van Deemter’s (2012) survey paper. We train and test our systems using 10-fold cross-validation. 5.4 GBA 0- COSTPROP 0- COSTATT FREE - NA¨I VE - ATT FREE - NA¨I VE - PROP K - PROP K - ATT propbased PO attbased PO Acc Dice Acc Dice Acc Dice Acc Dice Acc Dice Acc Dice Acc Dice Acc Dice 39.21 73.40 39.21 73.40 39.21 73.40 39."
W13-2108,P11-2041,1,0.863067,"r than the original GBA with the variant that encourages overspecification. With this model, we aim to further advance towards human-like reference by developing a method to capture speaker-specific variation. Speaker variation cannot easily be modeled by the classic input variables of REG algorithms, but a number of authors have shown that system output can be improved by using speaker identity as an additional feature; this has often been accompanied by the observation that commonalities can be found in the reference behaviour of different speakers (Bohnet, 2008; Di Fabbrizio et al., 2008a; Mitchell et al., 2011b), particularly for spatial relations (Viethen and Dale, 2009). In the second experiment reported in this paper, we combine these insights by automatically clustering groups of speakers with similar behaviour and then defining separate cost functions for each group to better guide the algorithms. Before we assess the ability of the GBA and our variant to produce human-like referring expressions containing relations (Sections 5 and 6), we will give an overview of the relevant background to the treatment of relations in REG, a short history of the GBA, and the relevance of individual variation"
W13-2108,W08-1131,0,0.248795,"s between entities. Relations such as on top of or to the left of fall out naturally from the graph-based representation of the domain, a facet missing in earlier algorithms. We believe that this makes the GBA particularly well-suited for generating language in spatial visual domains. In the years since the inception of the GBA, the REG community has become increasingly interested in evaluating algorithms against humanproduced data in visual domains, aiming to mimic human references to objects. This interest has manifested most prominently in the 2007-2009 REG Challenges (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009) based on the TUNA Corpus (van Deemter et al., 2012). The GBA performed among the best algorithms in all three of these challenges. However, in particular its ability to analyze relational information could not be assessed, because the TUNA Corpus does not contain annotated relational descriptions. We rectify this omission in the current work by testing the GBA on the GRE3D3 Corpus, which was designed to study the use of spatial relations in referring expressions (Viethen and Dale, 2008). We compare against a variant of the GBA that we introduce to build longer referring ex"
W13-2108,P11-2116,1,0.822871,"008) and was very successful as well in the following 2008 and 2009 REG Challenges (Gatt et al., 2008; Gatt et al., 2009) with a free-na¨ıve cost function. This cost function assigns 0 cost to the most common attributes, 2 to the rarest, and 1 to all others. By making the most common attributes free, it became possible to include these attributes redundantly in a referring expression, even if they were not strictly necessary for identifying the target. The cost functions used in the challenges were attribute-based, and did therefore not make use of the refined control capabilities of the GBA. Theune et al. (2011) used k-means clustering on the property frequencies in order to provide a more systematic method to transfer the FREE NA¨I VE cost function to new domains. They found that using only two clusters (a high frequency and a low frequency group with associated costs of 0 and 1) achieves the best results, with no significant differences to the FREE - NA¨I VE cost function on the TUNA Corpus. Subsequently they showed that on this corpus, a training set of only 20 descriptions suffices to determine a 2-means cost function that performs as well as one based on 165 descriptions. In (Koolen et al., 2012"
W13-2108,W06-1410,1,0.786588,"LongestFirst, in Section 3. 2 2.1 tween objects in their output, but all of them suffer from problems with the knowledge representation not being suited to relations. Dale and Haddock (1991) use a constraint network and a recursive loop to extend the Greedy Algorithm, which uses the discriminatory power of an attribute as the main selection criterion. They treat relations the same as other attributes; but in most cases a certain spatial relation to a particular other object is fully distinguishing, which easily leads to strange chains of relations in the output omitting most other attributes (Viethen and Dale, 2006). Krahmer and Theune (2002) suggest a similar adjustment for the IA by introducing a recursive loop if a relation to another object is introduced to the referring expression under construction. They treat relations as fundamentally different from other attributes in order to recognize when to enter the recursive loop, however, they fail to address the problem of infinite regress, whereby the objects in a domain might be described in a circular manner by the relations holding between them. Another relational extension to the IA has been proposed by Kelleher and Kruijff (2006), treating relation"
W13-2108,W08-1109,1,0.933121,"nterest has manifested most prominently in the 2007-2009 REG Challenges (Belz and Gatt, 2007; Gatt et al., 2008; Gatt et al., 2009) based on the TUNA Corpus (van Deemter et al., 2012). The GBA performed among the best algorithms in all three of these challenges. However, in particular its ability to analyze relational information could not be assessed, because the TUNA Corpus does not contain annotated relational descriptions. We rectify this omission in the current work by testing the GBA on the GRE3D3 Corpus, which was designed to study the use of spatial relations in referring expressions (Viethen and Dale, 2008). We compare against a variant of the GBA that we introduce to build longer referring expresWhen they introduced the Graph-Based Algorithm (GBA) for referring expression generation, Krahmer et al. (2003) flaunted the natural way in which it deals with relations between objects; but this feature has never been tested empirically. We fill this gap in this paper, exploring referring expression generation from the perspective of the GBA and focusing in particular on generating human-like expressions in visual scenes with spatial relations. We compare the original GBA against a variant that we intr"
W13-2108,U10-1013,1,0.869727,"erring expressions, an algorithm must account for speaker variation: Different speakers will refer to the same object in different ways, and modeling this variation can bring us closer to generating the rich variety of ex74 pressions that people produce. Several approaches have been made in this direction. Although this was not explicitly discussed in (Jordan and Walker, 2005), the machine-learned models presented there performed significantly better at replicating human-produced referring expressions when a feature set was used that included information about the identity of the speaker. In (Viethen and Dale, 2010), the impact of speaker identity as a machine-learning feature is more systematically tested. They show that exact knowledge about which speaker produced a referring expression boosts performance, but also find many commonalities between different speakers’ strategies for content selection. Mitchell et al. (2011b) used participant identity in a machine learner to successfully predict the kind of size modifier to be used in a referring expression. Additionally, various submissions to the REG challenges, particularly by Bohnet and Fabbrizio et al. (Bohnet, 2008; Bohnet, 2009; Di Fabbrizio et al."
W13-2108,viethen-etal-2008-controlling,1,0.887275,"nstruction problem. Assuming a scene graph G = hVG , EG i, where vertices VG represent objects and edges EG represent the properties and relations of these objects with associated costs, their algorithm returns the cheapest distinguishing subgraph that uniquely refers to the target object v 2 VG . Relations between objects (i.e., edges between different vertices) are a natural part of this representation, without requiring special computational mechanisms. In addition to cost functions, the GBA requires a preference ordering (PO) over the edges to arbitrate between equally cheap descriptions (Viethen et al., 2008). Relations, Graphs and Individual Variation Relations in REG In the knowledge representation underlying most work in REG, each object in a scene is modeled as a set of attribute-value pairs describing the object’s properties, such as hsize, largei. Such a representation is used in the two of the classic algorithms, the Greedy Algorithm (Dale, 1989) and the Incremental Algorithm (IA) (Dale and Reiter, 1995). Neither of these was originally intended to process relations between objects. Several attempts have been made to adapt the traditional REG algorithms to include relations be73 small right"
W13-2702,J03-1002,0,0.00938034,"on approach based on character bigrams performs best. 2 More work has been done in the area of translating between closely related languages and dealing with data sparsity that occurs within these language pairs (Hajiˇc et al., 2000; Van Huyssteen and Pilon, 2009). Koehn et al. (2003) have shown that there is a direct negative correlation between the size of the vocabulary of a language and the accuracy of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by machine translation within a language is a task that has not been studied extensively before. Related work is the study by Xu et al. (2012). They evaluate paraphrase systems that attempt to paraphrase a specific style of writing into another style. The plays of William Shakespeare and the modern translations of these works are used"
W13-2702,W12-4406,0,0.0413929,"Missing"
W13-2702,popovic-ney-2004-towards,0,0.0269097,"n from the Modern and Middle Polish Bible. The correspondences are extracted using machine translation with the aim of deriving historical grammar and lexical items. A larger amount of work has been published about spelling normalization of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are m"
W13-2702,A00-1002,0,0.107651,"Missing"
W13-2702,2009.eamt-1.3,0,0.0217281,"ters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion of our results in Section 8. letter-based system. However, a combined system performs marginally better in terms of BLEU scores. Tiedemann (2009) shows that combining character-based translation with phrase-based translation improves machine translation quality in terms of BLEU and NIST scores when translating between Swedish and Norwegian if the OOV-words are translated beforehand with the character-based model. Nakov and Tiedemann (2012) investigate the use of character-level models in the translation between Macedonian and Bulgarian movie subtitles. Their aim is to translate between the resource poor language Macedonian to the related language Bulgarian, in order to use Bulgarian as a pivot in order to translate to other languages s"
W13-2702,P03-1040,0,0.0383485,"n with the aim of deriving historical grammar and lexical items. A larger amount of work has been published about spelling normalization of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine t"
W13-2702,N03-1017,0,0.00334008,"s a dictionary to improve the statistical alignment process. The PBMT approach based on character bigrams rather than translating words, transliterates character bigrams and in this way improves the transformation process. We demonstrate that these two approaches outperform standard PBMT in this task, and that the PBMT transliteration approach based on character bigrams performs best. 2 More work has been done in the area of translating between closely related languages and dealing with data sparsity that occurs within these language pairs (Hajiˇc et al., 2000; Van Huyssteen and Pilon, 2009). Koehn et al. (2003) have shown that there is a direct negative correlation between the size of the vocabulary of a language and the accuracy of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by m"
W13-2702,W07-0705,0,0.0206163,"2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion"
W13-2702,W10-4223,1,0.910792,"Missing"
W13-2702,C12-1177,0,0.0160675,"of the translation. Alignment models are directly affected by data sparsity. Uncommon words are more likely to be aligned incorrectly to other words or, even worse, to large segments of words (Och and Ney, 2003). Out of vocabulary (OOV) words also pose a problem in the translation process, as systems are unable to provide translations for these words. A standard heuristic is to project them into the translated sentence untranslated. Related work Language transformation by machine translation within a language is a task that has not been studied extensively before. Related work is the study by Xu et al. (2012). They evaluate paraphrase systems that attempt to paraphrase a specific style of writing into another style. The plays of William Shakespeare and the modern translations of these works are used in this study. They show that their models outperform baselines based on dictionaries and out-of-domain parallel text. Their work differs from our work in that they target writing in a specific literary style and we are interested in translating between diachronic variants of a language. Work that is slightly comparable is the work by Zeldes (2007), who extrapolates correspondences in a small parallel"
W13-2702,2005.mtsummit-papers.11,0,0.00496556,"@let.ru.nl Abstract area that is now defined as the Netherlands and parts of Belgium. One of the factors that make Middle Dutch difficult to read is the fact that at the time no overarching standard language existed. Modern Dutch is defined as Dutch as spoken from 1500. The variant we investigate is contemporary Dutch. An important difference with regular paraphrasing is the amount of parallel data available. The amount of parallel data for the variant pair Middle Dutch - Modern Dutch is several orders of magnitude smaller than bilingual parallel corpora typically used in machine translation (Koehn, 2005) or monolingual parallel corpora used for paraphrase generation by machine translation (Wubben et al., 2010). We do expect many etymologically related words to show a certain amount of character overlap between the Middle and Modern variants. An example of the data is given below, from the work ’Van den vos Reynaerde’ (‘About Reynard the Fox’), part of the Comburg manuscript that was written between 1380-1425. Here, the first text is the original text, the second text is a modern translation in Dutch by Walter Verniers and a translation in English is added below that for clarity. Language tran"
W13-2702,N03-2016,0,0.0458324,"of historical texts. Baron and Rayson (2008) developed tools for research in Early Modern English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related langu"
W13-2702,D07-1092,0,0.0148357,"English. Their tool, VARD 2, finds candidate modern form replacements for spelling variants in historical texts. It makes use of a Various solutions to data sparsity have been studied, among them the use of part-of-speech tags, suffixes and word stems to normalize words (Popovic and Ney, 2004; De Gispert and Marino, 2006), the treatment of compound words in translation (Koehn and Knight, 2003), transliteration of names and named entities, and advanced models that combine transliteration and translation (Kondrak et al., 2003; Finch et al., 2012) or learn unknown words by analogical reasoning (Langlais and Patry, 2007). Vilar et al. (2007) investigate a way to handle data sparsity in machine translation between closely related languages by translating between characters as opposed to words. The words in the parallel sentences are segmented into characters. Spaces between words are marked with a special character. The sequences of characters are then used to train a standard machine translation model and a language model with n-grams up to n = 16. They apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We clo"
W13-2702,P12-2059,0,0.0542666,"hey apply their system to the translation between the related languages Spanish and Catalan, and find that a word based system outperforms their 12 tion 7. We close with a discussion of our results in Section 8. letter-based system. However, a combined system performs marginally better in terms of BLEU scores. Tiedemann (2009) shows that combining character-based translation with phrase-based translation improves machine translation quality in terms of BLEU and NIST scores when translating between Swedish and Norwegian if the OOV-words are translated beforehand with the character-based model. Nakov and Tiedemann (2012) investigate the use of character-level models in the translation between Macedonian and Bulgarian movie subtitles. Their aim is to translate between the resource poor language Macedonian to the related language Bulgarian, in order to use Bulgarian as a pivot in order to translate to other languages such as English. Their research shows that using character bigrams shows improvement over a word-based baseline. It seems clear that character overlap can be used to improve translation quality in related languages. We therefore use character overlap in language transformation between two diachroni"
W13-2702,P07-2045,0,\N,Missing
W15-4706,J12-1006,1,0.873003,"Missing"
W15-4706,P09-4010,0,\N,Missing
W16-6608,P05-1074,0,0.0273893,"ble rewrites. By using discriminative training, a weight is assigned to each grammar rule. These grammar rules are then used to generate compressions by a decoder. In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse. (Cohn et al., 2008) propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming. For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the related task of sentence simplification. (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia. Their models were able to perform rewording, reordering, insertion and deletion actions. (Woodsend and Lapata, 2011) use Simple Wik"
W16-6608,D14-1179,0,0.0286808,"Missing"
W16-6608,D07-1008,0,0.644752,"years (Lloret and Palomar, 2012). Extractive sentence compression entails finding a subset of words in the source sentence that can be dropped to create a new, shorter sentence that is still grammatical and contains the most important information. More formally, the aim is to shorten a sentence x = x1 , x2 , ..., xn into a substring y = y1 , y2 , ..., ym where all words in y also occur in x in the same order and m < n. A number of techniques have been used for extractive sentence compression, ranging from the noisy-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source sentence. Recent advanc"
W16-6608,C08-1018,0,0.500283,"et al., 2011), we selected only those cases with rougly equal character compression rate (we limited this by selecting within a 0.1 CCR resolution). From this selection, we randomly selected 30 source sentences with their corresponding system outputs and one short human description which served as the human compression. 46 We used Crowdflower6 to perform the evaluation study. CrowdFlower is a platform for data annotation by the crowd. We allowed only native English speakers with a trust level of minimally 90 percent to partcipate. Following earlier evaluation studies (Clarke and Lapata, 2008; Cohn and Lapata, 2008; Wubben et al., 2012) we asked 25 participants to evaluate Fluency and Importance of the target compressions on a seven point Likert scale. Fluency was defined in the instructions as the extent to which a sentence is in proper, grammatical English. Importance was defined as the extent to which the sentence has retained the important information from the source sentence. The order of the output of the various systems was randomized. The participants saw 30 source descriptions and for each source description they evaluated all three compressions: the aRNN, Moses and Human compression. They were"
W16-6608,J08-4005,0,0.0119509,"ing is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using discriminative training, a weight is assigned to each grammar rule. These grammar rules are then used to generate compressions by a decoder. In contrast to the large body of work on extractive sentence compression, work on abstractive sentence compression is relatively sparse. (Cohn et al., 2008) propose an abstractive sentence compression method based on a parse tree transduction grammar and Integer Linear Programming. For their abstractive model, the grammar that is extracted is augmented with paraphrasing rules obtained from a pivoting approach to a bilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the"
W16-6608,W11-1601,0,0.157528,"aper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an Attentive Recurrent Neural"
W16-6608,W05-0901,0,0.0410566,"rable variation between sentences. The general pattern for Fluency, in Figure 3, is comparable, but much more pronounced: Fluency scores for Moses are (much) lower than for aRNN, and the latter are very similar to those for the Human descriptions. 5.3 Correlations Interestingly, we found no significant correlations between the automatic measures and the human 7 https://github.com/cgevans/scikits-bootstrap 47 fluency 5.2 6 4 3 2 1 aRNN Moses system Human Figure 3: Fluency scores given by human subjects to the two systems and human description. judgements. This is in line with earlier findings (Dorr et al., 2005). We did find correlations between human judgements, as can be observed in Table 6. Strong correlations are reported between the Fluency and Importance for the systems, and moderate correlation for the Human compression. This indicates some difference in the nature of the errors the systems and the humans make. 5.4 Qualitative analysis When we look at the output in Table , we can observe a few interesting things. First, the human written descriptions sometimes contain errors, i.e. ’many toilets without its upper top part’. The aRNN system is robust to these errors as it can abstract away from"
W16-6608,D15-1042,0,0.281873,"Missing"
W16-6608,N07-1023,0,0.0299514,"n that the aRNN outperforms the Moses system and even performs on par with the human generated description. We also show that automatic measures such as ROUGE that are used generally to evaluate compression tasks do not correlate with human judgements. 2 Related work A large body of work is devoted to extractive sentence compression. Here, we mention a few. (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar. (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results. (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence. Discriminative learning is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using discriminative training, a weight i"
W16-6608,W11-2123,0,0.0132897,"ranslation P (X|Y ) with a language model that outputs the most likely sentence P (Y ): Y˜ = arg max∗ P (X|Y )P (Y ) Y ∈Y Moses augments this model by regarding logP (X|Y ) as a loglinear model with added features and weights. During decoding, the sentence X is segmented into a sequence of I phrases. Each phrase is then translated into a phrase to form sentence Y . During this process phrases may be reordered. The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) and the KenLM (Heafield, 2011) package is used to do language modelling on the target sentences. Because Moses performs Phrase-based Machine Translation where it is often not optimal to delete unaligned phrases from the source sentence, we pad the source sentence with special EMPTY tokens until the source and target sentences contain equally many tokens. We train the Moses system with default parameters on the 900,000 padded training pairs. Additionally, we train a KenLM language model on the target side sentences from the training set. We perform MERT tuning on the development set and manually set the word penalty weight"
W16-6608,A00-2024,0,0.0631613,"-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source sentence. Recent advances in Recurrent Neural Networks (RNNs) have boosted interest in text-to-text generation tasks (Sutskever et al., 2014). In this paper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sente"
W16-6608,W04-1013,0,0.0949412,"utomatic scores (BLEU scores, various ROUGE scores and character compression rates) as well as human judgements on two different dimensions (Fluency and Importance). model ARNN Moses Human BLEU 0.21 0.13 0.17 ROUGE 1 0.70 0.69 0.72 ROUGE 2 0.40 0.38 0.41 ROUGE 3 0.28 0.25 0.28 ROUGE 4 0.22 0.19 0.21 ROUGE SU4 0.49 0.48 0.50 Table 4: BLEU and ROUGE scores 4.2.1 Automatic Evaluation First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (Papineni et al., 2002), which is generally used for Machine Translation and variants of ROUGE (Lin, 2004), which is generally used for summarization evaluation. Both take into account reference sentences and calculate overlap on the n-gram level. ROUGE also accounts for compression. ROUGE 1-4 take into account unigrams up to four-grams and ROUGE SU4 also takes into account skipgrams. For BLEU we use multi-bleu.pl, and for ROUGE we used pyrouge. We also compute compression rate on the character level, as this tells us how much the source sentence has been compressed. We simply compute this by dividing the number of characters in the target sentence by the number of characters in the source sentenc"
W16-6608,W01-0100,0,0.341702,"ty of its output to a Phrasebased Machine Translation (PBMT) model and a human generated short description. An extensive evaluation is done using automatic measures and human judgements. We show that the neural model outperforms the PBMT model. Additionally, we show that automatic measures are not very well suited for evaluating this text-to-text generation task. 1 Introduction Text summarization is an important, yet challenging subfield of Natural Language Processing. Summarization can be defined as the process of finding the important items in a text and presenting them in a condensed form (Mani, 2001; Knight and Marcu, 2002). Summarization on the sentence level is called sentence compression. Sentence compression approaches can be classified into two categories: extractive and abstractive sentence compression. Most successful sentence compression models consist of extractive approaches that select the most relevant fragments from the source document and generate a shorter representation of this document by stitching the selected fragments together. In contrast, abstractive sentence compression is the process of producing a representation of the original sentence in a bottom-up manner. Thi"
W16-6608,E06-1038,0,0.484011,"ntion in recent years (Lloret and Palomar, 2012). Extractive sentence compression entails finding a subset of words in the source sentence that can be dropped to create a new, shorter sentence that is still grammatical and contains the most important information. More formally, the aim is to shorten a sentence x = x1 , x2 , ..., xn into a substring y = y1 , y2 , ..., ym where all words in y also occur in x in the same order and m < n. A number of techniques have been used for extractive sentence compression, ranging from the noisy-channel model (Knight and Marcu, 2002), large-margin learning (McDonald, 2006; Cohn and Lapata, 2007) to Integer Linear Programming (Clarke and Lapata, 2008). (Marsi et al., 2010) characterize these approaches in terms of two assumptions: (1) only word deletions are allowed and (2) the word order is fixed. They argue that these constraints rule out more complicated operations such as reordering, substitution and insertion, and reduce the sentence compression task to a word deletion task. This does not model human sentence compression accurately, as humans tend to paraphrase when summarizing (Jing and McKeown, 2000), resulting in an abstractive compression of the source"
W16-6608,W11-1611,0,0.196739,"Missing"
W16-6608,J03-1002,0,0.00483215,"model that finds the most likely translation P (X|Y ) with a language model that outputs the most likely sentence P (Y ): Y˜ = arg max∗ P (X|Y )P (Y ) Y ∈Y Moses augments this model by regarding logP (X|Y ) as a loglinear model with added features and weights. During decoding, the sentence X is segmented into a sequence of I phrases. Each phrase is then translated into a phrase to form sentence Y . During this process phrases may be reordered. The GIZA++ statistical alignment package is used to perform the word alignments, which are later combined into phrase alignments in the Moses pipeline (Och and Ney, 2003) and the KenLM (Heafield, 2011) package is used to do language modelling on the target sentences. Because Moses performs Phrase-based Machine Translation where it is often not optimal to delete unaligned phrases from the source sentence, we pad the source sentence with special EMPTY tokens until the source and target sentences contain equally many tokens. We train the Moses system with default parameters on the 900,000 padded training pairs. Additionally, we train a KenLM language model on the target side sentences from the training set. We perform MERT tuning on the development set and manual"
W16-6608,P02-1040,0,0.0952011,"order to evaluate our models. 45 Evaluation To evaluate the output of our systems we collect automatic scores (BLEU scores, various ROUGE scores and character compression rates) as well as human judgements on two different dimensions (Fluency and Importance). model ARNN Moses Human BLEU 0.21 0.13 0.17 ROUGE 1 0.70 0.69 0.72 ROUGE 2 0.40 0.38 0.41 ROUGE 3 0.28 0.25 0.28 ROUGE 4 0.22 0.19 0.21 ROUGE SU4 0.49 0.48 0.50 Table 4: BLEU and ROUGE scores 4.2.1 Automatic Evaluation First, we perform automatic evaluation using regular summarization and text generation evaluation metrics, such as BLEU (Papineni et al., 2002), which is generally used for Machine Translation and variants of ROUGE (Lin, 2004), which is generally used for summarization evaluation. Both take into account reference sentences and calculate overlap on the n-gram level. ROUGE also accounts for compression. ROUGE 1-4 take into account unigrams up to four-grams and ROUGE SU4 also takes into account skipgrams. For BLEU we use multi-bleu.pl, and for ROUGE we used pyrouge. We also compute compression rate on the character level, as this tells us how much the source sentence has been compressed. We simply compute this by dividing the number of"
W16-6608,D15-1044,0,0.64711,"compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an Attentive Recurrent Neural Network (aRNN) to the task of s"
W16-6608,P05-1036,0,0.422164,"ve automatic and human evaluation that the aRNN outperforms the Moses system and even performs on par with the human generated description. We also show that automatic measures such as ROUGE that are used generally to evaluate compression tasks do not correlate with human judgements. 2 Related work A large body of work is devoted to extractive sentence compression. Here, we mention a few. (Knight and Marcu, 2002) propose two models to generate a short sentence by deleting a subset of words: the decision tree model and the noisy channel model, both based on a synchronous context free grammar. (Turner and Charniak, 2005) and (Galley and McKeown, 2007) build upon this model reporting improved results. (McDonald, 2006) develop a system using largemargin online learning combined with a decoding algorithm that searches the compression space to produce a compressed sentence. Discriminative learning is used to combine the features and weight 42 their contribution to a successful compression. (Cohn and Lapata, 2007) cast the sentence compression problem as a tree-to-tree rewriting task. For this task, they train a synchronous tree substitution grammar, which dictates the space of all possible rewrites. By using disc"
W16-6608,D11-1038,0,0.0129165,"ilingual corpus (Bannard and Burch, 2005). They show that the abstractive model outperforms an extractive model on their dataset.(Cohn and Lapata, 2013) follow up on earlier work and describe a discriminative tree-to-tree transduction model that can handle mismatches on the structural and lexical level. There has been some work on the related task of sentence simplification. (Coster and Kauchak, 2011; Zhu et al., 2010) develop models using data from Simple English Wikipedia paired with English Wikipedia. Their models were able to perform rewording, reordering, insertion and deletion actions. (Woodsend and Lapata, 2011) use Simple Wikipedia edit histories and an aligned Wikipedia– Simple Wikipedia corpus to induce a model based on quasi-synchronous grammar and integer linear programming. (Wubben et al., 2012) propose a model for simplifying sentences using monolingual Phrase-Based Machine Translation obtaining state of the art results. Recently, significant advances have been made in sequence to sequence learning. The paradigm has shifted from traditional approaches that are more focused on optimizing the parameters of several subsystems, to a single model that learns mappings between sequences by learning f"
W16-6608,P12-1107,1,0.882858,"Missing"
W16-6608,Q14-1006,0,0.0527031,"ion-based RNN is unable to outperform a Moses system. Only after additional tuning on extractive compresssions do they get better ROUGE scores. This can be attributed to the fact that additional extractive features bias the system towards retaining more input words, which is beneficial for higher ROUGE scores. Following this work, we employ an attentive Recurrent Network as described in (Bahdanau et al., 2014) to the task of abstractive summarization of scene descriptions. 3 Data set To construct the data set to train the models on, we use the image descriptions in the MSCOCO1 and FLICKR30K2 (Young et al., 2014) data sets. These data sets contain images paired with multiple descriptions provided by human subjects. The FLICKR30K data set contains 158,915 captions describing 31,783 images and the MSCOCO data set contains over a million captions describing over 160,000 images. For this work, we assume that the shorter descriptions of the images are abstractive summaries of the longer descriptions. We constrain the long-short relation by stating that a short description should be at least 10 percent shorter than a long 1 2 http://mscoco.org/dataset/ http://shannon.cs.illinois.edu/DenotationGraph/ descrip"
W16-6608,C10-1152,0,0.0585816,", 2014). In this paper we focus on abstractive sentence compression Proceedings of The 9th International Natural Language Generation conference, pages 41–50, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics with RNNs. In order to be applied to sentence compression, RNNs typically need to be trained on large data sets of aligned sequences. In the domain of abstractive sentence compression, not many of such data sets are available. For the related task of sentence simplification, data sets are available of aligned sentences from Wikipedia and Simple Wikipedia (Zhu et al., 2010; Coster and Kauchak, 2011). Recently, (Rush et al., 2015) used the Gigaword corpus to construct a large corpus containing headlines paired with the article’s first sentence. Here, we present a data set compiled from scene descriptions taken from the MSCOCO dataset (Lin et al., 2014). These descriptions are generally only one sentence long, and humans tend to describe photos in different ways, which makes this task suitable for abstractive sentence compression. For each image, we align long descriptions with shorter descriptions to construct a corpus of abstractive compressions . We employ an"
W16-6608,N03-1031,0,\N,Missing
W16-6612,P10-1126,0,0.0245892,"ed out to be a win for Burton Albion, see quote (2). This results in very different emotions shining through in the corresponding texts: while all the frustration for Peterborough seems to be piled up in a long first sentence already (“suffer... a defeat”, “nightmare spell”, “anger”), the winners’ text is shorter and much more positive (“entertaining”). Knowing about these and other differences that occur in biased sports reporting would be especially valuable for automatic generation of natural language. NLG can be and is currently applied in many different ways, ranging from photo captions (Feng and Lapata, 2010) to neonatal intensive care reports (Portet et al., 2009) and narrative prose (Callaway and Lester, 2001). Bateman and Paris (1989) stress the importance of tailoring machine generated language to the needs of the intended audience. Taking 74 Proceedings of The 9th International Natural Language Generation conference, pages 74–78, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics this one step further, Hovy (1990) describes how considering different perspectives on the same event, by taking into account the speaker’s emotional state, rhetorical, and communicat"
W16-6612,W11-2803,0,0.103372,"Missing"
W16-6636,N16-1048,1,0.652729,"Missing"
W16-6636,J12-1006,1,0.892643,"Missing"
W16-6636,P14-5010,0,0.00428233,"the last tokens from the name and birth name. Middle names are all the tokens which are not the first token in the given and birth names and last token in the name and birth name. For instance, Charles Bukowski has Charles, Bukowski, Charles Bukowski and Heinrich Karl Bukowski as his given name, surname, name and birth name in DBpedia, respectively. Based on this information, the knowledge base for this entity would consist of Charles and Heinrich as first names; Karl as middle name; and Bukowski as last name. Discourse Annotation The webpages were parsed using the Stanford CoreNLP software (Manning et al., 2014). Using this tool, we performed part-of-speech tagging, lemmatization, named entity recognition, dependency parsing, syntactic parsing, sentiment analysis and coreference resolution. To improve the coreference resolution we performed a post hoc sanity check, to see whether references which were labelled as being to the same entity were correct. For each entity distinguished by the software, we checked the proper nouns of each proper name reference. If at least the proper nouns of one proper name were values present in the knowledge base of the target entity, all the references of the entity di"
W16-6636,J11-4007,0,0.324181,"ariants can be generated using standard algorithms for the generation of descriptions. In other words, van Deemter (2014) proposes describing proper names based on a knowledge base of attributevalue pairs. Just like a set of attribute-value pairs {(type, cube), (color, blue)} is generated when the target needs to be singled out from differently coloured objects, a proper name like Frida Kahlo can be seen to single out one person from a context set. When the set is smaller, generally a shorter name will suffice. Van Deemter, however, does not apply this model in the context of text generation. Siddharthan et al. (2011) presented a model to (re)generate referring expressions to people in extractive summaries. When generating a proper name, the model chooses between a full name or only a surname. Moreover, it also decides whether 222 Proceedings of The 9th International Natural Language Generation conference, pages 222–226, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics to use pre- (role, affiliation and temporal modifiers) or post-modifiers (appositives and relative clauses). As far as we know, this is the only study that introduced a corpus analysis of how humans produce"
W17-3501,D15-1198,0,0.0209926,"Netherlands 2 ADAPT Centre, Dublin City University, School of Computing, Ireland {tcastrof,e.j.krahmer,s.wubben}@tilburguniversity.edu {iacer.calixto}@adaptcentre.ie Abstract AMRs have increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the bene"
W17-3501,W13-2322,0,0.497527,"rocess of generating coherent natural language text from non-linguistic data (Reiter and Dale, 2000). While there is broad consensus among NLG scholars on the output of NLG systems (i.e., text), there is far less agreement on what the input should be; see Gatt and Krahmer (2017) for a recent review. Over the years, NLG systems have taken a wide range of inputs, including for example images (Xu et al., 2015), numeric data (Gkatzia et al., 2014) and semantic representations (Theune et al., 2001). This study focuses on generating natural language based on Abstract Meaning Representations (AMRs) (Banarescu et al., 2013). AMRs encode the meaning of a sentence as a rooted, directed and acyclic graph, where nodes represent concepts, and labeled directed edges represent relations among these concepts. The formalism strongly relies on the PropBank notation. Figure 1 shows an example. 1 https://github.com/ThiagoCF05/LinearAMR We assume that in practical applications, conceptualisation models or dialogue managers (models which decide “what to say”) output AMRs. In this paper we study different ways in which these AMRs can be converted into natural language (deciding “how to say it”). We approach this as a translati"
W17-3501,J16-3006,0,0.0211371,"Ferreira1 and Iacer Calixto2 and Sander Wubben1 and Emiel Krahmer1 1 Tilburg center for Cognition and Communication (TiCC), Tilburg University, The Netherlands 2 ADAPT Centre, Dublin City University, School of Computing, Ireland {tcastrof,e.j.krahmer,s.wubben}@tilburguniversity.edu {iacer.calixto}@adaptcentre.ie Abstract AMRs have increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocess"
W17-3501,N12-1047,0,0.0271648,"(-Preorder) or preorder it with our 2-step classifier (+Preorder). Finally, we translate a flattened AMR into text using a Phrase-based (PBMT) and a Neural Machine Translation model (NMT). In total, we evaluated 16 models. Phrase-based Machine Translation We used a standard PBMT system built using Moses toolkit (Koehn et al., 2007). At training time, we extract and score phrase sentences up to the size of 9 tokens. All the feature functions were trained using the gold-standard alignments from the training set and their weights were tuned on the development data using k-batch MIRA with k = 60 (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. Lexicalised reordering models were bidirectional. At decoding time, we use a stack size of 1000. Our language model P (e) is a 5-gram LM trained on the Gigaword Third Edition corpus using KenLM (Heafield et al., 2013). For the models with the Delexicalisation step, we trained the language model 6 (9) with a delexicalised version of Gigaword by parsing the corpus using the Stanford Named Entity Recognition tool (Finkel et al., 2005). All the entities labeled as LOCATION, PERSON, ORGANISATION or MISC"
W17-3501,W14-4012,0,0.122379,"Missing"
W17-3501,P05-1045,0,0.00481563,"e tuned on the development data using k-batch MIRA with k = 60 (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. Lexicalised reordering models were bidirectional. At decoding time, we use a stack size of 1000. Our language model P (e) is a 5-gram LM trained on the Gigaword Third Edition corpus using KenLM (Heafield et al., 2013). For the models with the Delexicalisation step, we trained the language model 6 (9) with a delexicalised version of Gigaword by parsing the corpus using the Stanford Named Entity Recognition tool (Finkel et al., 2005). All the entities labeled as LOCATION, PERSON, ORGANISATION or MISC were replaced by the tag nameX . Entities labeled as NUMBER or MONEY were replaced by the tag quantX . Finally, entities labeled as PERCENT or ORDINAL were replaced by valueX . In the tags, X is replaced by the ordinal position of the entity in the sentence. Neural Machine Translation The encoder is a bidirectional RNN with GRU, each with a 1024D hidden unit. Source and target word embeddings are 620D each and are both trained jointly with the model. All non-recurrent matrices are initialised by sampling from a Gaussian (µ ="
W17-3501,P14-1134,0,0.0237174,"Tilburg University, The Netherlands 2 ADAPT Centre, Dublin City University, School of Computing, Ireland {tcastrof,e.j.krahmer,s.wubben}@tilburguniversity.edu {iacer.calixto}@adaptcentre.ie Abstract AMRs have increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed hel"
W17-3501,N16-1087,0,0.0930534,"alixto}@adaptcentre.ie Abstract AMRs have increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is"
W17-3501,D08-1089,0,0.0257568,"is calculated by the loglinear model described at Equation 2. P (amr |e) = argmax J X λj hj (amr, e) (2) j=1 Each hj (amr, e) is an arbitrary feature function over AMR-sentence pairs. To calculate it, the flat amr is segmented into I phrases amr ¯ I1 , such that each phrase amr ¯ i is translated into a target phrase e¯i as described by Equation 3. We also used models to reorder a flat amr according to the target sentence e at decoding time. They work on the word-level (Koehn et al., 2003), at the level of adjacent phrases (Koehn et al., 2005) and beyond adjacent phrases (hierarchical-level) (Galley and Manning, 2008). Phrase- and hierarchical level models are also known as lexicalized reordering models. As Koehn et al. (2003), given si the start position of the source phrase amr ¯ i translated into the English phrase e¯i , and fi−1 the end position of the source phrase amr ¯ i−1 translated into the English phrase e¯i−1 , a distortion model α|si −fi−1 −1 |is defined as a distance-based reordering model. α is chosen by tunning the model. Lexicalised models are more complex than distance-based ones, but usually help the system to obtain better results (Koehn et al., 2005; Galley and Manning, 2008). Given a p"
W17-3501,P14-1116,0,0.0136642,"elps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data (Reiter and Dale, 2000). While there is broad consensus among NLG scholars on the output of NLG systems (i.e., text), there is far less agreement on what the input should be; see Gatt and Krahmer (2017) for a recent review. Over the years, NLG systems have taken a wide range of inputs, including for example images (Xu et al., 2015), numeric data (Gkatzia et al., 2014) and semantic representations (Theune et al., 2001). This study focuses on generating natural language based on Abstract Meaning Representations (AMRs) (Banarescu et al., 2013). AMRs encode the meaning of a sentence as a rooted, directed and acyclic graph, where nodes represent concepts, and labeled directed edges represent relations among these concepts. The formalism strongly relies on the PropBank notation. Figure 1 shows an example. 1 https://github.com/ThiagoCF05/LinearAMR We assume that in practical applications, conceptualisation models or dialogue managers (models which decide “what to"
W17-3501,P13-2121,0,0.0209316,"ehn et al., 2007). At training time, we extract and score phrase sentences up to the size of 9 tokens. All the feature functions were trained using the gold-standard alignments from the training set and their weights were tuned on the development data using k-batch MIRA with k = 60 (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. Lexicalised reordering models were bidirectional. At decoding time, we use a stack size of 1000. Our language model P (e) is a 5-gram LM trained on the Gigaword Third Edition corpus using KenLM (Heafield et al., 2013). For the models with the Delexicalisation step, we trained the language model 6 (9) with a delexicalised version of Gigaword by parsing the corpus using the Stanford Named Entity Recognition tool (Finkel et al., 2005). All the entities labeled as LOCATION, PERSON, ORGANISATION or MISC were replaced by the tag nameX . Entities labeled as NUMBER or MONEY were replaced by the tag quantX . Finally, entities labeled as PERCENT or ORDINAL were replaced by valueX . In the tags, X is replaced by the ordinal position of the entity in the sentence. Neural Machine Translation The encoder is a bidirectio"
W17-3501,N03-1017,0,0.0836323,"would be used to predict the permutation order of them (0-1 or 10). As features, the head node is also represented by its PropBank frameset, whereas the subtrees of the groups are represented by their parent edges, their head node framesets and by which side of the head node they are (before or after). We train classifiers for groups of sizes between 2 and 4 subtrees. For bigger groups, we used the depth first search order. Figure 2: Example of a Delexicalised, Compressed and Linearised AMR 3.4 Translation models To map a flat AMR representation into an English sentence, we use phrase-based (Koehn et al., 2003) and neural machine translation (Bahdanau et al., 2015) models. 3.4.1 Phrase-Based Machine Translation These models use Bayes rule to formalise the problem of translating a text from a source language f to a target language e. In our case, we want to translate a flat amr into an English sentence e as Equation 1 shows. P (e |amr) = argmax P (amr |e)P (e) (1) The a priori function P (e) usually is represented by a language model trained on the target language. The a posteriori equation is calculated by the loglinear model described at Equation 2. P (amr |e) = argmax J X λj hj (amr, e) (2) j=1 Ea"
W17-3501,P07-2045,0,0.0111137,"tion/Realisation (-Delex and +Delex) and Compression (-Compress and +Compress) steps. In models without the Compression step, we include all the elements from an AMR in the flattened representation. For the Linearisation step, we flatten the AMR structure based on a depth-first search (-Preorder) or preorder it with our 2-step classifier (+Preorder). Finally, we translate a flattened AMR into text using a Phrase-based (PBMT) and a Neural Machine Translation model (NMT). In total, we evaluated 16 models. Phrase-based Machine Translation We used a standard PBMT system built using Moses toolkit (Koehn et al., 2007). At training time, we extract and score phrase sentences up to the size of 9 tokens. All the feature functions were trained using the gold-standard alignments from the training set and their weights were tuned on the development data using k-batch MIRA with k = 60 (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. Lexicalised reordering models were bidirectional. At decoding time, we use a stack size of 1000. Our language model P (e) is a 5-gram LM trained on the Gigaword Third Edition corpus using KenLM (Heafield et al.,"
W17-3501,P17-1014,0,0.29097,"y because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of generating coherent natural language text from non-linguistic data (Reite"
W17-3501,W07-0734,0,0.504946,"attens’ the AMR in a specific order. Com1 Proceedings of The 10th International Natural Language Generation conference, pages 1–10, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics Figure 1: Example of an AMR bining all possibilities gives rise to 23 = 8 AMR preprocessing strategies, which we evaluate for two different MT systems: PBMT and NMT. Following earlier work in AMR-to-text generation and the MT literature, we evaluate the system outputs in terms of fluency, adequacy and post-editing effort, using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) scores, respectively. We show that preprocessing helps, although the extent of the benefits differs for the two MT systems. 2 Related Studies To the best of our knowledge, Flanigan et al. (2016) was the first study that introduced a model for natural language generation from AMRs. The model consists of two steps. First, the AMR-graph is converted into a spanning tree, and then, in a second step, this tree is converted into a sentence using a tree transducer. In Song et al. (2016), the generation of a sentence from an AMR is addressed as an asymmetric generalised"
W17-3501,D13-1049,0,0.0155441,"gure 1 for instance, the first edge :ARG1 is aligned to the preposition to from the sentence, whereas the second edge with a similar value is not aligned to any word in the sentence. Therefore, we need to train a classifier to de3 Linearisation After Compression, we flatten the AMR to serve as input to the translation step, similarly as proposed in Pourdamghani et al. (2016). We perform a depthfirst search through the AMR, printing the elements according to their visiting order. In a second step, also following Pourdamghani et al. (2016), we implemented a version of the 2-Step Classifier from Lerner and Petrov (2013) to preorder the elements from an AMR according to the target side. 2-Step Classifier We implement the preordering method proposed by Lerner and Petrov (2013) in the following way. We define the order among a head node and its subtrees in two steps. In the first, we use a trained maximum entropy classifier to predict for each subtree whether it should occur before or after the head node. As features, we represent the head node by its frameset, whereas the subtree is represented by its head node frameset and parent edge. Once we divide the subtrees into the ones which should occur before and af"
W17-3501,P03-1021,0,0.0392383,"use them with the same training data as our models. For Flanigan et al. (2016), we specifically use the version available on GitHub2 . For Pourdamghani et al. (2016), we use the version available at the first author’s website3 . The rules used for the preordering model and the feature functions from the PBMT system are trained using alignments over AMR–sentence pairs from the training set obtained with the aligner described by Pourdamghani et al. (2014). We do not use lexicalised reordering models as Pourdamghani et al. (2016). Moreover, we tune the weights of the feature functions with MERT (Och, 2003). Both models make use of a 5-gram language model trained on Gigaword Third Edition corpus with KenLM. 4.4 (Flanigan et al., 2016) (Pourdamghani et al., 2016) (Konstas et al., 2017) (Song et al., 2016) (Song et al., 2017) (Flanigan et al., 2016) (Pourdamghani et al., 2016) (Konstas et al., 2017) Results Table 1 depicts the scores of the different models by the size of the data they were trained on. For illustration, we depicted the BLEU scores of all the AMR-to-text systems described in the literature. The models of Flanigan et al. (2016) and Pourdamghani et al. (2016) were officially trained"
W17-3501,P02-1040,0,0.0995941,"xtual side and linearisation ‘flattens’ the AMR in a specific order. Com1 Proceedings of The 10th International Natural Language Generation conference, pages 1–10, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics Figure 1: Example of an AMR bining all possibilities gives rise to 23 = 8 AMR preprocessing strategies, which we evaluate for two different MT systems: PBMT and NMT. Following earlier work in AMR-to-text generation and the MT literature, we evaluate the system outputs in terms of fluency, adequacy and post-editing effort, using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) scores, respectively. We show that preprocessing helps, although the extent of the benefits differs for the two MT systems. 2 Related Studies To the best of our knowledge, Flanigan et al. (2016) was the first study that introduced a model for natural language generation from AMRs. The model consists of two steps. First, the AMR-graph is converted into a spanning tree, and then, in a second step, this tree is converted into a sentence using a tree transducer. In Song et al. (2016), the generation of a sentence from an AMR is addre"
W17-3501,D14-1048,0,0.30484,"guage. Motivated by this similarity, Pourdamghani et al. (2016) proposed an AMR-to-text method that organises some of these concepts and edges in a flat representation, commonly known as Linearisation. Once the linearisation is complete, Pourdamghani et al. (2016) map the flat AMR into an English sentence using 2 a Phrase-Based Machine Translation (PBMT) system. This method yields better results than Flanigan et al. (2016) on development and test set from the LDC2014T12 corpus. Pourdamghani et al. (2016) train their system using a set of AMR-sentence pairs obtained by the aligner described in Pourdamghani et al. (2014). In order to decrease the sparsity of the AMR formalism caused by the ratio of broad vocabulary and relatively small amount of data, this aligner drops a considerable amount of the AMR structure, such as role edges :ARG0, :ARG1, :mod, etc. However, inspection of the gold-standard alignments provided in the LDC2016E25 corpus revealed that this rulebased compression can be harmful for the generation of sentences, since such role edges can actually be aligned to function words in English sentences. So having these roles available arguably could improve AMR-to-text translation. This indicates tha"
W17-3501,W16-6603,0,0.0593257,"increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of generating coherent natural lan"
W17-3501,2006.amta-papers.25,0,0.552287,"r. Com1 Proceedings of The 10th International Natural Language Generation conference, pages 1–10, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics Figure 1: Example of an AMR bining all possibilities gives rise to 23 = 8 AMR preprocessing strategies, which we evaluate for two different MT systems: PBMT and NMT. Following earlier work in AMR-to-text generation and the MT literature, we evaluate the system outputs in terms of fluency, adequacy and post-editing effort, using BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) scores, respectively. We show that preprocessing helps, although the extent of the benefits differs for the two MT systems. 2 Related Studies To the best of our knowledge, Flanigan et al. (2016) was the first study that introduced a model for natural language generation from AMRs. The model consists of two steps. First, the AMR-graph is converted into a spanning tree, and then, in a second step, this tree is converted into a sentence using a tree transducer. In Song et al. (2016), the generation of a sentence from an AMR is addressed as an asymmetric generalised traveling salesman problem (AG"
W17-3501,D16-1224,0,0.492636,"Abstract AMRs have increased in popularity in recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of gene"
W17-3501,P17-2002,0,0.319087,"recent years, partly because they are relatively easy to produce, to read and to process automatically. In addition, they can be systematically translated into firstorder logic, allowing for a well-specified modeltheoretic interpretation (Bos, 2016). Most earlier studies on AMRs have focused on text understanding, i.e. processing texts in order to produce AMRs (Flanigan et al., 2014; Artzi et al., 2015). However, recently the reverse process, i.e. the generation of texts from AMRs, has started to receive scholarly attention (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016; Song et al., 2017; Konstas et al., 2017). In this paper, we study AMR-to-text generation, framing it as a translation task and comparing two different MT approaches (Phrasebased and Neural MT). We systematically study the effects of 3 AMR preprocessing steps (Delexicalisation, Compression, and Linearisation) applied before the MT phase. Our results show that preprocessing indeed helps, although the benefits differ for the two MT models. The implementation of the models are publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of generating coherent natural language text from non"
W17-3501,R15-1080,1,0.858962,"alignments provided in the LDC2016E25 corpus revealed that this rulebased compression can be harmful for the generation of sentences, since such role edges can actually be aligned to function words in English sentences. So having these roles available arguably could improve AMR-to-text translation. This indicates that a better comparison of the effects of different preprocessing steps is called for, which we do in this study. In addition, Pourdamghani et al. (2016) use PBMT, which is devised for translation but also utilised in other NLP tasks, e.g. text simplification ˇ (Wubben et al., 2012; Stajner et al., 2015). However, these systems have the disadvantage of having many different feature functions, and finding optimal settings for all of them increases the complexity of the problem from an engineering point of view. An alternative MT model has been proposed: Neural Machine Translation (NMT). NMT models frame translation as a sequence-to-sequence problem (Bahdanau et al., 2015), and have shown strong results when translating between many different language pairs (Bojar et al., 2015). Recently, Konstas et al. (2017) introduce sequence-to-sequence models for parsing (text-to-AMR) and generation (AMR-t"
W17-3501,P12-1107,1,0.896555,"Missing"
W17-3513,D10-1049,0,0.0195235,"tone would be more upbeat. The language of these reports was made to look similar to the reports written by professional journalists by using a corpus-driven approach in the development of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly div"
W17-3513,H05-1042,0,0.0741482,"d recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categories. The first category is the commentary category. Systems in this category produce texts in a style that is similar to the live commentary that can be heard when watching a live sports event. This means that content selection and organization is relatively simple: most, if not all, observable events are covered and this is done in a chronological order. Examples of data-to-text systems that fall into this category are Tanaka-Ishii et al. (1998), Chen and Mooney (2008), and Konstas and Lapata (2012), which all p"
W17-3513,W10-4217,0,0.030579,"audience wins, the tone would be more upbeat. The language of these reports was made to look similar to the reports written by professional journalists by using a corpus-driven approach in the development of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports doma"
W17-3513,W16-6612,1,0.344129,"1. While most of this Goal.com-specific data has not been used in the current version of PASS, the availability of this information makes it relatively easy to use this data in future versions. 3.2 Designing the templates With PASS, an attempt was made to produce reports where the tone of voice is emotional, while 4 http://www.goal.com 97 the report still appears to be relatively professional. The language in the templates therefore needed to be close to what could be encountered in humanwritten soccer reports. To achieve this, the templates were derived from sentences in the MeMo FC corpus (Braun et al., 2016). The MeMo FC corpus contains match reports copied directly from the websites of the soccer clubs that participated in the match. These reports are intended for the supporters of their respective club and often contain an emotional tone. These characteristics made the corpus particularly suitable for PASS. Three steps were undertaken to convert reports in the MeMo FC corpus to templates. The first step was to manually label a sample of reports in the corpus: for each sentence in the sample, we examined what event it described. This first step was done to cluster sentences that described simila"
W17-3513,P16-1054,1,0.825453,"iller modules to get an alternative template for the template with redundant information. The information variety module keeps going through the finished report until it cannot find any more redundant information. Like repetition of information, repetition of references can also have a negative impact on the text quality of the report. This can be observed in the following example: (6) works for the current version of PASS, it is possible that the module is too simple for longer, more complicated reports. Therefore, this module will probably be replaced by a probabilistic module as is seen in Ferreira et al. (2016) in future versions of PASS. Finally, we wanted to demonstrate the variety in outcomes PASS can generate. Therefore, the between-text variety module was implemented. This module keeps track of the templates that were used when generating a soccer report. When generating a new report, this module interacts with the template selection module, deleting all templates from the possible_templates list if they had been used in the previous soccer report. This ensures that every generated report is completely different from the previous one, thus increasing overall variety. ""Ajax obtained the victory"
W17-3513,P16-2043,0,0.0117358,"beat. The language of these reports was made to look similar to the reports written by professional journalists by using a corpus-driven approach in the development of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categori"
W17-3513,P13-1138,0,0.0567759,"Missing"
W17-3513,N12-1093,0,0.0146779,"t pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categories. The first category is the commentary category. Systems in this category produce texts in a style that is similar to the live commentary that can be heard when watching a live sports event. This means that content selection and organization is relatively simple: most, if not all, observable events are covered and this is done in a chronological order. Examples of data-to-text systems that fall into this category are Tanaka-Ishii et al. (1998), Chen and Mooney (2008), and Konstas and Lapata (2012), which all produce soccer reports. The second, summary category could provide a bigger challenge for content selection and organization. These texts are more similar to texts that can be read in newspapers or websites after the sports event and should provide a report on the most interesting elements of the game. This means that content selection is more important and a chronological order is not necessarily used. Examples of systems in this category are Robin (1994), and McKeown et al. 96 (1995), which produced basketball reports, and Theune et al. (2001), and Barzilay and Lapata (2005), whi"
W17-3513,W17-1603,0,0.0158785,"er, it is important to stress that PASS is a reimplementation of GoalGetter, not a replication. We aimed to make PASS generate soccer reports somewhat similar to those generated by GoalGetter, but we did not use any of the source code of GoalGetter. Instead, we have built a new system from the ground up, using the description and results of GoalGetter as inspiration, while simultaneously adding new techniques to emphasize the variety of the system’s output. In the last few years, people have become increasingly interested in replicating published research (Ioannidis, 2005; Nosek et al., 2015; Mieskes, 2017, among others). However, in order to replicate previous studies in this field, reimplementation of previous systems is often necessary. Many older systems such as GoalGetter have become abandonware. They are not (any longer) publicly available, their code is obsolete, and sometimes have never been properly evaluated. Reimplementation is required for these older systems before they can be replicated. Therefore, our goal was to develop a system according to modern standards that produces output that is similar to GoalGetter. Furthermore, we have made our implementation publicly available3 , and"
W17-3513,W16-3507,0,0.0889686,"pment of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categories. The first category is the commentary category. Systems in this category produce texts in a style that is similar to the live commentary that can be hea"
W17-3513,W07-2315,0,0.0482637,"nted or frustrated and if the 95 Proceedings of The 10th International Natural Language Generation conference, pages 95–104, c Santiago de Compostela, Spain, September 4-7 2017. 2017 Association for Computational Linguistics club of the targeted audience wins, the tone would be more upbeat. The language of these reports was made to look similar to the reports written by professional journalists by using a corpus-driven approach in the development of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization"
W17-3513,W16-3510,0,0.0731213,"and Lapata (2005), which produced reports on soccer matches. The current system falls in the latter category. PASS is a data-to-text system that produces Dutch summaries of soccer matches and that uses a template-based approach. Template-based systems can generally be characterized by their slot-filler structure: texts with gaps that can be filled with information. while this approach is sometimes contrasted with “real” NLG, research has shown that template-based approaches generally result in texts of relatively high quality (van Deemter et al., 2005), that are generated relatively quickly (Sanby et al., 2016). The current project is in line with the ongoing concerns about replication in science in general. However, it is important to stress that PASS is a reimplementation of GoalGetter, not a replication. We aimed to make PASS generate soccer reports somewhat similar to those generated by GoalGetter, but we did not use any of the source code of GoalGetter. Instead, we have built a new system from the ground up, using the description and results of GoalGetter as inspiration, while simultaneously adding new techniques to emphasize the variety of the system’s output. In the last few years, people hav"
W17-3513,W13-2119,0,0.0301439,"al journalists by using a corpus-driven approach in the development of the PASS system. 2 Related work Data-to-text systems, systems that ""generate texts from non-linguistic data, such as sensor data and event logs"" (Reiter, 2007, p. 97), have been around for a long time and still remain a popular topic for Natural Language Generation. Some of the datato-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others). The domain of sports is also a domain that is investigated quite frequently. This domain is appealing because the content organization could be (partly) fixed for many sports. At the same time, the sports domain is complex enough that it gives rise to many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categories. The first category is the commentary category. Systems in this category produce texts in a styl"
W17-3513,P98-2209,0,0.157012,"many challenges at almost every stage of the datato-text pipeline (Barzilay and Lapata, 2005). Datato-text systems in the sports domain can be roughly divided into two categories. The first category is the commentary category. Systems in this category produce texts in a style that is similar to the live commentary that can be heard when watching a live sports event. This means that content selection and organization is relatively simple: most, if not all, observable events are covered and this is done in a chronological order. Examples of data-to-text systems that fall into this category are Tanaka-Ishii et al. (1998), Chen and Mooney (2008), and Konstas and Lapata (2012), which all produce soccer reports. The second, summary category could provide a bigger challenge for content selection and organization. These texts are more similar to texts that can be read in newspapers or websites after the sports event and should provide a report on the most interesting elements of the game. This means that content selection is more important and a chronological order is not necessarily used. Examples of systems in this category are Robin (1994), and McKeown et al. 96 (1995), which produced basketball reports, and Th"
W17-3513,C98-2204,0,\N,Missing
W18-3604,D08-1089,0,0.0169526,"in the tokenized development sets. Language Dutch English French Italian Portuguese Spanish BLEU 32.28 55.29 52.03 44.46 30.82 49.47 DIST 57.81 79.29 55.54 58.61 60.70 51.73 NIST 8.05 10.86 9.85 9.11 7.55 11.12 Table 2: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets. MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrasemsd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5-gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Results and Discussion Table 1 summarizes the BLEU scores we obtained on the tokenized development data for the 6 relevant languages. For all languages (except Dutch) our approach yielded BLEU scores of 50 or higher, with the highest results obtained for French (with a BLEU score of 59). Table 2 depicts the BLEU, DIST and NIST scores of our approach on the test sets for the 6 target languages. For most lang"
W18-3604,W08-0509,0,0.0293903,", 2007). The MT model aims to convert a linearized dependency tree generated during the preprocessing step into text, adding the proper punctuation marks. Most of the model settings were copied from the Statistical MT system introduced in Castro Ferreira et al. (2017). At training time, we extract and score phrases up to the size of nine tokens. As feature functions, we used direct and inverse phrase translation probabilities and lexical weighting, as well as word, unknown word and phrase penalties. These feature functions were trained using alignments from the training set obtained by MGIZA (Gao and Vogel, 2008) (not by the ones extracted according to Section 2). Model weights were tuned on the development data using 60-batch Our model is based on the NLG approach introduced in Castro Ferreira et al. (2017), where a semantic graph structure is first preprocessed into a preordered linearized form, which is then converted into its textual counterpart using a statistical machine translation model implemented with Moses. However for this task, instead of a semantic structure, our approach takes as input a lemmatized dependency tree. In the next sections, we explain the preprocessing and translation phase"
W18-3604,W11-2123,0,0.0239163,".11 7.55 11.12 Table 2: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets. MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrasemsd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5-gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Results and Discussion Table 1 summarizes the BLEU scores we obtained on the tokenized development data for the 6 relevant languages. For all languages (except Dutch) our approach yielded BLEU scores of 50 or higher, with the highest results obtained for French (with a BLEU score of 59). Table 2 depicts the BLEU, DIST and NIST scores of our approach on the test sets for the 6 target languages. For most languages, the BLEU scores on development and test set are comparable, albeit somewhat lower. The scores for Portuguese, however, are substantially lower, which we explain as follows. In con"
W18-3604,2005.mtsummit-papers.11,0,0.0150058,".73 NIST 8.05 10.86 9.85 9.11 7.55 11.12 Table 2: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets. MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrasemsd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5-gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Results and Discussion Table 1 summarizes the BLEU scores we obtained on the tokenized development data for the 6 relevant languages. For all languages (except Dutch) our approach yielded BLEU scores of 50 or higher, with the highest results obtained for French (with a BLEU score of 59). Table 2 depicts the BLEU, DIST and NIST scores of our approach on the test sets for the 6 target languages. For most languages, the BLEU scores on development and test set are comparable, albeit somewhat lower. The scores for Portuguese, however, are substantially lower, which"
W18-3604,P07-2045,0,0.00411119,"lemmas as well as the dependency and part-of-speech tags of the head and the two child nodes involved in each comparison. 3 3.2 Partial realization aims to partially realize the lemmas in the linearized representation. For each language, it uses a lexicon created based on the aligned information extracted from the datasets, as explained in Section 2. Given a lemma and its features, our approach looks for the most likely morphological form in the lexicon. Model For each one of the 6 languages which our approach covers, we built a phrase-based machine translation model using the Moses toolkit (Koehn et al., 2007). The MT model aims to convert a linearized dependency tree generated during the preprocessing step into text, adding the proper punctuation marks. Most of the model settings were copied from the Statistical MT system introduced in Castro Ferreira et al. (2017). At training time, we extract and score phrases up to the size of nine tokens. As feature functions, we used direct and inverse phrase translation probabilities and lexical weighting, as well as word, unknown word and phrase penalties. These feature functions were trained using alignments from the training set obtained by MGIZA (Gao and"
W18-3604,W18-3601,0,0.0510921,"017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 40 for 4 different languages in development and test sets (English, French, Italian and Spanish) and above 30 for the Dutch and Portuguese languages. The model is publicly available1 . 1 Introduction This study presents the approach developed by the Tilburg University team for the shallow track of the Multilingual Surface Realization Shared Task 2018 (SR18) (Mille et al., 2018). Given a lemmatized dependency tree without word order information, the goal of this task consists of linearizing the lemmas in the correct order and realizing them as a surface string with the proper morphological form. For the task, parallel datasets were provided for 10 different languages and we developed our model for 6 out of the 10 languages (Dutch, English, French, Italian, Portuguese, Spanish). We started from the surface realization approach described in Castro Ferreira et al. (2017), where a semantic graph structure is first preprocessed into a preordered linearized form, which is"
W18-3604,W17-3501,1,0.937963,", with BLEU scores higher than 40 in development and test sets. In the remainder of this paper, we describe the method in more detail: Section 2 explains the alignment method, Section 3 describes the general approach, Section 4 describes the results and discussion of our approach in development and test sets and, finally, Section 5 concludes the study, also describing future work which can be done to improve the model. This study describes the approach developed by the Tilburg University team to the shallow track of the Multilingual Surface Realization Shared Task 2018 (SR18). Based on Castro Ferreira et al. (2017), the approach works by first preprocessing an input dependency tree into an ordered linearized string, which is then realized using a statistical machine translation model. Our approach shows promising results, with BLEU scores above 40 for 4 different languages in development and test sets (English, French, Italian and Spanish) and above 30 for the Dutch and Portuguese languages. The model is publicly available1 . 1 Introduction This study presents the approach developed by the Tilburg University team for the shallow track of the Multilingual Surface Realization Shared Task 2018 (SR18) (Mill"
W18-3604,N12-1047,0,0.0339958,"odes ← ordN odes ∪ node2 48: else 49: ordN odes ← ordN odes ∪ node2 50: ordN odes ← ordN odes ∪ node1 51: end if 52: end if 53: end while 54: return ordN odes 55: end function 56: 57: LINEAR(depT ree.root, 0) BLEU 35.26 58.92 59.28 50.33 54.76 54.88 Table 1: BLEU scores of our approach in the tokenized development sets. Language Dutch English French Italian Portuguese Spanish BLEU 32.28 55.29 52.03 44.46 30.82 49.47 DIST 57.81 79.29 55.54 58.61 60.70 51.73 NIST 8.05 10.86 9.85 9.11 7.55 11.12 Table 2: BLEU, DIST and NIST scores of our approach in the original (non-tokenized) test sets. MIRA (Cherry and Foster, 2012) with BLEU as the evaluation metric. A distortion limit of 6 was used for the reordering models. We used two lexicalized reordering models: a phrase-level (phrasemsd-bidirectional-fe) (Koehn et al., 2005) and a hierarchical-level one (hier-mslr-bidirectional-fe) (Galley and Manning, 2008). At decoding time, we used a stack size of 1000. To rerank the candidate texts, we used a 5-gram language model trained on the EuroParl corpus (Koehn, 2005) using KenLM (Heafield, 2011). 4 Results and Discussion Table 1 summarizes the BLEU scores we obtained on the tokenized development data for the 6 relevan"
W18-3910,W16-6615,0,0.156205,"Missing"
W18-3910,P17-1057,0,0.0231859,"Gella and Mitchell, 2016), and indeed apps are starting to appear which describe visual content for blind users (e.g. TapTapSee or Microsoft’s Seeing AI 1 ). These apps are commonly used together with screen readers, which convert on-screen text to speech. Given this presentation through speech, it is worth asking: should we not also collect spoken rather than written training data? That might give us more natural-sounding descriptions. But a big downside of collecting spoken training data is that it also requires a costly transcription procedure (unless we go for an end-to-end approach, see Chrupała et al., 2017). An alternative is to try to understand what the differences are between written and spoken image descriptions. Once we know those differences, and we know what kind of descriptions users prefer, we may be able to direct image description systems to produce more human-like descriptions, similar to the way we can modify the style of the descriptions, for example with positive/negative sentiment (Mathews et al., 2016), or humorous descriptions (Gan et al., 2017). This paper presents an exploratory study of the differences between spoken and written image descriptions, for two languages: English"
W18-3910,W16-3210,0,0.0766257,"Missing"
W18-3910,P17-1047,0,0.0698323,"Missing"
W18-3910,J84-3009,0,0.597165,"Average token length Drieman (1962a) and others have found that the tokens in spoken language are shorter than those in written language. We measure token length in terms of syllables (following e.g. Drieman 1962a) and characters (following e.g. Biber 1988), using Hunspell to obtain the syllables.8 2. Average description length Drieman (1962a) and others have shown that spoken language has a higher sentence length than written language. We measure description length in tokens and syllables. 3. Mean-segmental type-token ratio (MSTTR) corresponds to the average number of types per 1000 tokens (Johnson, 1944). It is used as a measure of lexical variation. Because it is computed for a fixed number of tokens, it is unaffected by corpus size or sentence length. Drieman (1962a) shows that written language is more diverse than spoken language. One issue is that the Places Audio Caption Corpus has only one description per image, versus five descriptions per image for MS COCO and Flickr30K. This means that for every description in Flickr30K or MS COCO, there are four very similar descriptions, which makes these corpora less diverse overall. For a fair comparison, we treat Flickr30K and MS COCO as collect"
W18-3910,D14-1086,0,0.0388238,"t is impossible to control the quality of the resulting descriptions. So they, like Mathews et al. (2016), further changed the description task to a description editing task. Content. Gella and Mitchell (2016) emphasize the importance of emotional or descriptive content and humor in the image, and explicitly ask for these to be annotated. This makes the elicited descriptions useful for training an assistive image description system which can provide descriptions for blind people. Other. Baltaretu and Castro Ferreira (2016) present variations on an object description task (the ReferIt task, by Kazemzadeh et al. (2014)). The authors show that asking participants to work very fast, or produce thorough or creative descriptions, results in very different kinds of descriptions. While the studies listed above cover a wide range of variables, there are many more possibilities that are still unexplored. Van Miltenburg et al. (2017) provide a (non-exhaustive) list of other factors that may influence the image description process. This paper aims to identify the role of modality. 3 Theoretical background: Spoken versus written language The differences between spoken and written language have been thoroughly studied"
W18-3910,W16-4801,0,0.0701695,"Missing"
W18-3910,P16-1168,0,0.0207767,"controlled replication study, and follow-up studies to assess what kind of descriptions users prefer. All of our code and data is available online.2 2 Technical background: Manipulating the image description task Recently, researchers have started to manipulate the image description task to obtain a better understanding of how this influences the resulting descriptions. This section presents a brief list of variables that have been considered in the literature. Language. The most common modification is the language in which the task is carried out (e.g. Elliott et al., 2016; Li et al., 2016; Miyazaki and Shimizu, 2016). This is typically done to be able to train an image description system in a different language, but van Miltenburg et al. (2017) use this manipulation to show that speakers of different languages may also provide different descriptions. For example, speakers of American English described sports fans barbecuing on a parking lot as tailgating, a concept unknown to Dutch and German speakers. Style. Another possible manipulation is the requested style of the descriptions. Gan et al. (2017) asked crowd workers to provide ‘humorous’ and ‘romantic’ descriptions, but found that it is impossible to c"
W18-3910,W16-3207,1,0.699007,"Missing"
W18-3910,W17-3503,1,0.84328,"Missing"
W18-3910,C18-1310,1,0.813405,"Missing"
W18-3910,Q14-1006,0,0.106224,"stigates whether there are differences between written and spoken image descriptions, even if they are elicited through similar tasks. We compare descriptions produced in two languages (English and Dutch), and in both languages observe substantial differences between spoken and written descriptions. Future research should see if users prefer the spoken over the written style and, if so, aim to emulate spoken descriptions. 1 Introduction Automatic image description systems (Bernardi et al., 2016) are commonly trained and evaluated on datasets of described images, such as Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). These datasets have been collected by asking workers on Mechanical Turk to write English descriptions that capture the contents of the images that are presented to them. But how much are these descriptions influenced by the modality of the task? This paper explores the differences between spoken and written image descriptions. While many papers at VarDial aim to distinguish similar dialects using machine learning (e.g. in shared tasks such as those in Malmasi et al., 2016; Zampieri et al., 2017), we aim to identify the features distinguishing two similar varieties (spoken"
W18-3910,W17-1201,0,0.0474993,"Missing"
W18-6504,W02-2112,0,0.0984429,"systems are difficult to extend to other domains. Statistical approaches may provide a solution for these shortcomings. These approaches are trained using a parallel corpus, thus require no handcrafted rules. This also makes conversion to other domains less time-intensive compared to rule-based approaches. 2.2 Trainable approaches Producing output by using such trainable approaches can be exercised in different ways. Retrieval-based models (e.g. Adeyanju, 2012), statistical approaches, such as Hidden Markov Models (e.g. Barzilay and Lee, 2004; Liang et al., 2009), and classification methods (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) have all been successfully implemented. Another way of approaching the problem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising results. Furthermore, an SMT model was consistently among the higher scores"
W18-6504,W17-3518,0,0.104529,"lem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising results. Furthermore, an SMT model was consistently among the higher scores in the WEB NLG Challenge, where the goal is to convert RDF data to text (Castro Ferreira et al., 2017; Gardent et al., 2017), thus showing the potential of SMT-based methods as a viable approach to data-to-text NLG. However, this SMT approach was less successful in other studies in which the SMT-based method was often outscored by other statistical approaches according to automated metrics as well as human evaluation (Belz and Kow, 2010). The impressive performance of deep learning 2.3 Current work The current work investigated the potential limitations of automatically generated corpora by using several corpora with differing characteristics, but also attempted to address the issue of small datasets by exploring t"
W18-6504,H05-1042,0,0.072405,"extend to other domains. Statistical approaches may provide a solution for these shortcomings. These approaches are trained using a parallel corpus, thus require no handcrafted rules. This also makes conversion to other domains less time-intensive compared to rule-based approaches. 2.2 Trainable approaches Producing output by using such trainable approaches can be exercised in different ways. Retrieval-based models (e.g. Adeyanju, 2012), statistical approaches, such as Hidden Markov Models (e.g. Barzilay and Lee, 2004; Liang et al., 2009), and classification methods (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) have all been successfully implemented. Another way of approaching the problem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising results. Furthermore, an SMT model was consistently among the higher scores in the WEB NLG Challenge, w"
W18-6504,N04-1015,0,0.352,"more, developing and maintaining these systems is cost intensive and most systems are difficult to extend to other domains. Statistical approaches may provide a solution for these shortcomings. These approaches are trained using a parallel corpus, thus require no handcrafted rules. This also makes conversion to other domains less time-intensive compared to rule-based approaches. 2.2 Trainable approaches Producing output by using such trainable approaches can be exercised in different ways. Retrieval-based models (e.g. Adeyanju, 2012), statistical approaches, such as Hidden Markov Models (e.g. Barzilay and Lee, 2004; Liang et al., 2009), and classification methods (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) have all been successfully implemented. Another way of approaching the problem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising"
W18-6504,W09-0603,0,0.0265758,"anju, 2012), statistical approaches, such as Hidden Markov Models (e.g. Barzilay and Lee, 2004; Liang et al., 2009), and classification methods (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) have all been successfully implemented. Another way of approaching the problem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising results. Furthermore, an SMT model was consistently among the higher scores in the WEB NLG Challenge, where the goal is to convert RDF data to text (Castro Ferreira et al., 2017; Gardent et al., 2017), thus showing the potential of SMT-based methods as a viable approach to data-to-text NLG. However, this SMT approach was less successful in other studies in which the SMT-based method was often outscored by other statistical approaches according to automated metrics as well as human evaluation (Belz and Kow, 2010). The i"
W18-6504,P17-4012,0,0.0319136,"n showers .) Figure 2: Templatization method of data-to-text conversion. Table 7: OpenNMT parameters per corpus. 3.2.2 Templatized data Retrieval/ Data input reTemplatized natural input reStatistical Machine presentation (e.g. language output presentation (e.g. Translation/ rainChance_mode: (e.g. <weather_type_ <weather&gt;Chance_ Neural Machine Def) and_chance&gt; .) mode: <chance&gt;) Translation Neural Machine Translation 5 Besides Statistical Machine Translation, a Neural Machine Translation approach was explored as well for the current work. These models were trained using the OpenNMT-py toolkit (Klein et al., 2017). Parameters were chosen using the same Bayesian optimization method as was used for SMT. For the smaller corpora (i.e. ProdigyMETEO and Robocup), pre-trained word embeddings were also added to the train model, since these are known to boost performance in lowresource scenarios (Qi et al., 2018). The detailed parameter settings are in Table 7. Results automated evaluation The quality of the generated sentences was assessed using NLTK’s corpus bleu that calculates BLEU scores based on 1-grams to 4-grams with equal weights and accounts for a micro-average precision score based on Papineni et al."
W18-6504,E06-1040,0,0.0592095,"equal. The exception involves the Weather.gov corpus, where the direct method resulted in much higher BLEU scores compared to its templatized counterpart. Although the results are equal, the metrics show a large decrease in BLEU scores when lexicalizing the templates. This means that the templatization method has the potential to significantly outperform the direct method if the quality of the lexicalization step is improved. See Table 8. — for translation tasks, which the current task is in some way. Furthermore, correlations have been found between automated metrics and human ratings (e.g. Belz and Reiter, 2006). Therefore, the BLEU scores were seen as a first step to investigate differences between methods and corpora. The BLEU scores show that the computergenerated corpora yielded the best results, with Weather.gov showing the best performance compared to the other corpora with BLEU scores for the lexicalized output varying from 34.52 (retrieval using the templatization method) to 78.90 (NMT using the direct method). This seems intuitively logical since the Weather.gov corpus is relatively large, and the sentences are also the most homogeneous out of the corpora, which makes producing output simila"
W18-6504,P07-2045,0,0.00527559,"ntence texts similar to the ones obtained with the templatization of the text. These obtained templates were finally lexicalized again using similar rules used for the templatization step. Using the original data, gaps were filled with the appropriate information. If multiple options were available to fill the gaps, a weighted random choice was made based on the occurrences of the possibilities in the training set (see figure 2). Thus, after these steps full natural language sentences were created based on a set of (templatized) data.3 score. Statistical Machine Translation The MOSES toolkit (Koehn et al., 2007) was used for SMT. This Statistical Machine Translation system uses Bayes’s rule to translate a source language string into a target language string. For this, it needs a translation model and a language model. The translation model was obtained from the parallel corpora described above, while the language model used in the current work is obtained from the text part of the aligned corpora. Translation in the MOSES toolkit is based on a set of heuristics. Parameters of these heuristics were tuned for each corpus using Bayesian Optimization2 (Snoek et al., 2012). The parameters that returned th"
W18-6504,P13-1138,0,0.0907561,"rainable approaches to find data-text connections more quickly. This means that the trainable approaches could be more 36 Data type Original input representation Tagged input representation Templatized tagged input representation Retrieval (direct) Retrieval (templatized) robust on smaller datasets and datasets with high variety in language. Whether this hypothesis holds true is also investigated using BLEU scores as well as human assessment on clarity, fluency and correctness. Combining trainable approaches with a template representation has been done previously, but such systems are scarce. Kondadadi et al. (2013) are one of the first and only researchers that have attempted this combination. However, their research experimented with automated sentence templatization and sentence aggregation rather than automatically generated sentences from data points. The aim of the current work can be seen as an exploratory first step in building a system that integrates these other automation techniques to generate text from data in a fully unsupervised fashion. Weather.gov Lines Words Tokens Domain Writer type 29,792 258,856 955,959 Weather Computer ProdigyMETEO 601 6,813 32,448 Weather Human Robocup 1,699 9,607"
W18-6504,W17-3513,1,0.889697,"Missing"
W18-6504,W17-3516,0,0.0155112,"xt models performed relatively well on automated metrics as well as human evaluations, although they still noted a significant performance gap between these models and their baselines. One possible reason for this performance difference Wiseman et al. (2017) found might be the nature of the datasets used. The authors noted that their data for one corpus was noisy and that many texts contained information that was not captured in the data. Other authors have also noted that the dataset is often a bottleneck of most trainable approaches, since many aligned datatext corpora are relatively small (Richardson et al., 2017). Furthermore, several data-text aligned corpora used for these tasks are the input and output of a (rule-based) data-to-text system, which means that experiments using these corpora are performing reverse-engineering and that these results may not reflect performance on human-written datasets (Reiter, 2017). and can produce high quality output given sufficient development time and cost. In addition, the output of these approaches is fully controlled by humans, which make them generally accurate in their representation of the data (e.g. van der Lee et al., 2018). However, capturing data using"
W18-6504,C18-1082,1,0.882033,"Missing"
W18-6504,W16-3510,0,0.02787,"s. 1 Introduction Most approaches to data-to-text generation fall into one of two broad categories: rule-based or trainable (Gatt and Krahmer, 2018). Rule-based systems are often characterised by a templatebased design: texts with gaps that can be filled with information. The application of these templates generally results in high quality text (e.g. van Deemter et al., 2005). The text quality of trainable systems — e.g. statistical models that select content based on what is the most likely realization according to probability — is generally lower (Reiter, 1995) and their development slower (Sanby et al., 2016). However, trainable systems use data-driven algorithms and do not rely on manually written resources for text generation, while most template systems require man2 2.1 Background Data-to-text Historically, most data-to-text systems use rulebased approaches which select and fill templates in order to produce a natural language text (e.g. Goldberg et al., 1994; van der Lee et al., 2017) and these approaches are still the most widely used in practical applications (Gkatzia, 2016). This is partly because rule-based approaches are robust 35 Proceedings of The 11th International Natural Language Gen"
W18-6504,P09-1011,0,0.400119,"nd neural methods Chris van der Lee Tilburg University c.vdrlee@tilburguniversity.edu Emiel Krahmer Tilburg University e.j.krahmer@tilburguniversity.edu Sander Wubben Tilburg University s.wubben@tilburguniversity.edu Abstract ually written templates and rules for text generation. This makes trainable systems potentially more adaptable and maintainable. Different approaches have been tried to decrease the building time and cost of data-to-text systems associated with trainable approaches, while limiting the drop in output quality compared to rule-based data-totext systems (e.g. Adeyanju, 2012; Liang et al., 2009; Mahapatra et al., 2016) by experimenting with the trainable method. The goal of the current study was to explore the combination of template and trainable approaches by giving statistical and deep learning-based systems templatized input to create templatized output. The more homogeneous nature of this templatized form was expected to make production of output that is fluent and clear as well as an accurate representation of the data more feasible compared to their untemplatized counterpart, generally used for trainable approaches. Furthermore, the usage of statistical and deep learning meth"
W18-6504,W16-6624,0,0.0143512,"ris van der Lee Tilburg University c.vdrlee@tilburguniversity.edu Emiel Krahmer Tilburg University e.j.krahmer@tilburguniversity.edu Sander Wubben Tilburg University s.wubben@tilburguniversity.edu Abstract ually written templates and rules for text generation. This makes trainable systems potentially more adaptable and maintainable. Different approaches have been tried to decrease the building time and cost of data-to-text systems associated with trainable approaches, while limiting the drop in output quality compared to rule-based data-totext systems (e.g. Adeyanju, 2012; Liang et al., 2009; Mahapatra et al., 2016) by experimenting with the trainable method. The goal of the current study was to explore the combination of template and trainable approaches by giving statistical and deep learning-based systems templatized input to create templatized output. The more homogeneous nature of this templatized form was expected to make production of output that is fluent and clear as well as an accurate representation of the data more feasible compared to their untemplatized counterpart, generally used for trainable approaches. Furthermore, the usage of statistical and deep learning methods reduces the reliance"
W18-6504,D15-1199,0,0.0597614,"Missing"
W18-6504,D17-1238,0,0.0267679,"Missing"
W18-6504,P02-1040,0,0.103039,"n et al., 2017). Parameters were chosen using the same Bayesian optimization method as was used for SMT. For the smaller corpora (i.e. ProdigyMETEO and Robocup), pre-trained word embeddings were also added to the train model, since these are known to boost performance in lowresource scenarios (Qi et al., 2018). The detailed parameter settings are in Table 7. Results automated evaluation The quality of the generated sentences was assessed using NLTK’s corpus bleu that calculates BLEU scores based on 1-grams to 4-grams with equal weights and accounts for a micro-average precision score based on Papineni et al. (2002). Automated metrics such as BLEU have been criticized over the last few years (e.g. Reiter, 2018; Novikova et al., 2017). Especially in the context of NLG. However, Reiter (2018) also suggested that the metric can be used — albeit with caution 3 Code for, and examples of, these steps can be found at https://github.com/TallChris91/ Automated-Template-Learning 2 https://github.com/fmfn/ BayesianOptimization 39 Corpus Weather.gov Prodigy-METEO Robocup Dutch Soccer Templates (unfilled) 63.94 44.47 31.39 2.49 Retrieval Templates (filled) 34.52 27.65 30.73 1.65 Direct 69.57 23.66 22.38 4.99 Template"
W18-6504,N07-1022,0,0.0384064,"based models (e.g. Adeyanju, 2012), statistical approaches, such as Hidden Markov Models (e.g. Barzilay and Lee, 2004; Liang et al., 2009), and classification methods (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) have all been successfully implemented. Another way of approaching the problem is by treating it as a translation challenge, where a machine translation system translates a data representation string into a target language string. Several authors have implemented Statistical Machine Translation (SMT) methods to generate natural language using aligned data-text test sets (e.g. Wong and Mooney, 2007; Belz and Kow, 2009, 2010; Langner et al., 2010; Pereira et al., 2015) all obtaining promising results. Furthermore, an SMT model was consistently among the higher scores in the WEB NLG Challenge, where the goal is to convert RDF data to text (Castro Ferreira et al., 2017; Gardent et al., 2017), thus showing the potential of SMT-based methods as a viable approach to data-to-text NLG. However, this SMT approach was less successful in other studies in which the SMT-based method was often outscored by other statistical approaches according to automated metrics as well as human evaluation (Belz a"
W18-6521,W16-6630,0,0.041037,"erreira1 Diego Moussallem2,3 Sander Wubben1 Emiel Krahmer1 1 Tilburg center for Cognition and Communication (TiCC), Tilburg University, The Netherlands 2 AKSW Research Group, University of Leipzig, Germany 3 Data Science Group, University of Paderborn, Germany {tcastrof,s.wubben,e.j.krahmer}@tilburguniversity.edu moussallem@informatik.uni-leipzig.de Abstract in English, limiting the development of NLGapplications to other languages, which is currently an emerging theme in NLG research community – see, for instance, the increased availability of SimpleNLG tools to languages other than English (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) and the recent Multilingual Surface Realization task (Mille et al., 2018). This paper describes how we addressed the aforementioned issues by enriching the WebNLG corpus with intermediate representations and exploiting the possibilities of automatically translating the corpus to a second language (German). To this end, we first manually replaced all referring expressions in the WebNLG texts with general tags in a process called Delexicalization. The original texts and the delexicalized templates were then translated to the Ge"
W18-6521,W18-3601,0,0.235681,"therlands 2 AKSW Research Group, University of Leipzig, Germany 3 Data Science Group, University of Paderborn, Germany {tcastrof,s.wubben,e.j.krahmer}@tilburguniversity.edu moussallem@informatik.uni-leipzig.de Abstract in English, limiting the development of NLGapplications to other languages, which is currently an emerging theme in NLG research community – see, for instance, the increased availability of SimpleNLG tools to languages other than English (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) and the recent Multilingual Surface Realization task (Mille et al., 2018). This paper describes how we addressed the aforementioned issues by enriching the WebNLG corpus with intermediate representations and exploiting the possibilities of automatically translating the corpus to a second language (German). To this end, we first manually replaced all referring expressions in the WebNLG texts with general tags in a process called Delexicalization. The original texts and the delexicalized templates were then translated to the German language using an existing state-of-art English-German Neural Machine Translation (NMT) system (Sennrich et al., 2017), providing a silve"
W18-6521,W11-2817,0,0.0822755,"llem2,3 Sander Wubben1 Emiel Krahmer1 1 Tilburg center for Cognition and Communication (TiCC), Tilburg University, The Netherlands 2 AKSW Research Group, University of Leipzig, Germany 3 Data Science Group, University of Paderborn, Germany {tcastrof,s.wubben,e.j.krahmer}@tilburguniversity.edu moussallem@informatik.uni-leipzig.de Abstract in English, limiting the development of NLGapplications to other languages, which is currently an emerging theme in NLG research community – see, for instance, the increased availability of SimpleNLG tools to languages other than English (Mazzei et al., 2016; Bollmann, 2011; Vaudry and Lapalme, 2013; Ramos-Soto et al., 2017) and the recent Multilingual Surface Realization task (Mille et al., 2018). This paper describes how we addressed the aforementioned issues by enriching the WebNLG corpus with intermediate representations and exploiting the possibilities of automatically translating the corpus to a second language (German). To this end, we first manually replaced all referring expressions in the WebNLG texts with general tags in a process called Delexicalization. The original texts and the delexicalized templates were then translated to the German language us"
W18-6521,L18-1481,1,0.885031,"Missing"
W18-6521,W17-3501,1,0.845239,"best silver-standard resource currently feasible. The University of Edinburgh system was modeled in a deep encoder attention-decoder architecture. Its translation model was trained on back-translated monolingual data (Sennrich et al., 2016a) in order to augment the training data. To have an open vocabulary, the rare words, which pose a well-known problem in NMT systems, Delexicalization To account for data sparsity and unseen entities, many “end-to-end” NLG models work by first generating a delexicalized template, where references are represented by general tags (Konstas et al., 2017; Castro Ferreira et al., 2017). The referring expressions are only surface realized once the template is generated. Motivated by these studies, we manually delexicalized the training and development parts of the WebNLG corpus, generating gold-standard templates. The test part 2 http://data.statmt.org/wmt17_systems http://matrix.statmt.org/matrix/ systems_list/1869 3 172 Subject Appleton International Airport Greenville, Wisconsin Greenville, Wisconsin Greenville, Wisconsin Appleton International Airport Predicate location isPartOf isPartOf country cityServed Object Greenville, Wisconsin Ellington, Wisconsin Menasha (town),"
W18-6521,W17-5525,0,0.161723,"Missing"
W18-6521,P18-1182,1,0.874224,"Missing"
W18-6521,W17-3521,0,0.0960495,"Missing"
W18-6521,P17-1017,0,0.176875,"es were then translated to the German language using an existing state-of-art English-German Neural Machine Translation (NMT) system (Sennrich et al., 2017), providing a silver-standard version of the corpus in another language. Finally, by automatically processing the original texts and the delexicalized templates for both English and (translated) German versions of the corpus, we obtained a collection of gold-standard referring expressions and discourse orderings. In sum, the main contributions of this study (all publicly available) are: This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a,b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, Lexicalization and Referring Expression Generation. We also produce a silverstandard German translation of the corpus to enable the exploitation of NLG approaches to other languages than English. The enriched corpus is publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of automatically converting non-linguistic data into a linguistic output format (Reiter and Dale, 2000; Gatt and Krahmer, 2018). Recently, the field has seen an incre"
W18-6521,W17-4739,0,0.0537251,"ace Realization task (Mille et al., 2018). This paper describes how we addressed the aforementioned issues by enriching the WebNLG corpus with intermediate representations and exploiting the possibilities of automatically translating the corpus to a second language (German). To this end, we first manually replaced all referring expressions in the WebNLG texts with general tags in a process called Delexicalization. The original texts and the delexicalized templates were then translated to the German language using an existing state-of-art English-German Neural Machine Translation (NMT) system (Sennrich et al., 2017), providing a silver-standard version of the corpus in another language. Finally, by automatically processing the original texts and the delexicalized templates for both English and (translated) German versions of the corpus, we obtained a collection of gold-standard referring expressions and discourse orderings. In sum, the main contributions of this study (all publicly available) are: This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a,b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, Lexical"
W18-6521,W17-3518,0,0.156859,"es were then translated to the German language using an existing state-of-art English-German Neural Machine Translation (NMT) system (Sennrich et al., 2017), providing a silver-standard version of the corpus in another language. Finally, by automatically processing the original texts and the delexicalized templates for both English and (translated) German versions of the corpus, we obtained a collection of gold-standard referring expressions and discourse orderings. In sum, the main contributions of this study (all publicly available) are: This paper describes the enrichment of WebNLG corpus (Gardent et al., 2017a,b), with the aim to further extend its usefulness as a resource for evaluating common NLG tasks, including Discourse Ordering, Lexicalization and Referring Expression Generation. We also produce a silverstandard German translation of the corpus to enable the exploitation of NLG approaches to other languages than English. The enriched corpus is publicly available1 . 1 Introduction Natural Language Generation (NLG) is the process of automatically converting non-linguistic data into a linguistic output format (Reiter and Dale, 2000; Gatt and Krahmer, 2018). Recently, the field has seen an incre"
W18-6521,P16-1009,0,0.021248,"ranslation We translated the original texts and the delexicalized templates by relying on the University of Edinburgh’s Neural MT System for WMT17 (Sennrich et al., 2017; Bojar et al., 2017a). Not only are the training models publicly available2 , but this system is state-of-the-art in translating Englishto-German at the time of writing3 , which guarantees we obtain arguably the best silver-standard resource currently feasible. The University of Edinburgh system was modeled in a deep encoder attention-decoder architecture. Its translation model was trained on back-translated monolingual data (Sennrich et al., 2016a) in order to augment the training data. To have an open vocabulary, the rare words, which pose a well-known problem in NMT systems, Delexicalization To account for data sparsity and unseen entities, many “end-to-end” NLG models work by first generating a delexicalized template, where references are represented by general tags (Konstas et al., 2017; Castro Ferreira et al., 2017). The referring expressions are only surface realized once the template is generated. Motivated by these studies, we manually delexicalized the training and development parts of the WebNLG corpus, generating gold-stand"
W18-6521,P16-1162,0,0.00683737,"ranslation We translated the original texts and the delexicalized templates by relying on the University of Edinburgh’s Neural MT System for WMT17 (Sennrich et al., 2017; Bojar et al., 2017a). Not only are the training models publicly available2 , but this system is state-of-the-art in translating Englishto-German at the time of writing3 , which guarantees we obtain arguably the best silver-standard resource currently feasible. The University of Edinburgh system was modeled in a deep encoder attention-decoder architecture. Its translation model was trained on back-translated monolingual data (Sennrich et al., 2016a) in order to augment the training data. To have an open vocabulary, the rare words, which pose a well-known problem in NMT systems, Delexicalization To account for data sparsity and unseen entities, many “end-to-end” NLG models work by first generating a delexicalized template, where references are represented by general tags (Konstas et al., 2017; Castro Ferreira et al., 2017). The referring expressions are only surface realized once the template is generated. Motivated by these studies, we manually delexicalized the training and development parts of the WebNLG corpus, generating gold-stand"
W18-6521,P09-5002,0,0.0213704,"Elliot See ’s Besatzung war ein Testpilot.”, where the apostrophe (’s) is placed wrongly. The same happens to the sentence “Bill Oddie Tochter ist Kate Hardie”, where the name “Oddie” should have had the “s” in the end of this German sentence. In terms of transliterations, the preposition “von” played a key role in the challenge, as in the case of the reference “University of Texas”, wrongly transliterated to “Universit¨at von Texas” instead of the correct form “Universit¨at Texas”. These problems are well-known in WMT English-German tasks and still take a place even using the best NMT model (Koehn, 2009; Bojar et al., 2017b). 5), the algorithm looks for a remaining instance on the triple set which relates the visited tag with a previous visited one (line 7). If a triple is found (line 8), this one is added on the ordering list (line 10) and removed from the input set (line 11). 6 Discussion Conclusion This study aimed to enrich the WebNLG corpus, facilitating its use in popular tasks of pipeline NLG models as well as in other languages than English. In future work, we envision translating the corpus for other morphologically rich languages, as Brazilian Portuguese (Moussallem et al., 2018)."
W18-6521,P17-1014,0,0.022854,"antees we obtain arguably the best silver-standard resource currently feasible. The University of Edinburgh system was modeled in a deep encoder attention-decoder architecture. Its translation model was trained on back-translated monolingual data (Sennrich et al., 2016a) in order to augment the training data. To have an open vocabulary, the rare words, which pose a well-known problem in NMT systems, Delexicalization To account for data sparsity and unseen entities, many “end-to-end” NLG models work by first generating a delexicalized template, where references are represented by general tags (Konstas et al., 2017; Castro Ferreira et al., 2017). The referring expressions are only surface realized once the template is generated. Motivated by these studies, we manually delexicalized the training and development parts of the WebNLG corpus, generating gold-standard templates. The test part 2 http://data.statmt.org/wmt17_systems http://matrix.statmt.org/matrix/ systems_list/1869 3 172 Subject Appleton International Airport Greenville, Wisconsin Greenville, Wisconsin Greenville, Wisconsin Appleton International Airport Predicate location isPartOf isPartOf country cityServed Object Greenville, Wisconsin Ellin"
W19-8643,J08-4004,0,0.365011,"m, in that different criteria may overlap or be inter-definable. As Gatt and Belz (2010) and Hastie and Belz (2014) suggest, common and shared evaluation guidelines should be developed for each task, and efforts should be made to standardise criteria and naming conventions. In the absence of such guidelines, care should be taken to explicitly define the criteria measured and highlight possible overlaps between them. 4.2 Evaluator agreement The varying opinions of judges are also reflected in low Inter-Annotator Agreement (IAA), where adequate thresholds also tend to be open to interpretation (Artstein and Poesio, 2008). Amidei et al. (2018b) argue that, given the variable nature of natural language, it is undesirable to use restrictive thresholds, since an ostensibly low IAA score could be due to a host of factors, including personal bias. The authors therefore suggest reporting IAA statistics with confidence intervals. However, narrower confidence intervals (suggesting a more precise IAA score) would normally be expected with large samples (e.g., 1000 or more comparisons McHugh, 2012), which are well beyond most sizes reported in our overview (§ 3.4). When the goal of an evaluation is to identify potential"
W19-8643,D19-1052,1,0.784368,"Missing"
W19-8643,W13-5608,0,0.0303571,"Missing"
W19-8643,W15-4708,0,0.124554,"et al., 2017; Sulem et al., 2018; Reiter, 2018, and the discussion in Section 2). Previous studies have also provided overviews of evaluation methods. Gkatzia and Mahamood (2015) focused on NLG papers from 2005-2014; Amidei et al. (2018a) provided a 2013-2018 overview of evaluation in question generation; and Gatt and Krahmer (2018) provided a more general survey of the state-of-the-art in NLG. However, the aim of these papers was to give a structured overview of existing methods, rather than discuss shortcomings and best practices. Moreover, they did not focus on human evaluation. Following Gkatzia and Mahamood (2015), Section 3 provides an overview of current evaluation practices, based on papers from INLG and ACL in 2018. Apart from the broad range of methods used, we also observe that evaluation practices have changed since 2015: for example, there is a significant decrease in the number of papers featuring extrinsic evaluation. This may be caused by the current focus on smaller, decontextualized tasks, which do not take users into account. Building on findings from NLG, but also statistics and the behavioral sciences, Section 4 provides a set of recommendations and best practices for human evaluation i"
W19-8643,W11-2308,0,0.0212041,"Missing"
W19-8643,W08-1120,0,0.042439,"Missing"
W19-8643,hastie-belz-2014-comparative,0,0.0294109,"the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is currently lacking for NLG. One obstacle to addressing the difficulties identified in this section is the lack of a standardised nomenclature for different text quality criteria. This presents a practical problem, in that it is hard to compare evaluation results to previously reported work; but it also presents a theoretical problem, in that different criteria may overlap or be inter-definable. As Gatt and Belz (2010) and Hastie and Belz (2014) suggest, common and shared evaluation guidelines should be developed for each task, and efforts should be made to standardise criteria and naming conventions. In the absence of such guidelines, care should be taken to explicitly define the criteria measured and highlight possible overlaps between them. 4.2 Evaluator agreement The varying opinions of judges are also reflected in low Inter-Annotator Agreement (IAA), where adequate thresholds also tend to be open to interpretation (Artstein and Poesio, 2008). Amidei et al. (2018b) argue that, given the variable nature of natural language, it is"
W19-8643,W17-3904,0,0.0279316,"a continuous (0-100) scale (Graham et al., 2017; Bojar et al., 2016b), similar to Magnitude Estimation (Bard et al., 1996). Zarrieß et al. (2015) used a mouse contingent reading paradigm in an evaluation study of generated text, finding that features recorded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit generated texts. Then processing these edits using post-editing distance measures like Translation Edit Rate (Snover et al., 2006), might be a viable method to investigate the time and cost associated with using a system. While more commonly seen in Machine Translation, authors have explored the use of such metrics in 361 want to compare various versions of their own novel system (e.g. with or without output variation, or relying on different word embedding models, to give j"
W19-8643,W04-3250,0,0.431351,"Missing"
W19-8643,W17-3503,1,0.907118,"Missing"
W19-8643,C18-1082,1,0.915049,"Missing"
W19-8643,W11-2804,1,0.812177,"instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion in the use of single-item scales, unless the construct in question is very simple, clear and onedimensional. Under most conditions, multi-item scales have much higher predictive validity. Using multiple items may well make the"
W19-8643,D17-1238,0,0.261633,"Missing"
W19-8643,N18-2012,0,0.15926,"Missing"
W19-8643,D08-1020,0,0.060535,"This may introduce sampling biases of the kind that have been critiqued in psychology in recent years, where experimental results based on samples of WEIRD (Western, Educated, Industrialised, Rich and Developed) populations may well have given rise to biased models (see, for example, Henrich et al., 2010). evaluation are usually treated as subjective (as in the case of judgments of fluency, adequacy and the like). It is also conceivable that these criteria can be assessed using more objective measures, similar to existing readability measures (e.g., Ambati et al., 2016; Kincaid et al., 1975; Pitler and Nenkova, 2008; Vajjala and Meurers, 2014), where objective text metrics (e.g. average word length, average parse tree height, average number of nouns) are used in a formula, or as features in a regression model, to obtain a score for a text criterion. Similarly, it may be possible to use separate subjective criteria as features in a regression model to calculate overall text quality scores. This would also provide information about the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is cu"
W19-8643,W18-6319,0,0.07533,"Missing"
W19-8643,2006.amta-papers.25,0,0.0928781,"orded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit generated texts. Then processing these edits using post-editing distance measures like Translation Edit Rate (Snover et al., 2006), might be a viable method to investigate the time and cost associated with using a system. While more commonly seen in Machine Translation, authors have explored the use of such metrics in 361 want to compare various versions of their own novel system (e.g. with or without output variation, or relying on different word embedding models, to give just two more or less random examples) to compare them to each other, to some other (‘state-of-the-art’) systems, and/or with respect to one or more baselines. Notice that this quickly gives rise to a rather complex statistical design with multiple fac"
W19-8643,W05-1615,0,0.0446487,"on the task itself, 7-point scales (with clear verbal anchoring) seem best for most tasks. Most of the experimental literature’s findings found that 7-point scales maximise reliability, validity and discriminative power (for instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion"
W19-8643,J09-4008,0,0.43907,"ost of the experimental literature’s findings found that 7-point scales maximise reliability, validity and discriminative power (for instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion in the use of single-item scales, unless the construct in question is very simple, clear and o"
W19-8643,D18-1081,0,0.0869369,"ilburg University c.vdrlee@uvt.nl Albert Gatt University of Malta albert.gatt@um.edu.mt Sander Wubben Tilburg University s.wubben@uvt.nl Emiel Krahmer Tilburg University e.j.krahmer@uvt.nl Abstract the evaluation of NLG systems (see Ananthakrishnan et al., 2007; Novikova et al., 2017; Sulem et al., 2018; Reiter, 2018, and the discussion in Section 2). Previous studies have also provided overviews of evaluation methods. Gkatzia and Mahamood (2015) focused on NLG papers from 2005-2014; Amidei et al. (2018a) provided a 2013-2018 overview of evaluation in question generation; and Gatt and Krahmer (2018) provided a more general survey of the state-of-the-art in NLG. However, the aim of these papers was to give a structured overview of existing methods, rather than discuss shortcomings and best practices. Moreover, they did not focus on human evaluation. Following Gkatzia and Mahamood (2015), Section 3 provides an overview of current evaluation practices, based on papers from INLG and ACL in 2018. Apart from the broad range of methods used, we also observe that evaluation practices have changed since 2015: for example, there is a significant decrease in the number of papers featuring extrinsic"
W19-8643,2003.mtsummit-papers.51,0,0.199911,"es not explain the decline in (relative) frequency. That might be because of the set-up of the tasks we see nowadays. Extrinsic evaluations require that the system is embedded in its target use context (or a suitable simulation thereof), which in turn requires that the system addresses a specific purpose. In practice, this often means the system follows the ‘traditional’ NLG 1 In theory this correlation might increase when more reference texts are used, since this allows for more variety in the generated texts. However, in contrast to what this theory would predict, both Doddington (2002) and Turian et al. (2003) report that correlations between metrics and human judgments in machine translation do not improve substantially as the number of reference texts increases. Similarly, Choshen and Abend (2018) found that reliability issues of reference-based evaluation due to low-coverage reference sets cannot be overcome by attainably increasing references. 2 For the ACL papers, we focused on the following tracks: Machine Translation, Summarization, Question Answering, and Generation. See Supplementary Materials for a detailed overview of the investigated papers and their evaluation characteristics 356 Crite"
W19-8643,E14-1031,0,0.0155286,"ng biases of the kind that have been critiqued in psychology in recent years, where experimental results based on samples of WEIRD (Western, Educated, Industrialised, Rich and Developed) populations may well have given rise to biased models (see, for example, Henrich et al., 2010). evaluation are usually treated as subjective (as in the case of judgments of fluency, adequacy and the like). It is also conceivable that these criteria can be assessed using more objective measures, similar to existing readability measures (e.g., Ambati et al., 2016; Kincaid et al., 1975; Pitler and Nenkova, 2008; Vajjala and Meurers, 2014), where objective text metrics (e.g. average word length, average parse tree height, average number of nouns) are used in a formula, or as features in a regression model, to obtain a score for a text criterion. Similarly, it may be possible to use separate subjective criteria as features in a regression model to calculate overall text quality scores. This would also provide information about the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is currently lacking for NLG. One"
W19-8643,W15-4705,0,0.0209534,"ssing data, or to remove participants that failed ‘attention checks’ (or related checks e.g. instructional manipulation checks, or trap questions) from the sample. However, the use of attention checks is Alternative evaluation instruments should not be ruled out either. Ever since a pilot in 2016 (Bojar et al., 2016a), recent editions of the Conference on Machine Translation (WMT), have used Direct Assessment, whereby participants compare an output to a reference text on a continuous (0-100) scale (Graham et al., 2017; Bojar et al., 2016b), similar to Magnitude Estimation (Bard et al., 1996). Zarrieß et al. (2015) used a mouse contingent reading paradigm in an evaluation study of generated text, finding that features recorded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit g"
W19-8649,W16-6615,0,0.0699735,"Missing"
W19-8649,W18-6539,0,0.0519894,"Missing"
W19-8649,D15-1021,0,0.0306186,"the Dutch Image Description and Eye-tracking Corpus (DIDEC; van Miltenburg et al. 2018a). For the latter condition, we collected additional data using a similar sample of participants. We analyzed the effects of modality on the elicited descriptions using mixedeffects models, controlling for variation in participants and images used to elicit the descriptions. We only found a significant effect for prepositions (used more in written descriptions); other effects disappear in a controlled setting. This paper contributes to our understanding of the linguistic aspects of image descriptions (e.g., Ferraro et al. 2015; van Miltenburg et al. 2016; Alikhani and Stone 2019). Still, the main takeaway from our study is methodological: for studying task effects in elicitation tasks, we should control for individual variation and the effects of the stimuli used in the experiment. We hope that this study can serve as an example for the use of mixed effects modeling in natural language generation.1 2 The original study Van Miltenburg et al. (2018b) aimed to identify 1 All our code and data is publicly available online. The interface for the written descriptions is available through: https://github.com/evanmiltenbur"
W19-8649,W18-6547,0,0.0174124,"duction Natural Language Generation (NLG) systems are increasingly trained on the basis of datasets of human-produced examples, for example in the recent E2E-challenge (Duˇsek et al., 2018), or in automatic image description (Bernardi et al., 2016). The quality of the system output depends to a large extent on the quality of the data that is used to train the system, which in turn depends on the way that data is collected. A recent trend in NLG is to study task effects in the creation of corpora for natural language generation (Baltaretu and Castro Ferreira, 2016; van Miltenburg et al., 2017; Ilinykh et al., 2018). However, there does not seem to be an established methodology to investigate whether differences in task design lead to any significant differences in the output. This paper uses a tightly controlled approach to study task effects in NLG. As a case study, we look at the effects of modality in an image description task. In their exploratory study, Van Miltenburg et al. (2018b) found that spoken and written descriptions differ in several ways, with the main result being that speakers have a greater tendency to show themselves through the use of ‘egocentric language’ (Akinnaso, 1982). The probl"
W19-8649,J84-3009,0,0.540065,"are detected using a part-of-speech tagger (SpaCy 2.0.4). S EMANTIC CATEGORIES: negations (no, not), pseudo-quantifiers (few, lots), consciousness-ofprojection terms (seem, appear, maybe, positive allness terms (all, every), and self-reference terms (I, me, my) are detected by matching word tokens with a word list. Table 1 provides an overview. OTHER: Propositional Information Density (PID; Turner and Greene 1977), which corresponds to the average number of propositional ideas per word in a text, and is computed through an external tool (Marckx, 2017). Mean-segmental type-token ratio (MSTTR; Johnson 1944), which is a measure of diversity (the average number of types per segment). Findings. Van Miltenburg et al. (2018b) found no consistent differences between spoken and written descriptions for token length, MSTTR, PID, or the use of adjectives or prepositions. The authors did find that spoken descriptions are longer, and contain more adverbs, negations, positive allness terms, self-reference terms, pseudo-quantifiers, and consciousness-of-projection terms. This led them to conclude that speakers have a greater tendency to show themselves through the use of ‘egocentric language’ (Akinnaso, 1982"
W19-8649,W17-3503,1,0.89793,"Missing"
W19-8649,C18-1310,1,0.856413,"Missing"
W19-8649,W18-3910,1,0.844014,"Missing"
W19-8649,W16-3207,1,0.900552,"Missing"
W19-8656,W17-3513,1,0.887171,"Missing"
W19-8656,W11-2803,0,0.423902,"e way of phrasing the information. This may depend on personal preferences, but might also have to do with the scenarios patients are in. 4.3 Affective NLG Although clinicians sometimes fear that patients cannot handle poor outcomes (de Haes and Koedoot, 2003), other research indicates that patients prefer to know all the information, even when this information is upsetting (Kehl et al., 2015). In any case, when dealing with delivering this sensitive information, it is crucial to think about the context in which such information is delivered. Earlier evaluation studies of NLG-systems, such as Mahamood and Reiter (2011), showed that all patients – regardless of their stress level – prefer affective texts over neutral ones. That is why affective language is used in all scenarios. For example, possible solutions such as (“if you experience physical problems, a physical therapist might help you”) are included in all scenarios. Also, fixed texts convey messages such as “getting a cancer diagnosis may be overwhelming”. Patient evaluation will reveal how such texts are received within this context. 5 5.2 For data issues, Smiley et al. (2017) identified four questions: (1) “how accurate is the underlying data”, (2)"
W19-8656,W12-1516,0,0.0199259,"anations. 6 6.3 Relevance of outcomes Both clinicians agreed that knowing how treatment is going to impact the quality of life for specific patients is valuable information and that this information is currently lacking in clinical practice. They agreed that when communicating QoL outcomes, patients may know better what to expect and clinicians can provide better care. However, one clinician noted that not all quality of life measures should be communicated in the same way. For example, the effect of chemotherapy on financial issues was communicated as in Text 5. Evaluation with clinicians As Mahamood and Reiter (2012) point out, involving clinicians in an early stage of the development of NLG systems can “significantly enhance the quality of many NLG systems” (p.100). Therefore, as an initial iteration, the tool was evaluated with two clinicians (both colorectal cancer surgeons). Two individual semi-structured interviews were conducted and both conversations lasted about 30 minutes. The goals of the interviews were to assess (1) what clinicians thought of the clinical information that was used, (2) how the tool should be implemented, (3) whether they thought the outcomes of the tool are relevant, (4) what"
W19-8656,W17-1613,0,0.0843492,"on is delivered. Earlier evaluation studies of NLG-systems, such as Mahamood and Reiter (2011), showed that all patients – regardless of their stress level – prefer affective texts over neutral ones. That is why affective language is used in all scenarios. For example, possible solutions such as (“if you experience physical problems, a physical therapist might help you”) are included in all scenarios. Also, fixed texts convey messages such as “getting a cancer diagnosis may be overwhelming”. Patient evaluation will reveal how such texts are received within this context. 5 5.2 For data issues, Smiley et al. (2017) identified four questions: (1) “how accurate is the underlying data”, (2) “are there any misleading rankings given”, (3) “are there (automatic) checks for missing data” and (4) “does the data contain any outliers”. The quality of the data used for the support tool is excellent. The information on QoL outcomes of patients is based on a representative sample of colorectal cancer survivors in the Netherlands (van de Poll-Franse et al., 2011) and all clinical information is registry based data from the NCR. With regard to the second question, patients need to be informed about the probabilities o"
W98-0129,P94-1023,0,0.0149781,"to that node gets a plus (minus) sign. But pluses and minuses cancel and terms that would get a ± by the previous rule will be left unmarked. Terms marked with a plus (minus) sign are to be compared with the bottorn (top) parts of Vijay-Shanker&apos;s &apos;quasi-nodes&apos; in (Vijay-Shankar 1992). There is also an obvious close connection with positive (negative) occurrences of types in complex types in Categorial Grammar. To the third and final kind of descriptions belang· a.""&apos;<ioms which say that <l, <l"" and -< behave like immediate dominance, dominance and precedence in trees (Al - AlO, see also e.g., Cornell 1994, Backofen et al. 1995:9)3 combined with other general information, such as the statements that labeling is functional (All), and that different labe! names denote different labels (A12). Al3 and Al4 say that all nodes must be positively anchored to lexical nodes and that all lexical nodes are positively anchored to themselves. The axioms for negative anchoring (Al5 and Al6) are similar, but allow the root r to be negatively anchored to itself. Al A2 A3 Vk [r <l+ k V r = kj Vk·k <l+ k Vk1 k2k3 [[k1 <J+ k2 / k2 <J+ k3J --t ki <J+ k3} A4 Vk•k-<k A5 Vk 1k2k3 [{k1 -< k2 / k2 -< k3J --t k1 -< kJ]"
W98-0129,P83-1020,0,0.117063,"he constants mentioned, the only possibility being that ns = ns and that n 6 = n1. The reader will note that in the resulting model <T(n4 ) = walk John. The general procedure for finding out which models satisfy a given description is to identify positively marked terms with negatively marked ones in a one-to-one fashion. The term r, denoting the root, counts as negatively marked. In the given example only one tree was described, but this is indeed an exceptional situation. lt is far more common that a multiplicity of trees satisfy a given description. This kind of underspecification enabled (Marcus et al. 1983) to define a parser which does not only work in a strict left-right fashion but is also incremental in the sense that at no point during a parse information need be destroyed. A necessary condition for this form of underspecification is that there are structures which can be described. In the context of semantic scope differences it therefore is natural to turn to (May 1977)&apos;s Logical Forms, as. these are the kind of models required. In fact we use a variant of May&apos;s trees which is very close to ordinary surface structure: although we will allow NPs to be raised, the syntactic material of such"
W98-0129,J92-4004,0,0.0256557,"descript ions are unwieldy we partially abbreviate them with the help of pictures: st Descriptions in our theory model three kinds of information. First, there are input descriptions, which ·we wish to thank Kurt Eberle, Barbara Partee, Stanley Peters and all other participants of the Bad Teinach Workshop on Models of Underspecification and the Representation of Meaning (May 1998) for their comments and criticisms on an earlier version of this paper. 1 The approach to underspecified semantics taken in (Muskens 1995) was very much inspired by Description Theory and the work ofVijay-Shanker in (Vijay-Shanker 1992) but did not offer an actual integration with TreeAdjoining Grammars. In this paper we endeavour to set this right. = n1 V n = n2))) ~ np; vp;; npt 1 john1 vpj 1 walks2 Here uninterrupted lines represent immediate dom: inance (<l) and dotted lines represent dominance ( <l•), as usual. Additionally we mark positive and 2 With lexical nodes we mean those leaves in a tree which carry a lexeme. s+ l s+ 7 np; s2 npj /&apos;---.... det4 nfi 1 every5 st~ ~ 1 vpfo ~ Vu np1a 11!1 1 man20 1 vp9 1 loves12 8 15 npt6 ~ det11 1 &apos;118 nj9 n!2 1 womann Figure 1: Elementary descriptions for every man loves a woman n"
wubben-etal-2014-creating,W96-0102,0,\N,Missing
wubben-etal-2014-creating,W04-3219,0,\N,Missing
wubben-etal-2014-creating,W10-4223,1,\N,Missing
wubben-etal-2014-creating,E06-1021,0,\N,Missing
wubben-etal-2014-creating,C04-1051,0,\N,Missing
wubben-etal-2014-creating,J10-3003,0,\N,Missing
wubben-etal-2014-creating,W03-1004,0,\N,Missing
wubben-etal-2014-creating,W07-1007,0,\N,Missing
wubben-etal-2014-creating,D09-1040,0,\N,Missing
wubben-etal-2014-creating,P02-1040,0,\N,Missing
wubben-etal-2014-creating,P07-1059,0,\N,Missing
wubben-etal-2014-creating,P01-1008,0,\N,Missing
wubben-etal-2014-creating,D11-1038,0,\N,Missing
wubben-etal-2014-creating,D08-1021,0,\N,Missing
wubben-etal-2014-creating,C10-1152,0,\N,Missing
wubben-etal-2014-creating,W06-1610,0,\N,Missing
wubben-etal-2014-creating,N03-1024,0,\N,Missing
wubben-etal-2014-creating,P07-2045,0,\N,Missing
wubben-etal-2014-creating,P09-1034,0,\N,Missing
wubben-etal-2014-creating,D10-1090,0,\N,Missing
wubben-etal-2014-creating,N03-1003,0,\N,Missing
wubben-etal-2014-creating,N06-1058,0,\N,Missing
wubben-etal-2014-creating,P05-1074,0,\N,Missing
wubben-etal-2014-creating,N06-1003,0,\N,Missing
wubben-etal-2014-creating,N03-1017,0,\N,Missing
wubben-etal-2014-creating,P12-1107,1,\N,Missing
wubben-etal-2014-creating,J03-1002,0,\N,Missing
wubben-etal-2014-creating,P11-1020,0,\N,Missing
wubben-etal-2014-creating,daelemans-etal-2004-automatic,0,\N,Missing
wubben-etal-2014-creating,C08-1013,0,\N,Missing
wubben-etal-2014-creating,J08-4005,0,\N,Missing
wubben-etal-2014-creating,vossen-etal-2008-integrating,0,\N,Missing
