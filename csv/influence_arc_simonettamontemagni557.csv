2020.lrec-1.114,W13-2308,1,0.803707,"nce of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,573 103,005 3,487 – 2,445 3,050 14,659 Table 2: For each genre, number of tokens manually revis"
2020.lrec-1.114,P05-1045,0,0.00891135,"ry (De Mauro, 2000): following Giuliani et al. (2005) who demonstrate the stability and conservativeness of the Italian lexicon throughout the centuries, we took the Basic Italian Dictionary as a reference, starting from the assumption that the most important areas of experience continue to be mostly indicated by a stable nucleus of words in use for at least seven centuries. 3.2. Named Entities Annotation The Named Entity annotation of VGG-Silver was carried out with an adaption of the CoLingLab Named Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian w"
2020.lrec-1.114,magnini-etal-2006-cab,0,0.0845611,"d Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian war bulletins of World War I (Boschetti et al., 2014), in which the military formations originally tagged as MIL were converted into ORG. Moreover, we added three texts from VGG (a sample from Mussolini’s diary, a parliamentary report and a speech by the politician Claudio Treves), whose NEs were manually annotated. Only the second model was also trained on a gazetteer including person names collected from the online Italian Treccani Encyclopedia,4 WWI military organization names gathered from Wikipedi"
2020.lrec-1.114,onelli-etal-2006-diacoris,0,0.0245846,"reover, they are not economically attractive, due to their limited use to develop downstream applications with a large-scale economic impact. Still, historical corpora represent an invaluable asset in the era of Digital Humanities, given the growing interest in applying quantitative and computational methods to diachronic linguistics and historical text analysis (Tahmasebi et al., 2019). Italian makes not exception to this trend, as diachronic corpora are still few, among which it is worth pointing out the Corpus OVI dell’Italiano antico and the Corpus TLIO (by OVI– CNR), the DiaCORIS corpus (Onelli et al., 2006) and the MIDIA corpus (Gaeta et al., 2013). One major shortcoming of such resources is the extremely large timespan they cover in comparison to their limited size. The project Voci della Grande Guerra (“Voices of the Great War”) (in short, VGG)1 aimed at filling this gap by creating the largest digital corpus to date of Italian texts at the time of World War I (WWI). The corpus includes a selection of texts representative of different textual genres and registers, including popular Italian. VGG texts have been 1 http://www.vocidellagrandeguerra.it/ automatically annotated with state-of-the-art"
2020.lrec-1.114,L16-1680,0,0.0216851,"cated venture (Piotrowski, 2012), due to e.g. the absence of standardized spelling, the occurrence of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,5"
2020.lrec-1.883,W13-1727,1,0.797509,"Missing"
2020.lrec-1.883,W11-2308,1,0.881623,"Missing"
2020.lrec-1.883,W14-1820,1,0.81729,"Missing"
2020.lrec-1.883,W17-0216,0,0.0645813,"Missing"
2020.lrec-1.883,L16-1680,0,0.0298962,"etical studies and applicative scenarios in which they were successfully used for text and author profiling. 2. Profiling–UD 1 Profiling–UD is a web–based application inspired to the methodology initially presented in Montemagni (2013) and successfully tested in different case studies (some of which are reported in section 3.), that performs linguistic profiling of a text, or a large collection of texts, for multiple languages. The tool implements a two–stage process: linguistic annotation and linguistic profiling. The first step, linguistic annotation, is automatically carried out by UDPipe (Straka et al., 2016), a state–of–the–art pipeline available for nearly all languages included in the Universal Dependencies (UD) initiative, which carries out basic pre-processing steps, i.e. sentence splitting and tokenization, POS tagging, lemmatization and dependency parsing. In the second step, a set of about 130 features representative of the linguistic structure underlying the text are extracted from the output of the different levels of linguistic annotation. These features capture a wide number of linguistic phenomena ranging from superficial, morpho–syntactic and syntactic properties, which were proven t"
2020.lrec-1.883,W13-1706,0,0.0295363,"is a type of authorship attribution problem. Instead of identifying a particular author from among a 5 For the purpose of these experiments the author considered word and lemma n-grams as lexical features. closed list of suspects, we wish to identify an author class, namely, those authors who share a particular native language” (Koppel et al., 2005). Koppel’s definition highlights how profiling authors according to their native language can be seen as a process of detection of fingerprints of groups of authors. Since the organization of the First Shared Task on Native Language Identification (Tetreault et al., 2013), stylistic characteristics of L2 writings have been used to model L1 features and predict the native language of the writer of a given document. Also in this scenario, Profiling– UD features turned out to be effective in i) classifying the L1 of the writer and ii) reconstructing the linguistic profile of L1 starting from L2 productions (Cimino et al., 2013). For instance, the authors highlighted that L1s belonging to the same language family (e.g. Japanese and Korean), or contact languages (e.g. Hindi and Telugu), show closer distributions of features. The authors carried out an additional an"
allegrini-etal-2000-controlled,C96-1064,1,\N,Missing
allegrini-etal-2000-controlled,C00-1002,1,\N,Missing
allegrini-etal-2000-controlled,P99-1014,0,\N,Missing
allegrini-etal-2000-controlled,W97-0813,1,\N,Missing
allegrini-etal-2000-controlled,W98-0712,1,\N,Missing
allegrini-etal-2000-controlled,P93-1024,0,\N,Missing
attardi-etal-2010-resource,picca-etal-2008-supersense,0,\N,Missing
attardi-etal-2010-resource,C04-1053,0,\N,Missing
attardi-etal-2010-resource,W02-1001,0,\N,Missing
attardi-etal-2010-resource,W03-1022,0,\N,Missing
attardi-etal-2010-resource,W06-1670,0,\N,Missing
attardi-etal-2010-resource,W03-0423,0,\N,Missing
attardi-etal-2010-resource,roventini-etal-2000-italwordnet,0,\N,Missing
bartolini-etal-2002-lexicon,W00-1903,0,\N,Missing
bartolini-etal-2002-lexicon,lenci-etal-2000-opposites,1,\N,Missing
bartolini-etal-2004-hybrid,W02-1501,1,\N,Missing
bartolini-etal-2004-semantic,bartolini-etal-2002-lexicon,1,\N,Missing
bartolini-etal-2004-semantic,C02-2011,0,\N,Missing
bosco-etal-2010-comparing,W06-2920,0,\N,Missing
bosco-etal-2010-comparing,W08-1004,0,\N,Missing
bosco-etal-2010-comparing,P07-1122,1,\N,Missing
bosco-etal-2010-comparing,P09-1008,0,\N,Missing
bosco-etal-2010-comparing,D07-1096,1,\N,Missing
C00-1002,allegrini-etal-2000-controlled,1,0.792424,"y orthogonal and semantically coherent noun classes, under the assumption that these classes highly correlate with the principal meaning components of the verb head of which input nouns are objects. In the algorithm evaluated here this is achieved in two steps: i) rst, we select the best possible centroids of the prospective classes among the noun clusters of Fig.2; secondly, ii) we lump outstanding clusters (i.e. clusters which have not been selected in step i)) around the identi ed centroids. In what follows, we will only focus on step i). Results and evaluation of step ii) are reported in (Allegrini et al. 2000). In step i) we assume that centroids are disjunctively de ned, maximally coherent classes; hence, there exists no pair of intersecting centroids. The best possible selection of centroids will include non{intersecting clusters with the highest possible cumulative score. In practice, the best centroid corresponds to the cluster with the topmost (SI ). The second best centroid is the cluster with the second highest (SI ) and no intersection with the rst centroid, and so on (the i-th centroid is the i-th highest cluster with no intersection with the rst i , 1 centroids) until all clusters in th"
C00-1002,P98-2127,0,0.0269225,"contexts to automatically induce lexico-semantic classes from parsed corpora (Pereira and Tishby 1992; Pereira et al. 1993; Rooth 1995; Rooth et al. 1999). This family of approaches takes a pair of words (usually a verb plus a noun), and a syntactic relation holding between the two in context (usually the object), and calculates its token distribution in a training corpus. These counts de ne the range of more or less typical syntactic collocates selected by a verb. The semantic similarity between words is then de ned in terms of substitutability in local contexts (see also Grefenstette 1994; Lin 1998): two verbs are semantically close if they typically share the same range of collocates; conversely, two nouns are semantically close if they take part in the same type of selection dependencies, i.e. if they are selected by the same verbs, with the same function. From this perspective, a syntactically asymmetric relation (a dependency) is reinterpreted as a semantic co{selection, where each term of the relation can be de ned with respect to the other. This symmetric similarity metric is often accompanied by the non trivial assumption that the semantic classi cation of both verbs and nouns be"
C00-1002,C96-1064,1,0.540421,"`reduction delay'). 4 Experiment and evaluation We were able to extract all SI 's relative to the entire KB . However, we report here an intrinsic evaluation of the accuracy of acquired centroids which involves only a small subset of our results, since provision of a reference class typology is extremely labour{intensive. 1 We consider 20 Italian verbs and their object collocates.2 The object collocates were automatically extracted from the Italian SPARKLE Reference Corpus"", a corpus of Italian nancial For an extrinsic evaluation of the proposed similarity measure the reader is referred to (Montemagni et al. 1996; Briscoe et al. 1999; Federici et al. 1999a). 2 The test verbs are: aggiungere `add', aiutare `help', aspettare `expect', cambiare `change', causare `cause', chiedere `ask', considerare `consider', dare `give', decidere `decide', fornire `provide', muovere `move', permettere `allow', portare `bring', produrre `produce', scegliere `choose', sentire `feel', stabilire `establish', tagliare `cut', terminare `end', trovare ` nd'. 1 newspapers of about one million word tokens (Federici et al. 1998). For each test verb, an independent classi cation of its collocates was created manually, by partitio"
C00-1002,P93-1024,0,0.177673,"della Faggiola 32, Pisa, Italy fallegrip,simo,vitog@ilc.pi.cnr.it Abstract The paper illustrates a linguistic knowledge acquisition model making use of data types, in nite memory, and an inferential mechanism for inducing new information from known data. The model is compared with standard stochastic methods applied to data tokens, and tested on a task of lexico{semantic classi cation. 1 Introduction and Background Of late, considerable interest has been raised by the use of local syntactic contexts to automatically induce lexico-semantic classes from parsed corpora (Pereira and Tishby 1992; Pereira et al. 1993; Rooth 1995; Rooth et al. 1999). This family of approaches takes a pair of words (usually a verb plus a noun), and a syntactic relation holding between the two in context (usually the object), and calculates its token distribution in a training corpus. These counts de ne the range of more or less typical syntactic collocates selected by a verb. The semantic similarity between words is then de ned in terms of substitutability in local contexts (see also Grefenstette 1994; Lin 1998): two verbs are semantically close if they typically share the same range of collocates; conversely, two nouns are"
C00-1002,P99-1014,0,0.0432862,"allegrip,simo,vitog@ilc.pi.cnr.it Abstract The paper illustrates a linguistic knowledge acquisition model making use of data types, in nite memory, and an inferential mechanism for inducing new information from known data. The model is compared with standard stochastic methods applied to data tokens, and tested on a task of lexico{semantic classi cation. 1 Introduction and Background Of late, considerable interest has been raised by the use of local syntactic contexts to automatically induce lexico-semantic classes from parsed corpora (Pereira and Tishby 1992; Pereira et al. 1993; Rooth 1995; Rooth et al. 1999). This family of approaches takes a pair of words (usually a verb plus a noun), and a syntactic relation holding between the two in context (usually the object), and calculates its token distribution in a training corpus. These counts de ne the range of more or less typical syntactic collocates selected by a verb. The semantic similarity between words is then de ned in terms of substitutability in local contexts (see also Grefenstette 1994; Lin 1998): two verbs are semantically close if they typically share the same range of collocates; conversely, two nouns are semantically close if they take"
C00-1002,C98-2122,0,\N,Missing
C92-2083,P84-1036,0,0.0637005,"ies is concerned, &quot;sucb demands are beyond the abilities of lhe best current extraction techuiqaes&quot; (Wilks et al. 1989, p.227). However, the current stile of the art in computational linguistics demands that semantic information beyond genus terms be available now, on a large scale, to push forward the current theories, whetber that is knowledge-based parsing or parsing first with a syntactic component, followed by a semantic component. In this paper, we will focus on analyzing the definitions not for the genus terms, but for the semantic relations that can be extracted from the differentiae (Calzolari 1984). Although many have accepted the use of syntactic analyses for this purpose for some time now (for example Jeosen and Binot 1987, Klavans 1990, Ravin 1990, and Vanderwende 1990, all of which use the PLNLP F~lglish Parser to provide the structural information), many others still do not. We will demonstrate with examples why only patterns based on syntactic information (henceforth, structural patterns) provide reliable semantic relations for the differentiae. Patterns that match definition text at the string level (henceforth, striug patterns) are conceivable, but cannot capture the variations"
C92-2083,J87-3005,0,0.58492,"Missing"
C92-2083,P86-1018,0,0.0869753,"Missing"
C92-2083,J83-3002,0,\N,Missing
C92-2083,P90-1033,0,\N,Missing
C96-1064,J93-1005,0,\N,Missing
C96-1064,C94-2114,0,\N,Missing
C96-1064,P90-1034,0,\N,Missing
dellorletta-etal-2006-searching,bel-etal-2000-simple,1,\N,Missing
dellorletta-etal-2014-t2k,N09-2066,1,\N,Missing
dellorletta-etal-2014-t2k,W09-1119,0,\N,Missing
dellorletta-etal-2014-t2k,P04-1026,0,\N,Missing
dellorletta-etal-2014-t2k,W03-0419,0,\N,Missing
dellorletta-etal-2014-t2k,W06-2922,0,\N,Missing
dellorletta-etal-2014-t2k,D07-1096,0,\N,Missing
dellorletta-etal-2014-t2k,magnini-etal-2006-cab,0,\N,Missing
giovannetti-etal-2008-ontology,W02-1501,1,\N,Missing
hepple-etal-2004-nlp,W02-2004,0,\N,Missing
L16-1014,berkling-etal-2014-database,0,0.0231897,"the years and types of schools (e.g. private vs. public schools, urban vs. rural). Or by McNamara et al. (2010) who collected 120 essays written by U.S. undergraduate students that were manually evaluated to investigate linguistic factors (e.g. syntactic complexity and lexical diversity) related to the level of student writing quality. If great attention has been payed so far to the construction of corpora of written essays to study English language development of L1 learners, little work has been carried out for other languages. The KoKo corpus (Abel et al., 2014) and the corpus collected by Berkling et al. (2014) represent two main exceptions for the German language. The former is a collection of authentic texts (for a total of 716,000 tokens) written by 1,319 German–speaking students attendhttp://www.cs.rochester.edu/∼tetreaul/naacl-bea10.html 88 ing the last year of secondary school, linguistically annotated using a battery of linguistic annotation tools and manually annotated for background information and errors. It was built to get insight into pupils writing competencies and difficulties. The latter is a corpus of essays collected via elicitation and written by 1,730 students (for a total of 159"
L16-1014,J93-2001,0,0.682489,"Missing"
L16-1014,boyd-etal-2014-merlin,0,0.0312104,"rces for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures"
L16-1014,brooke-hirst-2012-measuring,0,0.0232338,"ilt in the framework of an interdisciplinary study jointly carried out by computational linguistics and experimental pedagogists and aimed at tracking the development of written language competence over the years and students’ background information. Keywords: Italian Learner Corpus, Diachronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorn"
L16-1014,W13-1703,0,0.0290619,"ritten language competence over the years and students’ background information. Keywords: Italian Learner Corpus, Diachronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of langu"
L16-1014,R13-1025,1,0.905929,"Missing"
L16-1014,dickinson-ledbetter-2012-annotating,0,0.0299036,"an be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012)."
L16-1014,abel-etal-2014-koko,0,0.0770918,"the relative rate of progress in writing over the years and types of schools (e.g. private vs. public schools, urban vs. rural). Or by McNamara et al. (2010) who collected 120 essays written by U.S. undergraduate students that were manually evaluated to investigate linguistic factors (e.g. syntactic complexity and lexical diversity) related to the level of student writing quality. If great attention has been payed so far to the construction of corpora of written essays to study English language development of L1 learners, little work has been carried out for other languages. The KoKo corpus (Abel et al., 2014) and the corpus collected by Berkling et al. (2014) represent two main exceptions for the German language. The former is a collection of authentic texts (for a total of 716,000 tokens) written by 1,319 German–speaking students attendhttp://www.cs.rochester.edu/∼tetreaul/naacl-bea10.html 88 ing the last year of secondary school, linguistically annotated using a battery of linguistic annotation tools and manually annotated for background information and errors. It was built to get insight into pupils writing competencies and difficulties. The latter is a corpus of essays collected via elicitatio"
L16-1014,C14-1203,0,0.0136448,"English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP"
L16-1014,W13-3601,0,0.170421,"hronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has"
L16-1014,W14-1701,0,0.0216362,"of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed fo"
L16-1014,W07-1001,0,0.0641425,"Missing"
L16-1014,N13-1084,0,0.023759,"Missing"
L16-1014,P05-1025,0,0.0467593,"umber of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of t"
L16-1014,P12-2019,0,0.0211331,"kinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA) arrived in 2016 at its eleventh edition1 , language technolo"
L16-1014,P05-1065,0,0.0334174,"or different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA) arrived in 2016 at its eleventh edition1 , language technologies have been also exploited in educational settings to design and develop educational applications such as for instance Intelligent Computer–Assisted Language Learning systems (ICALL) (Granger, 2003) or Automatic Essay Scoring systems (Attali and Burstein, 2006). For all these studies and applications, the availability of electronically accessible corpora of student essays is of"
L16-1014,W15-1614,0,0.0222346,"t of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental"
L16-1520,cucurullo-etal-2006-dialectal,1,0.83175,"resource, but is also able to effortlessly obtain a quantitative aggregate view of (subsets of) the data . The dialect atlas is the Atlante Lessicale Toscano (ALT; Giacomelli et al., 2000), a specially designed linguistic atlas focusing on dialectal variation within Tuscany, a region which has a special status in the complex puzzle of linguistic variation in Italy. A few years after its construction, ALT was made available as an online resource, called ALT-Web, 1 providing close reading functionalities to inspect the data at the level of individual question items and/or individual locations (Cucurullo et al., 2006). No facilities to carry out aggregate analyses of the data were included in either of the two versions. The distant reading functionalities are provided by the 1 3265 http://serverdbt.ilc.cnr.it/altweb/ online application Gabmap (Nerbonne et al., 2011; Leinonen et al., forthcoming), which has been developed by the University of Groningen in the framework of CLARIN-NL as an accessible open source web application to analyze language variation data. Gabmap allows both aggregate dialectometric analyses and data inspection at the level of the individual items. In the following sections, we illustr"
L18-1719,W13-2308,1,0.758772,"e from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identifying annotation inconsistencies, and thus for also detecting annotation errors (Alzetta et al., 2018). Tusa et al. (2016) represent the first attempt to exploit the plausibility score returned by this class of algorithms to acquire linguistic evidence, i.e. to infer the prototypicality degree of specific linguistic constructions. The experiment was carried out against the Italian Universal Dependency Treebank (IUDT) (Bosco et al., 2013) with promising results: the plausibility-based ranking of dependencies corresponding to specific syntactic constructions turned out to closely reflect their linguistic “markedness” degree. In what follows, we focus on word pattern variation across three different languages, English, Italian and Spanish. This goal is pursued by applying a plausibility assessment algorithm against the UD treebanks available for these languages. Achieved results have been compared with the threefold aim of: i) reconstructing the frequency distributions of different word order patterns, with particular attention"
L18-1719,W11-0314,1,0.817185,"lity of automatically produced syntactic annotations could be used to acquire useful quantitative typological evidence. In fact, the result of these algorithms is typically driven by linguistic properties 2 Available online at http://wals.info 4540 http://universaldependencies.org/ characterizing the language being processed: by comparing the results achieved against different languages, it is possible to acquire information concerning typological similarities and differences. This kind of algorithms operate at the level of either the whole syntactic tree (cfr. for example Dell’Orletta et al. (2011) and Reichart and Rappoport (2009)), or individual dependencies (see, among others, Dell’Orletta et al. (2013) and Che et al. (2014)). Given the focus of this study on specific constructions, we selected the class of algorithms operating at the level of individual dependencies, and in particular on those ranking dependencies by decreasing plausibility of annotation. These algorithms, originally meant to discern reliable from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identi"
L18-1719,W15-2112,0,0.0257473,"n type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The goal in developing UD was not only to support compar"
L18-1719,W15-2115,0,0.138533,"scientific papers. But impact and role of this information type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The"
L18-1719,K15-1025,0,0.345505,"scientific papers. But impact and role of this information type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The"
L18-1719,J05-4001,0,0.111724,"different languages (English, Italian and Spanish). Keywords: Linguistic Knowledge Extraction, Dependency Treebanks, Linguistic Typology 1. Introduction The interaction between linguistics and computational linguistics has a long history dating back to the 60’s. In Kuˇcera (1982), it is explicitly stated that “computational linguistics provides important potential tools for the testing of theoretical linguistic constructs and of their power to predict actual language use”. This still appears to represent a key objective, as claimed e.g. by Martin Kay in his ACL Lifetime Award speech in 2005 (Kay, 2005), or by the more recent papers gathered in the Special Issue of the journal “Linguistic Issues in Language Technology” (LiLT) focusing on the relationship between language technology and linguistic insights (Baldwin and Kordoni, 2011). After more than 40 years from the first pioneering studies, the growing availability of linguistic resources such as annotated corpora for many languages combined with the increasing reliability of Natural Language Processing (NLP) methods and tools enables the acquisition of quantitative evidence ranging across different levels of linguistic description which c"
L18-1719,C82-1027,0,0.725667,"Missing"
L18-1719,P13-2017,0,0.0651344,"Missing"
L18-1719,P12-1066,0,0.0126894,"ed, existing typological databases such as WALS have still a partial coverage, and most importantly here, do not always reflect real language use. Methods for automatic induction of typological information are still at the beginning: this paper represents a promising attempt in this area. It is a widely acknowledged fact that word-order affects the automatic analysis of sentences: free–order languages are harder to parse (Gulordava and Merlo, 2015c). Acquired information from real language usage can be used among the “selective sharing parameters” in a cross–lingual transfer parsing scenario (Naseem et al., 2012). Further directions of research include: the application of the methodology to other languages, including typologically distant ones, to reconstruct shades of typological proximity starting from real language usage; the analysis of other dependency relations as well as of more complex structures such as dependency subtrees; the extension of the range of properties expected to influence the preference towards a given word order pattern. Last but not least, we are planning to test the effectiveness of acquired typological information in a cross–lingual parsing scenario. 4547 Acknowledgements Th"
L18-1719,C16-1123,0,0.0604288,"Missing"
L18-1719,W09-1103,0,0.0224685,"omatically produced syntactic annotations could be used to acquire useful quantitative typological evidence. In fact, the result of these algorithms is typically driven by linguistic properties 2 Available online at http://wals.info 4540 http://universaldependencies.org/ characterizing the language being processed: by comparing the results achieved against different languages, it is possible to acquire information concerning typological similarities and differences. This kind of algorithms operate at the level of either the whole syntactic tree (cfr. for example Dell’Orletta et al. (2011) and Reichart and Rappoport (2009)), or individual dependencies (see, among others, Dell’Orletta et al. (2013) and Che et al. (2014)). Given the focus of this study on specific constructions, we selected the class of algorithms operating at the level of individual dependencies, and in particular on those ranking dependencies by decreasing plausibility of annotation. These algorithms, originally meant to discern reliable from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identifying annotation inconsistencies,"
L18-1719,silveira-etal-2014-gold,0,0.0308992,"Missing"
L18-1719,L16-1680,0,0.0228477,"constituents. Because of these properties, highly related to the linguistic type each language belongs to, we expect to observe a similar behaviour for typologically close languages and, on the other hand, significant differences in case of typologically distant languages. The corpora used to collect the statistics to build the LISCA models (step 1 in Section 3.1. above) are represented by the English, Italian and Spanish Wikipedia, for a total of around 40 million tokens for each language. The Spanish and English corpora were morpho–syntactically annotated and parsed by the UDPipe pipeline (Straka et al., 2016) trained on the Universal Dependency treebanks, version ˇ 2.0 (J. Nivre and A. Zeljko and A.Lars and et alii, 2017). The Italian corpus was morpho–syntactically tagged using the ILC–POS–Tagger (Dell’Orletta, 2009), and then parsed with UDPipe. LISCA, trained on the models we created earlier, was then applied to the Italian, English and Spanish UD Treebanks in order to assign a plausibility score to each dependency relation. The English Web Treebank (Silveira et al., 2014) contains 16,624 sentences and 254,830 tokens, while the Italian Universal Dependency Treebank (Bosco et al., 2013) contains"
lenci-etal-2000-opposites,1995.iwpt-1.8,0,\N,Missing
lenci-etal-2000-opposites,W96-0209,0,\N,Missing
lenci-etal-2000-opposites,H91-1005,0,\N,Missing
lenci-etal-2000-opposites,W99-0407,1,\N,Missing
lenci-etal-2000-opposites,H94-1020,0,\N,Missing
lenci-etal-2008-unsupervised,J98-2002,0,\N,Missing
lenci-etal-2008-unsupervised,H91-1067,0,\N,Missing
lenci-etal-2008-unsupervised,J06-2001,0,\N,Missing
lenci-etal-2008-unsupervised,A97-1052,0,\N,Missing
lenci-etal-2008-unsupervised,J01-2001,0,\N,Missing
lenci-etal-2008-unsupervised,P98-2247,0,\N,Missing
lenci-etal-2008-unsupervised,C98-2242,0,\N,Missing
lenci-etal-2008-unsupervised,J01-3003,0,\N,Missing
lenci-etal-2008-unsupervised,zeman-sarkar-2000-learning,0,\N,Missing
lenci-etal-2008-unsupervised,L08-1000,0,\N,Missing
lenci-etal-2008-unsupervised,korhonen-etal-2006-large,0,\N,Missing
lenci-etal-2008-unsupervised,W97-0123,0,\N,Missing
lenci-etal-2012-enriching,burchardt-etal-2006-salsa,0,\N,Missing
lenci-etal-2012-enriching,N09-2066,0,\N,Missing
lenci-etal-2012-enriching,N04-4008,0,\N,Missing
lenci-etal-2012-enriching,P06-2057,0,\N,Missing
lenci-etal-2012-enriching,J08-4004,0,\N,Missing
lenci-etal-2012-enriching,J05-1004,0,\N,Missing
lenci-etal-2012-enriching,ohara-2008-lexicon,0,\N,Missing
R13-1025,W06-2922,0,0.0396621,"Missing"
R13-1025,J93-2001,0,0.746893,"Missing"
R13-1025,W01-0521,0,0.0306001,"Missing"
R13-1025,C10-1062,0,0.064829,"Missing"
R13-1025,C96-2123,0,0.116086,"Missing"
R13-1025,W11-2308,1,0.921951,"Missing"
R13-1025,D07-1013,0,0.0629438,"Missing"
R13-1025,W12-5812,1,0.75592,"Missing"
R13-1025,U09-1008,0,0.0744316,"Missing"
R13-1025,W13-1506,0,0.0549089,"Missing"
R13-1025,P04-1026,0,\N,Missing
simi-etal-2014-less,bosco-etal-2010-comparing,1,\N,Missing
simi-etal-2014-less,bosco-etal-2000-building,1,\N,Missing
simi-etal-2014-less,N09-2066,0,\N,Missing
simi-etal-2014-less,W08-1301,0,\N,Missing
simi-etal-2014-less,W13-3728,0,\N,Missing
simi-etal-2014-less,W13-3721,0,\N,Missing
simi-etal-2014-less,W13-2308,1,\N,Missing
simi-etal-2014-less,C12-2082,0,\N,Missing
simi-etal-2014-less,declerck-2006-synaf,0,\N,Missing
simi-etal-2014-less,P13-2017,0,\N,Missing
thompson-etal-2008-building,N03-4009,0,\N,Missing
thompson-etal-2008-building,W06-0602,0,\N,Missing
thompson-etal-2008-building,W04-3111,0,\N,Missing
thompson-etal-2008-building,J05-1004,0,\N,Missing
W02-1501,bartolini-etal-2002-lexicon,1,0.82688,"with the ISST relations in TC we make allowance for one level of subsumption. 4 Analysis of Results The parsing outputs of BP and LAP were compared and projected against ISST annotation to assess the contribution of lexical information to parse success. In this paper, we focus on the evaluation of how and to which extent lexicosyntactic information contributes to identification of the proper attachment of prepositional complements. For an assessment of the role and impact of lexical information in the analysis of dependency pairs headed by specific words, the interested reader is referred to Bartolini et al. (2002). 4.1 ent parsing configurations. Precision is defined as the ratio of correctly identified dependency relations over all relations found by the parser (prec = correctly identified relations / total number of identified relations); recall refers to the ratio of correctly identified dependency relations over all relations in ISST (recall = correctly identified relations / ISST relations). Finally, the overall performance of the parsing systems is described in terms of the f score, computed as follows: 2 prec recall / prec + recall. Quantitative Evaluation Table 1 summarises the results obtained"
W02-1501,briscoe-carroll-2002-robust,0,0.0176487,"ne third of verb frames contain positions realized by a PP, and this percentage raises up to the near totality noun-headed frames. 2 Robust Parsing of Italian The general architecture of the Italian parsing system used for testing adheres to the following principles: 1) modular approach to parsing, 2) underspecified output (whenever required), 3) cautious use of lexical information, generally resorted to in order to refine and/or further specify analyses already produced on the basis of grammatical information. These principles underlie other typical robust parsing architectures (Chanod 2001, Briscoe and Carroll 2002). The system consists of i.) CHUNK-IT (Federici et al. 1998a), a battery of finite state automata for non-recursive text segmentation (chunking), and ii.) IDEAL (Lenci et al. 2001), a dependency-based analyser of the full range of intra-sentential functional relations (e.g. subject, object, modifier, complement, etc.). CHUNK-IT requires a minimum of lexical knowledge: lemma, part of speech and morpho-syntactic features. IDEAL includes in turn two main components: (i.) a Core Dependency Grammar of Italian; (ii.) a syntactic lexicon of ~26,400 subcategorization frames for nouns, verbs and adject"
W02-1501,W98-1114,0,0.0125423,"y expert lexicographers, and their natural purpose is to provide general purpose, domain-independent syntactic information, covering the most frequent entries and frames. On the other hand, parsing systems often complement general lexicons with corpusdriven, automatically harvested syntactic information (Federici et al. 1998b, Briscoe 2001, Korhonen 2002). Automatic acquisition of subcategorization frames allows systems to access highly context dependent constructions, to fill in possible lexical gaps and eventually rely on frequency information to tune the relative impact of specific frames (Carroll et al. 1998). Lexicon coverage is usually regarded as the main parameter affecting use of lexical information for parsing. However, the real comparative impact of the type (rather than the mere quantity) of lexical information has been seldom discussed. Our results show that the contribution of various lexical information types to parse success is not uniform. The experiment focuses on a particular subset of the information available in syntactic lexicons - the representation of PP complements in lexical frames - tested on the task of PP-attachment. The reason for this choice is that this piece of informa"
W02-1501,C94-1042,0,0.0132148,"nstraints on the argument realization (e.g. the preposition heading a PP complement), and iv.) the argument functional role. Other types of syntactic information that are also found in syntactic lexicons are: argument optionality, verb control, auxiliary selection, order constraints, etc. On the other hand, collocation-based lexical information is only rarely provided by computational lexicons, a gap often lamented in robust parsing system development. A number of syntactic computational lexicons are nowadays available to the NLP community. Important examples are LDOCE (Procter 1987), ComLex (Grishman et al. 1994), PAROLE (Ruimy et al. 1998). These lexicons are basically hand-crafted by expert lexicographers, and their natural purpose is to provide general purpose, domain-independent syntactic information, covering the most frequent entries and frames. On the other hand, parsing systems often complement general lexicons with corpusdriven, automatically harvested syntactic information (Federici et al. 1998b, Briscoe 2001, Korhonen 2002). Automatic acquisition of subcategorization frames allows systems to access highly context dependent constructions, to fill in possible lexical gaps and eventually rely"
W02-1501,C88-2092,0,0.0185339,") {roberto.bartolini, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.it Abstract In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a nonlexicalised and a lexicalised mode. Results shed light on the contribution of types of lexical information to parsing. Introduction It is widely assumed that rich computational lexicons form a fundamental component of reliable parsing architectures and that lexical information can only have beneficial effects on parsing. Since the beginning of work on broadcoverage parsing (Jensen 1988a, 1988b), the key issue has been how to make effective use of lexical information. In this paper we put these assumptions to the test by addressing the following questions: to what extent should a lexicon be trusted for parsing? What is the neat contribution of lexical information to overall parse success? We present here the results of a preliminary evaluation of the interplay between lexical and grammatical information in parsing Italian using a robust parsing system based on an incremental approach to shallow syntactic analysis. The system can run in both a non-lexicalised and a lexicalise"
W02-1501,lenci-etal-2000-opposites,1,0.819064,"imum of lexical knowledge: lemma, part of speech and morpho-syntactic features. IDEAL includes in turn two main components: (i.) a Core Dependency Grammar of Italian; (ii.) a syntactic lexicon of ~26,400 subcategorization frames for nouns, verbs and adjectives derived from the Italian PAROLE syntactic lexicon (Ruimy et al. 1998). The IDEAL Core Grammar is formed by ~100 rules (implemented as finite state automata) covering major syntactic phenomena,1 and organized into structurally-based rules and lexically-based rules. IDEAL adopts a slightly simplified version of the FAME annotation scheme (Lenci et al. 2000), where functional relations are headbased and hierarchically organised to make provision for underspecified representations of highly ambiguous functional analyses. This feature allows IDEAL to tackle cases where lexical information is incomplete, or where functional relations cannot be disambiguated conclusively (e.g. in the case of the argument vs. adjunct distinction). A “confidence score” is associated with some of the identified dependency relations to determine a plausibility ranking among different possible analyses. In IDEAL, lexico-syntactic information intervenes only after possibly"
W02-1501,W00-1903,0,0.0336731,"Missing"
W05-0509,bartolini-etal-2004-hybrid,1,0.869022,"Missing"
W05-0509,J96-1002,0,0.00498103,"ound way to build a probabilistic model for SOI, which combines different linguistic cues. Given a linguistic context c and an outcome a∈A that depends on c, in the ME framework the conditional probability distribution p(a|c) is estimated on the basis of the assumption that no a priori constraints must be met other than those related to a set of features f j (a,c) of c, whose distribution is derived from the training data. It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following expone ntial form (Berger et al. 1996): (1) p (a |c) = 1 k f ( a ,c ) αjj ∏ Z (c) j =1 where Z(c) is a normalization factor, f j (a,c) are the values of k features of the pair (a,c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy. The parameters of the distribution a 1 , …, a k correspond to weights associated with the features, and determine the relevance of each feature in the overall model. In the experiments reported below feature weights have been estimated with the Genera"
W05-0509,bel-etal-2000-simple,1,0.836713,"Missing"
W06-0604,W05-0509,1,0.838535,"Missing"
W06-0604,bel-etal-2000-simple,1,0.758385,"Missing"
W10-3711,bonin-etal-2010-contrastive,1,0.520112,"Missing"
W11-0314,W06-2922,0,0.056052,"lows: weight((P d, P h, t)) = F ((P d, P h, t)) · F ((P d, X, t)) F ((P d, P h, t)) · F ((X, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P d, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P h, P h2, t2)) F (((P d, P h, t)(P h, P h2, t2))) · , F ((((P d, X, t))(X, P h2, t2))) · where F (x) is the frequency of x in I, X is a variable and (arc1 arc2) represent two consecutive arcs in the tree. 3 The Parsers ULISSE was tested against the output of two really different data–driven parsers: the first–order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. The 1 We set r=0 in the in–domain experiments and r=2 in the out–of–domain experiment reported in Sec 5.3. former is a graph–based parser (following the so– called “all–pairs” approach Buchholz et al. (2006)) where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for a maximum spanning tree in a directed graph. The latter is a Shift–Reduce parser (following a “stepwise” approach, Buchholz et al. (2006)), where the parser is trained and learns the sequence"
W11-0314,W06-2920,0,0.186517,"Missing"
W11-0314,I08-2097,0,0.578412,"studies devoted to detecting reliable parses from the output of a syntactic parser is spreading. They mainly differ with respect to the kind of selection algorithm they exploit. Depending on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of"
W11-0314,C96-2123,0,0.0615079,"ring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause. It is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post–verbal rather than in pre–verbal position (Miller, 1998). Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in accuracy when analysing long distance dependencies. This is in line with Lin (1996) and Gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. Dependency link plausibility (henceforth, ArcPOSFeat): this feature is used to calculate the plausibility of a dependency link given the part–of–speech of the dependent and the head, by also considering the PoS of the head father and the dependency linking the two. 2.2 Comput"
W11-0314,W06-2932,0,0.0512999,"The individual arc weight is computed as follows: weight((P d, P h, t)) = F ((P d, P h, t)) · F ((P d, X, t)) F ((P d, P h, t)) · F ((X, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P d, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P h, P h2, t2)) F (((P d, P h, t)(P h, P h2, t2))) · , F ((((P d, X, t))(X, P h2, t2))) · where F (x) is the frequency of x in I, X is a variable and (arc1 arc2) represent two consecutive arcs in the tree. 3 The Parsers ULISSE was tested against the output of two really different data–driven parsers: the first–order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. The 1 We set r=0 in the in–domain experiments and r=2 in the out–of–domain experiment reported in Sec 5.3. former is a graph–based parser (following the so– called “all–pairs” approach Buchholz et al. (2006)) where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for a maximum spanning tree in a directed graph. The latter is a Shift–Reduce parser (following a “stepwise” approach, Buchholz et al. (2006)), where the parse"
W11-0314,D07-1013,0,0.651407,"lauses: subordination is generally considered to be an index of structural complexity in language. Two distinct features are considered for monitoring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause. It is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post–verbal rather than in pre–verbal position (Miller, 1998). Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in accuracy when analysing long distance dependencies. This is in line with Lin (1996) and Gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. Dependency link plausibility (henceforth, ArcPOSFeat): this feature is used to calculate the plausibility of a dependency link given the part–of–"
W11-0314,J93-2004,0,0.0434808,"Missing"
W11-0314,P06-1043,0,0.225735,"Missing"
W11-0314,C08-1071,0,0.0182086,"dPUPA represents each sentence as a collection of sequences of POSs covering all identified dependency subtrees. In particular, each dependency tree is represented as the set of all subtrees rooted by non–terminal nodes. Each subtree is then represented as the sequence of POS tags of the words in the subtree (reflecting the word order of the original sentence) integrated with the POS of the leftmost and rightmost in the sentence (NULL when there are no neighbors). Figure 1 shows the example of the dependency tree for the sentence I will give you the ball. by Reichart and Rappoport (2007b) and McClosky et al. (2008), small and big treebanks pose different problems in the reliable parses selection. Last but not least, we aimed at demonstrating that ULISSE can be successfully used not only with texts belonging to the same domain as the parser training corpus. For this purpose, ULISSE was tested on a target corpus of Italian legislative texts, whose automatic linguistic analysis poses domain–specific challenges (Venturi, 2010). Out–of–domain experiments are being carried out also for English. Figure 1: Example of dependency tree. If we consider the subtree rooted by give (in the dotted circle), the resultin"
W11-0314,D08-1093,0,0.0234258,"g on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses prod"
W11-0314,P07-1052,0,0.596188,"d semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy a"
W11-0314,P07-1078,0,0.522868,"d semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy a"
W11-0314,W09-1120,0,0.162688,"ed methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser. To our knowledge, Reichart and Rappoport (2009a) are the first to address the task of high qual115 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115–124, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics ity parse selection by resorting to an unsupervised– based method. The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently. For this purpose, their PUPA (POS– based Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed"
W11-0314,W09-1103,0,0.157155,"ed methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser. To our knowledge, Reichart and Rappoport (2009a) are the first to address the task of high qual115 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115–124, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics ity parse selection by resorting to an unsupervised– based method. The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently. For this purpose, their PUPA (POS– based Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed"
W11-0314,D10-1067,0,0.0119059,"ncy parsers. ULISSE shows a promising performance against the output of two supervised parsers selected for their behavioral differences. In all experiments, ULISSE outperforms all baselines, including dPUPA and Sentence Length (SL), the latter representing a very strong baseline selection method in a supervised scenario, where parsers have a very high performance with short sentences. The fact of carrying out the task of reliable parse selection in a supervised scenario represents an important novelty: however, the unsupervised nature of ULISSE could also be used in an unsupervised scenario (Reichart and Rappoport, 2010). Current direction of research include a careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and effectivess of the set of linguistic features. This study is being carried out with a specific view to NLP tasks which might benefit from the ULISSE algorithm. This is the case, for instance, of the domain adaptation task in a self–training scenario (McClosky et al., 2006), of the treebank construction process by minimizing the human annotators’ efforts (Reichart and Rappoport, 2009b), of n–best ranking meth"
W11-0314,W07-1001,0,0.0257779,"a set of parsed sentences and it assigns to each dependency tree a score quantifying its reliability. It operates in two different steps: 1) it collects statistics about a set of linguistically–motivated features extracted from a corpus of parsed sentences; 2) it calculates a quality (or reliability) score for each analyzed sentence using the feature statistics extracted from the whole corpus. 2.1 Selection of features The features exploited by ULISSE are all linguistically motivated and rely on the dependency tree structure. Different criteria guided their selection. First, as pointed out in Roark et al. (2007), we needed features which could be reliably identified 116 within the automatic output of a parser. Second, we focused on dependency structures that are widely agreed in the literature a) to reflect sentences’ syntactic and thus parsing complexity and b) to impose a high cognitive load on the parsing of a complete sentence. Here follows the list of features used in the experiments reported in this paper, which turned out to be the most effective ones for the task at hand. Parse tree depth: this feature is a reliable indicator of sentence complexity due to the fact that, with sentences of appr"
W11-0314,D07-1111,0,0.0874791,"e web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of t"
W11-0314,W06-1604,0,0.196462,"erest has been shown in assessing the reliability of automatically produced parses: the selection of high quality parses represents nowadays a key and challenging issue. The number of studies devoted to detecting reliable parses from the output of a syntactic parser is spreading. They mainly differ with respect to the kind of selection algorithm they exploit. Depending on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches pro"
W11-0314,W06-1626,0,0.021146,"Missing"
W11-0314,D07-1096,0,\N,Missing
W11-2308,W10-1001,0,0.638024,"ng of documents by reading difficulty (e.g. in returning the results of web queries) to NLP tasks such as automatic document summarization, machine translation as well as text simplification. Again, also the application making use of the readability assessment, which is in turn strictly related to the intended audience of readers, strongly influences the typology of features to be taken into account. Advanced NLP–based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al., 2010) and German (Br¨uck, 2008). 75 3 Our Approach Our approach to readability assessment was developed with a specific application in mind, i.e. text simplification, and addresses a specific target audience of readers, namely people characterised by low literacy skills and/or by mild cognitive impairment. Following the most recent approaches, we treat readability assessment as a classification task: in particular, given the available corpora for the Italian language as well as the type of target audience, we resorted to a binary classification aimed at discerning easy–to–read textual objects from"
W11-2308,W06-2922,0,0.0706355,"Missing"
W11-2308,J08-1001,0,0.00807082,"tures from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent approaches esplored the role of these features in readability assessment: this is the case, for instance, of Barzilay and Lapata (2008) or Feng (2010). The last few years have been characterised by approaches based on the combination of features ranging over different linguistic levels, namely lexical, syntactic and discourse (see e.g. Pitler and Nenkova (2008), Kate (2010)). Another important factor determining the typology of features to be considered for assessing readability has to do with the intended audience of readers: it is commonly agreed that reading ease does not follow from intrinsic text properties alone, but it is also affected by the expected audience. Among the studies addressing readability with respect to s"
W11-2308,C10-2032,0,0.217984,"Missing"
W11-2308,E09-1027,0,0.0392423,"Missing"
W11-2308,N07-1058,0,0.764136,"ng approach (Smoothed Unigram model) to predict reading difficulty of short passages and web documents. These approaches can be seen as a generalization of the vocabulary-based approach, aimed at capturing finer-grained and more flexible information about vocabulary usage. If unigram language models help capturing important content information and variation of word usage, they do not cover other types of features which are reported to play a significant role in the assessment of readability. More recently, the role of syntactic features started being investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009): in these studies syntactic structure is tracked through a combination of features from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent"
W11-2308,W08-0909,0,0.0511786,"Missing"
W11-2308,C10-1062,0,0.0799183,"Missing"
W11-2308,C96-2123,0,0.0561653,"ing with respect to the main clause: according to Miller and Weinert (1998), sentences containing subordinate clauses in post–verbal rather than in pre–verbal position are easier to read. Two further features were introduced to capture the depth of embedded subordinate clauses since it is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses: in particular, a) the average depth of ‘chains’ of embedded subordinate clauses and b) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Length of dependency links feature: both Lin (1996) and Gibson (1998) showed that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links. This is also demonstrated in McDonald and Nivre (2007) who claim that statistical parsers have a drop in accuracy when analysing long dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. This feature is the dependency-based counterpart of the ‘phrase length’ feature used for readability assessment by Nenkova (2010) and Feng (2010). 5 The Corpora One challenge in this work was f"
W11-2308,D07-1013,0,0.0152867,"o read. Two further features were introduced to capture the depth of embedded subordinate clauses since it is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses: in particular, a) the average depth of ‘chains’ of embedded subordinate clauses and b) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Length of dependency links feature: both Lin (1996) and Gibson (1998) showed that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links. This is also demonstrated in McDonald and Nivre (2007) who claim that statistical parsers have a drop in accuracy when analysing long dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. This feature is the dependency-based counterpart of the ‘phrase length’ feature used for readability assessment by Nenkova (2010) and Feng (2010). 5 The Corpora One challenge in this work was finding an appropriate corpus. Although a possibly large collection of texts labelled with their target grade level (such as the Weekly Reader for English) would be ideal, we are not aware of any"
W11-2308,D08-1020,0,0.103077,"yntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent approaches esplored the role of these features in readability assessment: this is the case, for instance, of Barzilay and Lapata (2008) or Feng (2010). The last few years have been characterised by approaches based on the combination of features ranging over different linguistic levels, namely lexical, syntactic and discourse (see e.g. Pitler and Nenkova (2008), Kate (2010)). Another important factor determining the typology of features to be considered for assessing readability has to do with the intended audience of readers: it is commonly agreed that reading ease does not follow from intrinsic text properties alone, but it is also affected by the expected audience. Among the studies addressing readability with respect to specific audiences, it is worth mentioning here: Schwarm and Ostendorf (2005) and Heilman et al. (2007) dealing with language learners, or Feng (2009) focussing on people with mild intellectual disabilities. Interestingly,Heilman"
W11-2308,W07-1001,0,0.0205729,"Missing"
W11-2308,P05-1065,0,0.551298,"ted a similar language modeling approach (Smoothed Unigram model) to predict reading difficulty of short passages and web documents. These approaches can be seen as a generalization of the vocabulary-based approach, aimed at capturing finer-grained and more flexible information about vocabulary usage. If unigram language models help capturing important content information and variation of word usage, they do not cover other types of features which are reported to play a significant role in the assessment of readability. More recently, the role of syntactic features started being investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009): in these studies syntactic structure is tracked through a combination of features from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty o"
W11-2308,W10-1007,0,0.022255,"dentified within the output of NLP tools. Last but not least, as already done by Aluisio et al. (2010) the set of selected syntactic features also includes simplification oriented ones, with the final aim of aligning the readability assessment step with the text simplification process. Another qualifying feature of our approach to readability assessment consists in the fact that we are dealing with two types of textual objects: documents and sentences. The latter represents an important novelty of our work since so far most research focused on readability classification at the document level (Skory and Eskenazi, 2010). When the target application is text simplification, we strongly believe that also assessing readability at the sentence level could be very useful. We know that methods developed so far perform well to characterize the level of an entire document, but they are unreliable for short texts and thus also for single sentences. Sentence-based readability assessment thus represents a further challenge we decided to tackle: in fact, if all sentences occurring in simplified texts can be assumed to be easy-to-read sentences, the reverse does not necessarily hold since not all sentences occurring in co"
W11-2308,N04-1025,0,\N,Missing
W13-1727,J93-2001,0,0.350606,"c form rather than 207 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–215, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics the content of texts, such as readability assessment (Dell’Orletta et al., 2011a) or the classification of textual genres (Dell’Orletta et al., 2012). The exploitation of general features qualifying the lexical and grammatical structure of a text, rather than ad hoc features specifically selected for the task at hand, is not the only peculiarity of our approach to NLI. Following Biber (1993), we start from the assumption that “linguistic features from all levels function together as underlying dimensions of variation”. This choice stems from studies on linguistic variation, in particular from Biber and Conrad (2009) who claim that linguistic varieties – called “registers” from a functional perspective – differ “in their characteristic distributions of pervasive linguistic features, not the single occurrence of an individual feature”. This is to say that by carrying out the linguistic analysis of collections of essays each written by different L1 native speakers, we need to quanti"
W13-1727,C12-1025,0,0.321926,"es (Tetreault et al., 2013). The First Shared Task on Native Language Identification (Tetreault et al., 2013) can be seen as an answer to the above mentioned problems. In this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task. Following the guidelines by the Shared Task Organizers based on the previous literature on this topic, Native Language Identification is tackled as a text classification task combining NLP–enabled feature extraction and machine learning: see e.g. Tetreault et al. (2013) and Brooke and Hirst (2012). Interestingly, the same methodological paradigm is shared by other tasks like e.g. author recognition and verification (see e.g. van Halteren (2004), authorship attribution (see Juola (2008) for a survey), genre identification (Mehler et al., 2011) as well as readability assessment (see Dell’Orletta et al. (2011a) for an updated survey), all relying on feature extraction from automatically parsed texts and state–of–the–art machine learning algorithms. Besides obvious differences at the level of the typology of selected linguistic features and of learning techniques, these different tasks sha"
W13-1727,W11-2308,1,0.922176,"Missing"
W13-1727,W12-5812,1,0.813654,"Missing"
W13-1727,C96-2123,0,0.0531665,"ation and includes: a) the distribution of subordinate vs main clauses; b) the average depth of ‘chains’ of embedded subordinate clauses and c) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Similarly to parse tree depth, this set of features can be taken to reflect the structural complexity of sentences and can thus be indicative of specific difficulties of L2 learners. Length of dependency links: measured in terms of the words occurring between the syntactic head and the dependent. This is another feature which reflects the syntactic complexity of sentences (Lin, 1996; Gibson, 1998) and which can be successfully exploited to capture syntactic idiosyncracies of L2 learners due to L1 interferences. 2.4 Other features Two further features have been considered for NLI purposes, which were included in the distributed datasets. For each document, we have also considered i) the English language proficiency level (high, medium, or low) based on human assessment by language specialists, and ii) the topic of the essays. 3 Linguistic Profiling of TOEFL11 Corpus In this section, we illustrate the results of linguistic profiling carried out on the training and developm"
W13-1727,D07-1013,0,0.0509215,"Missing"
W13-1727,U09-1008,0,0.0339103,"ures (Figures 1(a) and 1(b)), both average sentence length and average word length vary significantly across L1s. In particular, if on the one hand the essays written by Arabic and Spanish L1 speakers contain the shortest words and the longest sentences, on the other hand the Hindi and Telugu L1 essays are characterized by the longest words; the L1 Japanese and Korean corpora contain the shortest sentences. Let us focus now on the distribution of unigrams of coarse grained Parts–Of–Speech. If we consider the distributions of determiners and nouns, two features typically used for NLI purposes (Wong and Dras, 2009) which also represent stylistic markers associated with different linguistic varieties (Biber and Conrad, 2009), it can be noticed (see Figures 1(c) and 1(d)) that for Japanese and Korean the essays show the lowest percentage of determiners, while for Hindi and Telugu they are characterized by the highest percentage of nouns. For what concerns syntactic features, we observe that essays by Japanese and Korean speakers are characterized by quite a different distribution with respect to the other L1 corpora. In particular, they show the shallowest parse trees, the shortest dependency links as wel"
W13-1727,C12-1158,0,0.126834,"Missing"
W13-1727,W13-1706,0,0.278211,"e achieved encouraging results, which show that the proposed approach is general–purpose and portable across different tasks, domains and languages. 1 Introduction Since the seminal work by Koppel et al. (2005), within the Computational Linguistics community there has been a growing interest in the NLP–based Native Language Identification (henceforth, NLI) task. However, so far, due to the unavailability of balanced and wide–coverage benchmark corpora and the lack of evaluation standards it has been difficult to compare the results achieved for this task with different methods and techniques (Tetreault et al., 2013). The First Shared Task on Native Language Identification (Tetreault et al., 2013) can be seen as an answer to the above mentioned problems. In this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task. Following the guidelines by the Shared Task Organizers based on the previous literature on this topic, Native Language Identification is tackled as a text classification task combining NLP–enabled feature extraction and machine learning: see e.g. Tetreault et al. (2013) and Brooke and Hirst (2012). In"
W13-1727,P04-1026,0,\N,Missing
W13-1906,P06-1043,0,0.304181,"Missing"
W13-1906,N10-1004,0,0.0154808,"orini, 1993) in order to carry out an inter–domain analysis of the typology of errors made by each parser and demonstrated that by integrating the output of the three parsers they achieved statistically significant performance gains. Three different methods of parser adaptation for the biomedical domain have been proposed by (Lease & Charniak, 2005) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB–trained parser by improving the Part–Of–Speech tagging accuracy and by relying on an external domain–specific lexicon. More recently, (McClosky, Charniak, & Johnson, 2010) and (Plank & van Noord, 2011) devised adaptation methods based on domain similarity measures. In particular, both of them adopted lexical similarity measures to automatically select from an annotated collection of texts those training data which is more relevant, i.e. lexically closer, to adapt the parser to the target domain. In this paper, a new self–training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically– driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a sign"
W13-1906,W06-2932,0,0.0355201,"Missing"
W13-1906,D07-1013,0,0.0533493,"Missing"
W13-1906,J93-2004,0,0.043758,"Missing"
W13-1906,D07-1096,0,0.0401129,"Missing"
W13-1906,P11-1157,0,0.0308721,"Missing"
W13-1906,P07-1078,0,0.0281545,"ved results (Section 6). identical analyses for the same sentence within the output of different parsing models trained on the same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the G"
W13-1906,W09-1120,0,0.0207968,"s and about 18,600 sentences. For testing, we used the subset of Section 23 of WSJ consisting of 5,003 tokens (214 sentences). All corpora have been morpho–syntactically tagged and lemmatized by a customized version In this paper, we address the second scenario with a main novelty: we use an unsupervised approach to select reliable parses from automatically parsed target domain texts to be combined with the gold–training set. Two unsupervised algorithms have been proposed so far in the literature for selecting reliable parses, namely: PUPA (POS– based Unsupervised Parse Assessment Algorithm) (Reichart & Rappoport, 2009) and ULISSE (Unsupervised LInguiStically–driven Selection of dEpendency parses) (Dell’Orletta, Venturi, & Montemagni, 2011). Both algorithms assign a quality score to each parse tree based on statistics collected from a large automatically parsed corpus, with a main difference: whereas PUPA operates on costituency trees and uses statistics about sequences of part–of–speech tags, ULISSE uses statistics about linguistic features checked against dependency–based representations. The self– training strategy presented in this paper is based on an augmented version of ULISSE. The reasons for this ch"
W13-1906,W10-2606,0,0.0910146,"same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHEM) and biomedical"
W13-1906,D07-1111,0,0.0143815,"notated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ at the level of the selection of parse trees to be added to the in–domain gold trees as further training data. Depending on whether or not external supervised classifiers are used to select the parses to be added to the gold–training set, two types of methods are envisaged in the literature. The first is the case, among others, of: (Kawahara & Uchimoto, 2008), using a machine learning classifier to predict the reliability of parses on the basis of different feature types; or (Sagae & Tsujii, 2007), selecting 45 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45–53, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics After introducing the in– and out–domain corpora used in this study (Section 2), we discuss the results of the multi–level linguistic analysis of these corpora carried out (Section 3) with a view to identifying the main features differentiating the biomedical language from ordinary language. In Section 4, the algorithm used to select reliable parses from automatically parsed domain–specific texts"
W13-1906,I05-2038,0,0.104202,"problem in the biomedical domain where, due to the rapidly expanding body of biomedical literature, the need for increasingly sophisticated and efficient biomedical text mining systems is becoming more and more pressing. In particular, the existence of natural language parsers reliably dealing with biomedical texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled within the biomedical NLP community from different perspectives. The development of a domain–specific annotated corpus, i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta, & Tsujii, 2005), played a key role by providing a sound basis for empirical performance evaluation as well as training of parsers. On the other hand, several attempts have been made to adapt general parsers to the biomedical domain. First experiments in this direction are reported in (Clegg & Shepherd, 2005) who first compared the performance of three different parsers against the Genia treebank and a sample of the Penn Treebank A variety of semi–supervised approaches, where unlabeled data is used in addition to labeled training data, have been recently proposed in the literature in order to adapt parsing s"
W13-1906,W06-2922,0,0.0713225,"Missing"
W13-1906,W06-2920,0,0.0307875,"trategy for domain adaptation able to capture reliable parses which are also representative of the syntactic peculiarities of the target domain. 1 http://categorizer.tmit.bme.hu/∼illes/genia ptb/ In order to be fully compliant with the PTB PoS tagset, we changed the PoS label of all punctuation marks. 2 46 of the pos–tagger described in (Dell’Orletta, n.d.) and dependency parsed by the DeSR parser using Multi–Layer Perceptron (MLP) as learning algorithm (Attardi, Dell’Orletta, Simi, & Turian, n.d.), a state–of–the–art linear–time Shift–Reduce dependency parser following a “stepwise” approach (Buchholz & Marsi, 2006). 3 Linguistic analysis of biomedical abstrats vs newspaper articles Figure 1: Average sentence length in biomedical and newspaper corpora. For the specific concerns of this study, we carried out a comparative linguistic analysis of four different corpora, taken as representative of ordinary language and biomedical language. In each case, we took into account a gold (i.e. manually annotated) corpus, and an unlabelled corpus, which was automatically annotated. By comparing the results obtained with respect to gold and automatically annotated texts, we intend to demonstrate the reliability of fe"
W13-1906,W05-1102,0,0.0314551,"texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled within the biomedical NLP community from different perspectives. The development of a domain–specific annotated corpus, i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta, & Tsujii, 2005), played a key role by providing a sound basis for empirical performance evaluation as well as training of parsers. On the other hand, several attempts have been made to adapt general parsers to the biomedical domain. First experiments in this direction are reported in (Clegg & Shepherd, 2005) who first compared the performance of three different parsers against the Genia treebank and a sample of the Penn Treebank A variety of semi–supervised approaches, where unlabeled data is used in addition to labeled training data, have been recently proposed in the literature in order to adapt parsing systems to new domains. Among these approaches, the last few years have seen a growing interest in self–training for domain adaptation, i.e. a method for using automatically annotated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ a"
W13-1906,W11-0314,1,0.722733,"Missing"
W13-1906,W01-0521,0,0.0175701,"hose training data which is more relevant, i.e. lexically closer, to adapt the parser to the target domain. In this paper, a new self–training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically– driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domain– specificity of linguistic constructions. 1 Introduction As firstly demonstrated by (Gildea, 2001), parsing systems have a drop of accuracy when tested against domain corpora outside of the data from which they were trained. This is a real problem in the biomedical domain where, due to the rapidly expanding body of biomedical literature, the need for increasingly sophisticated and efficient biomedical text mining systems is becoming more and more pressing. In particular, the existence of natural language parsers reliably dealing with biomedical texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled with"
W13-1906,W07-2416,0,0.0118006,"acts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHEM) and biomedical abstracts (BIO), made of 5,001 tokens (195 sentences) and of 5,017 tokens (200 sentences) respectively. The dependency– based version of Genia includes ∼493k tokens and ∼18k sentences which was generated by converting the PTB version of Genia created by Illes Solt1 using the (Johansson & Nugues, 2007) tool with the -conll2007 option to produce annotations in line with the CoNLL 2007 data set2 . As unlabelled data, we used the datasets distributed in the framework of the CoNLL 2007 Domain Adaptation Track. For CHEM the set of unlabelled data consists of 10,482,247 tokens (396,128 sentences) and for BIO of 9,776,890 tokens (375,421 sentences). For the experiments using Genia as test set, we used the BIO unlabelled data. This was possible due to the fact that both the Genia Treebank and the BIO dataset consist of biomedical abstracts extracted (though using different query terms) from PubMed."
W13-1906,I08-2097,0,0.0182218,"these approaches, the last few years have seen a growing interest in self–training for domain adaptation, i.e. a method for using automatically annotated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ at the level of the selection of parse trees to be added to the in–domain gold trees as further training data. Depending on whether or not external supervised classifiers are used to select the parses to be added to the gold–training set, two types of methods are envisaged in the literature. The first is the case, among others, of: (Kawahara & Uchimoto, 2008), using a machine learning classifier to predict the reliability of parses on the basis of different feature types; or (Sagae & Tsujii, 2007), selecting 45 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45–53, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics After introducing the in– and out–domain corpora used in this study (Section 2), we discuss the results of the multi–level linguistic analysis of these corpora carried out (Section 3) with a view to identifying the main features differentiating the biomedica"
W13-1906,I05-1006,0,0.016894,"ain Felice Dell’Orletta, Giulia Venturi, Simonetta Montemagni Istituto di Linguistica Computazionale “Antonio Zampolli” (ILC–CNR) Via G. Moruzzi, 1 – Pisa (Italy) {felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it Abstract (PTB) (Mitchell P. Marcus & Santorini, 1993) in order to carry out an inter–domain analysis of the typology of errors made by each parser and demonstrated that by integrating the output of the three parsers they achieved statistically significant performance gains. Three different methods of parser adaptation for the biomedical domain have been proposed by (Lease & Charniak, 2005) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB–trained parser by improving the Part–Of–Speech tagging accuracy and by relying on an external domain–specific lexicon. More recently, (McClosky, Charniak, & Johnson, 2010) and (Plank & van Noord, 2011) devised adaptation methods based on domain similarity measures. In particular, both of them adopted lexical similarity measures to automatically select from an annotated collection of texts those training data which is more relevant, i.e. lexically closer, to adapt the parser to the"
W13-1906,P08-2026,0,0.0180675,"t parsing models trained on the same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHE"
W13-2308,N09-2066,0,0.0843136,"Missing"
W13-2308,W06-2922,0,0.0103376,"f the SD conversion, shows that the same aux relation is used to link the preposition to the verb heading the infinitival complement as well as the auxiliary avere ‘to have’ to the main verb. This solution might not be so appropriate given the peculiarities of the Italian language, where different prepositions (lexically selected by the governing head) can introduce infinitival complements. 5 Using ISDT as training corpus In this section, we report the results achieved by using ISDT for training a dependency parser, namely DeSR (Dependency Shift Reduce), a transition–based statistical parser (Attardi, 2006), where it is possible to specify, through a configuration file, the set of features to use (e.g. POS tag, lemma, morphological features) and the classification algorithm (e.g. Multi-Layer Perceptron (Attardi and Dell’Orletta, 2009), Support Vector Machine, Maximum Entropy). DeSR has been trained on TUT and ISST–TANL in the framework of the evaluation campaigns Evalita, for the last time in 2011 (Bosco and Mazzei, 2012; Dell’Orletta et al., 2012). More recently DeSR has been trained and tested on MIDT: the results obDuring the conversion step, the SD scheme has been specialized with respect to"
W13-2308,ide-romary-2006-representing,0,0.0459982,"Missing"
W13-2308,W07-1501,0,0.0446618,"Missing"
W13-2308,bosco-etal-2000-building,1,0.618806,"source, whose origins and development are summarised below (for more details on this harmonization and merging step the interested reader is referred to Bosco et al. (2012)). • the analysis of the performance of a state–of– the–art dependency parser by using as training the source and the target treebanks; • the mapping of the MIDT annotation scheme onto the SD data categories. 2.1 The ancestors: TUT and ISST–TANL The TUT and ISST–TANL resources differ under different respects, at the level of both corpus composition and adopted annotation schemes. For what concerns size and composition, TUT (Bosco et al., 2000)4 currently includes 3,452 Italian sentences (i.e. 102,150 tokens in TUT native, 2 http://stp.lingfil.uu.se/˜nivre/swedish treebank/ talbanken-stanford-1.2.tar.gz 3 MIDT was developed within the project PARLI (http://parli.di.unito.it/project en.html) partially funded in 2008-2012 by the Italian Ministry for University and Research, for fostering the development of new resources and tools that can operate together, and the harmonization of existing ones. MIDT is documented at 4 http://medialab.di.unipi.it/wiki/MIDT/. 62 http://www.di.unito.it/˜tutreeb/ and 93,987 in CoNLL) and represents five"
W13-2308,bosco-etal-2010-comparing,1,0.890389,"Missing"
W13-2308,N12-1020,0,0.0128977,"s represented by the EAGLES (Expert Advisory Groups on Language Engineering Standards) initiative, which ended up with providing provisional standard guidelines (Leech et al., 1996), operating at the level of both content (i.e. the linguistic 1 http://www.tc37sc4.org/ 61 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 61–69, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics treebank2 , Chinese in the Classical Chinese Literature treebank (Seraji et al., 2012) or Persian in the Uppsala Persian Dependency Treebank (Lee and Kong, 2012). In this paper, we describe the conversion of an existing Italian resource into the SD annotation scheme, with the final aim of developing a standard–compliant treebank, the Italian Stanford Dependency Treebank (ISDT). The reference resource, called Merged Italian Dependency Treebank (MIDT)3 (Bosco et al., 2012), is the result of a previous effort in the direction of improving interoperability of data sets available for Italian by harmonizing and merging two existing dependency–based resources, i.e. TUT and ISST–TANL, adopting incompatible annotation schemes. The two conversion steps are visu"
W13-2308,W06-2920,0,0.0356028,"ource, called Merged Italian Dependency Treebank (MIDT)3 (Bosco et al., 2012), is the result of a previous effort in the direction of improving interoperability of data sets available for Italian by harmonizing and merging two existing dependency–based resources, i.e. TUT and ISST–TANL, adopting incompatible annotation schemes. The two conversion steps are visualized in Figure 1: note that in both of them the focus is on the conversion and merging of the content of linguistic annotation; for what concerns the representation format, all involved treebanks follow the CoNLL tab–separated format (Buchholz and Marsi, 2006) which nowadays represents a de facto standard within the international dependency parsing community. In this paper, we deal with the second step, focusing on the MIDT to ISDT conversion. Starting from a comparative analysis of the MIDT and SD annotation schemes, we developed a methodology for converting treebank annotations belonging to the same dependency–based family based on: Figure 1: Merging and conversion process from TUT and ISST–TANL to MIDT and ISDT. In this conversion process, we had to deal with the peculiarities of the Italian language: the tackled issues range from morphological"
W13-2308,W08-1301,0,0.209785,"Missing"
W13-2308,de-marneffe-etal-2006-generating,0,0.0603756,"Missing"
W13-2308,W07-1004,0,0.0206268,"rnative to de jure standards is represented by de facto standards. For what concerns dependency–based annotation, which in the recent past has been increasingly exploited for a wide range of NLP–based information extraction tasks, the Stanford Dependency (SD) scheme (de Marneffe et al., 2006) is gaining popularity as a de facto standard. Among the contexts where SD has been applied, we can observe e.g. parsers and corpora exploited in biomedical information extraction, where it has been suggested to be a suitable unifying syntax formalism for several incompatible syntactic annotation schemes (Pyysalo et al., 2007). SD has already been applied to different languages, e.g. Finnish in the Turku treebank (Haverinen et al., 2010), Swedish in the Talbanken The paper addresses the challenge of converting MIDT, an existing dependency– based Italian treebank resulting from the harmonization and merging of smaller resources, into the Stanford Dependencies annotation formalism, with the final aim of constructing a standard–compliant resource for the Italian language. Achieved results include a methodology for converting treebank annotations belonging to the same dependency–based family, the Italian Stanford Depen"
W13-2308,declerck-2008-framework,0,0.0365137,"Missing"
W13-2308,cer-etal-2010-parsing,0,\N,Missing
W14-1820,W11-2308,1,0.895607,"Missing"
W14-1820,W10-1001,0,0.0901504,"Missing"
W14-1820,W13-1704,0,0.0250712,"rimental setting and discuss achieved results. 2 Besides readability, sentence–based analyses are reported in the literature for related tasks: for instance, in a text simplification scenario by Drnˇ darevi´c et al. (2013), Alu´ısio et al. (2008), Stajner and Saggion (2013) and Barlacchi and Tonelli (2013); or to predict writing quality level by Louis and Nenkova (2013). Sheikha and Inkpen (2012) report the results of both document– and sentence– based classification in the different but related task of assessing formal vs. informal style of a document/sentence. For students learning English, Andersen et al. (2013) made a self–assessment and tutoring system available which was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features"
W14-1820,W14-1213,0,0.0195313,"Missing"
W14-1820,W06-2922,0,0.12205,"Missing"
W14-1820,C10-2032,0,0.0326186,"es in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with respect to readability levels. In both c"
W14-1820,J08-1001,0,0.0376934,"lity assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸"
W14-1820,D12-1043,0,0.0276398,"Missing"
W14-1820,W11-1603,0,0.0304112,".italianlp.it {name.surname}@ilc.cnr.it • Department of Humanities Computing, University of Groningen, The Netherlands ◦ Department of Quantitative Linguistics, University of T¨ubingen, Germany wieling@gmail.com Abstract sentences. Yet, for specific applications, assessing the readability level of individual sentences would be desirable. This is the case, for instance, for text simplification: in current approaches, text readability is typically assessed with respect to the entire document, while text simplification is carried out at the sentence level, as e.g. done in Alu´ısio et al. (2010), Bott and Saggion (2011) and Inui et al. (2003). By decoupling the readability assessment and simplification processes, the impact of simplification operations on the overall readability level of a given text may not always be clear. With sentence–based readability assessment, this is expected to be no longer a problem. Sentence readability assessment thus represents an open issue in the literature which is worth being further explored. To our knowledge, the only attempts in this direction are represented by Dell’Orletta et al. (2011) and Sj¨oholm (2012) for the Italian and Swedish languages respectively, followed mo"
W14-1820,N07-1058,0,0.0351889,"re for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features ranging across different linguistic levels. Starting from Schwarm and Ostendorf (2005) and Heilman et al. (2007), the role of syntactic Background In spite of the acknowledged need of performing readability assessment at the sentence level, so far very few attempts have been made to systematically investigate the issues and challenges concerned with the readability assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homo"
W14-1820,W11-2708,0,0.0598665,"Missing"
W14-1820,W03-1602,0,0.0253891,"@ilc.cnr.it • Department of Humanities Computing, University of Groningen, The Netherlands ◦ Department of Quantitative Linguistics, University of T¨ubingen, Germany wieling@gmail.com Abstract sentences. Yet, for specific applications, assessing the readability level of individual sentences would be desirable. This is the case, for instance, for text simplification: in current approaches, text readability is typically assessed with respect to the entire document, while text simplification is carried out at the sentence level, as e.g. done in Alu´ısio et al. (2010), Bott and Saggion (2011) and Inui et al. (2003). By decoupling the readability assessment and simplification processes, the impact of simplification operations on the overall readability level of a given text may not always be clear. With sentence–based readability assessment, this is expected to be no longer a problem. Sentence readability assessment thus represents an open issue in the literature which is worth being further explored. To our knowledge, the only attempts in this direction are represented by Dell’Orletta et al. (2011) and Sj¨oholm (2012) for the Italian and Swedish languages respectively, followed more recently by Vajjala"
W14-1820,I13-1043,0,0.0218337,"dressed in this study is represented by people characterised by low literacy skills and/or by mild cognitive impairment. The paper is organized as follows: Section 2 describes the background literature, Section 3 introduces our approach to the task, in terms of used corpora, features and learning algorithm. Finally, Sections 4 and 5 describe the experimental setting and discuss achieved results. 2 Besides readability, sentence–based analyses are reported in the literature for related tasks: for instance, in a text simplification scenario by Drnˇ darevi´c et al. (2013), Alu´ısio et al. (2008), Stajner and Saggion (2013) and Barlacchi and Tonelli (2013); or to predict writing quality level by Louis and Nenkova (2013). Sheikha and Inkpen (2012) report the results of both document– and sentence– based classification in the different but related task of assessing formal vs. informal style of a document/sentence. For students learning English, Andersen et al. (2013) made a self–assessment and tutoring system available which was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous r"
W14-1820,D08-1020,0,0.0330528,"(as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with"
W14-1820,P05-1065,0,0.0668192,"was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features ranging across different linguistic levels. Starting from Schwarm and Ostendorf (2005) and Heilman et al. (2007), the role of syntactic Background In spite of the acknowledged need of performing readability assessment at the sentence level, so far very few attempts have been made to systematically investigate the issues and challenges concerned with the readability assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal c"
W14-1820,W10-1007,0,0.0180699,"arsing and statistical language modeling) to capture highly complex linguistic features, and used statistical machine learning to build readability assessment tools. A variety of different NLP–based approaches has been proposed so far in the literature, differing at the level of the number of identified readability classes, the typology of features taken into account, the intended audience of the texts under evaluation, or the application within which readability assessment is carried out, etc. Research focused so far on readability assessment at the document level. However, as pointed out by Skory and Eskenazi (2010), methods developed perform well when the task is characterizing the readability level of an entire document, while they are unreliable for short texts, including single In this paper, we tackle the challenge of assessing the readability of individual sentences as a first step towards text simplification. The task is modelled as a classification task, with the final aim of shedding light on two open issues connected with it, namely the reference corpora to be used for training (i.e. collections of sentences classified according to their readability level), and the identification of the most ef"
W14-1820,W12-2206,0,0.0119548,"cused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with respect to readability levels. In both cases, the classes of featu"
W14-1820,D07-1013,0,\N,Missing
W14-1820,W05-0509,1,\N,Missing
W14-1820,C10-1062,0,\N,Missing
W14-1820,P11-2034,0,\N,Missing
W14-1820,W13-5608,0,\N,Missing
W15-1604,W06-2922,0,0.0136682,"h linguistic feature, we calculated the Spearman’s correlation between the feature values extracted from the original text and from the simplified version with respect to the selected rules. 5.1 Linguistic Features The set of linguistic features spans across different levels of linguistic analysis and are broadly classifiable into four main classes: raw text, lexical, morpho–syntactic and syntactic features, shortly described below. They were extracted from the corpora automatically tagged by the part–of–speech tagger described in Dell’Orletta (2009) and dependency– parsed by the DeSR parser (Attardi, 2006). Raw text features (Features [1–2] in Table 4) are typically used within traditional readability metrics and include sentence length (average number of words per sentence), and word length (average number of characters per words). Feature [3] refers to the percentage of all unique words (types) on the Basic Italian Vocabulary (BIV) by De Mauro (2000) in the sentence. The BIV includes a list of 7,000 words highly familiar to Italian native speakers. The set of morpho–syntactic features [4–19] ranges from the probability distribution of part–of– speech types, to the lexical density of the text,"
W15-1604,W14-1206,0,0.14631,"Missing"
W15-1604,C96-2183,0,0.770362,"ion extraction. Recently, ATS has been used in educational scenarios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g"
W15-1604,W11-2308,1,0.866329,"Missing"
W15-1604,W01-0521,0,0.340491,"Missing"
W15-1604,W03-1602,0,0.251637,"Missing"
W15-1604,P14-1041,0,0.0369706,"tly, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs; on the other side, data-driven approaches require the least linguistic knowledge but they are not feasible 31 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 31–41, c Denver, Colorado, June 5, 2015. 2015 Ass"
W15-1604,W10-4213,0,0.0289342,"educational scenarios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who base"
W15-1604,W11-2802,0,0.0496931,"ios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system o"
W15-1604,D11-1038,0,0.125334,"Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs;"
W15-1604,C10-1152,0,0.210003,"c phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs; on the other side, data-driven approaches require the least linguistic knowle"
W15-1604,E14-1076,0,\N,Missing
W15-2618,W14-1207,0,0.0166904,"syntactic features that can be extracted from the output of a syntactic parser. 2.2 Readability metrics developed so far typically deal with English, with few attempts tackling other languages. The most prominent exception is represented by Swedish, for which a quantitative corpus analysis of a collection of radiology reports was carried out as a preliminary step towards the development of a Swedish text simplification tool (Kvist and Velupillai, 2013). Similarly to English, simplification algorithms for Swedish health–related documents were devised by relying on synonym replacement methods (Abrahamsson et al., 2014), or on automatic detection of out– of–dictionary words and abbreviations, or on compound splitting and spelling correction (Grigonyte et al., 2014). Initiatives carried out so far for what concerns Italian are based on traditional readability formulas. This is the case of the ETHIC (Evaluation Tool of Health Information for Consumers) project (Cocchi et al., 2014), aimed at developing an effective tool for biomedical librarians and health information professionals to assess the quality of produced documents and to support them in preparing texts of increasing quality, suitable and comprehensi"
W15-2618,W06-2922,0,0.0987725,"us as the “Italian Informed Consent Corpus” (2IC). Table 1 reports a selection of linguistic features which turned out to strongly characterize the 2IC corpus with respect to the journalistic 2Par and Rep corpora. This analysis is meant to compare domain–specific (i.e. biomedical) and general purpose corpora with the final aim of detecting the main linguistic features characterizing the language used in informed consent forms. The features were extracted from the corpus automatically tagged by the part–of–speech tagger described in Dell’Orletta (2009) and dependency–parsed by the DeSR parser (Attardi, 2006). Starting from raw textual features, it can be noticed that the 2IC corpus is characterized by shorter sentences (calculated as the average number of words per sentence) and longer words (calculated as the average number of characters per word) if compared with the 2Par and Rep corpora. Starting from the assumption underlying traditional readability formulas assuming that longer Consider now the distribution of Parts–Of– Speech across the 2Par, Rep and 2IC corpora. In135 text more difficult–to–read and need to be simplified, as suggested in Barlacchi and Tonelli (2013). formed consents are ch"
W15-2618,W12-2207,0,0.0731553,"Missing"
W15-2618,J93-2001,0,0.279139,") and longer words (calculated as the average number of characters per word) if compared with the 2Par and Rep corpora. Starting from the assumption underlying traditional readability formulas assuming that longer Consider now the distribution of Parts–Of– Speech across the 2Par, Rep and 2IC corpora. In135 text more difficult–to–read and need to be simplified, as suggested in Barlacchi and Tonelli (2013). formed consents are characterized by a high percentage of adjectives, prepositions and nouns, and by a low percentage of verbs: this gives rise to a much higher noun/verb ratio. According to Biber (1993), such different distributions represent significant dimensions of variation across textual genres. In particular, the higher noun/verb ratio reveals that informed consent forms are more informative than newspaper articles (Biber and Conrad, 2009), while the higher occurrence of nouns and prepositions is strongly connected with their presence within embedded complement ‘chains’ governed by a nominal head and including either prepositional complements or nominal and adjectival modifiers. Similarly to Rep articles, health– related documents contain a high percentage of complex nominal constructi"
W15-2618,W11-2308,1,0.925609,"Missing"
W15-2618,W14-1820,1,0.885768,"Missing"
W15-2618,W14-1209,0,0.0662015,"Missing"
W15-2618,J93-2004,0,\N,Missing
W15-2618,dellorletta-etal-2014-t2k,1,\N,Missing
W17-7624,W11-3405,0,0.27017,"rring in the same context, in this case surrounded by identical words”. This methodology has been recently reimplemented and extended by de Marneffe et al. (2017) to detect inconsistencies in the UD treebanks. The idea that the cases where two “parsers predict dependencies different from the gold standard” are “the most likely candidates when looking for errors” was experimented by Volokh and Neumann (2011), who trained two parsers based on completely different parsing algorithms to reproduce the training data (i.e. the Penn Treebank). A similar patternbased approach has been also proposed by Ambati et al. (2011) who complemented their method with a statistical module that, based on contextual features extracted from the Hindi treebank, was in charge of pruning previously identified candidate erroneous dependencies. If all the aforementioned methods exploit corpus-internal evidence to detect inconsistencies within a given treebank, van Noord (2004) and de Kok et al. (2009) use external resources, i.e. they rely on the analysis of large automatically parsed corpora external to the treebank under validation. The underlying idea of these error mining techniques is that sentences with a low parsability sc"
W17-7624,D16-1239,0,0.0410478,"Missing"
W17-7624,bosco-etal-2000-building,0,0.175459,"s corresponding to 325,816 tokens. As de Marneffe et al. (2017) pointed out, UD treebanks represent a good testing bed for error detection techniques: most part of them originate from a conversion process, often combined with merging and cross-corpus harmonization. In particular, IUDT results from the harmonization and merging of smaller dependency–based resources adopting incompatible annotation schemes into the Universal Dependencies annotation formalism, with the final aim of constructing a standard-compliant and bigger resource for the Italian language: the Turin University Treebank (TUT, Bosco et al. (2000)) and ISST–TANL (originating from the ISST corpus, (Montemagni et al., 2003)). For the specific concerns of this study, we focused on the section of IUDT containing newspaper articles, composed by 10,891 sentences, for a total of 154,784 tokens. This choice was aimed at avoiding possible interferences in detecting anomalies due to textual genre variation: in this case, “abnormal” relations do not only include possible errors but also constructions peculiar to a specific genre. The corpus used to collect the statistics to build the LISCA model is represented by the La Repubblica corpus, a colle"
W17-7624,W13-2308,1,0.908629,"Missing"
W17-7624,W09-2609,0,0.0572566,"Missing"
W17-7624,W17-6514,0,0.0341157,"Missing"
W17-7624,P10-1075,0,0.0154589,"contain a parsing error. This paper aims at testing the potential of algorithms developed to measure the reliability of automatically produced dependency relations for detecting erroneously annotated arcs in gold treebanks. In the literature, the result of this type of algorithms varies from a binary classification (correct vs. wrong) as in Che et al. (2014), to the ranking of dependencies on the basis of a quality score reflecting the reliability and plausibily of the automatic analysis (Dell’Orletta et al., 2013). Although these algorithms typically work on corpora automatically annotated (Dickinson, 2010), they have also been tested against corpora with manually revised (i.e. “gold”) annotation: in this case, the typical aim is the identification of errors or simply inconsistencies in the annotation (Dickinson, 2015). In this work, we used an algorithm ranking dependencies by reliability, LISCA (Dell’Orletta et al., 2013), that was applied to a gold treebank to limit the search space for bootstrapping error patterns, i.e. systematic recurring errors (as opposed to random errors). Identified error patterns were then projected against the whole corpus. Like Ambati et al. (2011), here error detec"
W17-7624,P05-1040,0,0.121261,"Missing"
W17-7624,C12-1055,0,0.0502853,"Missing"
W17-7624,nivre-etal-2006-maltparser,0,0.122382,"Missing"
W17-7624,L16-1680,0,0.0784285,"Missing"
W17-7624,P04-1057,0,0.144055,"Missing"
W17-7624,P11-2060,0,0.0268022,"n and Meurers (2003, 2005) and Boyd et al. (2008) proposed a variation n-gram detection method where the source of variation is the so-called variation nucleus, i.e. “a word which has different taggings despite occurring in the same context, in this case surrounded by identical words”. This methodology has been recently reimplemented and extended by de Marneffe et al. (2017) to detect inconsistencies in the UD treebanks. The idea that the cases where two “parsers predict dependencies different from the gold standard” are “the most likely candidates when looking for errors” was experimented by Volokh and Neumann (2011), who trained two parsers based on completely different parsing algorithms to reproduce the training data (i.e. the Penn Treebank). A similar patternbased approach has been also proposed by Ambati et al. (2011) who complemented their method with a statistical module that, based on contextual features extracted from the Hindi treebank, was in charge of pruning previously identified candidate erroneous dependencies. If all the aforementioned methods exploit corpus-internal evidence to detect inconsistencies within a given treebank, van Noord (2004) and de Kok et al. (2009) use external resources"
W17-7624,W03-3023,0,0.164278,"Missing"
W18-6001,W06-2922,0,0.113576,"Missing"
W18-6001,W13-2308,1,0.853195,".org/ 1 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 1–7 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics LISCA on the basis of the evidence acquired from the reference corpus) concentrate in the bottom groups of the ranking, the manual search of error patterns is restricted to the last groups. Detected anomalous annotations include both systematic and random errors. Systematic errors, formalized as error patterns, are looked for in the whole target corpus, matching contexts are manually revised and, if needed, corrected. (Bosco et al., 2013), mainly targets systematic errors, which represent potentially “dangerous” relations providing systematic but misleading evidence to a parser. Note that with systematic errors we refer here to both real errors as well as annotation inconsistencies internal to the treebank, whose origin can be traced back to different annotation guidelines underlying the source treebanks, or that are connected with substantial changes in the annotation guidelines (e.g. from version 1.4 to 2.0). This error detection methodology is based on an algorithm, LISCA (LInguiStically–driven Selection of Correct Arcs) (D"
W18-6001,P05-1040,0,0.0985159,"Missing"
W18-6001,W17-6514,0,0.037778,"Missing"
W18-6001,W11-3405,0,0.0640163,"Missing"
W18-6001,P13-2109,0,0.0231012,"Missing"
W18-6001,K17-3009,0,0.0352933,"Missing"
W18-6001,L18-1711,0,0.0415736,"Missing"
W18-6012,W07-1427,0,0.0164425,"Missing"
W18-6012,C04-1204,0,0.0109514,"a Montemagni Sebastian Schuster? Maria Simi• ∗ Uppsala University, Department of Linguistics and Philology † University of Pavia, Department of Linguistics ‡ University of Turku, Department of Future Technologies  Institute for Computational Linguistics «A. Zampolli» – CNR, Italy ? Stanford University, Department of Linguistics • University of Pisa, Department of Computer Science Abstract and as input to manual validation. Further, enhanced UD graphs are in many respects very similar to semantic dependency representations that encode predicate-argument structures (e.g., Böhmová et al. 2003; Miyao and Tsujii 2004; Oepen and Lønning 2006). While the latter exist only for a small number of languages and are typically either produced by complex hand-written grammars or by manual annotation, basic UD treebanks currently exist for more than 60 languages. Hence, automatic methods capable of predicting enhanced dependencies from UD treebanks, have the potential to drastically increase the availability of semantic dependency treebanks. In this paper, we evaluate a rule-based system developed for English and a data-driven system trained on de-lexicalized Finnish data, for predicting enhanced dependencies on a"
W18-6012,W13-3728,1,0.552289,"ll Swe Ita LSI RBE RBE 660 112 162 0.85 0.85 0.76 0.78 19 15 0 65 2 35 Table 1: Evaluation of predicted enhanced dependencies for Italian and Swedish (RBE = rule-based English system, DDF = data-driven Finnish system, LSI = language-specific Italian system). 3.2 The Data-Driven Finnish System lines but does not yet handle null nodes. It provides an interesting point of comparison for the cross-lingual systems but cannot really be evaluated on the same conditions since it has been developed using data from the Italian treebank. This data-driven approach is adapted from the supervised method of Nyblom et al. (2013) originally developed for Finnish. First, patterns identify candidate relations, which are subsequently classified with a linear SVM, trained on gold standard annotation. The original method does not predict null nodes, and therefore we only discuss added subject relations and coordination below. Added subject relations For any infinitive verb attached to a higher predicate with an xcomp relation, the system adds a subject relation to a core or (dative) oblique dependent of the governing verb. In contrast to the other systems, this system uses external language-specific resources that specify"
W18-6012,oepen-lonning-2006-discriminant,0,0.0444464,"Schuster? Maria Simi• ∗ Uppsala University, Department of Linguistics and Philology † University of Pavia, Department of Linguistics ‡ University of Turku, Department of Future Technologies  Institute for Computational Linguistics «A. Zampolli» – CNR, Italy ? Stanford University, Department of Linguistics • University of Pisa, Department of Computer Science Abstract and as input to manual validation. Further, enhanced UD graphs are in many respects very similar to semantic dependency representations that encode predicate-argument structures (e.g., Böhmová et al. 2003; Miyao and Tsujii 2004; Oepen and Lønning 2006). While the latter exist only for a small number of languages and are typically either produced by complex hand-written grammars or by manual annotation, basic UD treebanks currently exist for more than 60 languages. Hence, automatic methods capable of predicting enhanced dependencies from UD treebanks, have the potential to drastically increase the availability of semantic dependency treebanks. In this paper, we evaluate a rule-based system developed for English and a data-driven system trained on de-lexicalized Finnish data, for predicting enhanced dependencies on a sample of 1,000 sentences"
W18-6012,D17-1009,0,0.0314015,"pe language-specific system. 1 Introduction Universal Dependencies (UD) is a framework for cross-linguistically consistent treebank annotation (Nivre et al., 2016). Its syntactic annotation layer exists in two versions: a basic representation, where words are connected by syntactic relations into a dependency tree, and an enhanced representation, which is a richer graph structure that adds external subject relations, shared dependents in coordination, and predicate-argument relations in elliptical constructions, among other things. Despite the usefulness of enhanced representations (see e.g., Reddy et al. 2017; Schuster et al. 2017), most UD treebanks still contain only basic dependencies1 and therefore cannot be used to train or evaluate systems that output enhanced UD graphs. In this paper, we explore cross-lingual methods for predicting enhanced dependencies given a basic dependencies treebank. If these predictions are accurate enough, they can be used as a first approximation of enhanced representations for the nearly 100 UD treebanks that lack them, 2 Basic and Enhanced Dependencies Basic dependencies are strict surface syntax trees that connect content words with argument and modifier relatio"
W18-6012,L16-1376,1,0.763597,"e been developed, such as the one by Candito et al. (2017) for French, but this is the first attempt to predict enhanced dependencies in a language-independent way. sion of the basic one, this does not hold in general (as shown by the treatment of ellipsis below). The current UD guidelines define five enhancements: 1. 2. 3. 4. 5. Added subject relations in control and raising Null nodes for elided predicates (gapping) Shared heads and dependents in coordination Co-reference in relative clause constructions Modifier relations typed by case markers 3.1 The system is an adaptation of the work by Schuster and Manning (2016), developed for English. It relies on Semgrex (Chambers et al., 2007) patterns to find dependency structures that should be enhanced and applies heuristics-based processing steps corresponding to the five types of enhancement described in Section 2. We briefly discuss the three steps that are relevant to our study. The last two enhancements can in most cases be predicted deterministically from the basic representation and are mainly a practical convenience. We therefore limit our attention to the first three types, illustrated in Figure 1 (a–c). Added subject relations Basic dependencies do no"
W18-6012,N18-1105,1,0.752943,"Missing"
W18-6012,W13-2308,1,0.82118,"enhanced errors attributed to the systems. Feature representation To enable transfer from models trained on Finnish to other languages, we remove lexical and morphological features except universal POS tags and morphological categories that we expect to generalize well: Number, Mood, Tense, VerbForm, Voice. Languagespecific dependency type features are generalized to universal types (e.g., from nmod:tmod to nmod). 3.3 Evaluation The Language-Specific Italian System The language-specific Italian system builds on the rule-based enhancer developed for the Italian Stanford Dependencies Treebank (Bosco et al., 2013, 2014). It has been adapted to predict enhanced dependencies according to the UD guide104 nsubj xcomp obj Om du ... låter pengarna stå kvar till 1971 års slut . nsubj (1) nsubj “If you ... let the money remain [in the account] until the end of 1971.” xcomp nsubj obl E le autorità di Zagabria hanno proibito ai giornalisti di andare a Petrinja ... nsubj nsubj (2) “And the Zagreb authorities have forbidden journalists to go to Petrinja ...” conj conj amod För fysiska personer , dödsbon och familjestiftelser slopas rätten att göra avdrag ... (3) amod “For natural persons, estates and family found"
W97-0813,C96-1005,0,0.0838053,"Missing"
W97-0813,C94-2113,0,0.0484791,"Missing"
W97-0813,C96-1064,1,0.550108,"Missing"
W97-0813,W95-0105,0,0.0605642,"Missing"
W98-0712,P91-1034,0,0.248428,"e not actually implemented yet. Nonetheless, it is not obvious how many o f these cross-classificatory dimensions should be overlaid on a taxonomy to attain the desirable level of context= sensitivity required by real applications. From an application-oriented perspective, there is the further problem o f how it is possible to regiment their role and relevance as a function of context variation. As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature (see, among others, Brown et al. 1991, Gale et al. 1992, Pereira and Tishby 1992), which emphasise the role played by context in this game. These approaches compute the semantic similarity between W z and W, on the basis of the extent to which W,/W,&apos;s average contexts of use overlap. Here, the context is generally defined as an n-word window centred on Wl/W:. The method rests on the assumption that words entering into the same syntagmatic relation with other words are perceived as semantically similar. The method has a potential for capturing word similarities grounded on contextual effects of the sort sketched out above, althoug"
W98-0712,W97-0813,1,0.925632,"ar the same syntactic relation to the same word sense form a somehow semantically coherent class. In the following section, we illustrate this point by describing a measure of semantic similarity based on distributional evidence and we show how helpful this is in capturing locally salient semantic similarity. 4. Distributionally-based semantic similarity A semantic similarity measure computed on the basis of distributional evidence is at work in SENSE, an example-based word sense disambiguation (WSD) system carrying out the task on the basis of a representative set of typical patterns of use (Federici et al. 1997). In particular, SENSE presupposes prior availability of verb-noun pairs where the contextually relevant sense of the verb token is assigned. At the same time, the accompanying noun is provided with its grammatical function. This set of verb-noun pairs constitutes the knowledge base of examples (or example base for short) on the basis of which SENSE is able to draw its inferences. Given an Input Pair IP to be disambiguated where the grammatical relation of the noun relative to the verb is specified, SENSE searches its example base looking for the set of examples which are most similar to IP. I"
W98-0712,C96-1064,1,0.75047,"Missing"
W98-0712,1993.tmi-1.4,0,0.119823,"Missing"
W98-0712,C96-1005,0,\N,Missing
W99-0407,1995.iwpt-1.8,0,0.0809344,"representations. It is then crucial to make it sure that constituencybased representations, or any other variants thereof, be mappable onto the functional reference annotation recta-scheme. The same point is convincingly argued for by Lin (1998), who also provides an algorithm for mapping a constituency-based representation onto a dependency-based format. To show that the requirement of intertranslatability is satisfied by FAME, we consider here four different analyses for the sentence John tried to open the window together with their translation equivalent in the FAME format: 1. ANLT Parser (Briscoe & Carroll, 1995) - traditional PSG representation: (Tp (V2 (N2 (Ni (NO John_NPl))) (VI (VO tried_VVD) (VI (YO to_T0) (Vl (VO open_VV0) (N2 (DT the_AT)(NI (NO window_NNl)) )))))). F A M E equivalent: subj ( t r y , John) arg (try, open. &lt;introducer=""to"">) dobj (open, window) 2. Fast Partial Parser (Grefenstette, 1994): SUBJ ( t r y , John) DOBJ (open, window) SUBJ (open, John) MODIF (open, try). FAME equivalent: subj (try, John) dobj (open, window) subj (open, John) mod(open, try) 3. Finite State Constraint G r a m m a r Parser (Karlsson et al., 1995): John N SUBJ tried V MVMAINC"" to INFMARK open V_INF MV OBJ"""
W99-0407,W96-0209,0,0.0629999,"6). By assuming that all levels are, in a sense, primitive, rather than some of them being derivative of others, one provides considerable leeway for radically different definitions of functional relations to be cast into a common, albeit redundant, core of required infor•mation. We will return to this point in section 3 of the paper. To be more concrete, a binary functional relationship can be represented formally as consisting of the following types of information: • it is comparatively easy and ""fair"" to evaluate since it overcomes some of the shortcomings of constituency-based evaluation (Carroll and Briscoe, 1996; Carroll et al., 1998; Sampson, 1998; Lin, 1998); • it represents a very informative ""lowest common ground"" of a variety of different syntactic annotation schemes (Lin, 1998); • it is naturally multi-lingual, as functional relations probably represent the most significant level of syntactic analysis at which crosslanguage comparability makes sense; • it permits joint evaluation of systems dealing with both spoken and written language. Spoken data are typically fraught with cases of disfluency, anacoluthon, syntactic incompleteness and any sort of non-canonical syntactic structure (Antoine, 19"
W99-0407,H94-1020,0,0.21444,"Missing"
W99-0407,H91-1005,0,0.382317,"on and recall are to be gauged jointly relative to all such levels. To be concrete, let us first show a full version of the FAME standard representation for the sentence John tried to open the window (cf. Section 2.2): i. ( t r y , John) 44 i i . &lt;try,John> i i i . subj adaptation to different domains and different languages. Nonetheless, the formalisms used for syntax and semantics must have a certain degree of similarity and some additional knowledge about the relationships between syntax and semantics is necessary. An example is provided by what has been done in the ESPRIT SUNDIAL project (Peckam, 1991), where Syntax is defined using a dependency grammar augmented with morphological agreement rules; Semantics is declared through case frames (Fillmore, 1968; Fillmore, 1985) using a conceptual graph formalism. An additional bulk of knowledge, called mapping knowledge, specifies possible links between the symbols of the dependency grammar and the concepts of case frames. In this way syntactic and semantic controls are performed at the same time, avoiding the generation of parse trees that must afterwards be validated semantically. The FAME meta-scheme fits in comparatively well with this approa"
