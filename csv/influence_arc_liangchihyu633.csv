2020.aacl-main.4,P11-1016,0,0.338735,"technique is widely used to analyze online posts reviews, mainly from Amazon reviews or Twitter, to help raise the ability to understand consumer needs or experiences with a product, guiding a manufacturer towards product improvement. Aspect-level sentiment classiﬁcation is much more complicated than sentence-level sentiment classiﬁcation. ASC task is necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context"
2020.aacl-main.4,S14-2076,0,0.0231667,"s for words according to their contribution to the ﬁnal classiﬁcation. Experiments are conducted on ﬁve datasets demonstrate how the proposed model outperforms baselines for aspect-level sentiment analysis. The remainder of this paper is organized as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention represen"
2020.aacl-main.4,P18-1087,0,0.0484508,"Missing"
2020.aacl-main.4,D17-1047,0,0.0839424,"necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model ass"
2020.aacl-main.4,P14-2009,0,0.0267853,"s, exp(ei ) α i = n k=1 exp(ek ) sg = n  α i hE i Lcls (θ) = − (11) 4 T E hC i W c hj = τ +m T E hC i W c hj 4.1 (13) exp(ri ) βi = n k=1 exp(rk ) sc = n  β i hE i (14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To compare the proposed model with other aspectlevel sentiment analysis models, we conduct experiments on the following ﬁve commonly used datasets: Twitter was originally proposed by Dong et al. (2014) and contains several Twitter posts, while the other four corpora (Lap14, Rest14, Rest15, Rest16) were respectively retrieved from SemEval 2014 task 4 (Pontiki et al., 2014), SemEval 2015 task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016), which include two types of data, i.e., reviews of laptops and restaurants. The statistical descriptions of these corpora are shown in Table 1. We use accuracy and Macro-average F1 -score as evaluation metrics; these are commonly used in ASC task (Huang and Carley, 2019; Zhang et al., 2019). A higher accuracy or F1 -score indicates"
2020.aacl-main.4,D14-1162,0,0.0858262,"Missing"
2020.aacl-main.4,C18-1066,0,0.0176272,"fy the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model assigns equal weight"
2020.aacl-main.4,S15-2082,0,0.0672426,"Missing"
2020.aacl-main.4,S14-2004,0,0.0444343,"14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To compare the proposed model with other aspectlevel sentiment analysis models, we conduct experiments on the following ﬁve commonly used datasets: Twitter was originally proposed by Dong et al. (2014) and contains several Twitter posts, while the other four corpora (Lap14, Rest14, Rest15, Rest16) were respectively retrieved from SemEval 2014 task 4 (Pontiki et al., 2014), SemEval 2015 task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016), which include two types of data, i.e., reviews of laptops and restaurants. The statistical descriptions of these corpora are shown in Table 1. We use accuracy and Macro-average F1 -score as evaluation metrics; these are commonly used in ASC task (Huang and Carley, 2019; Zhang et al., 2019). A higher accuracy or F1 -score indicates better prediction performance j=τ +1 j=1 Experimental Results This section conducts comparative experiments on ﬁve corpora against several previously proposed methods for as"
2020.aacl-main.4,D13-1170,0,0.0114589,"248 692 2328 638 3608 1120 1204 542 1748 616 Max Length 43 39 81 70 77 68 72 61 72 77 Mean Length 19 19 21 17 18 17 15 17 16 18 Table 1: The summary of datasets ei = n  T E hL i W l hj = τ +m T E hL i W l hj  T set x(t) , y (t) t=1 = 1, where x(t) is a training sample, y (t) is the corresponding actual sentiment label, and T is the number of training samples in the corpus. The training goal is to minimize the cross-entropy Lcls (θ) deﬁned as, (10) j=τ +1 j=1 where Wl ∈ Rdh ×dh is a bilinear term that interacts with these two vectors and captures the speciﬁc semantic relations. According to Socher et al. (2013), such a tensor operator can be used to model complicated compositions between those vectors. Therefore, the attention score weight and ﬁnal representation of G-ATT are computed as, exp(ei ) α i = n k=1 exp(ek ) sg = n  α i hE i Lcls (θ) = − (11) 4 T E hC i W c hj = τ +m T E hC i W c hj 4.1 (13) exp(ri ) βi = n k=1 exp(rk ) sc = n  β i hE i (14) (15) i=1 3.6 Sentiment Classiﬁcation After obtaining representation sg and sc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes, yˆ= softmax(Ws [sg ||sc ]+bs ) Dataset To c"
2020.aacl-main.4,D19-1464,0,0.0575234,"Missing"
2020.aacl-main.4,C16-1311,0,0.289307,"as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention representation of context words and aspect words. Huang et al. (2018) proposed a joint model based on an attention mechanism to model aspects and sentences. Tang et al. (2019) proposed a selfsupervised attention model that can dynamically update at"
2020.aacl-main.4,D16-1021,0,0.224665,"as follows. Section 2 brieﬂy reviews the existing works for aspect-level sentiment analysis. Section 3 presents a detailed description of the proposed graph attention model with memory fusion. Section 4 summarizes the implementation details and experimental results. The conclusions of this study are ﬁnally drawn in Section 5. 2 vector machine (SVM) (Kiritchenko et al., 2014). Due to the inefﬁciency of manually constructed features, several neural network methods have been proposed for aspect-level sentiment analysis (Jiang et al., 2011), which are mainly based on long shortterm memory (LSTM) (Tang et al., 2016a; Wang et al., 2020). Tang et al. (2016b) indicated that the ASC task’s challenge is to identify better the semantic correlation between context words and aspect words so that several recent works widely applied an attention mechanism and achieved good performance. Ma et al. (2017) used an interactive attention network to obtain a two-way attention representation of context words and aspect words. Huang et al. (2018) proposed a joint model based on an attention mechanism to model aspects and sentences. Tang et al. (2019) proposed a selfsupervised attention model that can dynamically update at"
2020.aacl-main.4,P19-1053,0,0.111208,"he sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Figure 1, when predicting the emotional polarity of price, the attention mechanism may focus on the word poor, which is not related to its syntax. To address this issue, Zhang et al. (2019) built a graph convolutional network (GCN) over a dependency tree to exploit syntactical information and word dependencies. However, the model assigns equal weight to the edges connec"
2020.aacl-main.4,S14-2036,0,0.0129247,"used to analyze online posts reviews, mainly from Amazon reviews or Twitter, to help raise the ability to understand consumer needs or experiences with a product, guiding a manufacturer towards product improvement. Aspect-level sentiment classiﬁcation is much more complicated than sentence-level sentiment classiﬁcation. ASC task is necessary to identify the parts of the sentence that describe the correspondence between multiple aspects. Traditional methods mostly use shallow machine learning models with hand-crafted features to build sentiment classiﬁers for the ASC task (Jiang et al., 2011; Wagner et al., 2014).However, the process for manual feature engineering is time-consuming and labor-intensive as well as limited in classiﬁcation performance Recently, with the development of deep learning techniques, various attention-based neural models have achieved remarkable success in ASC. (Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Gu et al., 2018; Tang et al., 2019). However, these methods ignored the syntactic dependence between context words and aspects in a sentence. As a result, the current attention model may inappropriately focus on syntactically unrelated context words. As shown in Fig"
2020.aacl-main.4,D19-1343,1,0.838617,"ning different weights to edges. Syntactic constraints can be imposed to block the graph convolutional propagation of unrelated words. A convolutional layer and a memory fusion were applied to learn and exploit multiword relations and draw different weights of words to improve performance further. Experimental results on ﬁve datasets show that the proposed method yields better performance than existing methods. The code of this paper is availabled at https://github.com/YuanLi95/ GATT-For-Aspect. 1 Introduction Aspect-level sentiment classiﬁcation is a ﬁnegrained subtask in sentiment analysis (Wang et al., 2019; Peng et al., 2020). Given a sentence and an aspect that appears in the sentence, ASC aims to determine the sentiment polarity of that aspect (e.g., negative, neutral, or positive). For example, a review of a restaurant “The price is reasonable although the service is poor.” expresses a positive sentiment for the price aspect, but also conveys a 27 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 27–36 c December 4 - 7, 2020. 2020 Association for Comp"
2020.rocling-1.22,C18-1074,0,0.0635704,"Missing"
2020.rocling-1.22,N16-1066,1,0.763373,"Missing"
2020.rocling-1.22,P18-1232,0,0.045959,"Missing"
2020.rocling-1.22,C18-1070,0,0.0618125,"Missing"
2020.rocling-1.26,E17-2092,0,0.0290841,"貼文、搜尋首頁的關鍵字如 Zhao et al.的研究[11]係使用百 度搜尋熱門關鍵字。在探討相關性或預測的對象的選擇上，大部分文獻是以個股為主如 Hwang et al.[12]的研究係使用特斯拉股價，部分文獻係採單一市場指數，如，Bourezk et al.[13]的研究係使用摩洛哥指數。文本的情感分析(Sentiment Analysis)，又稱意見挖掘或 意見探勘(Opinion Mining)，是運用自然語言處理(Natural Language Processing)、文字探 勘(Text Mining)及計算機語言學等技術，進而提取並辨識原文本中作者的情感及主觀意 見。常見的情感分析所適用的範圍可分為句子層次、段落層次和文章層次的情感分析方 法。類別型的情感分析方法相關研究，Hwang et al.[12]，通過將文本中的正向詞和負向 詞計數，以「正詞數」-「負詞數」來計算情感得分，如果情感分數大於 0，則文本標記 為「正」 ，如果小於 0，則文本標記為「負」 。維度型情感分析在財經領域之應用並不多 見，但已有維度型詞典，如：Affective Norms for English Words (ANEW)[14]、Extended ANEW[15]及語料庫，如：Affective Norms for English Texts (ANET) [16]、EmoBank[17]， 相關情感詞向量[18], [19]及迴歸模型[20], [21], [22], [23], [24], [25]也有相關研究發表， 均可做為財經領域維度型情感分析之研究材料。 三、研究方法 (一) 中文情感字典 本研究使用中文情感字典 CVAW (The Chinese Valence-Arousal Words )進行情感分析[16]， CVAW 是 一 個 包 含 5,512 個 情 感 多 維 度 型 字 典 ， 每 個 詞 彙 皆 包 含 一 組 二 維 的 Valence-Arousal 實數值，維度 Valence 的範圍為 1 至 9，值 1 和 9 分別表示最負面和最 正面的情緒表現，值 5 表示沒有特定傾向的中性情緒表現。維度 Arousal 的範圍為 1 至 9，值 1 和 9 分別表示平靜或激動。使用 CVAW 進行預測的實驗結果顯示，與使用英文 文本情感分析獲得的結果相當[26]。 (二)投資溫度 本研究將「投資溫度」定義為網路財經文本的情感，可作為投資大眾心態的參考。 1、財經文本資料蒐集 本研究先蒐集財經新聞做為情感分析之資料，資料來自蘋果日報、蘋果即時、工商時報、 278 The 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020) Tai"
2020.rocling-1.32,W11-2838,0,0.0615567,"Missing"
2020.rocling-1.32,W12-2006,0,0.0640657,"Missing"
2020.rocling-1.32,W13-3601,0,0.0476703,"Missing"
2020.rocling-1.32,W16-0506,0,0.0434903,"Missing"
2020.rocling-1.32,D14-1162,0,0.089668,"Missing"
2020.rocling-1.32,Q17-1010,0,0.0114955,"Missing"
2021.findings-acl.206,D19-1562,0,0.0167468,"-BERT model consisting of 12 layers of transformer encoders was implemented for comparison. ToBERT (Pappagari et al., 2019) was trained non-end2end using a word-to-segment strategy in a two-stage way. The second group includes existing methods incorporating user and product information such as NSC with user (U) and product (P) information incorporated into an attention (A) mechanism (NSC+UPA), user product neural network (UPNN) (Tang et al., 2015), hierarchical model with separate user attention and product attention (HUAPA) (Wu et al., 2018), and the chunkwise importance matrix model (CHIM) (Amplayo, 2019). The third group includes a set of BERT-based methods incorporating user and product information using different strategies, presented in Figs. 1(a)-(c). In detail, an uncased-base-BERT model first extracted fixed feature vectors from texts. Then, the BERT Concat (word) model incorporates attribute features into each word vector and stacks another 6 transformer encoders as the feature extractor. Similarly, the BERT Concat (text) incorporates attribute features into the representation of the special token [CLS] for the classification. Finally, the BERT Attention (bias) applied 6 more MA-Transf"
2021.findings-acl.206,D16-1171,0,0.134892,"low models to attach attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text re"
2021.findings-acl.206,P19-4007,0,0.0231606,"Missing"
2021.findings-acl.206,N19-1423,0,0.195549,"tion To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs were first fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to another task to be fed another smaller task-specific dataset. The abovementioned methods only use features from plain texts. Incorporating attribute information such as users and product"
2021.findings-acl.206,E17-1059,0,0.0211261,"attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text representations by adj"
2021.findings-acl.206,D17-1054,0,0.0183567,"ion to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Computational Linguistics tions between words and attributes (Wu et al., 2018; Chen et al., 2016b; Dong et al., 2017; Dou, 2017), as shown in Fig. 1(c). By using the sof tmax function for normalization to calculate the attention score, the incorporated attribute features only impact the allocation of the attention weights. As a result, the representation of input words has not been updated, and the information of these attributes will be lost. For example, depending on individual preferences for chili, readers may focus on reviews talking about spicy, but only those who like chili would consider such review recommendations useful. However, current self-attention models that learn text representations by adjusting the w"
2021.findings-acl.206,C18-1232,0,0.0116964,"Word Embeddings (d) Bilinear interaction in multi-attribute transformer Figure 1: Different strategies to incorporate external attribute knowledge into deep neural networks. Introduction To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs were first fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to"
2021.findings-acl.206,D14-1181,0,0.0222198,"ntation (a) Word representation Attention map Attr Feature Extractor Attr Attribute Embeddings Attribute Embeddings Scaled Dot-Product Attention Q Text Text K V Word Embeddings Feature Extractor (c) Bias terms in self-attention Word Embeddings (d) Bilinear interaction in multi-attribute transformer Figure 1: Different strategies to incorporate external attribute knowledge into deep neural networks. Introduction To learn a distributed text representation for sentiment classification (Pang and Lee, 2008; Liu, 2012), conventional deep neural networks, such as convolutional neural networks (CNN) (Kim, 2014) and long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and common integration technics, such as self-attention mechanisms (Vaswani et al., 2017; Chaudhari et al., 2019) and dynamic routing algorithms (Gong et al., 2018; Sabour et al., 2017), are usually applied to compose the vectors of constituent words. To further enhance the performance, pre-trained models (PTMs), such as BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLMRoBERTa (Conneau et al., 2019) can be fine-tuned and transferred for sentiment analysis tasks. Practically, PTMs wer"
2021.findings-acl.206,2021.ccl-1.108,0,0.0538441,"Missing"
2021.findings-acl.206,P15-1098,0,0.183868,"irst fed a large amount of unannotated data, and trained using a masked language model or next sentence prediction to learn the usage of various words and how the language is written in general. Then, the models are transferred to another task to be fed another smaller task-specific dataset. The abovementioned methods only use features from plain texts. Incorporating attribute information such as users and products can improve sentiment analysis task performance. Previous works typically incorporated such external knowledge by concatenating these attributes into word and text representations (Tang et al., 2015), as shown in Figs. 1(a) and (b). Such methods are often introduced in shallow models to attach attribute information to modify the representation of either words or texts. However, this may lack interaction between attributes and the text since it equally aligns words to attribute features, thus the model is unable to emphasize important tokens. Several works have used attribute features as a bias term in selfattention mechanisms to model meaningful rela2338 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2338–2343 August 1–6, 2021. ©2021 Association for Comp"
2021.rocling-1.51,P16-2037,1,0.831006,"t, Yuan Ze University 2 School of Information Science and Engineering, Yunnan University 3 Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion"
2021.rocling-1.51,D19-1343,1,0.83437,"Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as"
2021.rocling-1.51,W17-5207,0,0.0616408,"Missing"
2021.rocling-1.51,N16-1066,1,0.933056,"tive) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotion"
2021.rocling-1.51,I17-4002,1,0.863212,"n, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotional states of students (Yu et al., 2018). Dimensional sentiment analysis is an effective technique to recognize the valence"
C04-1164,1998.amta-tutorials.5,0,0.101907,"Missing"
C04-1164,P98-1116,0,0.0171832,"endent ontologies with different semantic features. Over the last few years, significant effort has been made to construct the ontology manually according to the domain expert’s knowledge. Manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive and error prone. Therefore, several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed (Noy and Musen 2000). To avoid the reiteration in ontology construction, the algorithm of ontology merging (UMLS http://umlsks.nlm.nih.gov/) (Langkilde and Knight 1998) and ontology alignment (Vossen and Peters 1997) (Weigard and Hoppenbrouwers 1998) (Asanoma 2001) were invested. The final ontology is a merged version of the original ontologies. The two original ontologies persist, with aligned links between them. Alignment usually is performed when the ontologies cover domains that are complementary to each other. In the past, domain ontology was usually constructed manually according to the knowledge or experience of the experts or ontology engineers. Recently, automatic and semi-automatic methods have been developed. OntoExtract (Fensel et al. 2002) (Miss"
C04-1164,C98-1112,0,\N,Missing
C04-1164,W97-0801,0,\N,Missing
C04-1164,W04-1110,1,\N,Missing
C08-1133,S07-1074,0,0.0319516,"Missing"
C08-1133,W06-2911,0,0.0156954,"D system evaluates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2,"
C08-1133,D07-1108,0,0.0183533,"in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the"
C08-1133,W02-0817,0,0.0470056,"Missing"
C08-1133,N06-2015,1,0.815676,"Missing"
C08-1133,S01-1004,0,0.0230452,"Missing"
C08-1133,W02-1006,0,0.0361716,"Missing"
C08-1133,W97-0207,0,0.205135,"Missing"
C08-1133,H93-1061,0,0.368997,"Missing"
C08-1133,P96-1006,0,0.206719,"Missing"
C08-1133,W04-2807,0,0.0754251,"Missing"
C08-1133,D07-1107,0,0.0332866,"Missing"
C08-1133,P07-1006,0,0.0170204,"disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the target word W0. Similarly, the multi-word n-grams include"
C08-1133,S07-1057,0,0.0240467,"luates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3)"
C08-1133,D07-1082,1,0.928022,"or management. management.02: The people in charge. The ones actually doing the managing. Management wants to start downsizing. John was promoted to Management. I spoke to their management, and they&apos;re ready to make a deal. Table 2. Example sentence for the target word management along with its sense definitions. tags and definitions for the word arm (noun sense). The OntoNotes sense tags have been used for many applications, including the SemEval2007 evaluation (Pradhan et al., 2007b), sense merging (Snow et al., 2007), sense pool verification (Yu et al., 2007), and class imbalance problems (Zhu and Hovy, 2007). In creating OntoNotes, each word sense annotation involves two annotators and an adjudicator. First, all sentences containing the target word along with its sense distinctions are presented independently to two annotators for sense annotation. If the two annotators agree on the same sense for the target word in a given sentence, then their selection is stored in the corpus. Otherwise, this sentence is double-checked by the adjudicator for the final decision. The major problem of the above annotation scheme is that only the instances where the two annotators disagreed are double-checked, whil"
C08-1133,N07-1025,0,\N,Missing
C08-1133,S07-1016,0,\N,Missing
C10-1141,P06-1057,0,0.0141471,"earsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the nearsynonyms in the set. However, near-synonyms share more common context words (features) than semantically dissimilar words in nature. Such similar contexts may decrease classifiers’ ability to disc"
C10-1141,P97-1067,0,0.266647,"thms to verify whether near-synonyms do match the given contexts. Applications can benefit from this ability to provide more effective services. For instance, a writing support system can assist users to select an alternative word that best fits a given context from a list of near-synonyms. In measuring the substitutability of words, the co-occurrence information between a target word 1254 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1254–1262, Beijing, August 2010 (the gap) and its context words is commonly used in statistical approaches. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the nearsynonym that is most typical or expected in a given context. Inkpen (2007) used the pointwise mutual information (PMI) formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus, which can alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the W"
C10-1141,U07-1007,0,0.464628,"and its context words is commonly used in statistical approaches. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the nearsynonym that is most typical or expected in a given context. Inkpen (2007) used the pointwise mutual information (PMI) formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus, which can alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Yu et al. (2007) presented a method to compute the substitution scores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direct"
C10-1141,N06-2015,0,0.0219066,"alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Yu et al. (2007) presented a method to compute the substitution scores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym su"
C10-1141,N07-1045,0,0.107541,"on Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications. For instance, they can be used for query expansion in information retrieval (IR) (Moldovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitable than “powerfu"
C10-1141,J06-2003,0,0.0274818,"us studies. 1 Introduction Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications. For instance, they can be used for query expansion in information retrieval (IR) (Moldovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitabl"
C10-1141,P98-2127,0,0.0480693,"ovan and Mihalcea, 2000; Bhogal et al., 2007), where a query term can be expanded by its nearsynonyms to improve the recall rate. They can also be used in an intelligent thesaurus that can automatically suggest alternative words to avoid repeating the same word in the composing of text when there are suitable alternatives in its synonym set (Inkpen and Hirst, 2006; Inkpen, 2007). These near-synonym sets can be derived from manually constructed dictionaries such as WordNet (called synsets) (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), or clusters derived using statistical approaches (Lin, 1998). Although the words in a near-synonym set have similar meaning, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints. Pearce (2001) presented an example of collocational constraints for the context “ coffee”. In the given near-synonym set {strong, powerful}, the word “strong” is more suitable than “powerful” to fill the gap, since “powerful coffee” is an anticollocation. Inkpen (2007) also presented several examples of collocations (e.g. ghastly mistake) and anti-collocations (e.g. ghastly error). Yu et al. (2007) described an exa"
C10-1141,W02-0816,0,0.0173461,"cores for each nearsynonym based on n-gram frequencies obtained by querying Google. A statistical test is then applied to determine whether or not a target word can be substituted by its near-synonyms. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007), where each near-synonym set corresponds to a sense pool in OntoNotes. Another direction to the task of near-synonym substitution is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the nearsynonyms in the set. However, near-synonyms share more common context words (features) than semantically dissimilar words in nature. Such similar contexts may decrease classif"
C10-1141,J90-1003,0,\N,Missing
C10-1141,C98-2122,0,\N,Missing
C12-3064,P97-1067,0,0.107122,"For instance, in (1), the word strong is more suitable than powerful in the context of “coffee”, since “powerful coffee” is an anti-collocation. These examples indicate that near-synonyms may have different usages in different contexts, and such differences are not easily captured by second language learners. Therefore, this study develops a computer-assisted near-synonym learning system to assist Chinese English-as-a-Second-Language (ESL) learners to better understand different usages of various English near-synonyms. To this end, this study exploits automatic near-synonym choice techniques (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010; Wang and Hirst, 2010; Yu et al., 2010a; 2010b; 2011) to verify whether near-synonyms match the given contexts. Figure 1 shows an example of near-synonym choice. Given a near-synonym set and a sentence containing one of the near-synonyms, the near-synonym is first removed from the sentence to form a lexical gap. The goal is to predict an answer (i.e., best near-synonym) to fill the gap from the near-synonym set according to the given context. The pointwise mutual information (PMI) (Inkpen, 2007; Gardiner and Dras, 2007), and n-gra"
C12-3064,U07-1007,0,0.0209358,"d strong is more suitable than powerful in the context of “coffee”, since “powerful coffee” is an anti-collocation. These examples indicate that near-synonyms may have different usages in different contexts, and such differences are not easily captured by second language learners. Therefore, this study develops a computer-assisted near-synonym learning system to assist Chinese English-as-a-Second-Language (ESL) learners to better understand different usages of various English near-synonyms. To this end, this study exploits automatic near-synonym choice techniques (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010; Wang and Hirst, 2010; Yu et al., 2010a; 2010b; 2011) to verify whether near-synonyms match the given contexts. Figure 1 shows an example of near-synonym choice. Given a near-synonym set and a sentence containing one of the near-synonyms, the near-synonym is first removed from the sentence to form a lexical gap. The goal is to predict an answer (i.e., best near-synonym) to fill the gap from the near-synonym set according to the given context. The pointwise mutual information (PMI) (Inkpen, 2007; Gardiner and Dras, 2007), and n-gram based methods (Islam and Inkpen, 2010"
C12-3064,W09-3735,0,0.0230838,"2: Demonstration Papers, pages 509–516, COLING 2012, Mumbai, December 2012. 509 1 Introduction Near-synonym sets represent groups of words with similar meanings, which can be derived from existing lexical ontologies such as WordNet (Fellbaum, 1998), EuroWordNet (Rodríguez et al., 1998), and Chinese WordNet (Huang et al., 2008). These are useful knowledge resources for many applications such as information retrieval (IR) (Moldovan and Mihalcea, 2000; Navigli and Velardi, 2003; Shlrl and Revle, 2006; Bhogal et al., 2007) and computer-assisted language learning (CALL) (Cheng, 2004; Inkpen, 2007; Ouyang et al., 2009; Wu et al., 2010). For instance, in CALL, near-synonyms can be used to automatically suggest alternatives to avoid repeating the same word in a text when suitable alternatives are available in its near-synonym set (Inkpen, 2007). Although the words in a near-synonym set have similar meanings, they are not necessarily interchangeable in practical use due to their specific usage and collocational constraints (Wible et al., 2003; Futagia et al., 2008). Consider the following examples. (1) {strong, powerful} coffee (2) ghastly {error, mistake} (Pearce, 2001) (Inkpen, 2007) Examples (1) and (2) bo"
C12-3064,C10-1133,0,0.0179989,"ntext of “coffee”, since “powerful coffee” is an anti-collocation. These examples indicate that near-synonyms may have different usages in different contexts, and such differences are not easily captured by second language learners. Therefore, this study develops a computer-assisted near-synonym learning system to assist Chinese English-as-a-Second-Language (ESL) learners to better understand different usages of various English near-synonyms. To this end, this study exploits automatic near-synonym choice techniques (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010; Wang and Hirst, 2010; Yu et al., 2010a; 2010b; 2011) to verify whether near-synonyms match the given contexts. Figure 1 shows an example of near-synonym choice. Given a near-synonym set and a sentence containing one of the near-synonyms, the near-synonym is first removed from the sentence to form a lexical gap. The goal is to predict an answer (i.e., best near-synonym) to fill the gap from the near-synonym set according to the given context. The pointwise mutual information (PMI) (Inkpen, 2007; Gardiner and Dras, 2007), and n-gram based methods (Islam and Inkpen, 2010; Yu et al., 2010b) are the two major approach"
C12-3064,I11-1155,1,0.877027,"Missing"
C12-3064,C10-1141,1,0.845272,"ce “powerful coffee” is an anti-collocation. These examples indicate that near-synonyms may have different usages in different contexts, and such differences are not easily captured by second language learners. Therefore, this study develops a computer-assisted near-synonym learning system to assist Chinese English-as-a-Second-Language (ESL) learners to better understand different usages of various English near-synonyms. To this end, this study exploits automatic near-synonym choice techniques (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010; Wang and Hirst, 2010; Yu et al., 2010a; 2010b; 2011) to verify whether near-synonyms match the given contexts. Figure 1 shows an example of near-synonym choice. Given a near-synonym set and a sentence containing one of the near-synonyms, the near-synonym is first removed from the sentence to form a lexical gap. The goal is to predict an answer (i.e., best near-synonym) to fill the gap from the near-synonym set according to the given context. The pointwise mutual information (PMI) (Inkpen, 2007; Gardiner and Dras, 2007), and n-gram based methods (Islam and Inkpen, 2010; Yu et al., 2010b) are the two major approaches to near-synony"
C12-3064,J90-1003,0,\N,Missing
C14-1080,P08-2006,0,0.0192293,"d1 d 2 d3 d 4 d5 w1 w2 w3 w4 Figure 2. Comparison of LSA and ICA for feature representation. users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion labels from psychiatric social texts. We cast this problem into a multi-label text classification task because a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use of concept-level features to build classifiers instead of using surface-level features such as words, ngrams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chitturi and Hansen, 2008; Li and Zong, 2008; Kessler and Schütze, 2012; Post and Bergsma, 2013; Yu et al., 2011). In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that do not frequently occur in the documents through the indirect associations between words and documents. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), is built using five document"
C14-1080,J13-3002,0,0.0141577,"for different emotion labels, and the dependence between them can also be minimized. The discriminant power of classifiers can thus be improved by training them on the independent components with minimized term overlap. Experimental results show that the use of conceptlevel features yielded better performance than the use of word-level features. Additionally, combining LSA and ICA improved the performance of using each LSA and ICA alone. 1 Introduction Sentiment analysis has been successfully applied for many applications (Picard, 1997; Pang and Lee, 2008; Calvo and D'Mello, 2010; Liu, 2012; Johansson and Moschitti, 2013; Balahur et al., 2014). Analysis of online psychiatric or mental health texts (Wu et al., 2005; Yu et al., 2009) is also an emerging field that could benefit from sentiment analysis techniques because more and more people search for help from the web when they suffered from depressive problems, which boost the development of online community-based services for Internet users to share their depressive problems with other users and health professionals. Through these services, individuals can describe their depressive symptoms via web forums and blogs. Other users or health professionals can th"
C14-1080,C12-2056,0,0.0262943,"ison of LSA and ICA for feature representation. users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion labels from psychiatric social texts. We cast this problem into a multi-label text classification task because a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use of concept-level features to build classifiers instead of using surface-level features such as words, ngrams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chitturi and Hansen, 2008; Li and Zong, 2008; Kessler and Schütze, 2012; Post and Bergsma, 2013; Yu et al., 2011). In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that do not frequently occur in the documents through the indirect associations between words and documents. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), is built using five documents with two different emotion labels Ei and Ej."
C14-1080,P08-2065,0,0.0305187,"w4 Figure 2. Comparison of LSA and ICA for feature representation. users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion labels from psychiatric social texts. We cast this problem into a multi-label text classification task because a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use of concept-level features to build classifiers instead of using surface-level features such as words, ngrams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chitturi and Hansen, 2008; Li and Zong, 2008; Kessler and Schütze, 2012; Post and Bergsma, 2013; Yu et al., 2011). In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that do not frequently occur in the documents through the indirect associations between words and documents. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), is built using five documents with two differen"
C14-1080,C08-1078,0,0.0182681,"4 d5 E E j i   d1 d 2 d3 d 4 d5 w1 w2 w3 w4 Figure 2. Comparison of LSA and ICA for feature representation. users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion labels from psychiatric social texts. We cast this problem into a multi-label text classification task because a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use of concept-level features to build classifiers instead of using surface-level features such as words, ngrams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chitturi and Hansen, 2008; Li and Zong, 2008; Kessler and Schütze, 2012; Post and Bergsma, 2013; Yu et al., 2011). In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that do not frequently occur in the documents through the indirect associations between words and documents. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), i"
C14-1080,P13-2150,0,0.0181276,"ture representation. users can learn self-help knowledge from the responses. Therefore, this study aims to identify emotion labels from psychiatric social texts. We cast this problem into a multi-label text classification task because a psychiatric social text may contain multiple emotion labels. Additionally, we propose the use of concept-level features to build classifiers instead of using surface-level features such as words, ngrams and dependency structure commonly used in the previous studies (Naughton et al., 2008; Chitturi and Hansen, 2008; Li and Zong, 2008; Kessler and Schütze, 2012; Post and Bergsma, 2013; Yu et al., 2011). In extraction of concept-level features, latent semantic analysis (LSA) (Landauer et al., 1998) has been demonstrated its effectiveness in exploring the latent structure from a collection of documents. It uses singular value decomposition (SVD) (Golub and Van Loan, 1996) to discover latent features that do not frequently occur in the documents through the indirect associations between words and documents. Figure 2 shows an example. The original matrix, as shown in Figure 2(a), is built using five documents with two different emotion labels Ei and Ej. Suppose that the words"
C14-2015,C12-1184,1,0.41187,"ign Language (EFL), support for CFL learners is relatively sparse, especially in terms of tools designed to automatically detect and correct Chinese grammatical errors. For example, while Microsoft Word has integrated robust English spelling and grammar checking functions for years, such tools for Chinese are still quite primitive. In contrast to the plethora of research related to EFL learning, relatively few studies have focused on grammar checking for CFL learners. Wu et al. (2010) proposed relative position and parse template language models to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect word-ordering errors in Chinese sentences from the HSK dynamic composition corpus. Chang et al. (2012) proposed a penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm for error diagnosis. In summary, although there are many approaches and tools to help EFL learners, the research problem described above for CFL learning is still under-explored. In addition, no common platform is available to compare different approaches and to promote the study of this important issue. This study develops a sentence judgment system using both rule-based and n"
C14-2015,C02-1049,0,0.0362478,"le-baased method, and n-gram statistical method. m 2.1 Prre-processin ng Chinese is written without w word boundaries.. As a result,, prior to thee implementaation of mosst Natural Languagge Processingg (NLP) task ks, texts musst undergo au utomatic wo ord segmentaation. Autom matic Chinese word segmenteers are generrally trained by an input lexicon and d probability models. Ho owever, it usually ssuffers from the unknown word (i.e., the out-of-v vocabulary, or o OOV) prooblem. In thiss study, a corpus-bbased learninng method is used to merg rge unknown n words to tacckle the OOV V problem (C Chen and Ma, 2002). This is foollowed by a reliable andd cost-effectiive POS-tagg ging method to label the segmented words with parts--of-speech (T Tsai and Cheen, 2004). For example, take the Chhinese senten nce “歐巴 馬是美國 國總統” (Obbama is the president p off the USA). It was segm mented and taagged in thee form of “POS:W Word” sequennce shown as follows: Nb:歐巴馬 馬 SHI:是 Nc:美國 N Naa:總統. Amo ong these words, tthe translatioon of a foreign proper naame “歐巴馬 馬” (Obama) is not likelyy to be inclu uded in a lexicon aand thereforee is extracted d by the unkknown word detection meechanism. Inn this case, th he special PO"
C14-2015,O04-2005,0,0.0729102,"Missing"
D17-1056,esuli-sebastiani-2006-sentiwordnet,0,0.0333551,"Missing"
D17-1056,N15-1184,0,0.0858873,"Missing"
D17-1056,P15-1162,0,0.00655799,"sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neighbors (i.e., those with an opposite polarity). The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and outperform previously proposed sentiment embeddings. To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) are selected, including convolutional neural networks (CNN) (Kim, 2014), deep averaging network (DAN) (Iyyer et al., 2015) and long-short term memory (LSTM) (Tai et al., 2015; Looks et al., 2017). The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed sentiment embeddings to re-run the classification for performance comparison. The SST is chosen because it can show the effect of using different word embeddings on fine-grained sentiment classification, whereas prior studies only reported binary classification results. The rest of this paper is organized as follows. Section 2 describes the proposed word vector refinement model. Section 3 presents"
D17-1056,D15-1242,0,0.0277778,"Missing"
D17-1056,D14-1181,0,0.0262736,"e closer to a set of both semantically and sentimentally similar nearest neighbors (i.e., those with the same polarity) and further away from sentimentally dissimilar neighbors (i.e., those with an opposite polarity). The proposed refinement model is evaluated by examining whether our refined embeddings can improve conventional word embeddings and outperform previously proposed sentiment embeddings. To this end, several deep neural network classifiers that performed well on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) are selected, including convolutional neural networks (CNN) (Kim, 2014), deep averaging network (DAN) (Iyyer et al., 2015) and long-short term memory (LSTM) (Tai et al., 2015; Looks et al., 2017). The conventional word embeddings used in these classifiers are then replaced by our refined versions and previously proposed sentiment embeddings to re-run the classification for performance comparison. The SST is chosen because it can show the effect of using different word embeddings on fine-grained sentiment classification, whereas prior studies only reported binary classification results. The rest of this paper is organized as follows. Section 2 describes the propos"
D17-1056,W16-0410,0,0.0339918,"Missing"
D17-1056,P13-2087,0,0.0223044,"and good-bad in (Tang et al., 2016). Composing these word vectors may produce sentence vectors with similar vector representations but opposite sentiment polarities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations). Building on such ambiguous vectors will affect sentiment classification performance. To enhance the performance of distinguishing words with similar vector representations but opposite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner (Maas et al., 2011; Labutov and Lipson, 2013; Lan et al., 2016; Ren et al., 2016; Tang et al., 2016). The common goal of these methods is to capture both semantic/syntactic and sentiment information such that sentimentally similar words have similar vector representations. They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and negative) given by the training instances. The use of such sentiment embeddings has improved the performance of binary sentiment classification. 534 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages"
D17-1056,P11-1015,0,0.0836917,"mmad et al., 2013) and good-bad in (Tang et al., 2016). Composing these word vectors may produce sentence vectors with similar vector representations but opposite sentiment polarities (e.g., a sentence containing happy and a sentence containing sad may have similar vector representations). Building on such ambiguous vectors will affect sentiment classification performance. To enhance the performance of distinguishing words with similar vector representations but opposite sentiment polarities, recent studies have suggested learning sentiment embeddings from labeled data in a supervised manner (Maas et al., 2011; Labutov and Lipson, 2013; Lan et al., 2016; Ren et al., 2016; Tang et al., 2016). The common goal of these methods is to capture both semantic/syntactic and sentiment information such that sentimentally similar words have similar vector representations. They typically apply an objective function to optimize word vectors based on the sentiment polarity labels (e.g., positive and negative) given by the training instances. The use of such sentiment embeddings has improved the performance of binary sentiment classification. 534 Proceedings of the 2017 Conference on Empirical Methods in Natural L"
D17-1056,D14-1162,0,0.109279,"rds and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST). 1 Introduction Word embeddings are a technique to learn continuous low-dimensional vector space representations of words by leveraging the contextual information from large corpora. Examples include C&W (Collobert and Weston, 2008; Collobert et al., 2011), Word2vec (Mikolov et al., 2013a; 2013b) and GloVe (Pennington et al., 2014). In addition to the contextual information, characterlevel subwords (Bojanowski et al., 2016) and semantic knowledge resources (Faruqui et al., 2015; Kiela et al., 2015) such as WordNet (Miller, 1995) are also useful information for learning word embeddings. These embeddings have been successfully used for various natural language processing tasks. In general, existing word embeddings are semantically oriented. They can capture semantic and syntactic information from unlabeled data in an unsupervised manner but fail to capture sufficient sentiment information. This makes it difficult to direc"
D17-1056,J11-2001,0,0.0298522,"er et al., 2013). It contains 13,915 words and each word is associated with a real-valued score in [1, 9] for the dimensions of valence, arousal and dominance. The valence represents the degree of positive and negative sentiment, where values of 1, 5 and 9 respectively denote most negative, neutral and most positive sentiment. In Fig. 1, good has a valence score of 7.89, which is greater than 5, and thus can be considered positive. Conversely, bad has a valence score of 3.24 and is thus negative. In addition to the E-ANEW, other lexicons such as SentiWordNet (Esuli and Fabrizio, 2006), SoCal (Taboada et al., 2011), SentiStrength (Thelwall et al., 2012), Vader (Hutto et al., 2014), ANTUSD (Wang and Ku, 2016) and SCL-NMA (Kiritchenko and Mohammad, 2016) also provide 535 real-valued sentiment intensity or strength scores like the valence scores. For each target word to be refined, the top-k semantically similar nearest neighbors are first selected and ranked in descending order of their cosine similarities. In Fig. 1, the left ranked list shows the top 10 nearest neighbors for the target word good. The semantically ranked list is then sentimentally re-ranked based on the absolute difference of the valence"
D17-1056,P15-1150,0,0.0708751,"Missing"
D17-1056,L16-1428,0,0.0266281,"[1, 9] for the dimensions of valence, arousal and dominance. The valence represents the degree of positive and negative sentiment, where values of 1, 5 and 9 respectively denote most negative, neutral and most positive sentiment. In Fig. 1, good has a valence score of 7.89, which is greater than 5, and thus can be considered positive. Conversely, bad has a valence score of 3.24 and is thus negative. In addition to the E-ANEW, other lexicons such as SentiWordNet (Esuli and Fabrizio, 2006), SoCal (Taboada et al., 2011), SentiStrength (Thelwall et al., 2012), Vader (Hutto et al., 2014), ANTUSD (Wang and Ku, 2016) and SCL-NMA (Kiritchenko and Mohammad, 2016) also provide 535 real-valued sentiment intensity or strength scores like the valence scores. For each target word to be refined, the top-k semantically similar nearest neighbors are first selected and ranked in descending order of their cosine similarities. In Fig. 1, the left ranked list shows the top 10 nearest neighbors for the target word good. The semantically ranked list is then sentimentally re-ranked based on the absolute difference of the valence scores between the target word and the words in the list. A smaller difference indicates that"
D19-1343,D14-1080,0,0.0378385,"EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are mor"
D19-1343,P15-1162,0,0.0162101,"ngton et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. As shown in Fig. 1(a), the priority for each word vector will be “fantastic reaall ris rstre this”. This prioritization seems satisfactory for this sentence, but note that the key components could appear anywhere"
D19-1343,P14-1062,0,0.0748188,"on. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these m"
D19-1343,D14-1181,0,0.0117456,"Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belon"
D19-1343,W17-0801,0,0.0511107,"Missing"
D19-1343,D14-1162,0,0.0868791,"proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015;"
D19-1343,N16-1174,0,0.0445282,"N story/NN is/VBZ (c) Capsule Tree-LSTM with Dynamic Routing is/VBZ (b) Tree-LSTM Figure 1: Illustrative examples of different LSTM models for sentiment analysis. A deeper color indicates more weight is assigned to the word according to its contribution to the prediction result. proposed method introduces a dynamic routing algorithm to consider all non-leaf nodes to build sentence vectors, instead of using the root alone in the tree-LSTM. In addition, different nodes will receive different weights according to their contributions to the prediction task. Unlike selfattention (Lin et al., 2017; Yang et al., 2016), which applies a fixed policy without considering the state of the final sentence vectors, the task of assigning weights in the proposed model is considered to be a routing issue to iteratively determine how much information can be passed from non-leaf nodes in the tree to the vector presentation of the sentence, according to the state of final output. For example, in the aforementioned example text, it would be useful for the model to emphasize fantastic that contains the most salient information, even when the word lies at the bottom of the parser tree. Based on the dynamic routing algorith"
D19-1343,N16-1066,1,0.82474,"in an update on the coupling coefficient ctj. 3 Experimental Results Datasets. This experiment used two datasets for evaluation. i) The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is used for sentiment classification. It contains 6920/872/1821 sentences for the train/dev/test sets with binary labels (positive/negative) and 8544/1101/2210 sentences with fine-grained labels (very negative/negative/ neutral/positive/very positive). ii) EmoBank (Buechel and Hahn, 2017; Buechel and Hahn, 2016) is used for sentiment regression to predict valence-arousal (VA) values (Wang et al., 2016b; Yu et al., 2016). It contains 10,000 sentences with real-valued VA ratings in the range of (1, 9), where the valence refers to the degree of positive and negative sentiment and the arousal refers to the degree of calm and excitement. The provided ratings have Reader and Writer perspectives, and the Reader was adopted as the ground-truth ratings due to its superiority reported in (Buechel and Implementation Details. Several deep neural networks were implemented for comparison, including CNN, GRU, LSTM, and tree-LSTM. For the sequential models (GRU and LSTM), we additionally implemented an enhanced version usin"
D19-1343,P15-1150,0,0.173847,"Missing"
D19-1343,P16-2037,1,0.853241,"ich will also result in an update on the coupling coefficient ctj. 3 Experimental Results Datasets. This experiment used two datasets for evaluation. i) The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is used for sentiment classification. It contains 6920/872/1821 sentences for the train/dev/test sets with binary labels (positive/negative) and 8544/1101/2210 sentences with fine-grained labels (very negative/negative/ neutral/positive/very positive). ii) EmoBank (Buechel and Hahn, 2017; Buechel and Hahn, 2016) is used for sentiment regression to predict valence-arousal (VA) values (Wang et al., 2016b; Yu et al., 2016). It contains 10,000 sentences with real-valued VA ratings in the range of (1, 9), where the valence refers to the degree of positive and negative sentiment and the arousal refers to the degree of calm and excitement. The provided ratings have Reader and Writer perspectives, and the Reader was adopted as the ground-truth ratings due to its superiority reported in (Buechel and Implementation Details. Several deep neural networks were implemented for comparison, including CNN, GRU, LSTM, and tree-LSTM. For the sequential models (GRU and LSTM), we additionally implemented an en"
D19-1343,P15-1130,0,0.0198154,"n addition, the deeper the tree structure, the bigger the improvement. 1 Introduction In sentiment analysis, word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014) and sentiment embeddings (Tang et al., 2016; Yu et al., 2018a; Yu et al., 2018b) have become a fundamental component to build deep neural networks such as convolutional neural networks (CNN) (Kalchbrenner et al., 2014; Kim, 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and long short-term memory (LSTM) (Tai et al., 2015; Wang et al., 2015). Given a variable-length text, one challenge of using these neural networks is to compose individual word vectors into sentence vectors with the same length (Iyyer et al., 2015; Joulin et al., 2016; Bojanowski et al., 2016). The sequential neural networks such as RNN, GRU, and LSTM are commonly used due to their ability to capture long-distance dependency in sequential texts. However, these methods belong to the biased model, where the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. As shown in Fig. 1(a), the priority"
I11-1155,P06-1057,0,0.0129679,"r-synonym choice. Yu et al. (2010) presented a method to compute the substitution scores for each near-synonym based on n-gram frequencies obtained by querying Google. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007; Yu et al., 2008), where each near-synonym set corresponds to a sense pool in OntoNotes. Besides the PMI and n-gram-based methods, another direction is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. 1367 3 Baseline Systems 3.2 The baseline systems used for Chinese nearsynonym choice are the PMI-based method (Inkpen, 2007; Gardiner and Dras, 2007) and the 5gram language model (Islam and Inkpen, 2010). We choose these two methods because they are commonly used in previous work. 3.1 PMI-based method The mutual information can measure the cooccurrence strength between a near-synonym"
I11-1155,P97-1067,0,0.751449,"iven contexts, previous studies have formulated the problem of near-synonym choice 1366 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1366–1370, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP English Sentence: This will make the message easier to interpret. Original word: error Near-synonym set: {error, mistake, oversight} Chinese Sentence: Original word: Near-synonym set: Figure 1. Example of the near-synonym choice evaluation for English and Chinese sentences. as the “fill-in-the-blank” (FITB) task, and evaluated on English sentences (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010). Figure 1 illustrates an example of FITB task on English and Chinese sentences. Given a near-synonym set and a sentence with one of the near-synonyms in it, the nearsynonym is removed from the sentence to form a lexical gap. The goal is to predict an answer (best near-synonym) that can fill the gap from the given near-synonym set (including the original word). An evaluation can then be performed to examine whether the involved systems can restore the original word by filling the gap with the best near-synonym. To our best knowled"
I11-1155,D08-1094,0,0.0475903,"Missing"
I11-1155,U07-1007,0,0.710334,"es have formulated the problem of near-synonym choice 1366 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1366–1370, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP English Sentence: This will make the message easier to interpret. Original word: error Near-synonym set: {error, mistake, oversight} Chinese Sentence: Original word: Near-synonym set: Figure 1. Example of the near-synonym choice evaluation for English and Chinese sentences. as the “fill-in-the-blank” (FITB) task, and evaluated on English sentences (Edmonds, 1997; Inkpen, 2007; Gardiner and Dras, 2007, Islam and Inkpen, 2010). Figure 1 illustrates an example of FITB task on English and Chinese sentences. Given a near-synonym set and a sentence with one of the near-synonyms in it, the nearsynonym is removed from the sentence to form a lexical gap. The goal is to predict an answer (best near-synonym) that can fill the gap from the given near-synonym set (including the original word). An evaluation can then be performed to examine whether the involved systems can restore the original word by filling the gap with the best near-synonym. To our best knowledge, there is no such evaluation for Chi"
I11-1155,W11-0114,0,0.0118194,"escribes the two baseline systems: pointwise mutual information (PMI) and a 5gram language model for Chinese near-synonym choice evaluation. Section 4 first introduces the Chinese near-synonym sets and test sets used in experiments, and then shows the evaluation results of the two baseline systems. Conclusions are finally drawn in Section 5. 2 Related Work In the field of lexical semantics, the contextual information is useful for representing the meaning of words, phrases, as well as sentences (Mitchell and Lapata, 2008; Erk and Pado, 2008; Thater et al., 2010; Ó Séaghdha and Korhonen, 2011; Grefenstette et al., 2011). Therefore, the co-occurrences between a target word (the gap) and its context words have been commonly used in statistical approaches to measuring the substitutability of words. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the near-synonym that is most typical or expected in a given context. Inkpen (2007) used the PMI formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus, which can alleviate the data"
I11-1155,N06-2015,0,0.0343099,"alleviate the data sparseness problem encountered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Islam and Inkpen (2010) also used the Web 1T 5-gram corpus to build a 5-gram language model for near-synonym choice. Yu et al. (2010) presented a method to compute the substitution scores for each near-synonym based on n-gram frequencies obtained by querying Google. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007; Yu et al., 2008), where each near-synonym set corresponds to a sense pool in OntoNotes. Besides the PMI and n-gram-based methods, another direction is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. 1367 3 Baseline Systems 3.2"
I11-1155,J06-2003,0,0.0285727,"ons. For instance, knowing that the word “arm” has (at least) the senses weapon and bodypart enables systems to perform word sense disambiguation. Knowing in addition the near-synonyms of a word can further improve the applications’ effectiveness. For instance, knowing that the weapon sense of “arm” corresponds to the weapon sense of “weapon” and of “arsenal” means that systems can in addition perform term expansion for information retrieval (Moldovan and Mihalcea, 2000; Bhogal et al., 2007), (near-)duplicate detection for summarization, alternative word selection for writing support systems (Inkpen and Hirst, 2006; Inkpen, 2007; Wu et al., 2010), as Example (1) and (2) present an example of collocational constraints in given contexts. In (1), the word “strong” in the near-synonym set {strong, powerful} is more suitable than “powerful” to fit the given context “coffee”, since “powerful coffee” is an anti-collocation. Similarly, in (2), “mistake” is more suitable than “error” because “ghastly mistake” is a collocation and “ghastly error” is an anti-collocation. In (3), the near-synonym set {bridge, overpass, tunnel} represents the meaning of a physical structure that connects separate places by traversin"
I11-1155,W02-0816,0,0.0276045,"ge model for near-synonym choice. Yu et al. (2010) presented a method to compute the substitution scores for each near-synonym based on n-gram frequencies obtained by querying Google. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007; Yu et al., 2008), where each near-synonym set corresponds to a sense pool in OntoNotes. Besides the PMI and n-gram-based methods, another direction is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. 1367 3 Baseline Systems 3.2 The baseline systems used for Chinese nearsynonym choice are the PMI-based method (Inkpen, 2007; Gardiner and Dras, 2007) and the 5gram language model (Islam and Inkpen, 2010). We choose these two methods because they are commonly used in previous work. 3.1 PMI-based method The mutual information can measure the cooccurrence strength b"
I11-1155,P08-1028,0,0.0369307,"In the following sections, we first present some previous work on near-synonym choice. Section 3 describes the two baseline systems: pointwise mutual information (PMI) and a 5gram language model for Chinese near-synonym choice evaluation. Section 4 first introduces the Chinese near-synonym sets and test sets used in experiments, and then shows the evaluation results of the two baseline systems. Conclusions are finally drawn in Section 5. 2 Related Work In the field of lexical semantics, the contextual information is useful for representing the meaning of words, phrases, as well as sentences (Mitchell and Lapata, 2008; Erk and Pado, 2008; Thater et al., 2010; Ó Séaghdha and Korhonen, 2011; Grefenstette et al., 2011). Therefore, the co-occurrences between a target word (the gap) and its context words have been commonly used in statistical approaches to measuring the substitutability of words. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the near-synonym that is most typical or expected in a given context. Inkpen (2007) used the PMI formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synony"
I11-1155,D11-1097,0,0.0129719,"r-synonym choice. Section 3 describes the two baseline systems: pointwise mutual information (PMI) and a 5gram language model for Chinese near-synonym choice evaluation. Section 4 first introduces the Chinese near-synonym sets and test sets used in experiments, and then shows the evaluation results of the two baseline systems. Conclusions are finally drawn in Section 5. 2 Related Work In the field of lexical semantics, the contextual information is useful for representing the meaning of words, phrases, as well as sentences (Mitchell and Lapata, 2008; Erk and Pado, 2008; Thater et al., 2010; Ó Séaghdha and Korhonen, 2011; Grefenstette et al., 2011). Therefore, the co-occurrences between a target word (the gap) and its context words have been commonly used in statistical approaches to measuring the substitutability of words. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the near-synonym that is most typical or expected in a given context. Inkpen (2007) used the PMI formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus, the Waterloo terabyte corpus,"
I11-1155,W09-3735,0,0.17687,"Missing"
I11-1155,P10-1097,0,0.0153407,"me previous work on near-synonym choice. Section 3 describes the two baseline systems: pointwise mutual information (PMI) and a 5gram language model for Chinese near-synonym choice evaluation. Section 4 first introduces the Chinese near-synonym sets and test sets used in experiments, and then shows the evaluation results of the two baseline systems. Conclusions are finally drawn in Section 5. 2 Related Work In the field of lexical semantics, the contextual information is useful for representing the meaning of words, phrases, as well as sentences (Mitchell and Lapata, 2008; Erk and Pado, 2008; Thater et al., 2010; Ó Séaghdha and Korhonen, 2011; Grefenstette et al., 2011). Therefore, the co-occurrences between a target word (the gap) and its context words have been commonly used in statistical approaches to measuring the substitutability of words. Edmonds (1997) built a lexical co-occurrence network from 1989 Wall Street Journal to determine the near-synonym that is most typical or expected in a given context. Inkpen (2007) used the PMI formula to select the best near-synonym that can fill the gap in a given context. The PMI scores for each candidate near-synonym are computed using a larger web corpus,"
I11-1155,C08-1133,1,0.855651,"countered in Edmonds’ approach. Following Inkpen’s approach, Gardiner and Dras (2007) also used the PMI formula with a different corpus (the Web 1T 5-gram corpus) to explore whether near-synonyms differ in attitude. Islam and Inkpen (2010) also used the Web 1T 5-gram corpus to build a 5-gram language model for near-synonym choice. Yu et al. (2010) presented a method to compute the substitution scores for each near-synonym based on n-gram frequencies obtained by querying Google. The dataset used in their experiments are derived from the OntoNotes copus (Hovy et al., 2006; Pradhan et al., 2007; Yu et al., 2008), where each near-synonym set corresponds to a sense pool in OntoNotes. Besides the PMI and n-gram-based methods, another direction is to identify the senses of a target word and its near-synonyms using word sense disambiguation (WSD), comparing whether they were of the same sense (McCarthy, 2002; Dagan et al., 2006). Dagan et al. (2006) described that the use of WSD is an indirect approach since it requires the intermediate sense identification step, and thus presented a sense matching technique to address the task directly. 1367 3 Baseline Systems 3.2 The baseline systems used for Chinese ne"
I11-1155,J90-1003,0,\N,Missing
I17-4002,D14-1162,0,0.0889845,"Missing"
I17-4002,W10-0208,0,0.0330003,", lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are useful resources for the developm"
I17-4002,N16-1066,1,0.779642,"e output format is “term_id, valence_rating, arousal_rating”. Below are the input/output formats of the example words “好” (good), “非常好” (very good), “滿意” (satisfy) and “不滿意” (not satisfy). Example 1: Input: 1, 好 Output: 1, 6.8, 5.2 Example 2: Input: 2, 非常好 Output: 2, 8.500, 6.625 Example 3: Input: 3, 滿意 Output: 3, 7.2, 5.6 Example 4: Input: 4, 不滿意 Output: 4, 2.813, 5.688 Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words (Yu et al, 2016b). Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth. Each multi-word phrase was rated by at least 10 different annotators. Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations. They were then excluded from the calculation of the average ratings for each phrase. The policy of this shared task was implemented as is an open test. That is, in addition to the above of"
I17-4002,P16-2037,1,0.861068,"ontact: lcyu@saturn.yzu.edu.tw, lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are usef"
I17-4002,W11-3704,0,0.0240642,"Missing"
I17-4025,K16-1017,0,0.0205954,"ll be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction. Forward and backward networks respectively capture past and future information. Therefore, we used the bi-directional LSTM f"
I17-4025,W16-0425,0,0.0197518,"ined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction. Forward and backward networks respectively capture past and future information. Therefore, we used the b"
I17-4025,D14-1080,0,0.0131989,"s (comment, request, bug, complaint, meaningless, and undetermined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction. Forward and backward networks respectively"
I17-4025,P14-1062,0,0.0112643,"ss language customer feedback into six categories (comment, request, bug, complaint, meaningless, and undetermined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction."
I17-4025,D14-1181,0,0.00337454,"ify the cross language customer feedback into six categories (comment, request, bug, complaint, meaningless, and undetermined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence i"
I17-4025,D15-1168,0,0.0126135,", complaint, meaningless, and undetermined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction. Forward and backward networks respectively capture past and fu"
I17-4025,D14-1162,0,0.0799234,"ernational Joint Conference on Natural Language Processing, Shared Tasks, pages 149–154, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP sentence as input, and the neural network as the 2 Model for Sentence Classification The model aims to classify a sentence into six labels according to its sentence texts. Figure 1 shows the framework of the bi-directional LSTM-CNN model for sentence classification. In the input layer, the sentence is transformed into a sentence matrix based on the word vector. The word vectors of vocabulary words are trained from a large corpus using the Glove (Pennington et al., 2014) toolkit. The sentence matrix is fed into a forward LSTM network and a backward LSTM network. The representation sequence is then averaged over all timesteps and then concatenated to produce the final sequence representation. The sequence is the input of the CNN layer. The CNN extract the sequence feature information followed by logistic regression layer whose target is the class label for given sentence. For a given corpus, we store the word vector in a look up matrix M ∈ R d ×|V |, where |V |is the Output Max Pooling Layer Convolutional Layer Bi-LSTM Layer Sentence Matrix vocabulary size of"
I17-4025,P15-1150,0,0.0301316,"Missing"
I17-4025,D15-1167,0,0.0147526,"ion 3 briefly introduces the ensemble method used in this paper. Section 4 reports experimental results and analysis. Section 5 presents conclusions and suggests directions for future work. xi is the word vector of word si in accordance with the look up matrix M . 2.1 Recurrent Neural Network To capture the relationship between the input sequences, we can use the RNN layer to transform word vector into the sentence feature representation. Due to the vanishing gradients and exploding gradients problem (Pascanu et al., 2012), the LSTM (a certain type of RNN) is introduced for sequence modeling (Tang et al., 2015; Tai et al., 2015). The LSTM consists of three gates: input gate i, forget gate f and output gate o. The gate mechanisms work collaboratively to learn the longterm dependencies. At each time step t, the LSTM is calculated as follows: 150 it = σ (Wi xt + U i ht −1 + bi ) f t = σ (W f xt + U f ht −1 + b f ) tanh(W g xt + U g ht −1 + bg ) g= t ot = σ (Wo xt + U o ht −1 + bo ) ct = f t ∗ ct −1 + it ∗ gt h= ot ∗ tanh(ct ) t 3 In statistics, ensemble methods use multiple learning algorithms to obtain better predictive perfor(1) mance (Maclin and Opitz, 1999; Rokach, 2010). In this paper, the ensemb"
I17-4025,P16-2037,1,0.774451,"al task for natural language processing (Collobert et al., 2011). The goal of this shared task is to classify the cross language customer feedback into six categories (comment, request, bug, complaint, meaningless, and undetermined). Each sentence will be assigned at least one tag. It can be treated as a multi-label classification problem. In recent years, deep neural network models such as convolutional neural networks (CNN) (Cun et al., 1990), recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix a"
I17-4025,D17-1056,1,0.816813,"recurrent neural networks (RNN) (Goller and Kuchler, 1996), long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), and their combinations (Wang et al., 2016) have achieved remarkable results in many NLP tasks, including sentence classification (Kim, 2014; Kalchbrenner et al., 2014), sentiment analysis (Irsoy and Cardie, 2014; Liu et al., 2015), sarcasm detection (Ghosh and Veale, 2016; Amir et al., 2016). The neural network models can automatically infer the features and can be used as a sentence classifier. The word embeddings (Mikolov et al., 2013a; 2013b; Pennington et al, 214; Yu et al., 2017) can provide word vector representation that captures semantic and syntactic information of words. The word vector is used to build the sentence matrix and then inject information into sentence classifier. The LSTM can provide the sentence sequence information in one direction. Forward and backward networks respectively capture past and future information. Therefore, we used the bi-directional LSTM for our model. This paper presents a system to classify English customer feedback into six labels (comment, request, bug, complaint, meaningless, and undetermined). The system uses the word vector o"
N16-1066,esuli-sebastiani-2006-sentiwordnet,0,0.00308504,"h, users proactively provide their feelings and opinions after browsing the web content. For example, users may read a news article and then offer comments. A user can also review the products available for sale in online stores. In the manual annotation method, trained annotators are asked to create affective annotations for specific language resources for research purposes. Several well-known affective resources are introduced as follows. SentiWordNet is a lexical resource for opinion mining, which assigns to each synset of WordNet three sentiment ratings: positive, negative, and objective (Esuli and Sebastiani, 2006). Linguistic Inquiry and Word Count (LIWC) calculates the degree to which people use different categories of words across a broad range of texts (Pennebaker et al., 2007). In the LIWC 2007 version, the annotators were asked to note their emotions and thoughts about personally relevant topics. The Affective Norms for English Words (ANEW) provides 1,034 English words with ratings in the dimensions of pleasure, arousal and dominance (Bradley and Lang, 1999). In addition to these English-language sentiment lexicons, a few Chinese lexicons have been constructed. The Chinese LIWC (C-LIWC) dictionary"
N16-1066,P14-1147,0,0.00719352,"fective states are generally represented using either categorical or dimensional approaches. The categorical approach represents affective states as several discrete classes such as positive, neutral, negative, and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various practical applications have been developed such as aspect-based sentiment analysis (Schouten and Frasincar, 2016; Pontiki et al., 2015), Twitter sentiment analysis (Saif et al., 2013; Rosenthal et al., 2015), deceptive opinion spam detection (Li et al., 2014), and cross-lingual portability (Banea et al., 2013; Xu et al., 2015). The dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement and calm. Based on this representation, any affective state can be represented as a point in the VA coordinate plane. For many application domains (e.g., product reviews, political stance detect"
N16-1066,W02-1011,0,0.044171,"WC with manual revisions to fit the practical characteristics of Chinese usages (Huang et al., 2012). The NTU Sentiment dictionary (NTUSD) has adopted a combination of manual and automatic methods to include positive and negative emotional words (Ku and Chen, 2007). Among the above affective lexicons, only ANEW is dimensional, providing realvalued scores for three dimensions, and the others are categorical, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the proc"
N16-1066,S15-2082,0,0.0179115,"Missing"
N16-1066,J11-2001,0,0.00435175,"Missing"
N16-1066,P15-2129,1,0.693208,"l, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the process of building Chinese affective resources with valence-arousal ratings, including the CVAW and CAVT. The CVAW is built on the Chinese affective lexicon C-LIWC, and then annotated with VA ratings for each word. Five annotators were trained to rate each word in the valence and arousal dimensions using the Self Assessment Manikin (SAM) model (Lang, 1980). The SAM model provides affective pictures, which c"
O05-2003,1998.amta-tutorials.5,0,0.103331,"Missing"
O05-2003,P98-1116,0,0.0370073,"Missing"
O05-2003,W04-1110,1,0.51065,"Missing"
O08-6002,W06-2911,0,0.0394,"Missing"
O08-6002,P08-1077,0,0.0275794,"Missing"
O08-6002,D07-1108,0,0.0406125,"Missing"
O08-6002,W02-0817,0,0.0522505,"Missing"
O08-6002,N06-2015,1,0.801507,"Missing"
O08-6002,S01-1004,0,0.0257832,"Missing"
O08-6002,D07-1113,1,0.854748,"Missing"
O08-6002,W02-1006,0,0.0386323,"Missing"
O08-6002,W97-0207,0,0.201173,"Missing"
O08-6002,H93-1061,0,0.364807,"Missing"
O08-6002,P96-1006,0,0.176477,"Missing"
O08-6002,W04-2807,0,0.0764947,"Missing"
O08-6002,J05-1004,0,0.101837,"Missing"
O08-6002,S07-1016,0,0.0311514,"Missing"
O08-6002,D07-1107,0,0.0337662,"Missing"
O08-6002,P07-1006,0,0.0264734,"Missing"
O08-6002,S07-1057,0,0.0583238,"Missing"
O08-6002,C04-1164,1,0.900351,"Missing"
O08-6002,P08-1089,0,0.0257891,"Missing"
O08-6002,D07-1082,1,0.893155,"Missing"
O08-6002,N07-1025,0,\N,Missing
O08-6002,S07-1074,0,\N,Missing
O10-5002,O10-5002,1,0.0513221,"Missing"
O10-5002,W06-2911,0,0.284892,"Missing"
O10-5002,D07-1108,0,0.067771,"Missing"
O10-5002,D07-1007,0,0.167846,"Missing"
O10-5002,N10-1030,0,0.232401,"Missing"
O10-5002,W02-0817,0,0.419747,"Missing"
O10-5002,P06-1057,0,0.272466,"Missing"
O10-5002,N09-1037,0,0.0867669,"Missing"
O10-5002,N06-2015,0,0.197466,"Missing"
O10-5002,W02-1006,0,0.31986,"Missing"
O10-5002,W02-0816,0,0.488792,"Missing"
O10-5002,H93-1061,0,0.177801,"Missing"
O10-5002,P96-1006,0,0.468206,"Missing"
O10-5002,W04-2807,0,0.374788,"Missing"
O10-5002,J05-1004,0,0.0606174,"Missing"
O10-5002,W11-1901,0,0.166828,"Missing"
O10-5002,D07-1107,0,0.214034,"Missing"
O10-5002,P07-1006,0,0.293679,"Missing"
O10-5002,S07-1057,0,0.24528,"Missing"
O10-5002,D07-1082,0,0.242821,"Missing"
O10-5002,S07-1074,0,\N,Missing
O10-5002,S07-1016,0,\N,Missing
O10-5002,P07-1005,0,\N,Missing
O11-2013,W03-1726,0,0.0212164,"Missing"
O11-2013,W04-1122,0,0.0486308,"Missing"
O12-1017,W09-3735,0,0.0361075,"Missing"
O12-1017,D09-1129,0,0.0421232,"Missing"
O12-1017,W09-2409,0,0.0245187,"Missing"
O12-1017,P97-1067,0,0.175432,"Missing"
O12-1017,U07-1007,0,0.0499579,"Missing"
O12-1017,W02-0816,0,0.0445698,"Missing"
O12-1017,P06-1057,0,0.0443684,"Missing"
O15-2001,W15-4401,1,0.880156,"Missing"
O15-2001,C14-2015,1,0.887239,"Missing"
O15-2001,W15-3106,1,0.873443,"Missing"
O15-2001,W13-4406,1,0.884207,"Missing"
O15-2001,C12-1184,0,0.230946,"Missing"
O15-2001,P00-1032,0,0.188091,"Missing"
O15-2001,W14-6820,1,\N,Missing
O17-1022,J06-3003,0,0.165262,"Missing"
O17-1022,N03-1032,0,0.0257239,"Missing"
O17-1022,W11-1303,0,0.0467026,"Missing"
O17-1022,D15-1291,0,0.0362563,"Missing"
O17-1022,Y12-1026,0,0.040627,"Missing"
O17-1022,Y15-2018,0,0.0279855,"Missing"
O17-1022,P09-2051,1,0.84604,"Missing"
P06-2121,P05-1046,0,0.0264135,"For some application domains, such annotated corpora may be unavailable. Therefore, we propose the use of web resources as the corpora. When facing with the web corpora, traditional corpus-based approaches may be infeasible. For example, it is impractical for health professionals to annotate the whole web corpora. Besides, it is also impractical to enumerate all possible combinations of words from the web corpora, and then search for the semantic patterns. To address the problems, we take the notion of weakly supervised (Stevenson and Greenwood, 2005) or unsupervised learning (Hasegawa, 2004; Grenager et al., 2005) to develop a framework able to bootstrap with a small set of seed patterns, and then induce more relevant patterns form the unannotated psychiatry web corpora. By this way, the reliance on annotated corpora can be significantly reduced. The proposed framework is divided into two parts: Hyperspace Analog to Language (HAL) model (Burgess et al., 1998; Bai et al., 2005), and a cascaded induction process (CIP). The HAL model, which is a cognitive motivated model, provides an informative infrastructure to make the CIP capable of learning from unannotated corpora. The CIP treats the variable-length"
P06-2121,P04-1053,0,0.0413752,"Missing"
P06-2121,P05-1047,0,0.0169164,"e corpora with annotated information to obtain more reliable parameters. For some application domains, such annotated corpora may be unavailable. Therefore, we propose the use of web resources as the corpora. When facing with the web corpora, traditional corpus-based approaches may be infeasible. For example, it is impractical for health professionals to annotate the whole web corpora. Besides, it is also impractical to enumerate all possible combinations of words from the web corpora, and then search for the semantic patterns. To address the problems, we take the notion of weakly supervised (Stevenson and Greenwood, 2005) or unsupervised learning (Hasegawa, 2004; Grenager et al., 2005) to develop a framework able to bootstrap with a small set of seed patterns, and then induce more relevant patterns form the unannotated psychiatry web corpora. By this way, the reliance on annotated corpora can be significantly reduced. The proposed framework is divided into two parts: Hyperspace Analog to Language (HAL) model (Burgess et al., 1998; Bai et al., 2005), and a cascaded induction process (CIP). The HAL model, which is a cognitive motivated model, provides an informative infrastructure to make the CIP capable of lear"
P06-2121,C04-1164,1,0.897763,"Missing"
P06-2121,M91-1033,0,\N,Missing
P06-2121,M92-1038,0,\N,Missing
P07-1129,H05-1121,0,0.0304208,"nts to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users' problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Query: Consultation Document I broke up with my boyfriend. &lt;Depressed&gt; I o"
P07-1129,C04-1164,1,0.82514,"Missing"
P09-2051,P08-2006,0,0.0175839,"e life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as <worry, children, health&gt;, <break up, boyfriend&gt;, <argue, friend&gt;, <loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, c Suntec,"
P09-2051,U06-1005,0,0.0171689,"nally, a dialog system can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance depende"
P09-2051,P08-2065,0,0.0267172,"The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order n-grams require large training data to obtain reliable estimation. For our task, the expressions of negative life events can be characterized by association language patterns, i.e., meaningful combinations of words, such as <worry, children, health&gt;, <break up, boyfriend&gt;, <argue, friend&gt;, <loss, job&gt;, and 201 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 201–204, c Suntec, Singapore, 4 August"
P09-2051,C08-1078,0,0.0312278,"tem can generate supportive responses like “Don’t worry”, “That’s really sad” and “Cheer up” if it can understand the negative life events embedded in the example sentences shown in Table 1. Therefore, this study proposes a framework for negative life event classification. We formulate this problem as a sentence classification task; that is, classify sentences according to the type of negative life events within them. The class labels used herein are presented in Table 1, which are derived from Brostedt and Pedersen (2003). Traditional approaches to sentence classification (Khoo et al., 2006; Naughton et al., 2008) or text categorization (Sebastiani 2002) usually adopt bag-of-words as baseline features to train classifiers. Since the bag-of-words approach treats each word independently without considering the relationships of words in sentences, some researchers have investigated the use of n-grams to capture sequential relations between words to boost classification performance (Chitturi and Hansen, 2008; Li and Zong, 2008). The use of ngrams is effective in capturing local dependencies of words, but tends to suffer from data sparseness problem in capturing long-distance dependencies since higher-order"
P15-2129,W10-0208,0,0.0893279,"ications rely on a handcrafted lexicon ANEW (Af788 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 788–793, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure space. 1. Two-dimensional valence-arousal fective Norms for English Words) (Bradley, 1999) which provides 1,034 English words with ratings in the dimensions of pleasure, arousal and dominance to predict the VA ratings of short and long texts (Paltoglou et al, 2013; Kim et al., 2010). Accordingly, the automatic prediction of VA ratings of affective words is a critical task in building a VA lexicon. Few studies have sought to predict the VA rating of words using regression-based methods (Wei et al., 2011; Malandrakis et al., 2011). This kind of method usually starts from a set of words with labeled VA ratings (called seeds). The VA rating of an unseen word is then estimated from semantically similar seeds. For instance, Wei et al. (2011) trained a linear regression model for each seed cluster, and then predicted the VA rating of an unseen word using the model of the cluste"
P15-2129,P14-1147,0,0.0242538,"(Pang and Lee, 2008; Calvo and D&apos;Mello, 2010; Liu, 2012; Feldman, 2013). In sentiment analysis, representation of affective states is an essential issue and can be generally divided into categorical and dimensional approaches. The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It re"
P15-2129,P07-1054,0,0.0309585,"Missing"
P15-2129,P12-1036,0,0.0195183,"o, 2010; Liu, 2012; Feldman, 2013). In sentiment analysis, representation of affective states is an essential issue and can be generally divided into categorical and dimensional approaches. The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numer"
P15-2129,N13-1123,0,0.00520985,"The categorical approach represents affective states as several discrete classes such as binary (positive and negative) and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive an"
P15-2129,E09-1077,0,0.0151091,"d seeds). The VA rating of an unseen word is then estimated from semantically similar seeds. For instance, Wei et al. (2011) trained a linear regression model for each seed cluster, and then predicted the VA rating of an unseen word using the model of the cluster to which the unseen word belongs. Malandrakis et al. (2011) used a kernel function to combine the similarity between seeds and unseen words into a linear regression model. Instead of estimating VA ratings of words, another direction is to determine the polarity (i.e., positive and negative) of words by applying the label propagation (Rao and Ravichandran, 2009; Hassan et al., 2011) and pagerank (Esuli et al., 2007) on a graph. Based on these methods, the polarity of an unseen word can be determined/ranked through its neighbor nodes (seeds). Although the pagerank algorithm has been used for polarity ranking, it can still be extended for VA prediction. Therefore, this study extends the idea of pagerank in two aspects. First, we implement pagerank for VA prediction by transforming ranking scores into VA ratings. Second, whereas pagerank assigns an equal weight to the edges connected between an unseen word and its neighbor nodes, we consider their simi"
P15-2129,J11-2001,0,0.0372614,"s, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on such a twodimensional representation, a common research goal is to determine the"
P15-2129,D14-1126,0,0.0051841,"ased on this representation, various techniques have been investigated to develop useful applications such as deceptive opinion spam detection (Li et al., 2014), aspect extraction (Mukherjee and Liu, 2012), cross-lingual portability (Banea et al., 2013; Xu et al., 2015), personalized sentiment analysis (Ren and Wu, 2013; Yu et al., 2009) and viewpoint identification (Qiu and Jiang, 2013). In addition to identifying sentiment classes, an extension has been made to further determine their sentiment strength in terms of a multi-point scale (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). The dimensional approach has drawn considerable attention in recent years as it can provide a more fine-grained sentiment analysis. It represents affective states as continuous numerical values on multiple dimensions, such as valencearousal (VA) space (Russell, 1980), as shown in Figure 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on such a twodimensional representation, a common research goal is to determine the degrees of valence and arousal of given texts such that a"
P15-2129,P11-2104,0,\N,Missing
P16-2037,D14-1080,0,0.0106674,"ds. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors fo"
P16-2037,D14-1181,0,0.00642009,") used a weighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two"
P16-2037,P14-1062,0,0.0710745,"ighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM"
P16-2037,D15-1168,0,0.0396127,"thod to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors for vocabulary words using word embedding. The regional CNN is then used"
P16-2037,D13-1170,0,0.00315544,"raining phase, a back propagation (BP) algorithm with stochastic gradient descent (SGD) is used to learn model parameters. Details of the BP algorithm can be found in (LeCun et al., 2012). 3 Experiments r 0.406 0.418 0.476 0.468 0.645 0.493 0.641 0.781* r 0.268 0.263 0.286 0.289 0.453 0.290 0.472 0.557* * Regional CNN-LSTM vs LSTM significantly different (p&lt;0.05) This section evaluates the performance of the proposed regional CNN-LSTM model against lexicon-based, regression-based, and NN-based methods. Datasets. This experiment used two affective corpora. i) Stanford Sentiment Treebank (SST) (Socher et al., 2013) contains 8,544 training texts, 2,210 test texts, and 1,101 validation texts. Each text was rated with a single dimension (valence) in the range of (0, 1). ii) Chinese ValenceArousal Texts (CVAT) (Yu et al., 2016) consists of 2,009 texts collected from social forums, manually rated with both valence and arousal dimensions in the range of (1, 9) using the SAM annotation scheme (Bradley et al. 1994). The word vectors for English and Chinese were respectively trained using the Google News and Chinese wiki dumps (zhwiki) datasets. The dimensionality for both word vectors are 300. Experimental Sett"
P16-2037,P15-1130,0,0.0223323,"a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic/geometric mean. Malandrakis et al. (2013) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts. Recently, word embedding (Mikolov et al., 2013a; Mikolov et al., 2013b) and deep neural networks (NN) such as convolutional neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Graves, 2012; Irsoy and Cardie, 2014) and long shortterm memory (LSTM) (Wang et al., 2015; Liu et al., 2015) have been successfully employed for categorical sentiment analysis. In general, CNN is capable of extracting local information but may fail to capture long-distance dependency. LSTM can address this limitation by sequentially modeling texts across sentences. Such NN-based and word embedding methods have not been well explored for dimensional sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts, regional CNN and LSTM, to predict the VA ratings of texts. We first construct word vectors for vocabulary words using word embedding. The regiona"
P16-2037,P15-2129,1,0.286062,"for Computational Linguistics, pages 225–230, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics … LSTM … L …… Sequential layer …… w1ri r Region ri w2i (xri) Max pooling layer Convolutional layer Word vector wIri Text vector r r L …… LSTM … (xrj) … w1 j r Region rj w2j wJj Text vector LSTM L …… …… … (x ) … w1rk Region rk w2rk rk wKrk Linear decoder V A Figure 1: System architecture of the proposed regional CNN-LSTM model. Research on dimensional sentiment analysis has addressed VA recognition at both the wordlevel (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015) and the sentence-level (Paltoglou et al., 2013; Malandrakis et al., 2013). At the word-level, Wei et al. (2011) used linear regression to transfer VA ratings from English affective words to Chinese words. Malandrakis et al. (2011) used a kernel function to combine the similarity between words for VA prediction. Yu et al. (2015) used a weighted graph model to iteratively determine the VA ratings of affective words. At the sentence level, Paltoglou et al. (2013) adopted a lexicon-based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using"
P16-2037,N16-1066,1,0.332709,"0.476 0.468 0.645 0.493 0.641 0.781* r 0.268 0.263 0.286 0.289 0.453 0.290 0.472 0.557* * Regional CNN-LSTM vs LSTM significantly different (p&lt;0.05) This section evaluates the performance of the proposed regional CNN-LSTM model against lexicon-based, regression-based, and NN-based methods. Datasets. This experiment used two affective corpora. i) Stanford Sentiment Treebank (SST) (Socher et al., 2013) contains 8,544 training texts, 2,210 test texts, and 1,101 validation texts. Each text was rated with a single dimension (valence) in the range of (0, 1). ii) Chinese ValenceArousal Texts (CVAT) (Yu et al., 2016) consists of 2,009 texts collected from social forums, manually rated with both valence and arousal dimensions in the range of (1, 9) using the SAM annotation scheme (Bradley et al. 1994). The word vectors for English and Chinese were respectively trained using the Google News and Chinese wiki dumps (zhwiki) datasets. The dimensionality for both word vectors are 300. Experimental Settings. Two lexicon-based methods were used for comparison: weighted arithmetic mean (wAM) and weighted geometric mean (wGM) (Paltoglou et al., 2013), along with two regression-based methods: average values regressi"
S16-1039,D14-1181,0,0.00289912,"ormed into a matrix representation. The sentence matrix is sequentially passed through a convolutional layer and max pooling layer for multi-point classification. Unlike a conventional LSTM model which directly uses word embeddings as input, the proposed model takes uses outputs from a singlelayer CNN with max pooling. 252 Convolutional Neural Network In our model, the input of the LSTM layer is an output from the CNN. CNNs have achieved the state-of-the-art results in computer vision applications, and also have been shown to be effective for various NLP applications (Krizhevsky et al., 2012; Kim, 2014; Ma et al., 2015). The CNN architecture used for our tasks is described as follows. Let V denote the vocabulary of words, while d denotes the dimensionality of word vectors, and S ∈ R d ×n denotes the sentence matrix built by concatenating the word vectors occurring in the sentences. Suppose that the sentence T is made up of a sequence of words [d1, d2, …, dn], where n is the length of sentence T. Then the representation of T is given by the matrix S T ∈ R d ×n , where the j-th column corresponds to the embeddings for word dj. Note that for batch processing we the zero-pad sentence matrix ST"
S16-1039,P15-2029,0,0.0222171,"a matrix representation. The sentence matrix is sequentially passed through a convolutional layer and max pooling layer for multi-point classification. Unlike a conventional LSTM model which directly uses word embeddings as input, the proposed model takes uses outputs from a singlelayer CNN with max pooling. 252 Convolutional Neural Network In our model, the input of the LSTM layer is an output from the CNN. CNNs have achieved the state-of-the-art results in computer vision applications, and also have been shown to be effective for various NLP applications (Krizhevsky et al., 2012; Kim, 2014; Ma et al., 2015). The CNN architecture used for our tasks is described as follows. Let V denote the vocabulary of words, while d denotes the dimensionality of word vectors, and S ∈ R d ×n denotes the sentence matrix built by concatenating the word vectors occurring in the sentences. Suppose that the sentence T is made up of a sequence of words [d1, d2, …, dn], where n is the length of sentence T. Then the representation of T is given by the matrix S T ∈ R d ×n , where the j-th column corresponds to the embeddings for word dj. Note that for batch processing we the zero-pad sentence matrix ST so that the number"
S16-1039,S16-1001,0,0.0332602,"e classes such as positive and negative polarities, or six basic emotions: anger, happiness, fear, sadness, disgust and surprise (Ekman, 1992). Based on this representation, various techniques have been investigated including supervised learning and lexicon-based approaches. Supervised learning approaches require training data for sentiment classification (Go et al., 2009; Yu et al., 2009; Saif et al., 2016), while lexicon-based approaches do not require training data but use a sentiment lexicon to determine the overall sentiment of a sentence (Liu, 2010; Hu et al., 2013). A five-point scale (Nakov et al., 2016) is also a popular way to evaluate sentiment. Many companies, such as Amazon, Google, and Alibaba all use a multi-point scale to evaluate product or APP reviews. Unlike typical classification approaches, ordinal classification can assign different ratings (e.g., very negative, negative, neutral, positive and very positive) according to sentiment strength (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). Task 4 subtask C of SemEval-2016 seeks to classify the sentiment of tweets into an ordinal five-point scale. This paper presents a system that uses word embeddings"
S16-1039,D14-1162,0,0.0787978,"Missing"
S16-1039,J11-2001,0,0.0284555,"u et al., 2009; Saif et al., 2016), while lexicon-based approaches do not require training data but use a sentiment lexicon to determine the overall sentiment of a sentence (Liu, 2010; Hu et al., 2013). A five-point scale (Nakov et al., 2016) is also a popular way to evaluate sentiment. Many companies, such as Amazon, Google, and Alibaba all use a multi-point scale to evaluate product or APP reviews. Unlike typical classification approaches, ordinal classification can assign different ratings (e.g., very negative, negative, neutral, positive and very positive) according to sentiment strength (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). Task 4 subtask C of SemEval-2016 seeks to classify the sentiment of tweets into an ordinal five-point scale. This paper presents a system that uses word embeddings (Mikolov et al., 2013) and recurrent convolutional networks to this end. The word embeddings can capture both semantic and syntactic information of words to provide a continuous vector representation of those words. These word vectors are then used to build sentence vectors through a recurrent convolutional neural network. For multi-point classification, we discretize the co"
S16-1039,D14-1126,0,0.0247024,"approaches do not require training data but use a sentiment lexicon to determine the overall sentiment of a sentence (Liu, 2010; Hu et al., 2013). A five-point scale (Nakov et al., 2016) is also a popular way to evaluate sentiment. Many companies, such as Amazon, Google, and Alibaba all use a multi-point scale to evaluate product or APP reviews. Unlike typical classification approaches, ordinal classification can assign different ratings (e.g., very negative, negative, neutral, positive and very positive) according to sentiment strength (Taboada et al., 2011; Li et al., 2011; Yu et al., 2013; Wang and Ester, 2014). Task 4 subtask C of SemEval-2016 seeks to classify the sentiment of tweets into an ordinal five-point scale. This paper presents a system that uses word embeddings (Mikolov et al., 2013) and recurrent convolutional networks to this end. The word embeddings can capture both semantic and syntactic information of words to provide a continuous vector representation of those words. These word vectors are then used to build sentence vectors through a recurrent convolutional neural network. For multi-point classification, we discretize the continuous sentiment intensity to a five partitions of equa"
S16-1039,P15-1109,0,0.0468617,"Missing"
W04-1110,W97-0801,0,\N,Missing
W12-6303,O11-2013,1,0.838508,"lligent systems (Fung and Schultz, 2008). For example, an intelligent traveling system which supports multiple language inputs and outputs can assist travelers in booking hotels, ordering in restaurants, and navigating attractions. Multinational corporations would benefit from developing automatic multilingual call centers to address customer problems 3 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–8, Tianjin, China, 20-21 DEC. 2012 Research on code-switching and multilingual language processing included applications of unknown word extraction (Wu, et al., 2011), text mining (Yang, et al., 2011; Zhang, et al., 2011), and information retrieval (Tsai, et al., 2011). Wu et al. (2011) proposed the use of mutual information and entropy to extract unknown words from codeswitched sentences. Yang et al. (2011) used selforganizing maps for multilingual document mining and navigation. Zhang et al. (2011) addressed the problem of multilingual sentence categorization and novelty mining on English, Malay, and Chinese sentences. Tsai et al. (2011) used the FRank ranking algorithm to build a merge model for multilingual information retrieval. switching language pro"
W12-6303,O09-1018,0,0.0150803,"output by the code-switching language model against the output of the non-codeswitching one to determine whether or not the test sentence is code-switched. To identify codeswitched words within the sentences, we select the n-gram with the highest probability output by the code-switching language model, and then compare it against the output of the non-code-switching one to verify whether the n-th word in the given sentence is a code-switched word. Related Work Research on code-switching speech processing mainly focuses on speech recognition and synthesis (Lyu, et al., 2008; Wu, et al., 2006; Hong, et al., 2009; Chan, et al., 2006; Qian, et al.,2009). Lyu et al. (2008) proposed a three-step data-driven phone clustering method to train an acoustic model for Mandarin, Taiwanese, and Hakka. They also discussed the issue of training with unbalanced data. Wu et al. (2006) proposed an approach to segmenting and identifying mixed-language speech utterances. They first segmented the input speech utterance into a sequence of language-dependent segments using acoustic features. The languagespecific features were then integrated in the identification process. Hong et al. (2009) developed a Mandarin-English mix"
W12-6303,O08-5006,0,0.0162017,"the probability of each test sentence output by the code-switching language model against the output of the non-codeswitching one to determine whether or not the test sentence is code-switched. To identify codeswitched words within the sentences, we select the n-gram with the highest probability output by the code-switching language model, and then compare it against the output of the non-code-switching one to verify whether the n-th word in the given sentence is a code-switched word. Related Work Research on code-switching speech processing mainly focuses on speech recognition and synthesis (Lyu, et al., 2008; Wu, et al., 2006; Hong, et al., 2009; Chan, et al., 2006; Qian, et al.,2009). Lyu et al. (2008) proposed a three-step data-driven phone clustering method to train an acoustic model for Mandarin, Taiwanese, and Hakka. They also discussed the issue of training with unbalanced data. Wu et al. (2006) proposed an approach to segmenting and identifying mixed-language speech utterances. They first segmented the input speech utterance into a sequence of language-dependent segments using acoustic features. The languagespecific features were then integrated in the identification process. Hong et al. ("
W12-6303,I11-1155,1,0.878594,"Missing"
W12-6303,W03-1705,0,0.0221935,"Missing"
W12-6335,W00-1205,0,0.221253,"language issues. Chinese parsing has been a resurged research area in recent years thanks to the commercial needs in mobile applications, and there is a pressing need for a common evaluation platform where different approaches can be fairly compared. Relevant events include the CoNLL-X (the 10th Conference on Computational Natural Language Learning, 2006) shared task, which evaluates multilingual dependency parsing techniques. This shared task provides the community with a benchmark for evaluating their parsers across different languages. The Chinese data is derived from the Sinica Treebank (Huang et al, 2000; Chen et al., 1999; Chen et al. 2003), which is regarded as the first data set designing for traditional Chinese parsing evaluation. The CoNLL 2007 shared task was the second year event devoted to dependency parsing. The task consists of two separate tasks: a multi-lingual track and a domain adaption track. The designed ideas of the shared task are motivated by the expectation that a parser should be trainable for any language, possibly by adjusting some parameters. The traditional Chinese data set can be used in this multilingual parsing evaluation. At SIGHAN Bake-offs 2012, we organize the"
W12-6335,P03-1054,0,0.0142258,"itted runs adopt a single parsing model, i.e. Single System, to accomplish the evaluated task. In Task 4-1, we received 8 submitted results, including 7 from closed track systems and 1 from an open track system. In Task 4-2, we received 4 submissions, including 3 from closed track systems and 1 from an open track system. 5.1 Analysis of sentence parsing We evaluated the sentence parsing performance of both tracks separately. Table 3 and Table 4 show the evaluated results in closed track and open track, respectively. For closed track, we implement the baseline system using the Stanford parser (Klein and Manning, 2003; Levy and Manning, 2003) with default parameters for performance comparison. We only adopt the training set to learn the Chinese parsing model. In formal testing phase, there were 75 sentences that cannot be parsed using the re-train Stanford parser. Experimental results indicate that the baseline system achieves micro-averaging and macro-averaging F1 at 0.5822 and 0.5757, respectively. Parts of the submitted runs perform better than the baseline results. Systems come from NEURun1 and NEU-Run2 achieve the best performance, i.e. 0.7078 for micro-averaging F1 and 0.7211 for macro-averaging F1."
W12-6335,I05-1016,0,0.74365,"els and their boundaries were evaluated in 202 sub-task 1, so the performance is the same. Note that the NCTU&NTUT-Run1 was submitted a few days after the formal test deadline. However, we also evaluated their results for more information. Only one team took part in the open track. The performance measures of this submission are micro-averaging F1 score: 0.4355 and macroaveraging F1 score: 0.4287. For performance ID comparison, we invited the Chinese Knowledge Information Processing Group (CKIP) in the Institute of information Science, Academia Sinica, to modify their designed Chinese parser (Yang et al. 2005; 2008; Hsieh et al. 2007) for this evaluation. The CKIP parser achieves the best microaveraging and macro-averaging F1 scores at 0.7287 and 0.7448, respectively. Task 4-1 Participants Open 1 Chaoyang University of Technology (CYUT) 2 National Central University (NCU) 3 National Chiayi University (NCYU) 4 National Chiao Tung University & National Taipei University of Technology (NCTU&NTUT) 5 6 7 8 University of Macau (UM) Northeastern University (NEU) Peking University (PKU) Japan Patent Information Organization (JAPIO) Total Task 4-2 Closed Open Closed 1 1 1 2 1 1 2 1 1 8 2 1 1 3 Table 2: Res"
W12-6335,I08-2098,0,0.0305006,"Missing"
W12-6335,P03-1056,0,0.0418737,"parsing model, i.e. Single System, to accomplish the evaluated task. In Task 4-1, we received 8 submitted results, including 7 from closed track systems and 1 from an open track system. In Task 4-2, we received 4 submissions, including 3 from closed track systems and 1 from an open track system. 5.1 Analysis of sentence parsing We evaluated the sentence parsing performance of both tracks separately. Table 3 and Table 4 show the evaluated results in closed track and open track, respectively. For closed track, we implement the baseline system using the Stanford parser (Klein and Manning, 2003; Levy and Manning, 2003) with default parameters for performance comparison. We only adopt the training set to learn the Chinese parsing model. In formal testing phase, there were 75 sentences that cannot be parsed using the re-train Stanford parser. Experimental results indicate that the baseline system achieves micro-averaging and macro-averaging F1 at 0.5822 and 0.5757, respectively. Parts of the submitted runs perform better than the baseline results. Systems come from NEURun1 and NEU-Run2 achieve the best performance, i.e. 0.7078 for micro-averaging F1 and 0.7211 for macro-averaging F1. These two runs have the s"
W12-6335,O07-4005,0,0.660252,"were evaluated in 202 sub-task 1, so the performance is the same. Note that the NCTU&NTUT-Run1 was submitted a few days after the formal test deadline. However, we also evaluated their results for more information. Only one team took part in the open track. The performance measures of this submission are micro-averaging F1 score: 0.4355 and macroaveraging F1 score: 0.4287. For performance ID comparison, we invited the Chinese Knowledge Information Processing Group (CKIP) in the Institute of information Science, Academia Sinica, to modify their designed Chinese parser (Yang et al. 2005; 2008; Hsieh et al. 2007) for this evaluation. The CKIP parser achieves the best microaveraging and macro-averaging F1 scores at 0.7287 and 0.7448, respectively. Task 4-1 Participants Open 1 Chaoyang University of Technology (CYUT) 2 National Central University (NCU) 3 National Chiayi University (NCYU) 4 National Chiao Tung University & National Taipei University of Technology (NCTU&NTUT) 5 6 7 8 University of Macau (UM) Northeastern University (NEU) Peking University (PKU) Japan Patent Information Organization (JAPIO) Total Task 4-2 Closed Open Closed 1 1 1 2 1 1 2 1 1 8 2 1 1 3 Table 2: Result submission statistics"
W12-6335,W04-1116,0,\N,Missing
W12-6335,O99-4004,0,\N,Missing
W13-4420,W10-4107,0,0.243949,"common error type in hand-writings of second-language learners. However, since it only exists in hand-writings of humans and because all characters used in computers are legal ones, it is not necessary to address this kind of spelling errors when given erroneous texts are of electronic forms. The task addressed in SIGHAN-7 is a restricted type of Substitution errors, where there exists at most one continuous error (mis-spelled) character in its context within a sentence, with only one exception in which there is a two-character error (Chen, Wu, Yang, & Ku, 2011; C.-L. Liu et al., 2010; S.-H. Wu, Chen, Yang, Ku, & Liu, 2010). This allows the system to assume that when a character is to be corrected, its adjacent characters are correct. The correction procedure is comprised of two consecutive steps: 1) Providing candidate corrections for each character in the sentence, and 2) Scoring the altered correction sentences and identifying which is the best corrected sentence (C.-H. Liu et al., 2008; C.-H. Wu et al., 2010). In this paper, a web-based measure is employed in the second step to score and identify the best correction sentence (Macias, Wong, Thangarajah, & Cavedon, 2012). This paper is organized as follows. S"
W13-4420,C10-2085,0,0.465802,"Missing"
W13-4420,W03-1726,0,0.0190327,"uracy in SIGHAN-7 Sub-Tasks 1 and 2. (4) where is the “normalized web distance” and is defined in Equation 5. , log max ||, || log |∩ | log || log min ||, || (5) where ||is the number of Wikipedia Chinese pages, which is 3,063,936 as of the time the system is implemented. It should be noted that Macias-Galindo et al.’s original work is used in English texts. Currently we have not administered any preliminary experiment to find better setups of these equations. 4 Experiments and Discussions In the proposed system, Academia Sinica’s CKIP Chinese Segmenter is used to derive segmentation results (Ma & Chen, 2003) and the language model (trigrams using Chen and Goodman’s modified Kneser-Ney discounting) is trained using SRILM with Chinese Gigaword (LDC Catalog No.: LDC2003T09) (Stolcke, 2002). In a brief summary of the results, our system did not perform well in the final test of SIGHAN-7 bakeoff. The authors would like to defend the proposed method with a major problem in the runtime of the final test. In theory, the Sub-Task 1 (Detection) NCKU&YZU-1 NTHU-3 SinicaCKIP-3 SJTU-3 NCYU-2 NCYU-3 Error Location Accuracy 0.705 0.820 0.771 0.809 0.652 0.748 Sub-Task 2 (Correction) Location Accuracy NCKU&YZU-1"
W13-4420,W09-3412,0,\N,Missing
W14-6820,W13-4406,1,0.29649,"data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques. 1 Introduction Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to"
W14-6820,W13-4418,1,0.817147,"licly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible correction"
W14-6820,W13-4414,1,0.830631,"lly similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check"
W14-6820,W13-4409,0,0.0469075,"ta sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage compo"
W14-6820,W13-4420,1,\N,Missing
W15-4401,C14-1028,0,0.0732007,"an language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical errors written by US learners (Wu et al., 2010). A penalized probabilistic first-order inductive learning algorithm has been presented for Chinese grammatical error diagnosis (Chang et al. 2012). A set of linguistic rules with syntactic information was manually crafted to detect CFL grammatical errors (Lee et al., 2013). A sentence judgment system has been 2 Task Description The goal of this shared task is to develop NLP tools for identifying the grammatical errors in sentences written by"
W15-4401,C14-2015,1,0.875633,"Missing"
W15-4401,W14-1701,0,0.204729,"Missing"
W15-4401,W13-3601,0,0.033284,"ummarizes the findings and offers futures research directions. Abstract This paper introduces the NLP-TEA 2015 shared task for Chinese grammatical error diagnosis. We describe the task, data preparation, performance metrics, and evaluation results. The hope is that such an evaluation campaign may produce more advanced Chinese grammatical error diagnosis techniques. All data sets with gold standards and evaluation tools are publicly available for research purposes. 1 Introduction Human language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical er"
W15-4401,C12-1184,0,0.156036,"All data sets with gold standards and evaluation tools are publicly available for research purposes. 1 Introduction Human language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical errors written by US learners (Wu et al., 2010). A penalized probabilistic first-order inductive learning algorithm has been presented for Chinese grammatical error diagnosis (Chang et al. 2012). A set of linguistic rules with syntactic information was manually crafted to detect CFL grammatical errors (Lee et al., 2013). A sentence judgment system has been 2 Task Descri"
W16-0513,K15-1010,0,0.0493788,"Missing"
W16-0513,P15-1068,0,0.0230547,"Missing"
W16-0513,C12-1038,0,0.0476375,"Missing"
W16-0513,P11-1092,0,0.0312142,"usions are finally drawn in Section 6. 2 Related Work Automated grammatical error detection and correction for second/foreign language learners has attracted considerable research attention. Although commercial products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram"
W16-0513,W11-2838,0,0.0349601,"best F-score of 0.6108 ranked second among the ten submissions. Our system also achieved an F-score of 0.7419 for the probabilistic estimation track, ranking fourth among the nine submissions. 1 Introduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various appl"
W16-0513,W12-2006,0,0.0264763,"ed second among the ten submissions. Our system also achieved an F-score of 0.7419 for the probabilistic estimation track, ranking fourth among the nine submissions. 1 Introduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automa"
W16-0513,W16-0506,0,0.0400992,"Missing"
W16-0513,D15-1052,0,0.0234013,"Missing"
W16-0513,W15-4401,1,0.83985,"correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automated Evaluation of Scientific Writing (AESW) shared task seeks to promote the use of NLP tools to help improve the quality of scientific writing in English by predicting whether a given sentence needs language editing or not. The AESW shared task contains two tracks: (1) a Boolean prediction track in which a sentence in need of editing will result in a binary classifier outputting true; otherwise the system should return false; and (2) a probabilistic estimation track in which the system e"
W16-0513,C14-2015,1,0.856882,"d to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et al., 2012). Relative position and parse template language models were proposed to correct grammatical errors (Wu et al., 2010). Dependency trees were used to train a language model for correcting grammati123 cal errors at the tree level (Zhang and Wang, 2014). A classification-based system and a statistical machine translation-based system were combined to improve correction quality (Susanto et al., 2014). Different from correcting grammatical errors independentl"
W16-0513,W14-1701,0,0.0295116,"Missing"
W16-0513,W13-3601,0,0.020586,"ntroduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automated Evaluation of Scientific Writing (AESW) shared task seeks to promote the use of NLP tools to help improve the quality of scientific writing in English by predicti"
W16-0513,D14-1162,0,0.080731,"of the F-score between human annotators. More recently, deep learning techniques have been widely applied to problems in natural language processing with promising results. This trend motivates us to explore convolutional neural networks to automatically evaluate scientific writing at the sentence level. 3 The NTNU-YZU System Figure 1 shows our Convolutional Neural Network (CNN) architecture for the AESW shared task. An input sentence is represented as a sequence of words. Each word refers to a row looked up in a word embedding matrix generating from Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). We use convolutions over the sentence matrix to extract the features. A single convolution layer is adopted. The sliding window is called a filter in the CNN. We obtain the full convolutions by sliding the filters over the whole Figure 1: The illustration of our convolutional neural network architecture for the AESW shared task. matrix. Each filter performs the convolution operation on the sentence matrix and generates a feature map. A pooling layer is then used to subsample features over each map. The most common approach to pooling is to apply a max operation to reduce the dimensionality f"
W16-0513,E14-1038,0,0.0143098,"ion for second/foreign language learners has attracted considerable research attention. Although commercial products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first"
W16-0513,D13-1074,0,0.0129857,"nt in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et al., 2012). Relative position and parse template language models were proposed to correct grammatical errors (Wu et al., 2010). Dependency trees were used to train a la"
W16-0513,D14-1102,0,0.0340242,"Missing"
W16-0513,P13-1143,0,0.0356618,"Missing"
W16-0513,C12-1184,0,0.030377,"al products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et"
W16-0513,D14-1033,0,0.0248309,"Missing"
W16-0513,N15-1060,0,\N,Missing
W16-4906,W11-2838,0,0.0260814,"Of the 15 teams registered for this shared task, 9 teams developed the system and submitted a total of 36 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based a"
W16-4906,W12-2006,0,0.0459908,"for this shared task, 9 teams developed the system and submitted a total of 36 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al.,"
W16-4906,W14-1701,0,0.174327,"Missing"
W16-4906,W13-3601,0,0.0358318,"cational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural La"
W16-4906,W15-4401,1,0.297001,"cations which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis again. The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results. Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://"
W16-4906,C14-2015,1,0.85477,"2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis again. The main purpose of these shared tasks is to provide a common setting"
W16-4906,C12-1184,0,0.124723,"rrors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis"
W17-5233,D14-1181,0,0.0104591,"y or degree to which an emotion is expressed in a tweet. That is, given a tweet and an emotion X, determine the intensity of emotion X felt by the speaker ranging from 0 (feeling the least amount of emotion X) to 1 (feeling the maximum amount of emotion X). The proposed system uses word embeddings (Mikolov et al., 2013a; 2013b) and a bi-directional LSTM-CNN model to complete the competition task. Word embeddings can capture both semantic and syntactic information of selected words and provide a low dimensional and continuous vector representation for them. Convoluational neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014) is effective for extracting features in texts without considering the global information of that text. Long short-term memory (LSTM) (Tai et al., 2015) can capture long-distance dependencies by sequentially modeling texts across words. The proposed bi-directional LSTM-CNN model combines LSTM and CNN to model texts, encoding global information captured by LSTM in the most principal features extracted by CNN. We first use word vectors to transform tweets into text matrices. The bi-directional LSTM is applied to these matrices to build new text matrices. CNN is applie"
W17-5233,P14-1062,0,0.0542641,"to which an emotion is expressed in a tweet. That is, given a tweet and an emotion X, determine the intensity of emotion X felt by the speaker ranging from 0 (feeling the least amount of emotion X) to 1 (feeling the maximum amount of emotion X). The proposed system uses word embeddings (Mikolov et al., 2013a; 2013b) and a bi-directional LSTM-CNN model to complete the competition task. Word embeddings can capture both semantic and syntactic information of selected words and provide a low dimensional and continuous vector representation for them. Convoluational neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014) is effective for extracting features in texts without considering the global information of that text. Long short-term memory (LSTM) (Tai et al., 2015) can capture long-distance dependencies by sequentially modeling texts across words. The proposed bi-directional LSTM-CNN model combines LSTM and CNN to model texts, encoding global information captured by LSTM in the most principal features extracted by CNN. We first use word vectors to transform tweets into text matrices. The bi-directional LSTM is applied to these matrices to build new text matrices. CNN is applied to the output of the bi-di"
W17-5233,W16-0410,0,0.0367586,"Missing"
W17-5233,P16-2037,1,0.844356,", sadness, disgust, and surprise), which have been successfully adopted in various sentiment applications (Pang and Lee 2008; Liu, 2012; Feldman, 2013). Based on this representation, application tasks focus on classification (i.e., identify 1 among n emotions for a given text). The dimensional approach provides a more fine-grained (real-valued) sentiment analysis. Knowing the intensity or degree to which an emotion is expressed in text is useful for more intelligent sentiment applications (Thelwall et al., 2012; Paltoglou et al., 2013; Malandrakis et al., 2013; Kiritchenko and Mohammad, 2016; Wang et al., 2016a; 2016b, Yu et al., 2016). The EmoInt-2017 task (Mohammad and BravoMarquez, 2017b) seeks to automatically determine a continuous numerical value representing the intensity or degree to which an emotion is expressed in a tweet. That is, given a tweet and an emotion X, determine the intensity of emotion X felt by the speaker ranging from 0 (feeling the least amount of emotion X) to 1 (feeling the maximum amount of emotion X). The proposed system uses word embeddings (Mikolov et al., 2013a; 2013b) and a bi-directional LSTM-CNN model to complete the competition task. Word embeddings can capture b"
W17-5233,S17-1007,0,0.0247529,"e local dependency to maintain the most important information for prediction. The obtained vectors are then fed to a dense layer to build a text representation. Since emotion intensity is a continuous value, a linear decoder Dense layer activation Dense layer dropout Table 2: Hyper-parameters Used layer uses a linear regression to transform the text representation into a real value. 3 Experiments and Evaluation This section evaluates the performance of the proposed bi-directional LSTM-CNN model by submitting the results to the EmoInt-2017 task. Dataset. The statistics of the official dataset (Mohammad and Bravo-Marquez, 2017a) used in this competition are summarized in Table 1. Each tweet was rated with a real-value (emotion intensity) in the range of (0, 1). Training, development and test datasets are provided for four emotions: joy, sadness, fear, and anger. We trained four models corresponding to four emotions using their respective training sets without their development sets. The anger, joy and fear models used the architecture of the proposed bi-directional LSTMCNN model. To improve results, the sadness model used the architecture of CNN model which excludes the bi-directional LSTM layer shown in Fig.1. For"
W17-5233,W17-5205,0,0.0213643,"e local dependency to maintain the most important information for prediction. The obtained vectors are then fed to a dense layer to build a text representation. Since emotion intensity is a continuous value, a linear decoder Dense layer activation Dense layer dropout Table 2: Hyper-parameters Used layer uses a linear regression to transform the text representation into a real value. 3 Experiments and Evaluation This section evaluates the performance of the proposed bi-directional LSTM-CNN model by submitting the results to the EmoInt-2017 task. Dataset. The statistics of the official dataset (Mohammad and Bravo-Marquez, 2017a) used in this competition are summarized in Table 1. Each tweet was rated with a real-value (emotion intensity) in the range of (0, 1). Training, development and test datasets are provided for four emotions: joy, sadness, fear, and anger. We trained four models corresponding to four emotions using their respective training sets without their development sets. The anger, joy and fear models used the architecture of the proposed bi-directional LSTMCNN model. To improve results, the sadness model used the architecture of CNN model which excludes the bi-directional LSTM layer shown in Fig.1. For"
W17-5233,D14-1162,0,0.0812287,"Missing"
