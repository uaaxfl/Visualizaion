2020.wmt-1.76,W19-5403,1,0.905677,"matic translation of a variety of biomedical texts. The first edition of the task (Bojar et al., 2016) focused on biomedical scientific abstracts in three language pairs. The second edition of the task offered ten language pairs and addressed scientific abstracts as well as patient-oriented health information (Jimeno Yepes et al., 2017). The third edition of the task offered six language pairs and addressed scientific abstracts (Neves et al., 2018). The fourth edition of the task offered ten language pairs. It addressed scientific abstracts and introduced the task of terminology translation (Bawden et al., 2019). This year’s edition of the task continues to address the translation of scientific abstracts and terminologies. It builds on previous tasks by offering a large range of training and test sets to support participants’ systems. The following language pairs are addressed this year: • During the construction of the test sets, and after the manual validation of the automatic alignment, we ran a pilot project for a couple of languages in which we manually finetuned the alignment of the test sets (cf. Section 2.2.3). • We ran a second pilot study in which we split the sentences according to the rep"
2020.wmt-1.76,federmann-2010-appraise,0,0.141593,"where it was sufficient to split sentences according to the Chinese punctuation (。) that marks the end of a sentence. Sentence alignment was carried out for all languages (except for zh/en) with the GMA tool using specific stopword lists for each language. For zh/en, we used the Champollion tool17 with the same configurations and stopword lists since 2018. 663 17 http://champollion.sourceforge.net/ We randomly retrieved a set of 100 abstracts for each language pair, and the automatic aligned sentences were manually validated by native speakers of the foreign languages using the Appraise tool (Federmann, 2010). Results of the validation are shown in Table 2. For the ru/en set, an additional set of 100 abstracts were randomly retrieved for a second round of manual validation. This was due to the low quality of the alignments that we obtained in the first round of validation. The official test set for ru/en was composed of the abstracts with better quality from the totality of 200 abstracts that were validated. As a pilot study this year, we performed a manual correction of the alignment which were identified as not being correct during the validation in the Appraise tool. This step was only carried"
2020.wmt-1.76,2020.wmt-1.88,0,0.0500914,"Missing"
2020.wmt-1.76,L18-1141,1,0.8847,"Missing"
2020.wmt-1.76,2020.wmt-1.89,1,0.806962,"Missing"
2020.wmt-1.76,P07-2045,0,0.00948999,"ei-Bot YerevaNN A3T3 A3T3 A3T3 A3T3 - A3 A3 A3 A3 A2 A3 A3 A1 A1 A3 - A2 A2 A1 A1 A1 - A2 A1 - A1 A2 - A2 A1 A1 A3 A3 A1 A1 A3 A1 A2 A3 A1 A2 - 6 3 1 1 9 12 9 2 3 1 1 2 5 6 4 6 2 6 2 5 Total 24 14 11 7 3 3 7 17 86 Table 4: Overview of the submissions from all teams and test sets translating from English. We identify submissions to the abstracts testsets with an “A” and to the terminology test set with a “T”. The value next to the letter indicates the number of runs for the corresponding test set, language pair, and team. using BLEU with the MULTI-EVAL v14 tool20 provided by the Moses package (Koehn et al., 2007). This means as well that we reused the tokenization approach used for Chinese. Results for MEDLINE BLEU are shown in Tables 10 and 11. 20 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/mteval-v14.pl 5.2 News The test set of our challenge was included in the News challenge data set. We identified the translations in the News files and used the same evaluation procedure as applied to MEDLINE abstracts. Results of the systems are shown in Tables 12 and 13. 666 Teams de2en es2en fr2en it2en pt2en ru2en zh2en Total ai_not_intellegent Alibuba baidu_translation Huawei United"
2020.wmt-1.76,2020.wmt-1.90,0,0.0246879,"Missing"
2020.wmt-1.76,2020.acl-main.448,0,0.0154667,"ta, teams used the training data distributed by us and many of the sources described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metrics. chrF scores are obtained using the nltk implementation.19 665 5.1 MEDLINE Similarly to previous years, we compared the submitted translations to the reference translations 19 https://www.nltk.org/_modules/nltk/ translate/chrf_score.html Team ID Institution ADAPT (Nayak et al., 2020) ai_not_intellegent Alibuba baidu_translation Elhuyar_NLP (Corral and Saralegi, 2020) Huawei United (Peng et al., 2020) Ixamed (Soto et al., 2020) LIMSI (Abdul Rauf et al., 2020) NLE nrpu-fjwu (Naz et al., 2020) one_connect"
2020.wmt-1.76,2020.wmt-1.91,0,0.05524,"Missing"
2020.wmt-1.76,W18-6403,1,0.794194,"ation for Computational Linguistics • We include a novel test set for the automatic translation of biomedical terminologies from English to Basque (cf. Section 2.2.1) matic translation of a variety of biomedical texts. The first edition of the task (Bojar et al., 2016) focused on biomedical scientific abstracts in three language pairs. The second edition of the task offered ten language pairs and addressed scientific abstracts as well as patient-oriented health information (Jimeno Yepes et al., 2017). The third edition of the task offered six language pairs and addressed scientific abstracts (Neves et al., 2018). The fourth edition of the task offered ten language pairs. It addressed scientific abstracts and introduced the task of terminology translation (Bawden et al., 2019). This year’s edition of the task continues to address the translation of scientific abstracts and terminologies. It builds on previous tasks by offering a large range of training and test sets to support participants’ systems. The following language pairs are addressed this year: • During the construction of the test sets, and after the manual validation of the automatic alignment, we ran a pilot project for a couple of language"
2020.wmt-1.76,L16-1470,1,0.865582,"Missing"
2020.wmt-1.76,W19-5333,0,0.047214,"Missing"
2020.wmt-1.76,L18-1043,1,0.851824,"d 35 seconds). All teams used transformerbased neural machine translation (except for team TRAMECAT, who used sequence2sequence) and mostly relied on existing implementations: 19 teams submitted runs using available libraries, one team submitted runs using a mix of libraries and inhouse implementations, one team submitted runs exclusively relying on their own implementation of NMT. Teams often used the same setup for a range of language pairs. Table 6 shows details about the teams methods. For in-domain data, teams used the training data distributed by us and many of the sources described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metr"
2020.wmt-1.76,P02-1040,0,0.111539,"urces described in (Névéol et al., 2018). Tables 7 and 8 provide details of the in-domain data used by the teams. For relevant language pairs, parallel data from other WMT tracks (e.g., News Task) was used. Interestingly, some teams used similarity measures based on biomedical corpora to extract additional biomedical sentences from out-of-domain corpora. Out-of-domain data was also used in the form of pre-trained base models. Table 9 shows details of the out-of-domain data used by the teams. 5 Automatic evaluation Following (Mathur et al., 2020), we used chrF (Popovi´c, 2015) as well as BLEU (Papineni et al., 2002) as automatic metrics. chrF scores are obtained using the nltk implementation.19 665 5.1 MEDLINE Similarly to previous years, we compared the submitted translations to the reference translations 19 https://www.nltk.org/_modules/nltk/ translate/chrf_score.html Team ID Institution ADAPT (Nayak et al., 2020) ai_not_intellegent Alibuba baidu_translation Elhuyar_NLP (Corral and Saralegi, 2020) Huawei United (Peng et al., 2020) Ixamed (Soto et al., 2020) LIMSI (Abdul Rauf et al., 2020) NLE nrpu-fjwu (Naz et al., 2020) one_connect_000 OOM_20 Sheffield (Soares and Vaz, 2020) TMT (Wang et al., 2020) TR"
2020.wmt-1.76,2020.wmt-1.93,0,0.0298397,"Missing"
2020.wmt-1.76,W15-3049,0,0.0407701,"Missing"
2020.wmt-1.76,2020.wmt-1.94,0,0.0324894,"Missing"
2020.wmt-1.76,L18-1546,0,0.104812,": UFAL medical and MEDLINE abstracts corpus supplied by organizers. FINE - TUNING: MEDLINE abstracts UFAL medical and MEDLINE abstracts corpus supplied by organizers. MEDLINE abstracts corpus supplied by organizers; alignment was fixed using XLM-R 29 k 34,710 No No No - 2.5M UFAL (en) 5.4M 32,466 No - Elhuyar_NLP Scielo and corpora supplied by organizers. Ixamed MEDLINE corpus supplied by organizers and TAUS Corona Crisis Corpus UNICAM TRAINING : UFAL medical, Scielo (Neves et al., 2016), and MEDLINE abstracts corpus supplied by organizers. FINE - TUNING : MEDLINE abstracts BVS, EMEA, Scielo (Soares et al., 2018) and MEDLINE Sheffield corpus supplied by organizers as well as new crawled PubMed data. The data was checked against the official test set to avoid including test data during training. TRAMECAT Biomedical translation repository, EMEA, IBECS, ICD10, Kreshmoi, MEDLINE corpus supplied by organizers, in-house MEDLINE (dated 2018), Medem glossaries, MSDManuals, Portal Clinic corpus, Scielo, SNOMED 560k 1,290,201 No No - TRAINING : 1.3M FINE - TUNING: 67K No - 2.5M No - 7,232,784 No - ADAPT - Common Crawl selected by TermFinder SNOMED descriptions, hospital notes and wikipedia medical articles (en)"
2020.wmt-1.76,2020.lrec-1.465,0,0.0448156,"Missing"
2020.wmt-1.76,2020.wmt-1.95,0,0.0268683,"Missing"
2020.wmt-1.76,2020.wmt-1.96,1,0.796163,"Missing"
2020.wmt-1.76,2020.eamt-1.61,0,0.0113521,"f a single language pair: English to German. Each of the 10 models were trained for up to two days. The training was stopped when there were no improvements on the validation dataset for more than 10 epochs, as measured through cross-validation score. The corpora we used to train the models were the same as last year – when we had baselines generated using RNN-based sequence2sequence models: the UFAL medical corpus (UFA) without the “Subtitles” subset, and as validation we again used Khreshmoi (Dušek et al., 2017). For en/it and en/ru and en/eu we used the Helsinki-NLP/opus-mt-SRC-TRG models (Tiedemann and Thottingal, 2020) included in the huggingface transformers library 18 , trained with MarianNMT on the entirety of the OPUS corpora (Tiedemann, 2012). These models are not uniformly good; they performed very well for Italian, but fairly poor for Russian and Basque. Discussion. It is interesting that the models for English to/from Italian performed so well in the biomedical task, as they were trained on generic text, not targeting the biomedical domain. It is interesting in general to what extent models that excel on generic text (e.g. news) perform well on the biomedical texts as well. 4 Teams and systems This"
2020.wmt-1.76,tiedemann-2012-parallel,0,0.0512473,"provements on the validation dataset for more than 10 epochs, as measured through cross-validation score. The corpora we used to train the models were the same as last year – when we had baselines generated using RNN-based sequence2sequence models: the UFAL medical corpus (UFA) without the “Subtitles” subset, and as validation we again used Khreshmoi (Dušek et al., 2017). For en/it and en/ru and en/eu we used the Helsinki-NLP/opus-mt-SRC-TRG models (Tiedemann and Thottingal, 2020) included in the huggingface transformers library 18 , trained with MarianNMT on the entirety of the OPUS corpora (Tiedemann, 2012). These models are not uniformly good; they performed very well for Italian, but fairly poor for Russian and Basque. Discussion. It is interesting that the models for English to/from Italian performed so well in the biomedical task, as they were trained on generic text, not targeting the biomedical domain. It is interesting in general to what extent models that excel on generic text (e.g. news) perform well on the biomedical texts as well. 4 Teams and systems This year, 22 teams submitted a total of 151 runs. Two teams withdrew after submitting their runs. The remaining teams were from China ("
2020.wmt-1.76,2020.wmt-1.97,0,0.0847722,"Missing"
C02-1112,S01-1028,1,0.901898,"follows a two-step process: 1. Choosing the representation as a set of features for the context of occurrence of the target word senses. 2. Applying a Machine Learning (ML) algorithm to train on the extracted features and tag the target word in the test examples. Current WSD systems attain high performances for coarse word sense differences (two or three senses) if enough training material is available. In contrast, the performance for finer-grained sense differences (e.g. WordNet senses as used in Senseval 2 (Preiss & Yarowsky, 2001)) is far from application needs. Nevertheless, recent work (Agirre and Martinez, 2001a) shows that it is possible to exploit the precision-coverage trade-off and build a high precision WSD system Lluís Màrquez TALP Research Center Polytechnical University of Catalonia Barcelona, Spain lluism@lsi.upc.es that tags a limited number of target words with a predefined precision. This paper explores the contribution of a broad set of syntactically motivated features that ranges from the presence of complements and adjuncts, and the detection of subcategorization frames, up to grammatical relations instantiated with specific words. The performance of the syntactic features is measured"
C02-1112,A00-1031,0,0.0122473,"al features correspond to open-class lemmas that appear in windows of different sizes around the target word. In this experiment, we used two different window-sizes: 4 lemmas around the target (coded as win_lem_4w), and the lemmas in the sentence plus the 2 previous and 2 following sentences (win_lem_2s). Local features include bigrams and trigrams (coded as big_, trig_ respectively) that contain the target word. An index (+1, -1, 0) is used to indicate the position of the target in the bigram or trigram, which can be formed by part of speech, lemmas or word forms (wf, lem, pos). We used TnT (Brants, 2000) for PoS tagging. For instance, we could extract the following features for the target word known from the sample sentence below: word form “whole” occurring in a 2 sentence window (win_wf_2s), the bigram “known widely” where target is the last word (big_wf_+1) and the trigram “RB RB N” formed by the two PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements:"
C02-1112,W01-1808,0,0.0287421,"target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First, we distinguish between direct"
C02-1112,P96-1025,0,0.0275141,"the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic information, in the Japanese tasks, several groups relied on dependency trees to extract features that were used by different models (SVM, Bayes, or vector space models). For the English tasks, the team from the University"
C02-1112,J94-4003,0,0.0603174,"Missing"
C02-1112,P93-1016,0,0.0270377,"PoS before the target word (trig_pos_+1). “There is nothing in the whole range of human experience more widely known and universally …” 4. Set of Syntactic Features. In order to extract syntactic features from the tagged examples, we needed a parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on wellestablished corpora, domain independent, and fast enough. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll & Briscoe, 2001). We installed the first two parsers, and performed a set of small experiments (John Carroll helped out running his own parser). Unfortunately, we did not have a comparative evaluation to help choosing the best. We performed a little comparative test, and all parsers looked similar. At this point we chose Minipar mainly because it was fast, easy to install and the output could be easily processed. The choice of the parser did not condition the design of the experiments (cf. section 7). From the output of the parser, we extracted different sets of features. First,"
C02-1112,P96-1006,0,0.149392,"Missing"
C02-1112,W98-0701,0,0.126818,"-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a basic set of features similar to those defined by Yarowsky, but they also use syntactic information: verb-object and subjectverb relations. The results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given. Stetina et al. (1998) achieve good results with syntactic relations as features. They use a measure of semantic distance based on WordNet to find similar features. The features are extracted using a statistical parser (Collins, 1996), and consist of the head and modifiers of each phrase. Unfortunately, they do not provide a comparison with a baseline system that would only use basic features. The Senseval-2 workshop was held in Toulouse in July 2001 (Preiss & Yarowsky, 2001). Most of the supervised systems used only a basic set of local and topical features to train their ML systems. Regarding syntactic informatio"
C02-1112,P94-1013,0,0.288745,"n WSD system based on the precision-coverage trade-off is also investigated. The paper is structured as follows. Section 2 reviews the features previously used in the literature. Section 3 defines a basic feature set based on the preceding review. Section 4 presents the syntactic features as defined in our work, alongside the parser used. In section 5 the two ML algorithms are presented, as well as the strategies for the precision-coverage trade-off. Section 6 shows the experimental setting and the results. Finally section 7 draws the conclusions and summarizes further work. 2. Previous work. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. It consisted on words appearing in a window of ±k positions around the target and bigrams and trigrams constructed with the target word. He used words, lemmas, coarse part-of-speech tags and special classes of words, such as “Weekday”. These features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word. Ng (1996) uses a"
C02-1112,C02-1013,0,\N,Missing
I08-2108,P01-1004,1,0.808695,"ally expensive. We thus adopt a cheaper scoring mechanism which normalises relative to the length of w and di,j , but ignores the length of substring matches. Namely, we use the Dice coefficient. 4.2 Tokenisation Tokenisation is particularly important in Japanese because it is a non-segmenting language with a logographic orthography (kanji). As such, we can chose to either word tokenise via a word splitter such as ChaSen, or character tokenise. Character and word tokenisation have been compared in the context of Japanese information retrieval (Fujii and Croft, 1993) and translation retrieval (Baldwin, 2001), and in both cases, characters have been found to be the superior representation overall. Orthogonal to the question of whether to tokenise into words or characters, we adopt an n-gram segment representation, in the form of simple unigrams and simple bigrams. In the case of word tokenisation and simple bigrams, e.g., example (1) would be represented as { おとなしい犬 , 犬を , を飼いたい }. 4.3 Extended Glosses The main direction in which Banerjee and Pedersen (2002) successfully extended the Lesk algorithm was in including hierarchically-adjacent glosses (i.e. hyponyms and hypernyms). We take this a step"
I08-2108,J98-1006,0,0.0481197,"is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended the Lesk method for WordNetbased WSD tasks, to include hierarchical data from the WordNet ontology (Fellbaum, 1998). They observed that the hierarchical relations significantly enhance the basic model. Both these methods will be described extensively in Section 3.1, as our approach is based on them. Other notable unsupervised and semi-supervised approaches are those of McCarthy et al. (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al. (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. Parallel corpora have also been used to avoid the need for hand-tagged data, e.g. by Chan and Ng (2005). 3 Background As background to our work, we first describe the basic and extended Lesk algorithms that form the core of our approach. Then we present the Lexeed lexical resource we have used in our experiments, and finally we outline aspects of Japanese relevant for this work. 3.1 Basic and Extended Lesk The original Lesk algorithm (Lesk, 1986) performs WSD by calculating the relative word overlap betw"
I08-2108,P04-1036,0,0.226288,"l definitions. In our experiments, we will make clear when hand-tagged sense information is being used. Unsupervised methods rely on different knowledge sources to build their models. Primarily the following types of lexical resources have been used for WSD: MRDs, lexical ontologies, and untagged corpora (monolingual corpora, second language corpora, and parallel corpora). Although early approaches focused on exploiting a single resource (Lesk, 1986), recent trends show the benefits of combining different knowledge sources, such as hierarchical relations from an ontology and untagged corpora (McCarthy et al., 2004). In this summary, we will focus on a few representative systems that make use of different resources, noting that this is an area of very active research which we cannot do true justice to within the confines of this paper. The Lesk method (Lesk, 1986) is an MRD-based system that relies on counting the overlap between the words in the target context and the dictionary definitions of the senses. In spite of its simplicity, it has been shown to be a hard baseline for unsupervised methods in Senseval, and it is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended th"
I08-2108,W03-2408,0,0.0276544,"o the knowledge sources they use to build their models. A top-level distinction is made between supervised and unsupervised systems. The former rely on training instances that have been hand-tagged, while the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexee"
I08-2108,shirai-2002-construction,0,0.48783,"the POS tag of the target word should match the word class of the word sense, and this provides a coarse-grained filter for discriminating homographs with different word classes. We also experiment with a stop word-based filter which ignores a closed set of 18 lexicographic markers commonly found in definitions (e.g. 略 [ryaku] “an abbreviation for ...”), in line with those used by Nichols et al. (2005) in inducing the ontology. 5 Evaluation We evaluate our various extensions over two datasets: (1) the example sentences in the Lexeed sensebank, and (2) the Senseval-2 Japanese dictionary task (Shirai, 2002). All results below are reported in terms of simple precision, following the conventions of Senseval evaluations. For all experiments, precision and recall are identical as our systems have full coverage. For the two datasets, we use two baselines: a random baseline and the first-sense baseline. Note that the first-sense baseline has been shown to be hard to beat for unsupervised systems (McCarthy et al., 2004), and it is considered supervised when, as in this case, the first-sense is the most frequent sense from hand-tagged corpora. 5.1 Lexeed Example Sentences The goal of these experiments i"
I08-2108,W06-0608,1,0.912613,"hile the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexeed sensebank, we investigate a number of areas of general interest to the WSD community. First, we test extensions of the Lesk algorithm (Lesk, 1986) over Japanese, focusing specifically on th"
martinez-agirre-2004-effect,mihalcea-2002-bootstrapping,0,\N,Missing
martinez-agirre-2004-effect,J98-1006,0,\N,Missing
martinez-agirre-2004-effect,agirre-de-lacalle-2004-publicly,1,\N,Missing
martinez-agirre-2004-effect,H93-1061,0,\N,Missing
martinez-agirre-2004-effect,P95-1026,0,\N,Missing
martinez-agirre-2004-effect,W00-1326,1,\N,Missing
martinez-agirre-2004-effect,W00-1702,1,\N,Missing
martinez-agirre-2004-effect,S01-1001,0,\N,Missing
N10-1002,I05-1015,0,0.0145244,"000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carroll and Oepen (2005) and Zhang et al. (2007b) have proposed efficient k-best unpacking algorithms that can selectively extract the most probable readings from the packed parse forest according to a discriminative parse disambiguation model, by minimising the number of potential unifications. The algorithm can be applied to unpack any passive edges. Because of the dynamic programming used in the algorithm and the hierarchical structure of the edges, the cost of the unpacking routine is empirically linear in the number of desired readings, and O(1) when invoked more than once on the same edge. The other challenge c"
N10-1002,W09-2609,0,0.0339964,"Missing"
N10-1002,P99-1069,0,0.0532038,"ired readings, and O(1) when invoked more than once on the same edge. The other challenge concerns the selection of informative and representative pieces of knowledge from the massive sea of partial analyses in the parsing chart. How to effectively extract the indicative features for a specific language phenomenon is a very task-specific question, as we will show in the context of the VPC extraction task in Section 3.2. However, general strategies can be applied to generate parse ranking scores on each passive edge. The most widely used parse ranking model is the loglinear model (Abney, 1997; Johnson et al., 1999; Toutanova et al., 2002). When the model does not use non-local features, the accumulated score on a sub-tree under a certain (unpacked) passive edge can be used to approximate the probability of the partial analysis conditioned on the sub-string within that span.3 3.2 The Application: Acquiring Features for VPC Extraction As stated above, the target task we use to illustrate the capabilities of our chart mining method is VPC extraction. The grammar we apply our chart mining method to in this paper is the English Resource Grammar (ERG, Flickinger (2002)), a large-scale precision HPSG for Engl"
N10-1002,P99-1061,0,0.00941937,"merican Chapter of the ACL, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be"
N10-1002,W09-1410,1,0.834536,"tures mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with precision grammars less"
N10-1002,W02-2018,0,0.0128608,"candidate VPC v - le:4, v np le:3, v p le:1, v p-np le:2 LE:M AX C ONS LE:M AX CR ANK PARTICLE off Table 1: Chart mining features used for VPC extraction S3−subjh(.875) S1−subjh(.125) S2−subjh(.925) VP5−hcomp VP1−hadj VP2−hadj(.325) VP3−hcomp v_−_le v_np_le v_p_le VP4−hcomp PP−hcomp v_p−np_le PRTL PREP NP1 NP2 DUMMY−V shows the boy 0 off 2 3 his new toys 4 7 Figure 1: Example of a parsing chart in chart-mining for VPC extraction with the ERG category classification: non-VPC, transitive VPC, or intransitive VPC. For the parameter estimation of the ME model, we use the TADM open source toolkit (Malouf, 2002). The token-level predictions are then combined with a simple majority voting to derive the type-level prediction for the VPC candidate. In the case of a tie, the method backs off to the na¨ıve baseline model described in Section 4.2, which relies on the combined probability of the verb and particle forming a VPC. We have also experimented with other ways of deriving type-level predictions from token-level classification results. For instance, we trained a separate classifier that takes the token-level prediction as input in order to determine the type-level VPC predic14 tion. Our results indi"
N10-1002,A00-2022,0,0.0296662,"que. First, there is potentially a huge number of parsing edges in the chart. For instance, when parsing with a large precision grammar like the HPSG English Resource Grammar (ERG, Flickinger (2002)), it is not unusual for a 20-word sentence to receive over 10,000 passive edges. In order to achieve high efficiency in parsing (as well as generation), ambiguity packing is usually used to reduce the number of productive passive edges on the parsing chart (Tomita, 1985). For constraint-based grammar frameworks like LFG and HPSG, subsumption-based packing is used to achieve a higher packing ratio (Oepen and Carroll, 2000), but this might also potentially lead to an inconsistent packed parse forest that does not unpack successfully. For chart mining, this means that not all passive edges are directly accessible from the chart. Some of them are packed into others, and the derivatives of the packed edges are not generated. Because of the ambiguity packing, zero or more local analyses may exist for each passive edge on the chart, and the cross-combination of the packed daughter edges is not guaranteed to be compatible. As a result, expensive unification operations must be reapplied during the unpacking phase. Carr"
N10-1002,2004.tmi-1.2,0,0.0187101,"Missing"
N10-1002,J93-1007,0,0.167573,"ston and Pullum, 2002; Baldwin and Kim, 2009); for the purposes of our dataset, we assume that all particles are prepositional—by far the most common and productive of the three types—and further restrict our attention to single-particle VPCs (i.e., we ignore VPCs such as get along together). 11 One aspect of VPCs that makes them a particularly challenging target for lexical acquisition is that the verb and particle can be non-contiguous (for instance, hand the paper in and battle right on). This sets them apart from conventional collocations and terminology (cf., Manning and Sch¨utze (1999), Smadja (1993) and McKeown and Radev (2000)) in that they cannot be captured effectively using ngrams, due to their variability in the number and type of words potentially interceding between the verb and the particle. Also, while conventional collocations generally take the form of compound nouns or adjective–noun combinations with relatively simple syntactic structure, VPCs occur with a range of valences. Furthermore, VPCs are highly productive in English and vary in use across domains, making them a prime target for lexical acquisition (Deh´e, 2002; Baldwin, 2005; Baldwin and Kim, 2009). In the VPC datas"
N10-1002,P04-1057,0,0.0524824,"Missing"
N10-1002,zhang-kordoni-2006-automated,1,0.860652,"Determine whether each verb–preposition combination is a VPC or not, and further predict its valence(s) (i.e. unknown if VPC, and unknown valence(s)) VPC Determine whether each verb–preposition combination is a VPC or not ignoring valence (i.e. unknown if VPC, and don’t care about valence) Table 2: Definitions of the three DLA tasks 2001). We use a slightly modified version of the ERG in our experiments, based on the nov-06 release. The modifications include 4 newly-added dummy lexical entries for the verb DUMMY- V and the corresponding inflectional rules, and a lexical type prediction model (Zhang and Kordoni, 2006) trained on the LOGON Treebank (Oepen et al., 2004) for unknown word handling. The parse disambiguation model we use is also trained on the LOGON Treebank. Since the parser has no access to any of the verbs under investigation (due to the DUMMYV substitution), those VPC types attested in the LOGON Treebank do not directly impact on the model’s performance. The chart mining feature extraction process took over 10 CPU days, and collected a total of 44K events for 4,090 candidate VPC triples.4 5-fold cross validation is used to train/test the model. As stated above (Section 2), the VPC triples at"
N10-1002,zhang-kordoni-2008-robust,1,0.826589,"es over unlexicalised features mined from a partial parsing chart. The proposed technique is shown to outperform a state-of-the-art parser over the target task, despite being based on relatively simplistic features. 1 Introduction Parsing with precision grammars is increasingly achieving broad coverage over open-domain texts for a range of constraint-based frameworks (e.g., TAG, LFG, HPSG and CCG), and is being used in real-world applications including information extraction, question answering, grammar checking and machine translation (Uszkoreit, 2002; Oepen et al., 2004; Frank et al., 2006; Zhang and Kordoni, 2008; MacKinlay et al., 2009). In this context, a “precision grammar” is a grammar which has been engineered to model grammaticality, and contrasts with a treebank-induced grammar, for example. Inevitably, however, such applications demand complete parsing outputs, based on the assumption that the text under investigation will be completely analysable by the grammar. As precision grammars generally make strong assumptions about complete lexical coverage and grammaticality of the input, their utility is limited over noisy or domain-specific data. This lack of complete coverage can make parsing with"
N10-1002,W07-1217,1,0.887544,"Missing"
N10-1002,W07-2207,1,0.950451,"L, pages 10–18, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics extract a list of non-compositional VPCs optionally with valence information. For comparison, we parse the same sentence set using a state-of-the-art statistical parser, and extract the VPCs from the parser output. Our results show that our chart mining method produces a model which is superior to the treebank parser. To our knowledge, the only other work that has looked at partial parsing results of precision grammars as a means of linguistic error analysis is that of Kiefer et al. (1999) and Zhang et al. (2007a), where partial parsing models were proposed to select a set of passive edges that together cover the input sequence. Compared to these approaches, our proposed chart mining technique is more general and can be adapted to specific tasks and domains. While we experiment exclusively with an HPSG grammar in this paper, it is important to note that the proposed method can be applied to any grammar formalism which is compatible with chart parsing, and where it is possible to describe an unlexicalised lexical entry for the different categories of lexical item that are to be extracted (see Section"
N10-1002,A00-2018,0,\N,Missing
N10-1002,J97-4005,0,\N,Missing
N10-1002,P03-1059,1,\N,Missing
P08-1037,J07-4002,0,0.269959,"Missing"
P08-1037,W00-1320,0,0.706722,"line Bikel parser thus represents an advancement in state-of-the-art performance. That we speciﬁcally present results for PP attachment in a parsing context is a combination of us supporting the new research direction for PP attachment established by Atterer and Sch¨utze, and us wishing to reinforce the ﬁndings of Stetina and Nagao that word sense information signiﬁcantly enhances PP attachment performance in this new setting. Lexical semantics in parsing There have been a number of attempts to incorporate word sense information into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with"
P08-1037,P05-1022,0,0.0092543,"evel of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks rela"
P08-1037,C02-1126,0,0.036289,"Missing"
P08-1037,J05-1003,0,0.0177864,"that the appropriate level of generalisation differs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing"
P08-1037,P96-1025,0,0.0195782,"information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to ha"
P08-1037,J03-4003,0,0.060359,"of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and thus likely to have the same attachment preferences. In order to"
P08-1037,P94-1016,0,0.178508,"and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to conside"
P08-1037,W07-1204,0,0.305836,". The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection"
P08-1037,W01-0521,0,0.036849,"Missing"
P08-1037,1997.iwpt-1.15,0,0.634222,"produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over a basic parse selection model in the context of the Hinoki treebank. Other notable examples of the successful incorporation of lexical semantics into parsing, not through word sense information but indirectly via selectional preferences, are Dowding et al. (1994) and Hektoen (1997). For a broader review of WSD in NLP applications, see Resnik (2006). 3 Integrating Semantics into Parsing Our approach to providing the parsers with sense information is to make available the semantic denotation of each word in the form of a semantic class. This is done simply by substituting the original words with semantic codes. For example, in the earlier example of open with a knife we could substitute both knife and scissors with the class TOOL, and thus directly facilitate semantic generalisation within the parser. There are three main aspects that we have to consider in this process:"
P08-1037,N06-2015,0,0.00853735,"Missing"
P08-1037,J98-2002,0,0.0122187,"A selection of SFs is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 co"
P08-1037,P98-2127,0,0.0468725,"ote that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet development, and also changes in WordNet from the original version used for the SemCor tagging. S YSTEM Baseline SF SFn SFv word + SF"
P08-1037,P95-1037,0,0.0185297,"that word sense information can indeed enhance the performance of syntactic disambiguation. 1 Introduction Traditionally, parse disambiguation has relied on structural features extracted from syntactic parse trees, and made only limited use of semantic information. There is both empirical evidence and linguistic intuition to indicate that semantic features can enhance parse disambiguation performance, however. For example, a number of different parsers have been shown to beneﬁt from lexicalisation, that is, the conditioning of structural features on the lexical head of the given constituent (Magerman, 1995; Collins, 1996; Charniak, 1997; Charniak, 2000; Collins, 2003). As an example of lexicalisation, we may observe in our training data that knife often occurs as the manner adjunct of open in prepositional phrases headed by with (c.f. open with a knife), which would provide strong evidence for with (a) knife attaching to open and not box in open the box with a knife. It would not, however, provide any insight into the correct attachment of with scissors in open the box with scissors, as the disambiguation model would not be able to predict that knife and scissors are semantically similar and th"
P08-1037,J93-2004,0,0.0412602,"Below, we outline these tasks. Parsing As our baseline parsers, we use two state-of-theart lexicalised parsing models, namely the Bikel parser (Bikel, 2004) and Charniak parser (Charniak, 2000). While a detailed description of the respective parsing models is beyond the scope of this paper, it is worth noting that both parsers induce a context free grammar as well as a generative parsing model from a training set of parse trees, and use a development set to tune internal parameters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the"
P08-1037,J03-4004,0,0.0329229,"is presented in Table 1 for illustration purposes. We experiment with both full synsets and SFs as instances of ﬁne-grained and coarse-grained semantic representation, respectively. As an example of the difference in these two representations, knife in its tool sense is in the EDGE TOOL USED AS A CUTTING INSTRUMENT singleton synset, and also in the ARTIFACT SF along with thousands of other words including cutter. Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al. (2005), Fujita et al. (2007)). As a hybrid representation, we tested the effect of merging words with their corresponding SF (e.g. knife+ARTIFACT ). This is a form of semantic specialisation rather than generalisation, and allows the parser to discriminate between the different senses of each word, but not generalise across words. For each of these three semantic representations, we experimented with substituting each of: (1) all open-class POSs (nouns, verbs, adjectives and adverbs), (2) nouns only, and (3) verbs only. There are thus a total of 9 combinations of representation"
P08-1037,P04-1036,0,0.0610971,"1: A selection of WordNet SFs 2. First Sense (1 ST): All token instances of a given word are tagged with their most frequent sense in WordNet.4 Note that the ﬁrst sense predictions are based largely on the same dataset as we use in our evaluation, such that the predictions are tuned to our dataset and not fully unsupervised. 3. Automatic Sense Ranking (ASR): First sense tagging as for First Sense above, except that an unsupervised system is used to automatically predict the most frequent sense for each word based on an independent corpus. The method we use to predict the ﬁrst sense is that of McCarthy et al. (2004), which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin (1998), coupled with WordNetbased similarity measures. This method is fully unsupervised and completely unreliant on any annotations from our dataset. 4 1. Gold-standard: Gold-standard annotations from SemCor. This gives us the upper bound performance of the semantic representation. 321 verbs of eating and drinking verbs of feeling verbs of seeing, hearing, feeling There are some differences with the most frequent sense in SemCor, due to extra corpora used in WordNet d"
P08-1037,N06-1020,0,0.00748381,"rs across POS and even the relative syntactic role, e.g. ﬁner-grained semantics are needed for the objects than subjects of verbs. On the other hand, the parsing strategy is very simple, as we just substitute words by their semantic class and then train statistical parsers on the transformed input. The semantic class should be an information source that the parsers take into account in addition to analysing the actual words used. Tighter integration of semantics into the parsing models, possibly in the form of discriminative reranking models (Collins and Koo, 2005; Charniak and Johnson, 2005; McClosky et al., 2006), is a promising way forward in this regard. 7 Conclusions In this work we have trained two state-of-the-art statistical parsers on semantically-enriched input, where content words have been substituted with their semantic classes. This simple method allows us to incorporate lexical semantic information into the parser, without having to reimplement a full statistical parser. We tested the two parsers in both a full parsing and a PP attachment context. This paper shows that semantic classes achieve signiﬁcant improvement both on full parsing and PP attachment tasks relative to the baseline par"
P08-1037,H94-1048,0,0.517562,"eters. Traditionally, the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. (1993)). We diverge from this norm in focusing exclusively on a sense-annotated subset of the Brown Corpus portion of the Penn Treebank, in order to investigate the upper bound performance of the models given gold-standard sense information. PP attachment in a parsing context Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP, conventionally in the form of the noun 318 or verb in a V NP PP structure (Ratnaparkhi et al., 1994; Mitchell, 2004). For instance, in I ate a pizza with anchovies, the PP with anchovies could attach either to the verb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the"
P08-1037,W97-0109,0,0.0957769,"rb (c.f. ate with anchovies) or to the noun (c.f. pizza with anchovies), of which the noun is the correct attachment site. With I ate a pizza with friends, on the other hand, the verb is the correct attachment site. PP attachment is a structural ambiguity problem, and as such, a subproblem of parsing. Traditionally the so-called RRR data (Ratnaparkhi et al., 1994) has been used to evaluate PP attachment algorithms. RRR consists of 20,081 training and 3,097 test quadruples of the form (v,n1,p,n2), where the attachment decision is either v or n1. The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classiﬁer. Their work is particularly inspiring in that it signiﬁcantly outperformed the plethora of lexicalised probabilistic models that had been proposed to that point, and has not been beaten in later attempts. In a recent paper, Atterer and Sch¨utze (2007) criticised the RRR dataset because it assumes that an oracle parser provides the two hypothesised structures to choose between. This is needed to derive the fact that there are two possible attachment sites, as well as information about the lex"
P08-1037,I05-1007,0,0.0689819,"ation into parsing tasks. The most closely related research is that of Bikel (2000), who merged the Brown portion of the Penn Treebank with SemCor (similarly to our approach in Section 4.1), and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing. He evaluated his proposed model in a parsing context both with and without WordNet-based sense information, and found that the introduction of sense information either had no impact or degraded parse performance. The only successful applications of word sense information to parsing that we are aware of are Xiong et al. (2005) and Fujita et al. (2007). Xiong et al. (2005) experimented with ﬁrst-sense and hypernym features from HowNet and CiLin (both WordNets for Chinese) in a generative parse model applied to the Chinese Penn Treebank. The combination of word sense and ﬁrst-level hypernyms produced a signiﬁcant improvement over their basic model. Fujita et al. (2007) extended this work in implementing a discriminative parse selection model incorporating word sense information mapped onto upper-level ontologies of differing depths. Based on gold-standard sense information, they achieved large-scale improvements over"
P08-1037,A00-2018,0,\N,Missing
P08-1037,J04-4004,0,\N,Missing
P08-1037,C98-2122,0,\N,Missing
S01-1028,W01-0703,1,0.881715,"Missing"
S01-1028,J01-3001,0,0.0324269,"Missing"
S01-1028,P94-1013,0,0.322503,"Missing"
S01-1028,W00-1702,1,\N,Missing
S07-1050,agirre-de-lacalle-2004-publicly,0,0.0691583,"Missing"
S07-1050,P98-2127,0,0.246172,"Missing"
S07-1050,U06-1008,1,0.898149,"Missing"
S07-1050,C98-2122,0,\N,Missing
S07-1076,P06-1013,0,0.0199303,"the integration of this method into a supervised system by different means. Thus, this paper describes both the unsupervised system (UBC-UMB-1), and the combined supervised system (UBC-UMB-2) submitted to the all-words task. Our motivation in building unsupervised systems comes from the difﬁculty of creating hand-tagged data for all words and all languages, which is colloquially known as the knowledge acquisition bottleneck. There have also been promising results in recent work on the combination of unsupervised approaches that suggest the gap with respect to supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation"
S07-1076,W06-1663,0,0.012481,"o supervised systems is narrowing (Brody et al., 2006). In this section, we will describe the standalone algorithms (three unsupervised and one supervised) and the combination schemes we explored. The unsupervised methods are based on different intuitions for disambiguation (topical features, local context, and WordNet relations), which is a desirable characteristic for combining algorithms. 2.1 Topic Signatures (TS) Topic signatures (Agirre and de Lacalle, 2004) are lists of words related to a particular sense. They can be built from a variety of sources, and be used directly to perform WSD. Cuadros and Rigau (2006) present a detailed evaluation of topic signatures built from a variety of knowledge sources. In this work we built those coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 200"
S07-1076,U06-1008,1,0.677397,"se coming from the following: • the relations in the Multilingual Central Repository (TS-MCR) • the relations in the Extended WordNet (TSXWN) In order to apply this resource for WSD, we simply measured the word-overlap between the target context and each of the senses of the target word. The sense with highest overlap is chosen as the correct sense. 350 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 350–353, c Prague, June 2007. 2007 Association for Computational Linguistics 2.2 Relatives in Context (RIC) This is an unsupervised method presented in Martinez et al. (2006). This algorithm makes use of the WordNet relatives of the target word for disambiguation. The process is carried out in these steps: (i) obtain a set of close relatives from WordNet for each sense (the relatives can be polysemous); (ii) for each test instance deﬁne all possible word sequences that include the target word; (iii) for each word sequence, substitute the target word with each relative and query a web search engine; (iv) rank queries according to the following factors: length of the query, distance of the relative to the target word, and number of hits; and (v) select the sense ass"
S07-1076,J01-3001,0,0.0361181,"nstraints. 2.5 Combination of systems We explored two approaches to combine the standalone systems. The ﬁrst consisted simply of adding up the normalized weights that each system would give to each sense. We tested this voting approach both for the unsupervised and supervised settings. The second method could only be applied in combination with the supervised kNN system. The idea was to include the unsupervised predictions as weighted features for the supervised system. We refer to this method as “stacking”, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). 3 Development experiments We tested the single algorithms and their combination over both Semcor and the training distribution of the SemEval-2007 lexical-sample subtask of task 17 (S07LS for short). The goal of these experiments was to obtain an estimate of the expected performance, and submit the most promising conﬁguration. We present ﬁrst the tests on the unsupervised setting, and then the supervised setting. It is important to note that the hand-tagged corpora was not used to ﬁne-tune the parameters of the unsupervised algorithms. 3.1 Unsupervised systems For the ﬁrst evaluation of our"
S07-1076,agirre-de-lacalle-2004-publicly,1,\N,Missing
U06-1008,N06-2036,0,0.0161641,"l method is explained in Section 4. Section 5 presents our experimental setting, and in Section 6 we report the performance of our technique and the improvement over the monosemous relatives method. Section 7 is devoted to compare our system to other unsupervised techniques and analyse the prospects for system combination. Finally, we conclude and discuss future work in Section 8. 2 Related Work The construction of unsupervised WSD systems applicable to all words in context has been the goal of many research initiatives, as can be seen in special journals and devoted books - see for instance (Agirre and Edmonds, 2006) for a recent book. We will now describe different trends that are being explored. Some recent techniques seek to alleviate the knowledge acquisition bottleneck by combining training data from different words. Kohomban and Lee (2005) build semantic classifiers by merging data from words in the same semantic class. Once the class is selected, simple heuristics are applied to obtain the fine-grained sense. The classifier fol3 Monosemous Relatives method The “monosemous relatives” approach is a technique to acquire training examples automatically and then feed them to a Machine Learning (ML) meth"
U06-1008,W04-0811,0,0.104501,"Missing"
U06-1008,mihalcea-2002-bootstrapping,0,0.505407,"(Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each relative, (iii) create a collection of training examples for each sense, and (iv) use an ML algorithm trained on the acquired collections to tag the test instances. This method has been used to bootstrap large sense-tagged corpora (Mihalcea, 2002; Agirre and Martinez, 2004). Two important shortcomings of this method are the lack of monosemous relatives for some senses of the target words, and the noise introduced by some distant relatives. In this paper we directly address those problems by developing a new method that makes use of polysemous relatives and relies on the context of the target word to reduce the presence of noisy examples. The remaining of the paper is organised as follows. In Section 2 we describe related work in this area. Section 3 briefly introduces the monosemous relatives algorithm, and our novel method is explain"
U06-1008,J96-2004,0,0.0250267,"Missing"
U06-1008,W05-0605,0,0.0646559,"Missing"
U06-1008,W06-1670,0,0.0352819,"Missing"
U06-1008,W04-0850,0,0.0528597,"Missing"
U06-1008,P05-3009,0,0.0133486,"of the different features. The church was rebuilt in the 13th century and further modifications and restoration were carried out in the 15th century. 44 We can extract different features from this context, for instance using a dependency parser. We can obtain that there is a object-verb relation between church and rebuild. Then we can incorporate this knowledge to the relative-based query and obtain training examples that are closer to our target sentence. In order to implement this approach with rich features we require tools that allow for linguistic queries, such as the linguist’s engine (Resnik and Elkiss, 2005), but other approach would be to use simple features, such as strings of words, in order to benefit directly from the examples coming from search engines in the Internet. In this paper we decided to explore the latter technique to observe the performance we can achieve with simple features. Thus, in the example above, we query the Internet with snippets such as “The cathedral was rebuilt” to retrieve training examples. We will go back to the example at the end of this section. With this method we can obtain a separate training set starting from each test instance and the pool of relatives for"
U06-1008,H05-1069,1,0.893661,"Missing"
U06-1008,S01-1004,0,0.0151229,"this paper we devised a simple algorithm to rank queries according to the three factors, but we plan to apply other techniques in the acquired training data in the future. Thus, we build a disambiguation algorithm that can be explained in the following four steps: 1. Obtain pool of relatives: for each sense of the target word we gather its synonyms, hyponyms, and hypernyms. We also take polysemous nouns, as we expect that in similar local contexts the relative will keep its related meaning. 5 Experimental setting For our experiments we relied on the lexicalsample datasets of both Senseval-2 (Kilgarriff, 2001) and Senseval-3 (Mihalcea et al., 2004). We 45 Query The nave was rebuilt in the 13th century The abbey was rebuilt in the 13th century The cathedral was rebuilt in the 13th century The Catholic Church was rebuilt in The Christian church was rebuilt The church service was The religious service was Sense 2 2 2 1 1 3 3 S2LS Word MR art 61.1 authority 22.0 bar 52.1 bum 18.8 chair 62.9 channel 28.7 child 1.6 church 62.1 circuit 52.8 day 2.2 detention 16.7 dyke 89.3 facility 26.8 fatigue 73.8 feeling 51.0 grip 8.0 hearth 37.5 holiday 7.4 lady 79.3 material 50.8 mouth 41.2 nation 80.6 nature 44.4 po"
U06-1008,W06-2007,1,0.868399,"Missing"
U06-1008,P05-1005,0,0.0189502,"system to other unsupervised techniques and analyse the prospects for system combination. Finally, we conclude and discuss future work in Section 8. 2 Related Work The construction of unsupervised WSD systems applicable to all words in context has been the goal of many research initiatives, as can be seen in special journals and devoted books - see for instance (Agirre and Edmonds, 2006) for a recent book. We will now describe different trends that are being explored. Some recent techniques seek to alleviate the knowledge acquisition bottleneck by combining training data from different words. Kohomban and Lee (2005) build semantic classifiers by merging data from words in the same semantic class. Once the class is selected, simple heuristics are applied to obtain the fine-grained sense. The classifier fol3 Monosemous Relatives method The “monosemous relatives” approach is a technique to acquire training examples automatically and then feed them to a Machine Learning (ML) method. This algorithm is based on (Leacock et al., 1998), and follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each rel"
U06-1008,J98-1006,0,0.852726,"s in foreign languages back into English and achieved good results on English WSD. Regarding portability, methods to automatically rank the senses of a word given a raw corpus, such as (McCarthy et al., 2004), have shown good flexibility to adapt to different domains, which is a desirable feature of all-words systems. We will compare the performance of the latter two systems and our approach in Section 7. English, relying on WordNet as thesaurus (Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples for each relative, (iii) create a collection of training examples for each sense, and (iv) use an ML algorithm trained on the acquired collections to tag the test instances. This method has been used to bootstrap large sense-tagged corpora (Mihalcea, 2002; Agirre and Martinez, 2004). Two important shortcomings of this method are the lack of monosemous relatives for some senses of the target words, and the nois"
U06-1008,P98-2127,0,0.0642606,"developing such resources is difficult and sometimes not feasible, which has been motivating us to explore unsupervised techniques to open up the knowledge acquisition bottleneck in WSD. The unsupervised systems that we will apply on this paper require raw corpora and a thesaurus with relations between word senses and words. Although these resources are not available for all languages, there is a growing number of WordNets in different languages that can be used1 . Other approach would be to apply methods based on distributional similarity to build a thesaurus automatically from raw corpora (Lin, 1998). The relations can then be applied in our algorithm. In this paper we have focused on the results we can obtain for The current situation for Word Sense Disambiguation (WSD) is somewhat stuck due to lack of training data. We present in this paper a novel disambiguation algorithm that improves previous systems based on acquisition of examples by incorporating local context information. With a basic configuration, our method is able to obtain state-of-the-art performance. We complemented this work by evaluating other well-known methods in the same dataset, and analysing the comparative results"
U06-1008,S01-1026,0,0.0719669,"Missing"
U06-1008,magnini-cavaglia-2000-integrating,0,0.0214303,"Missing"
U06-1008,P04-1036,0,0.276704,"aluated in the all-words task of Senseval-2. However, parallel corpora is an expensive resource to obtain for all target words. A related approach is to use monolingual corpora in a second language and use bilingual dictionaries to translate the training data (Wang and Carroll, 2005). Instead of using bilingual dictionaries, Wang and Martinez (2006) tried to apply machine translation on translating text snippets in foreign languages back into English and achieved good results on English WSD. Regarding portability, methods to automatically rank the senses of a word given a raw corpus, such as (McCarthy et al., 2004), have shown good flexibility to adapt to different domains, which is a desirable feature of all-words systems. We will compare the performance of the latter two systems and our approach in Section 7. English, relying on WordNet as thesaurus (Fellbaum, 1998). A well known approach for unsupervised WSD consists of the automatic acquisition of training data by means of monosemous relatives (Leacock et al., 1998). This technique roughly follows these steps: (i) select a set of monosemous words that are related to the different senses of the target word, (ii) query the Internet to obtain examples"
U06-1008,W04-0807,0,0.248868,"d Senseval 3 lexical sample datasets. 1 Introduction Word Sense Disambiguation (WSD) is an intermediate task that potentially can benefit many other NLP systems, from machine translation to indexing of biomedical texts. The goal of WSD is to ground the meaning of words in certain contexts into concepts as defined in some dictionary or lexical repository. Since 1998, the Senseval challenges have been serving as showcases for the state-of-the-art WSD systems. In each competition, Senseval has been growing in participants, labelling tasks, and target languages. The most recent Senseval workshop (Mihalcea et al., 2004) has again shown 1 http://www.globalwordnet.org/gwa/wordnet table.htm Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006), pages 42–50. 42 lows memory-based learning, and the examples are weighted according to their semantic similarity to the target word. Niu et al. (2005) use all-words training data to build a word-independent model to compute the similarity between two contexts. A maximum entropy algorithm is trained with the all-words corpus, and the model is used for clustering the instances of a given target word. One of the problems of clustering algorithms for W"
U06-1008,P94-1013,0,0.0752537,"emcor), or a prior algorithm like (McCarthy et al., 2004). In this paper we present the results of the basic approach that uses all the retrieved examples per sense, which is the best standalone unsupervised alternative. • Metaphors: the relative cathedral (2nd sense) appears in very different collocations that are not related to any sense of church, e.g. the cathedral of football. • Named entities: the relative kirk (2nd sense), which is a name for a Scottish church, will retrieve sentences that use Kirk as a proper noun. The ML technique Agirre and Martinez (2004) applied is Decision Lists (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its loglikelihood (see Formula 1). For this implementation, they used a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1. weight(sk , fi ) = log( P P r(sk |fi ) ) j6=k P r(sj |fi ) • Frequent words as relatives: relatives like hebraism (1st sense) could provide useful examples, but if the query is not restricted can also be the source of many noisy examples. The idea behind the “relatives in context” method is to combine local contexts of the target wor"
U06-1008,C98-2122,0,\N,Missing
U06-1008,W04-3204,1,\N,Missing
U12-1016,P05-1045,0,0.113548,"Missing"
U12-1016,C00-2137,0,0.0415274,"tures boosts accuracy. Ultimately, the best-performing classiﬁer utilised the top result from both DBpedia and GeoNames, using the bag-of-toponyms and topresult frequency features, achieving an accuracy of 0.892, well above the accuracy of both the majority class baseline at 0.415 and the simple bagof-words classiﬁer at 0.729, and only slightly below the human-based upper bound of 0.969. The difference between this best-performing SVM classifer and the majority vote classiﬁer of the same toponym resolution approach was found to be statistically signiﬁcant (p = .001) using randomization tests (Yeh, 2000). 5 Conclusion and Future Work We have demonstrated that NLP approaches paired with toponym resolution are highly successful at identifying the study region from the abstracts of publications within the environmental science domain, with our best classiﬁer achieving an accuracy of 0.892, compared to a human-based upper bound of 0.969. Possible future work could include weighting of different toponym granularities, exploiting geo-spatial relationships between identiﬁed toponyms, and domain-adapting a NER for the environmental sciences. Acknowledgments NICTA is funded by the Australian Governmen"
U12-1017,I08-1050,0,0.0838659,"nt of the state-of-the-art system presented by (Kim et al., 2011), using a machine learning algorithm for predictions. 2.1 Naive Baseline For the naive baseline we merely rely on the most frequent label occurring in the training data, given the position of a sentence. For instance, for the ﬁrst four sentences in the abstract the most frequent label is Background, for the ﬁfth it is Other, etc. 2.2 Conditional Random Field (CRF) Benchmark CRFs (Lafferty et al., 2001) were designed to label sequential data, and we chose this approach because it has shown success in sentence-level classiﬁcation (Hirohata et al., 2008; Chung, 2009; Kim et al., 2011). Thus we tried to replicate the classiﬁer used by (Kim et al., 2011). However our systems differ in the selection of features used for training. We use lexical and structural features: 1. Lexical features: bag of words and Part Of Speech (POS) tags for the lexical features; and 2. Structural features: position of the sentences and the rhetorical headings from the structured abstracts. If a heading h1 covered three lines in the abstract, all the three lines will be labeled as h1. We used NLTK (Bird et al., 2009) to produce a list of POS tags and for the CRF clas"
U12-1017,U12-1019,0,0.12011,"Missing"
U12-1017,C00-2137,0,\N,Missing
U12-1017,U12-1020,1,\N,Missing
U13-1018,N10-1124,0,0.0238308,"or moving towards an evidence-based model of environmental management have obvious parallels to the motivation for the practice of EBM. Although the structure of evidence will differ between the domains, many of the techniques applied in research for EBM are likely to have application for our current task. Successful applications of NLP to EBM include sentence categorization for information on randomized controlled trials (Chung, 2009; Kim et al., 2011), the labelling of sentences with “PICO” (Patient/Problem, Intervention, Comparison and Outcome) labels to aid clinical information retrieval (Boudin et al., 2010), and the automatic assignment of Medical Subject Headings (MeSH) terms to PubMed abstracts (Gaudinat and Boyer, 2002). 3 Resources In this section, we provide details of key resources used in this paper, namely: • Eco Evidence, a manually-curated database of metadata for environmental science literature, which provides the basis of the data used in our experiments • DBpedia and GeoNames, as resources for toponym resolution • the K¨oppen-Geiger Climate Map of Peel et al. (2007) 124 3.1 Eco Evidence Eco Evidence (Webb et al., 2011) is a tool for literature review and evidence synthesis, consist"
U13-1018,P05-1045,0,0.00890167,"5 21 0 39 13 0 236 NA 1055 51 162 5 278 102 1 1654 SA 9 98 7 1 3 1 0 119 OC 98 10 0 1 0 1 0 110 MU 13 7 1 2 1 113 0 137 OT 1 1 0 0 2 1 0 5 T OTAL 2478 380 356 12 463 286 2 3977 Table 1: Distribution for the gold standard climate classifications across the gold standard study region classifications (EU = Europe, AU = Australia, AF = Africa, AN = Antarctica, AS = Asia, NA = North America, SA = South America, OC = Oceania [other than Australia], MU = Multiple, OT = Other; the boldfaced number indicates the majority-class for a given continent) (2012). First, the Stanford Named Entity Recogniser (Finkel et al., 2005) is used to identify location-type NEs in each abstract. Each NE is then mapped to a set of toponyms, based on DBpedia or GeoNames, and the counts of toponyms are aggregated into bag-of-toponyms (BoT) features. Finally, a linear-kernel support vector machine (SVM) is used to train a supervised classifier. We experiment with both: (1) study region classification (at the continent level), and a majorityclass classification for that continent; and (2) replacement of continent-level classes from the original paper with climate-based classes. In the latter case, the toponyms are resolved to climate"
U13-1018,U12-1016,1,0.893088,"eek to automate the climate annotation process with natural language processing (NLP) techniques. The task of climate type classification is complex as although the label set is relatively small, the geographic granularity is fine and toponym ambiguity becomes a significant problem — toponyms commonly mentioned in the environmental sciences (e.g. Murray River) are often large and cover multiple climates, which presents difficulties for a point-based representation of toponyms. Initially, experiments are run to examine the effectiveness of the direct application of the classifiers developed by Willett et al. (2012) for study region classification. We then investigate methods for adapting these techniques to the climate task through the modification of the toponym resolution component of our classifiers. These approaches include utilizing a K¨oppen-Geiger climate classification world map to resolve toponyms to climate instead of region, in addition to experiments with targeting types of toponyms reliable for identifying climate. 2 Related Work The methodology used to extract and disambiguate toponyms is based on a standard approach to geographic information retrieval, which was presented, e.g., by Stokes"
W00-1326,J98-1001,0,0.0395951,"Missing"
W00-1326,H93-1061,0,0.399557,"Missing"
W00-1326,P96-1006,0,0.48276,"lem. The paper is organized as follows. The resources used and the experimental settings are presented first. Section 3 presents the collocations considered and Section 4 explains how decision lists have been adapted to n-way ambiguities. Sections 5 and 6 show the incorpus and cross-corpora experiments, respectively. Section 7 discusses the effect of drawing training and testing data from the same documents. Section 8 evaluates the impact of genre and topic variations, which is fiarther discussed in Section 9. Finally, Section 10 presents some conclusions. 1 Resources used The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. Overall, there are 112,800 sentences, where 192,874 occurrences of the target words were hand-tagged with WordNet senses (Miller et al., 1990). The DSO collection was built with examples from the Wall Street Journal (WSJ) and Brown Corpus (BC). The Brown Corpus is balanced, and the texts are classified according some predefined categories (el. Table 1). The examples from the Brown Corpus comprise 78,080 occurrences of word senses, and the examples from the WSJ 114,794 occurrences. The"
W00-1326,W99-0502,0,0.0402376,"Missing"
W00-1326,H93-1052,0,0.384116,"llowing genre and topic variations. This explains the low results when performing word sense disambiguation across corpora. In fact, we demonstrate that when two independent corpora share a related genre/topic, the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models. Introduction In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (Gale et al., 1992) and one sense per collocation (Yarowsky, 1993). These hypotheses were shown to hold for some particular corpora (totaling 380 Mwords) on words with 2-way ambiguity. The word sense distinctions came from different sources (translations into French, homophones, homographs, pseudo-words, etc.), but no dictionary or lexical resource was linked to them. In the case of the one sense per collocation paper, several corpora were used, but nothing is said on whether the collocations hold across corpora. Since the papers were published, word sense disambiguation has m o v e d to deal with fine207 This study has special significance at this point of"
W00-1326,J98-1006,0,\N,Missing
W00-1326,W00-1322,0,\N,Missing
W00-1326,C94-2174,0,\N,Missing
W00-1326,P95-1026,0,\N,Missing
W00-1326,A00-1031,0,\N,Missing
W00-1326,P94-1013,0,\N,Missing
W00-1326,W00-1702,1,\N,Missing
W00-1702,C96-1005,1,0.828824,"Missing"
W00-1702,W00-1702,1,0.107184,"alent to choosing at random in ties. The experiments are organized as follows: • Evaluate decision lists on SemCor and DSO separately, focusing on baseline features, other features, local vs. topical features, learning curve, noise, overall in SemCor and overall in DSO (section 4). All experiments were performed using 10-fold cross-validation. • Evaluate cross-corpora tagging. Train on DSO and tag SemCor and vice versa (section 5). • Evaluate the Web corpus. Train on Webacquired texts and tag SemCor (section 6). Because of length limitations, it is not possible to show all the data, refer to (Agirre & Martinez, 2000) for more comprehensive results. 4 Results on SemCor and DSO data We first defined an initial set of features and compared the results with the random baseline (Rand) and the most frequent sense baseline (MFS). The basic combination of features comprises word-form bigrams and trigrams, part of speech bigrams and trigrams, a bag with the word-forms in a window spanning 4 words left and right, and a bag with the word forms in the sentence. The results for SemCor and DSO are shown in Table 1. We want to point out the following: • The number of examples per word sense is very low for SemCor (aroun"
W00-1702,J98-1006,0,0.753691,"urek ) = Log ( Pr( sensei |featurek ) ) ∑ Pr( sense j |featurek ) j ≠i Features with 0 or negative values were are not inserted in the decision list. When testing, the decision list is checked in order and the feature with highest weight that is present in the test sentence selects the winning word sense. An example is shown below. The probabilities have been estimated using the maximum likelihood estimate, smoothed using a simple method: when the denominator in the formula is 0 we replace it with 0.1. We analyzed several features already mentioned in the literature (Yarowsky, 1994; Ng, 1997; Leacock et al. 1998), and new features like the word sense or semantic field of the words around the target which are available in SemCor. Different sets of features have been created to test the influence of each feature type in the results: a basic set of features (section 4), several extensions (section 4.2). The example below shows three senses of the noun interest, an example, and some of the features for the decision lists of interest that appear in the example shown. Sense 1: interest, involvement => curiosity, wonder Sense 2: interest, interestingness => power, powerfulness, potency Sense 3: sake, interes"
W00-1702,H93-1061,0,0.0453621,", thesaurus, homographs, ...) instead of widely recognized semantic lexical resources (ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, David Martínez IxA NLP group. 649 pk. Donostia, Basque Country, E-20.080 jibmaird@si.ehu.es etc., or machine-readable dictionaries like OALDC, Webster&apos;s, LDOCE, etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in"
W00-1702,H94-1046,0,0.0120846,"or semantic files does not significantly alter the results. Using a simplified set of PoS tags (only 5 tags) does not degrade performance. Local features, i.e. collocations, are the strongest kind of features, but topical features enable to extend the coverage. • Kinds of words: the highest results can be expected for words with a dominating word sense. Nouns attain better performance with local features when enough data is provided. Individual words exhibit distinct behavior regarding to the feature sets. • SemCor has been cited as having scarce data to train supervised learning algorithms (Miller et al., 1994). Church, for instance, occurs 128 times, but duty only 25 times and account 27. We found Word Account Age Church Duty Head Interest Member People Die Include Know Seek Understand PoS # Examples Rand. DL on SemCor N 1175 .10 .00/.85 .29/.97 N 630 .20 .46/.98 N 386 .33 .35/1.0 N 449 .33 N 3636 .03 .04/.44 .25/.88 N 1043 .14 N 696 .20 .16/.86 N 591 .25 .16/.95 V 1615 .09 .04/.93 V 577 .25 .11/.99 V 1423 .09 .07/.64 .49/.98 V 714 .20 V 780 .20 .12/.92 Table 9: Results on Web data. out that SemCor nevertheless provides enough data to perform some basic general disambiguation, at 0.68 precision on"
W00-1702,P96-1006,0,0.282183,"widely recognized semantic lexical resources (ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, David Martínez IxA NLP group. 649 pk. Donostia, Basque Country, E-20.080 jibmaird@si.ehu.es etc., or machine-readable dictionaries like OALDC, Webster&apos;s, LDOCE, etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other. This will allow us to compa"
W00-1702,W99-0502,0,0.0189563,"Missing"
W00-1702,P94-1013,0,0.817804,"etc.) which usually have fine-grained sense differences. We chose to work with WordNet (Miller et al. 1990). 3. Unavailability of training data: current handtagged corpora seem not to be enough for state-ofthe-art systems. We test how far can we go with existing hand-tagged corpora like SemCor (Miller et al. 1993) and the DSO corpus (Ng and Lee, 1996), which have been tagged with word senses from WordNet. Besides we test an algorithm that automatically acquires training examples from the Web (Mihalcea & Moldovan, 1999). In this paper we focus on one of the most successful algorithms to date (Yarowsky 1994), as attested in the Senseval competition (Kilgarriff & Palmer, 2000). We will evaluate it on both SemCor and DSO corpora, and will try to test how far could we go with such big corpora. Besides, the usefulness of hand tagging using WordNet senses will be tested, training on one corpus and testing in the other. This will allow us to compare hand tagged data with automatically acquired data. If new ways out of the acquisition bottleneck are to be explored, previous questions about supervised algorithms should be answered: how much data is needed, how much noise can they accept, can they be port"
W00-1702,J98-1001,0,\N,Missing
W00-1702,P95-1026,0,\N,Missing
W01-0703,P93-1016,0,0.0222404,"ted corpus. 1 Introduction Previous literature on selectional preference has usually learned preferences for words in the form of classes, e.g., the object of eat is an edible entity. This paper extends previous statistical models to classes of verbs, yielding a relation between classes in a hierarchy, as opposed to a relation between a word and a class. The model is trained using subject-verb and object-verb associations extracted from Semcor, a corpus (Miller et al., 1993) tagged with WordNet word-senses (Miller et al., 1990). The syntactic relations were extracted using the Minipar parser (Lin, 1993). A peculiarity of this exercise is the use of a small sensedisambiguated corpus, in contrast to using a large corpus of ambiguous words. We think that David Martinez IXA NLP Group University of the Basque Country 649 pk. 20.080 Donostia. Spain. jibmaird@si.ehu.es two factors can help alleviate the scarcity of data: the fact that using disambiguated words provides purer data, and the ability to use classes of verbs in the preferences. Nevertheless, the approach can be easily extended to larger, nondisambiguated corpora. We have defined a word sense disambiguation exercise in order to evaluate"
W01-0703,H93-1061,0,0.0129359,"l is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus. 1 Introduction Previous literature on selectional preference has usually learned preferences for words in the form of classes, e.g., the object of eat is an edible entity. This paper extends previous statistical models to classes of verbs, yielding a relation between classes in a hierarchy, as opposed to a relation between a word and a class. The model is trained using subject-verb and object-verb associations extracted from Semcor, a corpus (Miller et al., 1993) tagged with WordNet word-senses (Miller et al., 1990). The syntactic relations were extracted using the Minipar parser (Lin, 1993). A peculiarity of this exercise is the use of a small sensedisambiguated corpus, in contrast to using a large corpus of ambiguous words. We think that David Martinez IXA NLP Group University of the Basque Country 649 pk. 20.080 Donostia. Spain. jibmaird@si.ehu.es two factors can help alleviate the scarcity of data: the fact that using disambiguated words provides purer data, and the ability to use classes of verbs in the preferences. Nevertheless, the approach can"
W01-0703,P92-1053,0,0.0148559,"s, e.g. a verb like ‘eat’ prefers as object edible things, and as subject animate entities, as in, (1) “She was eating an apple”. Selectional preferences get more complex than it might seem: (2) “The acid ate the metal”, (3) “This car eats a lot of gas”, (4) “We ate our savings”, etc. Corpus-based approaches for selectional preference learning extract a number of (e.g. verb/subject) relations from large corpora and use an algorithm to generalize from the set of nouns for each verb separately. Usually, nouns are generalized using classes (concepts) from a lexical knowledge base (e.g. WordNet). Resnik (1992, 1997) defines an informationtheoretic measure of the association between a verb and nominal WordNet classes: selectional association. He uses verb-argument pairs from Brown. Evaluation is performed applying intuition and WSD. Our measure follows in part from his formalization. Abe and Li (1995) follow a similar approach, but they employ a different informationtheoretic measure (the minimum description length principle) to select the set of concepts in a hierarchy that generalize best the selectional preferences for a verb. The argument pairs are extracted from the WSJ corpus, and evaluation"
W01-0703,W97-0209,0,0.173669,"Missing"
W01-0703,W98-0701,0,0.0600365,"ciation between a verb and nominal WordNet classes: selectional association. He uses verb-argument pairs from Brown. Evaluation is performed applying intuition and WSD. Our measure follows in part from his formalization. Abe and Li (1995) follow a similar approach, but they employ a different informationtheoretic measure (the minimum description length principle) to select the set of concepts in a hierarchy that generalize best the selectional preferences for a verb. The argument pairs are extracted from the WSJ corpus, and evaluation is performed using intuition and PP-attachment resolution. Stetina et al. (1998) extract word-arg-word triples for all possible combinations, and use a measure of “relational probability” based on frequency and similarity. They provide an algorithm to disambiguate all words in a sentence. It is directly applied to WSD with good results. 3 Our approach The model explored in this paper emerges as a result of the following observations: • Distinguishing verb senses can be useful. The examples for eat above are taken from WordNet, and each corresponds to a different word sense1: example (1) is from the “take in solid food” sense of eat, (2) from the ”cause to rust” sense, and"
W01-0703,W00-1702,1,\N,Missing
W02-0801,A00-1031,0,0.00712382,"nitions above. One triple is obtained twice from two different definitions. furrow#ground#by#plow surco#tierra#con#arado surco#tierra#con#arado Definitions that do not have a matching triple are discarded, leaving Basque triples without matching triple ambiguous. For instance we could not find triples for irauli#INS#egin(cf. example in section 2.1). The instrumental suffix is sometimes translated without prepositions (in this case “… made turning …”). Looking up the bilingual dictionaries for translation requires lemmatization and Part of Speech tagging. For English we use the TnT PoS tagger (Brants, 2000) and WordNet for lemmatization (Miller et al., 1990). For Spanish we use (Atserias et al., 1998). 2.4 Disambiguation For each Basque case suffix, Spanish preposition and English preposition we have a list of interpretations (cf. Table 1). We assign the interpretations of the preposition to each Spanish/English triple. The intersection of all the interpretations is assigned to it. Continuing with out example, we can see that the intersection between the interpretations of the English by preposition (three interpretations) and the interpretations of the Spanish con preposition (four interpretati"
W02-0801,dorr-etal-1998-thematic,0,\N,Missing
W04-0801,C00-1001,0,0.0372793,"Missing"
W04-0801,agirre-etal-2004-exploring,1,0.816165,"(Aduriz et al. 2000), filtered using some heuristics to ensure quality of context, and finally filtered for PoS mismatches. Table 1 shows the number of examples from each source. 2.4 Words chosen Basically, the words employed in this task are the same words used in Senseval 2 (40 words, 15 nouns, 15 verbs and 10 adjectives), only the sense inventory changed. Besides, in Senseval 3 we replaced 5 verbs with new ones. The reason for this is that in the context of the MEANING project1 we are exploring multilingual lexical acquisition, and there are ongoing experiments that focus on those verbs. (Agirre et al. 2004; Atserias et al. 2004). In fact, 10 words in the English lexical-sample have translations in the Basque, Catalan, Italian, Romanian and Spanish lexical tasks: channel, crown, letter, program, party (nouns), simple (adjective), play, win, lose, decide (verbs). 2.5 Selection of examples from corpora The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+15*senses+7*multiwords As the number of senses in WordNet is very high, we decided to first estimate the number of senses and multiwords that really occur in the corpus. The taggers were"
W04-0801,atserias-etal-2004-cross,1,0.821741,"), filtered using some heuristics to ensure quality of context, and finally filtered for PoS mismatches. Table 1 shows the number of examples from each source. 2.4 Words chosen Basically, the words employed in this task are the same words used in Senseval 2 (40 words, 15 nouns, 15 verbs and 10 adjectives), only the sense inventory changed. Besides, in Senseval 3 we replaced 5 verbs with new ones. The reason for this is that in the context of the MEANING project1 we are exploring multilingual lexical acquisition, and there are ongoing experiments that focus on those verbs. (Agirre et al. 2004; Atserias et al. 2004). In fact, 10 words in the English lexical-sample have translations in the Basque, Catalan, Italian, Romanian and Spanish lexical tasks: channel, crown, letter, program, party (nouns), simple (adjective), play, win, lose, decide (verbs). 2.5 Selection of examples from corpora The minimum number of examples for each word according to the task specifications was calculated as follows: N=75+15*senses+7*multiwords As the number of senses in WordNet is very high, we decided to first estimate the number of senses and multiwords that really occur in the corpus. The taggers were provided with a suffic"
W04-0813,C00-1001,0,0.0604085,"mble in cross-validation. The main diﬀerence between the Basque and English systems was the feature set. A rich set of features was used for English, including syntactic dependencies and domain information, extracted with diﬀerent tools, and also from external resources like WordNet Domains (Magnini and Cavagli´ a, 2000). The features for Basque were diﬀerent, as Basque is an agglutinative language, and syntactic information is given by inﬂectional suﬃxes. We tried to represent this information in local features, relying on the analysis of a deep morphological analyzer developed in our group (Aduriz et al., 2000). In order to improve the performance of the algorithms, diﬀerent smoothing techniques were David Martinez IXA NLP Group Basque Country University Donostia, Spain davidm@si.ehu.es tested on the English Senseval-2 lexical sample data (Agirre and Martinez, 2004), and applied to Senseval-3. These methods helped to obtain better estimations for the features, and to avoid the problem of 0 counts Decision Lists and Naive Bayes. This paper is organized as follows. The learning algorithms are ﬁrst introduced in Section 2, and Section 3 describes the features applied to each task. In Section 4, we pres"
W04-0813,magnini-cavaglia-2000-integrating,0,0.0373521,"Missing"
W04-0813,W97-0323,0,0.0317258,"ethods and Parameters DL: On Senseval-2 data, we observed that DL improved signiﬁcantly its performance with a smoothing technique based on (Yarowsky, 1995a). For our implementation, the smoothed probabilities were obtained by grouping the observations by raw frequencies and feature types. As this method seems sensitive to the feature types and the amount of examples, we tested 3 DL versions: DL smooth (using smoothed probabilities), DL ﬁxed (replacing 0 counts with 0.1), and DL discard (discarding features appearing with only one sense). NB: We applied a simple smoothing method presented in (Ng, 1997), where zero counts are replaced by the probability of the given sense divided by the number of examples. V: The same smoothing method used for NB was applied for vectors. For Basque, two versions were tested: as the Basque parser can return ambiguous analyses, partial weights are assigned to the features in the context, and we can chose to use these partial weights (p), or assign the full weight to all features (f). SVM: No smoothing was applied. We estimated the soft margin using a greedy process in cross-validation on the training data per each word. Combination: Single voting was used, whe"
W04-0813,N01-1006,0,0.0416795,"t words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´ a, 2000), we deﬁned 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predeﬁned threshold3 . Other experiments using domains from SUMO, the EuroWordNet 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provided by Gerard Escudero’s group, from Universitat Politecnica de Catalunya top-ontology, and WordNet’s Semantic Fields were performed, but these features were discarded from the ﬁnal set. 3.2 Features for Basque Basque is an agglutinative language, and syntactic information is given by inﬂectional sufﬁxes. The morphological analysis of the text is a necessary previous step in order to select informative features. The data provided by the t"
W04-0813,N01-1011,0,0.262954,"wordforms, or PoS tags1 . Other local features are those formed with the previous/posterior lemma/word-form in the context. Syntactic dependencies: syntactic dependencies were extracted using heuristic patterns, and regular expressions deﬁned with the PoS tags around the target2 . The following relations were used: object, subject, noun-modiﬁer, preposition, and sibling. Bag-of-words features: we extract the lemmas of the content words in the whole context, and in a ±4-word window around the target. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´ a, 2000), we deﬁned 2 feature types: (1) the most relevant domain, and (2) a list of domains above a predeﬁned threshold3 . Other experiments using domains from SUMO, the EuroWordNet 1 The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). 2 This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University. 3 The software to obtain the relevant domains was kindly provide"
W04-0813,P95-1026,0,0.415154,"e learning algorithms are ﬁrst introduced in Section 2, and Section 3 describes the features applied to each task. In Section 4, we present the experiments performed on training data before submission; this section also covers the ﬁnal conﬁguration of each algorithm, and the performance obtained on training data. Finally, the oﬃcial results in Senseval-3 are presented and discussed in Section 5. 2 Learning Algorithms The algorithms presented in this section rely on features extracted from the context of the target word to make their decisions. The Decision List (DL) algorithm is described in (Yarowsky, 1995b). In this algorithm the sense with the highest weighted feature is selected, as shown below. We can avoid undetermined values by discarding features that have a 0 probability in the divisor. More sophisticated smoothing techniques have also been tried (cf. Section 4). arg max w(sk , fi ) = log(  k P r(sk |fi ) ) j=k P r(sj |fi ) The Naive Bayes (NB) algorithm is based on the conditional probability of each sense given the features in the context. It also requires smoothing. arg max P (sk ) k m i=1 P (fi |sk ) For the Vector Space Model (V) algorithm, we represent each occurrence context a"
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W04-3204,agirre-de-lacalle-2004-publicly,1,0.739887,"ated these nouns in 2 sets, depending on the number of examples they have in Semcor: Set A contained the 16 nouns with more than 10 examples in Semcor, and Set B the remaining low-frequency words. 4 Building the monosemous relatives web corpus In order to build this corpus3 , we have acquired 1000 Google snippets for each monosemous word in WordNet 1.7. Then, for each word sense of the ambiguous words, we gathered the examples of its monosemous relatives (see below). This method is inspired in (Leacock et al., 1998), and has shown to be effective in experiments of topic signature acquisition (Agirre and Lopez, 2004). This last paper also shows that it is possible to gather examples based on 3 The automatically acquired corpus will be referred indistinctly as web-corpus, or monosemous-corpus monosemous relatives for nearly all noun senses in WordNet4 . The basic assumption is that for a given word sense of the target word, if we had a monosemous synonym of the word sense, then the examples of the synonym should be very similar to the target word sense, and could therefore be used to train a classifier of the target word sense. The same, but in a lesser extent, can be applied to other monosemous relatives,"
W04-3204,W00-1702,1,0.919628,"n Lists The learning method used to measure the quality of the corpus is Decision Lists (DL). This algorithm is described in (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its log-likelihood (see Formula 1). For our implementation, we applied a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and part-of-speech2 of the surrounding words. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and"
W04-3204,A00-1031,0,0.00285494,"the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and part-of-speech2 of the surrounding words. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and has been widely used for WSD. It was necessary to use an automatic mapping between the WordNet 1.6 senses in Semcor and the WordNet 1.7 senses in testing (Daude et al., 2000). For evaluation, the test part of the Senseval-2 English lexical-sample task was chosen. The advantage of this corpus was that we could focus on a word-set with"
W04-3204,P00-1064,0,0.00998965,"s. Also the content lemmas in a ±4 word window around the target. • Topical features: All the content lemmas in the context. 2 The PoS tagging was performed using TnT (Brants, 2000) We have analyzed the results using local and topical features separately, and also using both types together (combination). 3.3 Hand-tagged corpora Semcor was used as training data for our supervised system. This corpus offers tagged examples for many words, and has been widely used for WSD. It was necessary to use an automatic mapping between the WordNet 1.6 senses in Semcor and the WordNet 1.7 senses in testing (Daude et al., 2000). For evaluation, the test part of the Senseval-2 English lexical-sample task was chosen. The advantage of this corpus was that we could focus on a word-set with enough examples for testing. Besides, it is a different corpus, so the evaluation is more realistic than that made using cross-validation. The test examples whose senses were multiwords or phrasal verbs were removed, because they can be efficiently detected with other methods in a preprocess. It is important to note that the training part of Senseval-2 lexical-sample was not used in the construction of the systems, as our goal was to"
W04-3204,S01-1001,0,0.0183951,"have used to train disambiguation systems. The corpus-building process has highlighted important factors, such as the distribution of senses (bias). The corpus has been used to train WSD algorithms that include supervised methods (combining automatic and manuallytagged examples), minimally supervised (requiring sense bias information from hand-tagged corpora), and fully unsupervised. These methods were tested on the Senseval-2 lexical sample test set, and compared successfully to other systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of ex"
W04-3204,S01-1018,0,0.0581651,"ame test data and set of nouns. From the 5 unsupervised systems presented in the Senseval-2 lexical-sample task as unsupervised, the WASP-Bench system relied on lexicographers to hand-code information semi-automatically (Tugwell and Kilgarriff, 2001). This system does not use the training data, but as it uses manually coded knowledge we think it falls clearly in the supervised category. The results for the other 4 systems and our own are shown in Table 7. We show the results for the totally unsupervised system and the minimally unsupervised system (Semcor bias). We classified the UNED system (Fernandez-Amoros et al., 2001) as minimally supervised. It does not use hand-tagged examples for training, but some of the heuristics that are applied by the system rely on the bias information available in Semcor. The distribution of senses is used to discard low-frequency senses, and also to choose the first sense as a back-off strategy. On the same conditions, our minimally supervised system attains 49.8 recall, nearly 5 points more. The rest of the systems are fully unsupervised, and they perform significantly worse than our system. 6 Conclusions and Future Work This paper explores the large-scale acquisition of sense-"
W04-3204,fernandez-etal-2004-automatic,0,0.0310314,"Missing"
W04-3204,J98-1006,0,0.839126,", which would tag each word with the sense occurring most frequently in Semcor. In our approach, we will also rely on Semcor as the basic resource, both for training examples and as an indicator of the distribution of the senses of the target word. The goal of our experiment is to evaluate up to which point we can automatically acquire examples for word senses and train accurate supervised WSD systems on them. This is a very promising line of research, but one which remains relatively understudied (cf. Section 2). The method we applied is based on the monosemous relatives of the target words (Leacock et al., 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). Basically, we built three systems, one fully supervised (using examples from both Semcor and automatically acquired examples), one minimally supervised (using the distribution of senses in Semcor and automatically acquired examples) and another fully unsupervised (using an automatically acquired sense rank (McCarthy et al., 2004) and automatically acquired e"
W04-3204,P98-2127,0,0.0138491,"Distribution of examples for the senses of authority in different corpora. Pr (proportional) and MR (minimum ratio) columns correspond to different ways to apply Semcor bias. each sense. In order to test the impact of bias, different settings have been tried: • No bias: we take an equal amount of examples for each sense. • Web bias: we take all examples gathered from the web. • Automatic ranking: the number of examples is given by a ranking obtained following the method described in (McCarthy et al., 2004). They used a thesaurus automatically created from the BNC corpus with the method from (Lin, 1998), coupled with WordNet-based similarity measures. • Semcor bias: we take a number of examples proportional to the bias of the word senses in Semcor. For example, Table 1 shows the number of examples per type (0,1,...) that are acquired for church following the Semcor bias. The last column gives the number of examples in Semcor. We have to note that the 3 first methods do not require any hand-labeled data, and that the fourth relies in Semcor. The way to apply the bias is not straightforward in some cases. In our first approach for Semcorbias, we assigned 1,000 examples to the major sense in Se"
W04-3204,W00-1326,1,0.834551,"Missing"
W04-3204,P04-1036,0,0.135688,"elatives of the target words (Leacock et al., 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). Basically, we built three systems, one fully supervised (using examples from both Semcor and automatically acquired examples), one minimally supervised (using the distribution of senses in Semcor and automatically acquired examples) and another fully unsupervised (using an automatically acquired sense rank (McCarthy et al., 2004) and automatically acquired examples). This paper is structured as follows. First, Section 2 describes previous work on the field. Section 3 introduces the experimental setting for evaluating the acquired corpus. Section 4 is devoted to the process of building the corpus, which is evaluated in Section 5. Finally, the conclusions are given in Section 6. 2 Previous work As we have already mentioned, there is little work on this very promising area. In (Leacock et al., 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. In this work, they retrieve the same n"
W04-3204,mihalcea-2002-bootstrapping,0,0.702145,"est set, and compared successfully to other systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David"
W04-3204,H93-1061,0,0.292651,"leneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David Martinez IXA NLP Group University of the Basque Country Donostia, Spain davidm@si.ehu.es pus only provides a handful of tagged examples. In fact, only a few systems could overcome the Most Frequent Sense (MFS) baseline, which would tag each word with the sense occurring most frequently in Semcor. In our approach, we will also rely on Semcor as the ba"
W04-3204,S01-1037,0,0.0111625,"et B (< 10) all words 51.9 40.1 47.8 50.5 47.4 50.9 47.7 49.8 Semcor + Web 51.6 47.8 50.3 MFS & Web 51.9 47.8 50.5 Table 6: Recall training in Semcor, the acquired web corpus (Semcor bias), and a combination of both, compared to that of the Semcor MFS. available. This made possible to compare our results and those of other systems deemed unsupervised by the organizers on the same test data and set of nouns. From the 5 unsupervised systems presented in the Senseval-2 lexical-sample task as unsupervised, the WASP-Bench system relied on lexicographers to hand-code information semi-automatically (Tugwell and Kilgarriff, 2001). This system does not use the training data, but as it uses manually coded knowledge we think it falls clearly in the supervised category. The results for the other 4 systems and our own are shown in Table 7. We show the results for the totally unsupervised system and the minimally unsupervised system (Semcor bias). We classified the UNED system (Fernandez-Amoros et al., 2001) as minimally supervised. It does not use hand-tagged examples for training, but some of the heuristics that are applied by the system rely on the bias information available in Semcor. The distribution of senses is used"
W04-3204,P94-1013,0,0.0749358,"iterative process, the system obtained new seeds from the retrieved examples. An experiment in the lexical-sample task showed that the method was useful for a subset of the Senseval-2 testing words (results for 5 words are provided). 3 Experimental Setting for Evaluation In this section we will present the Decision List method, the features used to represent the context, the two hand-tagged corpora used in the experiment and the word-set used for evaluation. 3.1 Decision Lists The learning method used to measure the quality of the corpus is Decision Lists (DL). This algorithm is described in (Yarowsky, 1994). In this method, the sense sk with the highest weighted feature fi is selected, according to its log-likelihood (see Formula 1). For our implementation, we applied a simple smoothing method: the cases where the denominator is zero are smoothed by the constant 0.1 . weight(sk , fi ) = log(  3.2 P r(sk |fi ) ) j=k P r(sj |fi ) (1) Features In order to represent the context, we used a basic set of features frequently used in the literature for WSD tasks (Agirre and Martinez, 2000). We distinguish two types of features: • Local features: Bigrams and trigrams, formed by the word-form, lemma, and"
W04-3204,P95-1026,0,0.285835,"systems with minimum or no supervision. 1 Introduction The results of recent WSD exercises, e.g. Senseval21 (Edmonds and Cotton, 2001) show clearly that WSD methods based on hand-tagged examples are the ones performing best. However, the main drawback for supervised WSD is the knowledge acquisition bottleneck: the systems need large amounts of costly hand-tagged data. The situation is more dramatic for lesser studied languages. In order to overcome this problem, different research lines have been explored: automatic acquisition of training examples (Mihalcea, 2002), bootstrapping techniques (Yarowsky, 1995), or active learning (ArgamonEngelson and Dagan, 1999). In this work, we have focused on the automatic acquisition of examples. When supervised systems have no specific training examples for a target word, they need to rely on publicly available all-words sense-tagged corpora like Semcor (Miller et al., 1993), which is tagged with WordNet word senses. The systems performing best in the English all-words task in Senseval-2 were basically supervised systems trained on Semcor. Unfortunately, for most of the words, this cor1 http://www.senseval.org. David Martinez IXA NLP Group University of the B"
W04-3204,C98-2122,0,\N,Missing
W06-1669,W06-3814,1,0.224507,"Missing"
W06-1669,W04-0807,0,0.0274208,"Missing"
W06-1669,H05-1052,0,0.051105,"lected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eronis shows that this kind of graph fulfills the properties of small world graphs, and thus possesses highly connected This paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Our main contribution is the optimization of the free parameters of"
W06-1669,H93-1061,0,0.398686,"rds tasks. The results show that, in spite of the information loss inherent to mapping the induced senses to the gold-standard, the optimization of parameters based on a small sample of nouns carries over to all nouns, performing close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the te"
W06-1669,W05-0605,0,0.0149339,"et, and then measured the quality of the mapping. More recently, tagged corpora have been used to map the induced senses, and then compare the systems over publicly available benchmarks (Puran2.3 Using hubs for WSD Once the hubs that represent the senses of the word are selected (following any of the methods presented in the last section), each of them is linked to the target word with edges weighting 0, and the Minimum Spanning Tree (MST) of the whole graph is calculated and stored. 5 As G is undirected, the in-degree of a vertex v is equal to its out-degree. 587 is. dare and Pedersen, 2004; Niu et al., 2005; Agirre et al., 2006), which offers the advantage of comparing to other systems, but converts the whole system into semi-supervised. See Section 5 for more details on these systems. Note that the mapping introduces noise and information loss, which is a disadvantage when comparing to other systems that rely on the gold-standard senses. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. In this case t"
W06-1669,W04-2406,0,0.432566,"duce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eroni"
W06-1669,J98-1004,0,0.750175,"Missing"
W06-1669,W04-0811,0,0.038389,"rming close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 585 Proceedings of the 2006 Conference on Empirical Methods in"
W06-1669,W04-3252,0,\N,Missing
W06-2007,W04-0813,1,0.915157,"for each sense. Note that a sense example produced here is simply a bag of words without ordering. We prepared the other set of sense examples by translating text snippets with the MT software SysStandard, where each example contains tran much richer features that potentially can be exploited by ML algorithms.         3 Experimental Settings 3.1 Training We applied the Vector Space Model (VSM) algorithm on the two different kinds of sense examples (i.e., dictionary translated ones vs. MT software translated ones), as it has been shown to perform well with the features described below (Agirre and Martinez, 2004a). In VSM, we represent each context as a vector, where each feature has an 1 or 0 value to indicate its occurrence or absence. For each sense in training, a centroid vector is obtained, and these centroids are compared to the vectors that represent test examples, by means of the cosine similarity function. The closest centroid assigns its sense to the test example. For the sense examples translated by MT software, we analysed the sentences using different 3 See: http://mtgroup.ict.ac.cn/ zhp/ICTCLAS  47 tools and extracted relevant features. We applied stemming and POS tagging, using the fn"
W06-2007,W04-3204,1,0.905837,"for each sense. Note that a sense example produced here is simply a bag of words without ordering. We prepared the other set of sense examples by translating text snippets with the MT software SysStandard, where each example contains tran much richer features that potentially can be exploited by ML algorithms.         3 Experimental Settings 3.1 Training We applied the Vector Space Model (VSM) algorithm on the two different kinds of sense examples (i.e., dictionary translated ones vs. MT software translated ones), as it has been shown to perform well with the features described below (Agirre and Martinez, 2004a). In VSM, we represent each context as a vector, where each feature has an 1 or 0 value to indicate its occurrence or absence. For each sense in training, a centroid vector is obtained, and these centroids are compared to the vectors that represent test examples, by means of the cosine similarity function. The closest centroid assigns its sense to the test example. For the sense examples translated by MT software, we analysed the sentences using different 3 See: http://mtgroup.ict.ac.cn/ zhp/ICTCLAS  47 tools and extracted relevant features. We applied stemming and POS tagging, using the fn"
W06-2007,J94-4003,0,0.308937,"nse Disambiguation (WSD) systems tend to outperform their unsupervised counterparts. However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were consid"
W06-2007,P02-1033,0,0.0280056,"D) systems tend to outperform their unsupervised counterparts. However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were considered to have been disam"
W06-2007,1992.tmi-1.9,0,0.103735,"supervised Word Sense Disambiguation (WSD) systems tend to outperform their unsupervised counterparts. However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parall"
W06-2007,J04-1001,0,0.0260364,"pervised counterparts. However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were considered to have been disambiguated and “sense tagged” by the"
W06-2007,magnini-cavaglia-2000-integrating,0,0.15508,"Missing"
W06-2007,P04-1036,0,0.185996,"Missing"
W06-2007,W04-0807,0,0.342771,"of some use for ML classifiers. In this approach, the 3rd step can be omitted, since MT software should be able to take care of segmentation. Figure 1 illustrates our adapted acquisition process. As described above, we prepared two sets of training examples for each English word sense to disambiguate: one set was translated word-byword by looking up a bilingual dictionary, as proposed in (Wang and Carroll, 2005), and the other translated using MT software. In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al., 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. We did this by looking up an English-Chinese dictionary PowerWord 20022 . This mapping process involved human intervention, but it only took an annotator (fluent speaker in both Chinese and English) 4 hours. Since some Chinese translations are also ambiguous, which may affect WSD performance, the annotator was asked to select the Chinese words that are relatively unambiguous (or ideally monosemous) in Chinese for the target word senses, when it was possible. Sometimes multiple senses of an English word can m"
W06-2007,H93-1061,0,0.0498567,"d adjectives, and Wordsmyth8 for verbs. We only evaluated our WSD systems on nouns and adjectives.  4     This software was kindly provided by David Yarowsky’s group at Johns Hopkins University. 5 Preliminary experiments using local features (bigrams and trigrams) showed low performance, which was expected because of noise in the automatically acquired data. 6 This software was kindly provided by Gerard Escudero’s group at Universitat Politecnica de Catalunya. The threshold was set in previous work. 7 http://wordnet.princeton.edu 8 http://www.wordsmyth.net We also used the SemCor corpus (Miller et al., 1993) for tuning our relative-threshold heuristic. It contains a number of texts, mainly from the Brown Corpus, comprising about 200,000 words, where all content words have been manually tagged with senses from WordNet. Throughout the paper we will use the concepts of precision and recall to measure the performance of WSD systems, where precision refers to the ratio of correct answers to the total number of answers given by the system, and recall indicates the ratio of correct answers to the total number of instances. Our ML systems attempt every instance and always give a unique answer, and hence"
W06-2007,P03-1058,0,0.0243039,"erform their unsupervised counterparts. However, supervised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were considered to have been disambiguated and “sen"
W06-2007,N01-1006,0,0.0169084,"e represent each context as a vector, where each feature has an 1 or 0 value to indicate its occurrence or absence. For each sense in training, a centroid vector is obtained, and these centroids are compared to the vectors that represent test examples, by means of the cosine similarity function. The closest centroid assigns its sense to the test example. For the sense examples translated by MT software, we analysed the sentences using different 3 See: http://mtgroup.ict.ac.cn/ zhp/ICTCLAS  47 tools and extracted relevant features. We applied stemming and POS tagging, using the fnTBL toolkit (Ngai and Florian, 2001), as well as shallow parsing4 . Then we extracted the following types of topical and domain features5 , which were then fed to the VSM machine learner: Topical features: we extracted lemmas of the content words in two windows around the target word: the whole context and a 4 word window. We also obtained salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). We included another feature type, which match the closest words (for each POS and in both directions) to the target word (e.g. LEFT NOUN “dog” or LEFT VERB “eat”).  Domain features: The “WordNet D"
W06-2007,N01-1011,0,0.0340063,"analysed the sentences using different 3 See: http://mtgroup.ict.ac.cn/ zhp/ICTCLAS  47 tools and extracted relevant features. We applied stemming and POS tagging, using the fnTBL toolkit (Ngai and Florian, 2001), as well as shallow parsing4 . Then we extracted the following types of topical and domain features5 , which were then fed to the VSM machine learner: Topical features: we extracted lemmas of the content words in two windows around the target word: the whole context and a 4 word window. We also obtained salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). We included another feature type, which match the closest words (for each POS and in both directions) to the target word (e.g. LEFT NOUN “dog” or LEFT VERB “eat”).  Domain features: The “WordNet Domains” resource was used to identify the most relevant domains in the context. Following the relevance formula presented in (Magnini and Cavagli´a, 2000), we defined two feature types: (1) the most relevant domain, and (2) a list of domains above a threshold6 . For the dictionary-translated sense examples, we simply used bags of words as features. 3.2 Evaluation We evaluated our WSD classifier on"
W06-2007,H05-1069,1,0.890886,"rvised systems rely on large amounts of accurately senseannotated data to yield good results and such resources are very costly to produce. It is difficult for supervised WSD systems to perform well and reliably on words that do not have enough sensetagged training data. This is the so-called knowledge acquisition bottleneck. To overcome this bottleneck, unsupervised WSD approaches have been proposed. Among them, systems under the multilingual paradigm have shown great promise (Gale et al., 1992; Dagan and Itai, 1994; Diab and Resnik, 2002; Ng et al., 2003; Li and Li, 2004; Chan and Ng, 2005; Wang and Carroll, 2005). The underlying hypothesis is that mappings between word forms and meanings can be different from language to language. Much work have been done on extracting sense examples from parallel corpora for WSD. For example, Ng et al. (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. They grouped senses that share the same Chinese translation, and then the occurrences of the word on the English side of the parallel corpora were considered to have been disambiguated and “sense tagged” by the appropriate Chinese translations. Their sys"
W06-3814,W04-0807,0,0.0444102,"es” envisioned in (Cruse, 2000). We now think that the idea of having many micro-senses is very attractive for further exploration, especially if we are able to organize them into coarser hubs. 6.3 Comparison to related work Table 4 shows the performance of different systems on the nouns of the S3LS benchmark. When not reported separately, we obtained the results for nouns running the official scorer program on the filtered results, as available in the S3LS web page. The second column shows the type of system (supervised, unsupervised). We include three supervised systems, the winner of S3LS (Mihalcea et al., 2004), an in-house system (kNN-all, CITATION OMITTED) which uses optimized kNN, and the same in-house system restricted to bag-of-words features only (kNN-bow), i.e. discarding other local features like bigrams or trigrams (which is what most unsupervised systems do). The table shows that we are one point from the bag-ofwords classifier kNN-bow, which is an impressive result if we take into account the information loss of the mapping step and that we tuned our parameters on a different set of words. The full kNN system is state-of-the-art, only 4 points below the S3LS win95 System S3LS-best kNN-all"
W06-3814,H93-1061,0,0.0169005,"e). Our results for nouns show that thanks to the optimization of parameters and the mapping method, HyperLex obtains results close to supervised systems using the same kind of bag-ofwords features. Given the information loss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and seman"
W06-3814,W05-0605,0,0.200491,"in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedp1 p2 p3 p4 p5 p6 p7 Default value 5 10 0.9 4 6 0.8 0.001 p180 Range Best 2-3 2 3-4 3 0.7-0.9 0.7 4 4 6-7 6 0.5-0.8 0.6 0.0005-0.001 0.0009 p1800 Range Best 1-3 2 2-4 3 0.5-0.7 0.5 4 4 3-7 3 0.4-0.8 0.7 0.0005-0.001 0.0009 p6700 Range Best 1-3 1 2-4 3 0.3-0.7 0.4 4 4 1-7 1 0.6-0.95 0.95 0.0009-0.003 0.001 Table 1: Parameters of the HyperLex algorithm ersen, 2004; Niu et al., 2005). See Section 6 for more details on these systems. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. As we wanted to focus on the comparison against a standard data-set, we decided to leave aside this otherwise interesting option. In this section we present a framework for automatically evaluating unsupervised WSD systems against publicly available hand-tagged corpora. The framework uses three data s"
W06-3814,W04-2406,0,0.126924,"r instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised WSD has followed this line of thinking, and tries to induce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), where each example is represented by a vector of features (e.g. the words occurring in the context). Recently, V´eronis (V´eronis, 2004) has 1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 89 Workshop on TextGraphs, at HLT-NAACL 2006, pages 89–96, c New York City, June 2006. 2006 Association for Computational Linguistics proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graph"
W06-3814,J98-1004,0,0.775251,"Missing"
W06-3814,W04-0811,0,0.0472142,"ss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions i"
W08-0611,W04-0813,1,0.901904,"Missing"
W08-0611,P04-1036,0,0.341367,"Missing"
W08-0611,W04-0807,0,0.200016,"Missing"
W08-0611,N01-1011,0,0.180154,"ncies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features based on UMLS Concept Unique Identifiers (CUIs). The MetaMap program (Aronson, 2001) identifies all words and terms in a text which could be mapped onto a UMLS CUI. MetaMap d"
W08-0611,W96-0213,0,0.0227158,"t-POS “NNS IN”, leftcontent-word-form “area adjustments”, rightfunction-word-form “adjustment of ”, etc. • Syntactic Dependencies: These features model longer-distance dependencies of the ambiguous words than can be represented by the local collocations. Five relations are extracted: object, subject, noun-modifier, preposition and sibling. These are identified using heuristic patterns and regular expressions applied to PoS tag sequences around the ambiguous word. In the above example, “heparin” is noun-modifier feature of “adjustment”. 2 A maximum-entropy-based part of speech tagger was used (Ratnaparkhi, 1996) without the adaptation to the biomedical domain. 83 • Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). • Unigrams: Lemmas of unigrams which appear more frequently than a predefined threshold in the entire corpus, excluding those in a list of stopwords. We empirically set the threshold to 1. This feature was not used by Agirre and Mart´ınez (2004), but Joshi et al. (2005) found them to be useful for this task. Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al. (2007) to generate features ba"
W08-0611,J01-3001,1,\N,Missing
W08-0611,J06-1003,0,\N,Missing
W09-1410,P06-4020,0,0.0414435,"Missing"
W09-1410,copestake-flickinger-2000-open,0,0.00911965,"he ERG It seemed likely that syntactico-semantic analysis would be useful for task 3. To identify negation or speculation with relatively high precision, it is probable that knowledge of the relationships of possibly distant elements (such as the negation particle not) to a particular target word would provide valuable information for classification. Further to this, it was our intention to evaluate the utility of deep parsing in such an approach, 78 rather than a shallower annotation such as the output of a dependency parser. With this in mind, we selected the English Resource Grammar1 (ERG: Copestake and Flickinger (2000)), an open-source, broad-coverage high-precision grammar of English in the HPSG framework. While the ERG is relatively robust across different domains, it is a general-purpose resource, and there are some aspects of the language used in the biomedical abstracts that cause difficulties; unknown word handling is especially important given the nature of terms in the domain. Fortunately we can make some optimisations to mitigate this. The GENIA tagger mentioned in Section 2.1.1 provides both POS and named entity annotations, which we used to constrain the input to the ERG in two ways: • Biological"
W09-1410,C04-1185,0,0.0412547,"Missing"
W10-0508,P95-1005,0,0.516276,"Missing"
W10-0508,J05-2005,0,0.0605956,"ce IR effectiveness. 3 Conclusions This paper provides an outline of the ILIAD project, focusing on the tasks of crawling, thread-level analysis, post-level analysis, user-level analysis and IR reranking. We have designed a series of class sets for the component tasks, and carried out experimentation over a range of data sources, achieving encouraging results. 2.3 Post-level analysis Acknowledgements We automatically analyse the post-to-post discourse structure of each thread, in terms of which (preceding) post(s) each post relates to, and how, building off the work of Ros´e et al. (1995) and Wolf and Gibson (2005). For example, a given post may refute the solution proposed in an earlier post, and also propose a novel solution in response to the initiating post. Separately, we are developing techniques for identifying whether a new post to a given forum is sufficiently similar to other (ideally resolved) threads that the author should be prompted to first check the existing threads for redundancy before a new thread is initiated. Our experiments on post-level analysis are, once again, based on data from LinuxQuestions and CNET. NICTA is funded by the Australian Government as represented by the Departmen"
W13-2005,W13-2010,1,0.893498,"with Noisy Training Data Andrew MacKinlay♦ , David Martinez♦ , Antonio Jimeno Yepes♦ , Haibin Liu♠ , W. John Wilbur♠ and Karin Verspoor♦ ♦ NICTA Victoria Research Laboratory, University of Melbourne, Australia {andrew.mackinlay, david.martinez}@nicta.com.au {antonio.jimeno, karin.verspoor}@nicta.com.au ♠ National Center for Biotechnology Information, Bethesda, MD, USA haibin.liu@nih.gov, wilbur@ncbi.nlm.nih.gov Abstract itive regulation of gene expression). In our submission, we built on a system originally developed for the BioNLP-ST 2011 (Liu et al., 2011) and extended in more recent work (Liu et al., 2013a; Liu et al., 2013b). This system learns to recognise subgraphs of syntactic dependency parse graphs that express a given bio-molecular event, and matches those subgraphs to new text using an algorithm called Approximate Subgraph Matching. Due to the method’s fundamental dependency on the syntactic dependency parse of the text, in this work we set out to explore the impact of substituting the previously employed dependency parsers with a different parser which has been demonstrated to achieve higher performance than other commonly used parsers for full-text biomedical literature (Verspoor et"
W13-2005,P13-1104,0,0.0214701,"op-k events overall, versus choosing the top-k events for each event type. We also tested adding as many instances per event-type as there were in the manually-annotated dataset, with different multiplying factors. Finally, we evaluated the effect of using different splits of the data for the evaluation and optimisation steps of ASM. This is the full list of parameters that we tested over held-out data: 2.1.2 Parsing Pipeline In our parsing pipeline, we first split sentences using the JULIE Sentence Boundary Detector, or JSBD (Tomanek et al., 2007). We then parse using a version of clearnlp1 (Choi and McCallum, 2013), a successor to ClearParser (Choi and Palmer, 2011), which was shown to have stateof-the-art performance over the CRAFT corpus of full-text biomedical articles (Verspoor et al., 2012). We use dependency and POS-tagging models trained on the CRAFT corpus (except where noted); these pre-trained models are provided with clearnlp. Our fork of clearnlp integrates token span marking into the parsing process, so the dependency nodes can easily be matched to the standoff annotations provided with the shared task data. This pipeline is not dependent on any preannotated data, so can thus be trivially a"
W13-2005,P08-2026,0,0.147871,"edings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also consists of an event type, and a mapping from event arguments to nodes from the pattern graph, or to an event type/node pair for nested event arguments. After processing all training documents, we get on the order of a few thousand rules; this can be decreased slightly by removing rules with subgraphs that are isomorphic to those of other rules. very effective for improving parsing performance (McClosky et al., 2006; McClosky and Charniak, 2008). Self-training of the TEES system has been previously explored (Bjorne et al., 2012), with somewhat mixed results, but with evidence suggesting it could be useful with an appropriate strategy for selecting training examples. Here, rather than training our system with its own output over external data, we explore a semi-supervised learning approach in which we train our system with the outputs of a different system (TEES) over external data. 2 In principle, this set of rules could then be directly applied to the test documents, by searching for any matching subgraphs. However, in practice doin"
W13-2005,P11-2121,0,0.0309941,"for each event type. We also tested adding as many instances per event-type as there were in the manually-annotated dataset, with different multiplying factors. Finally, we evaluated the effect of using different splits of the data for the evaluation and optimisation steps of ASM. This is the full list of parameters that we tested over held-out data: 2.1.2 Parsing Pipeline In our parsing pipeline, we first split sentences using the JULIE Sentence Boundary Detector, or JSBD (Tomanek et al., 2007). We then parse using a version of clearnlp1 (Choi and McCallum, 2013), a successor to ClearParser (Choi and Palmer, 2011), which was shown to have stateof-the-art performance over the CRAFT corpus of full-text biomedical articles (Verspoor et al., 2012). We use dependency and POS-tagging models trained on the CRAFT corpus (except where noted); these pre-trained models are provided with clearnlp. Our fork of clearnlp integrates token span marking into the parsing process, so the dependency nodes can easily be matched to the standoff annotations provided with the shared task data. This pipeline is not dependent on any preannotated data, so can thus be trivially applied to extra data not provided as part of the sha"
W13-2005,N06-1020,0,0.0244081,"ents (e.g., pos35 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also consists of an event type, and a mapping from event arguments to nodes from the pattern graph, or to an event type/node pair for nested event arguments. After processing all training documents, we get on the order of a few thousand rules; this can be decreased slightly by removing rules with subgraphs that are isomorphic to those of other rules. very effective for improving parsing performance (McClosky et al., 2006; McClosky and Charniak, 2008). Self-training of the TEES system has been previously explored (Bjorne et al., 2012), with somewhat mixed results, but with evidence suggesting it could be useful with an appropriate strategy for selecting training examples. Here, rather than training our system with its own output over external data, we explore a semi-supervised learning approach in which we train our system with the outputs of a different system (TEES) over external data. 2 In principle, this set of rules could then be directly applied to the test documents, by searching for any matching subgra"
W13-2005,W00-0901,0,0.0142391,"e basic set of cue lemmas came from a variety of sources. Some were manually specified and some were derived from previous work on modification detection (Cohen et al., 2011; MacKinlay et al., 2012). We manually expanded this cue list to include obvious derivational variants. This gave us a basic set of 34 S PECULA TION and 21 N EGATION cues. (Genetic Phenomena[MH] OR Metabolic Phenomena[MH] OR Cell Physiological Phenomena[MH] OR Biochemical Processes[MH]) AND open access[filter] We also used a data-driven strategy to find additional lemmas indicative of modification. We adapted the method of Rayson and Garside (2000) which uses log-likelihood for finding words that characterise differences between corpora. Here, the “corpora” are sentences attached to all events in the training set, and sentences attached to events which are subject to N EGATION or S PECULATION (treated separately). We build a frequency distribution over lemmas in each set of sentences, and calculate the log-likelihood for all lemmas, usFurthermore, the articles were split into sections and specific sections from the full text like Introduction, Background and Methods were removed to reduce the quantity of text to be annotated by TEES. Th"
W13-2005,W08-1301,0,0.02079,"Missing"
W13-2005,W09-1401,0,0.130196,"Introduction In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task. This task requires the development of systems that are capable of identifying bio-molecular events as those events are expressed in full-text publications. The task represents an important contribution to the broader problem of converting unstructured information captured in the biomedical literature into structured information that can be used to index and analyse bio-molecular relationships. This year’s task builds on previous instantiations of this task (Kim et al., 2009; Kim et al., 2012), with only minor changes in the task definition introduced for 2011. The task organisers provided full text publications annotated with mentions of biological entities including proteins and genes, and asked participants to provide annotations of simple events including gene expression, binding, localization, and protein modification, as well as higher-order regulation events (e.g., pos35 Proceedings of the BioNLP Shared Task 2013 Workshop, pages 35–44, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics nent of an event rule. The rule also cons"
W13-2005,W11-0204,0,0.0426084,"Missing"
W13-2005,W11-1826,1,\N,Missing
W13-2005,W11-1828,0,\N,Missing
