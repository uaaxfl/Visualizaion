2020.alta-1.17,2020.alta-1.18,0,0.0414709,"s )+R(ys ,ˆys ) |ys ∩ˆ ys | |ys | |ys ∩ˆ ys | |ˆ ys | 7 Results Table 2 shows the results of the systems in the private leaderboard. (1) where ys is the set of predicted labels in sample s, yˆs ) is the set of true labels in the sample, and S is the set of samples. If there were no true or no predicted labels, F1 (ys , yˆs ) := 0. 6 tree and Extreme Gradient Boosting (XGBoost) classifiers. Participating Systems In total 5 teams registered for the competitions, all of them in the student category. Of these, 3 teams submitted runs. Team NLP-CIC experimented with logistic regression and Roberta (Aroyehun and Gelbukh, 2020). Whereas the logistic regression classifier obtained the best results in the public leaderboard, it performed much worse in the private leaderboard. In contrast, the Roberta classifier obtained consistent results in both the public and private leaderboards. Team OrangutanV2 designed classifiers using ALBERT and transfer learning (Parameswaran et al., 2020). After observing that 22 tweets from the test set are also in the training set, they also incorporated a component that performed cosine similarity with the samples from the training data. Team NITS experimented with ensemble approaches (Kh"
2020.alta-1.17,2020.alta-1.20,0,0.0377318,"0). Whereas the logistic regression classifier obtained the best results in the public leaderboard, it performed much worse in the private leaderboard. In contrast, the Roberta classifier obtained consistent results in both the public and private leaderboards. Team OrangutanV2 designed classifiers using ALBERT and transfer learning (Parameswaran et al., 2020). After observing that 22 tweets from the test set are also in the training set, they also incorporated a component that performed cosine similarity with the samples from the training data. Team NITS experimented with ensemble approaches (Khilji et al., 2020). They obtained pretrained word embeddings and incorporated polynomial features. These features were fed to decision Team NLP-CIC OrangutanV2 NITS F1 p 0.155 0.105 0.053 0.313 0.010 Table 2: Results of the participating teams according to the private leaderboard. Column p indicates the Wilcoxon Signed Rank test between a team and the top team after removing ties. The results indicate that this task has been particularly challenging and there is room for improvement. A possible reason for the difficulty of this task is the small number (200) of annotated samples available. Another reason for th"
2020.alta-1.17,2020.alta-1.19,0,0.0430134,"Boosting (XGBoost) classifiers. Participating Systems In total 5 teams registered for the competitions, all of them in the student category. Of these, 3 teams submitted runs. Team NLP-CIC experimented with logistic regression and Roberta (Aroyehun and Gelbukh, 2020). Whereas the logistic regression classifier obtained the best results in the public leaderboard, it performed much worse in the private leaderboard. In contrast, the Roberta classifier obtained consistent results in both the public and private leaderboards. Team OrangutanV2 designed classifiers using ALBERT and transfer learning (Parameswaran et al., 2020). After observing that 22 tweets from the test set are also in the training set, they also incorporated a component that performed cosine similarity with the samples from the training data. Team NITS experimented with ensemble approaches (Khilji et al., 2020). They obtained pretrained word embeddings and incorporated polynomial features. These features were fed to decision Team NLP-CIC OrangutanV2 NITS F1 p 0.155 0.105 0.053 0.313 0.010 Table 2: Results of the participating teams according to the private leaderboard. Column p indicates the Wilcoxon Signed Rank test between a team and the top t"
2021.clpsych-1.5,N19-1423,0,0.0203429,"Missing"
2021.clpsych-1.5,W16-6208,0,0.0250832,"lue (stddev) across the five runs. Label category Precision Recall F1 Affection 0.62 (0.005) 0.65 (0.005) 0.63 (0.005) Anger 0.57 (0.005) 0.57 (0.004) 0.57 (0.000) Fear 0.54 (0.005) 0.49 (0.005) 0.52 (0.005) Happiness 0.58 (0.000) 0.59 (0.004) 0.58 (0.005) Sadness 0.54 (0.005) 0.60 (0.000) 0.56 (0.004) Surprise 0.52 (0.004) 0.47 (0.004) 0.49 (0.004) Figure 3: Confusion matrix for BERT model’s predictions. Numbers correspond to mean values (stddev) across the five runs. vlin et al., 2019).12 The model’s standard lexicon is manually augmented with emojis, using emoji2vec pre-trained embeddings (Eisner et al., 2016). We use the following hyperparameters. The maximum sequence length for the BERT tokeniser is set at 128. The learning rate is 3·10−5 . The batch size is 512 (spread over 4 GPUs). The number of epochs is 2, with checkpoints every 150 batches. The best checkpoint (as measured by macro F1) is saved. We train a separate model on each random subset. Macro F1 score ranges from 0.560 to 0.562, with a mean of 0.561 and a standard deviation of 0.001. Table 3 shows F1-score by class, and Figure 3 shows the confusion matrix for the model’s predictions; in both cases the values are averaged across the fi"
2021.clpsych-1.5,S12-1033,0,0.0351579,"Joy, Sadness, Disgust, Shame, Guilt) and to answer questions about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus s"
2021.clpsych-1.5,P17-1067,0,0.0154623,"all, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or Facebook, which support many other activities. This makes Vent particularly interesting for"
2021.clpsych-1.5,W17-1612,0,0.0287794,". As before, the category of Surprise appears to be less clearly connected with the texts properties: the classifier made the biggest number of mistakes on it, and these mistakes were relatively evenly spread across the other 5 categories. To better understand the classifier’s performance, we visually inspect 60 random sentences (10 per label category) in which the classifier made a wrong prediction. Given that the variability between the models in the five runs is small, we only examine predictions from a single model with the best macro F1 score. Table 4 shows the results. As recommended by Benton et al. (2017), all specific examples are rephrased to protect users privacy. In the majority of the vents (45), the label assigned by the classifier is consistent with the text. Common reasons for the mistakes include lack of context which would allow to clearly differentiate between several possible affective states (e.g., Affection and Happiness, or Anger and Sadness); multiple emotions clearly expressed in the text (in some cases the classifier did capture one of the emotions, while the label reflected another). In a minority of cases, it is not immediately clear whether the labels fit the text (8 cases"
2021.clpsych-1.5,C18-1179,0,0.0361885,"Missing"
2021.clpsych-1.5,W13-1602,0,0.0244196,"s about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or"
2021.clpsych-1.5,roberts-etal-2012-empatweet,0,0.028901,"sgust, Shame, Guilt) and to answer questions about their physiological and psychological state during these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towa"
2021.clpsych-1.5,P16-1148,0,0.0217899,"these emotion episodes. Overall, roughly 3,000 people from 37 countries completed the questionnaire, providing 7,666 textual descriptions. In comparison, our dataset contains considerably more data. A more widely used approach to produce emotion annotation without using experts is to rely on distant supervision — for example, treating Twitter hashtags like #happy or #sad as selfassigned emotion labels. Examples of datasets constructed with distant supervision include those by Mohammad (2012); Roberts et al. (2012); Wang et al. (2012); Qadir and Riloff (2013); Mohammad and Kiritchenko (2015); Volkova and Bachrach (2016); Abdul-Mageed and Ungar (2017). Emotion classifiers using these datasets are reported to perform well: the best results thus far were produced by Abdul-Mageed and Ungar (2017), who used a Gated Recurrent Neural Network (GRNN) classifier on 1.6 million tweets labelled with emotions from Plutchik’s categorisation (Plutchik, 1980) and 3 Vent and its Dataset Vent advertises itself as a platform to “Express your feelings and connect with people who care”. Vent is thus specifically geared towards sharing one’s emotions, unlike Twitter or Facebook, which support many other activities. This makes Ven"
J07-1004,A97-1012,0,0.0108158,"n knowledge, such as the medical domain (Lindberg, Humphreys, and McCray 1993) or the chemistry domain (Barker et al. 2004). Steady achievements in knowledge representation and reasoning (KR&R) techniques, which enable precise representation of both domain-related information and domain-related reasoning and deduction mechanisms (Barker et al. 2004). Advances in the development of modular and robust natural language processing systems (Abney 1996; Hobbs et al. 1997; Basili and Zanzotto 2002) in the context of the use of ontological resources for both textual interpretation and representation (Ait-Mokhtar and Chanod 1997) and database access (Popescu, Etzioni, and Kautz 2003). 5 This was the case for the on-line version of WordNet 2.1 (http://wordnet.princeton.edu/) on 8 October 2006. 49 Computational Linguistics r Volume 33, Number 1 Increasing success in the development of ontology-based QA frameworks where answers are derived from reasoning processes over questions and document ontological representations (Zajac 2001). Ontology-based question answering systems attack the answer-retrieval problem by means of an internal unambiguous knowledge representation. Both questions and knowledge are represented using"
J07-1004,W04-2510,0,0.0170523,"Missing"
J07-1004,W04-0506,0,0.00665651,"n is used for answering medical-related questions. Niu and Hirst (2004) describe a method for identifying semantic classes and the relations between them in medical texts. This approach is able to build an ontology for the domain automatically. Yu, Sable, and Zhu (2005) present an algorithm to classify medical questions based on a well-known hierarchical evidence taxonomy (Ely et al. 2002). Rinaldi, Dowdall, and Schneider (2004) describe the difficulties in adapting an existing RDQA system developed for assisting questions on UNIX technical manuals (Moll´a et al. 2000) to the Genomics domain. Benamara (2004) reports in detail on one of the currently most advanced RDQA systems. WEBCOOP is a logic-based system that integrates knowledge representation and advanced reasoning procedures to generate responses to natural queries. This system has been developed for the tourism domain. As for any knowledge intensive application, using ontologies for QA has as a limitation the restrictions imposed by the underlying knowledge representation models. Thus, in the following subsections we will focus on the efforts that are being employed from both historical trends in QA research (structured knowledge–based an"
J07-1004,D17-1090,0,0.0263215,"Missing"
J07-1004,W04-0507,0,0.0409138,"Missing"
J07-1004,W04-0502,0,0.0405149,"Missing"
J07-1004,A00-2008,0,0.00939492,"and, formalisms, theories, and algorithms either designed for domain document representation or reasoning may be made independent from the chosen domain ontology and can also be applied to different domains, thus enhancing system portability between domains. Research on using ontologies for QA has benefited from the following: r r r The increasing availability of ontologies encoding different kinds of knowledge. We can find ontologies ranging from general world knowledge resources, such as WordNet (Fellbaum 1998), EuroWordNet (Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson and Fillmore 2000, to very specific domain knowledge, such as the medical domain (Lindberg, Humphreys, and McCray 1993) or the chemistry domain (Barker et al. 2004). Steady achievements in knowledge representation and reasoning (KR&R) techniques, which enable precise representation of both domain-related information and domain-related reasoning and deduction mechanisms (Barker et al. 2004). Advances in the development of modular and robust natural language processing systems (Abney 1996; Hobbs et al. 1997; Basili and Zanzotto 2002) in the context of the use of ontological resources for both textual interpretat"
J07-1004,lin-2002-web,0,0.00891897,"specified semantic expressions where names of the semantic atoms and predicates are defined in an interlingual ontology. Answer retrieval is done using subsumption and unification, and queries are expanded using ontological rules. 51 Computational Linguistics Volume 33, Number 1 6.3 Integrating Heterogeneous Sources of Information More interesting than using a single database is the combination of databases with semistructured information (such as text with some XML markup) or even unstructured information (i.e., plain text). This has been proposed for World Wide Web–based question answering (Lin 2002), given the availability of pockets of information stored in databases on the World Wide Web. The idea is to analyze the question and find the relevant database among a preselected list if this is possible. If there are no suitable databases or it is not possible to determine the appropriate database query, then standard question-answering techniques are applied using the World Wide Web as a resource. The same strategy can be applied to question answering over restricted domains by keeping a set of relevant databases and a corpus of documents to query over in case the question is not covered i"
J07-1004,C02-1167,0,0.0067915,"ordNet contains a large list of open-class words grouped 47 Computational Linguistics Volume 33, Number 1 into synonym sets (the “synsets”). A range of synset relations is encoded, such as hypernymy/hyponymy, meronymy, and entailment. WordNet also includes word relations, such as antonymy. A departure from ontologies like Cyc (Lenat and Guha 1990) is that WordNet does not include formal definitions of the features of the objects. Still, for the purposes of this article, WordNet is an ontology. This view is supported by its use in many systems, including open-domain question answering systems (Moldovan and Novischi 2002). Open-domain ontologies like WordNet, however, are of limited use for QA in restricted domains. This is so because the information is unlikely to be well balanced with respect to the chosen domain. For example, parts of open-domain ontologies are too coarse-grained for specific restricted domains, whereas other parts are too fine-grained. And worse, open-domain ontologies may contain information that is not appropriate for specific restricted domains. Open-domain ontologies are too coarse-grained. Restricted domains, and especially technical domains, abound in terms that are specific to the d"
J07-1004,W04-0509,0,0.135077,"corpora of authoritative material. Demner-Fushman and Lin (2005) operationalize knowledge extraction for populating a database with PICO (Population, Intervention, Comparison, and Outcome) elements from medical abstracts obtained from MEDLINE. PICO structures are the frames used for evidence-based medicine. Sang, Bouma, and de Rijke (2005a) describe several strategies for populating a database with medical information related to diseases, symptoms, and treatments, which is automatically extracted from medical texts. This structured information is used for answering medical-related questions. Niu and Hirst (2004) describe a method for identifying semantic classes and the relations between them in medical texts. This approach is able to build an ontology for the domain automatically. Yu, Sable, and Zhu (2005) present an algorithm to classify medical questions based on a well-known hierarchical evidence taxonomy (Ely et al. 2002). Rinaldi, Dowdall, and Schneider (2004) describe the difficulties in adapting an existing RDQA system developed for assisting questions on UNIX technical manuals (Moll´a et al. 2000) to the Genomics domain. Benamara (2004) reports in detail on one of the currently most advanced"
J07-1004,W04-0508,0,0.00991019,"Missing"
J07-1004,A00-1023,0,0.00989365,"asks a question. There is a wealth of research in the area of NLIDBs and it is not within the scope of this article to survey this important area of research. Rather, we refer the reader to Androutsopoulos, Ritchie, and Thanisch (1995). Work in NLIDBs assumes an existing database that is queried over. If the database does not exist, it is created by using methods based on information-extraction technology. The aim is to extract all the information that might be used as an answer. A clear candidate is the use of named entities, but the creation of templates has also been tried in open domains (Srihari and Li 2000) and restricted domains (Weischedel, Xu, and Licuanan 2004). There are other systems that support this kind of knowledge-based questionanswering, including some dealing with questions unanticipated at the time of system construction. These include the AP Chemistry question-answering system (Barker et al. 2004), Cyc (Lenat and Guha 1990), the Botany Knowledge Base system (Porter et al. 1988), the two systems developed for DARPA’s High Performance Knowledge Base (HPKB) project (Cohen et al. 1988), and the two systems developed for DARPA’s Rapid Knowledge Formation (RKF) project (Schrag et al. 20"
J07-1004,W04-0505,0,0.0317105,"Missing"
J07-1004,W01-1205,0,0.0271262,"essing systems (Abney 1996; Hobbs et al. 1997; Basili and Zanzotto 2002) in the context of the use of ontological resources for both textual interpretation and representation (Ait-Mokhtar and Chanod 1997) and database access (Popescu, Etzioni, and Kautz 2003). 5 This was the case for the on-line version of WordNet 2.1 (http://wordnet.princeton.edu/) on 8 October 2006. 49 Computational Linguistics r Volume 33, Number 1 Increasing success in the development of ontology-based QA frameworks where answers are derived from reasoning processes over questions and document ontological representations (Zajac 2001). Ontology-based question answering systems attack the answer-retrieval problem by means of an internal unambiguous knowledge representation. Both questions and knowledge are represented using specific knowledge models based on ontological entities, concepts, and relations. The answering of questions is performed by applying different reasoning and proof techniques that allow the detection of textual entailment, which is useful in determining whether a given sentence answers a particular question. 6. The State of the Art in RDQA Current work on QA in restricted domains tends to exploit the cha"
J07-1004,W06-1910,0,\N,Missing
U03-1014,P99-1042,0,0.0136619,"xical, domain, or world knowledge. Furthermore, there is no disambiguation step and there is no anaphora resolution module. The resulting semantic interpreters may therefore be less accurate, but the resulting QA systems are in a better position to be compared with the QA systems based on grammatical relations described in Section 2. Once it is decided which methodology is better, it is conceivable to add the additional modules that further enhance the expressivity of the sentence image. For the present evaluation we used the Remedia Publications Reading Comprehension corpus used by DeepRead (Hirschman et al., 1999). The corpus is aimed at testing the degree of reading comprehension by children, and the documents Answer candidate John saw Mary Question Did John see Mary? Did Mary see John? Flat Logical Form object(’john’,o1,[x1]), object(’mary’,o3,[x3]), evt(’see’,e2,[x1,x3]) Flat Logical Form object(’mary’,O,[X]), evt(’see’,E,[Y,X]), object(’john’,O2,[Y]) object(’john’,O,[X]), evt(’see’,E,[Y,X]), object(’mary’,O2,[Y]) Table 3: Question answering using flat logical forms. Overlap shown in bold. Regex ˆWho ˆWhat ˆWhen ˆWhere ˆWhy Figure 4: Architecture of the question-answering system. in this corpus are"
U03-1014,W03-2806,1,0.808121,"Missing"
U03-1014,1993.iwpt-1.22,0,0.0108644,"ly SUBJ(eat,man, ) and SUBJ(come,man, ). Furthermore, in dependency grammar a word can have at most one dependent of each argument type, and so ate can have at most one object. But the same is not true for grammatical relations, and we get both OBJ(eat,banana, ) and OBJ(eat,apple, ). Thus, grammatical relations provide a sentence representation that is closer to the semantic contents of a sentence than the representation provided by dependency arcs. Moll´a and Hutchinson (2003) used the grammatical relations to compare the accuracy of two broad-coverage dependency-based parsers, Link Grammar (Sleator and Temperley, 1993) and Conexor Functional Dependency Grammar (Tapanainen and J¨arvinen, 1997) — henceforth referred to as Conexor FDG. The evaluation used a subset of the original relations: SUBJ, OBJ, XCOMP, and MOD. This subset was used because of limitations of the output of the parsers and the algorithms for the automatic construction of the grammatical relations. Thus, the reduced grammatical relations for the example The man that came ate bananas and apples with a fork without asking is: MOD(that,man,come), SUBJ(eat,man, ), Precision Recall SUBJ OBJ XCOMP MOD Average SUBJ OBJ XCOMP MOD Average Link Gramma"
U03-1014,A97-1011,0,0.170497,"Missing"
U04-1002,U03-1014,1,0.873343,"Missing"
U04-1002,A97-1011,0,0.357349,"Missing"
U05-1005,U04-1002,1,0.879664,"Missing"
U05-1005,W03-1604,1,0.848279,"Missing"
U05-1005,I05-1045,0,0.0999333,"Missing"
U05-1005,P79-1010,0,0.817616,"Missing"
U05-1005,A97-1011,0,0.460324,"Missing"
U05-1016,C02-1150,0,0.598165,"nt when the best information of each one are extracted and weighted. The common framework for question answering systems consists of three main phases: Question Analysis: The question is classified into several types, possibly forming a classification hierarchy such as (Moldovan et al., 1999). The question type is typically related to the type of the expected answer, which in turn is typically related to the named-entity types available to the system. The question classification can be based on regular expressions (Moll´a-Aliod, 2004; Chen et al., 2002; Hovy et al., 2000) or machine learning (Li and Roth, 2002; Zhang and Lee, 2003). Apart from the question type and expected answer type, this phase may return the question focus and other important words or concepts found in the question. Information Retrieval: The question and/or the question features obtained by the question analysis are fed to an information retrieval system that returns the documents or document fragments that may contain the answer. Typically a generic document retrieval system is used or even a web search engine, though there are suggestions that the type of information retrieval required for this phase is different from the ge"
U05-1016,E99-1001,0,0.0286151,"grained categories are used. 3.1 Pinpointing an exact answer Given the preferences explained above, we define the exact answer by extracting all the named-entities that match the expected answer category provided by our question analyser. The answer categories follow Li and Roth (2002) classification. They are divided into coarse and fine grained categories as shown on Table 1. We used a large collection of gazetteer files, involving most types of named-entities, along with the LingPipe named-entity recognizer12 for the definition of persons, organization and location names. In the spirit of Mikheev et al. (1999), we developed a set of internal and 12 http://www.alias-i.com/lingpipe/ 108 Table 1: Answer classification and examples Coarse HUM HUM LOC NUM NUM Fine IND GR CITY SPEED MONEY DESC ENTY ENTY ENTY ENTY ENTY DEF ANIMAL FOOD SUBSTANCE DISMED TERMEQ HUM (human) LOC (location) DEF (definition) TERMEQ (equivalent term) Example Who killed JFK? What ISPs exist in the NYC? What is the capital of Brazil? How fast is light? How much does the President get paid? What is ethology? What is a female rabbit called? What was the first Lifesaver flavor? What is a golf ball made of? What is a fear of disease? W"
U05-1016,U04-1004,1,0.812048,"that fail to be retrieved will be ignored in the next phase. Answer Extraction: The retrieved documents or passages are analysed in order to find the exact answers. Techniques to find the answer range from the selection of named-entities that are compatible with the expected answer type to the use of logical methods to find and/or validate the answer. As it can be observed in Figure 1, our system structure is very similar to the common framework, however the approaches for performing each of the tasks are different. The question analysis is performed using the Trie-based question classifier (Pizzato, 2004; Zaanen et al., 106 User question Trie-based Question Analyser Information Retrieval answer Answer Extraction Named Entity Recognition LingPipe Google Altavista MSNSearch Gigablast AskJeeves Lexical Rules N-Grams WordNet Mutual Information Answerbus Brainboost START snippets Figure 1: Overview of the system’s architecture 2005) trained over the set of near 5500 questions prepared by Li and Roth (2002). As already stated, the information retrieval stage is a combination of several Web search engine results, and the answer extraction combines namedentity, n-grams and lexico-semantic information"
U06-1009,C02-1025,0,0.0395136,"Missing"
U06-1009,P99-1042,0,0.0157143,"Missing"
U06-1009,C02-1150,0,0.0608387,"Missing"
U06-1009,U04-1002,1,0.870402,"Missing"
U06-1009,U05-1005,1,0.844757,"Missing"
U06-1009,W06-3807,1,0.830348,"Missing"
U06-1009,M95-1002,0,0.0197602,"Missing"
U06-1009,A97-1011,0,0.0155758,"Missing"
U06-1013,C02-1150,0,0.0948066,"Missing"
U07-1010,C02-1025,0,0.023716,". The first type focuses on internal evidence and highlights token properties including capitalisation, alpha/numeric information, etc. Some specific features are listed in Table 1. The second type of features focuses on external evidence that relates a token to tokens in surrounding text. Features that indicate which class has been assigned to the previous tokens and all of its class probabilities are also part of this type of feature. The last type of features focuses on global evidence related to all occurrences of the same token. These features are mainly inspired on features described by Chieu and Ng (2002). Currently AFNER only checks whether a token is always capitalised in a passage of text. 3.2 General Method The features described in the previous section are used in a maximum entropy classifier which for each token and for each category computes the probability of the token belonging to the category. Categories in this case are the named entity types prepended with ‘B’ and ‘I’ (indicating whether the token is at the beginning or inside a NE respectively), and a general ‘OUT’ category for tokens not in any entity. So for n named entities, n ∗ 2 + 1 61 categories are used. The classifier retu"
U07-1010,P07-1097,0,0.0174654,"uestion Answering (QA) has been very active during the last decade, mostly thanks to the Question Answering track of the Text REtrieval Conference (TREC) (Voorhees, 1999). The kinds of questions being asked range from factbased questions (also known as factoid questions) to questions whose answer is a list of facts, or definitions. The methods and techniques used have converged to a prototypical, pipeline-based architecture like the one we will describe here, and only recently the task has been diversified to more complex tasks such as TREC’s QA task of complex interactive question answering (Dang and Lin, 2007) or the Document Understanding Conference (DUC)’s track of query-driven summarisation (Dang, 2006). Whereas the TREC competitions concentrate on searching in English texts, CLEF (Cross-Language Evaluation Forum) focuses on non-English, crosslingual and multi-lingual search. Within this forum several competitions are organised. The QAst track deals with question answering on speech data. Prior to the QAst pilot track of CLEF there has been very little work on the area of question answering of speech data. Much of the work has focused on the task of recognising named entities by applying machine"
U07-1010,W06-0707,0,0.0235138,"g track of the Text REtrieval Conference (TREC) (Voorhees, 1999). The kinds of questions being asked range from factbased questions (also known as factoid questions) to questions whose answer is a list of facts, or definitions. The methods and techniques used have converged to a prototypical, pipeline-based architecture like the one we will describe here, and only recently the task has been diversified to more complex tasks such as TREC’s QA task of complex interactive question answering (Dang and Lin, 2007) or the Document Understanding Conference (DUC)’s track of query-driven summarisation (Dang, 2006). Whereas the TREC competitions concentrate on searching in English texts, CLEF (Cross-Language Evaluation Forum) focuses on non-English, crosslingual and multi-lingual search. Within this forum several competitions are organised. The QAst track deals with question answering on speech data. Prior to the QAst pilot track of CLEF there has been very little work on the area of question answering of speech data. Much of the work has focused on the task of recognising named entities by applying machine learning using features that leverage the very special kinds of information of speech data, parti"
U07-1010,W03-1211,0,0.0649843,"Missing"
U07-1010,U06-1009,1,0.876566,"Missing"
U07-1010,M95-1002,0,0.0528051,"l training set (a subset of BBN and AMI). Class ENAMEX TIMEX NUMEX Type Organization Person Location Date Time Money Percent of names that was part of the initial AFNER was left untouched. Table 3: Entity types used in the original version of AFNER 3.3 Adaptation of AFNER to QAst AFNER has been developed to work on news data, and as such, we had to modify parts of the system to allow it to be used in the QAst task. The first adaptation of AFNER is the selection of NE types. Originally AFNER focused on a limited set of entities similar to those defined in the Message Understanding Conferences (Sundheim, 1995), and listed in Table 3. For QAst we used a set of entity types that closely resembles the kinds of answers expected, as described by the QAst 2007 specification. The types used by the modified AFNER are listed in Table 2. The regular expressions that are used in AFNER to find MUC-type named entities were extended to cover the new types of entities. This process did not require much additional work, other than adding a few common names of shapes and colours. The lists 62 The general machine learning mechanism was left unmodified, and the set of features was also left untouched. The only differ"
U07-1014,J05-1004,0,0.00972826,"n for this model is due to its capability of generating questions regarding the sentence that has been modeled. In this section, we have shown how our model represents the semantic information. In the next section we focus on the implementation of QPLM and its usage. 3 Building and using QPLM As observed in Figure 2, a training set of QPLM triples was created using mapping rules from a corpus of semantic role labels. Using a syntactic parser and a NE recognizer with our training set, we were able to learn pattern rules that we further applied in the processing of the AQUAINT corpus. PropBank (Palmer et al., 2005) is a corpus with annotated predicate-argument relations from the same newswired source of information as the Penn Treebank2 . We used PropBank as our starting point 2 http://www.cis.upenn.edu/ treebank handcrafted mapping rules PropBank Connexor QPLM training data automated learning AQUAINT QPLM pattern rules PropBank with parse trees and named entities QPLM annotated AQUAINT LingPipe Figure 2: Creation and usage of pattern rules. because it comprises the same textual style, and the predicate-argument relations (also referred to as semantic roles) can be mapped to QPLM triples. We studied the"
U07-1014,U06-1013,1,0.883537,"Missing"
U10-1012,W04-1013,0,0.00990228,"reviews such as the Cochrane Reviews7 as a source for our corpus. Figure 1: Extract of a clinical inquiry from the Journal of Family Practice for the question “Which treatments work best for hemorrhoids?”. 1. The format of each inquiry is relatively uniform across all inquiries and therefore it enables a semi-automatic method to convert the data to a corpus that can be used by a machine. The authors present a fine review of possible evaluation methods and they finally settled for a combination of a factoid-based evaluation method, together with the automatic tool for summary evaluation ROUGE (Lin, 2004). The model summaries used for the automatic evaluation were the original paper abstracts. However, by evaluating on a set of abstracts the evaluation was not able to measure the system’s ability to perform query-based summarisation, since the abstracts were written prior to any query. The system by Fiszman et al. (2009) uses factoid-based evaluation that tests the summary ability to find good interventions. This kind of evaluation is not suitable for assessing the summary’s ability to indicate the quality of the clinical evidence or other aspects of the summaries that could be important to th"
U10-1012,N09-1003,0,0.0631476,"Missing"
U10-1012,P06-1106,0,0.127592,". The system was built using an iterative design that accommodates the feedback of a cohort of users. However, their developers acknowledge the lack of appropriate corpora, and to our knowledge neither CENTRIFUSER nor PERSIVAL were tested on a specific corpus for comparison with other systems. SemRep (Fiszman et al., 2004) provides abstractive summarisation by producing a semantic representation based on the UMLS concepts and their relations (Bodenreider, 2004) as found in the text. The evaluation was based on human judgement and therefore its results are not readily comparable. The system by Demner-Fushman and Lin (2006) produces multi-document summaries based on clusters of the main intervention found. Diego Molla. 2010. A Corpus for Evidence Based Medicine Summarisation. In Proceedings of Australasian Language Technology Association Workshop, pages 76−80 pointers to primary literature. Therefore, as they stand these collections could be used for questionanswering tasks but not for query-based summarisation. 3 A Corpus for Summarisation We are currently developing a corpus of questions and evidence-based information sourced from the Journal of Family Practice (JFP)5 . We are using all the 496 publicly availa"
U10-1012,W04-2611,0,0.351491,"orpus readily available specifically for the task. Afantenos et al. (2005) surveys research in summarisation from medical documents. One such summariser is CENTRIFUSER/PERSIVAL (Elhadad et al., 2005), which builds structured query-based representations of the documents as source for the summaries. The system was built using an iterative design that accommodates the feedback of a cohort of users. However, their developers acknowledge the lack of appropriate corpora, and to our knowledge neither CENTRIFUSER nor PERSIVAL were tested on a specific corpus for comparison with other systems. SemRep (Fiszman et al., 2004) provides abstractive summarisation by producing a semantic representation based on the UMLS concepts and their relations (Bodenreider, 2004) as found in the text. The evaluation was based on human judgement and therefore its results are not readily comparable. The system by Demner-Fushman and Lin (2006) produces multi-document summaries based on clusters of the main intervention found. Diego Molla. 2010. A Corpus for Evidence Based Medicine Summarisation. In Proceedings of Australasian Language Technology Association Workshop, pages 76−80 pointers to primary literature. Therefore, as they sta"
U11-1003,U11-1012,1,0.657753,"Missing"
U11-1012,W04-2611,0,0.0112893,"Missing"
U11-1012,W09-1320,0,0.0298689,"Missing"
U11-1012,W04-1013,0,0.0468931,"Missing"
U11-1012,U10-1012,1,0.881725,"Missing"
U11-1012,W03-1310,0,0.0721028,"Missing"
U11-1012,P07-1097,0,\N,Missing
U11-1012,J07-1005,0,\N,Missing
U11-1012,P06-1106,0,\N,Missing
U11-1014,W10-3104,0,0.0199652,"2010). We apply SVMs in our experiments and compare its performance with some other popular classifiers. Another important aspect of our work is negation detection. Negated terms in medical text usually indicate the presence or absence of specific medical findings. Additionally, they may also indicate the polarity of the outcome presented in a medical article (e.g., drug X shows no improvement for patients suffering from condition Y). Recent research work has shown that information on the polarity of phraselevel assertions does not improve performance in a document level classification task (Goldstein and Uzuner, 2010). However, statistics based on the presence/absence of negations have not been incorporated for text classification in this domain. Negation identification has shown to markedly improve performance of medical information retrieval systems. Therefore, there has been a significant amount of work on automatic negation detection techniques in the medical domain, such as the works of Elkin et al. (2005) and Huang et al. (2007) . Rokach et al. (2008) provides a detailed survey of negation detection techniques for the medical domain. A popular and simple negation detection approach is NegEx (Chapman"
U11-1014,J07-1005,0,0.128657,"egarding the efficacy of drug X for condition Y). Manually assessing the outcomes presented by multiple medical papers on a given topic is a time-consuming task and often cannot be efficiently performed at point of care (Ely et al., 1999). Hence, there is a strong need for automatic outcome polarity identification techniques to aid the decision making process of practitioners. 1.1 Motivation In order to appease the problem of information overload faced by medical domain experts, research has focused on information retrieval, automatic summarisation and question answering of medical documents (Lin and Demner-Fushman, 2007; Fiszman et al., 2009). Intelligent text processing systems that perform automatic summarisation and question answering for this domain can benefit significantly from techniques that can automatically detect the polarity of outcomes presented in documents. Such techniques will be particularly useful for multidocument summarisation, where the detection of contradictory or consistent outcomes presented in separate documents is vital. Furthermore, recent research on Abeed Sarker, Diego Moll´ a and C´ecile Paris. 2011. Outcome Polarity Identification of Medical Papers. In Proceedings of Australas"
U11-1014,P04-1035,0,0.370601,"Missing"
U11-1014,W02-1011,0,0.0228866,"Missing"
U11-1014,P02-1053,0,0.0228385,"Missing"
U11-1014,J11-2001,0,\N,Missing
U12-1017,I08-1050,0,0.0838659,"nt of the state-of-the-art system presented by (Kim et al., 2011), using a machine learning algorithm for predictions. 2.1 Naive Baseline For the naive baseline we merely rely on the most frequent label occurring in the training data, given the position of a sentence. For instance, for the ﬁrst four sentences in the abstract the most frequent label is Background, for the ﬁfth it is Other, etc. 2.2 Conditional Random Field (CRF) Benchmark CRFs (Lafferty et al., 2001) were designed to label sequential data, and we chose this approach because it has shown success in sentence-level classiﬁcation (Hirohata et al., 2008; Chung, 2009; Kim et al., 2011). Thus we tried to replicate the classiﬁer used by (Kim et al., 2011). However our systems differ in the selection of features used for training. We use lexical and structural features: 1. Lexical features: bag of words and Part Of Speech (POS) tags for the lexical features; and 2. Structural features: position of the sentences and the rhetorical headings from the structured abstracts. If a heading h1 covered three lines in the abstract, all the three lines will be labeled as h1. We used NLTK (Bird et al., 2009) to produce a list of POS tags and for the CRF clas"
U12-1017,U12-1019,0,0.12011,"Missing"
U12-1017,C00-2137,0,\N,Missing
U12-1017,U12-1020,1,\N,Missing
U13-1008,U11-1012,1,0.414703,"Missing"
U13-1008,J13-3008,0,0.0718542,"Missing"
U13-1008,U04-1000,0,\N,Missing
U13-1019,P03-1020,0,0.534177,"obtained cheaply. One only needs to take a piece of text and remove case and punctuation. By doing this we can obtain the input data (the text with the case and punctuation information removed) and the target data (the original text). It has been observed that using this approach to generate training data suffices to obtain reasonable results (Niu et al., 2004), and this observation agrees with the results obtained in this shared task, as we will show in this paper. 2 The 2013 ALTA Shared Task Case and punctuation restoration can be formulated as a text classification task. Baldwin and Joseph (2009) used a multi-label classification approach where a word can have multiple labels, each label indicating the information to be restored in the word. For example, the set of labels CAP1+FULLSTOP+COMMA indicates that the word has the first character as uppercase and is followed by a full stop and a comma. Thus, if the word was corp, the labels indicate that the word should be restored to Corp.,. There is a label ALLCAPS to indicate that all letters in the word need to be uppercased, and the specific label NOCHANGE indicates that the word does not need any special restoration. Diego Molla. 2013."
U13-1019,de-marneffe-etal-2006-generating,0,\N,Missing
U13-1019,W12-4304,0,\N,Missing
U13-1019,J08-1002,0,\N,Missing
U13-1019,W11-1816,0,\N,Missing
U13-1019,D07-1111,0,\N,Missing
U13-1019,P05-1022,0,\N,Missing
U13-1019,W11-1806,0,\N,Missing
U13-1019,W13-2011,0,\N,Missing
U13-1019,W13-2010,0,\N,Missing
U13-1019,W13-2003,0,\N,Missing
U13-1019,W13-2012,0,\N,Missing
U13-1019,W11-1826,0,\N,Missing
U13-1019,W11-1828,0,\N,Missing
U13-1019,W11-1801,0,\N,Missing
U13-1019,E12-2021,0,\N,Missing
U14-1010,P11-1051,0,0.0151935,"ion from citing texts has been used in decades-old studies (Garfield et al., 1964). More recently, Nakov et al. (2004) proposed the use of citations for the semantic interpretation of bioscience text. They used the text surrounding the citations, which they named “citances”, to summarise the original text. Further research focused on the extraction of the citances and surrounding text (Qazvinian and Radev, 2010) and on the use of these citances to gather information about the original text, which could be used as a surrogate of, or in addition to, a summary of the text (Mohammad et al., 2009; Abu-Jbara and Radev, 2011). The Biomedical Summarization Track of the 2014 Text Analysis Conference (TAC 2014 BiomedSumm Track)1 was designed as a set of shared tasks that focus on the use of the citances to build summaries of biomedical documents. The track organisers provided a small data set of 20 biomedical documents for training and fine-tuning. Each paper of the data set (henceforth “reference paper”) has 10 citing papers, and the data are annotated with the citances found in the citing papers. For each citance, four annotators appointed by the National Institute of Standards and Technology (NIST) identified vari"
U14-1010,N10-1133,0,0.120264,"of sentences that haven been chosen in the summary so far. Figure 2: Maximal Marginal Relevance (MMR) concepts, and s stands for the tf.idf of the UMLS semantic types. We did not observe any improvement of the results when incorporating the WordNet synsets in our preliminary experiments and therefore we did not use them in the experiments reported in this paper. it is very difficult for two different extractive summarisation systems to produce ROUGE F1 scores with non-overlapping confidence intervals due to the long-tailed nature of the distribution of ROUGE F1 scores among different systems (Ceylan et al., 2010). In our case, in addition, the amount of data is fairly small. Nevertheless, it appears that using UMLS improves the results, and whereas MMR gives better results than UMLS, the difference is so small that it might not be worth incorporating MMR. SVD appears to improve the results over plain tf.idf, but again the improvement is small and the computation time increased dramatically. None of the methods approached the results of the oracle, so there is room for improvement. Still, as we will show below, these techniques are useful for extractive summarisation. Note that the best result overall"
U14-1010,W04-1013,0,0.00564046,"till, as we will show below, these techniques are useful for extractive summarisation. Note that the best result overall is plain tf.idf where the data have been expanded with the citing papers and the sentences have been expanded with a large context window (50, instead of 20). The computation time of this approach far exceeded that of the other approaches in the table, so there is still room for further exploring the use of UMLS, or smart forms to determine the related documents and extending the sentence context. 2.7 Results To evaluate the methods we have used the ROUGE-L F1 score. ROUGE (Lin, 2004) is a popular evaluation method for summarisation systems that compares the output of the system against a set of target summaries. For each citance, we used the target reference text provided by the annotators, except for the Oracle setting, as described above, where the reference text of one annotator was compared against the reference text of the other three annotators. Table 1 summarises the results of our experiments. The results of the oracle approach are relatively poor. This indicates relatively low agreement among the annotators. We can observe an improvement of the results when using"
U14-1010,N09-1066,0,0.0237074,"et al. (2005). Information from citing texts has been used in decades-old studies (Garfield et al., 1964). More recently, Nakov et al. (2004) proposed the use of citations for the semantic interpretation of bioscience text. They used the text surrounding the citations, which they named “citances”, to summarise the original text. Further research focused on the extraction of the citances and surrounding text (Qazvinian and Radev, 2010) and on the use of these citances to gather information about the original text, which could be used as a surrogate of, or in addition to, a summary of the text (Mohammad et al., 2009; Abu-Jbara and Radev, 2011). The Biomedical Summarization Track of the 2014 Text Analysis Conference (TAC 2014 BiomedSumm Track)1 was designed as a set of shared tasks that focus on the use of the citances to build summaries of biomedical documents. The track organisers provided a small data set of 20 biomedical documents for training and fine-tuning. Each paper of the data set (henceforth “reference paper”) has 10 citing papers, and the data are annotated with the citances found in the citing papers. For each citance, four annotators appointed by the National Institute of Standards and Techn"
U14-1010,U11-1012,1,0.906411,"Missing"
U14-1010,P10-1057,0,0.0170572,"ant information. This is certainly the case in the medical domain, and several approaches for the automated summarisation of medical text have been proposed, e.g. as surveyed by Afantenos et al. (2005). Information from citing texts has been used in decades-old studies (Garfield et al., 1964). More recently, Nakov et al. (2004) proposed the use of citations for the semantic interpretation of bioscience text. They used the text surrounding the citations, which they named “citances”, to summarise the original text. Further research focused on the extraction of the citances and surrounding text (Qazvinian and Radev, 2010) and on the use of these citances to gather information about the original text, which could be used as a surrogate of, or in addition to, a summary of the text (Mohammad et al., 2009; Abu-Jbara and Radev, 2011). The Biomedical Summarization Track of the 2014 Text Analysis Conference (TAC 2014 BiomedSumm Track)1 was designed as a set of shared tasks that focus on the use of the citances to build summaries of biomedical documents. The track organisers provided a small data set of 20 biomedical documents for training and fine-tuning. Each paper of the data set (henceforth “reference paper”) has"
U14-1010,U04-1000,0,\N,Missing
U14-1022,U14-1023,0,0.0381277,"Missing"
U14-1022,U14-1025,0,0.0382852,"Missing"
U14-1022,U14-1024,0,0.0119386,"assessment component of existing academic subjects, in contrast with, for example, the shared task of 2013. Still, some of the participants were very active, and for example, the total number of runs submitted among the 4 teams was 168. The details of some of the systems participating in this year’s competition have been included in the proceedings of the 2014 Australasian Language Technology Workshop (ALTA 2014). The systems used a range of techniques, including the use of sequence labellers, feature engineering, and combination of classifiers following ensemble and staking processes. Parma Nand et al. (2014) report on AUT NLP’s team. They used the Stanford named entity recogniser without training it with the tweed data due to the reduced amount of training data available, in conjunction with various rule-based modules and knowledge infusion. Fei Liu (2014) report on Yarra’s team. They use a variety of lexical, structural and geospatial features together with CRF++’s Conditional Random Field (CRF) sequence labeller. They also experimented with classifier stacking and methods for self-training. Finally, Bo Han et al. (2014) report on JKRowling’s team. They used a CRF sequence labeller and experimen"
U14-1022,D09-1026,0,0.011625,"of the sample solution. 3 Related Work Research community has been active in location extraction and inferencing locations based on the extracted location mentions from both formal text and social media. Below, we briefly cover two areas of named entity recognition and location extraction in social media, especially Twitter. 3.1 Named entity recognition in Twitter Ritter et al. (2011) developed a set of tools designed specifically to perform tasks such as NER 152 and part of speech tagging (POS) on tweets. They use distant supervision with topic modelling using an approach called LabelledLDA (Ramage et al., 2009). One of the entities in the NER tool provided by Ritter et al, was geo-location. TwiNER (Li et al., 2012) is another NER system for Twitter. It follows an unsupervised approach which exploits the co-occurrence information of named entities in a tweet stream. A significant difference with Ritter et. al.. (Ritter et al., 2011) is that TwiNER does not rely on linguistic features asserting that they are unreliable in the tweet domain. Instead its algorithm relies on external sources such as Wikipedia. This system however only identifies named entities and it does not classify them into a type suc"
U14-1022,D11-1141,0,0.0403193,"so asked if a location had multiple words, to separate them with blank space so that, in effect, it does not matter whether it is one location expression with two words or two different location expressions. Table 2 shows an extract of the sample solution. 3 Related Work Research community has been active in location extraction and inferencing locations based on the extracted location mentions from both formal text and social media. Below, we briefly cover two areas of named entity recognition and location extraction in social media, especially Twitter. 3.1 Named entity recognition in Twitter Ritter et al. (2011) developed a set of tools designed specifically to perform tasks such as NER 152 and part of speech tagging (POS) on tweets. They use distant supervision with topic modelling using an approach called LabelledLDA (Ramage et al., 2009). One of the entities in the NER tool provided by Ritter et al, was geo-location. TwiNER (Li et al., 2012) is another NER system for Twitter. It follows an unsupervised approach which exploits the co-occurrence information of named entities in a tweet stream. A significant difference with Ritter et. al.. (Ritter et al., 2011) is that TwiNER does not rely on linguis"
U15-1006,U11-1012,1,0.775097,"Missing"
U15-1006,U13-1008,1,0.88776,"Missing"
U15-1006,U15-1008,1,0.884965,"Missing"
U15-1017,N01-1014,0,0.0446401,"irs of words in both languages. Early work investigated the use of single orthographic or phonetic similarity measures, such as Edit Distance (ED) (Levenshtein, 1966), Dice coefficient (Brew and McKelvie, 1996), Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). Kondrak and Dorr (2004) reported that a simple average of several orthographic similarity measures outperformed all the measures on the task of the identification of cognates for drug names. More recently, Rama (2014) combined the subsequence features and a number of word shape similarity scores as features to train a SVM model. Kondrak (2001) proposed COGIT, a cognate-identification system that combines phonetic similarity with semantic similarity, the latter being measured from a distance between glosses in a lexical handcrafted resource. Frunza (2006) explored a range of machine learning techniques for word shape similarity measures, and also investigated the use of bi-lingual dictionaries to detect if the words were likely translations of each other. Mulloni, Pekar, Mitkov and Blagoev (2007) also combined orthographic similarity and semantic similarity, the latter being measured based on lists of collocated words. Participants"
U15-1017,C04-1137,0,0.0232263,"if one refers to furniture (false cognate) as chaire if one refers to a University position (true cognate), while chair in French means flesh in English (false cognate)). 3 The task of detecting potential cognates is in contrast to many experimental settings in the literature that focused on detecting pairs of cognates amongst pairs of words in both languages. Early work investigated the use of single orthographic or phonetic similarity measures, such as Edit Distance (ED) (Levenshtein, 1966), Dice coefficient (Brew and McKelvie, 1996), Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). Kondrak and Dorr (2004) reported that a simple average of several orthographic similarity measures outperformed all the measures on the task of the identification of cognates for drug names. More recently, Rama (2014) combined the subsequence features and a number of word shape similarity scores as features to train a SVM model. Kondrak (2001) proposed COGIT, a cognate-identification system that combines phonetic similarity with semantic similarity, the latter being measured from a distance between glosses in a lexical handcrafted resource. Frunza (2006) explored a range of machine learning techniques for word shape"
U15-1017,J99-1003,0,0.135525,"rench as chaise if one refers to furniture (false cognate) as chaire if one refers to a University position (true cognate), while chair in French means flesh in English (false cognate)). 3 The task of detecting potential cognates is in contrast to many experimental settings in the literature that focused on detecting pairs of cognates amongst pairs of words in both languages. Early work investigated the use of single orthographic or phonetic similarity measures, such as Edit Distance (ED) (Levenshtein, 1966), Dice coefficient (Brew and McKelvie, 1996), Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). Kondrak and Dorr (2004) reported that a simple average of several orthographic similarity measures outperformed all the measures on the task of the identification of cognates for drug names. More recently, Rama (2014) combined the subsequence features and a number of word shape similarity scores as features to train a SVM model. Kondrak (2001) proposed COGIT, a cognate-identification system that combines phonetic similarity with semantic similarity, the latter being measured from a distance between glosses in a lexical handcrafted resource. Frunza (2006) explored a range of machine learning"
U15-1017,U14-1003,1,0.748224,"Missing"
U16-1020,P98-1012,0,0.292638,"ucerzan, 2007) or Freebase (Zheng et al., 2012). In this case, the domain of linkable entities is limited by the coverage of the target knowledge base and those which fall outside this domain are classified as NILs. NIL mention clustering is often addressed separately, and has been the focus of Text Analysis Conference (TAC) Knowledge Base Population shared tasks since 2011 (Ji et al., 2011). A more generalised approach to resolving mention ambiguity is that of cross-document coreference resolution — where systems cluster mentions of the same entity together without reference to a central KB (Bagga and Baldwin, 1998; Singh et al., 2011). Both NIL clustering and crossdocument coreference deal with ambiguity at the mention level. In contrast, the task of cross-KB coreference resolution deals with entity coreference at the KB level, by attempting to cluster entity records across distinct KBs. This task is similarly structured to that of record linkage (Fellegi and Sunter, 1969; Xu et al., 2013). But, where record linkage commonly operates over structured databases, cross-KB coreference relies primarily on unstructured nodes as input. Cross-KB coreference can draw on the content of the entity endpoint, inclu"
U16-1020,W16-1302,1,0.748073,"hat link to an entity endpoint. In the web domain, work on finding links associated with existing KB entities (Hachenberg and Gottron, 2012) and web person search (WePS) (Artiles et al., 2007) is also closely related. WePS takes the output of a web search for some entity name and attempts to cluster the results that refer to the same underlying entity. The ultimate aim of cross-KB coreference is also to cluster web pages. By contrast, however, it focuses on clustering entity endpoint pages instead of entity mention pages. The task builds in part on the Knowledge Base Discovery (KBD) system of Chisholm et al. (2016), where the existence of web endpoints may be inferred from their usage on the web. Shared task data and evaluation are described below. 162 Data Constructing a balanced corpus of endpoint URL pairs which present non-trivial cases of entity ambiguity is a challenging task. Randomly sampling from a corpus of web links is insufficient as any two URLs are unlikely to refer to the same entity, leading to a highly imbalanced dataset of negative samples. Conversely, if we constrain our sampling to pairs linked from similar anchor text, almost all pairs will be coreferent since entity mentions follow"
U16-1020,D07-1074,0,0.0493395,"s fall into the open category. The prize is awarded to the team that performs best on the private test set — a subset of the evaluation data for which participant scores are only revealed at the end of the evaluation period (see Section 5). Andrew Chisholm, Ben Hachey and Diego Mollá. 2016. Overview of the 2016 ALTA Shared Task: Cross-KB Coreference. In Proceedings of Australasian Language Technology Association Workshop, pages 161−164. 3 4 Related Work Entity disambiguation work has traditionally focused on the reconciliation of textual mentions to records in a centralised KB like Wikipedia (Cucerzan, 2007) or Freebase (Zheng et al., 2012). In this case, the domain of linkable entities is limited by the coverage of the target knowledge base and those which fall outside this domain are classified as NILs. NIL mention clustering is often addressed separately, and has been the focus of Text Analysis Conference (TAC) Knowledge Base Population shared tasks since 2011 (Ji et al., 2011). A more generalised approach to resolving mention ambiguity is that of cross-document coreference resolution — where systems cluster mentions of the same entity together without reference to a central KB (Bagga and Bald"
U16-1020,U16-1021,0,0.0130112,"ue positives and false positives f p). Recall is the ratio of true positives to all actual positives (the number of pairs of endpoints that are coreferring according to the test data, computed as the sum of true positives and false negatives f p). The formula of the F1 score is: p·r F1 = 2 p+r where p= tp , tp + f p r= tp tp + f n This section presents short descriptions of some of the participating systems. For further details, refer to the shared task section of the proceedings of the 2016 ALTA workshop. 6.1 2 https://inclass.kaggle.com/c/ alta-2016-challenge 163 EOF The system by team EOF (Khirbat et al., 2016) follows a two-stage approach. First, in the entity endpoint determination stage, the system determines the most likely underlying entities being referred to by each URL. Second, in the entity disambiguation stage, the two endpoints are disambiguated. Entity endpoint determination is achieved by extracting the named entities of the text pointed by the URL using the Stanford NER, and ranking the entities using logistic regression. The top 3 entities are passed to the entity disambiguation stage, together with additional features based on the URLs, anchor texts of the URLs, and the text pointed"
U16-1020,P11-1080,0,0.0251476,"e (Zheng et al., 2012). In this case, the domain of linkable entities is limited by the coverage of the target knowledge base and those which fall outside this domain are classified as NILs. NIL mention clustering is often addressed separately, and has been the focus of Text Analysis Conference (TAC) Knowledge Base Population shared tasks since 2011 (Ji et al., 2011). A more generalised approach to resolving mention ambiguity is that of cross-document coreference resolution — where systems cluster mentions of the same entity together without reference to a central KB (Bagga and Baldwin, 1998; Singh et al., 2011). Both NIL clustering and crossdocument coreference deal with ambiguity at the mention level. In contrast, the task of cross-KB coreference resolution deals with entity coreference at the KB level, by attempting to cluster entity records across distinct KBs. This task is similarly structured to that of record linkage (Fellegi and Sunter, 1969; Xu et al., 2013). But, where record linkage commonly operates over structured databases, cross-KB coreference relies primarily on unstructured nodes as input. Cross-KB coreference can draw on the content of the entity endpoint, including the URL, the tex"
U18-1011,U18-1012,0,0.0525886,"port/Categorization/dataset/ index.html 85 ID Label 0 1 2 3 4 5 A G A A D A 31.9 32.8 A 20 19.7 G 14.6 16 13 11.1 10.2 8.3 6.3 7.2 3.8 4.5 C B Table 2: First 5 rows of the training data H E 4 Data F The data used in the 2018 ALTA Shared Task consists of a collection of Australian patents partitioned into 3,972 documents for training and 1,000 documents for test. The documents are plain text files which are the result of applying a text extracting tool on the original PDF files. As a result, there are errors in the documents, some of which are documented by the participants of the shared task (Benites et al., 2018; Hepburn, 2018). In particular, 61 documents contain the string “NA[newline]parse failure”. In addition, meta-data information such as titles, authors, etc. are not marked up in the documents. D Train Test Figure 1: Distribution of labels in percentages lenge”.3 This enabled the participants to submit runs prior to the submission deadline for immediate feedback and compare submissions in a leaderboard. The framework provided by Kaggle in Class allowed the partition of the test data into a public and a private section. Whenever a participating team submitted a run, the evaluation results of th"
U18-1011,U18-1013,0,0.131833,"taset/ index.html 85 ID Label 0 1 2 3 4 5 A G A A D A 31.9 32.8 A 20 19.7 G 14.6 16 13 11.1 10.2 8.3 6.3 7.2 3.8 4.5 C B Table 2: First 5 rows of the training data H E 4 Data F The data used in the 2018 ALTA Shared Task consists of a collection of Australian patents partitioned into 3,972 documents for training and 1,000 documents for test. The documents are plain text files which are the result of applying a text extracting tool on the original PDF files. As a result, there are errors in the documents, some of which are documented by the participants of the shared task (Benites et al., 2018; Hepburn, 2018). In particular, 61 documents contain the string “NA[newline]parse failure”. In addition, meta-data information such as titles, authors, etc. are not marked up in the documents. D Train Test Figure 1: Distribution of labels in percentages lenge”.3 This enabled the participants to submit runs prior to the submission deadline for immediate feedback and compare submissions in a leaderboard. The framework provided by Kaggle in Class allowed the partition of the test data into a public and a private section. Whenever a participating team submitted a run, the evaluation results of the public partiti"
U19-1026,L18-1424,1,0.682663,"been reported for automatic detection of sarcasm in text, spanning rule-based approaches to deep neural architectures (Joshi et al., 2017). Since sarcasm is a peculiar form of sentiment expression, the target of a sarcastic text bears implications on attribution of the negative sentiment to the appropriate target. For example, for an aspect-based sentiment analysis system, the sarcasm target will be the aspect towards which a negative sentiment will be assigned. Two prior papers report approaches for sarcasm target identification. The problem of sarcasm target identification was introduced in Joshi et al. (2018). They present three kinds of methods: (a) rule-based which use heuristics to determine sarcasm targets, (b) learning-based which use a sequence labelling algorithm trained on a dataset labelled with sarcasm targets, and (c) a hybrid of the two where output of the two systems is combined to make the final predictions. More recently, Patro et al. (2019) present a deep learning-based architecture for sarcasm target identification. The semantic representation of each word is captured in terms of its context window using a bidirectional LSTM. This semantic representation is then concatenated with"
U19-1026,maynard-greenwood-2014-cares,0,0.0334276,"s posted on social media. We introduce the task, describe the data and present the results of baselines and participants. This year’s shared task was particularly challenging and no participating systems improved the results of our baseline. 1 Introduction Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Sarcastic text has been understood to be a challenge to sentiment analysis because a sarcastic text may appear to be positive on the surface but is intended to be negative. Empirical results also show that sarcastic text is detrimental to sentiment analysis (Maynard and Greenwood, 2014). Applications where sentiment understanding is important are also impacted by sarcastic text. These applications include dialogue systems where the correct prediction of sentiment is important to generate appropriate responses. Towards this, computational sarcasm has gained interest in the research community. Sentiment in a text can be understood to be composed of valence (positive/negative) and the target (Liu, 2012). The connection between sarcasm detection and sentiment analysis is the target. Sarcastic text bears a target of ridicule. It is this target that receives negative sentiment in"
U19-1026,D19-1663,0,0.02034,"ment analysis system, the sarcasm target will be the aspect towards which a negative sentiment will be assigned. Two prior papers report approaches for sarcasm target identification. The problem of sarcasm target identification was introduced in Joshi et al. (2018). They present three kinds of methods: (a) rule-based which use heuristics to determine sarcasm targets, (b) learning-based which use a sequence labelling algorithm trained on a dataset labelled with sarcasm targets, and (c) a hybrid of the two where output of the two systems is combined to make the final predictions. More recently, Patro et al. (2019) present a deep learning-based architecture for sarcasm target identification. The semantic representation of each word is captured in terms of its context window using a bidirectional LSTM. This semantic representation is then concatenated with features based on LIWC, NER, empathy and POS tags, to learn a classifier. They show an improvement over the prior work. 4 Data The data used in the 2019 ALTA Shared Task consists of 950 training samples and 544 test samples. A count of the words appearing in the targets of the training data (Figure 1) reveals that a large percentage of the data is labe"
W00-0604,P99-1042,0,0.0802165,"advance Computational Linguistics as a science rather than as an engineering discipline. The suggestion made by the organizers of this workshop on how this could be achieved has-four comPonents. First, they suggest to use full-fledged text-based question answering (QA) as task. Second, t h e y suggest a relatively small amount off text (compared with the volumes of text used in TREC) as test data. Third they (seem to) suggest to .use texts from a wide range off domains. Finally they suggest to use pre-existing question/answer pairs, developed for and tested on humans, as evaluation benchmark (Hirschman et al., 1999). Third, we think it is very important to restrict the domain of the task. We certainly do not argue in favour of some toy domain but we get the impression that the reading comprehension texts under consideration cover a far too wide range of topics. We think t h a t technical manuals are a better choice. T h e y cover a narrow domain (such as computer operating systems, or airplanes), and they also use a relatively restricted type of language with a reasonably clear semantic foundation. Fourth, we think that tests that are specifically designed to evaluate to what extent a human being underst"
W03-1604,C94-2195,0,0.0913695,"Missing"
W03-1604,W03-1801,1,0.862694,"Missing"
W03-1604,P98-1092,0,0.011593,"he query Where are the stowage compartments installed? is translated internally into the Horn query (2). (2) evt(install,A,[B,C]), object(D,E,[B]), object(s stowage compartment,G,[C]) This means that a term (belonging to the same synset as stowage compartment) is involved in an install event with an anonymous object. If there is an MLF from the document that can match example (2), then it is selected as a candidate answer and the sentence it originates from is shown to the user. The process of terminological variation is well investigated (Ibekwe-SanJuan and Dubois, 2002; Daille et al., 1996; Ibekwe-Sanjuan, 1998). The primary focus has been to use linguistically based variation to expand existing term sets through corpus investigation or to produce domain representations. A subset of such variations identifies terms which are strictly synonymous. ExtrAns gathers these morpho-syntactic variations into synsets. The sets are augmented with terms exhibiting three weaker synonymy relations described by Hamon & Nazarenko (2001). These synsets are organized into a hyponymy (isa) hierarchy, a small example of which can be seen in Figure 5. Figure 4 shows a schematic representation of this process. The first s"
W03-1604,J94-4002,0,0.0203516,"Missing"
W03-1604,W98-0604,0,0.172057,"the stapler is $10. Where is Thimphu located? / Thimphu is capital of what country? Of course combinations of the different types are possible, e.g. Oswald killed Kennedy / Kennedy was assassinated by Oswald is a combination of (1) and (2). Different types of knowledge and different linguistic resources are needed to deal with each of the above types. While type (1) can be dealt with using a resource such as WordNet (Fellbaum, 1998), type (2) needs effective parsing and mapping of syntactic structures into a common deeper structure, possibly using a repository of nominalisations like NOMLEX (Meyers et al., 1998). More complex approaches are needed for the other types, up to type (6) where generic world knowledge is required, for instance to know that being a capital of a country implies being located in that country. 1 Such world knowledge could be expressed in the form of axioms, like the following: (X costs Y) iff (the price of X is Y) In this paper we focus on the role of paraphrases in a Question Answering (QA) system targeted at 1 Note that the reverse is not true, and therefore this is not a perfect paraphrase. technical manuals. Technical documentation is characterised by vast amounts of domai"
W03-1604,1993.iwpt-1.22,0,0.0116038,"Missing"
W03-1604,C98-1089,0,\N,Missing
W03-2806,P85-1008,0,0.109009,"Missing"
W03-2806,H91-1060,0,0.142145,"an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to test the parsing systems. In particular, the extrinsic evaluation uses ExtrAns, an answer extraction system that operates over Unix manual pages (Moll´a et al., 20"
W03-2806,P96-1025,0,0.0302918,"on: “Intrinsic criteria are those relating to a system’s objective, extrinsic criteria those relating to its function i.e. to its role in relation to its setup’s purpose.” (Galliers and Sparck Jones, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic ev"
W03-2806,1993.iwpt-1.22,0,0.748683,"m’s objective, extrinsic criteria those relating to its function i.e. to its role in relation to its setup’s purpose.” (Galliers and Sparck Jones, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real ap"
W03-2806,A97-1011,0,0.155126,"Missing"
W06-3807,P04-3020,0,0.0371201,"tions together with their answers and sentences containing the answers. To obtain such a corpus Ravichandran and Hovy (2002) mine the Web to gather the relevant data. Other methods learn patterns based on syntactic information. For example, Shen et al. (2005) develop a method of extracting dependency paths connecting answers with words found in the question. However we are not aware of any method that attempts to learn patterns based on logical information, other than our own. There is recent interest on the use of graph methods for Natural Language Processing, such as document summarisation (Mihalcea, 2004) document retrieval (Montes-y-G´omez et al., 2000; Mishne, 2004), and recognition of textual entailment (Pazienza et al., 2005). The present very workshop shows the current interest on the area. However, we are not aware of any significant research about the use of conceptual graphs (or any other form of graph representation) for question answering other than our own. 6 Conclusions We have presented a method to learn question answering rules by applying graph manipulation methods on the representations of questions and answer sentences. The method is independent of the actual graph representat"
W06-3807,U05-1005,1,0.661337,"Missing"
W06-3807,P02-1006,0,0.114258,"Missing"
W06-3807,C69-0201,0,0.217353,"Missing"
W06-3807,I05-1045,0,0.0187921,"d it does not use any NE recogniser, the results are satisfactory. 5 Related Research There have been other attempts to learn QA rules automatically. For example, Ravichandran and Hovy (2002) learns rules based on simple surface patterns. Given that surface patterns ignore much linguistic information, it becomes necessary to gather a large 43 corpus of questions together with their answers and sentences containing the answers. To obtain such a corpus Ravichandran and Hovy (2002) mine the Web to gather the relevant data. Other methods learn patterns based on syntactic information. For example, Shen et al. (2005) develop a method of extracting dependency paths connecting answers with words found in the question. However we are not aware of any method that attempts to learn patterns based on logical information, other than our own. There is recent interest on the use of graph methods for Natural Language Processing, such as document summarisation (Mihalcea, 2004) document retrieval (Montes-y-G´omez et al., 2000; Mishne, 2004), and recognition of textual entailment (Pazienza et al., 2005). The present very workshop shows the current interest on the area. However, we are not aware of any significant rese"
W06-3807,P79-1010,0,0.737823,"ss it is useful to associate a weight to every rule learnt. The rule weight is computed by testing the accuracy of the rule in the training corpus. This way, rules that overgeneralise acquire a low weight. The weight W(r) of a rule r is computed according to its precision on the training set: W(r) = 4 # correct answers found # answers found Application: QA with Logical Graphs The above method has been applied to graphs representing the logical contents of sentences. There has been a long tradition on the use of graphs for this kind of sentence representation, such as Sowa’s Conceptual Graphs (Sowa, 1979), and Quillian’s Semantic Nets (Quillian, 1968). In our particular experiment we have used a graph representation that can be built automatically and that can be used efficiently for QA (Moll´a and van Zaanen, 2006). A Logical Graph (LG) is a directed, bipartite graph with two types of vertices, concepts and relations. 42 Concepts Examples of concepts are objects dog, table, events and states run, love, and properties red, quick. Relations Relations act as links between concepts. To facilitate the production of the LGs we have decided to use relation labels that represent verb argument positio"
W06-3807,A97-1011,0,0.368205,"Missing"
W08-1810,J05-1004,0,0.0216806,"sed as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strat"
W08-1810,U05-1016,1,0.836388,"back into the TREC corpus. As observed in Table 1, the SRL approach gives the best results for all question sets on all evaluation metrics, with the exception of c@50 on the 2006 question set. In most other retrieval sets the baseline performs worse than both QPLM and SRL, however for 2004 questions it performed better than QPLM on p@50 and r@50. It is interesting to observe that the QPLM results for the same year on c@50 are better than the BoW approach indicating that a larger amount of questions can potentially be answered by QPLM. • MetaQA System: Similar to the Aranea QA system, MetaQA (Pizzato and Molla, 2005) makes heavy use of redundancy and the information provided by Web Search Engines. However it goes a step further by combining different classes of Web Search engines (including Web Question Answering Systems) and assigning different confidence scores to each of the classes. 78 • AnswerFinder: Developed by Moll´a and Van Zaanen (2006), the AnswerFinder QA system unique feature is the use of QA graph rules learned automatically from a small training corpus. These graph rules are based on the maximum common subgraph between the deep syntactic representation of a question and a candidate answer s"
W08-1810,P98-1013,0,0.0977646,"mantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predica"
W08-1810,U07-1014,1,0.896877,"Missing"
W08-1810,P99-1071,0,0.0157455,"onfirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The methods were tested on four different QA systems and the results suggest that QPLM can be used as a good compromise between speed and accuracy. 1 Introduction Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecom"
W08-1810,W07-1206,0,0.0689156,"ans to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al., 2005), multidocument summarization (Barzilay et al., 1999) and machine translation (Quantz and Schmitz, 1994). Question Answering (QA) is one task that takes advantage of SRL, and in fact much of the research about the application of SRL to NLP is related to QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al., 2005) together with the semantic frames from FrameNet (Baker et al., 1998) to create an inference mechanism to improve QA. Kaisser and Webber (2007) apply semantic c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM 74 Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74–81 Manchester, UK. August 2008 mantic relations, we have developed an alternative annotation strategy based on word-to-word relations instead of noun phrase-to-predicate relations. We define semantic triples based on syntactic clues; this ap"
W08-1810,D07-1002,0,0.0715002,"Missing"
W08-1810,W05-0625,0,0.0420783,"e consistent results such that the improvements were due to additional correct answers and not to a larger but different set of correct answers. The AnswerFinder QA system showed a similar performance for both semantic-based strategies and both outperformed the BoW strategy. In this section we have shown an evaluation of different retrieval sets of documents using four distinct QA systems. We have observed that semantic strategies not only assist the retrieval of better documents, but also help in finding answers for questions when used with QA systems. Only recently we have been able to test Koomen et al. (2005) SRL tool. This SRL tool is the top ranking SRL tool at the CoNLL-2005 Shared Task Evaluation and it seems to be much faster than SwiRL. Preliminary tests suggest that it is able to perform the annotation of AQUAINT in almost one full year using a single computer; however, this tool, like SwiRL, is not very stable, crashing several times during the experiments. As further work, we plan to employ several computers and attempt to parse the whole AQUAINT corpus with this tool. It is important to point out that although the tool of Koomen et al. seems much faster than SwiRL, QPLM still outperforms"
W08-1810,W05-0635,0,0.176727,"uracy by providing an implicit query analyzer as well as more precise retrieval. Theoretically, the inclusion of this information at indexing time can also speed up the overall QA process since syntactic rephrasing or re-ranking of documents based on semantic roles would not be necessary. However, SRL techniques are still highly complex and they demand a computational power that is not yet available to most research groups when working with large corpora. In our experience the annotation of a 3GB corpus, such as the AQUAINT (Graff, 2002), using a semantic role labeler, for instance SwiRL from Surdeanu and Turmo (2005) can take more than one year using a standard PC configuration1 . In order to efficiently process a corpus with seSemantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it difficult to use at the indexing stage of the document retrieval component. In this paper we confirm the intuition that SRL at indexing stage improves the performance of QA and propose a simplified technique named the Question Prediction Language Model (QPLM), which provides similar information with a much lower cost. The me"
W08-1810,C04-1100,0,0.0276813,"Missing"
W08-1810,C98-1013,0,\N,Missing
W13-3405,U11-1002,1,0.846121,"Missing"
W13-3405,W08-0211,0,0.0595041,"Missing"
W16-4205,U13-1008,1,0.885225,"123], [5523] [1080], [8545] [5523], [3321], [6434] [8545], [3321], [6434], [6755] titioning. However, in order to determine a proper partitioning, optimizing a single cluster validity index is not always sufficient, especially in the situation when we deal with text documents having clusters of different shapes and sizes. The concept of multi-objective optimization (MOO) can be brought into consideration where we need to optimize several objective functions at the same time. The advantage of MOO is that we can generate clusters by optimizing several cluster validity indices. Inspired by this, Ekbal et al. (2013) proposed a MOO-based approach for clustering medical documents for EBM by using the search capability of a simulated annealing based approach, AMOSA (Archived MultiObjective Simulated Annealing based technique) (Bandyopadhyay et al., 2008). However, it has been shown that for some benchmark datasets, AMOSA performs slowly compared to a popular genetic algorithm based MOO technique, NSGA-II (Non-dominated Sorting Genetic Algorithm-II) (Bandyopadhyay et al., 2008). Therefore, an alternative MOO-based approach is needed in order to verify whether we can improve the run-time complexity of AMOSA."
W16-4205,U11-1012,1,0.896402,"Missing"
W17-2308,D14-1181,0,0.00280637,"aining the word embeddings, we experimented with the following approaches to produce the sentence vectors: CNN: Convolutional Neural Nets (CNN) were originally developed for image processing, for tasks where the important information may appear on arbitrary fragments of the image (Fukushima, 1980). By applying a convolutional layer, the image is scanned for salient information. When the convolutional layer is followed by a maxpool layer, the most salient information is kept for further processing. We follow the usual approach for the application of CNN for word sequences, e.g. as described by Kim (2014). In particular, the embeddings of the words in a sentence (or question) are arranged in a matrix where each row represents a word embedding. Then, a set of convolutional filters are applied. Each convolutional filter uses a window of width the total number of columns (that is, the entire word embedding). Each convolutional filter has a fixed height, ranging from 2 to 4 rows in our experiments. These filters aim to capture salient ngrams. The convolutional filters are then followed by a maxpool layer. Our final sentence embedding concatenates the output of 32 different convolutional filters, e"
W17-2308,W14-1504,0,\N,Missing
W18-5303,W04-1013,0,0.0300458,"and Python implementations, but in general the mean between ROUGE-2 and ROUGE-L was a better approximation of the Perl implementation of ROUGE-SU4. In general, we observed lower results of the Python versions of ROUGE-L and ROUGE-2 compared with the Perl versions. On the light of this, we strongly advise always to specify the implementation of ROUGE being used, since the results produced by different versions may not be comparable. p = 0.2 × 3000/(3000 + episode) 3.1 Kernel ROUGE Variants The evaluation scripts of the BioASQ task used the original Perl implementation of ROUGE-2 and ROUGE-SU4 (Lin, 2004). Our experiments aimed to use ROUGE-SU4. Whereas the Perl implementation of ROUGE was used for the deep learning experiments described in section 2, as an implementation decision we used Python’s pyrouge library for the reinforcement learning experiments. Python’s pyrouge provides ROUGE-1, ROUGE-2 and ROUGE-L, but not ROUGE-SU4. Figures 4 and 5 compare the ROUGE F1 scores of the Python libraries against the ROUGE-SU4 F1 score of the Perl implementation. Figure 4 4 Settings The snippets were split into sentences using NLTK’s sentence tokeniser. The tf.idf information used for the baselines was"
W18-5303,W17-2308,1,0.418532,"Missing"
W18-5303,U17-1012,0,0.0367806,"Missing"
W18-5604,W97-0703,0,0.102287,"-of-words (BOW) approaches. BOW models including word frequency and tf-idf are the most frequently used methods to discover the important content (Wu et al., 2008). More recently, word embeddings generated by deep learning approaches have also been shown to be useful for text summarisation (Malakasiotis et al., 2015; Moll´a, 2017). In recent years, the main focus of research in the summarisation field has been directed towards the application of machine learning to generate better summaries. Popular features such as multiple words, noun phrases, main verbs, named entities and word embeddings (Barzilay and Elhadad, 1997; Filatova and Hatzivassiloglou, 2004; Harabagiu and Lacatusu, 2002; Malakasiotis et al., 2015; Moll´a, 2017) have been heavily exploited for summarisation. In contrast to other domains, research on automatic text processing in the medical domain is still very much in its infancy. In the recent past, there has been steady ongoing research in biomedical text processing (Zweigenbaum et al., 2007). Factors such as the requirement of large Regression: Regression approaches for summarisation try to fit the predicted score of a sentence as close as possible to the target score instead of labelling t"
W18-5604,W04-1017,0,0.01656,"BOW models including word frequency and tf-idf are the most frequently used methods to discover the important content (Wu et al., 2008). More recently, word embeddings generated by deep learning approaches have also been shown to be useful for text summarisation (Malakasiotis et al., 2015; Moll´a, 2017). In recent years, the main focus of research in the summarisation field has been directed towards the application of machine learning to generate better summaries. Popular features such as multiple words, noun phrases, main verbs, named entities and word embeddings (Barzilay and Elhadad, 1997; Filatova and Hatzivassiloglou, 2004; Harabagiu and Lacatusu, 2002; Malakasiotis et al., 2015; Moll´a, 2017) have been heavily exploited for summarisation. In contrast to other domains, research on automatic text processing in the medical domain is still very much in its infancy. In the recent past, there has been steady ongoing research in biomedical text processing (Zweigenbaum et al., 2007). Factors such as the requirement of large Regression: Regression approaches for summarisation try to fit the predicted score of a sentence as close as possible to the target score instead of labelling the sentences. An early work using reg"
W18-5604,C12-1056,0,0.122016,"In the recent past, there has been steady ongoing research in biomedical text processing (Zweigenbaum et al., 2007). Factors such as the requirement of large Regression: Regression approaches for summarisation try to fit the predicted score of a sentence as close as possible to the target score instead of labelling the sentences. An early work using regression for summarisation is by Ouyang et al. (2011) using support vector regression (SVR). Support vector regression (SVR) has also been used in conjunction with other techniques like integer linear programming (ILP) for generating summaries (Galanis et al., 2012) and has achieved state-of-the-art results in comparison to other competitive extractive summarisers. A system named FastSum (Schilder and Kon30 Query: Name synonym of Acrokeratosis paraneoplastica. dadadi, 2008) used regression SVM for training their data set by using the least computationally expensive NLP techniques to generate the summary. The system used a set of clusters as input data and simple pre-processing was performed on the sentences. A comparison of this system with MEAD (Radev et al., 2000) showed that it is more than 4 times faster than MEAD. Some of the recent work on biomedic"
W18-5604,W04-1013,0,0.0724811,"the tf-idf vector of the candidate sentence. Since the intent of this work is to compare the performance of regression and classification approaches, and not to obtain the best possible results, the feature set used is fairly simple and is commonly used on the most popular supervised approaches for query-based extractive summarisation. For the regression approaches, each sentence of the training data is annotated with the F1 ROUGESU4 score of the sentence compared to the target summary. ROUGE-SU4 considers skip bigrams with a maximum distance of 4 words between the words of each skip bigram (Lin, 2004). This measure has also been found to correlate well with human judgements in extractive summarisation. Other systems have used ROUGE for annotating data and its application has been proved useful, e.g the system by Galanis et al. (2012); Peyrard and Eckle-Kohler (2016). We use Support Vector Regression (SVR), which has performed well in past regression approaches to summarisation. For the classification approaches, we use the standard two-class labelling approach where class 1 indicates sentences that are selected for the final summary, and class 0 indicates sentences that are not selected. W"
W18-5604,W17-2308,1,0.850364,"Missing"
W18-5604,P16-1172,0,0.0175312,"used on the most popular supervised approaches for query-based extractive summarisation. For the regression approaches, each sentence of the training data is annotated with the F1 ROUGESU4 score of the sentence compared to the target summary. ROUGE-SU4 considers skip bigrams with a maximum distance of 4 words between the words of each skip bigram (Lin, 2004). This measure has also been found to correlate well with human judgements in extractive summarisation. Other systems have used ROUGE for annotating data and its application has been proved useful, e.g the system by Galanis et al. (2012); Peyrard and Eckle-Kohler (2016). We use Support Vector Regression (SVR), which has performed well in past regression approaches to summarisation. For the classification approaches, we use the standard two-class labelling approach where class 1 indicates sentences that are selected for the final summary, and class 0 indicates sentences that are not selected. We use Support Vector Machine (SVM), which has performed well in many other classification problems. Figure 2: The overall summarisation model. is trained with data from the BioASQ 5b Challenge. We follow a three-stage summarisation model for the generation of the summar"
W18-5604,P08-2052,0,0.0783615,"Missing"
