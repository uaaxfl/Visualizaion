2020.coling-main.411,P17-1183,0,0.0217803,"at the current position, as well as the embedding of the previous tag is fed to GRUdec . The output of GRUdec is fed to a softmax layer to generate the output tagging sequence. We model P (Y |X) with an encoder-decoder RNN. The encoder generates hidden state sequences of length I. At decoding step i, the decoder attends to position i in both the hidden state sequences and the input embedding sequence. Figure 1 illustrates the architecture of the model. Our architecture is a modification of the stacking LSTM architecture of Ma et al. (2018) which can also be understood as using hard attention (Aharoni and Goldberg, 2017) in the decoder. We do not use bigram character features. Instead, we rely on forward and backward RNNs for implicit input feature extraction. We make the decoder auto-regressive by feeding the previously predicted tag to the decoder RNN and applying beam search in inference. 3.1 Pre-training RNN models demand a large number of training examples. While labelled data is often scarce, un-labelled data with matching domain is often abundant. Pre-training the encoder component of a sequence-tosequence model for a different task such as language modelling has been extremely successful, as manifeste"
2020.coling-main.411,N19-1423,0,0.0119081,"Missing"
2020.coling-main.411,D18-1295,0,0.0147265,"(2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequen"
2020.coling-main.411,W16-2012,0,0.0158912,"r FSTs following the work of Goldwater et al. (2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We"
2020.coling-main.411,D18-1529,0,0.102267,"sk of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequence tagging model. Given an input character sequence X = x1 , . . . , xI , the model predicts an output tag sequence Y = y1 , . . . , yI , with yi ∈ {B, I} being the tag for the character xi . B indicates the underlying character starts a new segment. I indicates the underlying character continues t"
2020.coling-main.411,P11-1140,0,0.0097515,"models as features. Fine-tuning on in-domain data is the counterpart in our system. Both have created training or evaluation data sets of URL domain names for their experiments, but these have not been publicly released. We contribute a data set of URLs crawled from a public repository of Web texts with their internal segments annotated by crowd sourcing. Chiang et al. (2010) reported experiments on a related artificial problem of splitting of space-free English through Bayesian inference for FSTs following the work of Goldwater et al. (2009).1 A closely related problem is compound splitting. Macherey et al. (2011) presented an unsupervised probabilistic model for splitting compound words into parts, with the compound part sub-model being a zero-order model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these"
2020.coling-main.411,C04-1081,0,0.0715852,"er model to enable efficient dynamic programming inference. The model is optimized for the task of machine translation. They only reported results for seven (Germanic, Hellenic, and Uralic) languages other than English. More recently, fully supervised letter sequence labelling models have been introduced for German compound splitting (Ma et al., 2016) and Sanskrit word splitting (Hellwig and Nehrdich, 2018). Pre-training can potentially further improve these models. There is a large body of research on Chinese word segmentation. The best models are supervised ones using structured prediction (Peng et al., 2004), transition-based models (Zhang and Clark, 2007), and most recently RNNs (Ma et al., 2018) and BERT-based models (Huang et al., 2019). The superior results of the BERT-based models demonstrate that pre-training is effective on word segmentation. Unlike BERT, we pre-train the entire model, not just the encoder. 3 RNN Tagging Model We formulate the segmentation problem as a character sequence tagging model. Given an input character sequence X = x1 , . . . , xI , the model predicts an output tag sequence Y = y1 , . . . , yI , with yi ∈ {B, I} being the tag for the character xi . B indicates the"
2020.coling-main.411,P17-1161,0,0.0306123,"Missing"
2020.sigtyp-1.3,N09-1067,0,0.0587225,"Missing"
2020.sigtyp-1.3,P07-1009,0,0.767979,"Missing"
2020.sigtyp-1.3,N19-1156,0,0.0612793,"llyinspired objec tive on WALS data with missing values imputed us ing a regularized iterative variant of Multiple Cor respondence Analysis (MCA) (Josse and Husson, 2012; Josse et al., 2012). In later work, Murawaki (2017, 2019) and Murawaki and Yamauchi (2018) abandoned an earlier model in favor of a Bayesian autologistic approach and demonstrated the superi ority of Bayesian predictor over MCA. In this ap proach, the languages are represented as random variables that are explained in terms of other lan guages related to each other through phylogenetic and spatial neighborhood graphs. Bjerva et al. (2019) introduce a generative model inspired by the Chomskyan principlesandparameters frame work, drawing on the correlations between typo logical features of languages to tackle the novel task of typological collaborative filtering, a con cept borrowed from the area of recommender sys tems. 3 Method Here we outline the details of our approach used to generate the final submission. The opensource implementation of our training and evaluation pipeline has been released in public domain.1 3.1 Precomputation of Features Typological features were preprocessed to find likely associations between g"
2020.sigtyp-1.3,2020.sigtyp-1.1,0,0.0348564,"plications to capture correlations between other types of typo logical features as well. We describe how these features were derived in detail in Section 3. As we report below in Section 4, the per formance of machine learning algorithms varied across different feature predictions, with some al gorithms working better for some features, and less well for others. On balance however we found that ridge regression (Hoerl and Kennard, 1970), also known as Tikhonov regularization (Franklin, 1974), was the most useful approach. This paper describes the NEMO submission to SIGTYP 2020 shared task (Bjerva et al., 2020) which deals with prediction of linguis tic typological features for multiple languages using the data derived from World Atlas of Language Structures (WALS). We employ fre quentist inference to represent correlations be tween typological features and use this repre sentation to train simple multiclass estimators that predict individual features. We describe two submitted ridge regressionbased configu rations which ranked second and third overall in the constrained task. Our best configuration achieved the microaveraged accuracy score of 0.66 on 149 test languages. 1 Introduction The r"
2020.sigtyp-1.3,P12-1066,0,0.366259,"Missing"
2020.sigtyp-1.3,J19-3005,0,0.0473047,"Missing"
2020.sigtyp-1.3,N15-1036,0,0.01946,"e is a special inter est in the geographic aspect of the modeling, for our final submission we limited ourselves to the rather orthodox approach of representing language associations through fixed neighborhoods, in or der not to overcomplicate our method (described in the next section) that can be difficult to rec oncile with the more sophisticated models, such as the very promising model of language evolu tion from Kauhanen et al. (2019) and the findings emerging from the fields of dialectology and di alectometry (Szmrecsanyi, 2011; Wieling and Ner bonne, 2015; Nerbonne et al., 2020). Murawaki (2015) proposed a deep learning ap proach to phylogenetic inference by mapping the language vectors to a latent space using an auto encoder trained using typologicallyinspired objec tive on WALS data with missing values imputed us ing a regularized iterative variant of Multiple Cor respondence Analysis (MCA) (Josse and Husson, 2012; Josse et al., 2012). In later work, Murawaki (2017, 2019) and Murawaki and Yamauchi (2018) abandoned an earlier model in favor of a Bayesian autologistic approach and demonstrated the superi ority of Bayesian predictor over MCA. In this ap proach, the languages a"
2020.sigtyp-1.3,I17-1046,0,0.343783,"ing model of language evolu tion from Kauhanen et al. (2019) and the findings emerging from the fields of dialectology and di alectometry (Szmrecsanyi, 2011; Wieling and Ner bonne, 2015; Nerbonne et al., 2020). Murawaki (2015) proposed a deep learning ap proach to phylogenetic inference by mapping the language vectors to a latent space using an auto encoder trained using typologicallyinspired objec tive on WALS data with missing values imputed us ing a regularized iterative variant of Multiple Cor respondence Analysis (MCA) (Josse and Husson, 2012; Josse et al., 2012). In later work, Murawaki (2017, 2019) and Murawaki and Yamauchi (2018) abandoned an earlier model in favor of a Bayesian autologistic approach and demonstrated the superi ority of Bayesian predictor over MCA. In this ap proach, the languages are represented as random variables that are explained in terms of other lan guages related to each other through phylogenetic and spatial neighborhood graphs. Bjerva et al. (2019) introduce a generative model inspired by the Chomskyan principlesandparameters frame work, drawing on the correlations between typo logical features of languages to tackle the novel task of typologica"
2020.sigtyp-1.3,L16-1011,0,0.0180145,"rop erties of the language. For each typological feature ? and value ? from a set of its values ?? we com puted the following probability estimates: While most stateoftheart missing feature im putation methods are modelbased, recently Buis and Hulden (2019) employed the iterative tech nique based on singular value decomposition (SVD) from the wellstudied area of lowrank ma trix completion and reported the performance on par with the prior art. Although lacking explana tory power, similarly to MCA, such techniques are attractive due to their simplicity and computational efficiency. Takamura et al. (2016) took a stan dard machine learning approach by training multi nomial logistic regression classifiers for individ ual WALS features based on other features present in the database under various experimental condi tions, hypothesizing that the classifier would cap ture feature correlations implicit in the data. ?genus (?|?) = countgenus (?) countgenus (?) (1) ?family (?|?) = countfamily (?) countfamily (?) (2) ?area (?|?) = countarea (?) , countarea (?) (3) where count(?) = ∑? ∈? count(?? ). Here, “fam ? ? ily” and “genus” in equations 1 and 2 were as given in the data, and “area” in equat"
2020.sigtyp-1.3,J19-2001,0,0.0427497,"Missing"
2021.findings-emnlp.85,P12-3006,0,0.0232849,"roat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-availa"
2021.findings-emnlp.85,W15-4319,0,0.0448684,"Missing"
2021.findings-emnlp.85,P10-1079,0,0.0430969,"r text-tospeech synthesis (TTS); Sproat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware"
2021.findings-emnlp.85,P00-1037,0,0.341039,"l text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence of ei (in the case that it is an abbreviation of ei ). At inference time, the system is presented with an abbreviated A sequence of length n and is asked to propose a single hypothˆ esis expansion of lengt"
2021.findings-emnlp.85,P14-2111,0,0.0208137,"omies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthet"
2021.findings-emnlp.85,W09-2010,0,0.034024,"ately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence"
2021.findings-emnlp.85,2020.lrec-1.773,0,0.061171,"Missing"
2021.findings-emnlp.85,N13-1037,0,0.21779,"combines an abbreviation model, applied independently to each word in the sentence, and a language model enforcing ﬂuency and local coherence in the expansion. Text normalization refers to transformations used to prepare text for downstream processing. Originally, this term was reserved for transformations mapping between “written” and “spoken” forms required by technologies like speech recognition and speech synthesis (Sproat et al., 2001), but it is now used for many other transformations, including normalizing informal text genres found on mobile messaging and social media platforms (e.g., Eisenstein, 2013; van der Goot, 2019). 1.1 Contributions Spans of text may require different kinds of The contributions of this study are three-fold. First, normalization depending on their semiotic class (Taylor, 2009) and the requirements of the down- we describe a large data set for English abbrevistream application. For example, cardinal num- ation expansion made freely available to the rebers such as 123 need to be normalized to a spo- search community. Secondly, we validate this data set using exploratory data analysis to identify ken form (one hundred twenty three) for speech common abbreviation strate"
2021.findings-emnlp.85,P19-3032,0,0.0254298,"Missing"
2021.findings-emnlp.85,P11-1038,0,0.0532032,"at (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, var"
2021.findings-emnlp.85,W17-4002,1,0.845388,"ed by An example of the resulting lattice is shown in Figa type of weighted ﬁnite-state transducer known ure 3. λ is an unweighted transducer in which each variously as a joint multigram model, pair n-gram path maps an in-vocabulary word, encoded as a model, or pair language model (pair LM). Such 4 models have been used for grapheme-to-phoneme Computing the conditional probability P(ai |ei ) in eq. 3 conversion (Bisani and Ney, 2008; Novak et al., from the joint probability P(ai , ei ) requires a computationally expensive summation over all possible alignments. However 2016), transliteration (Hellsten et al., 2017; Mer- we ﬁnd it can be effectively approximated using the most probable alignment according to the joint probability model. hav and Ash, 2018), and abbreviation expansion 5 We assume the reader is familiar with ﬁnite-state au(Roark and Sproat, 2014) among other tasks. tomata and algorithms such as composition, concatenation, A pair LM α is a joint model over input/output projection, and shortest path. See Mohri 2009 for a review strings P(ai , ei ) where ai is an abbreviation and ei of ﬁnite-state automata and these algorithms. 999 character sequence, to that same word encoded as a single sym"
2021.findings-emnlp.85,P82-1020,0,0.739654,"Missing"
2021.findings-emnlp.85,N09-1069,0,0.0626793,"occurs as an expansion of ai in proximate back-offs. It is then shrunk using relathe training set. tive entropy pruning (Stolcke, 1998). 1000 Figure 3: An example η lattice corresponding to the example sentence in Figure 1. Ellipses indicate that arcs have been pruned for reasons of space. Non-zero costs are indicated by negative log probability arc weights. Abbreviation model The abbreviation model is a pair n-gram language model over input/output character pairs, encoded as a weighted transducer. The OpenGrm-BaumWelch toolkit and a stepwise interpolated variant of expectation maximization (Liang and Klein, 2009) are used to compute alignments between abbreviations and expansions. OpenGrm-NGram is then used to train a 4-gram pair LM. As with the expansion model, KneserNey smoothing with ε-arc back-offs are used, but no shrinking is performed.6 5.2 Neural implementation Expansion model The expansion model consists of an embedding layer of dimensionality 512 and two LSTM (Hochreiter and Schmidhuber, 1997) layers, each with 512 hidden units. Each sentence is padded with reserved start and end symbols. The model, implemented in TensorFlow (Abadi et al., 2016), is trained in batches of 256 until convergenc"
2021.findings-emnlp.85,P11-2013,0,0.0365892,"sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers, generative and neural language models, and even machine translation systems. In this work we focus on supervised models, though unsupervised approaches have also been proposed (e.g., Cook and Stevenson, 2009; Liu et al., 2011; Yang and Eisenstein, 2013). The noisy channel paradigm we use to build baseline models here is inspired by earlier models for contextual spelling correction (e.g., Brill and Moore, 2000). 1.3 Task deﬁnition We assume the following task deﬁnition. Let A = [a0 , a1 , . . . , an ] be a sequence of possiblyabbreviated words and let E be a sequence of expanded words [e0 , e1 , . . . , en ], both of length n. If ei is an element of E, then the corresponding element of A, ai , must either be identical to ei (in the case that it is not abbreviated), or a proper, nonnull subsequence of ei (in the cas"
2021.findings-emnlp.85,C18-1053,0,0.0410728,"Missing"
2021.findings-emnlp.85,P14-2060,1,0.90379,"son rsn i i went went to to the the store str was ws to to buy buy milk mlk and and bread brd . . Figure 1: Example of paired data for this task; above: expanded sequence; below: abbreviated sequence. Note that the data has been case-folded. 1.2 Related work Text normalization was ﬁrst studied for text-tospeech synthesis (TTS); Sproat et al. (2001) and van Esch and Sproat (2017) provide taxonomies of semiotic classes important for speech applications. Normalization for this application remains a topic of active research (e.g., Ebden and Sproat, 2015; Ritchie et al., 2019; Zhang et al., 2019). Roark and Sproat (2014) focus on abbreviation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and B"
2021.findings-emnlp.85,P12-3011,1,0.579742,"h 2.7m additional sentences from Wikipedia as described in subsection 2.1. The development set was used to tune the Markov order of the ﬁnite-state components, and to ablate the subsequence model heuristics. Final evaluations are conducted on the test set. The full vocabulary consists of all 75k wordtypes appearing in the language model training set, simulating a general-domain normalization task. 5.1 Finite-state implementation Expansion model The expansion model is a conventional language model over expansion tokens. • LexBlock: If ai is in-vocabulary, set the probThe OpenGrm-NGram toolkit (Roark et al., 2012) ability of all other output candidates to zero. is used to build a trigram model with Kneser-Ney • Memory: Do not prune an expansion candi- smoothing (Ney et al., 1994) and ε-arcs used to apdate ei if it is occurs as an expansion of ai in proximate back-offs. It is then shrunk using relathe training set. tive entropy pruning (Stolcke, 1998). 1000 Figure 3: An example η lattice corresponding to the example sentence in Figure 1. Ellipses indicate that arcs have been pruned for reasons of space. Non-zero costs are indicated by negative log probability arc weights. Abbreviation model The abbrevia"
2021.findings-emnlp.85,D13-1007,0,0.0234033,"viation expansion and enforce a high-precision operating point, since, for TTS, incorrect expansions are judged more costly than leaving novel abbreviations unexpanded. Abbreviation expansion has also been studied using data from SMS (Choudhury et al., 2007; Beaufort et al., 2010), chatrooms (Aw and Lee, 2012), and social media platforms such as Twitter (Chrupała, 2014; Baldwin et al., 2015). Most of the prior studies use small, manually curated databases in which “ground truth” labels were generated by asking annotators to expand the abbreviations using local context. Han and Baldwin (2011), Yang and Eisenstein (2013), and the organizers of the W-NUT 2015 shared task (Baldwin et al., 2015) all released data sets containing Englishlanguage Tweets annotated with expansions for abbreviations. Unfortunately, none of these data sets are presently available.1 We are unaware of any large, publicly-available data set for abbreviation expansion, excluding a synthetic, automaticallygenerated data set for informal text normalization (Dekker and van der Goot, 2020). A wide variety of machine learning techniques have been applied to abbreviation expansion, including hidden Markov models, various taggers and classiﬁers,"
2021.findings-emnlp.85,J19-2004,1,0.885334,"is related to, but distinct from, spelling correction, in that ad hoc abbreviations are intentional and may involve substantial differences from the original words. Ad hoc abbreviations are productively generated on-the-ﬂy, so they cannot be resolved solely by dictionary lookup. We generate a large, open-source data set of ad hoc abbreviations. This data is used to study abbreviation strategies and to develop two strong baselines for abbreviation expansion. 1 Introduction and Sproat, 2015), possibly augmented with machine learning systems for contextual disambiguation (e.g., Ng et al., 2017; Zhang et al., 2019). In this study we are interested in a different subclass of abbreviations, those which are neither frequent nor conventionalized. We refer to these as ad hoc abbreviations. Such abbreviations are particularly common on those communication channels which demand or favor brevity, such as mobile messaging and social media platforms (Crystal, 2001, 2008; McCulloch, 2019). Unlike conventionalized abbreviations, ad hoc abbreviations are an open class, generated on-the-ﬂy. Unfortunately, there is little annotated data available to study ad hoc abbreviations as they occur in natural text. To remedy t"
2021.findings-emnlp.85,L18-1295,0,0.0216196,"n,8 or to a function word with comparable syntactic effect. (4) they {recog, 3 recognized, 7 recognize} accomps by musicians frm th prev yr . (5) {th, 3 the, 7 this} behavr s strengthnd by an automatc reinfrcng consequenc . 7 Ethical concerns The proposed technology is intended as a component of other speech and language processing systems. We note that abbreviation expansion systems have some small potential for abuse beyond those of the larger systems they might be integrated into. For instance, this technology could be used to (2) anothr criticism is abt th absenc o a stndrd 8 We note that Żelasko (2018) considers the problem of {auditin, 3 auditing, 7 audition} procedr disambiguating abbreviations in Polish, a language with far . richer inﬂectional morphology. 1002 defeat abbreviation as a strategy for circumventing algorithmic state censorship. The data is drawn from English Wikipedia text and was produced by a team of professional annotators based in the United States; its use to disambiguate abbreviations generated by other Englishspeaking communities would likely introduce bias. 8 Conclusions We introduce a large, freely-available data set for ad hoc abbreviation expansion, describing th"
C02-2026,P97-1003,0,0.0133068,"antics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain lexemes because t"
C02-2026,C94-1079,0,0.0338234,"pecific Semantics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain l"
C02-2026,W02-2214,1,0.870509,"Missing"
C02-2026,J00-1003,0,0.0131956,"beled by the name of an elementary tree machine by the lexicalized version of that tree machine. Of course, in each iteration, there are many more replacements than in the previous iteration. We use 5 rounds of iteration; obviously, the number of iterations restrict the syntactic complexity (but not the length) of recognized input. However, because we output brackets in the FSTs, we obtain a parse with full syntactic/lexical semantic (i.e., dependency) structure, not a “shallow parse”. This construction is in many ways similar to similar constructions proposed for CFGs, in particular that of (Nederhof, 2000). One difference is that, since we start from TAG, recursion is already factored, and we need not find cycles in the rules of the grammar. 5 Experimental Results We present results in which our classes are defined entirely with respect to syntactic behavior. This is because we do not have available an important corpus annotated with semantics. We train on the Wall Street Journal (WSJ) corpus. We evaluate by taking a list of 205 sentences which are chosen at random from entries to W ORDS E YE made by the developers (who were testing the graphical component using a different parser). Their avera"
C12-1042,N09-2047,0,0.0129862,"s section we discuss how we prepared a set of approximately 1000 core verbs and their arguments to serve as input for the action vignette annotation process. These verbs were manually chosen, using subjective judgements, to include verbs that are commonly used as well as those that are concrete in nature and hence could be readily depicted. We also specified relative priorities to further guide our annotation efforts. Some of these verbs are shown in Figure 6. The list of actual verb phrases to annotate was obtained from the British National Corpus (BNC), which we parsed with the MICA parser (Bangalore et al., 2009). An AWK script was written to extract verbs from the output along with particles, direct and indirect objects, and prepositional phrases which convey salient information about the action. These verb-argument tuples provided the skeletal sentences and valence patterns representing the typical ways these verbs would be used. It is these verb-argument tuples that would then be annotated. 3.1 Parsing MICA (Bangalore et al., 2009) is a dependency parser that uses tree insertion grammars and supertagging. A supertag is an elementary tree of a tree grammar, associated with a lexical item. The tree i"
C12-1042,W11-0905,1,0.929121,"hniques. Natural language offers an interface that is intuitive and immediately accessible to anyone, without requiring any special skill or training. The WordsEye system (Coyne and Sproat, 2001) lets users create 3D scenes by describing them in language. It has been used by several thousand users to create over 10,000 scenes by merely describing them. We have tested WordsEye as an educational tool with rising 5th grade children in a summer enrichment program where it was found to significantly improve literacy skills over the students who had taken the more traditional version of the course (Coyne et al., 2011b). As one of the students said “When you read a book, you don’t get any pictures. WordsEye helps you create your own pictures, so you can picture in your mind what happens in the story.” The students were also introduced to WordsEye’s face and emotion manipulation capabilities – the children loved including themselves and other people in scenes and modifying the facial expressions. We are currently experimenting with automatically depicting Twitter tweets as a way to bring text-to-scene visualization to a wider audience and to test the limits of the system in an open domain with ill-formed te"
C12-1042,W11-0147,1,0.836327,"bjects. One of our main tasks is to define vignettes to represent a wide variety of actions and locations. Previous work explored different methods for building location vignettes. Sproat (2001) attempted to extract associations between actions and locations from text corpora. While producing interesting associations, the extracted data was fairly noisy and required hand 681 editing. Furthermore, much of the information that we are looking for is common-sense knowledge that is taken for granted by human beings and is not explicitly stated in corpora. In other work Rouhizadeh et al. (2010) and Rouhizadeh et al. (2011b), Amazon Mechanical Turk (AMT) was used to collect information about locations of different objects, their parts, and surrounding objects. Various corpus association and WordNet similarity measures were applied to filter the undesirable AMT inputs.Reasonably clean data was achieved by this approach. In Rouhizadeh et al. (2011c) AMT was used for building a low-level description corpus for locations by collecting free-form text descriptions of room locations based on their pictures. The WordsEye NLP module was used to extract location elements from processed descriptions. Objects and other ele"
C18-1123,P17-1183,0,0.122352,"comes with a high computational cost. The time complexity of the attention mechanism is O(N 2 ) if the number of decoding steps is proportional to the input length N . In addition to the computational concern, for dominantly-monotonic translation tasks, such as text normalization (Sproat and Jaitly, 2017), soft attention can be too relaxed and sub-optimal in terms of modeling efficiency. Hence, to reduce computational complexity as well as to improve model accuracy, recently there has been a surge of research interest in enforcing monotonic attention (Raffel et al., 2017) and hard attention (Aharoni and Goldberg, 2017) based on the observation that many sequence-to-sequence tasks are monotonic, including speech recognition and morphological inflection. In reality, the monotonicity assumption has to be made carefully. Even in the highly monotonic translation task of text normalization, which is mapping written text to spoken text, there are systematic reordering patterns like from “2018-10-03” to “October third, two thousand eighteen”, and from “$100” to “one hundred dollars”. These reordering patterns can involve arbitrarily long chunks of input. Without handling the infrequent but systematic reordering, mo"
C18-1123,P17-1177,0,0.025046,"with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action"
C18-1123,P05-1066,0,0.254977,"Missing"
C18-1123,D11-1018,0,0.0259011,"1/S X log p(P |S) + log p(T |P (S)) (2) (T,S)∈S and the decoding objective is Tˆ = arg max p(P |S)·p(T |P (S)) T,P (3) The term − log p(P |S) is the reordering loss and the term − log p(T |P (S)) is the translation loss. Both share the input S. In a computation graph, they share the BiRNN component. The hidden states (h1 , . . . , hTx ) are trained for both tasks. At decoding time, we approximate the decoding objective with a beam search over P , followed by another beam search over T . 1458 5 Related Work There are three papers that are most relevant to ours that relate to ITG pre-ordering. DeNero and Uszkoreit (2011) induce binary source trees first and learn pre-reordering rules for these binary trees from parallel data. Neubig et al. (2012) discriminatively train an ITG parser with CYK parsing for pre-reordering, essentially combining the two steps in DeNero and Uszkoreit (2011) into one. Nakagawa (2015) improve upon Neubig et al. (2012) with a linear time top-down ITG parsing algorithm. They all rely on feature engineering as they use linear models for training. None of them does transition-based parsing for ITG. There are two papers most relevant to ours that relate to combining transition systems wit"
C18-1123,P15-1033,0,0.0256749,"binary source trees first and learn pre-reordering rules for these binary trees from parallel data. Neubig et al. (2012) discriminatively train an ITG parser with CYK parsing for pre-reordering, essentially combining the two steps in DeNero and Uszkoreit (2011) into one. Nakagawa (2015) improve upon Neubig et al. (2012) with a linear time top-down ITG parsing algorithm. They all rely on feature engineering as they use linear models for training. None of them does transition-based parsing for ITG. There are two papers most relevant to ours that relate to combining transition systems with RNNs. Dyer et al. (2015) propose a stack-LSTM for transition-based parsing. The difference with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation,"
C18-1123,P16-1078,0,0.0264784,"ed parsing. The difference with our approach to modeling is that we do not encode the entire stack explicitly. At each time step, we only feed the context indexed by the current stack and buffer configuration. We do not have an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence an"
C18-1123,P17-2012,0,0.0205351,"rporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering layer into neural MT models has also been studied by Huang et al. (2018). They use a simple feed-forward soft and local reordering layer similar to the soft attention mechanism. A fixed window size is used for local reordering. Our RNN reordering layer can handle long distance r"
C18-1123,D18-1549,0,0.0230176,"l machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering layer into neural MT models has also been studied by Huang et al. (2018). They use a simple feed-forward soft and local reordering layer similar to the soft attention mechanism. A fixed window size is used for local reordering. Our RNN reordering layer can handle long distance reordering. Another important difference is that we use discrete variables (permutations) for reordering while the soft reordering mechanism has no latent variables. We leave it as future work to train the end-to-end system by treating ITG transitions and permutations as latent variables. 6 Experiments 6.1 Reordering Experiments In this part, we analyze the effectiveness of the ITG RNN reord"
C18-1123,D13-1049,0,0.022378,"rough swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, for example $100 with (1, 2"
C18-1123,P15-1021,0,0.0788692,"t source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, for example $100 with (1, 2, 3, 0) to indica"
C18-1123,D12-1077,0,0.124436,"tematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with their permutations, fo"
C18-1123,W03-3017,0,0.124468,"i X → w0 /w0 ... X → wn−1 /wn−1 The first rule is called the straight rule because it keeps the order of two constituents unchanged on the target side. The second rule is called the inverted rule because it inverts the order of two constituents on the target side. For example, a sentence pair (w0 , w1 , w2 |w0 , w2 , w1 ) can be derived with three pre-terminal rules to link the words with the same subscripts on both sides, plus one inverted rule for grouping w1 and w2 and one straight rule on the top for grouping w0 and the phrase of w1 , w2 . We can also devise a transition system following (Nivre, 2003). The following is the deductive description of the transition system. Each configuration in the transition system consists of a stack and a pointer to the next word in the input buffer. At the beginning, we have an empty stack and a pointer to the first input word. At each time step, we can choose S HIFT if the buffer is not empty and choose to apply R EDUCE S or R EDUCE I if the stack has a height of at least two. We use four indices to uniquely represent each synchronous constituent that is constructed in the parsing process. The first two index into the source side and the last two index i"
C18-1123,D15-1044,0,0.046712,"eature engineering. In experiments, we apply the model to the task of text normalization. Compared to a strong baseline of attention-based RNN, our ITG RNN reordering model can reach the same reordering accuracy with only 1/10 of the training data and is 2.5x faster in decoding. 1 Introduction The encoder-decoder neural network architecture for sequence-to-sequence problems has achieved enormous success especially after the introduction of the attention mechanism (Bahdanau et al., 2014). Its applications in NLP range from machine translation (Bahdanau et al., 2014) and sentence summarization (Rush et al., 2015) to text normalization (Sproat and Jaitly, 2017). The attention mechanism is effectively a random memory access mechanism, enabling access to any source sequence position at any decoding step. In principle, it can handle arbitrary reordering of any input length. But its power comes with a high computational cost. The time complexity of the attention mechanism is O(N 2 ) if the number of decoding steps is proportional to the input length N . In addition to the computational concern, for dominantly-monotonic translation tasks, such as text normalization (Sproat and Jaitly, 2017), soft attention"
C18-1123,P16-2049,0,0.026732,"an stack RNN, which can be expensive. Our transition RNN can be viewed as a special case of DRAGNN (Kong et al., 2017) which combines fixed features as input with recurrence links at each time step. Our recurrence link is only to the previous time step. For the approach of incorporating syntactic constraints into neural translation, the following papers are most relevant. Eriguchi et al. (2016) and Chen et al. (2017) assume the existence of source parse trees and enhance the encoder and the attention mechanism to attend to both words and syntactic phrases. We do not rely on external parsers. Stahlberg et al. (2016) let a hierarchical phrase-based decoder guide neural machine translation decoding. Reordering decisions can only be indirectly influenced by the hierarchical decoder. In contrast, we have an explicit hierarchical reordering model applied pre-translation. Eriguchi et al. (2017) train a joint parsing and translation model to maximize the log likelihood of output sequence and input parsing action sequence. This is similar to our multi-task training setup. The key difference is our subtask is ITG parsing for reordering instead of linguistically-motivated parsing. The idea of adding a reordering l"
C18-1123,J97-3002,0,0.445659,"work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1454 Proceedings of the 27th International Conference on Computational Linguistics, pages 1454–1463 Santa Fe, New Mexico, USA, August 20-26, 2018. one hundred 1 0 1 2 dollars 0 3 Decoder $ 0 Reorderer 0 $ 1 1 2 3 0 0 Encoder Figure 1: The encoder-reorderer-decoder architecture. The input encoder is shared by the reorderer and the decoder. The reorderer permutes the RNN states of the encoder. key to our solution is the Inversion Transduction Grammars (Wu, 1997), a type of synchronous context free grammar limiting reordering to adjacent source spans. For machine translation across very different languages, ITGs have been reported to cover most of the alignments observed in parallel data (Zhang et al., 2006). For text normalization, we have not found a single example of reordering that cannot be covered by an ITG. This observation motivates us to factorize translation into two steps: an ITG reordering step followed by a monotonic translation step. The task of the reordering step is to handle systematic reordering through swapping of adjacent source sp"
C18-1123,N09-1028,0,0.0319698,"is to handle systematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of Raffel et al. (2017) and Aharoni and Goldberg (2017). Of course, the burden of reordering has shifted from the attention mechanism to the dedicated reordering model. This idea has produced fruitful results in the era of phrase-based machine translation (Collins et al., 2005; Xu et al., 2009; Neubig et al., 2012; Lerner and Petrov, 2013; Nakagawa, 2015). In this paper, we revive the old idea with a neural treatment. First of all, we modify the encoder-decoder architecture by adding a reorderer which shares the encoder with the decoder. Figure 1 shows the architecture. This change enables multi-task training of the encoder states and turns the inference into three steps: encoding, reordering, decoding, each of which is a linear chain RNN. Our main algorithmic contribution is an ITG-transition-based RNN reorderer. What we feed to the RNN for training are input strings paired with t"
C18-1123,N06-1033,1,0.855184,"1463 Santa Fe, New Mexico, USA, August 20-26, 2018. one hundred 1 0 1 2 dollars 0 3 Decoder $ 0 Reorderer 0 $ 1 1 2 3 0 0 Encoder Figure 1: The encoder-reorderer-decoder architecture. The input encoder is shared by the reorderer and the decoder. The reorderer permutes the RNN states of the encoder. key to our solution is the Inversion Transduction Grammars (Wu, 1997), a type of synchronous context free grammar limiting reordering to adjacent source spans. For machine translation across very different languages, ITGs have been reported to cover most of the alignments observed in parallel data (Zhang et al., 2006). For text normalization, we have not found a single example of reordering that cannot be covered by an ITG. This observation motivates us to factorize translation into two steps: an ITG reordering step followed by a monotonic translation step. The task of the reordering step is to handle systematic reordering through swapping of adjacent source spans. This step “normalizes” the input in terms of word order which opens up the capacity for improvement in translation accuracy. It also opens up the opportunity for more “lightweight” neural translation models such as monotonic attention models of"
D13-1088,P85-1030,0,0.863416,"ual features and the linguistically-motivated non-local features performs best in identifying both primary and secondary stress. 1 2 Introduction In many languages, one component of accurate word pronunciation prediction is predicting the placement of lexical stress. While in some languages (e.g. Spanish) the lexical stress system is relatively simple, in others (e.g. English, Russian) stress prediction is quite complicated. Much as with other work on pronunciation prediction, previous work on stress assignment has fallen into two camps, namely systems based on linguistically motivated rules (Church, 1985, for example) and more recently datadriven techniques where the models are derived directly from labeled training data (Dou et al., 2009). In this work, we present a machinelearned system for predicting Russian stress Previous Work on Stress Prediction Pronunciation prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” co"
D13-1088,J05-1003,0,0.11685,"alternatives, each representing a different primary stress placement. Some words also have secondary stress which, if it occurs, always occurs before the primary stressed syllable. For each primary stress alternative, we generate all possible secondary stressed alternatives, including an alternative that has no secondary stress. (In the experiments reported below we actually consider two conditions: one where we ignore secondary stress in training and evaluation; and one where we include it.) Formally, we model the problem using a Maximum Entropy ranking framework similar to that presented in Collins and Koo (2005). For each example, xi , we generate the set of possible stress patterns Yi . Our goal is to rank the items in Yi such that all of the valid stress patterns Yi∗ are above all of the invalid stress patterns. Our objective function is the likelihood, L of this conditional distribution: ∏ L = p(Yi∗ |Yi , xi ) (1) i log L = ∑ log p(Yi∗ |Yi , xi ) i = ∑ ∑ log i y 0 ∈Yi∗ e ∑ k Z (2) θk fk (y 0 ,x) (3) Z is defined as the sum of the conditional likelihood over all hypothesized stress predictions for example xi : ∑ ∑ 00 Z= e k θk fk (y ,x) (4) y 00 ∈Yi The objective function in Equation 3 can be optim"
D13-1088,P09-1014,0,0.817571,"Introduction In many languages, one component of accurate word pronunciation prediction is predicting the placement of lexical stress. While in some languages (e.g. Spanish) the lexical stress system is relatively simple, in others (e.g. English, Russian) stress prediction is quite complicated. Much as with other work on pronunciation prediction, previous work on stress assignment has fallen into two camps, namely systems based on linguistically motivated rules (Church, 1985, for example) and more recently datadriven techniques where the models are derived directly from labeled training data (Dou et al., 2009). In this work, we present a machinelearned system for predicting Russian stress Previous Work on Stress Prediction Pronunciation prediction, of which stress prediction is a part, is important for many speech applications including automatic speech recognition, text-to-speech synthesis, and transliteration for, say, machine translation. While there is by now a sizable literature on pronunciation prediction from spelling (often termed “grapheme-to-phoneme” conversion), work that specifically focuses on stress prediction is more limited. One of the best-known early pieces of work is (Church, 198"
H05-1073,P97-1023,0,0.0168789,"emotional meaning, 580 such as happy or sad, matter, emotion classification probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study"
H05-1073,W05-0625,1,0.133096,"Missing"
H05-1073,P04-1045,0,0.0053032,"n order to be effective, emotion recognition must go beyond such resources; the authors note themselves that lexical affinity is fragile. The method was tested on 20 users’ preferences for an email-client, based on user-composed text emails describing short but colorful events. While the users preferred the emotional client, this evaluation does not reveal emotion classification accuracy, nor how well the model generalizes on a large data set. Whereas work on emotion classification from the point of view of natural speech and humancomputer dialogues is fairly extensive, e.g. (Scherer, 2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthesis (TTS). A short study by (Sugimoto et al., 2004) addresses sentence-level emotion recognition for Japanese TTS. Their model uses a composition assumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence. They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects and compute words’ emotional strength based on the ratio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects. Additionally, t"
H05-1073,W04-3253,0,0.252605,"Missing"
H05-1073,P04-1035,0,0.0487695,"Missing"
H05-1073,P02-1053,0,0.0132851,"on probably needs to consider additional inference mechanisms. Moreover, a na¨ıve compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation. Many NLP problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g. (Wiebe et al, 2004), measuring strength of subjective clauses (Wilson, Wiebe and Hwa, 2004), determining word polarity (Hatzivassiloglou and McKeown, 1997) or texts’ attitudinal valence, e.g. (Turney, 2002), (Bai, Padman and Airoldi, 2004), (Beineke, Hastie and Vaithyanathan, 2003), (Mullen and Collier, 2003), (Pang and Lee, 2003). Here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passages in children’s stories, and eventually use this information for rendering expressive child-directed storytelling in a text-to-speech application. This can be useful, e.g. in therapeutic education of children with communication disorders (van Santen et al., 2003). 4 Empirical study This part covers the experimental study with a form"
H05-1073,C02-1150,1,\N,Missing
H05-1073,J04-3002,0,\N,Missing
H05-1073,P04-1034,0,\N,Missing
H94-1050,J94-3001,0,0.0630817,"re detail in the next section. In this paper we will be concerned with the weighted rational case, although some of the theory can be profitably extended beyond the finite-state case [3, 4]. 262 The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [5, 6] and algebraic automata theory [7, 8, 9]. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [10] and at the University of Paris 7 [11], among others, to several problems in language processing, includifig morphological analysis, dictionary compression and syntactic analysis. Hidden Markov Models and probabilistic finite-state language models can be shown to be equivalent to WFSAs. In algebraic automata theory, rational series and rational transductions [8] are the algebraic counterparts of WFSAs and WFSTs and give the correct generalizations to the weighted case of the standard algebraic operations on formal languages and transductions, such as union, concatenation, intersection, restric"
H94-1050,C90-3049,0,0.12806,"re derived via productive morphological processes are not generally to be found in the dictionary. One such case in Chinese involves words derived via the nominal plural affix r~l -men. While some words in ~I will be found in the dictionary ( e . g . , / ! ! ~ tal-men 'they'; ~ ren2-men 'people'), many attested instances will not: for example, ~ f ~ jiang4-men '(military) generals', ~ qingl-wal-men 'frogs'. Given that the basic dictionary is represented as a finite-state automaton, it is a simple matter to augment the model just described with standard techniques from finite-state morphology ([15, 16], inter alia). For in3We are currently using the 'Behavior Chinese-English Electronic Dictionary', Copyright Number 112366, from Behavior Design Corporation, R.O.C.; we also wish to thank United Informaties, Inc., R.O.C. for providing us with the Chinese text corpus that we used in estimating lexieal probabilities. Finally we thank Dr. Jyun-Sheng Chang for kindly providing us with Chinese personal name corpora. 266 stance, we can represent the fact that f ] attaches to nouns by allowing e-transitions from the final states of noun entries, to the initial state of a sub-transducer containing f ]"
J10-3012,J10-4016,0,\N,Missing
J10-3012,J10-4015,0,\N,Missing
J10-3012,J10-4017,1,\N,Missing
J10-4017,J10-3012,1,0.823392,"ic, and (pace Lee et al.) European heraldry would all count as writing from that standpoint. After all, all of these are systems of markings, with conventional reference, that communicate information. It is hard to believe that Powell himself really wants this broad a deﬁnition: The subtitle of his book is “Theory and History of the Technology of Civilization.” Although nonlinguistic notation systems such as mathematics have been central in the history of the world, I submit that what makes writing work as the “technology of civilization” is precisely that it allows us to record language (see Sproat 2010b). In any event if that is the breadth that Lee wants, then certainly Pictish symbols (and Mesopotamian deity symbols) are writing, because they are conventional systems of marks that (presumably) communicated information. But the broader deﬁnition is also highly misleading. When lay people think of writing they will undoubtedly think of the script and standard orthography they learned in school, which allows them to write anything from letters, to shopping lists, to poetry, to journal articles. Calling Pictish symbols “writing” easily conjures up in the mind of the reader the notion that suc"
J14-4002,W98-0903,0,0.123032,"does not appear in this form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicogra"
J14-4002,P03-1006,1,0.663102,"Missing"
J14-4002,N01-1018,0,0.0492683,"OR 97239-3098, USA. E-mails: {mahsa.yarmohamadi,zakshafran}@gmail.com. Submission received: 1 March 2013; revised version received: 5 November 2013; accepted for publication: 23 December 2013. doi:10.1162/COLI_a_00198 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 4 1950s and 1960s to implement small hand-built grammars (e.g., Joshi and Hopely 1996) through their applications in computational morphology in the 1980s (Koskenniemi 1983), finite-state models are now routinely applied in areas ranging from parsing (Abney 1996), to machine translation (Bangalore and Riccardi 2001; de Gispert et al. 2010), text normalization (Sproat 1996), and various areas of speech recognition including pronunciation modeling and language modeling (Mohri, Pereira, and Riley 2002). The development of weighted finite state approaches (Mohri, Pereira, and Riley 2002; Mohri 2009) has made it possible to implement models that can rank alternative analyses. A number of weight classes—semirings—can be defined (Kuich and Salomaa 1986; Golan 1999), though for all practical purposes nearly all actual applications use the tropical semiring, whose most obvious instantiation is as a way to combin"
J14-4002,J10-3008,0,0.273047,"Missing"
J14-4002,D10-1080,0,0.0209975,"Missing"
J14-4002,C00-1038,0,0.0594641,", because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of elements of the tuple is the sam"
J14-4002,C94-2163,0,0.351637,"use the suffix does not appear in this form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely,"
J14-4002,J98-2006,0,0.107901,"form; again this violation is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of el"
J14-4002,W98-1301,0,0.0802897,"tion is critical, because the remaining two forms do not violate the constraint. Both dishis and and dishiz violate Dep because there is an inserted segment and they are thus equally bad according to that constraint. So to decide between the two forms, we go to the next lower constraint, Ident(Voi), which dishis violates because the underlying z is changed to an s. This violation is therefore critical, and the winning form is dishiz, indicated by the right-pointing hand. There have been many finite-state models of Optimality Theory (Ellison 1994; Albro 1998; Eisner 1998; Frank and Satta 1998; Karttunen 1998; Eisner 2000), and our point here is not to provide a fully worked out implementation of the model. Rather, we wish 736 Sproat et al. Lexicographic Semirings to show that an appropriately defined lexicographic semiring can readily model the constraint ranking. We start by defining the violation semiring V as (Z ∪ {∞}, min, +, ∞, 0); V is clearly just a special case of the tropical semiring where the values of the weights are restricted to be non-negative integers. We then define the optimality semiring O as hV, V, . . . i, namely, a lexicographic tuple over V. The number of elements of the tu"
J14-4002,J93-2004,0,0.0461717,"Missing"
J14-4002,P12-3011,1,0.896656,"Missing"
J14-4002,P11-2001,1,0.497098,"Missing"
J14-4002,J95-2004,0,0.293631,"Missing"
J14-4002,N10-1023,0,0.0420037,"Missing"
J14-4002,2010.iwslt-keynotes.2,0,\N,Missing
J19-2004,D16-1162,0,0.0776354,"Missing"
J19-2004,P12-3006,0,0.0949107,"ow the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how on"
J19-2004,P10-1079,0,0.0565589,"categories finer-grained classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normali"
J19-2004,P18-1008,0,0.0293253,"ls pass through digits. # Then try digits @ o, and if that fails pass through digits. # Then try digits @ cardinals, which shall surely work. # Oh, and then make it a disjunction with thousand to allow both # &quot;twenty ten&quot; and &quot;two thousand ten&quot; readings. export YEAR = Optimize[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[ LenientlyCompose[digits, hundreds, sigstar], pairwise, sigstar], o, sigstar], cardinal, sigstar] | thousand]; A.2 Transformer Model Details We utilize a Transformer sequence-to-sequence model (Vaswani et al. 2017), using the architecture described in Appendix A.2 of Chen et al. (2018), with: • 6 Transformer layers for both the encoder and the decoder, • 8 attention heads, • a model dimension of 512, and • a hidden dimension of 2,048. 333 Computational Linguistics Volume 45, Number 2 Table A.1 Default parameters for the sliding window model. Input embedding size Output embedding size Number of encoder layers Number of decoder layers Number of encoder units Number of decoder units Attention mechanism size 256 512 1 1 256 512 256 Dropout probabilities are uniformly set to 0.1. We use a dictionary of 32k word pieces (Schuster and Nakajima 2012) covering both input and output v"
J19-2004,P14-2111,0,0.484934,"dern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text,"
J19-2004,P13-1155,0,0.070932,"Missing"
J19-2004,C08-1056,0,0.0378842,"Missing"
J19-2004,P12-1109,0,0.0772642,"Missing"
J19-2004,P11-2013,0,0.0562114,"ned classifications depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at s"
J19-2004,P12-1055,0,0.0199831,"n, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally o"
J19-2004,D16-1096,0,0.0278276,"m the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstruction loss, which is the"
J19-2004,W15-4317,0,0.613959,"e, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of social media text, though it is essent"
J19-2004,I11-1109,0,0.0148426,"ns depend in part on how the input maps to the output verbalization, and in part on the kind of entity denoted by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. Fo"
J19-2004,P14-2060,1,0.776607,"o occur naturally. Machine translation can to a large extent rely on “found” data because people translate texts for practical reasons, such as providing access to documents to people not able to read the source language. In contrast, there is no reason why people would spend resources producing verbalized equivalents of ordinary written text: in most cases, English speakers 3 As a consequence, much of the subsequent work on applying machine learning to text normalization for speech applications focuses on specific semiotic classes, like letter sequences (Sproat and Hall 2014), abbreviations (Roark and Sproat 2014), or cardinal numbers (Gorman and Sproat 2016). 4 In fact, Kestrel (Ebden and Sproat 2014) uses a machine-learned morphosyntactic tagger for Russian. 298 Zhang et al. Neural Models of Text Normalization do not need a gloss to know how to read $10 million. Thus, if one wants to train neural models to verbalize written text, one must produce the data.5 Second, the bar for success in this domain seems to be higher than it is in other domains in that users expect TTS systems to correctly read numbers, dates, times, currency amounts, and so on. As we show subsequently, deep learning models produce"
J19-2004,P12-3011,1,0.93381,"arate words. Thus the Kestrel grammars recognize Jan.1, 2012 as a date and parse it as a single token, identifying the month, day, and year, and represent it internally using a protocol-buffer representation like the following:6 date { month: &quot;January&quot; day: &quot;1&quot; year: &quot;2012&quot;} Verbalization grammars then convert from a serialization of the protocol buffer representation into actual word sequences, such as January the first twenty twelve. Tokenization/classification and verbalization grammars are compiled into weighted finite-state transducers (WFSTs) using the Thrax grammar development library (Roark et al. 2012). One advantage of separating tokenization/classification from verbalization via the intermediate protocol buffer representation is that it allows for reordering of elements, something that is challenging with WFSTs.7 The need for reordering arises, for example, in the treatment of currency expressions where currency symbols such as ‘$’ or ‘’ often occur before digits, but are verbalized after the corresponding digits. An input $30 might be parsed as something like money { currency: &quot;USD&quot; amount { integer: &quot;30&quot; } } 6 https://developers.google.com/protocol-buffers/. 7 As we will show subsequent"
J19-2004,P16-1162,0,0.148348,"Missing"
J19-2004,N10-1023,0,0.0238932,"a, and in that we define more precisely how the covering grammars are actually used during decoding. Finally, we report results on new data sets. Arik et al. (2017) present a neural network TTS system that mimics the traditional separation into linguistic analysis (or front-end) and synthesis (or back-end) modules. It is unclear to what degree this system in fact performs text normalization since the only front-end component they describe is grapheme-to-phoneme conversion, which is a separate process from text normalization and usually performed later in the pipeline. Some prior work, such as Shugrina (2010), focuses on the inverse problem of denormalizing spoken sequences into written text in the context of ASR so that two hundred fifty would get converted to 250, or three thirty as a time would get formatted as 3:30. Pusateri et al. (2017) describe a system in which denormalization is treated as a neural network sequence labeling problem using a rich tag set. 8 See http://github.com/google/sparrowhawk. 301 Computational Linguistics Volume 45, Number 2 The data we report on in this article was recently released and was the subject of a Kaggle competition (see later in this article), and a few re"
J19-2004,P16-1008,0,0.0268692,"ame word position from the GRU← layer. 5.5 Incorporating Reconstruction Loss We observe that unrecoverable errors usually involve linguistically coherent output, but simply fail to correspond to the input. In the terminology used in machine translation, 311 Computational Linguistics Volume 45, Number 2 one might say that they favor fluency over adequacy. The same pattern has been identified in neural machine translation (Arthur, Neubig, and Nakamura 2016), which motivates a branch of research that can be summarized as enforcing the attention-based decoder to pay more “attention” to the input. Tu et al. (2016) and Mi et al. (2016) argue that the root problem lies in the attention mechanism itself. Unlike traditional phrase-based machine translation, there is no guarantee that the entire input can be “covered” at the end of decoding, and thus they strengthen the attention mechanism to approximate a notion of input coverage. Tu et al. (2017) suggest that the fix can also be made in the decoder RNN. The key insight here is that the hidden states in the decoder RNN should keep memory of the correspondence with the input. In addition to the standard translation loss, there should also be a reconstructio"
J19-2004,P06-1125,0,0.033252,"Missing"
J19-2004,D13-1007,0,0.0413732,"ed by the token. From a modern perspective, this taxonomy has a number of obvious omissions. Some of these omitted types did not exist, or were considerably less common, at the time of writing, such as hashtags or “funny spellings” like slloooooww. The increasing prominence of such categories has led to a considerable body of work on normalizing SMS and social media text (Xia, Wong, and Li 2006; Choudhury et al. 2007; Kobus, Yvon, and Damnati 2008; Beaufort et al. 2010; Liu et al. 2011; Pennell and Liu 2011; Aw and Lee 2012; Liu, Weng, and Jiang 2012; Liu et al. 2012; Hassan and Menezes 2013; Yang and Eisenstein 2013; Chrupala 2014; Min and Mott 2015, inter alia). 2 Text normalization is thus a task of great importance for many diverse real-world applications, although the requirements of large-scale speech applications such as TTS and automatic speech recognition (ASR) have received comparatively little attention in the natural language 2 Text normalization of social media tends to focus on different problems from those that are the main concern of normalization aimed at speech applications. For example, how one pronounces number sequences is generally of little or no concern in the normalization of soci"
J19-2004,N18-1122,0,0.0317744,"able errors, and for these we have argued that using trainable finite-state covering grammars is a reasonable approach, but we continue to look for ways to improve covering grammar training and coverage. 18 At the same time, we are currently exploring whether other neural approaches can help mitigate against unrecoverable errors. One approach that seems plausible is generative adversarial networks (Goodfellow et al. 2014; Goodfellow 2016), which have achieved impressive results in vision-related tasks but which have been also applied to NLP tasks including machine translation (Wu et al. 2017; Yang et al. 2018). Given the great success of deep learning for many problems, it is tempting to simply accrete speech and language tasks to a general class of problems and to worry less about the underlying problem being solved. For example, at a certain level of abstraction, all of text-to-speech synthesis can be thought of as a sequence-to-sequence problem where the input sequence is a string of characters and the output sequence is some representation of a waveform. “End-to-end” TTS models such as Char2Wav (Sotelo et al. 2017) treat the problem in this way, with no attempt to consider the many subproblems"
J19-2004,D14-1179,0,\N,Missing
J19-2004,Q16-1036,1,\N,Missing
J96-2001,A88-1019,0,0.0344807,"of the various potential functions of such forms. 1 - - 2. Estimating the Lexical Priors for Rare Forms For a common form such as lopen 'walk' a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form. So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again brings up the question of what to do about unseen or low-frequency forms. In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in Church (1988), for example. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to .> ""8 I I I I 0 2 4 6 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the frequency of the word"
J96-2001,J88-1003,0,0.114014,"s potential functions of such forms. 1 - - 2. Estimating the Lexical Priors for Rare Forms For a common form such as lopen 'walk' a reasonable estimate of the lexical prior probabilities is the MLE, computed over all occurrences of this form. So, in the UdB corpus, lopen occurs 92 times as an infinitive and 43 times as a finite plural, so the MLE 1 Even models of disambiguation that make use of context, such as statistical n-gram taggers, often presume some estimate of lexical priors, in addition to requiring estimates of the transition probabilities of sequences of lexical tags (Church 1988; DeRose 1988; Kupiec 1992), and this again brings up the question of what to do about unseen or low-frequency forms. In working taggers, a common approach is simply to apply a uniform small probability to the various senses of unseen or low-frequency forms: this was done in the tagger discussed in Church (1988), for example. 156 Baayen and Sproat Lexical Priors for Low-Frequency Forms to .> ""8 I I I I 0 2 4 6 log frequency class Figure 1 Relative frequency of Dutch infinitives versus finite plurals in the Uit den Boogaart corpus, as a function of the (natural) log of the frequency of the word forms. The h"
J96-2001,C92-2070,0,0.0741648,"Missing"
J96-2001,P94-1013,0,\N,Missing
J96-3004,O91-1004,0,0.0778176,"Missing"
J96-3004,C92-1019,0,0.65039,"ereof include Liang (1986), Li et al. (1991), Gu and Mao (1994), and Nie, Jin, and Hannan (1994). The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation. Methods that allow multiple segmentations must provide criteria for choosing the best segmentation. Some approaches depend upon some form of constraint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach). Others depend upon various lexical heuristics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word. Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994). Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation"
J96-3004,P89-1010,0,0.0122645,"Missing"
J96-3004,J94-3001,0,0.00852222,"o r d is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element S of H x p , which is terminated with a w e i g h t e d arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word. Next, we represent the input sentence as an u n w e i g h t e d finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as o u t p u t a transducer that m a p s all and only the strings of symbols accepted b y A to themselves (Kaplan and Kay 1994). We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected. 383 Computational Linguistics Volume 22, Number 3 then define the best segmentation to be the cheapest or best path in Id(I) o D* (i.e., Id(I) composed with the transitive closure of D). 6 Consider the abstract example illustrated in Figure 2. In this example there are four &quot;input characters,&quot; A, B, C and D, and these map respectively to four &quot;pronunc"
J96-3004,C92-1025,0,0.00750365,"Missing"
J96-3004,O93-1004,0,0.138021,"Missing"
J96-3004,C94-1032,0,0.0582974,"nese lacks anything analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984). Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988, 1989, inter alia) has revealed the incorrectness of this traditional view. All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog1 For a related approach to the problem of word-segmention in Japanese, see Nagata (1994), inter alia. 2 Chinese ~ - - ~ han4zi4 &apos;Chinese character&apos;; this is the same word as Japanese kanji. 3 Throughout this paper we shall give Chinese examples in traditional orthography, followed immediately by a Romanization into the pinyin transliteration scheme; numerals following each pinyin syllable represent tones. Examples will usually be accompanied by a translation, plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose. In the pinyin transliterations a dash (-) separates syllables that may be considered part of the same ph"
J96-3004,H94-1050,1,0.430597,"Missing"
J96-3004,C90-3049,0,0.0258882,"Missing"
J96-3004,C92-4199,0,0.0676546,"Missing"
J96-3004,A94-1030,0,0.00654676,"hat therefore such simplistic measures should be mistrusted. This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca. May 1995). However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion. Second, comparisons of different methods are not meaningful unless one can evaluate them on the same corpus. Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods. One hopes that such a corpus will be forthcoming. Finally, we wish to reiterate an important point. The major problem for our segmenter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]). We have provided methods for handling cert"
J96-3004,J90-1003,0,\N,Missing
J96-3004,P87-1010,1,\N,Missing
L16-1317,jansche-2014-computer,1,0.827128,"license. Our phonological representation closely follows the description of Bangladeshi colloquial Bangla in (Ud Dowla Khan, 2010). It uses 39 segmental phonemes, much fewer than the 47 phonemes used by (Prahallad et al., 2012). A team of five linguists transcribed more than 65,000 words into a phonemic representation. This includes mostly words in Bengali script, as well as nearly 5,000 words in Latin script, including English words and foreign entity names. The transcription effort utilized a version of our phonemic transcription tools (Ainsley et al., 2011) and quality control methodology (Jansche, 2014). Our transcribers were further aided by the output of a pronunciation model, which was used to pre-fill the 1 https://github.com/googlei18n/ language-resources/tree/master/bn/data 2006 Input Internal Output “এ Archive Type rewrite tokenize/classify verbalize total 3cm হয়” (“X is 3cm”) { name: &quot;এ &quot; } { measure { decimal { integer_part: &quot;3&quot; } units: &quot;centimeter&quot; } } { name: &quot;হয়&quot; } “এ Compressed (kB) 22 1,714 3,330 5,066 Ratio ×5.3 ×3.2 ×4.3 ×3.9 Table 1: FST grammars and their disk footprint (in kilobytes). িতন সি িমটার হয়” Figure 3: Example of a basic Kestrel workflow. The output corresponding"
P06-1010,C00-2159,0,0.0480783,"— e.g. (Meng et al., 2001; Gao et al., 2004), the problem has been cast as the problem of producing, for a given Chinese name, an English equivalent such as one might need in a machine translation system. For example, for wei wei-lian-mu-si, one the name would like to arrive at the English name V(enus) Williams. Common approaches include sourcechannel methods, following (Knight and Graehl, 1998) or maximum-entropy models. Comparable corpora have been studied extensively in the literature (e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2003)), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996)).Recently, a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005), an idea similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 200"
P06-1010,P03-2025,0,0.033258,"001; Gao et al., 2004), the problem has been cast as the problem of producing, for a given Chinese name, an English equivalent such as one might need in a machine translation system. For example, for wei wei-lian-mu-si, one the name would like to arrive at the English name V(enus) Williams. Common approaches include sourcechannel methods, following (Knight and Graehl, 1998) or maximum-entropy models. Comparable corpora have been studied extensively in the literature (e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2003)), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996)).Recently, a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005), an idea similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to th"
P06-1010,J96-3004,1,0.663854,"n in the training data as corresponding to the initial of the Chinese syllable some minimum number of times. For consonant-initial syllables we set the minimum to 4. We omit further details of our estimation technique for lack of space. This phonetic correspondence model can then be used to score putative transliteration pairs. Candidate Selection The English named entity candidate selection process was already described above. Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names. As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese. We use a list of 495 such characters, derived from various online dictionaries. A sequence of three or more characters from the list is taken as a possible name. If the character “ ” occurs, which is frequently used to represent the space between parts of an English name, then at least one character to the left and right of this character will be collected, even if the character in question is not in the list of “foreign” characters. Armed with the En"
P06-1010,C96-2098,0,0.177174,"p. 2 Previous Work In previous work on Chinese named-entity transliteration — e.g. (Meng et al., 2001; Gao et al., 2004), the problem has been cast as the problem of producing, for a given Chinese name, an English equivalent such as one might need in a machine translation system. For example, for wei wei-lian-mu-si, one the name would like to arrive at the English name V(enus) Williams. Common approaches include sourcechannel methods, following (Knight and Graehl, 1998) or maximum-entropy models. Comparable corpora have been studied extensively in the literature (e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2003)), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996)).Recently, a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005), an idea similar to the method used in (Kay and Roscheisen, 1993) for sentence al"
P06-1010,P95-1032,0,0.0785321,"’s badminton championship. 2 Previous Work In previous work on Chinese named-entity transliteration — e.g. (Meng et al., 2001; Gao et al., 2004), the problem has been cast as the problem of producing, for a given Chinese name, an English equivalent such as one might need in a machine translation system. For example, for wei wei-lian-mu-si, one the name would like to arrive at the English name V(enus) Williams. Common approaches include sourcechannel methods, following (Knight and Graehl, 1998) or maximum-entropy models. Comparable corpora have been studied extensively in the literature (e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2003)), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996)).Recently, a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005), an idea similar to the method used in (Kay"
P06-1010,W06-1630,1,0.271954,"Missing"
P06-1010,J93-1006,0,0.0880907,"995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2003)), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting frequency correlations to acquire word translations from comparable corpora has been explored in several previous studies (e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996)).Recently, a method based on Pearson correlation was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005), an idea similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to the problem of transliteration. We also study several variations of the similarity measures. Mining transliterations from multilingual web pages was studied in (Zhang and Vines, 2004); 1. Given an English name, identify candidate Chinese character n-grams as possible transliterations. ¬¤þ®· 2. Score each candidate based on how likely the candidate is to be a transliteration of the English name. We propose two different scoring methods. The first involves phonetic scoring, and the second"
P06-1010,N04-1003,0,0.123997,"ñ í Ô  ö 11:1á ¡ ¤ ó ¡ Ö Ù¤ ·,ÅþÚÏçÔ11:2Í11:9ÔËÉ Ä Ú¤¬×ÏË,ÜÛÚÂçÔ11:4Í11:1 ¤Ëú ãÛ¡Öèñ We assume that we have comparable corpora, consisting of newspaper articles in English and Chinese from the same day, or almost the same day. In our experiments we use data from the English and Chinese stories from the Xinhua News agency for about 6 months of 2001.2 We assume that we have identified names for persons and locations—two types that have a strong tendency to be transliterated wholly or mostly phonetically—in the English text; in this work we use the named-entity recognizer described in (Li et al., 2004), which is based on the SNoW machine learning toolkit (Carlson et al., 1999). To perform the transliteration task, we propose the following general three-step approach: Figure 1: Sample from two stories about an international women’s badminton championship. 2 Previous Work In previous work on Chinese named-entity transliteration — e.g. (Meng et al., 2001; Gao et al., 2004), the problem has been cast as the problem of producing, for a given Chinese name, an English equivalent such as one might need in a machine translation system. For example, for wei wei-lian-mu-si, one the name would like to"
P06-1010,J98-4003,0,\N,Missing
P07-1015,J96-4003,0,0.0286626,"mparable corpora has not been well addressed. In our work, we adopt the method proposed in (Tao et al., 2006) and apply it to the problem of transliteration. Measuring phonetic similarity between words has been studied for a long time. In many studies, two strings are aligned using a string alignment algorithm, and an edit distance (the sum of the cost for each edit operation), is used as the phonetic distance between them. The resulting distance depends on the costs of the edit operation. There are several approaches that use distinctive features to determine the costs of the edit operation. Gildea and Jurafsky (1996) counted the number of features whose values are different, and used them as a substitution cost. However, this approach has a crucial limitation: the cost does not consider the importance of the features. Nerbonne and Heeringa (1997) assigned a weight for each feature based on 113 entropy and information gain, but the results were even less accurate than the method without weight. 3 Phonetic transliteration method In this paper, the phonetic transliteration is performed using the following steps: 1) Generation of the pronunciation for English words and target words: a. Pronunciations for Engl"
P07-1015,N04-1003,0,0.0200691,"Missing"
P07-1015,W97-1102,0,0.0311754,"me. In many studies, two strings are aligned using a string alignment algorithm, and an edit distance (the sum of the cost for each edit operation), is used as the phonetic distance between them. The resulting distance depends on the costs of the edit operation. There are several approaches that use distinctive features to determine the costs of the edit operation. Gildea and Jurafsky (1996) counted the number of features whose values are different, and used them as a substitution cost. However, this approach has a crucial limitation: the cost does not consider the importance of the features. Nerbonne and Heeringa (1997) assigned a weight for each feature based on 113 entropy and information gain, but the results were even less accurate than the method without weight. 3 Phonetic transliteration method In this paper, the phonetic transliteration is performed using the following steps: 1) Generation of the pronunciation for English words and target words: a. Pronunciations for English words are obtained using the Festival text-to-speech system (Taylor et al., 1998). b. Target words are automatically converted into their phonemic level transcriptions by various language-dependent means. In the case of Mandarin C"
P07-1015,J96-3004,1,0.809675,"Missing"
P07-1015,W06-1630,1,0.668724,"nsliteration Using Feature based Phonetic Method Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat University of Illinois at Urbana-Champaign {syoon9,kkim36,rws}@uiuc.edu Abstract In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages – Arabic, Chinese, Hindi and Korean – and one source language – English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages. 1 Introduction. In this paper, we develop a multi-lingual translitera"
P07-1015,W02-0505,0,\N,Missing
P07-1015,J98-4003,0,\N,Missing
P11-2001,P03-1006,1,0.843689,"as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. 1 Introduction and Motivation Representing smoothed n-gram language models as weighted finite-state transducers (WFST) is most naturally done with a failure transition, which reflects the semantics of the “otherwise” formulation of smoothing (Allauzen et al., 2003). For example, the typical backoff formulation of the probability of a word w given a history h is as follows  P(w |h) = P(w |h) if c(hw) &gt; 0 αh P(w |h0 ) otherwise (1) where P is an empirical estimate of the probability that reserves small finite probability for unseen n-grams; αh is a backoff weight that ensures normalization; and h0 is a backoff history typically achieved by excising the earliest word in the history h. The principle benefit of encoding the WFST in this way is that it only requires explicitly storing n-gram transitions for observed n-grams, i.e., count greater than zero, as"
P12-3011,J94-3001,0,0.391573,"iring by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string. 4 NGram Library The OpenGrm NGram library contains tools for building, manipulating and using n-gram language models represented as weighted finite-state transducers. The same finite-state topology"
P12-3011,P96-1031,1,0.748517,"adds the weight 3 (in the tropical semiring by default) to a. Functions lacking operators (hence only called by function name) include: ArcSort, Connect, Determinize, RmEpsilon, Minimize, Optimize, Invert, Project and Reverse. Most of these call the obvious underlying OpenFst function. One function in particular, CDRewrite is worth further discussion. This function takes a transducer and two context acceptors (and the alphabet machine), and generates a new FST that performs a context dependent rewrite everywhere in the provided contexts. The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). The fourth argument (sigma star) needs to be a minimized machine. The fifth argument selects the direction of rewrite; we can either rewrite left-to-right or rightto-left or simultaneously. The sixth argument selects whether the rewrite is optional. would parse a sequence of tab-separated pairs, using utf8 parsing for the left-hand string, and the symbol table my symtab for the right-hand string. 4 NGram Library The OpenGrm NGram library contains tools for building, manipulating and using n-gram language models represented as weighted finite-state transduc"
P14-2060,P12-3006,0,0.0892958,"ut the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is proba"
P14-2060,P12-1109,0,0.0365213,"be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying"
P14-2060,P10-1079,0,0.111581,"on are taken from Google MapsTM and web pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand,"
P14-2060,P12-1055,0,0.0417532,"be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying"
P14-2060,I11-1109,0,0.0499091,"with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a produ"
P14-2060,P12-3011,1,0.734833,"for terms that could be abbreviations of full words that occur in the same context. Thus: the heating dry svc/service clng/cooling clng/cleaning center system system contributes evidence that svc is an abbreviation of service. Similarly instances of clng in contexts that can contain cooling or cleaning are evidence that clng could be an abbreviation of either of these words. (The same contextual information of course is used later on to disambiguate which of the expansions is appropriate for the context.) To compute the initial guess as to what can be a possible abbreviation, a Thrax grammar (Roark et al., 2012) is used that, among other things, specifies that: the abbreviation must start with the same letter as the full word; if a vowel is deleted, all adjacent vowels should also be deleted; consonants may be deleted in a cluster, but not the last one; and a (string) suffix may be deleted.2 We count a pair of words as ‘co-occurring’ if they are observed in the same context. For a given context C, e.g., the center, let WC be the set of words found in that context. Then, for any pair of words u, v, we can assign a pair count based on the count of contexts where both occur: Methods Since our target app"
P14-2060,J93-1003,0,0.117038,"LM from the data. Because of the size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations Table 1: Examples of automatically mined abbreviation/expansion pairs. P Let c(u) be defi"
P14-2060,P13-1155,0,0.0435058,"relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviation"
P14-2060,P06-1125,0,0.460358,"Missing"
P14-2060,C08-1056,0,0.150982,"size of the data set, this is heavily pruned using relative entropy pruning (Stolcke, 1998). Second, we use log likelihood and log odds ratios (this time using standardly defined n-gram counts) to extract reliable bigram and trigram contexts for words. Space precludes a detailed treatment of these two statistics, but, briefly, both can be derived from contingency table values calculated from the frequencies of (1) the word in the particular context; (2) the word in any context; (3) the context with any word; and (4) all words in the corpus. See Agresti (2002), Dunning (1993) and Monroe et al. (2008) for useful overviews of how to calculate these and other statistics to derive reliable associations. In our case, we use them to derive associations between contexts and words occuring in those contexts. The contexts include trigrams with the target word in any of the three positions, and bigrams with the target word in either position. We filter the set of n-grams based on both their log likelihood and log odds ratios, and provide those scores as features. 3.3 Manual annotations Table 1: Examples of automatically mined abbreviation/expansion pairs. P Let c(u) be defined as v c(u, v). From th"
P14-2060,D13-1007,0,0.23954,"ich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for service center in a product review is probably merely trying to save time and so expanding the abbreviations in that case is neutral wi"
P14-2060,P11-2013,0,0.110172,"pages associated with its map entries, but the methods can be applied to any data source that is relatively abbreviation rich. We note in passing that similar issues arise in automatic spelling correction work (WilcoxO’Hearn et al., 2008), where it is better to leave a word alone than to “correct” it wrongly. 2 Related work There has been a lot of interest in recent years on “normalization” of social media such as Twitter, but that work defines normalization much more broadly than we do here (Xia et al., 2006; Choudhury et al., 2007; Kobus et al., 2008; Beaufort et al., 2010; Kaufmann, 2010; Liu et al., 2011; Pennell and Liu, 2011; Aw and Lee, 2012; Liu et al., 2012a; Liu et al., 2012b; Hassan and Menezes, 2013; Yang and Eisenstein, 2013). There is a good reason for us to focus more narrowly. For Twitter, much of the normalization task involves nonstandard language such as ur website suxx brah (from Yang and Eisenstein (2013)). Expanding the latter to your website sucks, brother certainly normalizes it to standard English, but one could argue that in so doing one is losing information that the writer is trying to convey using an informal style. On the other hand, someone who writes svc ctr for se"
P15-2035,N13-1084,1,0.597616,"Missing"
P15-2035,P02-1040,0,0.131546,"r the couch Well, that guy got stuck on the tree and then he, and then Pepper, his shoe fell out of the tree. Anna rescued it. Pepper brought his shoe back and Anna rescued them. Figure 1: Two topically different NNM retellings with similar free recall scores (6 and 5, respectively). S(e, n) will be 1. If not, the most similar word to e will be chosen from words in n using Lin’s universal similarity and S(e, n) will be that maximum score. The same procedure is applied to vm , and finally the similarity score between n and m is derived from the cosine score between vn and vm . 3.1.4 BLEU BLEU (Papineni et al., 2002) is commonly used measure of n-gram overlap for automatically evaluating machine translation output. Because it is a precision metric, the BLEU score for any pair of narratives n and m will depend on which narrative is considered the “reference”. To create a single BLEU-based overlap score for each pair of narratives, we calculate SimBLEU (n,m) as the mean of BLEU (m, n) and BLEU (n, m). 3.2 3.2.2 In a modified version of WordNet-based mutual similarity (SimWM ) (Mihalcea et al., 2006), we find the maximum similarity score S(wi , m) for each word wi in narrative n with words in narrative m as"
P87-1010,P86-1009,0,0.0310283,"ducers which implement phonological rules mapping surface strings to lexical representations. Not only are phonological rules finite state, but the control structure of the model is itself finite state. 2. Two Facts about Morphology We will now consider two issues in morphology, namely prosody and the nonisomorphism of syntactic and phonological structure. We maintain that these are are central to the task of a morphological analyzer and, hence, have incorporated them into our model. Two criticisms of this model can be put forth. First, KIMMO is not guaranteed to be computationally efficient (Barton, 1986). Second, there are many interesting morphological phenomena that KIMMO cannot cover without significantly redesigning the model. In this paper we will address the 2.1 The Relevance of Prosody to Morphology It has become increasingly evident from research within Generative Linguistics that 65 morphology cannot be limited to the concatenation and subsequent modification of strings of segments, but must recognize prosodic constituents devoid of segmental content (McCarthy, 1979; Levin, 1985). Work on reduplication I by Marantz (1982) and by Levin (1985) has argued convincingly that reduplication"
P94-1010,C90-3049,0,0.014772,"ation. We therefore prefer to build particular models for different classes of unknown words, rather than building a single general model. DICTIONARY MORPHOLOGICAL ANALYSIS The method just described segments dictionary words, but as noted there are several classes of words that should be handled that are not in the dictionary. One class comprises words derived by productive morphological processes, such as plural noun formation using the suffix ~I menO. The morphological analysis itself can be handled using well-known techniques from finite-state morphology (Koskenniemi, 1983; Antworth, 1990; Tzoukermann and Liberman, 1990; Karttunen et al., 1992; Sproat, 1992); so, we represent the fact that ~ attaches to nouns by allowing c-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing ~I. However, for our purposes it is not sufficient to represent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, ~ I jiang4-menO &apos;(military) generals&apos; occurs and we estimate its cost at 15.02"
P94-1010,C92-4199,0,0.21016,"Missing"
P94-1010,O93-1004,0,0.0703619,"orrect way to segment a text, 1. Many hanzi are homographs whose pronunciation depends upon word affiliation. So, ~ is pronounced deO~ when it is a prenominal modification marker, but di4 in the word [] ~ mu4di4 &apos;goal&apos;; ~ is normally ganl &apos;dry&apos;,but qian2 in a person&apos;s given name. PREVIOUS WORK There is a sizable literature on Chinese word segmentation: recent reviews include (Wang et al., 1990; Wu and Tseng, 1993). Roughly, previous work can be classified into purely statistical approaches (Sproat and Shih, 1990), statistical approaches which incorporate lexical knowledge (Fan and Tsai, 1988; Lin et al., 1993), and approaches that include lexical knowledge combined with heuristics (Chen and Liu, 1992). 2. Some phonological rules depend upon correct wordsegmentation, including Third Tone Sandhi (Shih, 1986), which changes a 3 tone into a 2 tone before another 3 tone: , J ~ ] ~ xiao3 [lao3 shu3] &apos;littWe use pinyin transliteration with numbers representing tones. 66 sentences consist of one or more entries from the dictionary, and we can generalize the word recognition problem to the word segmentation problem, by leftrestricting the transitive closure of the dictionary with the input. The result of th"
P94-1010,H94-1050,1,0.794889,"eby the aggregate probability of previously unseen members of a construction is estimated as NI/N, where N is the total number of observed tokens and N1 is the number of types observed only once. For r~l this gives prob(unseen(f~) I f~l), and to get the aggregate probability of novel ~l-constructions in a corpus we multiply this by prob,e~,(¢{~) to get probte~t(unseen(f~)). Finally, to estimate the probability of particular unseen word i~1/1 ~I, we use the simple bigram backoff model REPRESENTATION The lexicon of basic words and stems is represented as a weightedfinite-state tranducer (WFST) (Pereira et al., 1994). Most transitions represent mappings between hanzi and pronunciations, and are costless. Transitions between orthographic words and their parts-of-speech are represented by e-to-category transductions and a unigram cost (negative log probability) of that word estimated from a 20M hanzi training corpus; a portion of the WFST is given in Figure 1.2 Besides dictionary words, the lexicon contains all hanzi in the Big 5 Chinese code, with their pronunciation(s), plus entries for other characters (e.g., roman letters, numerals, special symbols). Given this dictionary representation, recognizing a s"
P94-1010,J94-3001,0,\N,Missing
P94-1010,J90-1003,0,\N,Missing
P94-1010,C94-1032,0,\N,Missing
P94-1010,A94-1030,0,\N,Missing
P94-1010,C92-1025,0,\N,Missing
P94-1010,C92-1019,0,\N,Missing
P96-1029,H89-2048,1,0.762645,"leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 2 Quick Review Modeling of Tree-Based A general introduction to classification and regression trees ('CART') including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English p h o n e m e / a a / ( / a / ) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of / a a / , one starts at the root of the t"
P96-1029,P95-1002,0,0.0834416,"clear. Finitestate machines provide a mathematically wellunderstood computational framework for representing a wide variety of information, both in NLP and speech processing. Lexicons, phonological rules, Hidden Markov Models, and (regular) grammars are all representable as finite-state machines, and finite-state operations such as union, intersection and composition mean that information from these various sources can be combined in useful 1The work reported here can thus be seen as complementary to recent reports on methods for directly inferring transducers from data (Oncina et al., 1993; Gildea and Jurafsky, 1995). 215 A concrete example will serve to illustrate. Consider that we h a v e / a a / i n some environment. The first question that is asked concerns the number of segments, including the / a a / i t s e l f , that occur to the left of t h e / a a / i n the word in w h i c h / a a / o c curs. (See Table 1 for an explanation of the symbols used in Figure 1.) In this case, if the / a a / is initial - - i.e., lseg is 1, one goes left; if there is one or more segments to the left in the word, go right. Let us assume that this / a a / i s initial in the word, in which case we go left. The next questi"
P96-1029,J94-3001,0,0.0875174,"ision on whether to go left to 2n or right to 2n + 1 depends on the answer to the question that is being asked at node n. Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanisms is clear. Finitestate machines provide a mathematically wellunderstood computational framework for representing a wide variety of information, both in NLP and speech processing. Lexicons, phonological rules, Hidden Markov Models, and (regular) grammars are all representable as finite-state machines, and finite-state operations such as union, intersection and composition mean that information from these various sources can be combined in useful 1The work reported here can thus be seen as compleme"
P96-1029,P95-1037,0,0.019278,"re the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 2 Quick Review Modeling of Tree-Based A general introduction to classification and regression trees ('CART') including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained on the TIMIT database (Fisher et al., 1987), and which models the phonetic realization of the English p h o n e m e / a a / ( / a / ) in various environments (Riley, 1991). When this tree is used in predicting the allophonic form of a particular instance of / a a / , one starts at the root of the tree, and asks questions about the environment in which t"
P96-1029,P96-1031,1,0.922881,"es. Abstract We report on a method for compiling decision trees into weighted finite-state transducers. The key assumptions are that the tree predictions specify how to rewrite symbols from an input string, and the decision at each tree node is stateable in terms of regular expressions on the input string. Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf. These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996). 1 2 Quick Review Modeling of Tree-Based A general introduction to classification and regression trees ('CART') including the algorithm for growing trees from data can be found in (Breiman et al., 1984). Applications of tree-based modeling to problems in speech and NLP are discussed in (Riley, 1989; Riley, 1991; Wang and Hirschberg, 1992; Magerman, 1995, inter alia). In this section we presume that one has already trained a tree or set of trees, and we merely remind the reader of the salient points in the interpretation of those trees. Consider the tree depicted in Figure 1, which was trained"
P96-1029,H92-1073,0,0.0433277,"m hand-built rewrite rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). For example, the text-analysis ruleset for 6 Future Applications We have presented a practical algorithm for converting decision trees inferred from data into weighted finite-state transducers that directly implement the models implicit in the trees, and we have empirically verified that the algorithm is correct. Several interesting areas of application come to mind. In addition to speech recognition, where we hope to apply the phonetic realization models described above to the much larger North American Business task (Paul and Baker, 1992), there are also applications to TTS where, for example, the decision trees for prosodic phrase-boundary prediction discussed in (Wang and Hirschberg, 1992) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multi-lingual version of the Bell Laboratories TTS system, described in (Sproat, 1995; Sproat, 1996). 7 Acknowledgments The authors wish to thank Fernando Pereira, Mehryar Mohri and two anonymous referees for useful comments. References Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 1984. Clas5Having said this,"
P96-1029,H94-1050,1,0.808684,"left to 2n or right to 2n + 1 depends on the answer to the question that is being asked at node n. Introduction Much attention has been devoted recently to methods for inferring linguistic models from data. One powerful inference method that has been used in various applications are decision trees, and in particular classification and regression trees (Breiman et al., 1984). An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models, in particular finitestate transducers and weighted finite-state transducers; see (Kaplan and Kay, 1994; Pereira et al., 1994, inter alia). The reason for the renewed interest in finite-state mechanisms is clear. Finitestate machines provide a mathematically wellunderstood computational framework for representing a wide variety of information, both in NLP and speech processing. Lexicons, phonological rules, Hidden Markov Models, and (regular) grammars are all representable as finite-state machines, and finite-state operations such as union, intersection and composition mean that information from these various sources can be combined in useful 1The work reported here can thus be seen as complementary to recent report"
P96-1029,P87-1010,1,\N,Missing
P96-1031,J94-3001,0,0.646429,"h , att. com r w s @ b e l l - l a b s , com Abstract finite-state transducers, under the condition that no rule be allowed to apply any more than a finite number of times to its own output. Kaplan and Kay (1994), or equivalently Karttunen (1995), provide an algorithm for compiling rewrite rules into finite-state transducers, under the condition that they do not rewrite their noncontextual part 2. We here present a new algorithm for compiling such rewrite rules which is both simpler to understand and implement, and computationally more efficient. Clarity is important since, as pointed out by Kaplan and Kay (1994), the representation of rewrite rules by finite-state transducers involves many subtleties. Time and space efficiency of the compilation are also crucial. Using naive algorithms can be very time consuming and lead to very large machines (Liberman, 1994). In some applications such as those related to speech processing, one needs to use weighted rewrite rules, namely rewrite rules to which weights are associated. These weights are then used at the final stage of applications to output the most probable analysis. Weighted rewrite rules can be compiled into weighted finite-state transducers, namel"
P96-1031,P95-1003,0,0.0756816,"Missing"
P96-1031,J94-3002,0,0.0250518,"algorithm for compiling rewrite rules into finite-state transducers, under the condition that they do not rewrite their noncontextual part 2. We here present a new algorithm for compiling such rewrite rules which is both simpler to understand and implement, and computationally more efficient. Clarity is important since, as pointed out by Kaplan and Kay (1994), the representation of rewrite rules by finite-state transducers involves many subtleties. Time and space efficiency of the compilation are also crucial. Using naive algorithms can be very time consuming and lead to very large machines (Liberman, 1994). In some applications such as those related to speech processing, one needs to use weighted rewrite rules, namely rewrite rules to which weights are associated. These weights are then used at the final stage of applications to output the most probable analysis. Weighted rewrite rules can be compiled into weighted finite-state transducers, namely transducers generalized by providing transitions with a weighted output, under the same context condition. These transducers are very useful in speech processing (Pereira et al., 1994). We briefly describe how we have augmented our algorithm to handle"
P96-1031,P94-1028,1,0.948561,"orithms. Further, many of our applications demand the ability to compile weighted rules into weighted FSTs, transducers generalized by providing transitions with weights. We have extended the algorithm to allow for this. 1. M o t i v a t i o n Rewrite rules are used in many areas of natural language and speech processing, including syntax, morphology, and phonology1. In interesting applications, the number of rules can be very large. It is then crucial to give a representation of these rules that leads to efficient programs. Finite-state transducers provide just such a compact representation (Mohri, 1994). They are used in various areas of natural language and speech processing because their increased computational power enables one to build very large machines to model interestingly complex linguistic phenomena. They also allow algebraic operations such as union, composition, and projection which are very useful in practice (Berstel, 1979; Eilenberg, 1974 1976). And, as originally shown by Johnson (1972), rewrite rules can be modeled as 1Parallel rewrite rules also have interesting applications in biology. In addition to their formal language theory interest, systems such as those of Aristid"
P96-1031,H94-1050,1,0.939556,"e algorithms can be very time consuming and lead to very large machines (Liberman, 1994). In some applications such as those related to speech processing, one needs to use weighted rewrite rules, namely rewrite rules to which weights are associated. These weights are then used at the final stage of applications to output the most probable analysis. Weighted rewrite rules can be compiled into weighted finite-state transducers, namely transducers generalized by providing transitions with a weighted output, under the same context condition. These transducers are very useful in speech processing (Pereira et al., 1994). We briefly describe how we have augmented our algorithm to handle the compilation of weighted rules into weighted finite-state transducers. In order to set the stage for our own contribution, we start by reviewing salient aspects of the Kaplan and Kay algorithm. Context-dependent rewrite rules are used in many areas of natural language and speech processing. Work in computational phonology has demonstrated that, given certain conditions, such rewrite rules can be represented as finite-state transducers (FSTs). We describe a new algorithm for compiling rewrite rules into FSTs. We show the alg"
Q16-1036,C69-0101,0,0.486948,"o categories such as cardinal numbers, times, and dates— each of which is semantically well-circumscribed— as semiotic classes. Some previous work on text normalization proposes minimally-supervised machine learning techniques for normalizing specific semiotic classes, such as abbreviations (e.g., Chang et al., 2002; Pennell and Liu, 2011; Roark and Sproat, 2014). This paper continues this tradition by contributing minimally-supervised models for normalization of cardinal number expressions (e.g., ninetyseven). Previous work on this semiotic class include formal linguistic studies by Corstius (1968) and Hurford (1975) and computational models proposed by Sproat (1996; 2010) and Kanis et al. (2005). Of all semiotic classes, numbers are by far the most important for speech, as cardinal (and ordinal) numbers are not only semiotic classes in their own right, but knowing how to verbalize numbers is important for most of the other classes: one cannot verbalize times, dates, measures, or currency expressions without knowing how to verbalize that language’s numbers as well. One computational approach to number name verbalization (Sproat, 1996; Kanis et al., 2005) employs a cascade of two finite-"
Q16-1036,P03-1054,0,0.0202342,"orithm could identify an even smaller training set. The grammar induction method used for the FST verbalizer is near to the simplest imaginable such procedure: it treats rules as well-formed if and only if they have at least one unambiguous occurrence in the training data. More sophisticated induction methods could be used to improve both generalization and robustness to errors in the training data. Generalization might be improved by methods that “hallucinate” unobserved productions (Mohri and Roark, 2006), and 516 robustness could be improved using manual or automated tree annotation (e.g., Klein and Manning, 2003; Petrov and Klein, 2007). We leave this for future work. Above, we focused solely on cardinal numbers, and specifically their citation forms. However, in all four languages studied here, ordinal numbers share the same factorization and differ only superficially from cardinals. In this case, the ordinal number verbalizer can be constructed by applying a trivial transduction to the cardinal number verbalizer. However, it is an open question whether this is a universal or whether there may be some languages in which the discrepancy is much greater, so that separate methods are necessary to const"
Q16-1036,D11-1140,0,0.0198449,"but as a sum if X > Y, as in vingt-quatre ‘24’. Thus the problem of number denormalization—that is, recovering the integer denoted by a verbalized number— can be thought of as a special case of grammar induction from pairs of natural language expressions and 6 Some languages make use of half-counting, or multiplication by one half (e.g., Welsh hanner cant, ‘50’, literally ‘half hundred’), or back-counting, i.e., subtraction (e.g., Latin undevīgintī, ‘19’, literally ‘one from twenty’; Menninger, 1969, 94f.). But these do not reduce the generality of the approach here. their denotations (e.g., Kwiatkowski et al., 2011). 4.1 FST model The complete model consists of four components: 1. A language-independent covering grammar F, transducing from integers expressed as digit sequences to the set of possible factorizations for that integer 2. A language-specific numeral map M, transducing from digit sequences to numerals 3. A language-specific verbalization grammar G, accepting only those factorizations which are licit in the target language 4. A language-specific lexical map L, transducing from sequences of numerals (e.g., 20) to number names (already defined) As the final component, the lexical map L, has alrea"
Q16-1036,N06-1040,0,0.0108511,"to be a case of the set cover problem—-which is NP-complete (Karp, 1972)—but it is plausible that a greedy algorithm could identify an even smaller training set. The grammar induction method used for the FST verbalizer is near to the simplest imaginable such procedure: it treats rules as well-formed if and only if they have at least one unambiguous occurrence in the training data. More sophisticated induction methods could be used to improve both generalization and robustness to errors in the training data. Generalization might be improved by methods that “hallucinate” unobserved productions (Mohri and Roark, 2006), and 516 robustness could be improved using manual or automated tree annotation (e.g., Klein and Manning, 2003; Petrov and Klein, 2007). We leave this for future work. Above, we focused solely on cardinal numbers, and specifically their citation forms. However, in all four languages studied here, ordinal numbers share the same factorization and differ only superficially from cardinals. In this case, the ordinal number verbalizer can be constructed by applying a trivial transduction to the cardinal number verbalizer. However, it is an open question whether this is a universal or whether there"
Q16-1036,N07-1051,0,0.0550971,"even smaller training set. The grammar induction method used for the FST verbalizer is near to the simplest imaginable such procedure: it treats rules as well-formed if and only if they have at least one unambiguous occurrence in the training data. More sophisticated induction methods could be used to improve both generalization and robustness to errors in the training data. Generalization might be improved by methods that “hallucinate” unobserved productions (Mohri and Roark, 2006), and 516 robustness could be improved using manual or automated tree annotation (e.g., Klein and Manning, 2003; Petrov and Klein, 2007). We leave this for future work. Above, we focused solely on cardinal numbers, and specifically their citation forms. However, in all four languages studied here, ordinal numbers share the same factorization and differ only superficially from cardinals. In this case, the ordinal number verbalizer can be constructed by applying a trivial transduction to the cardinal number verbalizer. However, it is an open question whether this is a universal or whether there may be some languages in which the discrepancy is much greater, so that separate methods are necessary to construct the ordinal verbaliz"
Q16-1036,P14-2060,1,0.848609,"rt et al., 2010) and text from social media sites (e.g., Yang and Eisenstein, 2013) has focused on detecting and expanding novel abbreviations (e.g., cn u plz hlp). Collectively, such conversions all fall under the rubric of text normalization (Sproat et al., 2001), but this term means radically different things in different applications. For instance, it is not necessary to detect and verbalize dates and times when preparing social media text for downstream information extraction, but this is essential for speech applications. While expanding novel abbreviations is also important for speech (Roark and Sproat, 2014), numbers, times, dates, measure phrases and the like are far more common in a wide variety of text genres. Following Taylor (2009), we refer to categories such as cardinal numbers, times, and dates— each of which is semantically well-circumscribed— as semiotic classes. Some previous work on text normalization proposes minimally-supervised machine learning techniques for normalizing specific semiotic classes, such as abbreviations (e.g., Chang et al., 2002; Pennell and Liu, 2011; Roark and Sproat, 2014). This paper continues this tradition by contributing minimally-supervised models for normal"
Q16-1036,N10-1023,0,0.154876,"a transducer that maps number names to digit sequences. (Invertibility is not a property of any RNN solution.) This allows one, with the help of the appropriate target-side language model, to convert a normalization system into a denormalization system, that maps from spoken to written form rather than from written to spoken. During ASR decoding, for example, it is often preferable to use spoken representations (e.g., twenty-three) rather than the written forms (e.g., 23), and then perform denormalization on the resulting transcripts so they can be displayed to users in a more-readable form (Shugrina, 2010; Vasserman et al., 2015). In ongoing work we are evaluating FST verbalizers for use in ASR denormalization. 6 Conclusions We have described two approaches to number normalization, a key component of speech recognition and synthesis systems. The first used a recurrent neural network and large amounts of training data, but very little knowledge about the problem space. The second used finite-state transducers and a learning method totally specialized for this domain but which requires very little training data. While the former approach is certainly more appealing given current trends in NLP, o"
Q16-1036,D13-1007,0,0.0580107,"ta than the former, making it particularly useful for low-resource languages. 1 Introduction Many speech and language applications require text tokens to be converted from one form to another. For example, in text-to-speech synthesis, one must convert digit sequences (32) into number names (thirtytwo), and appropriately verbalize date and time expressions (12:47 → twelve forty-seven) and abbreviations (kg → kilograms) while handling allomorphy and morphological concord (e.g., Sproat, 1996). Quite a bit of recent work on SMS (e.g., Beaufort et al., 2010) and text from social media sites (e.g., Yang and Eisenstein, 2013) has focused on detecting and expanding novel abbreviations (e.g., cn u plz hlp). Collectively, such conversions all fall under the rubric of text normalization (Sproat et al., 2001), but this term means radically different things in different applications. For instance, it is not necessary to detect and verbalize dates and times when preparing social media text for downstream information extraction, but this is essential for speech applications. While expanding novel abbreviations is also important for speech (Roark and Sproat, 2014), numbers, times, dates, measure phrases and the like are fa"
qian-etal-2010-python,W06-1630,1,\N,Missing
qian-etal-2010-python,P07-1015,1,\N,Missing
qian-etal-2010-python,P06-1103,0,\N,Missing
qian-etal-2010-python,P06-1010,1,\N,Missing
qian-etal-2010-python,P06-4018,0,\N,Missing
qian-etal-2010-python,W02-0109,0,\N,Missing
sproat-etal-2014-database,C10-2001,0,\N,Missing
W03-1719,1997.eamt-1.1,0,0.142338,"Missing"
W03-1719,O97-4003,0,0.0530522,"Word Segmentation Bakeoff, provides some analysis of the results, and makes specific recommendations for future bakeoffs. One thing we do not do here is get into the details of specific systems; each of the participants was required to provide a four page description of their system along with detailed discussion of their results, and these papers are published in this volume. 2 Details of the contest 2.1 Corpora The corpora are detailed in Table 1. Links to descriptions of the corpora can be found at http://www.sighan.org/bakeoff2003/ bakeoff_instr.html; publications on specific corpora are (Huang et al., 1997) (Academia Sinica), (Xia, 1999) (Chinese Treebank); the Beijing University standard is very similar to that outlined in (GB/T 13715–92, 1993). Table 1 lists the abbreviations for the four corpora that will be used throughout this paper. The suffixes “o” and “c” will be used to denote open and closed tracks, respectively: Thus “ASo,c” denotes the Academia Sinica corpus, both open and closed tracks; and “PKc” denotes the Beijing University corpus, closed track. During the course of this bakeoff, a number of inconsistencies in segmentation were noted in the CTB corpus by one of the participants."
W03-1719,O03-4001,0,0.082309,"three from Hong Kong, one from Japan, one from Singapore, one from Taiwan and four from the United States.   Secondly, as we have already noted, there are at least four distinct standards in active use in the sense that large corpora are being developed according to those standards; see Section 2.1. It has also been observed that different segmentation standards are appropriate for different purposes; that the segmentation standard that one might prefer for information retrieval applications is likely to be different from the one that one would prefer for text-to-speech synthesis; see (Wu, 2003) for useful discussion. Thus, while we do not subscribe to the view that any of the extant standards are, in fact, appropriate for any particular application, nevertheless, it seems desirable to have a contest where people are tested against more than one standard. A third point is that we decided early on that we would not be lenient in our scoring, so that alterMao native segmentations as in the case of Zedong, cited above, would not be allowed. While it would be fairly straightforward (in many cases) to automatically score both alternatives, we felt we could provide a more objective measure"
W06-1630,C00-2159,0,0.0780256,"ntity transliteration is the problem of producing, for a name in a source language, a set of one or more transliteration candidates in a target language. Previous work — e.g. (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) — has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a source-channel or MaxEnt-based model. Comparable corpora have been studied extensively in the literature — e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2004), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies — e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and ap"
W06-1630,P95-1050,0,0.0527938,"nciation-based methods — independently, and in combination. 2 Previous Work 3.1 Named entity transliteration is the problem of producing, for a name in a source language, a set of one or more transliteration candidates in a target language. Previous work — e.g. (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) — has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a source-channel or MaxEnt-based model. Comparable corpora have been studied extensively in the literature — e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2004), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies — e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Roscheisen, 1993"
W06-1630,P95-1032,0,0.156856,"on and pronunciation-based methods — independently, and in combination. 2 Previous Work 3.1 Named entity transliteration is the problem of producing, for a name in a source language, a set of one or more transliteration candidates in a target language. Previous work — e.g. (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) — has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a source-channel or MaxEnt-based model. Comparable corpora have been studied extensively in the literature — e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2004), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies — e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Rosc"
W06-1630,J96-3004,1,0.684007,"el, which does not depend on the sample size; or else one must expand the data set and hope for more occurrence. To generate the Hindi and Arabic candidates, all words from the same seven days were extracted. The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c 1 , c1 c2 , and c1 c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996). A sequence of three or more such characters from the list is taken as a possible name. The number of candidates for each target language is presented in the last column of Table 5. We measured the accuracy of transliteration by Mean Reciprocal Rank (MRR), a measure commonly used in information retrieval when there is precisely one correct answer (Kantor and Voorhees, 2000). We attempted to create a complete set of answers for 200 English names in our test set, but a small number of English names do not seem to have any standard transliteration in the target language according to the resource"
W06-1630,P06-1010,1,0.263416,"Missing"
W06-1630,C96-2098,0,0.0929203,"ed methods — independently, and in combination. 2 Previous Work 3.1 Named entity transliteration is the problem of producing, for a name in a source language, a set of one or more transliteration candidates in a target language. Previous work — e.g. (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) — has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a source-channel or MaxEnt-based model. Comparable corpora have been studied extensively in the literature — e.g.,(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2004), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies — e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment."
W06-1630,J93-1006,0,0.0471167,",(Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996; Franz et al., 1998; Ballesteros and Croft, 1998; Masuichi et al., 2000; Sadat et al., 2004), but transliteration in the context of comparable corpora has not been well addressed. The general idea of exploiting time correlations to acquire word translations from comparable corpora has been explored in several previous studies — e.g., (Fung, 1995; Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to the problem of transliteration; note that (Tao and Zhai, 2005) compares several different metrics for time correlation, as we also note below — and see (Sproat et al., 2006). Candidate scoring based on pronunciation Our phonetic transliteration score uses a standard string-alignment and alignment-scoring technique based on (Kruskal, 1999) in that the distance is determined by a combination of substitution, insertion and deletion costs. These costs are computed from a language-universal co"
W06-1630,P00-1027,0,0.0760244,"Missing"
W06-1630,N04-1003,0,\N,Missing
W06-1630,W02-0505,0,\N,Missing
W06-1630,P03-2025,0,\N,Missing
W06-1630,J98-4003,0,\N,Missing
W07-1711,P06-1038,0,0.0146439,"the target. They show that the method is capable of identifying polysemous English words. 2 Levinson (1999) presents an approach to WSD that is evaluated on English and Hebrew. He finds 50 most similar words to the target and clusters them into groups, the number of groups being the number of senses. He reports comparable results for the two languages, but he uses both morphologically and lexically ambiguous words. Moreover, the evaluation methodology focuses on the success of disambiguation for an ambiguous word, and reports the number of ambiguous words that were disambiguated successfully. Davidov and Rappoport (2006) describe an algorithm for unsupervised discovery of word categories and evaluate it on Russian and English corpora. However, the focus of their work is on the discovery of semantic categories and from the results they report for the two languages it is difficult to infer how the languages compare against each other. We conduct a more thorough evaluation. We also Related Work First, we describe several approaches to WSD that are most relevant to the present project: Since we are dealing with languages that do not have many linguistic resources available, we chose a most unsupervised, knowledge"
W07-1711,C04-1112,0,0.0283468,"viewed as clusters of words. 83 2.2 Cross-Linguistic Study of WSD control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more training data. The results indicate an improvement of this approach over classification based on wordforms. 3 Approach Our algorithm relies on the method for selection of relevant contextual terms and on distance measure between them introduced in (Sproat and van Santen, 1998) and on the approach described in (Sch¨utze, 1998), though the details of clustering differ slightly. Th"
W07-1711,J92-1001,0,0.0968866,"evaluation for two languages. Finally, we describe work that is concerned with the role of morphology for the task. 2.1 Approaches to Word Sense Discrimination Pantel and Lin (2002) learn word sense induction from an untagged corpus by finding the set of the most similar words to the target and by clustering the words. Each word cluster corresponds to a sense. Thus, senses are viewed as clusters of words. 83 2.2 Cross-Linguistic Study of WSD control cross-linguistically for number of training examples and level of ambiguity of selected words, as described in Section 4. 2.3 Morphology and WSD McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. She reports that morphology is useful, but the focus is on derivational morphology of the English language. In the present context, we are interested in the effect of inflectional morphology on WSD, especially for languages, such as Russian and Hebrew. Gaustad (2004) proposes a lemma-based approach to a Maximum Entropy Word Sense Disambiguation System for Dutch. She shows that collapsing wordforms of an ambiguous word yields a more robust classifier due to the availability of more"
W07-1711,W97-0322,0,0.0169377,"that we use. We then describe the experiments and the evaluation methodology in Sections 4 and 5, respectively. We conclude with a discussion of the results and directions for future work. Another approach is based on clustering the occurrences of an ambiguous word in a corpus into clusters that correspond to distinct senses of the word. Based on this approach, a sense is defined as a cluster of contexts of an ambiguous word. Each occurrence of an ambiguous word is represented as a vector of features, where features are based on terms occurring in the context of the target word. For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. Sch¨utze (1992) presents a method that explores the similarity between the context terms occurring around the target. This is accomplished by considering feature vectors of context terms of the ambiguous word. The algorithm is evaluated on natural and artificially-constructed ambiguous English words. Sproat and van Santen (1998) introduce a technique for automatic detection of ambiguous words in a corpus and measuring their degree of polysemy. This technique employs a similar"
W07-1711,J98-1004,0,0.0660193,"Missing"
W09-3505,P04-1021,0,0.348981,"Missing"
W09-3505,P07-1015,1,0.828328,"owel forms, in order to map back from phonemic transcription into the script form, it is necessary to know whether a vowel comes after a consonant or not, in order to select the correct form. These and other constraints were implemented with a simple hand-constructed WFST for each script. The main transliteration model for the Standard Run was a 6-gram pair language model trained on an alignment of English letters to Hindi, Kannada 2 Korean For Korean, we created a mapping between each Hangul glyph and its phonetic transcription in WorldBet (Hieronymus, 1993) based on the tables from Unitran (Yoon et al., 2007). Vowel-initial syllables were augmented with a “0” at the beginning of the syllable, to avoid spurious resyllabifications: Abbott should be 애버트, never 앱엍으. We also filtered the set of possible Hangul syllable combinations, since certain syllables are never used in transliterations, e.g. any with two consonants in the coda. The mapping 32 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 32–35, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP sponding Chinese hanzi strings using the same memoryless monotonic alignment model as before. We then built standard n-gram m"
W09-3505,C04-1103,0,0.0296642,"ion found in the LDC list. For our first Non-Standard Run, we trained a 7gram language model based on the Shared Task training data (31,961 name pairs) plus an additional 95,576 name pairs from the intersection of the LDC list and the Standard model predictions. Since the selection of additional training data was, by design, very conservative, we got a small improvement over the Standard Run. Chinese For Chinese, we built a direct stochastic model between strings of Latin characters representing the English names and strings of hanzi representing their Chinese transcription. It is well known (Zhang et al., 2004) that the direct approach produces significantly better transcription quality than indirect approaches based on intermediate pinyin or phoneme representations. This observation is consistent with our own experience during system development. In our version of the direct approach, we first aligned the English letter strings with their correThe reason for this cautious approach was that the additional LDC data did not match the provided training and development data very well, partly due to noise, partly due to different transcription conventions. For example, the Pinyin syllable bo´ is predomin"
W09-3505,W09-3501,0,\N,Missing
W09-3505,W09-3502,0,\N,Missing
W11-0147,W10-0701,0,0.037374,"nual results with the output of some automatic filtration techniques which use WN similarity and corpus association measures. 2 Data collection from Amazon’s Mechanical Turk Amazon’s Mechanical Turk is an online marketplace that provides a way to pay people small amounts of money to perform tasks that are simple for humans but difficult for computers. Examples of these Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing feedback on the relevance of results for a search query. The highly accurate, cheap and efficient results of several NLP tasks (Callison-Burch and Dredze, 2010) have encouraged us to explore using AMT. We designed three separate tasks to collect information about typical nearby objects, typical location and typical parts of the objects of our library. For task 1, we asked the workers to name 10 common objects that they might typically find around or near a given object. For task 2, we asked the workers to name 10 locations in which they might typically find a given object and in task 3, we asked the workers to list 10 parts of a given object. Given that some objects might not consist of 10 parts, (i.e, they are very simple objects), we wanted the wor"
W11-0147,J93-1003,0,0.0478402,"in unigram and bigram portions of the corpus and then the number of times the two words co-occur within a +/- 4-word window in the 5-gram portion of the corpus. We also computed the sentential co-occurrences of each target-response pair (i.e. the number of sentences in which the target or the response words appear and the number of sentences in which both words occur together) on the English Gigaword corpus (LDC2007T07) which is a 1 billion word corpus of articles marked up from English press texts (mainly the New York Times). Based on these counts, we used log-likelihood and log-odds ratio (Dunning, 1993) to compute the association between the two words. 4.3 Discussion and evaluation of automatic filtaration techniques The collected responses of each AMT task were ranked separately by each of the above similarity and association measures. We classify the ranked responses into “keep” (higher-scoring) and “reject” (lowerscoring) classes by defining a specific threshold for each list. Then we evaluated the accuracy of each filtration approach by computing their precision and recall on correct “keep” items (see table 2). In this table the baseline score shows the accuracy of the responses of each"
W11-2303,J93-2004,0,0.039489,"ions. 27 The idea in using this data was to provide some number of utterances of dialog context (from the 10 previous dialog turns), and then ask subjects to provide word completions for some number of subsequent utterances. While the Switchboard corpus does represent the kind of conversational dialog we are interested in, it is a spoken language corpus, yet we are modeling written (typed) language. The difference between written and spoken language does present something of an issue for our task. To mitigate this mismatch somewhat, we made use of the Switchboard section of the Penn Treebank (Marcus et al., 1993), which contains syntactic annotations of the Switchboard transcripts, including explicit marking of disfluencies (“EDITED” non-terminals in the treebank), interjections or parentheticals such as “I mean” or “you know”. Using these syntactic annotations, we produced edited transcripts that omit much of the spoken language specific phenomena, thus providing a closer approximation to the kind of written dialogs we would like to simulate. In addition, we decased the corpus and removed all characters except the following: the 26 letters of the English alphabet, the apostrophe, the space, and the d"
W11-2303,W09-0601,0,0.0161603,"ginbotham (2008; 2009) proposed a novel method that uses automatic speech recognition (ASR) on the speech of the communication partner, extracts noun phrases from the speech, and presents those noun phrases on the AAC device, with frame sentences that the AAC user can select. Thus if the communication partner says “Paris”, the AAC user will be able to select from phrases like “Tell me more about Paris” or “I want to talk about Paris”. This can speed up the conversation by providing topically-relevant responses. Perhaps the most elaborate system of this kind is the How Was School Today system (Reiter et al., 2009). This system, which is geared towards children with severe communication disabilities, uses data from sensors, the Web, and other sources as input for a natural language generation system. The system acquires information about the child’s day in school: which classes he or she attended, what activities there were, information about visitors, food choices at the cafeteria, and so forth. The data are then used to generate natural language sentences, which are converted to speech via a speech synthesizer. At the end of the day, the child uses a menu to select sentences that he or she wants the s"
W11-2303,N07-2044,0,0.100839,"ugh alternatives (Lesher et al., 1998). Typing speed is a challenge, yet is critically important for usability, and as a result there is a significant line of research into 22 {gibbons,mfo}@ohsu.edu the utility of statistical language models for improving typing speed (McCoy et al., 2007; Koester and Levine, 1996; Koester and Levine, 1997; Koester and Levine, 1998). Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007; Todman et al., 2008). To the extent that the predictions are accurate, the number of keystrokes required to type a message can be dramatically reduced, greatly speeding typing. AAC devices for spontaneous and novel text generation are intended to empower the user of the system, to place them in control of their own communication, and reduce their reliance on others for message formulation. As a result, all such devices (much like standard personal computers) are built for a single user, with a single keyboard and/or alternative input interface, which is driven b"
W11-2303,D07-1053,0,0.523895,"sher et al., 1998). Typing speed is a challenge, yet is critically important for usability, and as a result there is a significant line of research into 22 {gibbons,mfo}@ohsu.edu the utility of statistical language models for improving typing speed (McCoy et al., 2007; Koester and Levine, 1996; Koester and Levine, 1997; Koester and Levine, 1998). Methods of word, symbol, phrase and message prediction via statistical language models are widespread in both direct selection and scanning devices (Darragh et al., 1990; Li and Hirst, 2005; Trost et al., 2005; Trnka et al., 2006; Trnka et al., 2007; Wandmacher and Antoine, 2007; Todman et al., 2008). To the extent that the predictions are accurate, the number of keystrokes required to type a message can be dramatically reduced, greatly speeding typing. AAC devices for spontaneous and novel text generation are intended to empower the user of the system, to place them in control of their own communication, and reduce their reliance on others for message formulation. As a result, all such devices (much like standard personal computers) are built for a single user, with a single keyboard and/or alternative input interface, which is driven by the user of the system. The"
W12-2107,P11-1045,1,\N,Missing
W12-2903,W10-1302,0,0.0200229,"1). One of the limitations of this approach is that information associated with each word is primarily hand-coded on the basis of intuition; as a result, the system cannot handle the problem of unrestricted vocabulary. Similar issues arise in semantic authoring systems (Netzer and Elhadad, 2006), where at each step of the sentence creation process, the system offers possible symbols for a small set of concepts, and the user can select which is intended. Recent work has also tried to handle the complexity of conversation by providing full sentences with slots that can be filled in by the user. Dempster et al. (2010) define an ontology where pieces of handcoded knowledge are stored and realized within several syntactic templates. Users can generate utterances by entering utterance types and topics, and these are filled into the templates. The Frametalker system (Higginbotham et al., 1999) uses contextual frames — basic sentences for different contexts — with a set vocabulary for each. The intuition be10 hind this system is that there are typical linguistic structures for different situations and the kinds of words that the user will need to fill in will be semantically related to the context. Wisenburn an"
W12-2903,P05-1045,0,0.00349511,"sentence of the next utterance is much more likely to have some values than others. A useful measure of this is the difference between the entropy of the predicted feature values fi of a feature g: H(g) = − n X log(p(fi )) · p(fi ) (1) i=0 4 Data Analysis, Feature Extraction and Utterance Prediction and the maximum possible entropy of g given n predicted features, namely: Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011). From the output of the Stanford tools, the following features were extracted for each utterance: word bigrams (pairs of adjacent words); dependency-head relations, along with the type of dependency relation (basically, governors — e.g., verbs — and their dependents — e.g., nouns); named entities (persons, organizations, etc.); and the whole utterance. Extracted named entities include noun phrases that were explicitly tagged as named entities, as well as any phrases that were marked as coreferential with named entities. Thus if the pronoun she oc"
W12-2903,D11-1108,0,0.0449001,"Missing"
W12-2903,P03-1054,0,0.00591033,"that if the turn has a given value, then the first sentence of the next utterance is much more likely to have some values than others. A useful measure of this is the difference between the entropy of the predicted feature values fi of a feature g: H(g) = − n X log(p(fi )) · p(fi ) (1) i=0 4 Data Analysis, Feature Extraction and Utterance Prediction and the maximum possible entropy of g given n predicted features, namely: Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011). From the output of the Stanford tools, the following features were extracted for each utterance: word bigrams (pairs of adjacent words); dependency-head relations, along with the type of dependency relation (basically, governors — e.g., verbs — and their dependents — e.g., nouns); named entities (persons, organizations, etc.); and the whole utterance. Extracted named entities include noun phrases that were explicitly tagged as named entities, as well as any phrases that were marked as coreferential"
W12-2903,W11-1902,0,0.0235083,"Missing"
W12-2903,E12-1076,1,0.784132,"Missing"
W12-2903,N06-2027,0,0.0206745,"ut. One approach relies on telegraphic input, where full sentences are constructed from a set of uninflected words, as in the Compansion system (McCoy et al., 1998). This system employs a semantic parser to capture the meaning of the input words and generates using the Functional Unification Formalism (FUF) system (Elhadad, 1991). One of the limitations of this approach is that information associated with each word is primarily hand-coded on the basis of intuition; as a result, the system cannot handle the problem of unrestricted vocabulary. Similar issues arise in semantic authoring systems (Netzer and Elhadad, 2006), where at each step of the sentence creation process, the system offers possible symbols for a small set of concepts, and the user can select which is intended. Recent work has also tried to handle the complexity of conversation by providing full sentences with slots that can be filled in by the user. Dempster et al. (2010) define an ontology where pieces of handcoded knowledge are stored and realized within several syntactic templates. Users can generate utterances by entering utterance types and topics, and these are filled into the templates. The Frametalker system (Higginbotham et al., 19"
W12-2903,W11-2303,1,0.830693,"this work. For example, particular classes of response types, comprising a variety of related utterances, may be predictable using the extracted features. Finally, we have assumed for this discussion that the AAC system is only within the control of the impaired user. There is no reason to make that assumption in general: many AAC situations in real life involve a helper who will often co-construct with the impaired user. Such helpers usually know the impaired user very well and can often make reasonable guesses as to the whole utterance intended by the impaired user. Recent work reported in Roark et al. (2011) suggests one way in which the results of a language modeling system and those of a human coconstructor may be integrated into a single system, and such an approach could easily be applied here. 7 Conclusions We have proposed and evaluated an approach to whole utterance prediction for AAC. While the approach is fairly simple, it is able to generate correct or at least reasonable responses in some cases. Such a system could be used in conjunction with other techniques, such as language-model-based prediction, or co-construction. One of the potentially useful side-effects of this work is an esti"
W12-2903,N03-1033,0,0.0229669,"e values that are highly skewed in their predictions, meaning that if the turn has a given value, then the first sentence of the next utterance is much more likely to have some values than others. A useful measure of this is the difference between the entropy of the predicted feature values fi of a feature g: H(g) = − n X log(p(fi )) · p(fi ) (1) i=0 4 Data Analysis, Feature Extraction and Utterance Prediction and the maximum possible entropy of g given n predicted features, namely: Each dialog was processed using the Stanford Core NLP tools. The Stanford tools perform part of speech tagging (Toutanova et al., 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al., 2005), and coreference resolution (Lee et al., 2011). From the output of the Stanford tools, the following features were extracted for each utterance: word bigrams (pairs of adjacent words); dependency-head relations, along with the type of dependency relation (basically, governors — e.g., verbs — and their dependents — e.g., nouns); named entities (persons, organizations, etc.); and the whole utterance. Extracted named entities include noun phrases that were explicitly tagged as named enti"
W12-2903,N07-2044,0,0.0570455,"Missing"
W12-2903,D07-1053,0,0.0475207,"Missing"
W14-3206,N13-1021,1,0.876381,"Missing"
W14-3206,N13-1084,1,0.777854,"Missing"
W15-1214,W02-1001,0,0.0325189,"milarity of the two turns I and J by the weighted rela119 wj ∈J (5) P S(wi , J) 1  wi ∈I P KBS(I, J)= + 2 idfwi wi ∈I P wj ∈J S(wj , I) P wj ∈J  idfwj (6) Lin’s universal similarity can only be applied to word pairs with the same part-of-speech (POS). For automatic POS tagging of the ADOS corpus, we trained a multi-class classifier (Yarmohammadi, 2014) from labeled training data from the CHILDES corpus of transcripts of children’s conversational speech (MacWhinney, 2000). The classifier uses a discriminative linear model, learning the model parameters with the averaged perceptron algorithm (Collins, 2002). The feature set includes bigrams of surrounding words, a window of size 2 of the next 120 0.10 0.08 0.06 0.04 &lt;D !1 2 8 !6 4 32 64 Turn Distance Window &lt;D !3 2 16 &lt;D !1 6 D 8&lt; !8 D 4&lt; !4 D 2&lt; As described in Section 3, we use our measures to calculate the similarity scores of all turn pairs for each distance window. Table 1 shows examples of similar turn pairs in the four distance windows based on the Weighted Jaccard Similarity Coefficient score. We then calculate the SOR of each child in each given distance window by averaging the similarity scores of turn pairs in that window. Finally, we"
W15-1214,W04-1013,0,0.0204762,"A second area to investigate in the future is determining the children’s conversation topics, especially the ones that are repeated. We could combine the child specificity scores such as IDF with the highly overlapping lexical items across different turns. We could also use manual annotation and clinical impression to determine if a child has a particular (idiosyncratic) topic of interest . We could then compare these annotations with the findings from our automated measures. Third, we are also interested in trying additional similarity measures including BLEU (Papineni et al., 2002), ROUGE, (Lin, 2004), and Latent Semantic Analysis (Deerwester et al., 1990) to verify the robustness of our findings even further. Finally, we plan to apply our methods to the output of Automatic Speech Recognition (ASR) systems to eliminate the transcription process. Measuring semantic similarity on ASR output will be an interesting challenge since it will likely contain word errors especially in children’s spontaneous speech. Acknowledgments This work was supported in part by NSF grant #BCS-0826654, and NIH NIDCD grants #R01122 DC007129 and #1R01DC012033-01. Any opinions, findings, conclusions or recommendatio"
W15-1214,P02-1040,0,0.103155,"ith the examiner in a dialogue. A second area to investigate in the future is determining the children’s conversation topics, especially the ones that are repeated. We could combine the child specificity scores such as IDF with the highly overlapping lexical items across different turns. We could also use manual annotation and clinical impression to determine if a child has a particular (idiosyncratic) topic of interest . We could then compare these annotations with the findings from our automated measures. Third, we are also interested in trying additional similarity measures including BLEU (Papineni et al., 2002), ROUGE, (Lin, 2004), and Latent Semantic Analysis (Deerwester et al., 1990) to verify the robustness of our findings even further. Finally, we plan to apply our methods to the output of Automatic Speech Recognition (ASR) systems to eliminate the transcription process. Measuring semantic similarity on ASR output will be an interesting challenge since it will likely contain word errors especially in children’s spontaneous speech. Acknowledgments This work was supported in part by NSF grant #BCS-0826654, and NIH NIDCD grants #R01122 DC007129 and #1R01DC012033-01. Any opinions, findings, conclusi"
W15-1214,N13-1084,1,0.878902,"Missing"
