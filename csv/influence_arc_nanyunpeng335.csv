2020.acl-main.711,P19-1470,0,0.263513,"s. We make the code and data from our experiments publicly available. 1 2. Retrieval of Commonsense Context: Adding commonsense context could be important to make explicit the semantic incongruity factor (e.g., GenSarc4 vs. GenSarc3 in Table 1), or could enhance the humorous effect of the generated sarcastic message (e.g., GenSarc2 vs. GenSarc1 in Table 1). We propose an approach where retrieved relevant commonsense context sentences are to be added to the generated sarcastic message. At first, we use a pre-trained language model fine-tuned on the ConceptNet (Speer et al., 2017) called COMET (Bosselut et al., 2019) to generate relevant commonsense knowledge. COMET gives us that, “inherited unfavorable genes from my mother” causes “to be ugly” or that “getting sick from fast food” causes “stomach ache” (Section 4.2.1). The derived commonsense concept is then used to retrieve relevant sentences — from a corpus — that could be added to the sentence obtained through reversal of valence (e.g., “Stomach ache is just an additional side effect” in Table 1) (Section 4.2.2). 3. Ranking of Semantic Incongruity: The previous module generates a list of candidate commonsense contexts. Next, we measure contradiction b"
2020.acl-main.711,D15-1075,0,0.0150114,"Corrections System 3 (Zhao et al., 2019) to correct any pronoun or gender specific errors introduced by the replacements. 4.3 Ranking for Semantic Incongruity After the grammatical error correction, the next step is to select the best context sentence from the retrieved results. Since we expect the context sentences to be incongruous with the sentence generated by the reversal of valence approach (Section 4.1), we rank the context sentences by semantic incongruity scores and select the best candidate. We frame the problem of semantic incongruity based on the Natural Language Inference (NLI) (Bowman et al., 2015) task. The Multi-Genre NLI (Williams et al., 2018) covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization, making it an ideal choice as our NLI Dataset. We first fine-tune RoBERTa-large (Liu et al., 2019), a state-of-the-art pre-trained language model for a 3-way classification (i.e., contradiction, entailment, and neutral) by training on the Multi-NLI dataset. Next, for each retrieved sentence, we treat it as the premise and the sentence generated by the reversal of valence as the hypothesis, and thus, obtain a contradiction score from the t"
2020.acl-main.711,W17-5523,1,0.83734,"eakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4) be aimed at some ta"
2020.acl-main.711,P11-2102,1,0.783483,"rous effect. Introduction Studies have shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the"
2020.acl-main.711,W10-2914,0,0.103038,"create incongruity or enhance the humorous effect. Introduction Studies have shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based"
2020.acl-main.711,esuli-sebastiani-2006-sentiwordnet,0,0.0124467,"onic criticism (Kreuz and Link, 2002)). This observation is also supported by research on sarcasm detection, particularly on social media. Hence, for our sarcasm generation task, we focus on transforming a literal utterance with negative valence into positive valence. To implement the reversal of valence, as highlighted in the yellow background in Figure 1, we first identify the evaluative words and replace them with their lexical antonyms using WordNet (Miller, 1995). As we expect the evaluative words to be negative words, we rely on the word level negative scores obtained from SentiWordNet (Esuli and Sebastiani, 2006). In the absence of words with negative polarity, we check if there is the negation word not or words ending with n’t and remove these words. In case there are both negative words and not (or words ending in n’t), we handle only one of them. Given the non sarcastic example “zero visibility in fog makes driving difficult” shown in Figure 1 and which we use as our running example, the reversal of valence module generates “zero visibility in fog makes driving easy”. 4.2 Retrieval of Commonsense Context As discussed before, a straightforward reversal of valence might not generate sarcastic message"
2020.acl-main.711,D17-1050,0,0.0364701,"eativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4)"
2020.acl-main.711,J18-4009,1,0.883065,"Missing"
2020.acl-main.711,D15-1116,1,0.901471,"shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared common"
2020.acl-main.711,2020.scil-1.10,1,0.796904,"Missing"
2020.acl-main.711,P15-2124,0,0.229843,"of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowl"
2020.acl-main.711,N18-1169,0,0.0885691,"Missing"
2020.acl-main.711,2021.ccl-1.108,0,0.0836509,"Missing"
2020.acl-main.711,D19-1636,0,0.326336,"h as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4) be aimed at some target, and 5) be relevant to the communicative situation in some way. To simplify the problem, we foc"
2020.acl-main.711,P02-1040,0,0.112297,"oRV): This model only retrieves commonsense context and ranks them based on semantic incongruity. 4. No Semantic Incongruity (NSI): This model relies only on the reversal of valence and retrieval of commonsense context, without ranking based on semantic incongruity. A randomly selected retrieved sentence is used. 5. MTS2019: We make use of the model released by Mishra et al. (2019) as it is the stateof-the-art sarcasm generation system.6 6. Human (Gold) Sarcasm: As described in Section 5.1, we have gold sarcasm created by humans for every non-sarcastic utterance. 5.3 Evaluation Criteria BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metric for generation tasks such as Machine Translation. However, for creative text generation, it is not ideal to expect significant n-gram overlaps between the machinegenerated and the gold-standard utterances. Hence, we performed a human evaluation. We evaluate a total of 900 generated utterances since our ablation study consisted of six different systems with 150 utterances each. Sarcasm is often linked with intelligence, creativity, and wit; thus we propose a set of 4 criteria to evaluate the generated output: (1) Creativity (“How creat"
2020.acl-main.711,N19-1014,0,0.0394513,"ve any pronoun, but the retrieved sentence does, we simply change that pronoun to “I”. For example, if the non-sarcastic input sentence is “Ignoring texts is literally the worst part of communication.” and the retrieved commonsense sentence is “He has never suffered the torment of rejection.”, we modify the retrieved sentence to “I have never suffered the torment of rejection.” to have consistency among the pronoun use. After correcting the pronouns and proper names (in the same way as pronoun correction), we feed the corrected sentences into the Neural Grammatical Error Corrections System 3 (Zhao et al., 2019) to correct any pronoun or gender specific errors introduced by the replacements. 4.3 Ranking for Semantic Incongruity After the grammatical error correction, the next step is to select the best context sentence from the retrieved results. Since we expect the context sentences to be incongruous with the sentence generated by the reversal of valence approach (Section 4.1), we rank the context sentences by semantic incongruity scores and select the best candidate. We frame the problem of semantic incongruity based on the Natural Language Inference (NLI) (Bowman et al., 2015) task. The Multi-Genr"
2020.acl-main.711,P17-1155,0,0.251405,"odule 1) and select the commonsense context that received the highest contradiction score. Finally, we concatenate the selected context to the sentence obtained through reversal of valence. Here, conceptually, contradiction detection is aimed to capture the semantic incongruity between the output of valence reversal and its 2 2.1 Related Work Sarcasm Generation Research on sarcasm generation is in its infancy. Joshi et al. (2015a) proposed SarcasmBot, a sarcasm generation system that implements eight rulebased sarcasm generators, each of which generates a certain type of sarcastic expression. Peled and Reichart (2017) introduced a novel task of sarcasm interpretation, defined as the generation of a nonsarcastic utterance conveying the same message as the original sarcastic one. They use supervised machine translation models for the same in presence of parallel data. However, it is impractical to assume the existence of large corpora for training supervised generative models using deep neural nets; we hence resort to unsupervised approaches. Mishra et al. (2019) employed reinforced neural seq2seq learning and information retrieval based approaches to generate sarcasm. Their models are trained using only unl"
2020.acl-main.711,D13-1066,0,0.137191,"Missing"
2020.acl-main.711,W18-0902,0,0.0391012,"text generation, it is not ideal to expect significant n-gram overlaps between the machinegenerated and the gold-standard utterances. Hence, we performed a human evaluation. We evaluate a total of 900 generated utterances since our ablation study consisted of six different systems with 150 utterances each. Sarcasm is often linked with intelligence, creativity, and wit; thus we propose a set of 4 criteria to evaluate the generated output: (1) Creativity (“How creative are the utterances ?”), (2) Sarcasticness (“How sarcastic are the utterances ?”), (3) Humour (“How funny are the sentences ?”) (Skalicky and Crossley, 2018), and (4) Grammaticality (“How grammatical are the sentences ?”). We design a MTurk task where Turkers were asked to rate outputs from all the six systems. Each Turker was given the non-sarcastic utterance as well as a group of sarcastic utterances generated by all the six systems (randomly shuffled). Each criteria was rated on a scale from 1 (not at all) to 5 (very). Finally, each utterance was rated by three individual Turkers. 55, 59, 66, and 60 Turkers 7981 6 https://github.com/TarunTater/sarcasm generation System State-of-the-art (Mishra et al., 2019) Human Generated Reversal of Valence ("
2020.acl-main.711,J18-4010,0,0.0295381,"Missing"
2020.acl-main.711,N18-1101,0,0.0779299,"re not concerned with the fifth characteristic, while the first and to some degree, the fourth are specified by the input (literal) utterances. 7976 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7976–7986 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Given the lack of “training” data for the sarcasm generation task, we propose a novel unsupervised approach that has three main modules guided by the above mentioned sarcasm factors: context. Contradiction scores are obtained from a model trained on the Multi-Genre NLI Corpus (Williams et al., 2018) (Section 4.3). 1. Reversal of Valence: To generate sarcastic utterances that satisfy the second characteristic we identify the evaluative word and use negation or lexical antonyms to generate the sarcastic utterance by reversing the valence (Section 4.1). For example, given, “I hate getting sick from fast food” this module will generate “I love getting sick from fast food” (GenSarc1 in Table 1). We test our approach on 150 non-sarcastic utterances randomly sampled from two existing data sets. We conduct human evaluation using several criteria: 1) how sarcastic is the generated message; 2) how"
2020.emnlp-main.351,W18-1505,1,0.774396,"tween the three systems, as it will give the plot more influence over the story surface realisation and ensure that plot improvements appear downstream. 7 Related Work Story Generation without Plots. Diverse efforts have focused on generating stories. Fan et al. (2018) re-purpose an approach for Neural Machine Translation to translate from prompt to a story via Convolutional Seq2Seq models. Guan et al. (2020); Mao et al. (2019) use a similar approach, however they incorporate structured commonsense knowledge from external datasets or knowledge bases to improve a story generated from a prompt. Peng et al. (2018) add control to the story ending valence. Story Generation with Plots. Riedl and Young (2010) use refinement search as a technique to balance between character and plot for solving the narrative generation problem. Li et al. (2013) use plot graphs for story generation that model the intended logical flow of events in the virtual world as a set of precedence constraints between plot events. Martin et al. (2018) decompose the problem of story generation into generation of successive events (event2event) followed by generation of natural language sentences from events (event2sentence). Ammanabrol"
2020.emnlp-main.351,K19-1079,0,0.0339709,"t content plan, or that plan in an unprincipled way.1 1. 2. 3. 4. event choice and arrangement character relevant content2 diction Despite many recent advances in Natural Language Generation, successful creative narrative composition remains elusive. Current neural approaches are plagued by difficulty in mastering structure, will veer between topics, and lack long-range cohesion. They successfully imitate the fluency and style of human writing, but on closer inspection sentences do not fit together to form a whole, and the reader is left with the impression that the generation has no content (See et al., 2019). This lack of structure also degrades the relevance of generations conditioned on a prompt or other source text - a strong language model will repeat key phrases from a given prompt but will not remain on topic. These issues are illustrated in the Naive Generated Story in Table 1, where many of the sentences individually are fine, but do not fit together as one story, and do not all relate to the prompt. An amateur masters skills later in the list, but mastery of event choice and event arrangement is what distinguishes a good writer (Aristotle). Next is character, then relevance, and only fin"
2020.emnlp-main.461,S13-2002,0,0.0245351,"P n , rˆ), we assume there is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained language models to build an end-to-end system. Our wo"
2020.emnlp-main.461,S15-2136,0,0.0564448,"Missing"
2020.emnlp-main.461,S17-2093,0,0.0349871,"Missing"
2020.emnlp-main.461,P14-2082,0,0.0216471,"in Table 6 in the Appendix. 5 Experimental Setup This section describes the two event temporal relation datasets used in this paper and then explains the evaluation metrics. 5.1 Data TimeBank-Dense. Temporal relation corpora such as TimeBank (Pustejovsky et al., 2003) and RED (O’Gorman et al., 2016) consist of expert annotations of news articles. The common issue of these corpora is missing annotations. Collecting densely annotated temporal relation corpora with all events and relations fully annotated is a challenging task as annotators could easily overlook some facts (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017). 5721 TimeBank-Dense Event Feature-based Benchmark Han et al. (2019b) End-to-end Baseline End-to-end + Inference 2012 i2b2 Challenge (I2 B 2-T EMPORAL) Relation Event Relation (TempEval Metrics) F1 R P F1 Span F1 Type Accuracy R P F1 87.4 90.9 90.3 90.3 43.8 52.6 51.5 53.4 35.7 46.5 45.9 47.9 39.4 49.4 48.5 50.5 90.1 87.8 87.8 86.0 87.8 87.8 37.8 73.4 73.3 74.0 51.8 76.3 79.9 80.8 43.0 74.8 76.5 77.3 Table 2: Overall experiment results: per MacNemar’s test, the improvements against the end-to-end baseline models by adding inference with distributiona"
2020.emnlp-main.461,S13-2012,0,0.0268276,"event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained language models to build an end-to-end system. Our work differs from these prior work in that we build a structur"
2020.emnlp-main.461,Q14-1022,0,0.706129,"ndix. 5 Experimental Setup This section describes the two event temporal relation datasets used in this paper and then explains the evaluation metrics. 5.1 Data TimeBank-Dense. Temporal relation corpora such as TimeBank (Pustejovsky et al., 2003) and RED (O’Gorman et al., 2016) consist of expert annotations of news articles. The common issue of these corpora is missing annotations. Collecting densely annotated temporal relation corpora with all events and relations fully annotated is a challenging task as annotators could easily overlook some facts (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017). 5721 TimeBank-Dense Event Feature-based Benchmark Han et al. (2019b) End-to-end Baseline End-to-end + Inference 2012 i2b2 Challenge (I2 B 2-T EMPORAL) Relation Event Relation (TempEval Metrics) F1 R P F1 Span F1 Type Accuracy R P F1 87.4 90.9 90.3 90.3 43.8 52.6 51.5 53.4 35.7 46.5 45.9 47.9 39.4 49.4 48.5 50.5 90.1 87.8 87.8 86.0 87.8 87.8 37.8 73.4 73.3 74.0 51.8 76.3 79.9 80.8 43.0 74.8 76.5 77.3 Table 2: Overall experiment results: per MacNemar’s test, the improvements against the end-to-end baseline models by adding inference with distributional constraints are both"
2020.emnlp-main.461,P07-2044,0,0.0297079,"TimeBank-Dense does not predict event types. That is, when counting the triplet (P m , P n , rˆ), we assume there is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neu"
2020.emnlp-main.461,P17-2001,0,0.0811479,"ll experiment results: per MacNemar’s test, the improvements against the end-to-end baseline models by adding inference with distributional constraints are both statistically significant for TimeBank-Dense (p-value < 0.005) and I2 B 2-T EMPORAL (p-value < 0.0005). For I2 B 2-T EMPORAL, our end-to-end system is optimized for the F1 score of the gold pairs. The TimeBank-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and this dataset has been widely evaluated on this task (Chambers et al., 2014; Ning et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018). Temporal relations consist of BEFORE, AFTER, INCLUDES, INCLUDED, SIMULTANEOUS, and VAGUE. Moreover, each event has several properties, e.g., type, tense, and polarity. Event types include occurrence, action, reporting, state, etc. Event pairs that are more than 2 sentences away are not annotated. I2 B 2-T EMPORAL. In the clinical domain, one of the earliest event temporal datasets was provided in the 2012 Informatics for Integrating Biology and the Bedside (i2b2) Challenge on NLP for Clinical Records (Sun et al., 2013). Clinical events are categorized into 6 types:"
2020.emnlp-main.461,S16-1192,0,0.0162143,"un et al., 2013)) is one of the earliest efforts to advance event temporal relation extraction of clinical data. The challenge hosted three tasks on event (and event property) classification, temporal relation extraction, and the end-to-end track. Following this early effort, a series of clinical event temporal relation challenges were created in the following years ((Bethard et al., 2015, 2016, 2017)). However, data in these challenges are relatively hard to acquire, and therefore they are not used in this paper. As in the news data, traditional machine learning approaches (Lee et al., 2016; Chikka, 2016; Xu et al., 2013; Tang et al., 2013; Savova et al., 2010) that tackle the end-to-end event and temporal relation extraction problem require timeconsuming feature engineering such as collecting lexical and syntax features. Some recent work (Dligach et al., 2017; Leeuwenberg and Moens, 2017; Galvan et al., 2018) apply neural network-based methods to model the temporal relations, but are not capable of incorporating prior knowledge about clinical events and temporal relations as proposed by our framework. 9 Conclusion In conclusion, we propose a general framework that augments deep neural networ"
2020.emnlp-main.461,E17-2118,0,0.0248793,"wing this early effort, a series of clinical event temporal relation challenges were created in the following years ((Bethard et al., 2015, 2016, 2017)). However, data in these challenges are relatively hard to acquire, and therefore they are not used in this paper. As in the news data, traditional machine learning approaches (Lee et al., 2016; Chikka, 2016; Xu et al., 2013; Tang et al., 2013; Savova et al., 2010) that tackle the end-to-end event and temporal relation extraction problem require timeconsuming feature engineering such as collecting lexical and syntax features. Some recent work (Dligach et al., 2017; Leeuwenberg and Moens, 2017; Galvan et al., 2018) apply neural network-based methods to model the temporal relations, but are not capable of incorporating prior knowledge about clinical events and temporal relations as proposed by our framework. 9 Conclusion In conclusion, we propose a general framework that augments deep neural networks with distributional constraints constructed using probabilistic domain knowledge. We apply it in the setting of end-to-end temporal relation extraction task with event-type and relation constraints and show that the MAP inference with distributional constrai"
2020.emnlp-main.461,W18-5607,0,0.0115455,"temporal relation challenges were created in the following years ((Bethard et al., 2015, 2016, 2017)). However, data in these challenges are relatively hard to acquire, and therefore they are not used in this paper. As in the news data, traditional machine learning approaches (Lee et al., 2016; Chikka, 2016; Xu et al., 2013; Tang et al., 2013; Savova et al., 2010) that tackle the end-to-end event and temporal relation extraction problem require timeconsuming feature engineering such as collecting lexical and syntax features. Some recent work (Dligach et al., 2017; Leeuwenberg and Moens, 2017; Galvan et al., 2018) apply neural network-based methods to model the temporal relations, but are not capable of incorporating prior knowledge about clinical events and temporal relations as proposed by our framework. 9 Conclusion In conclusion, we propose a general framework that augments deep neural networks with distributional constraints constructed using probabilistic domain knowledge. We apply it in the setting of end-to-end temporal relation extraction task with event-type and relation constraints and show that the MAP inference with distributional constraints can significantly improve the final results. We"
2020.emnlp-main.461,K19-1062,1,0.672985,"mulate domain-knowledge between event types and relations as distributional constraints in Integer Linear Programming (ILP), and finally apply Lagrangian Relaxation to solve the constrained inference problem. Our base model is trained end-toend with cross-entropy loss and multitask learning to obtain relation scores. We need to perform an additional inference step in order to incorporate domain-knowledge as distributional constraints. 3.1 End-to-end Event Relation Extraction As illustrated in the left column in Figure 2, our end-to-end model shares a similar work-flow as the pipeline model in Han et al. (2019b), where multi-task learning with a shared feature extractor is used to train the pipeline model. Let E, EE and R denote event, candidate event pairs and the feasible relations, respectively, in an input instance xn , where n is the instance index. The combined training loss is L = cE LE + LR , where LE and LR are the losses for the event extractor and the relation module, respectively, and cE is a hyper-parameter balancing the two losses. Feature Encoder. Input instances are first sent to pre-trained language models such as BERT (Devlin 5718 Figure 2: An overview of the proposed framework. T"
2020.emnlp-main.461,D19-1041,1,0.716327,"Missing"
2020.emnlp-main.461,S13-2015,0,0.0243946,"re is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained language models to build an end-to-end system. Our work differs from these prior work"
2020.emnlp-main.461,S16-1201,0,0.0570396,"Missing"
2020.emnlp-main.461,E17-1108,0,0.0360191,"Missing"
2020.emnlp-main.461,2021.ccl-1.108,0,0.120653,"Missing"
2020.emnlp-main.461,P06-1095,0,0.0786347,"Han et al., 2019b), our baseline model for TimeBank-Dense does not predict event types. That is, when counting the triplet (P m , P n , rˆ), we assume there is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. ("
2020.emnlp-main.461,D19-1103,1,0.889767,"Missing"
2020.emnlp-main.461,P18-1049,0,0.338604,"per MacNemar’s test, the improvements against the end-to-end baseline models by adding inference with distributional constraints are both statistically significant for TimeBank-Dense (p-value < 0.005) and I2 B 2-T EMPORAL (p-value < 0.0005). For I2 B 2-T EMPORAL, our end-to-end system is optimized for the F1 score of the gold pairs. The TimeBank-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and this dataset has been widely evaluated on this task (Chambers et al., 2014; Ning et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018). Temporal relations consist of BEFORE, AFTER, INCLUDES, INCLUDED, SIMULTANEOUS, and VAGUE. Moreover, each event has several properties, e.g., type, tense, and polarity. Event types include occurrence, action, reporting, state, etc. Event pairs that are more than 2 sentences away are not annotated. I2 B 2-T EMPORAL. In the clinical domain, one of the earliest event temporal datasets was provided in the 2012 Informatics for Integrating Biology and the Bedside (i2b2) Challenge on NLP for Clinical Records (Sun et al., 2013). Clinical events are categorized into 6 types: treatment, problem, test,"
2020.emnlp-main.461,D17-1092,0,0.304542,"se local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained language models to build an end-to-end system. Our work differs from these prior work in that we build a structured neural model with distributional constraints that combines both the benefits of both Clinical Domain. The 2012 i2b2 Challenge ((Sun et al., 2013)) is one of the earliest efforts to advance event temporal relation extraction of clinical data. The challenge hosted three tasks on event (and event property) classification, temporal relation extrac"
2020.emnlp-main.461,D17-1108,0,0.214832,"ext, we explain the details of each component in our MAP inference. 3.2.1 Distributional constraints Much of the domain-knowledge required for realworld problems are probabilistic in nature. In the task of event relation extraction, domainknowledge can be the prior probability of a specific event-pair’s occurrence acquired from large corpora or knowledge base (Ning et al., 2018b); domain-knowledge can also be event-property and relation distribution obtained using corpus statistics, as we study in this work. Previous work mostly leverage hard constraints for inference (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2018a; Han et al., 2019a,b), where constraints such as transitivity and event-relation consistency are assumed to be absolutely correct. As we discuss in Section 1, hard constraints are rigid and thus cannot be used to model probabilistic domain-knowledge. The right column in Figure 2 illustrates how our work leverages corpus statistics to construct distributional constraints. Let P be a set of event properties such as clinical types (e.g. treatment or problem). For the pair (P m , P n ) and the triplet (P m , P n , r), where P n , P m ∈ P and r ∈ R,"
2020.emnlp-main.461,P18-1212,0,0.0747073,"the structured knowledge can be used to adjust neural baseline model scores and optimize the final model outputs. We formulate our MAP inference with distributional constraints as an LR problem and solve it with an iterative algorithm. Next, we explain the details of each component in our MAP inference. 3.2.1 Distributional constraints Much of the domain-knowledge required for realworld problems are probabilistic in nature. In the task of event relation extraction, domainknowledge can be the prior probability of a specific event-pair’s occurrence acquired from large corpora or knowledge base (Ning et al., 2018b); domain-knowledge can also be event-property and relation distribution obtained using corpus statistics, as we study in this work. Previous work mostly leverage hard constraints for inference (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2018a; Han et al., 2019a,b), where constraints such as transitivity and event-relation consistency are assumed to be absolutely correct. As we discuss in Section 1, hard constraints are rigid and thus cannot be used to model probabilistic domain-knowledge. The right column in Figure 2 illustrates how our work leverage"
2020.emnlp-main.461,W16-5706,0,0.0676487,"Missing"
2020.emnlp-main.461,P11-1008,0,0.0209666,"p∗t − θ ≤ pˆt ≤ p∗t + θ, ∀t ∈ T , and X r r yi,j ∈ {0, 1} , yi,j = 1, 1: procedure 2: for t ∈ T do 3: λ0t = 0 4: k=0 5: while k < K do 6: yˆk+1 ← arg max L(λk ) 7: for t ∈ T do 8: ∆t = p∗t − pˆt 9: if |∆t |&gt; θ then 10: λk+1 = λkt + α∆t t 11: if ∆t ≤ θ, ∀t then 12: break 13: k =k+1 14: α = γα 3.2.3 . K: max iteration . γ: decay rate Lagrangian Relaxation Solving Eq. (2) is NP-hard. Thus, we reformulate it as a Lagrangian Relaxation problem by introducing Lagrangian multipliers λt for each distributional constraint. Lagrangian Relaxation has been applied in a variety NLP tasks, as described by Rush and Collins (2011, 2012) and Zhao et al. (2017). The Lagrangian Relaxation problem can be written as X X X r r L(y, λ) = yi,j S(yi,j , x) + λt F (t). (i,j)∈EE r∈R r∈R t∈T (4) r , x), ∀r S(yi,j where ∈ R is the scoring function obtained from the relation module. For t = PEE r (i:P m ,j:P n ) yi,j m n PR r 0 . (P , P , r), we have pˆt = PEE (i:P m ,j:P n ) r0 yi,j The output of the MAP inference, y ˆ, is a collection of optimal label assignments for P all relation r =1 candidates in an input instance xn . r∈R yi,j ensures that each event pair gets one label assignment and this is the only hard constraint we use."
2020.emnlp-main.461,P17-2035,0,0.0161352,". Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained language models to build an end-to-end system. Our work differs from these prior work in that we build a structured neural model with distributional constraints that combines both the benefits of both Clinical Domain. The 2012 i2b2 Challenge ((Sun et al., 2013)) is one of the earliest efforts to advance event temporal relation extraction of clinical data. The challenge hosted three tasks on event (and event property"
2020.emnlp-main.461,S07-1014,0,0.0686354,"our baseline model for TimeBank-Dense does not predict event types. That is, when counting the triplet (P m , P n , rˆ), we assume there is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most rec"
2020.emnlp-main.461,P09-1046,0,0.165709,"n iterative algorithm. Next, we explain the details of each component in our MAP inference. 3.2.1 Distributional constraints Much of the domain-knowledge required for realworld problems are probabilistic in nature. In the task of event relation extraction, domainknowledge can be the prior probability of a specific event-pair’s occurrence acquired from large corpora or knowledge base (Ning et al., 2018b); domain-knowledge can also be event-property and relation distribution obtained using corpus statistics, as we study in this work. Previous work mostly leverage hard constraints for inference (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017; Ning et al., 2018a; Han et al., 2019a,b), where constraints such as transitivity and event-relation consistency are assumed to be absolutely correct. As we discuss in Section 1, hard constraints are rigid and thus cannot be used to model probabilistic domain-knowledge. The right column in Figure 2 illustrates how our work leverages corpus statistics to construct distributional constraints. Let P be a set of event properties such as clinical types (e.g. treatment or problem). For the pair (P m , P n ) and the triplet (P m , P n , r), where P n ,"
2020.emnlp-main.461,D17-1323,0,0.0288682,"and X r r yi,j ∈ {0, 1} , yi,j = 1, 1: procedure 2: for t ∈ T do 3: λ0t = 0 4: k=0 5: while k < K do 6: yˆk+1 ← arg max L(λk ) 7: for t ∈ T do 8: ∆t = p∗t − pˆt 9: if |∆t |&gt; θ then 10: λk+1 = λkt + α∆t t 11: if ∆t ≤ θ, ∀t then 12: break 13: k =k+1 14: α = γα 3.2.3 . K: max iteration . γ: decay rate Lagrangian Relaxation Solving Eq. (2) is NP-hard. Thus, we reformulate it as a Lagrangian Relaxation problem by introducing Lagrangian multipliers λt for each distributional constraint. Lagrangian Relaxation has been applied in a variety NLP tasks, as described by Rush and Collins (2011, 2012) and Zhao et al. (2017). The Lagrangian Relaxation problem can be written as X X X r r L(y, λ) = yi,j S(yi,j , x) + λt F (t). (i,j)∈EE r∈R r∈R t∈T (4) r , x), ∀r S(yi,j where ∈ R is the scoring function obtained from the relation module. For t = PEE r (i:P m ,j:P n ) yi,j m n PR r 0 . (P , P , r), we have pˆt = PEE (i:P m ,j:P n ) r0 yi,j The output of the MAP inference, y ˆ, is a collection of optimal label assignments for P all relation r =1 candidates in an input instance xn . r∈R yi,j ensures that each event pair gets one label assignment and this is the only hard constraint we use. To improve computational effi"
2020.emnlp-main.461,C08-3012,0,0.0344833,"t predict event types. That is, when counting the triplet (P m , P n , rˆ), we assume there is an oracle model that provides event types P m , P n for the predicted relation rˆ. One could potentially extend our work by training a similar multi-task learning model to predict both types and relations as our model does for the I2 B 2-T EMPORAL dataset. We leave this as a future research direction. 8 Related Work News Domain. Early work on temporal relation extraction use local pair-wise classification with hand-engineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014), improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). Ning et al. (2018c) and Han et al. (2019b) are the most recent work leveraging neural network and pre-trained langu"
2020.emnlp-main.524,P19-1470,0,0.253793,"k as a style-transfer problem we make three contributions: 4 Automatic creation of a parallel corpus of [literal sentence, simile] pairs. Our constructed corpus contains 87,843 such pairs. As a first step, we use distant supervision to automatically collect a set of self-labeled similes using the phrase like a. We then convert these similes to their literal versions by removing the COMPARATOR and replacing the VEHICLE with the associated PROPERTY 4 Code & Data at https://github.com/ tuhinjubcse/SimileGeneration-EMNLP2020 by leveraging the structured common sense knowledge achieved from COMET (Bosselut et al., 2019), a language model fine-tuned on ConceptNet (Speer et al., 2017). For example, for the simile “Love is like a unicorn” our method will generate “Love is rare” (Section 2.1). Transfer learning from a pre-trained model for generating high quality similes. Our system SCOPE, fine-tunes BART (Lewis et al., 2019) — a state of the art pre-trained denoising autoencoder built with a sequence to sequence model, on our automatically collected parallel corpus of [literal sentence, simile] pairs (Section 2.2) to generate similes. Human evaluations show that this approach generates similes that are better 3"
2020.emnlp-main.524,N16-1098,0,0.0526042,"Missing"
2020.emnlp-main.524,W17-7403,0,0.0709637,"Missing"
2020.emnlp-main.524,D14-1215,0,0.0294031,"cativeness. Experimental results from Table 7 prove that effective usage of similes can improve evocativeness of machine generated stories. 7 Related Work Simile generation is a relatively new task. Most prior work has focused on detection of similes. The closest task in NLP to simile generation is generating metaphors. However, it should be noted that 6462 the overlap between the expressive range of similes and metaphors is known to be only partial: there are similes that cannot be rephrased as metaphors, similarly the other way around (Israel et al., 2004). 7.1 Simile Detection and Analysis Niculae and Danescu-Niculescu-Mizil (2014) proposed frameworks for annotating similes from product reviews by considering their semantic and syntactic characteristics as well as the challenges inherent to the automatic detection of similes. Qadir et al. (2015, 2016) built computational models to recognize affective polarity and implicit properties in similes. Unlike these works, we focus on generating similes by transforming a literal sentence while still being faithful to the property in context. 7.2 Metaphor Generation 8 Conclusion We establish a new task for NLG: simile generation from literal sentences. We propose a novel way of c"
2020.emnlp-main.524,N19-4009,0,0.0323896,"Missing"
2020.emnlp-main.524,P02-1040,0,0.107509,"ores (BERT-S) and Novelty. Boldface denotes the best results. Baseline Systems 1. BART: This is the pre-trained BART model. Since BART is a pre-trained sequence to sequence model, it can still be used for conditional text generation. To this end we use the same literal sentence (For example The city was beautiful) as an input to the encoder and force the decoder to begin with same prefix by removing the adjective/adverb at the end and appending the comparator and the article (The city was like a) and generate a simile. B-1 0.0 3.25 3.73 8.03 3.2 Evaluation Criteria Automatic evaluation. BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metric for generation tasks such as Machine Translation. However, for creative text generation, it is not ideal to expect significant n-gram overlaps between the machine-generated and the gold-standard sentences. We still report the BLEU scores for generated VEHICLE after discarding the common prefix with the gold. BERTScore (Zhang et al., 2019) has been used recently for evaluating text generation using contextualized embeddings and it is said to somewhat ameliorate the problems with BLEU. It computes a similarity score using contextual emb"
2020.emnlp-main.524,W18-1505,1,0.906961,"Missing"
2020.emnlp-main.524,D15-1019,0,0.0223654,"f similes. The closest task in NLP to simile generation is generating metaphors. However, it should be noted that 6462 the overlap between the expressive range of similes and metaphors is known to be only partial: there are similes that cannot be rephrased as metaphors, similarly the other way around (Israel et al., 2004). 7.1 Simile Detection and Analysis Niculae and Danescu-Niculescu-Mizil (2014) proposed frameworks for annotating similes from product reviews by considering their semantic and syntactic characteristics as well as the challenges inherent to the automatic detection of similes. Qadir et al. (2015, 2016) built computational models to recognize affective polarity and implicit properties in similes. Unlike these works, we focus on generating similes by transforming a literal sentence while still being faithful to the property in context. 7.2 Metaphor Generation 8 Conclusion We establish a new task for NLG: simile generation from literal sentences. We propose a novel way of creating parallel corpora and a transfer-learning approach for generating similes. Human and automatic evaluations show that our best model is successful at generating similes. Our experimental results further show tha"
2020.emnlp-main.524,N16-1146,0,0.0398205,"Missing"
2020.emnlp-main.524,D19-1322,0,0.0225191,"tructure is: “[The city/TOPIC] [was/EVENT] [like/COMPARATOR] [a painting/VEHICLE]” (PROPERTY is implicit). Unlike metaphors, the semantic context of similes tends to be very shallow, transferring a single property (Hanks, 2013). Moreover, the explicit syntactic structure of similes allows, in exchange, for more lexical creativity (Niculae and DanescuNiculescu-Mizil, 2014). We focus on the task of generating a simile starting from a literal utterance that contains the TOPIC, EVENT and PROPERTY. We frame this task as a style-transfer problem (Shen et al., 2017; Fu et al., 2017; Li et al., 2018; Sudhakar et al., 2019), where the author’s intent is to make the description of the TOPIC more emphatic by introducing a comparison with the VEHICLE via a shared PROPERTY (See Figure 1 for examples of literal descriptive sentences and the generated similes). We call our approach SCOPE (Style transfer through COmmonsense PropErty). There are two main challenges we need to address: 1) the lack of training data that consists of pairs of literal utterances and their equivalent simile in order to train a supervised model; 2) ensuring that the generated simile makes a meaningful comparison between the TOPIC and the VEHIC"
2020.emnlp-main.524,N19-1014,0,0.022565,"EVENT, and a PROPERTY if stated explicitly. We take the top 5 properties from COMET to form 5 possible literal versions for a particular simile. To rank these literal versions and select the best one, we rely on perplexity scores obtained from a pre-trained language model GPT (Radford et al., 2018). Table 1 shows human written similes collected from Reddit, the top 5 common sense properties associated with the VEHICLE, and the literal version created by taking the best PROPERTY. To correct any grammatical errors introduced by this manipulation, we rely on a grammatical error correction model (Zhao et al., 2019). Test Data Collection. Our task is to generate a simile given a literal input. The automaticallygenerated parallel data might contain stylistic biases. To truly measure the effectiveness of our approach, we need to evaluate on a dataset independent of our training and validation data. Towards this end, we again scrape WRITINGPROMPTS subreddits for sentences which are this time literal in nature (without any comparators like, as). Since literal utterances contains the description of TOPIC via a PROPERTY and usually the PROPERTY is an adjective or adverb, we restrict the last word of our litera"
2020.emnlp-main.525,P18-1082,0,0.44985,"writing (Roemmele and Gordon, 2015). To spur research in this area, we partner with STO RIUM ,1 an online collaborative storytelling platform, to introduce a new dataset and evaluation methodology for story generation. The open-endedness of story writing does not just pose a barrier to humans—it also presents a challenge for building and evaluating computational models. Prior work relies on datasets that are either too artificial to generalize to longform stories, such as the crowdsourced ROCStories (Mostafazadeh et al., 2016) corpus, or too unconstrained, as in the r/writingprompts dataset (Fan et al., 2018), which pairs mediumlength stories with short prompts. Furthermore, lack of standardized evaluation makes measuring progress difficult: most prior work evaluates outputs using a combination of simple automatic metrics not designed for long-form creative text generation (e.g., BLEU and ROUGE against a single reference) and crowdsourced ratings (McIntyre and Lapata, 2009; Yao et al., 2019; Fan et al., 2019) that preclude evaluating long-form narratives. We address these limitations by (1) collecting a dataset of stories (Section 2) containing finegrained structural annotations written in natural"
2020.emnlp-main.525,P19-1254,0,0.371856,"are either too artificial to generalize to longform stories, such as the crowdsourced ROCStories (Mostafazadeh et al., 2016) corpus, or too unconstrained, as in the r/writingprompts dataset (Fan et al., 2018), which pairs mediumlength stories with short prompts. Furthermore, lack of standardized evaluation makes measuring progress difficult: most prior work evaluates outputs using a combination of simple automatic metrics not designed for long-form creative text generation (e.g., BLEU and ROUGE against a single reference) and crowdsourced ratings (McIntyre and Lapata, 2009; Yao et al., 2019; Fan et al., 2019) that preclude evaluating long-form narratives. We address these limitations by (1) collecting a dataset of stories (Section 2) containing finegrained structural annotations written in natural language, and (2) providing a platform for evaluating models in a machine-in-the-loop setting by allowing real STORIUM authors to interact with the generated stories (Section 4). Our dataset contains nearly 6K longform stories (125M tokens) written by STORIUM authors, each of which is broken into discourse-level scene entries annotated with narrative elements, such as character goals or abilities. Condit"
2020.emnlp-main.525,2020.emnlp-main.5,0,0.0447323,"Missing"
2020.emnlp-main.525,N16-1098,0,0.121259,"Missing"
2020.emnlp-main.525,W18-1505,1,0.840763,"AI-guided narratives are prevalent enough that we manually exclude these games from our experiments as they 6477 6 Related Work Our work builds on prior research in computational modeling for story generation. Early narrative prose generation systems (Meehan, 1977; Callaway and Lester, 2001; Riedl and Young, 2004) relied on graph-based planning formalisms and custom rules to structure their narratives, while story graphs have been used for interactive storytelling (Riedl and Bulitko, 2013). More recent work uses deep learning to generate stories by training neural models with limited context (Peng et al., 2018; Fan et al., 2018; Goldfarb-Tarrant et al., 2019) and structured knowledge, either external (Mao et al., 2019; Guan et al., 2020; Goldfarb-Tarrant et al., 2020) or derived (Yao et al., 2019; Fan et al., 2019). Compared to the datasets studied in those works, our STORIUM dataset contains much longer stories with built-in structural annotations written in natural language in the form of cards (Table 2). Our work connects more closely to existing machine-in-the-loop storytelling work (Roemmele and Gordon, 2015; Samuel et al., 2016; Clark et al., 2018), in which systems work in concert with users"
2020.emnlp-main.525,D19-1509,0,0.0699079,"Missing"
2020.emnlp-main.525,S18-2024,0,0.0955944,"2.7 2.8 3.2 L 3.8 3.9 3.6 4.1 R 2.3 2.3 2.4 2.7 Ppl 25.1 22.4 22.9 21.0 Jdg 90 77 62 85 Table 4: Exploratory experiments indicate optimally packing tokens using Cassowary (Cas), and including more history (His) is key to achieving low perplexity (Ppl), along with high fluency (F), coherence (C), likability (L), and relevance (R) based on a number of user judgments (Jdg). 4 A Machine-in-the-Loop Evaluation Platform The inadequacies of existing human and automatic evaluation methods are a major roadblock for story generation research. Automatic evaluations correlate weakly with human judgments (Sagarkar et al., 2018), and these judgments are obtained from crowd workers who are not invested in the narratives they are assessing. These concerns are magnified with STORIUM, as the story contexts are far too long for crowd workers to reliably evaluate (Section 5). In this section, we propose an improved evaluation methodology by directly integrating our models onto the STORIUM platform. This allows story authors to query a machine (Clark et al., 2018) for suggestions during the process of writing their own stories. We develop a new evaluation metric, User Story Edit Ratings (USER), computed on top of the edits"
2020.emnlp-main.525,K19-1079,0,0.184525,"lu 0.28 0.40 0.28 0.38 — — — — — — Coh 0.55 0.57 0.35 0.55 0.54 0.61 — — — — USER 0.51 0.39 0.34 0.35 0.13† 0.23 0.25 0.36 — — Rating 2.55 2.47 3.32 3.21 3.96 3.76 3.41 2.96 15.63 9.86 Table 5: Despite its low rating, relevance is clearly important as indicated by the moderately strong Pearson’s r correlations (first four columns) with USER and the remaining human judgments. All correlations are significant (p &lt; 0.01), except those indicated by † (p &gt; 0.05). p = 0.9.11 The sampling parameters, such as the k in top-k sampling, can significantly affect output quality of story generation models (See et al., 2019), so we choose values that worked well in prior work (Qin et al., 2019).12 Interestingly, while Holtzman et al. (2020) show that nucleus sampling improves over top-k sampling on measures like repetition, STORIUM users clearly prefer the top-k variant across all categories (last column of Table 5). We collect roughly 200 feedback ratings and 175 edits for each model over a span of three months beginning in late February 2020. We discover that both configurations score best on fluency and worst on relevance. This is unsurprising as (1) GPT-2 is known to produce fluent text and (2) the complex an"
2020.emnlp-main.525,2020.acl-main.704,0,0.0571905,"Missing"
2020.emnlp-main.88,S16-1165,0,0.0235264,"hat happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event and a time point (e.g., “won the championshi"
2020.emnlp-main.88,S17-2093,0,0.0585138,"Missing"
2020.emnlp-main.88,P14-2082,0,0.317752,"niently incorporate different modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened"
2020.emnlp-main.88,Q14-1022,0,0.13069,"large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula doe"
2020.emnlp-main.88,P17-2001,0,0.0210933,"how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorm"
2020.emnlp-main.88,D19-1606,1,0.782633,"Missing"
2020.emnlp-main.88,N19-1423,0,0.0968295,"Missing"
2020.emnlp-main.88,E17-2118,0,0.0176129,"lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IA"
2020.emnlp-main.88,D12-1062,1,0.857082,"code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific"
2020.emnlp-main.88,N19-1246,1,0.761929,"ns are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” st"
2020.emnlp-main.88,E17-1108,0,0.0440822,"Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018"
2020.emnlp-main.88,D18-1155,0,0.613672,"long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but s"
2020.emnlp-main.88,K17-1034,0,0.137647,"l many relations that cannot be expressed because the assumption that every event has a time interval is inaccurate: The time scope of an event may be fuzzy, an event can have a nonfactual modality, or events can be repetitive and invoke multiple intervals (see Fig. 5). To better handle these phenomena, we move away from the fixed set of relations used in prior work and instead use natural language to annotate the relationships between events, as described in the next section. 3 Natural Language Annotation of Temporal Relations Motivated by recent works (He et al., 2015; Michael et al., 2017; Levy et al., 2017; Gardner et al., 2019b), we propose using natural language question answering as an annotation for2 We could also include relationships between two fixed time points (e.g., compare 2011-03-24 with 2011-04-05), but these are mostly trivial, so we do not discuss them further. 1160 Confusing relations between the following events Questions that query events in different modes Fuzzy time scope: Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the southwest of England. [Negated] What didn’t the lion do after a large meal? “Follow” is negated: Co"
2020.emnlp-main.88,W17-2341,0,0.0161293,"The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy"
2020.emnlp-main.88,D19-5808,1,0.793705,"is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfall” or if ”disruption” started before “flooding.” SNOW, RAINFALL"
2020.emnlp-main.88,2021.ccl-1.108,0,0.26394,"Missing"
2020.emnlp-main.88,S15-2134,0,0.0363119,"2015) and QAMR (Michael et al., 2017), where QA pairs were used as representations for predicate-argument structures. In zeroshot relation extraction (RE), they reduced relation slot filling to an MRC problem so as to build very large distant training data and improve zero-shot learning performance (Levy et al., 2017). However, our work differs from zero-shot RE since it centers around entities, while T ORQUE is about events; the way to ask and answer questions, and the way to design a corresponding crowdsourcing pipeline, are thus significantly different between us. The QA-TempEval workshop (Llorens et al., 2015), desipte its name, is actually not studying temporal relations in an RC setting. The differences between T ORQUE and QA-TempEval 1165 are as follows. First, QA TempEval is an evaluation approach for systems that generate TimeML annotations and actually is not a QA task. For instance, QA TempEval is to evaluate whether a system can answer questions like “I S <E NTITY 1> <R ELATION> <E NTITY 2>?”, where one clearly knows which event that <E NTITY> is referring to and where R ELATION is selected from a predefined label set. Second, QA-TempEval’s annotation relies on the existence of a TimeML cor"
2020.emnlp-main.88,P18-1049,0,0.0823981,"n things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at"
2020.emnlp-main.88,W15-0809,0,0.0260832,") ) E2: [?!""#$"" , ?&'( ] [Hypothetical] If the lion has a large meal, it will sleep for 24 hours. [Repetitive] The lion used to sleep for 24 hours after having large meals. E1 starts with E2 E1 is equal to E2 E1 starts E2 E1 includes E2 E1 ends with E2 [Generic] After having a large meal, lions may sleep longer. E1 is before and overlapped with E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb a"
2020.emnlp-main.88,W16-1007,0,0.0771783,"fferent modes of events. Figure 7 shows how to accurately query the relation between “having a meal” and “sleeping” in different modes (original sentences can be found in Fig. 3). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig. 8). Third, a major issue that prior works wanted to address was deciding when two events should have a relation (Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016; Ning et al., 2018b). To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the1161 When should two events have a relation? Penalizing shortcuts by contrast questions Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. He ate his breakfast and went out. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Q: What happened after he ate his breakfast"
2020.emnlp-main.88,D17-1108,1,0.902955,"?”), or modify it to ask about the start/end time (e.g., “what happened after he started eating his breakfast?” or “what would finish after he ate his breakfast?”). We also instructed workers to make sure that the answers to the new question are different from the original one to avoid trivial modifications (e.g., changing “what happened” to “what occurred”). 4 Data Collection We used Amazon Mechanical Turk to build T ORQUE. Following prior work, we focus on passages that consist of two contiguous sentences, as this is sufficient to capture the vast majority of non-trivial temporal relations (Ning et al., 2017). We took all the articles used in the TempEval3 (TE3) workshop (2.8k articles) (UzZaman et al., 2013) and created a pool of 26k two-sentence passages. Given a random passage from this pool, the annotation process for crowd workers was: 1. Label all the events 2. Repeatedly do the following3 (a) Ask a temporal relation question and point out all the answers from the list of events (b) Modify the temporal relation to create one or more new questions and answer them The annotation guidelines4 and interface5 are public. In the following sections, we further discuss issues of quality control and c"
2020.emnlp-main.88,P18-1212,1,0.933706,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,D19-1642,1,0.780018,"equire temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler IV et al. (2014); O’Gorman et al. (2016) aimed at covering more phenomena but suffered from low IAA"
2020.emnlp-main.88,P17-2035,0,0.0725231,"is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig. 3), a predefined label set (Fig. 4), different time axes for events (Ning et al., 2018b), and specific rules to follow when there is confusion. For example, Bethard et al. (2007); Ning et al. (2018b) focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while Cassidy et al. (2014); Styler I"
2020.emnlp-main.88,S13-2001,0,0.478562,"ile a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These relationships between an event a"
2020.emnlp-main.88,N18-2026,0,0.0208006,"wer than but already comparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior work"
2020.emnlp-main.88,P18-1122,1,0.934335,"ith E2 E1 is immediately before E2 Figure 3: Various modes of events that prior work needed to categorize. Section 3 shows that they can be handled naturally without explicit categorization. ing, an event involves a predicate and its arguments (ACE, 2005; Mitamura et al., 2015). When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals (Pustejovsky et al., 2003). Later works on event and time have largely followed this definition, e.g., TempEval (Verhagen et al., 2007), TimeBankDense (Chambers et al., 2014), RED (O’Gorman et al., 2016), and MATRES (Ning et al., 2018b). This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig. 1). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of “she was on the east coast yesterday” and “she was happy”, which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing"
2020.emnlp-main.88,S07-1014,0,0.484675,"andslide Q6: What happened while a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found User-provided Q8: What happened at about the same time as the snow? A: rainfall Q9: What happened after the snow started? A: causing disruption bringing flooding searching trapped landslide said found Q10: What happened before the snow started? A: No answers. Introduction User-provided Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016, 2017; Laparra et al., 2018). For instance, “he won the championship yesterday” is different from “he will win the championship tomorrow”: he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is “expecting the birth of her first child”, we know that the birth is in the future, while if she is “mourning the death of her mother”, the death is in the past. These rela"
2020.emnlp-main.88,W16-5706,0,0.191361,"Missing"
2020.emnlp-main.88,P18-2124,0,0.0526628,"Missing"
2020.emnlp-main.88,D19-1332,1,0.800439,"omparable to that of using the entire training set. This means that the learning curve on T ORQUE is already flat and the current size of T ORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into T ORQUE.8 Human F1 Human EM Human C Figure 11: RoBERTa-large with different percentage of training data. Human performance in dashed lines. 7 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge (Vempala et al., 2018; Zhou et al., 2019, 2020), the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference (Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), structured learning (Leeuwenberg and Moens, 2017; Ning 8 https://allennlp.org/torque.html et al., 2017), and neural networks (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; Ning et al., 2019). Formalisms. The approach that prior works took to handle th"
2020.emnlp-main.88,2020.acl-main.678,1,0.892419,"Missing"
2020.emnlp-main.88,D16-1264,0,0.0598335,"in color and contrast questions are grouped. “birth” and “mourning” is after “death”) are called temporal relations (Pustejovsky et al., 2003). This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig. 1). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension (Rajpurkar et al., 2016, 2018; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019), and existing systems are hence brittle when handling questions in T ORQUE (Table 1). Reading comprehension for temporal relationships has the following challenges. First, reading 1158 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1158–1172, c November 16–20, 2020. 2020 Association for Computational Linguistics We know disruption/flooding started after snow/rainfall started, but we don’t know if they started earlier than the landslide. We don’t know if “snow” started before “rainfal"
2020.emnlp-main.88,D15-1076,0,\N,Missing
2020.emnlp-main.88,Q14-1012,0,\N,Missing
2020.emnlp-main.88,S18-1011,0,\N,Missing
2020.findings-emnlp.114,P15-1017,0,0.0184703,", into tree-LSTM models with distributional representations. Instead, our strategy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE’11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger"
2020.findings-emnlp.114,P09-2093,0,0.0421192,"17b; Jagannatha and Yu, 2016; Bj¨orne and Salakoski, 2018). Li et al. (2019) incorporated information from Gene Ontology, a biomedical knowledge base, into tree-LSTM models with distributional representations. Instead, our strategy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. E"
2020.findings-emnlp.114,W11-1801,0,0.111823,"Missing"
2020.findings-emnlp.114,N19-1145,0,0.272237,"Missing"
2020.findings-emnlp.114,P13-1008,0,0.0344813,"Yu, 2016; Bj¨orne and Salakoski, 2018). Li et al. (2019) incorporated information from Gene Ontology, a biomedical knowledge base, into tree-LSTM models with distributional representations. Instead, our strategy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results"
2020.findings-emnlp.114,2020.acl-main.713,0,0.152127,"g challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE’11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger words associated nested events. We also show that our method is robust when applied to different amount of training data, while being advantageous in low-resource scenarios. Future works include grounding adjective triggers to"
2020.findings-emnlp.114,D18-1156,0,0.0776626,"Missing"
2020.findings-emnlp.114,W16-6308,0,0.0130552,"onducted on the dev set. The edge and node representation in KGs were intialized with 300 dimensional pre-trained embeddings using TransE (Wang et al., 2014). The entire framework is optimized with BERTAdam optimizer for a maximum of 100 epochs with batch size of 4. Training is stopped if the dev set F1 does not improve for 5 consecutive epochs (more details see Appendix). 4.2 Results and Analysis Comparison with existing methods We compare our method with the following prior works: TEES and Stacked Gen. use SVM-based models with token and sentence-level features (Bj¨orne and Salakoski, 2011; Majumder et al., 2016); 4820 40 60 GAENet-SciBERT SciBERT-FT 80 100 Percentage of Training Data Figure 3: Performance comparison on the test set w.r.t. different amount of training data. TEES CNN leverages Convolutional Neural Networks and dependency parsing graph (Bj¨orne and Salakoski, 2018); KB-driven T-LSTM adopts an external knowledge base with type and sentence embeddings, into a Tree-LSTM model (Li et al., 2019). SciBERT-FT is a fine-tuned SciBERT without external resources, the knowledgeagnostic counterpart of GEANet-SciBERT. According to Table 1, SciBERT-FT achieves similar performance to KB-driven T-LSTM,"
2020.findings-emnlp.114,P06-1128,0,0.0710039,"ctional concept Can be qualiﬁed by Physiologic function Positive regulation of biological process Protein Cause of Component of possibly included BMP-6 Smad Phosphorylation Theme Argument labels Introduction Biomedical event extraction is a task that identifies a set of actions among proteins or genes that are associated with biological processes from natural language texts (Kim et al., 2009, 2011). Development of biomedical event extraction tools enables many downstream applications, such as domain-specific text mining (Ananiadou et al., 2015; Spangher et al., 2020), semantic search engines (Miyao et al., 2006) and automatic population and enrichment of database (Hirschman et al., 2012). A typical event extraction system 1) finds triggers that most clearly demonstrate the presence of events, 2) recognizes the protein participants (arguments), and 3) associates the arguments with the corresponding event triggers. For instance, the Theme Cause Trigger labels Protein Sentence BMP-6 Positive Regulation induces Phosphorylation Protein phosphorylation of Smad 1/5/8 Figure 1: An example of a UMLS-based hierarchical KG assisting event extraction. Circles represent concept nodes and triangles represent seman"
2020.findings-emnlp.114,N16-1034,0,0.015251,"tegy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE’11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger words associated nested events. We also show that our method is robust when appli"
2020.findings-emnlp.114,P15-2060,0,0.0144933,"dels with distributional representations. Instead, our strategy is to model two knowledge graphs from UMLS hierarchically with conceptual and semantic reasoning paths, providing stronger clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE’11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger words associated nested even"
2020.findings-emnlp.114,D19-1005,0,0.0640542,"Missing"
2020.findings-emnlp.114,W17-2315,0,0.142823,"Missing"
2020.findings-emnlp.114,D11-1001,0,0.0782405,"Missing"
2020.findings-emnlp.114,2020.nlpcovid19-acl.4,1,0.697774,"Mechanism of action qualiﬁer Allowed qualiﬁer Functional concept Can be qualiﬁed by Physiologic function Positive regulation of biological process Protein Cause of Component of possibly included BMP-6 Smad Phosphorylation Theme Argument labels Introduction Biomedical event extraction is a task that identifies a set of actions among proteins or genes that are associated with biological processes from natural language texts (Kim et al., 2009, 2011). Development of biomedical event extraction tools enables many downstream applications, such as domain-specific text mining (Ananiadou et al., 2015; Spangher et al., 2020), semantic search engines (Miyao et al., 2006) and automatic population and enrichment of database (Hirschman et al., 2012). A typical event extraction system 1) finds triggers that most clearly demonstrate the presence of events, 2) recognizes the protein participants (arguments), and 3) associates the arguments with the corresponding event triggers. For instance, the Theme Cause Trigger labels Protein Sentence BMP-6 Positive Regulation induces Phosphorylation Protein phosphorylation of Smad 1/5/8 Figure 1: An example of a UMLS-based hierarchical KG assisting event extraction. Circles represe"
2020.findings-emnlp.114,D14-1090,0,0.344023,"that: 1) “induces” is an action of biological function, 2) a biological function can be quantified by positive regulation, and 3) positive regulation can result in phosphorylation. sentence “Protein A inhibits the expression of Protein B” will be annotated with two nested events: Gene expression(Trigger: expression, Arg-Theme: Protein B) and Negative Regulation(Trigger: inhibits, Arg-Theme: Gene expression(Protein B), Arg-Cause: Protein A). Early attempts on biomedical event extraction adopted hand-crafted features (Bj¨orne et al., 2009; Bj¨orne and Salakoski, 2011; Riedel and McCallum, 2011; Venugopal et al., 2014a). Recent advances have shown improvements using deep neural networks via distributional word representations in the 1277 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1277–1285 c November 16 - 20, 2020. 2020 Association for Computational Linguistics biomedical domain (Moen and Ananiadou, 2013; Rao et al., 2017a; Bj¨orne and Salakoski, 2018; ShafieiBavani et al., 2019). Li et al. (2019) further extends the word representations with embeddings of descriptive annotations from a knowledge base and demonstrates the importance of domain knowledge in biomedical event"
2020.findings-emnlp.114,D19-1585,0,0.0859848,"clues for identifying challenging events in biomedical corpus. Related Works Event Extraction Most existing event extraction systems focus on extracting events in news. Early attempts relied on hand-crafted features and a pipeline architecture (Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural architectures, such as convolutional neural networks (Chen et al., 2015; Nguyen and Grishman, 2015), and recurrent neural networks (Nguyen et al., 2016). More recent studies leverages large pre-trained language models to obtain richer contextual information (Wadden et al., 2019; Lin et al., 2020). Another line of works utilized GNN to enhance event extraction performance. Liu et al. (2018) applied attention-based Conclusion We have proposed a framework to incorporate domain knowledge for biomedical event extraction. Evaluation results on GE’11 demonstrated the efficacy of GEANet and hierarchical KG representation in improving extraction of non-indicative trigger words associated nested events. We also show that our method is robust when applied to different amount of training data, while being advantageous in low-resource scenarios. Future works include grounding ad"
2020.findings-emnlp.114,P19-1393,0,0.0447373,"Missing"
2020.findings-emnlp.291,P17-4008,0,0.0376559,"trollable language generation, including the earlier introductions by Hu 17 Results for gender biases are in the Appendix. We also annotate 200 samples; the inter-annotator correlation is 0.71 and (annotation, automatic label) correlation is 0.66. Details are in the Appendix. 3246 18 et al. (2017) and Ficler and Goldberg (2017); we discuss the specific works most closely related to our own. Previous works have applied control to various components in a model pipeline. Keskar et al. (2019) present a large language model that learns during training to control for style and other specific tasks. Ghazvininejad et al. (2017) use weighted decoding to control poem style. Dathathri et al. (2019) combine attribute classifiers and pretrained language models to guide generation in different styles. Our gradient-based methods are most closely related to the latter work. Whereas Dathathri et al. (2019) update latent representations given gradients computed from the attribute classifier, we use gradients from target samples to form a bias trigger to control the model’s generated text. We believe these two gradient methods for control are parallel directions of work, and that our general formulation of bias associations an"
2020.findings-emnlp.291,W19-3823,0,0.0288688,"tion are manually selected, our analysis shows that DialoGPT tends to generate more negatively biased text for Black names, motivating the need for bias mitigation techniques. Introduction With the advent of more effective, large-scale natural language processing (NLP) techniques, the issue of fairness and bias is more important than ever. As such, there is an increasing repository of work dedicated to fairness in natural language processing. Much of the work on social biases in natural language processing focuses on biases in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b, 2019; Kurita et al., 2019) and natural language understanding tasks (Rudinger et al., 2018; Zhao et al., 2018a; Park et al., 2018). In contrast, there has been relatively little work on examining biases in natural language generation (NLG). However, biases in NLG applications arguably have more direct social impact, since many NLG applications (e.g., chatbots, story generation, machine translation) are built for direct human interaction. Table 1 shows biases in real responses from DialoGPT (Zhang et al., 2020) and how prepending a hidden trigger to the user input can mitigate biases in DialoGPT’s response. Motivated by"
2020.findings-emnlp.291,P16-1162,0,0.00816364,"er expand on the difference between the previous work and our work, the former uses non-racist triggers to prompt models to generate racist output for any input, while we adapt the former’s techniques as an implementation of our framework to induce and mitigate biases for targeted demographics. Note that the found trigger phrases are expected to be nonsensical, in part due to the unconstrained replacement strategy, and in part because GPT-2 operates at the subword level with Byte Pair Encodings 2 More demographic surface forms are explored in the dialogue generation experiments in Section 5. (Sennrich et al., 2016). Regardless, the triggers are still able to effectively influence the model’s generated texts. Input prompts. In conditional language generation, an input prompt conditions the model’s generated text. We control biases in generated text by prepending a trigger to an input prompt, which contains a demographic mention and a bias context, as shown in Figure 1. Bias contexts, a concept introduced by Sheng et al. (2019), are textual contexts which may induce biases towards a demographic, e.g., “[PERSON] was described as __” or “[PERSON] was regarded as __”.3 In Figure 1, given the trigger “Asked E"
2020.findings-emnlp.291,D19-1339,1,0.83617,"as control objective by comparing the ratio of bias polarities across large sets of text generated from different bias objectives. Figure 1 gives an overview of an implementation of our framework. First, we find a “bias control trigger” that can influence the bias polarity of text generated under a specified bias objective by extending gradient-based adversarial trigger phrase search techniques (Wallace et al., 2019). We can prepend the trigger to input prompts (consisting of a demographic mention and a bias context, which are contexts that may induce biases in generated output, as defined by Sheng et al. (2019)), give the prepended input prompts to a language model, and evaluate the bias polarity ratio of the generated text. Throughout this work, we expand on how the procedure in Figure 1 can be used for both bias analysis and mitigation. One dimension for bias analysis is analyzing specific topics that correspond to demographic inequalities in generated text. For example, we find that a trigger that induces more negative bias towards RACE - BLACK versus towards RACE - WHITE results in more generated text on the subject of international relations. Another dimension for bias analysis is observing the"
2020.findings-emnlp.291,D19-1221,0,0.193469,"induces positive biases for woman and negative biases for man. tion, each text containing a demographic mention has a bias polarity towards the demographic, and we evaluate the effectiveness of our bias control objective by comparing the ratio of bias polarities across large sets of text generated from different bias objectives. Figure 1 gives an overview of an implementation of our framework. First, we find a “bias control trigger” that can influence the bias polarity of text generated under a specified bias objective by extending gradient-based adversarial trigger phrase search techniques (Wallace et al., 2019). We can prepend the trigger to input prompts (consisting of a demographic mention and a bias context, which are contexts that may induce biases in generated output, as defined by Sheng et al. (2019)), give the prepended input prompts to a language model, and evaluate the bias polarity ratio of the generated text. Throughout this work, we expand on how the procedure in Figure 1 can be used for both bias analysis and mitigation. One dimension for bias analysis is analyzing specific topics that correspond to demographic inequalities in generated text. For example, we find that a trigger that ind"
2020.findings-emnlp.291,2020.acl-demos.30,0,0.20968,"l language processing focuses on biases in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b, 2019; Kurita et al., 2019) and natural language understanding tasks (Rudinger et al., 2018; Zhao et al., 2018a; Park et al., 2018). In contrast, there has been relatively little work on examining biases in natural language generation (NLG). However, biases in NLG applications arguably have more direct social impact, since many NLG applications (e.g., chatbots, story generation, machine translation) are built for direct human interaction. Table 1 shows biases in real responses from DialoGPT (Zhang et al., 2020) and how prepending a hidden trigger to the user input can mitigate biases in DialoGPT’s response. Motivated by the importance of understanding biases in NLG tasks, our goals are to develop new insights for and to mitigate biases in NLG models. To this end, we introduce a general framework to study how to control societal biases in NLG models. The framework is a model-agnostic formulation of a general bias control objective that can induce negative, neutral, or positive biases in generated text when the NLG model input contains mentions of specified demographic groups (e.g., “Black person” for"
2020.findings-emnlp.291,N19-1064,1,0.909786,"Missing"
2020.findings-emnlp.291,N18-2003,1,0.871272,"he examples without mitigation are manually selected, our analysis shows that DialoGPT tends to generate more negatively biased text for Black names, motivating the need for bias mitigation techniques. Introduction With the advent of more effective, large-scale natural language processing (NLP) techniques, the issue of fairness and bias is more important than ever. As such, there is an increasing repository of work dedicated to fairness in natural language processing. Much of the work on social biases in natural language processing focuses on biases in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b, 2019; Kurita et al., 2019) and natural language understanding tasks (Rudinger et al., 2018; Zhao et al., 2018a; Park et al., 2018). In contrast, there has been relatively little work on examining biases in natural language generation (NLG). However, biases in NLG applications arguably have more direct social impact, since many NLG applications (e.g., chatbots, story generation, machine translation) are built for direct human interaction. Table 1 shows biases in real responses from DialoGPT (Zhang et al., 2020) and how prepending a hidden trigger to the user input can mitigate biases in Dial"
2020.findings-emnlp.291,D18-1521,1,0.882054,"he examples without mitigation are manually selected, our analysis shows that DialoGPT tends to generate more negatively biased text for Black names, motivating the need for bias mitigation techniques. Introduction With the advent of more effective, large-scale natural language processing (NLP) techniques, the issue of fairness and bias is more important than ever. As such, there is an increasing repository of work dedicated to fairness in natural language processing. Much of the work on social biases in natural language processing focuses on biases in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b, 2019; Kurita et al., 2019) and natural language understanding tasks (Rudinger et al., 2018; Zhao et al., 2018a; Park et al., 2018). In contrast, there has been relatively little work on examining biases in natural language generation (NLG). However, biases in NLG applications arguably have more direct social impact, since many NLG applications (e.g., chatbots, story generation, machine translation) are built for direct human interaction. Table 1 shows biases in real responses from DialoGPT (Zhang et al., 2020) and how prepending a hidden trigger to the user input can mitigate biases in Dial"
2020.findings-emnlp.291,N18-2002,0,0.0956888,"Missing"
2020.findings-emnlp.291,D17-1323,1,\N,Missing
2020.findings-emnlp.291,N19-1423,0,\N,Missing
2020.findings-emnlp.291,D19-1531,1,\N,Missing
2020.findings-emnlp.369,2020.emnlp-main.99,1,0.524801,"Missing"
2020.findings-emnlp.369,D19-1269,1,0.698938,"018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge paths dynamically, even when these are absent in the KG. Dynamic Knowledge Path Generation. Several methods generate knowledge paths instead of extracting them from static KGs. Asai et al. (2019) learn reasoning paths by forming sequences of evidence documents, however, their approach relies on the inter-document hyperlinks to establish relations in the constructed KG. The extractor of Fu et al. (2019) retrieves missing facts in order to address the sparsity of KGs. Unlike our work, their setting is limited to knowledge graph completion, where both a query entity and a single query relation are given. The most similar existing work to ours is that by Bosselut and Choi (2019), which also leverages GPT-2 to dynamically generate knowledge paths. We see two key differences between this method and ours: (1) they expand their paths gradually by predicting the next entity one at a time, while we generate the paths in an end-to-end manner; (2) their method is restricted to a setting where the conte"
2020.findings-emnlp.369,D19-1243,0,0.0628846,"Missing"
2020.findings-emnlp.369,P16-1137,0,0.0502898,"ted explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 16 - 20, 2020. 2020 Association for Computational Linguistics work on commonsense KG completion (Li et al., 2016; Bosselut et al., 2019; Bosselut and Choi, 2019) is limited to predicting the tail of a statement with known head and relation, or a single-hop"
2020.findings-emnlp.369,D19-1282,1,0.84587,"ternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 16 - 20, 2020. 2020 Association for"
2020.findings-emnlp.369,2021.ccl-1.108,0,0.177303,"Missing"
2020.findings-emnlp.369,D19-6003,0,0.111195,"Missing"
2020.findings-emnlp.369,D18-1260,0,0.549363,"contributions are: 1. We propose a method to generate task-relevant knowledge paths that may not exist in the original KG, thus addressing the contextualization and sparsity challenges of KGs. 2. We design and implement a framework with three variants of our PG, to understand the role of local and global graph information. 3. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our method compared to previous methods, as well as its robustness to limited training data. 2 Preliminaries Our multiple-choice commonsense QA setup follows prior work (Talmor et al., 2018; Mihaylov et al., 2018; Bisk et al., 2020): given a question q, a system selects exactly one of the choices a as an answer. To experiment with contextualized background knowledge, we adopt a general framework (Figure 2) consisting of a context module, a knowledge module and a reasoning module. The context module encodes both the question q and a choice a as unstructured evidence, while the knowledge module encodes external facts as structured evidence. Both the unstructured and the structured evidence are fed to the reasoning module, which produces a score for a question-choice pair. The choice with a highest score"
2020.findings-emnlp.369,P18-1076,0,0.0441303,"ly uninterpretable (Mitra et al., 2019). Alternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher interpretability. Existing systems retrieve knowledge from a KG in the form of: triplets (Mihaylov and Frank, 2018), multihop paths (Lin et al., 2019; Bauer et al., 2018), or subgraphs (Kapanipathi et al., 2019). Despite the aforementioned benefits, exploiting these KGs poses the following challenges. Firstly, as KGs are known to suffer from sparsity (Li et al., 2016), they might not contain the knowledge needed to fill the gaps between the question and the answer. For example, a missing link (cave, IsA, geological feature) in Figure 1 might prevent the QA system from choosing the correct answer. Recent 4129 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4129–4140 c November 1"
2020.findings-emnlp.369,P19-1459,0,0.0290101,"s, such as caves, and that a cave is a type of geological feature. Such commonsense knowledge is obvious for humans but most existing QA systems do not have it or cannot reason with it. Although recent advances in pre-trained language models (LMs) have resulted in impressive performance on commonsense-related benchmarks (Zellers et al., 2018; Bhagavatula et al., 2019; 1 AtLocation IsA The code is available at https://github.com/ wangpf3/Commonsense-Path-Generator. Huang et al., 2019), it is unclear whether this is due to commonsense reasoning or to capturing spurious correlations in the data (Niven and Kao, 2019). Pre-trained LMs may answer a question correctly for wrong reasons, making them highly uninterpretable (Mitra et al., 2019). Alternatively, a set of systems retrieve external knowledge either from large text corpora or knowledge graphs (KGs). A corpus, however, might not be an ideal source of commonsense knowledge, as such knowledge is seldom stated explicitly in text (Storks et al., 2019). In contrast, commonsense KGs, like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019), provide structured evidence about the relevant entities, thus enabling effective reasoning and higher inter"
2020.findings-emnlp.369,N19-1368,0,0.066864,"its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge paths dynamically, even when these are absent in the KG. Dynamic Knowledge Path Generation. Several methods generate knowledge paths instead of extracting them from static KGs. Asai et al. (2019) learn reasoning paths by forming sequences of evidence documents, however, their approach relies on the inter-document hyperlinks to establish relations in"
2020.findings-emnlp.369,P16-1162,0,0.0222642,"orpora. We foresee two benefits of combining a pre-trained model such as GPT-2 and a static KG: (1) the language model would be able to generate commonsense knowledge paths, by being enriched with relevant structured knowledge; (2) the unstructured knowledge encoded in the language model would help to alleviate the sparsity challenge of the static KGs. Unlike COMET (Bosselut et al., 2019) which fine-tunes GPT (an earlier version of GPT-2) with independent triplets, we fine-tune GPT-2 with consecutive triplets that form paths (see Section 3.1). To do so, we first use GPT-2’s BytePair Encoding (Sennrich et al., 2016) to convert each symbolic path p to its textual form as a sequence {x0 , y0 , x1 , y1 , ..., yT −1 , xT }, where xt = ∣e ∣ 1 2 {xt , xt , ..., xt t } are phrase tokens of the entity et ∣r ∣ 1 2 and yt = {yt , yt , ..., yt t } are phrase tokens of the Table 1: Example Transformation of a Symbolic Path into Text. {predator, DistinctFrom, prey, IsA, animal} → { animal, [SEP], predator , distinct, from, prey, is, a, animal} relation rt . The reverse relations are represented by adding a special prefix token “ ”. The resulting paths mimic natural language sentences to facilitate optimal usage of th"
2020.findings-emnlp.369,Q18-1021,0,0.0329985,"by committing perjury. In Q3, the path from our Global generator is able to predict the relevant property of an entity and realizes that a 1-hop relation suffices in this case. Our Scratch variant, however, predicts a less precise relation ( HasContext). These cases show the path generalization ability of the fine-tuned pre-trained GPT-2, owed to its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a static KG, our PG is able to generate knowledge"
2020.findings-emnlp.369,D18-1259,0,0.0382914,"ontains incorrect information: peace is caused by committing perjury. In Q3, the path from our Global generator is able to predict the relevant property of an entity and realizes that a 1-hop relation suffices in this case. Our Scratch variant, however, predicts a less precise relation ( HasContext). These cases show the path generalization ability of the fine-tuned pre-trained GPT-2, owed to its unstructured knowledge. We refer readers to Table 12 in Appendix for more cases. 5 Related Work Multi-hop Reasoning on KGs. Recent benchmarks for commonsense QA and related tasks like open domain QA (Yang et al., 2018) and reading comprehension (Welbl et al., 2018), require systems to conduct multi-hop reasoning. Existing systems typically employ entity linking to recognize the relevant entities, ground them to a KG, and retrieve the paths from the local graph neighborhood around the entities. The retrieved paths are scored or ranked using graph-based metrics (e,g., PageRank, centrality) (Paul and Frank, 2019; Fadnis et al., 2019; Bauer et al., 2018), handcrafted rules (Kapanipathi et al., 2019) or neural methods (e.g., attention mechanisms) (Kundu et al., 2018; Lin et al., 2019). Rather than relying on a s"
2020.findings-emnlp.369,D18-1009,0,\N,Missing
2020.findings-emnlp.369,P19-1263,0,\N,Missing
2020.nlpcovid19-acl.4,P07-1056,0,0.36466,"Missing"
2020.nlpcovid19-acl.4,D19-1041,1,0.827068,"0.63 Past 0.08 0.37 Table 4: Percentage of sentences in tweets and policy that are in past or present/future tense. of their sentences, as shown in Table 45 . While lemmatization should mitigate that difference, it does not measurably improve the accuracy of our classifiers. Additionally, for the LogisticRegression classifier, we experimented with different vocabulary thresholds before choosing an optimal set, based on performance. Another set of features we consider are extracted events: we extract event arguments – agents and patients – and anchors using a BERT + BiLSTM neural architecture (Han et al., 2019), as well as the lemmatized version of these extracted events. We consider event-extraction after observing that tweets are significantly more likely to contain opinionated text – policy text has a median subjectivity of .23 while tweet text has a median subjectivity of .33.6 We hypothesize event-extraction can help transfer accuracy by abstracting the content of tweets from opinions. 3 3.1 Methodology Classification We test two classifiers: Logistic Regression on a TF-IDF normalized7 bag-of-words representation of each input document, and a pretrained RoBERTa-base model. We use Logistic Regre"
2020.nlpcovid19-acl.4,2020.emnlp-demos.2,0,0.0537572,"th views used for co-training. View #1 is the extracted events, View #2 is the text with event words removed. The labels generated from one view are iteratively added to another view’s training set. Then, the full text along with all added labels are used to train the final classifier. Not shown but tested as baseline views are: “noun-phrases”, “verb-phrases”, “random words”. RoBERTa-base pretrained model8 for our classification task, although we acknowledge that a more corpus-specific pretraining, like a Twitterspecific pretraining or a law-specific pretraining might achieve higher accuracy (Nguyen et al., 2020). 3.2 Co-Training Additionally, we test co-training as a method for increasing transfer accuracy. As formulated by Blum and Mitchell (1998), co-training is a method for increasing the accuracy of a classifier by using labels generated by other classifiers with different “views” of the data (i.e. non-overlapping feature sets). Previous work has found co-training advantageous in transfer learning tasks (Wan, 2009). The views we use are shown in Table 5. For View #1, we extract events using the method as described in Section 3.1, and for View #2, we leave the text without events as the other view"
2020.nlpcovid19-acl.4,W01-0501,0,0.299778,"his suggests that a more domain-specific pretrained model, like a Twitterspecific RoBERTa (Nguyen et al., 2020), might have even greater benefits from co-training, but we leave that to future work. We also leave to future work an exploration of further views that could increase co-training accuracy. Further engineering tricks, such as selective lemmatizing, might perform well as views. However, as indicated in Figure 4, event-extraction is a particularly useful view for imparting signal, relative to the baseline views we considered – baselines which have been, in fact, used in the literature (Pierce and Cardie, 2001). While it’s not immediately clear why this is the case, we hypothesize that extracting events as one view gives us the clearest conditional independence of views, which is necessary for co-training to be effective. Interestingly, both the “noun-phrase” baseline and the “verb-phrase” baseline degrade in performance over time for the transfer task (Figure 4b). It may be that these two tasks separate the views similarly: i.e., what is left over when extracting noun-phrases is mostly verb-phrases, and vice-versa. As shown in (Nigam and Ghani, 2000), if the accuracy of even the most confident co-t"
2020.nlpcovid19-acl.4,P09-1027,0,0.147512,"Missing"
2021.acl-long.330,W19-3822,0,0.199437,"al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et al. (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a,b) Religion Autocomplete Solaiman et al. (2019); Brown et al. (2020); Dhamala et al. (2021); Kirk et al. (2021); Abid et al. (2021) Sexuality Autocomplete Sheng et al. (2019, 2020); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Other Autocomplete Shwartz e"
2021.acl-long.330,S18-2005,0,0.0224595,"Missing"
2021.acl-long.330,2020.wmt-1.39,0,0.534408,"and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et"
2021.acl-long.330,2020.acl-main.154,0,0.0123573,"l. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et al. (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a,b) Religion Autocomplete Solaiman et al. (2019); Brown et"
2021.acl-long.330,2020.findings-emnlp.7,0,0.0904999,"a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et al. (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a,b) Religion Autocomplete Solaiman et al. (2019); Brown et al. (2020); Dhamala et al. (2021); Kirk et al. (2021); Abid et al. (2021) Sexuality Autocomplete Sheng et al. (2019, 2020); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Other Autocomplete Shwartz et al. (2020); Peng et al. (2020); Huang et al. (2020); Dhamala et al. (2021); Kirk et al. (2021) Dialogue She"
2021.acl-long.330,2021.acl-long.522,0,0.093528,"techniques that rely on training from scratch use smaller architectures (exceptions are from larger institutions). 5.3 Inference Methods While the existing literature on inference time methods for bias mitigation is sparse, decoding-based methods are a promising alternative to data- and training-based methods. Specifically, these methods are compatible with any pre-trained language model for generation without additional training. Given recent development of inference-time methods for control that can reduce toxicity (e.g., PPLM (Dathathri et al., 2019), GeDi (Krause et al., 2020), DExperts (Liu et al., 2021)), there is potential for extending these methods to bias mitigation. Bias Mitigation For autocomplete and dialogue generation, Sheng et al. (2020) formulate bias triggers using gradient-based methods of Wallace et al. (2019). These triggers are appended to prompts during inference time to control text generation to be more equalized towards different demographics. For translation, Saunders and Byrne (2020) present a lattice rescoring procedure that creates genderinflected search spaces to rescore text for more accurate translations, and Saunders et al. (2021) subsequently use this lattice str"
2021.acl-long.330,2020.coling-main.390,0,0.218102,"ls. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Cho"
2021.acl-long.330,2020.emnlp-main.64,0,0.48688,"ls. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Cho"
2021.acl-long.330,2020.inlg-1.43,0,0.426037,"l. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et al. (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a,b) Religion Autocomplete Solaiman et al. (2019); Brown et al. (2020); Dhamala et al. (2021); Kirk et al. (2021); Abid et al. (2021) Sexuality Autocomplete Sheng et al. (2019, 2020); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Other Autocomplete Shwartz et al. (2020); Peng et al. (2020); Huang et al. (2020); Dhamala et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Re-writing Pryzant et al. (2020); Ma et al. (2020) Table 1: Existing bias studies on different demographic dimensions in various NLG tasks: autocomplete generation, dialogue generation, machine translation (MT), and text re-writing. on mitigating gender biases and Shah et al. (2020) categorize sources of biases—both largely focus on natural language understanding (NLU) tasks, while we examine biases in NLG tasks. Additionally, Blodgett et al. (2020) urge for more explicitly tying “biases” in NLP to s"
2021.acl-long.330,P19-2031,0,0.169789,"o be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020). Thus, we limit the scope of this survey to the latter models. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and"
2021.acl-long.330,2020.acl-main.468,0,0.0284315,"own et al. (2020); Dhamala et al. (2021); Kirk et al. (2021); Abid et al. (2021) Sexuality Autocomplete Sheng et al. (2019, 2020); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Other Autocomplete Shwartz et al. (2020); Peng et al. (2020); Huang et al. (2020); Dhamala et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Re-writing Pryzant et al. (2020); Ma et al. (2020) Table 1: Existing bias studies on different demographic dimensions in various NLG tasks: autocomplete generation, dialogue generation, machine translation (MT), and text re-writing. on mitigating gender biases and Shah et al. (2020) categorize sources of biases—both largely focus on natural language understanding (NLU) tasks, while we examine biases in NLG tasks. Additionally, Blodgett et al. (2020) urge for more explicitly tying “biases” in NLP to societal normative definitions of biases and social hierarchies; with their recommendations in mind, we discuss the negative impacts of biases in NLG techniques. Our contributions are a comprehensive survey on societal biases in language generation and an experimental study on biases from decoding techniques. To start, we describe classes of NLG tasks (Sec. 2) and subsequently"
2021.acl-long.330,2020.wmt-1.73,0,0.183763,"Missing"
2021.acl-long.330,P19-1164,0,0.241972,"l Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. ("
2021.acl-long.330,D19-1339,1,0.149455,"ng and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020). Thus, we limit the scope of this survey to the latter models. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020)"
2021.acl-long.330,P19-1159,1,0.832759,"enumerating how NLG techniques contribute to biases and examining progress towards bias analysis and mitigation, we contextualize the discussion of broader trends and challenges. Specifically, we focus on techniques for NLG tasks, i.e., tasks that generate a sequence of text.1 Finding a lack of studies on biases from decoding techniques, we additionally present an experimental study to quantify the effects of various decoding techniques. Before we delve into the details of biases in language generation, we first position our survey in the context of other relevant surveys and position papers. Sun et al. (2019) present a focused survey 1 Although bi-directional language models like BERT (Devlin et al., 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020). Thus, we limit the scope of this survey to the latter models. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Associa"
2021.acl-long.330,2020.findings-emnlp.291,1,0.919363,"ls. Dialogue Generation Dialogue generation is conditioned on user inputs and can be for specific domains (e.g., health, customer service) and tasks (e.g., behavior intervention, booking flights) or general chit-chat. These dialogue applications directly interact with users, and any propagated biases directly affect user behavior and actions. In terms of recurrent dialogue models, Henderson et al. (2018) analyze biases in hierarchical recurrent encoder-decoder architectures and Liu et al. (2020a,b) analyze LSTM-based encoder-decoder models. Other works on dialogue biases (Dinan et al., 2020a; Sheng et al., 2020, 2021b) focus on Transformer-based models such as DialoGPT (Zhang et al., 2020) and other custom architectures. 2.2 Transformation Generation Tasks The transformation class includes machine translation and various formulations of text re-writing. The general goal of these tasks is to transform text into a form with targeted properties. Machine Translation Translation is the task of transforming text between languages while preserving the meaning. Existing works on biases in machine translation have almost exclusively focused on issues of gender biases2 in a variety of academic and commercial"
2021.acl-long.330,2021.naacl-main.60,1,0.406202,"the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (2021) Dialogue Henderson et al. (2018); Dinan et al. (2020a); Liu et al. (2020a,b); Cercas Curry et al. (2020); Sheng et al. (2021a,b) MT Vanmassenhove et al. (2018); Elaraby et al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin"
2021.acl-long.330,2020.gebnlp-1.9,1,0.765887,"etection) for different demographics. Language generation tasks often involve stochastic generation of open-ended and lengthy texts, traits that are not directly compatible with traditional algorithmic bias definitions (e.g., 4 https://www.bing.com/translator https://aws.amazon.com/translate 6 https://www.systransoft.com 7 https://papago.naver.com 8 https://translate.kakao.com 9 https://translate.yandex.com 10 Lucy and Bamman (2021) is an exception that analyzes gender in generated stories. While there are studies of biases in poetry generation and summarization, they focus on non-NLG biases: Sheng and Uthus (2020) investigate biases in a poetry composition system, but in the context of information retrieval; Celis and Keswani (2020) analyze biases in extractive summarization. 4277 5 equalized odds, equal opportunity, demographic parity (Dwork et al., 2012; Hardt et al., 2016)). Because of the difficulty in defining metrics, existing works define bias loosely as demographic inequality and use intermediate proxy metrics to comparatively measure bias. Examples include: • Regard Ratio: negative-neutral-positive regard score ratios of text generated from bias-inducing prompts (Sheng et al., 2019) • Sentimen"
2021.acl-long.330,2020.emnlp-main.556,0,0.0919634,"om decoding techniques, we additionally present an experimental study to quantify the effects of various decoding techniques. Before we delve into the details of biases in language generation, we first position our survey in the context of other relevant surveys and position papers. Sun et al. (2019) present a focused survey 1 Although bi-directional language models like BERT (Devlin et al., 2019) can also be used for auto-regressive generation (Wang and Cho, 2019; Chen et al., 2020), traditional auto-regressive models are still typically of better quality and more widely used for generation (Shwartz et al., 2020). Thus, we limit the scope of this survey to the latter models. 4275 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4275–4293 August 1–6, 2021. ©2021 Association for Computational Linguistics Demo. Dim. NLG Task Works Gender Autocomplete Bordia and Bowman (2019); Qian et al. (2019); Solaiman et al. (2019); Sheng et al. (2019, 2020); Vig et al. (2020); Yeo and Chen (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Nozza et al. (2021); Kirk et al. (20"
2021.acl-long.330,2021.naacl-main.189,0,0.190007,"find that more data can increase translation fluency but may also make the system more biased. 4.2 Biases from Model Architecture There are relatively few studies that examine model architectural properties that could lead to biases. We discuss the few efforts towards understanding model biases in NLG tasks and emphasize the need for more to generalize. For autocomplete generation, Vig et al. (2020) analyze GPT-2 variants through a causal mediation analysis, finding that larger models contain more gender bias, and bias tends to be concentrated in a small number of neurons and attention heads. Silva et al. (2021) observe amplified biases in distilled versus original models. For machine translation, Costa-juss`a et al. (2020) note that language-specific architectures are less biased because they encode more gender information than shared language encoder-decoder architectures. Studies like the aforementioned are useful for designing targeted bias mitigation methods (e.g., controlled generation to target specific attention heads or regularization to retain gender information). However, more evidence would be needed to generalize findings across models.15 4.3 Biases from Decoding While NLU and NLG models"
2021.acl-long.330,2020.acl-demos.30,0,0.0695603,"Missing"
2021.acl-long.330,D18-1521,1,0.877643,"Missing"
2021.acl-long.330,P19-1161,0,0.272043,"al. (2018); Prates et al. (2019); Stanovsky et al. (2019); Escud´e Font and Costa-juss`a (2019); Cho et al. (2019); Moryossef et al. (2019); Saunders and Byrne (2020); Saunders et al. (2020); Kocmi et al. (2020); Costa-juss`a and de Jorge (2020); Costa-juss`a et al. (2020); Basta et al. (2020); Farkas and N´emeth (2020); Stafanoviˇcs et al. (2020); Gonen and Webster (2020); Hovy et al. (2020); Roberts et al. (2020); Cho et al. (2021); Savoldi et al. (2021); Renduchintala and Williams (2021); Choubey et al. (2021); Saunders et al. (2021); Tomalin et al. (2021) Re-writing Habash et al. (2019); Zmigrod et al. (2019); Alhafni et al. (2020); Sun et al. (2021) Profession Autocomplete Huang et al. (2020); Dhamala et al. (2021) Race Autocomplete Solaiman et al. (2019); Sheng et al. (2019, 2020); Groenwold et al. (2020); Brown et al. (2020); Dhamala et al. (2021); Schick et al. (2021); Kirk et al. (2021) Dialogue Sheng et al. (2021a,b) Religion Autocomplete Solaiman et al. (2019); Brown et al. (2020); Dhamala et al. (2021); Kirk et al. (2021); Abid et al. (2021) Sexuality Autocomplete Sheng et al. (2019, 2020); Kirk et al. (2021) Dialogue Sheng et al. (2021a) Other Autocomplete Shwartz et al. (2020); Peng et a"
2021.acl-long.524,W19-0606,0,0.0127048,"roxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3.1.1 Figure 2: Lexical generation process Learning Frame Embeddings In order to exploit FrameNet frames as conceptual domains, we will embed them in vector space. While lexical and contextualized embeddings have proven effective, the field of embedding concepts from lexical resources is less well explored (Sikos and Pad´o, 2018; Alhoshan et al., 2019). These methods involve tagging raw corpora using automatic FrameNet parsing and then inputting some combination of the original text and the FrameNet information into standard embedding algorithms. To train and evaluate frame embeddings, we use 211k sentences of Gold annotations used to train the Open-SESAME parser (Swayamdipta et al., 2017), along with a variety of other automatically tagged datasets: 250k individual sentence from the Gutenberg Poetry Corpus (Jacobs, 2018), 17k from various fiction section of the Brown Corpus (Francis and Kucera, 1979), and 80k sentences randomly selected fr"
2021.acl-long.524,P98-1013,0,0.356034,"ith metaphoric counterparts. This can be done by employing vector spaces, identifying the word most likely to fit in an appropriate context and subjecting them to some constraints of metaphoricity. We build on this paradigm by incorporating facets of conceptual metaphor theory. Our procedure is as follows: we learn a joint embedded representations for domains and lexical items. We then use the linear transformation between two domains as a mapping, which can be applied to input words from the target domain to generate a word from the source domain. As a proxy for domains, we utilize FrameNet (Baker et al., 1998), which contains semantic frames along with the set of lexical units that evoke them. Frames can be defined as related systems of concepts (Fillmore, 1982), which is exchangeable with the term “domain” used in conceptual metaphor theory (Cruse and Croft, 2004). Thus, we consider the transformation from one frame to another as a proxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3"
2021.acl-long.524,Q17-1010,0,0.0358182,"a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016)"
2021.acl-long.524,P19-1470,0,0.0569006,"horic verbs, and replacing them with infilling from a language model. We use a BERT-based metaphor classification model trained on the VUA metaphor corpus (Steen et al., 2010) to identify metaphoric verbs in a sentence (i.e “died” in The house where love had died). Then we convert it to a literal sentence (The house where love had ended) using infillings from pre-trained BERT (Devlin et al., 2019). To ensure the literal sentence with replacements convey the same semantic meaning as the metaphorical sentence they are then filtered using symbolic meaning (SymbolOf relation) obtained from COMET (Bosselut et al., 2019), a GPT based language model fine-tuned on ConceptNet (Speer et al., 2017). COMET returns top 5 symbolic beams of (loss, loneliness, despair, sadness and sorrow) for the sentence “The house where love had died” whereas it replaces sorrow with life for the literal version. While Chakrabarty et al. (2021) filter down to only those candidates with an exact match between the top 5 symbolic beams for the literal and metaphorical sentences returned by the COMET model, we ease the restriction to cases where at least four of five symbols are the same. In order to learn more direct metaphoric informati"
2021.acl-long.524,W15-1405,0,0.022996,"ctional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition datab"
2021.acl-long.524,P18-1082,0,0.0223628,"she left. where hEOT i and hV i are delimiters, DEATH is the source frame, and CAUSE TO END the target frame. The decoding target is the metaphoric text “The party died as soon as she left”, which evokes the CAUSE TO END IS DEATH mapping. Note that our training data differs only at the level of a single verb. We use the generative BART seq2seq model to generate metaphoric paraphrases, 6727 but due to the nature of the training data and the importance of verbs in metaphoric expressions, this is often realized in the output as lexical replacement. Post fine-tuning, we use top-k (k=5) sampling (Fan et al., 2018) to generate metaphors conditioned on the input literal sentence and source and target domains for the required metaphoric mapping.5 We evaluate the lexical model (CM-Lex) and the sequence-to-sequence model (CM-BART) under two experimental settings. 4 Experimental Setup We evaluate our metaphor generation methods against two previous approaches to metaphoric paraphrase generation: the MERMAID system (Chakrabarty et al., 2021) and the metaphor masking model (MetMask) (Stowe et al., 2020). We explore two tasks: generating against gold standard metaphoric expressions, and using rare and unseen me"
2021.acl-long.524,W16-0203,0,0.138294,"janowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016), we sum the embedding of the target verb and the mapping m for the selected conceptual mapping, and select the most similar word to the resulting vector. This word is then delemmatized using fitbert (Havens and Stal, 2019) and inserted into the original sentence (Figure 2). Note that these resulting words are generated without context, as they rely only on the input word and the conceptual mappings. This approach has benefits: we require no labeled metaphor data, using only embeddings trained on FrameNet-tagged corpora. However, ignoring context is likely detrimental. In order to better use c"
2021.acl-long.524,2020.acl-main.703,0,0.22681,"6726 4 For full frame embedding evaluation, see Appendix A. Literal (filled from LM) That tyranny is destroyed The house where love had ended As the moments passed on What I learned my senses fraught Target Frame DESTRUCTION CAUSE TO END PROCESS END COMING TO BELIEVE Metaphoric (original) That tyranny is slain The house where love had died As the moments roll on What I bear my senses fraught Source Frame KILLING DEATH CAUSE MOTION BRINGING Table 1: Sample of extracted pairs from the data collection process. 3.2 CM-BART For sequence-to-sequence learning, we fine-tune a pre-trained BART model (Lewis et al., 2020), adding source and target information to guide generation towards the intended metaphors. We first outline a procedure for generating semi-supervised paired data, then detail the training and generation process. 3.2.1 Method for Creating Parallel Data In order to train sequence-to-sequence models for metaphor generation, we require large scale parallel corpora. We follow the approach of Chakrabarty et al. (2021) and build a corpus of literal/metaphoric paraphrases by starting with the Gutenberg Poetry corpus (Jacobs, 2018), identifying and masking metaphoric verbs, and replacing them with inf"
2021.acl-long.524,P18-1113,0,0.0555435,"Missing"
2021.acl-long.524,J04-1002,0,0.189424,"taphoricity, provide a strong signal for which domains to generate in. This highlights a possible benefit to the interaction between deep, pre-trained models such as BART and available lexical resources: by combining these, we are able to leverage the strength of each to build a powerful metaphor generation system. 6 Related Work We broadly cover two areas of related work: previous computational approaches to CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metap"
2021.acl-long.524,S16-2003,0,0.180733,"ings. For the former, we build a gold test set of metaphoric paraphrases that evoke a particular source/target mapping. For the latter, we apply a variety of source/target mappings to literal inputs for which we do not have gold outputs. 4.1 Building a Test Set For a test set, we use the same procedure as our data collection approach from Section 3.2.1. We apply this procedure to two datasets: a sample of the Gutenberg Poetry Corpus and a sample of fiction from the Brown Corpus (Francis and Kucera, 1979). This generates an initial set of literal/metaphoric pairs. We also tagged the pairs from Mohammad et al. (2016) with FrameNet tags, as these generally contain novel, well-formed metaphors. These three datasets each have different properties with regard to metaphor. The Gutenberg Poetry corpus has consistent, novel metaphors, but often unconventional syntactic constructions, due to the poetic nature of the text. The Mohammad 2016 corpus contains manually constructed metaphors which are novel, following relatively basic syntactic patterns. The Brown Corpus is standard fiction texts, so the metaphors within tend to be very conventional. From these sources, we draw pairs randomly, checking that they reflec"
2021.acl-long.524,C14-1165,0,0.0248913,"CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses te"
2021.acl-long.524,N19-4009,0,0.042058,"Missing"
2021.acl-long.524,D14-1162,0,0.0858506,"en frames (eg. used-by, uses), yielding a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and"
2021.acl-long.524,D19-1410,1,0.846699,"Missing"
2021.acl-long.524,W14-4725,0,0.0741936,"Missing"
2021.acl-long.524,W18-3813,0,0.0520256,"Missing"
2021.acl-long.524,W16-1105,0,0.0462399,"Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition databases (Ovchinnikova et al., 2014), and embedding based approaches to identify poetic relationships (Gagliano et al., 2016). However, the goal of these works is to generate mappings, rather than linguistic expressions that evoke them. Amongst deep learning approaches Yu and Wan (2019) identify literal and metaphoric words in corpora based"
2021.acl-long.524,D19-1221,0,0.0577942,"f-the-art performance in metaphor generation by both automatic and human evaluations. Future work can expand these models to go beyond verbs, incorporating nominal and other types of metaphors. The next necessary step is to go beyond lexicalized metaphors: good, consistent conceptual metaphors often span long stretches of text, and we need to design models that can learn and generate metaphors over larger texts. Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. It should also be noted that our CM-BART model is fine-tuned on the poetry corpus which is devoid of harmful and toxic text especially targeted at marginalized communities Advances in generative AI inherently come with concerns about models’ ability to deceive, persuade, and misinform. Metaphorical language has been shown to express and elicit stronger emotion than literal language (Citron and"
2021.acl-long.524,N19-1092,0,0.214231,"domain from which we draw the metaphorical expressions, while the target domain is the conceptual domain that we try to understand. A classical mapping is ARGUMENT IS WAR, in which we conceptualize the target argumentation domain as the more concrete source domain of war: Introduction Recent neural models have led to important progress in natural language generation (NLG) tasks. While pre-trained models have facilitated advances in many areas of generation, the field of metaphor generation remains relatively unexplored. Moreover, the few existing deep learning models for metaphor generation (Yu and Wan, 2019; Stowe et al., 2020; Chakrabarty et al., 2020) lack any conceptualization of the meaning of the metaphors. This work proposes the first step towards metaphor generation informed by the conceptual metaphor theory (CMT) (Lakoff and Johnson, 1980; Lakoff, 1993; Reddy, 1979). CMT holds • They fought against the contract. • They defended their new proposal. We focus on verbs, as they are often the key component of metaphoric expressions (Steen et al., 2010; Martin, 2006). When used metaphorically, verbs typically evoke source domains (e.g. fought, defended in the above examples): they are concrete"
2021.acl-short.45,D14-1162,0,0.0846027,"Missing"
2021.acl-short.45,P18-1043,0,0.021271,"thority such as court briefs (Gerken, 2010). Implicit biases in such knowledge sources could have a significant impact on audiences’ perception of different groups, thus propagating and even amplifying societal biases. Therefore, analyzing potential biases in Wikipedia is imperative. In particular, studying events in Wikipedia is important. An event is a specific occurrence under a certain time and location that involves participants (Yu et al., 2015); human activities are essentially sequences of events. Therefore, the distribution and perception of events shape the understanding of society. Rashkin et al. (2018) discovered implicit gender biases in film scripts using events as a lens. For example, they found that events with female agents are intended to be helpful to other people, while events with male agents are motivated by achievements. However, they focused on the intentions and reactions of events rather than events themselves. In this work, we propose to use events as a lens to study gender biases and demonstrate that events are more efficient for understanding biases in corpora than raw texts. We define gender bias as the 350 Proceedings of the 59th Annual Meeting of the Association for Comp"
2021.acl-short.45,2020.acl-main.442,0,0.0170141,"that contains target events; 2) testing the model performance for females and males separately in the generated data, 3) and using the model performance to estimate real event occurrence frequencies. We aim to calibrate the top 50 most skewed events in females’ and males’ Career and Personal Life descriptions after using the OR separately. First, we follow two steps to generate a synthetic dataset: 2. For each template sentence, we find the celebrity’s first name and mark it as a Name Placeholder, then we replace it with 50 female names and 50 male names that are sampled from the name list by Ribeiro et al. (2020). If the gender changes during the name replacement (e.g., Mike to Emily), we replace the corresponding pronouns (e.g., he to she) and gender attributes (Zhao et al., 2018) (e.g., Mr to Miss) in the template sentences. As a result, we get 100 data points for each template sentence with automatic annotations. If there is no first name in the sentence, we replace the pronouns and gender attributes. After getting the synthetic data, we run the event extraction model again. We use the detection recall among the generated instances to calibrate the frequency |e |for each target event and estimate t"
2021.acl-short.45,N18-2003,0,0.0289306,"occurrence frequencies. We aim to calibrate the top 50 most skewed events in females’ and males’ Career and Personal Life descriptions after using the OR separately. First, we follow two steps to generate a synthetic dataset: 2. For each template sentence, we find the celebrity’s first name and mark it as a Name Placeholder, then we replace it with 50 female names and 50 male names that are sampled from the name list by Ribeiro et al. (2020). If the gender changes during the name replacement (e.g., Mike to Emily), we replace the corresponding pronouns (e.g., he to she) and gender attributes (Zhao et al., 2018) (e.g., Mr to Miss) in the template sentences. As a result, we get 100 data points for each template sentence with automatic annotations. If there is no first name in the sentence, we replace the pronouns and gender attributes. After getting the synthetic data, we run the event extraction model again. We use the detection recall among the generated instances to calibrate the frequency |e |for each target event and estimate the actual frequency |e|∗ , following: (1) j∈[1,...,F ] The larger the OR is, the more likely an event will occur in male than female sections by Equation 1. After obtaining"
2021.eacl-main.218,Q16-1026,0,0.0121062,"used the same schema (De Waard and Maat, 2012) by exploring a variety of methods for balancing classes before applying classification algorithms. Deep Learning for Scientific Discourse Tagging. Due to the prevalence of deep learning, neural sequence labeling approach using bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CRF (BiLSTM-CRF) (Huang et al., 2015) has been prevailing for classic word-level sequence tagging problems such as named entity recognition (NER), part of speech tagging (POS), and word segmentation (Huang et al., 2015; Peng and Dredze, 2015, 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Peng and Dredze, 2017; Wang et al., 2017; Huang et al., 2019). Since scientific discourse tagging, which is a sentence-level sequence tagging problem, has one additional dimension of input comparing to word-level sequence tagging problems, an encoder is required to encode word-level representations to clause/sentence-level representations. While one simple way is to pre-compute sentence embeddings from word embeddings (Arora et al., 2016), there are more sophisticated methods to compute sentence-level embeddings on-the-fly using BiLSTM (Jin and Szolovits, 2018; Srivastava et al., 2019) or at"
2021.eacl-main.218,W12-4306,0,0.0876587,"Missing"
2021.eacl-main.218,I17-2052,0,0.195007,"CRF sequence taggers outperform neural-network based sequence taggers, we thus adopt the featurebased model. In addition to the scientific discourse tags, we use explicitly mentioned subfigure codes as well as unigram, bigram and trigram words as features. For each clause, we use all features described previously from the current clause in addition to the same sets of features from the adjacent previous and next clauses. Figure 5: Count of each label in three datasets. The lines correspond to the mappings from SciDT dataset (Burns et al., 2016; Dasigi et al., 2017) and PubMed 20k RCT dataset (Dernoncourt and Lee, 2017) to CODA-19 dataset (Huang et al., 2020) for zero-shot predictions (Section 5.2). 4 Experimental Setup We evaluate the performance of our scientific discourse tagger on PubMed-RCT dataset (Dernoncourt and Lee, 2017) and SciDT dataset (Burns et al., 2016; Dasigi et al., 2017) (Section 5.1). We also examine the transferablity of our scientific discourse tagger to new datasets using CODA-19 dataset (Huang et al., 2020) (Section 5.2). We further study the efficiency of scientific discourse tags on claim-extraction task via transfer learning as well as evidence fragment detection task in a pipeline"
2021.eacl-main.218,D17-1245,0,0.0271992,"vails among various natural language processing (NLP) tasks, a simple baseline method is to directly use a BERT-like model’s (e.g. SciBERT (Beltagy et al., 2019)) prefix token ([CLS]) representation of each sentence as the sentence representation for classification task (Huang et al., 2020). In this work, we combine these methods to present a state-of-the-art scientific discourse tagger. 2.2 Downstream Applications Claim Extraction. Claim extraction has been extensively studied in various domains. In addition to scientific articles (Stab et al., 2014), previous work has analyzed social media (Dusmanu et al., 2017), news (Habernal et al., 2014; Sardianos et al., 2015) and Wikipedia (Thorne et al., 2018; Fr´eard et al., 2010) for a task called Argumentation Mining to extract claims and premises. However, there are less attention and dataset available in the biomedical domain. Achakulvisut et al. (2019) composed a Figure 2: An example abstract with claim sentences highlighted in claim-extraction dataset (Achakulvisut et al., 2019). Figure 3: An example paragraph of evidence fragment detection. The explicit mention of subfigure codes are underlined. The red lines indicate the borders of the evidence fragme"
2021.eacl-main.218,W10-1913,0,0.0411053,"d Moens (2002) described argumentative zoning, which groups sentences into a few rhetorical zones highlighted by important clauses such as “in this paper we develop a method for”. Hirohata et al. (2008) used conditional random field (CRF) (Lafferty et al., 2001) with handcrafted features to classify sentences in abstracts into 4 categories: objective, methods, results, and conclusions. Liakata (2010) defined “zone of conceptualization” which classifies sentences into 11 categories in scientific papers and Liakata et al. (2012) used CRF and LibSVM to identify these “zone of conceptualization”. Guo et al. (2010) used Naive Bayes and Support Vector Machine (SVM) (Cortes and Vapnik, 1995) to compare three schema: section names, argumentative zones and conceptual structure of documents. Burns et al. (2016) studied the problem of scientific discourse tagging, which identifies the discourse type of each clause in a biomedical experiment paragraph and composed a dataset for it. They adopted the discourse type taxonomy for biomedical papers proposed by De Waard and Maat (2012). The taxonomy contains eight types including goal, fact, result, hypothesis, method, problem, implication and none as Table 1 shows."
2021.eacl-main.218,I08-1050,0,0.06542,"2017), the labels L = {goal, fact, hypothesis, problem, method, result, implication, none} as defined by De Waard and Maat (2012). Table 1 gives more details about the definitions of the tags. 2.1 Prior Works on Scientific Discourse Tagging Feature-based Scientific Discourse Tagging. There has been a significant amount of work aimed at understanding types of scientific discourse. Teufel and Moens (1999) and Teufel and Moens (2002) described argumentative zoning, which groups sentences into a few rhetorical zones highlighted by important clauses such as “in this paper we develop a method for”. Hirohata et al. (2008) used conditional random field (CRF) (Lafferty et al., 2001) with handcrafted features to classify sentences in abstracts into 4 categories: objective, methods, results, and conclusions. Liakata (2010) defined “zone of conceptualization” which classifies sentences into 11 categories in scientific papers and Liakata et al. (2012) used CRF and LibSVM to identify these “zone of conceptualization”. Guo et al. (2010) used Naive Bayes and Support Vector Machine (SVM) (Cortes and Vapnik, 1995) to compare three schema: section names, argumentative zones and conceptual structure of documents. Burns et"
2021.eacl-main.218,K19-1048,1,0.851675,"ety of methods for balancing classes before applying classification algorithms. Deep Learning for Scientific Discourse Tagging. Due to the prevalence of deep learning, neural sequence labeling approach using bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CRF (BiLSTM-CRF) (Huang et al., 2015) has been prevailing for classic word-level sequence tagging problems such as named entity recognition (NER), part of speech tagging (POS), and word segmentation (Huang et al., 2015; Peng and Dredze, 2015, 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Peng and Dredze, 2017; Wang et al., 2017; Huang et al., 2019). Since scientific discourse tagging, which is a sentence-level sequence tagging problem, has one additional dimension of input comparing to word-level sequence tagging problems, an encoder is required to encode word-level representations to clause/sentence-level representations. While one simple way is to pre-compute sentence embeddings from word embeddings (Arora et al., 2016), there are more sophisticated methods to compute sentence-level embeddings on-the-fly using BiLSTM (Jin and Szolovits, 2018; Srivastava et al., 2019) or attention (Dasigi et al., 2017), before feeding them into a claus"
2021.eacl-main.218,D18-1349,0,0.0360428,"Missing"
2021.eacl-main.218,W10-3101,0,0.0378329,"on Scientific Discourse Tagging Feature-based Scientific Discourse Tagging. There has been a significant amount of work aimed at understanding types of scientific discourse. Teufel and Moens (1999) and Teufel and Moens (2002) described argumentative zoning, which groups sentences into a few rhetorical zones highlighted by important clauses such as “in this paper we develop a method for”. Hirohata et al. (2008) used conditional random field (CRF) (Lafferty et al., 2001) with handcrafted features to classify sentences in abstracts into 4 categories: objective, methods, results, and conclusions. Liakata (2010) defined “zone of conceptualization” which classifies sentences into 11 categories in scientific papers and Liakata et al. (2012) used CRF and LibSVM to identify these “zone of conceptualization”. Guo et al. (2010) used Naive Bayes and Support Vector Machine (SVM) (Cortes and Vapnik, 1995) to compare three schema: section names, argumentative zones and conceptual structure of documents. Burns et al. (2016) studied the problem of scientific discourse tagging, which identifies the discourse type of each clause in a biomedical experiment paragraph and composed a dataset for it. They adopted the d"
2021.eacl-main.218,P16-1101,0,0.0206791,"Cox et al. (2017) used the same schema (De Waard and Maat, 2012) by exploring a variety of methods for balancing classes before applying classification algorithms. Deep Learning for Scientific Discourse Tagging. Due to the prevalence of deep learning, neural sequence labeling approach using bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CRF (BiLSTM-CRF) (Huang et al., 2015) has been prevailing for classic word-level sequence tagging problems such as named entity recognition (NER), part of speech tagging (POS), and word segmentation (Huang et al., 2015; Peng and Dredze, 2015, 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Peng and Dredze, 2017; Wang et al., 2017; Huang et al., 2019). Since scientific discourse tagging, which is a sentence-level sequence tagging problem, has one additional dimension of input comparing to word-level sequence tagging problems, an encoder is required to encode word-level representations to clause/sentence-level representations. While one simple way is to pre-compute sentence embeddings from word embeddings (Arora et al., 2016), there are more sophisticated methods to compute sentence-level embeddings on-the-fly using BiLSTM (Jin and Szolovits, 2018; Srivas"
2021.eacl-main.218,D15-1064,1,0.721522,"1 shows. Most recently, 2551 Cox et al. (2017) used the same schema (De Waard and Maat, 2012) by exploring a variety of methods for balancing classes before applying classification algorithms. Deep Learning for Scientific Discourse Tagging. Due to the prevalence of deep learning, neural sequence labeling approach using bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CRF (BiLSTM-CRF) (Huang et al., 2015) has been prevailing for classic word-level sequence tagging problems such as named entity recognition (NER), part of speech tagging (POS), and word segmentation (Huang et al., 2015; Peng and Dredze, 2015, 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Peng and Dredze, 2017; Wang et al., 2017; Huang et al., 2019). Since scientific discourse tagging, which is a sentence-level sequence tagging problem, has one additional dimension of input comparing to word-level sequence tagging problems, an encoder is required to encode word-level representations to clause/sentence-level representations. While one simple way is to pre-compute sentence embeddings from word embeddings (Arora et al., 2016), there are more sophisticated methods to compute sentence-level embeddings on-the-fly using BiLSTM (Jin an"
2021.eacl-main.218,P16-2025,1,0.876988,"Missing"
2021.eacl-main.218,D14-1162,0,0.0845413,"Missing"
2021.eacl-main.218,E99-1023,0,0.363234,"the word embeddings into a clause embedding. Detailed equations are provided in section A.1. The dashed circle in Figure 4 illustrates our LSTM-Attention based clause encoder. Sentence-level Sequence Tagging. We observe that the discourse labels have a clear transition of logic flow (e.g. result usually followed by implication, and method usually followed by hypothesis). Therefore, we extend LSTM sequence tagger used by Dasigi et al. (2017) to BiLSTM-CRF sequence tagger (Huang et al., 2015) to label discourse types for each sentence in a paragraph. Labels in BIO Scheme. We use the BIO scheme (Sang and Veenstra, 1999) to train all of our models (Baseline models for SciDT dataset do not use BIO2 scheme). Specifically, we convert the labels into BIO scheme where none label represents O and all other labels are converted into B label when the previous label type is different from the current label and I label when the previous label is the same as the current label. 3.2 3.2.1 Downstream Applications Claim Extractor Due to the similar problem formulation of evidence extraction task (Achakulvisut et al., 2019), we directly employ the discourse tagging model for claim extraction. 3.2.2 Evidence Fragment Detector"
2021.eacl-main.218,P13-1045,0,0.0165444,"ion. SciDT Dataset. Similar to PubMed-RCT (Dernoncourt and Lee, 2017), SciDT dataset (Burns et al., 2016; Dasigi et al., 2017) is a clause-based 2554 dataset with more fine-grained taxonomy. We further expand SciDT dataset by applying the same clause parsing and annotation pipeline described by Dasigi et al. (2017). This dataset is derived from the Pathway Logic (Eker et al., 2002) and INTACT databases (Orchard et al., 2013). Texts from all sections of each of those papers were pre-processed by parsing each sentence to generate a sequence of main and subordinate clauses using Stanford Parser (Socher et al., 2013). Domain experts were asked to label each of the clauses using the 7-label taxonomy proposed by De Waard and Maat (2012) whose distributions are shown in Figure 5. We apply sequential methods to sequences of clauses in individual paragraphs. Overall, SciDT dataset has a total of 634 paragraphs and 6124 clauses. We randomly split 570 paragraphs as the training and validation set and the rest as the test set. Each paragraph contains up to 30 clauses and the number of word per clause has a mean of 17.7 and a standard deviation of 12.5. The total vocabulary size is 8563, which is a small dataset f"
2021.eacl-main.218,J02-4002,0,0.375287,"d, methods, results, conclusions}, in CODA19 (Huang et al., 2020), L = {background, purpose, method, finding/contribution, other} while in SciDT dataset (Burns et al., 2016; Dasigi et al., 2017), the labels L = {goal, fact, hypothesis, problem, method, result, implication, none} as defined by De Waard and Maat (2012). Table 1 gives more details about the definitions of the tags. 2.1 Prior Works on Scientific Discourse Tagging Feature-based Scientific Discourse Tagging. There has been a significant amount of work aimed at understanding types of scientific discourse. Teufel and Moens (1999) and Teufel and Moens (2002) described argumentative zoning, which groups sentences into a few rhetorical zones highlighted by important clauses such as “in this paper we develop a method for”. Hirohata et al. (2008) used conditional random field (CRF) (Lafferty et al., 2001) with handcrafted features to classify sentences in abstracts into 4 categories: objective, methods, results, and conclusions. Liakata (2010) defined “zone of conceptualization” which classifies sentences into 11 categories in scientific papers and Liakata et al. (2012) used CRF and LibSVM to identify these “zone of conceptualization”. Guo et al. (20"
2021.eacl-main.218,W18-5501,0,0.0533502,"Missing"
2021.eacl-main.218,I17-2065,1,0.771279,"by exploring a variety of methods for balancing classes before applying classification algorithms. Deep Learning for Scientific Discourse Tagging. Due to the prevalence of deep learning, neural sequence labeling approach using bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and CRF (BiLSTM-CRF) (Huang et al., 2015) has been prevailing for classic word-level sequence tagging problems such as named entity recognition (NER), part of speech tagging (POS), and word segmentation (Huang et al., 2015; Peng and Dredze, 2015, 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Peng and Dredze, 2017; Wang et al., 2017; Huang et al., 2019). Since scientific discourse tagging, which is a sentence-level sequence tagging problem, has one additional dimension of input comparing to word-level sequence tagging problems, an encoder is required to encode word-level representations to clause/sentence-level representations. While one simple way is to pre-compute sentence embeddings from word embeddings (Arora et al., 2016), there are more sophisticated methods to compute sentence-level embeddings on-the-fly using BiLSTM (Jin and Szolovits, 2018; Srivastava et al., 2019) or attention (Dasigi et al., 2017), before feed"
2021.eacl-main.218,2020.nlpcovid19-acl.1,0,0.0286324,"ion set and the rest as the test set. Each paragraph contains up to 30 clauses and the number of word per clause has a mean of 17.7 and a standard deviation of 12.5. The total vocabulary size is 8563, which is a small dataset for an NLP task. However, we note the difficulties of obtaining such dataset. We further perform a quality assessment of the dataset by re-annotating the test set. We obtain Cohen’s kappa coefficient κ = 0.823, which indicates a high quality of the dataset. CODA-19 Dataset. CODA-19 (Huang et al., 2020) is a human-annotated dataset on a subset of the abstracts of CORD-19 (Wang et al., 2020), which is a corpus of scholarly articles about COVID-19. Wang et al. (2020) segmented each abstract into sentence fragments by comma (,), semicolon (;), and period (.). Each sentence fragment is labeled with one of the research aspects: background, purpose, method, finding/contribution or other, which is similar to the label sets of PubMedRCT (Dernoncourt and Lee, 2017). There are 10966 abstracts in total. We use this dataset to further examine our scientific discourse tagger architecture’s applicability to new datasets as well as the transferability of our trained scientific discourse tagger"
2021.emnlp-main.126,D14-1162,0,0.087698,"Missing"
2021.emnlp-main.126,2021.acl-long.426,1,0.754437,"efforts to obtained. 3.1 Robust training. Recently, adversarial attacks are presented to check the robustness of NLP models, such as character manipulation (Ebrahimi et al., 2018; Gil et al., 2019), word replacements (Alzantot et al., 2018; Li et al., 2020; Garg and Ramakrishnan, 2020; Jin et al., 2020), and syntactic rearrangements (Iyyer et al., 2018). To against those attacks, various robust training methods are proposed. For example, Alzantot et al. (2018) trains a robust model by data augmentation with generated adversarial examples. Other works (Ebrahimi et al., 2018; Dong et al., 2021; Zhou et al., 2021) consider adversarial training, which includes the adversarial accuracy to the training objective. A few studies propose transformations on inputs before feeding them to models (Edizel et al., 2019; Jones et al., 2020). Randomized smoothing (Cohen et al., 2019; Ye et al., 2020) is presented to make models robust against noise in input representations. Another line of research aims at providing theoretical guarantee of robustness, including interval bound propagation methods (Jia et al., 2019; Huang et al., 2019) and verification methods (Shi et al., 2020). Most of those robust training methods"
2021.emnlp-main.126,N19-1162,0,0.0244896,"Conneau et al., 2018; Yang et al., 2019; Clark et al., 2020; Artetxe et al., 2020; Lewis et al., 2020). XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) further provide benchmarks for zero-shot cross-lingual transfer learning. Embedding space alignments. Learning to align embedding spaces have always been an important research topic to improve multilinguality. Early works focus on word embedding spaces (Mikolov et al., 2013; Smith et al., 2017; Artetxe et al., 2017). Recently, many approaches are proposed to align contextual word embedding spaces, such as learning rotation projections (Schuster et al., 2019; Aldarmaki and Diab, 2019; Conneau et al., 2020b) and fine-tuning pre-trained multilingual language models (Chi et al., 2020; Feng et al., 1 2020; Cao et al., 2020; Qin et al., 2020; Liu et al., Our code is available at https://github.com/ uclanlp/Robust-XLT 2020; Dou and Neubig, 2021; Wei et al., 2021). 1685 However, most of them require additional supervision signals, such as parallel sentence pairs (Chi et al., 2020; Feng et al., 2020; Wei et al., 2021), bilingual dictionary (Cao et al., 2020; Qin et al., 2020; Liu et al., 2020), or both (Pan et al., 2021). These additional supervised corp"
2021.emnlp-main.126,D19-1030,0,0.0605948,"Missing"
2021.emnlp-main.162,D16-1120,0,0.0664583,"Missing"
2021.emnlp-main.162,D19-1243,0,0.0485699,"Missing"
2021.emnlp-main.162,P17-2009,0,0.0223859,"aphic Bias. Geographic bias is a serious issue that may cause harmful effects on certain groups of people. In computer vision, researchers (Shankar et al., 2017; de Vries et al., 2019) find that most images from two large-scale image datasets, ImageNet (Deng et al., 2009) and OpenImages (Krasin et al., 2017), are amerocentric and eurocentric. When a model trained on these datasets is applied to images from other regions, the performance will drop drastically. There also exists geographic bias in language technology. For example, it underlies natural language processing (Blodgett et al., 2016; Jurgens et al., 2017; Ghosh et al., 2021) and automatic speech recognition (Tatman, 2017; Koenecke et al., 2020) models. Our work seeks to reveal and test the geographic bias in the visual commonsense reasoning task and models. 3 Benchmark Construction 3.1 Image Collection In the image collection stage, we request annotators to follow two principles: Images with Regional Characteristics. In our annotation instruction, we require that the collected images should have representative scenarios containing cultural elements of the annotators’ regions. We further recommend annotators choose scenarios that are ubiquitou"
2021.emnlp-main.162,D14-1086,0,0.027793,"first find that the performance gap et al., 2021) are two multilingual benchmarks, but 2116 most samples in both benchmarks are simply translated from English and cannot reflect the regional characteristics. Different from previous benchmarks, GD-VCR focuses on geo-diverse commonsense instead of viewing commonsense as a universal monolith. Vision-and-Language Tasks. A long line of research seeks to build vision-and-language datasets that test a model’s ability to understand the visual world and how it is grounded in natural language. The tasks take on various forms, such as phrase grounding (Kazemzadeh et al., 2014; Plummer et al., 2015), visual question answering (Antol et al., 2015; Goyal et al., 2017), and visual reasoning (Zellers et al., 2019; Suhr et al., 2019). To solve these tasks, a wide range of visual grounding skills are required. However, in existing tasks, little consideration is taken into reasoning on the images with regional characteristics. Geographic Bias. Geographic bias is a serious issue that may cause harmful effects on certain groups of people. In computer vision, researchers (Shankar et al., 2017; de Vries et al., 2019) find that most images from two large-scale image datasets,"
2021.emnlp-main.162,P19-1644,0,0.0204666,"and cannot reflect the regional characteristics. Different from previous benchmarks, GD-VCR focuses on geo-diverse commonsense instead of viewing commonsense as a universal monolith. Vision-and-Language Tasks. A long line of research seeks to build vision-and-language datasets that test a model’s ability to understand the visual world and how it is grounded in natural language. The tasks take on various forms, such as phrase grounding (Kazemzadeh et al., 2014; Plummer et al., 2015), visual question answering (Antol et al., 2015; Goyal et al., 2017), and visual reasoning (Zellers et al., 2019; Suhr et al., 2019). To solve these tasks, a wide range of visual grounding skills are required. However, in existing tasks, little consideration is taken into reasoning on the images with regional characteristics. Geographic Bias. Geographic bias is a serious issue that may cause harmful effects on certain groups of people. In computer vision, researchers (Shankar et al., 2017; de Vries et al., 2019) find that most images from two large-scale image datasets, ImageNet (Deng et al., 2009) and OpenImages (Krasin et al., 2017), are amerocentric and eurocentric. When a model trained on these datasets is applied to i"
2021.emnlp-main.162,N19-1421,0,0.0190721,"ons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/ WadeYin9712/GD-VCR. 1 Introduction that this woman is attending a wedding and likely to be the bride. Recently, the field of commonsense reasoning is progressing with the development of large-scale benchmark datasets (Zellers et al., 2018; Talmor et al., 2019), intended to cover a wide range of commonsense knowledge, such as physical interactions (Bisk et al., 2020), social conventions (Sap et al., 2019), and commonsense grounded in vision (Zellers et al., 2019). However, existing benchmarks are often composed by data from sources in certain regions (e.g., Western movies) and overlook the differences across groups in different regions1 due to factors including cultural differences. In the aforementioned wedding example, while brides are usually in white in Western weddings, they often wear red and their faces are covered with a red cloth in traditi"
2021.emnlp-main.162,W17-1606,0,0.0161341,"s on certain groups of people. In computer vision, researchers (Shankar et al., 2017; de Vries et al., 2019) find that most images from two large-scale image datasets, ImageNet (Deng et al., 2009) and OpenImages (Krasin et al., 2017), are amerocentric and eurocentric. When a model trained on these datasets is applied to images from other regions, the performance will drop drastically. There also exists geographic bias in language technology. For example, it underlies natural language processing (Blodgett et al., 2016; Jurgens et al., 2017; Ghosh et al., 2021) and automatic speech recognition (Tatman, 2017; Koenecke et al., 2020) models. Our work seeks to reveal and test the geographic bias in the visual commonsense reasoning task and models. 3 Benchmark Construction 3.1 Image Collection In the image collection stage, we request annotators to follow two principles: Images with Regional Characteristics. In our annotation instruction, we require that the collected images should have representative scenarios containing cultural elements of the annotators’ regions. We further recommend annotators choose scenarios that are ubiquitous but have specific characteristics across regions, e.g., wedding, f"
2021.emnlp-main.162,D18-1009,0,0.0282575,"n. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/ WadeYin9712/GD-VCR. 1 Introduction that this woman is attending a wedding and likely to be the bride. Recently, the field of commonsense reasoning is progressing with the development of large-scale benchmark datasets (Zellers et al., 2018; Talmor et al., 2019), intended to cover a wide range of commonsense knowledge, such as physical interactions (Bisk et al., 2020), social conventions (Sap et al., 2019), and commonsense grounded in vision (Zellers et al., 2019). However, existing benchmarks are often composed by data from sources in certain regions (e.g., Western movies) and overlook the differences across groups in different regions1 due to factors including cultural differences. In the aforementioned wedding example, while brides are usually in white in Western weddings, they often wear red and their faces are covered with"
2021.emnlp-main.420,N19-1254,0,0.0457351,"as shown in Equation 2, which results in k (= m ∗ k/m) target syntactic parses in total. 0 #(T H , T H ) 0 TtH ∼ P (TtH |TsiH ) = PN si 0 Ht H . (2) j=1 #(Tsi , Ttj ) 3.2 Architecture of AESOP AESOP takes as inputs the source sentence X, its full syntactic parse TS and target syntactic parse(s) Y , and generates as outputs a paraphrase Z of X together with a duplication of the target parse Y . Specifically, given source sentences X, we tokenize and get their constituency-based parse trees3 , denoted as Ts (shown as source parse tree in Figure 3). Similar to previous works (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), we linearize the constituency parse tree to a sequence (shown as source full syntactic parse in Figure 3). To utilize the encoder-decoder BART (Lewis et al., 2020) model for syntactic-controlled paraphrase generation, we propose an effective design of having source sentence&lt;sep&gt;source full syntactic parse&lt;sep&gt;target syntactic parse as the input sequence for the encoder. The output sequence from the decoder is the sequence of target syntactic parse&lt;sep&gt;paraphrase. We will showcase the efficiency of our model design in Section 4 and provide a visual interpretation that AE"
2021.emnlp-main.420,N19-1423,0,0.0173207,"rollability of AESOP. Take the example in Figure 4, without target parse in the decoder, the model outputs a large black dog sits in the corner beside him. as the paraphrase to by his side crouched a huge black wolfish dog .. After adding the target parse in the decoder, the model no longer generates prepositional phrase in the corner and outputs a large black dog sits beside him., which matches better with the input target parse. 6 Improve Robustness Interpretation. In Figure 4, we visualize cross Recent works show that powerful LMs (e.g., attentions between encoder and decoder for two BERT (Devlin et al., 2019)) are capturing the sudesigns, i.e., AESOP with (right) and without (left) perficial lexical features McCoy et al. (2019) and target syntactic parse in the decoder on the test set are vulnerable to simple perturbations (Jin et al., of ParaNMT-small. Technically, we search for the 2020). Motivated by this, we first test if BERT is final output with beam = 4 and take the average robust to syntactic perturbations by paraphrasing. of cross attention scores of 12 attention heads from We fine-tune BERT models on two the last layer of the decoder. Finally, we add the GLUE (Wang et al., 2018) tasks (S"
2021.emnlp-main.420,2020.acl-main.60,0,0.0174653,"rcing to collect exemplars that can provide compatible synwhich aims to generate paraphrases that conform with given syntactic structures, has drawn increas- tax with the source sentence to guide generation. ing attention in the community. On the one hand, Disadvantages with this method are that the crowdsourcing process is costly, and one exemplar senparaphrase generation has benefited a wide range of NLP applications, such as neural machine trans- tence can only provide a specific syntactic guidance, while there are many syntactic parses that lation (Yang et al., 2019), dialogue generation (Gao et al., 2020), as well as improving model robust- can properly guide the paraphrase generation (as shown in Figure 1). ness (Huang et al., 2021) and interpretability (Jiang et al., 2019). On the other hand, syntacticallyIn contrast, we propose to automatically secontrolled paraphrasing has been used for diverse lect multiple syntactic parse structures to conquestion generation (Yu and Jiang, 2021), diversi- trol paraphrase generation for more diverse and fying creative generation (Tian et al., 2021) and higher quality generation. Our first contribution improving model robustness (Iyyer et al., 2018; is the"
2021.emnlp-main.420,2020.acl-main.22,0,0.0825146,"ses during training. Wieting and Gimpel (2018); Wieting et al. (2017) use back-translation to generate paraphrases. Huang and Chang (2021) propose a transformer-based model SynPG for paraphrase generation. AESOP is a supervised paraphrase generation model, which means that we require parallel paraphrases during training. Previous supervised paraphrase models are mostly RNNbased models, including SCPN (Iyyer et al., 2018), CGEN (Chen et al., 2019a) and SGCP (Kumar et al., 2020). Such models suffer from generating long sentences and do not utilize the power of recent pretrained language models. Goyal and Durrett (2020a) is a concurrent work with ours that also builds on BART to generate paraphrases but has a different model design. For syntactic control, Goyal and Durrett (2020b) use target syntactic parses to reorder source sentences to guide the generation, while other works, including AESOP, directly use target syntactic parses to guide the generation. CGEN (Chen et al., 2019a) and SGCP (Kumar et al., 2020) use target syntactic parses from crowd-sourced exemplars, SCPN (Iyyer et al., 2018) and SynPG (Huang and Chang, 2021) use pre-designed templates, while AESOP retrieves target syntactic parses automat"
2021.emnlp-main.420,2021.naacl-main.108,0,0.020052,"ructures, has drawn increas- tax with the source sentence to guide generation. ing attention in the community. On the one hand, Disadvantages with this method are that the crowdsourcing process is costly, and one exemplar senparaphrase generation has benefited a wide range of NLP applications, such as neural machine trans- tence can only provide a specific syntactic guidance, while there are many syntactic parses that lation (Yang et al., 2019), dialogue generation (Gao et al., 2020), as well as improving model robust- can properly guide the paraphrase generation (as shown in Figure 1). ness (Huang et al., 2021) and interpretability (Jiang et al., 2019). On the other hand, syntacticallyIn contrast, we propose to automatically secontrolled paraphrasing has been used for diverse lect multiple syntactic parse structures to conquestion generation (Yu and Jiang, 2021), diversi- trol paraphrase generation for more diverse and fying creative generation (Tian et al., 2021) and higher quality generation. Our first contribution improving model robustness (Iyyer et al., 2018; is the proposal of AESOP (Adaptive SyntacticallyHuang and Chang, 2021). Controlled Paraphrasing), a model that integrates However, select"
2021.emnlp-main.420,2021.eacl-main.88,0,0.475906,") (VP ) (. ) ) How to guide paraphrasing? What to say? target parse paraphrase (ROOT (FRAG (PP ) (. ))) AESOP (ROOT (S (NP ) (VP ) (. ))) (ROOT (SBAR (IN ) (S ) (. ))) with the gold the gold will change because of the gold , everything will everything changes . everything . change . Figure 1: Given a source sentence, AESOP selects target syntactic parses adaptively to guide paraphrase generation. Paraphrases here are all generated by AESOP, which preserve the semantics from source sentences and conform with the selected syntactic parses. structures for all input sentences (Iyyer et al., 2018; Huang and Chang, 2021). A challenge with this method is that not all sentences can be paraphrased into the same set of syntactic structures. For example, it is impossible to turn a long sentence with 1 Introduction multiple clauses into a noun phrase. Thus, Chen Syntactically-controlled paraphrase generation, et al. (2019b) proposed to use crowd-sourcing to collect exemplars that can provide compatible synwhich aims to generate paraphrases that conform with given syntactic structures, has drawn increas- tax with the source sentence to guide generation. ing attention in the community. On the one hand, Disadvantages"
2021.emnlp-main.420,P16-1195,0,0.0138513,"try to use the syntactic information from exemplar sentences as shallow as possible. We train separate models by using target syntactic parses from pruning the constituency parse tree of paraphrases at heights 2, 3 and 4.4 Correspondingly, we denote them as AESOP(-H2/H3/H4). During evaluation, we only use the target syntactic parse from the exemplar sentences at that corresponding height. Evaluation Metrics. We evaluate the quality of paraphrases with: 1) alignment-based metrics to examine the semantics preservation: including BLEU (Papineni et al., 2002), ROUGE scores (Lin, 2004) and METEOR (Iyer et al., 2016) between the generated paraphrase and gold paraphrase. 2) syntactic conformation metrics: Tree-Edit Distances (TED) scores (Zhang and Shasha, 1989) between the constituency parse trees of generated paraphrases versus exemplar sentences (TED-E) and parallel-annotated paraphrases (TED-R). Quality Check. We use source sentences and exemplar sentences to check the quality of the datasets in Table 1. Using the source sentences as paraphrases will lead to high semantic preservation scores, but they have distinct syntactic structure with paraphrases, so TED-R scores are poor. On the other hand, exemp"
2021.emnlp-main.420,N18-1170,0,0.302907,"erent. (ROOT (S (NP ) (VP ) (. ) ) How to guide paraphrasing? What to say? target parse paraphrase (ROOT (FRAG (PP ) (. ))) AESOP (ROOT (S (NP ) (VP ) (. ))) (ROOT (SBAR (IN ) (S ) (. ))) with the gold the gold will change because of the gold , everything will everything changes . everything . change . Figure 1: Given a source sentence, AESOP selects target syntactic parses adaptively to guide paraphrase generation. Paraphrases here are all generated by AESOP, which preserve the semantics from source sentences and conform with the selected syntactic parses. structures for all input sentences (Iyyer et al., 2018; Huang and Chang, 2021). A challenge with this method is that not all sentences can be paraphrased into the same set of syntactic structures. For example, it is impossible to turn a long sentence with 1 Introduction multiple clauses into a noun phrase. Thus, Chen Syntactically-controlled paraphrase generation, et al. (2019b) proposed to use crowd-sourcing to collect exemplars that can provide compatible synwhich aims to generate paraphrases that conform with given syntactic structures, has drawn increas- tax with the source sentence to guide generation. ing attention in the community. On the"
2021.emnlp-main.420,2020.tacl-1.22,0,0.234936,"2, which results in k (= m ∗ k/m) target syntactic parses in total. 0 #(T H , T H ) 0 TtH ∼ P (TtH |TsiH ) = PN si 0 Ht H . (2) j=1 #(Tsi , Ttj ) 3.2 Architecture of AESOP AESOP takes as inputs the source sentence X, its full syntactic parse TS and target syntactic parse(s) Y , and generates as outputs a paraphrase Z of X together with a duplication of the target parse Y . Specifically, given source sentences X, we tokenize and get their constituency-based parse trees3 , denoted as Ts (shown as source parse tree in Figure 3). Similar to previous works (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), we linearize the constituency parse tree to a sequence (shown as source full syntactic parse in Figure 3). To utilize the encoder-decoder BART (Lewis et al., 2020) model for syntactic-controlled paraphrase generation, we propose an effective design of having source sentence&lt;sep&gt;source full syntactic parse&lt;sep&gt;target syntactic parse as the input sequence for the encoder. The output sequence from the decoder is the sequence of target syntactic parse&lt;sep&gt;paraphrase. We will showcase the efficiency of our model design in Section 4 and provide a visual interpretation that AESOP successfully disen"
2021.emnlp-main.420,2020.acl-main.703,0,0.248206,"uch that Z’s syntax conforms to Y while retaining the semantics of X. We use the term target syntactic parses to refer to the syntactic structure that guides the generation, which could be from exemplar sentences, a set of fixed templates, or our adaptive selection module. 1 Data and code can be found at https://github. com/PlusLabNLP/AESOP AESOP: Adaptive SyntacticallyControlled Paraphrasing AESOP has two components: i) a retrieval-based module that adaptively selects a set of target syntactic parses to guide the paraphrase generation; ii) an encoder-decoder architecture that leverages BART (Lewis et al., 2020) to generate paraphrases. 3.1 Adaptive Target Syntactic Parse Selection In AESOP, we propose a retrieval-based strategy to select target syntactic parse adaptively (i.e., Algorithm 1). For a given syntactic parse of source sentence pruned at height H (as shown in Figure 2), denoted as TsH , we aim to find k suitable target syntactic parses to guide the generation. First, we collect (source sentence X, paraphrase Z) pairs from the training data. Then, we prune X and Z’s constituency parse trees at height H simultaneously and get corresponding (TsH , TtH ) pairs. By counting, we have the frequen"
2021.emnlp-main.420,P14-5010,0,0.00457279,"Missing"
2021.emnlp-main.420,P19-1334,0,0.0251882,"og sits in the corner beside him. as the paraphrase to by his side crouched a huge black wolfish dog .. After adding the target parse in the decoder, the model no longer generates prepositional phrase in the corner and outputs a large black dog sits beside him., which matches better with the input target parse. 6 Improve Robustness Interpretation. In Figure 4, we visualize cross Recent works show that powerful LMs (e.g., attentions between encoder and decoder for two BERT (Devlin et al., 2019)) are capturing the sudesigns, i.e., AESOP with (right) and without (left) perficial lexical features McCoy et al. (2019) and target syntactic parse in the decoder on the test set are vulnerable to simple perturbations (Jin et al., of ParaNMT-small. Technically, we search for the 2020). Motivated by this, we first test if BERT is final output with beam = 4 and take the average robust to syntactic perturbations by paraphrasing. of cross attention scores of 12 attention heads from We fine-tune BERT models on two the last layer of the decoder. Finally, we add the GLUE (Wang et al., 2018) tasks (SST-2 and attention of all tokens within each component (ss, RTE). Then, we generate 10 paraphrases using fp and tp). To m"
2021.emnlp-main.420,P02-1040,0,0.11132,"grained syntactic information even for experts. In AESOP, we try to use the syntactic information from exemplar sentences as shallow as possible. We train separate models by using target syntactic parses from pruning the constituency parse tree of paraphrases at heights 2, 3 and 4.4 Correspondingly, we denote them as AESOP(-H2/H3/H4). During evaluation, we only use the target syntactic parse from the exemplar sentences at that corresponding height. Evaluation Metrics. We evaluate the quality of paraphrases with: 1) alignment-based metrics to examine the semantics preservation: including BLEU (Papineni et al., 2002), ROUGE scores (Lin, 2004) and METEOR (Iyer et al., 2016) between the generated paraphrase and gold paraphrase. 2) syntactic conformation metrics: Tree-Edit Distances (TED) scores (Zhang and Shasha, 1989) between the constituency parse trees of generated paraphrases versus exemplar sentences (TED-E) and parallel-annotated paraphrases (TED-R). Quality Check. We use source sentences and exemplar sentences to check the quality of the datasets in Table 1. Using the source sentences as paraphrases will lead to high semantic preservation scores, but they have distinct syntactic structure with paraph"
2021.emnlp-main.420,2021.findings-emnlp.136,1,0.715928,"ntactic guidance, while there are many syntactic parses that lation (Yang et al., 2019), dialogue generation (Gao et al., 2020), as well as improving model robust- can properly guide the paraphrase generation (as shown in Figure 1). ness (Huang et al., 2021) and interpretability (Jiang et al., 2019). On the other hand, syntacticallyIn contrast, we propose to automatically secontrolled paraphrasing has been used for diverse lect multiple syntactic parse structures to conquestion generation (Yu and Jiang, 2021), diversi- trol paraphrase generation for more diverse and fying creative generation (Tian et al., 2021) and higher quality generation. Our first contribution improving model robustness (Iyyer et al., 2018; is the proposal of AESOP (Adaptive SyntacticallyHuang and Chang, 2021). Controlled Paraphrasing), a model that integrates However, selecting suitable target syntactic struc- pretrained Language Models (LMs) with a novel tures to control paraphrase generation for diverse retrieval-based target syntactic parse selection modand high-quality results is a lesser explored direc- ule to control paraphrase generation. By levertion. Prior works usually use a fixed set of syntactic aging the expressive"
2021.emnlp-main.420,W18-5446,0,0.0510143,"Missing"
2021.emnlp-main.420,P18-1042,0,0.0124858,"ining data, models’ robustness to such perturbations all get improved. Among all models, 5183 AESOP yields the best ParaGAP on the combined dataset of original dev sets and collected datasets, which shows that using AESOP improves the classification model’s robustness to syntactic perturbations more effectively.11 7 Related Work Recent advances have been using neural models for syntactically controlled paraphrase generation. From the modeling perspective, there are roughly two categories: unsupervised and supervised methods. Unsupervised models do not use parallel paraphrases during training. Wieting and Gimpel (2018); Wieting et al. (2017) use back-translation to generate paraphrases. Huang and Chang (2021) propose a transformer-based model SynPG for paraphrase generation. AESOP is a supervised paraphrase generation model, which means that we require parallel paraphrases during training. Previous supervised paraphrase models are mostly RNNbased models, including SCPN (Iyyer et al., 2018), CGEN (Chen et al., 2019a) and SGCP (Kumar et al., 2020). Such models suffer from generating long sentences and do not utilize the power of recent pretrained language models. Goyal and Durrett (2020a) is a concurrent work"
2021.emnlp-main.420,D17-1026,0,0.0174317,"ess to such perturbations all get improved. Among all models, 5183 AESOP yields the best ParaGAP on the combined dataset of original dev sets and collected datasets, which shows that using AESOP improves the classification model’s robustness to syntactic perturbations more effectively.11 7 Related Work Recent advances have been using neural models for syntactically controlled paraphrase generation. From the modeling perspective, there are roughly two categories: unsupervised and supervised methods. Unsupervised models do not use parallel paraphrases during training. Wieting and Gimpel (2018); Wieting et al. (2017) use back-translation to generate paraphrases. Huang and Chang (2021) propose a transformer-based model SynPG for paraphrase generation. AESOP is a supervised paraphrase generation model, which means that we require parallel paraphrases during training. Previous supervised paraphrase models are mostly RNNbased models, including SCPN (Iyyer et al., 2018), CGEN (Chen et al., 2019a) and SGCP (Kumar et al., 2020). Such models suffer from generating long sentences and do not utilize the power of recent pretrained language models. Goyal and Durrett (2020a) is a concurrent work with ours that also bu"
2021.emnlp-main.420,D19-1072,0,0.0198435,"et al. (2019b) proposed to use crowd-sourcing to collect exemplars that can provide compatible synwhich aims to generate paraphrases that conform with given syntactic structures, has drawn increas- tax with the source sentence to guide generation. ing attention in the community. On the one hand, Disadvantages with this method are that the crowdsourcing process is costly, and one exemplar senparaphrase generation has benefited a wide range of NLP applications, such as neural machine trans- tence can only provide a specific syntactic guidance, while there are many syntactic parses that lation (Yang et al., 2019), dialogue generation (Gao et al., 2020), as well as improving model robust- can properly guide the paraphrase generation (as shown in Figure 1). ness (Huang et al., 2021) and interpretability (Jiang et al., 2019). On the other hand, syntacticallyIn contrast, we propose to automatically secontrolled paraphrasing has been used for diverse lect multiple syntactic parse structures to conquestion generation (Yu and Jiang, 2021), diversi- trol paraphrase generation for more diverse and fying creative generation (Tian et al., 2021) and higher quality generation. Our first contribution improving mode"
2021.emnlp-main.420,2021.eacl-main.279,0,0.0222986,"ited a wide range of NLP applications, such as neural machine trans- tence can only provide a specific syntactic guidance, while there are many syntactic parses that lation (Yang et al., 2019), dialogue generation (Gao et al., 2020), as well as improving model robust- can properly guide the paraphrase generation (as shown in Figure 1). ness (Huang et al., 2021) and interpretability (Jiang et al., 2019). On the other hand, syntacticallyIn contrast, we propose to automatically secontrolled paraphrasing has been used for diverse lect multiple syntactic parse structures to conquestion generation (Yu and Jiang, 2021), diversi- trol paraphrase generation for more diverse and fying creative generation (Tian et al., 2021) and higher quality generation. Our first contribution improving model robustness (Iyyer et al., 2018; is the proposal of AESOP (Adaptive SyntacticallyHuang and Chang, 2021). Controlled Paraphrasing), a model that integrates However, selecting suitable target syntactic struc- pretrained Language Models (LMs) with a novel tures to control paraphrase generation for diverse retrieval-based target syntactic parse selection modand high-quality results is a lesser explored direc- ule to control pa"
2021.emnlp-main.426,D19-1371,0,0.139765,"ven scientific article. Due to the long distances between entities, S CI REX-P struggles to extract the right entity pair that has a relation, while our approach correctly identifies them. This reflects our method’s advantage in modeling long-term cross-entity dependencies. task-specific classifiers on top of large pre-trained language models. For example, Du and Cardie (2020a) builds a sequence tagging framework with multi-granularity representations based on BERT (Devlin et al., 2019) for role-filler entity extraction. Jain et al. (2020) builds a relation extraction pipeline upon S CI BERT (Beltagy et al., 2019). However, there are a few drawbacks of this model architecture. First, as the size of the document increases, it becomes increasingly difficult for extractive methods to capture cross-entity dependencies among entitiy types due to long distances between entities, as shown in Figure 1. Additionally, discriminative models have no information regarding the semantics of the labels when classifying relations or entity types. Thus, it is unable to take advantage of the label semantics embedded in the pre-trained encoders. Motivated by these challenges, we propose to formulate REE and RE tasks as te"
2021.emnlp-main.426,D12-1091,0,0.026729,"al., 2019) S CI REX-P (Jain et al., 2020) 56.82 64.89 64.19 57.04 - 48.92 47.75 47.36 46.77 - 52.58 55.02 54.50 51.40 - 0.74 2.9 6.5 0.67 12.8 41.1 0.62 3.8 9.6 0.00 0.7 0.00 17.3 0.00 0.8 T EMP G EN 68.55 49.90 57.76 17.11 13.56 14.47∗ 3.19 4.26 3.55∗ Table 1: Performance comparison on role-filler entity extraction, binary and 4-ary relation extraction tasks. TANL results are re-implemented and evaluated by ourselves. T EMP G EN outperforms all previous systems on REE, binary RE, and 4-ary RE. Statistical significance over previous best systems computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012) is indicated with ∗ (p &lt; .01). The S CI REX dataset5 consists of scientific articles, with entity, coreference, and relation annotations. With an average token count of about 5700, the articles are significantly longer than the documents in MUC-4. We use the pre-processed data from Jain et al. (2020), which contains 306 documents for training, 66 for validation, and 66 for testing. In contrast to conventional relation extraction datasets, such as ACE05, relations are not typed in S CI REX. Hence, the official S CI REX evaluator (Jain et al., 2020) only considers the correctness of predicted e"
2021.emnlp-main.426,2020.acl-main.670,0,0.0349147,"Missing"
2021.emnlp-main.426,N19-1370,0,0.0346244,"Missing"
2021.emnlp-main.426,D19-1498,0,0.0230027,"(Du et al., 2021) formulates the problem as sequence generation, and employs a single transformer layer whose parameters are shared between encoder and decoder to enrich semantics in the shared parameters. A pointer selection network is used for the final layer of decoding. 8.2 Document-level Relation Extraction Due to long-term dependencies that often span over hundreds of tokens, capturing entity relations have proven to be a challenging task. One approach was constructing a document-level graph from sentence encoding, then extracting entity relations from edge representations in the graph (Christopoulou et al., 2019). Other works such as Jia et al. (2019) layer classifiers in a pipeline architecture to obtain hierarchical representation of N -ary relations. 8.3 IE as Sequence Generation Paolini et al. (2021) uses a very similar generative approach, which constructs decoder targets by inserting text markers and labels around entity mentions in the input sentence. The key idea is that augmenting the decoder targets with original input sentence and labels provides stronger semantics to the model. Unfortunately, modeling cross-entity dependencies remains a challenge as entities are further apart in their deco"
2021.emnlp-main.426,2020.acl-main.703,0,0.0219482,"&gt; : end of entity. be transformed into template sequences with special tags delimiting templates, slot names, and slot values. Formally, a document of tokens D = {Di }ni=1 may correspond to a decoding target of zero to many template sequences {Ti }li=1 . A template sequence Ti is characterized by multiple slot sequences {Si,j }m j=1 , plate structures while capturing the dependencies between the input document and decoder targets, and (2) ensuring that salient mentions in the input document are correctly identified and outputted by the decoder. To achieve the first sub-goal, we leverage BART (Lewis et al., 2020), a pre-trained sequence-to-sequence model. The second sub-goal is achieved using a novel copy mechanism incorporated into BART. Ti = &lt;SOT&gt; Si,1 , ..., Si,m &lt;EOT&gt; . Seq2Seq Model for Template Generation BART (Lewis et al., 2020) is a pre-trained language model that combines bidirectional and auto-regressive transformers. Pre-training (e ) Si,j = &lt;SOSN&gt; L &lt;EOSN&gt; &lt;SOE&gt; D1 k ...Dn(ek ) &lt;EOE&gt; . with multiple denoising objectives, BART has demonstrated significant advantages in various (e ) (e ) where L is the slot name3 , and D1 k , ..., Dn k is text generation tasks, especially on summarization t"
2021.emnlp-main.426,2021.naacl-main.69,0,0.0348664,"stness across all settings, while being advantageous in lower-resource regime. Recently, there has been an increasing number of works framing information extraction tasks as sequence generation problem. Zeng et al. (2018) formed triple extraction as a sequence generation task and adopted a RNN-based model with copy mechanisms. To encourage the faithfullness of the extracted triplets, Ye et al. (2021) designed a triplet contrastive training objective. These works focus on sentence-level triplet extraction, while our work extracts role-filler entities and entity relations at the document level. Li et al. (2021); Hsu et al. (2021) formulates the document-level event argument extraction task as a conditional generation problem Acknowledgements by providing event ontology. However, their work cannot be applied to REE or RE due to the lack of We appreciate insightful feedback from PLUSLab ontology for role-filler entities and relations. Du members and the anonymous reviewers. This reet al. (2021) relied on a pointer-network-based de- search was sponsored by the Intelligence Advanced coder (Vinyals et al., 2015) to extract event role- Research Projects Activity (IARPA), via Contract filler entities, and"
2021.emnlp-main.426,2020.acl-main.714,0,0.0647601,"ure 1: A comparison between our approach and a competitive extractive system, S CI REX-P (Jain et al., 2020), on a relation extraction example from S CI REX. The task is to extract entities and identify which entities are related from the given scientific article. Due to the long distances between entities, S CI REX-P struggles to extract the right entity pair that has a relation, while our approach correctly identifies them. This reflects our method’s advantage in modeling long-term cross-entity dependencies. task-specific classifiers on top of large pre-trained language models. For example, Du and Cardie (2020a) builds a sequence tagging framework with multi-granularity representations based on BERT (Devlin et al., 2019) for role-filler entity extraction. Jain et al. (2020) builds a relation extraction pipeline upon S CI BERT (Beltagy et al., 2019). However, there are a few drawbacks of this model architecture. First, as the size of the document increases, it becomes increasingly difficult for extractive methods to capture cross-entity dependencies among entitiy types due to long distances between entities, as shown in Figure 1. Additionally, discriminative models have no information regarding the"
2021.emnlp-main.426,2021.eacl-main.52,0,0.431851,"eration problem. This formulation then allows us to capture cross-entity dependencies easily with our proposed generative model, a pre-trained sequence-tosequence model integrated with a copy mechanism. 3.1 2 Relation Extraction Template Generation Formulation We frame the REE and RE tasks as template generation problem, as shown in Figure 2. A template is composed of slot names and slot values. For both tasks, slot names are entity types, and slot values are all entity mentions corresponding to such entity types. Similar to previous works on REE (Huang and Riloff, 2011; Du and Cardie, 2020a; Du et al., 2021), we only generate one template per document without differentiating which event template each entity mention associates with. In contrast, for RE, we generate multiple templates, each corresponding to a relation. A binary relation can be represented by a template of 2 slots, whereas a 4-ary relation forms a 4-slot template. A relation template consists of typed mentions of corresponding salient entities. After transforming REE and RE annotation to templates, each template can then The REE task aims to extract all entities involved in events from the input article (Du et al., 2021). It differs"
2021.emnlp-main.426,2021.nuse-1.4,1,0.872226,"h attention head, we first transform W O to dimension h × dv × dmodel (Equation (5)), and then sum over the last two dimensions of W O (Equation (6)), (9) T 1 X − log Pfinal (yt ). T t=0 (11) Experimental Setup Dataset and Evaluation Metric Experiments are conducted on two English datasets: MUC-4 (1992) for the role-filler entity extraction task and SciREX (Jain et al., 2020) for the binary and 4-ary end-to-end relation extraction tasks. MUC-4 contains 1700 documents, with on average about 400 tokens per document. Documents are annotated with zero to multiple event templates. As per Du et al. (2021)’s pre-processing, we have a 13:2:2 split on the documents for train, development, and test, respectively. We evaluate the W O ∈ Rhdv ×dmodel → W O ∈ Rh×dv ×dmodel (5) REE task on this dataset using the entity-level metX O scorei = |Wi,j,k |. (6) ric, CEAF-REE (Du et al., 2021). The metric j,k aligns predicted entities with gold entities using where scorei denotes the significance score for Kuhn–Munkres algorithm (Kuhn, 1955; Munkres, head i. We take the attention heads with Top- 1957), where a predicted entity is considered cork highest significance scores in the last cross- rect if and only"
2021.emnlp-main.426,P11-1114,0,0.0412397,"E and RE tasks can be framed as a template generation problem. This formulation then allows us to capture cross-entity dependencies easily with our proposed generative model, a pre-trained sequence-tosequence model integrated with a copy mechanism. 3.1 2 Relation Extraction Template Generation Formulation We frame the REE and RE tasks as template generation problem, as shown in Figure 2. A template is composed of slot names and slot values. For both tasks, slot names are entity types, and slot values are all entity mentions corresponding to such entity types. Similar to previous works on REE (Huang and Riloff, 2011; Du and Cardie, 2020a; Du et al., 2021), we only generate one template per document without differentiating which event template each entity mention associates with. In contrast, for RE, we generate multiple templates, each corresponding to a relation. A binary relation can be represented by a template of 2 slots, whereas a 4-ary relation forms a 4-slot template. A relation template consists of typed mentions of corresponding salient entities. After transforming REE and RE annotation to templates, each template can then The REE task aims to extract all entities involved in events from the inp"
2021.emnlp-main.426,P17-1099,0,0.0312255,"obability Pfinal of a word wt is a weighted sum of vocabulary distribution computed by BART Pvocab and copy distribution Pcopy , Pfinal (wt ) = pgen Pvocab (wt ) + (1 − pgen )Pcopy (wt ). where pgen ∈ [0, 1] is the generation probability computed by passing the dot Pn product of the mean ei encoder hidden state e = i=0 and decoder hidn den state st at time step t through the sigmoid function σ, pgen = σ(e · st ) (10) Using the final probability distribution Pfinal , we can then compute the loss function as the average negative log likelihood of the target word yt over all timesteps, following See et al. (2017), L= 4 4.1 WiQ , WiK , WiV ∈ Rdmodel ×d are the projection matrices for computing attention. W O ∈ Rhdv ×dmodel is the matrix that allows interaction between different attention heads, where h is the number of heads. To determine the importance of each attention head, we first transform W O to dimension h × dv × dmodel (Equation (5)), and then sum over the last two dimensions of W O (Equation (6)), (9) T 1 X − log Pfinal (yt ). T t=0 (11) Experimental Setup Dataset and Evaluation Metric Experiments are conducted on two English datasets: MUC-4 (1992) for the role-filler entity extraction task a"
2021.emnlp-main.426,P19-1580,0,0.0163007,",h = softmax( dk P h αt,h , Pcopy = |H| probabilities outputted by these heads as the copy distribution as shown in equations 7 and 8, K = Top-k(score) P h∈K αt,h . Pcopy = k (7) (8) (1) (2) where αt,h is the attention scores over input tokens at decoding step t for head h. Ws and We are the projection matrices for the encoder and the decoder. st is the decoder hidden states at step t, and e denotes the encoder hidden states. However, recent studies have shown that attention heads are not equally important, and that some heads can be pruned out with a marginal decrease in overall performance (Voita et al., 2019; Michel et al., 2019). We hypothesize that the attention probabilities produced by insignificant attention heads may be noisy. Thus, computing copy distributions without these heads could improve the model’s ability to infer the importance of each token in the input document. Motivated by this hypothesis, we propose T OP K C OPY, a copy mechanism where only the Top-k important attention heads are used for computing copy distributions. Consider the formulation of multi-head attention, following the notation from Vaswani et al. (2017): MultiHead(Q, K, V ) = Concat(head1 , ..., headh )W O (3) he"
2021.emnlp-main.426,D19-1585,0,0.0755936,"igns predicted entities with gold entities using where scorei denotes the significance score for Kuhn–Munkres algorithm (Kuhn, 1955; Munkres, head i. We take the attention heads with Top- 1957), where a predicted entity is considered cork highest significance scores in the last cross- rect if and only if its corresponding mentions are a attention layer, and use the mean of the attention subset of the aligned gold entity’s mentions. 5260 REE Binary RE 4-ary RE Model Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 NST (Du and Cardie, 2020a) TANL (Paolini et al., 2021) GRIT (Du et al., 2021) DY GIE++ (Wadden et al., 2019) S CI REX-P (Jain et al., 2020) 56.82 64.89 64.19 57.04 - 48.92 47.75 47.36 46.77 - 52.58 55.02 54.50 51.40 - 0.74 2.9 6.5 0.67 12.8 41.1 0.62 3.8 9.6 0.00 0.7 0.00 17.3 0.00 0.8 T EMP G EN 68.55 49.90 57.76 17.11 13.56 14.47∗ 3.19 4.26 3.55∗ Table 1: Performance comparison on role-filler entity extraction, binary and 4-ary relation extraction tasks. TANL results are re-implemented and evaluated by ourselves. T EMP G EN outperforms all previous systems on REE, binary RE, and 4-ary RE. Statistical significance over previous best systems computed using the paired bootstrap procedure (Berg-Kirkpa"
2021.emnlp-main.426,2020.acl-main.125,0,0.209811,"n problem can be broken cross-attentions often imply saliency of input todown into two sub-goals: (1) generating valid tem4 A slot sequence Si,j is represented by slot names and entities, 3 RE. Slot name corresponds to role in REE and entity type in We have considered the SOTA abstractive summarization LM, PEGASUS (Zhang et al., 2019). Yet, the GPU memory consumption is too high for us to test it. 5259 kens, a naive approach of computing copy distributions Pcopy at time step t over the input tokens is taking the mean of the last decoder layer’s crossattention across all heads, as mentioned in Xu et al. (2020), (Ws st )T We e √ ) αt,h = softmax( dk P h αt,h , Pcopy = |H| probabilities outputted by these heads as the copy distribution as shown in equations 7 and 8, K = Top-k(score) P h∈K αt,h . Pcopy = k (7) (8) (1) (2) where αt,h is the attention scores over input tokens at decoding step t for head h. Ws and We are the projection matrices for the encoder and the decoder. st is the decoder hidden states at step t, and e denotes the encoder hidden states. However, recent studies have shown that attention heads are not equally important, and that some heads can be pruned out with a marginal decrease i"
2021.emnlp-main.426,P18-1047,0,0.0260882,"takes the top-k important cross-attentions as copy distributions is incorporated into BART for capturing key information in the input document. Experimental results on MUC-4 and S CI REX showed that T EMP G EN outperforms prior approaches on rolefiller entity extraction and end-to-end documentlevel relation extraction tasks. Under different amount of training data, T EMP G EN demonstrates robustness across all settings, while being advantageous in lower-resource regime. Recently, there has been an increasing number of works framing information extraction tasks as sequence generation problem. Zeng et al. (2018) formed triple extraction as a sequence generation task and adopted a RNN-based model with copy mechanisms. To encourage the faithfullness of the extracted triplets, Ye et al. (2021) designed a triplet contrastive training objective. These works focus on sentence-level triplet extraction, while our work extracts role-filler entities and entity relations at the document level. Li et al. (2021); Hsu et al. (2021) formulates the document-level event argument extraction task as a conditional generation problem Acknowledgements by providing event ontology. However, their work cannot be applied to R"
2021.emnlp-main.436,S15-2136,0,0.05928,"Missing"
2021.emnlp-main.436,S17-2093,0,0.0408444,"Missing"
2021.emnlp-main.436,Q14-1022,0,0.0219478,"ction, we describe details of implementing ECONET, datasets and evaluation metrics, and discuss compared methods reported in Section 4. 3.1 Implementation Details Event Detection Model. As mentioned briefly in Section 2, we train a highly accurate event preAfter training with ECONET, we fine-tune the up- diction model to mask event (triggers). We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to s"
2021.emnlp-main.436,glavas-etal-2014-hieve,0,0.0200926,". We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to show both models produce comparable results. Continual Pretraining. We randomly selected only 200K out of 10 million samples to speed up our experiments and found the results can be as good as using a lot more data. We used half of these 200K samples for temporal masked samples and the other half for the event masked samples. We ensure none"
2021.emnlp-main.436,W16-1007,0,0.0355074,"Missing"
2021.emnlp-main.436,2020.acl-main.678,0,0.244347,"rence between ERE and QA / MRC samples of event temporal reasoning. Bottom: our targeted masking strategy for ECONET v.s. random masking in PTLMs. about event temporal relations is presented, and models need to provide correct answers using the information in a given passage. Recent approaches leveraging large pre-trained language models (PTLMs) achieved state-of-the1 Introduction art results on a range of event temporal reasoning Reasoning event temporal relations is crucial for tasks (Ning et al., 2020; Pereira et al., 2020; Wang natural language understanding, and facilitates et al., 2020; Zhou et al., 2020c; Han et al., 2019b). many real-world applications, such as tracking Despite the progress, vanilla PTLMs do not focus biomedical histories (Sun et al., 2013; Bethard on capturing event temporal knowledge that can et al., 2015, 2016, 2017), generating stories (Yao be used to infer event relations. For example, in et al., 2019; Goldfarb-Tarrant et al., 2020), and Figure 1, an annotator of the QA sample can easily forecasting social events (Li et al., 2020; Jin et al., infer from the temporal indicator “following” that 2020). In this work, we study two prominent event “transfer” happens BEFORE “"
2021.emnlp-main.436,S13-2001,0,0.0365703,"mask event (triggers). We experidated MLM on the downstream tasks. ERE sam- mented with two models using event annotations ples can be denoted as [P, ei , ej , ri,j ], where P is in TORQUE (Ning et al., 2020) and TB-Dense the passage and (ei , ej ) is a pair of event trigger (Chambers et al., 2014). These two event annotatokens in P. As Figure 3a shows, we feed (P, ei , ej ) tions both follow previous event-centric reasoning 5370 2.6 Fine-tuning on Target Tasks research by using a trigger word (often a verb or an noun that most clearly describes the event’s occurrence) to represent an event (UzZaman et al., 2013; Glavaš et al., 2014; O’Gorman et al., 2016). In both cases, we fine-tune RoBERTaLARGE on the train set and select models based on the performance on the dev set. The primary results shown in Table 2 uses TORQUE’s annotations, but we conduct additional analysis in Section 4 to show both models produce comparable results. Continual Pretraining. We randomly selected only 200K out of 10 million samples to speed up our experiments and found the results can be as good as using a lot more data. We used half of these 200K samples for temporal masked samples and the other half for the event masked sa"
2021.emnlp-main.436,2020.emnlp-main.51,0,0.0295574,"pposing feedback that trains the overall model to better capture indicators with similar temporal signals. 5 Related Work with concepts randomly shuffled or generated by models, which enables language models to capture large-scale commonsense knowledge. Event Temporal Reasoning. There has been a surge of attention to event temporal reasoning research recently. Some noticeable datasets include ERE samples: TB-Dense (Chambers et al., 2014), M ATRES (Ning et al., 2018) and RED (O’Gorman et al., 2016). Previous SOTA systems on these data leveraged PTLMs and structured learning (Han et al., 2019c; Wang et al., 2020; Zhou et al., 2020c; Han et al., 2020) and have substantially improved model performances, though none of them tackled the issue of lacking event temporal knowledge in PTLMs. TORQUE (Ning et al., 2020) and MCTACO (Zhou et al., 2019) are recent MRC datasets that attempt to reason about event temporal relations using natural language rather than ERE formalism. Zhou et al. (2020a) and Zhao et al. (2020) are two recent works that attempt to incorporate event temporal knowledge in PTLMs. The formal one focuses on injecting temporal commonsense with targeted event time, frequency and duration masks"
2021.emnlp-main.513,N16-1102,0,0.0255717,"odel needs to select the original region within the set of candidate regions given masked inputs. Denoting the pre-trained vision model representations and our model representations of {vk }B k=1 as k )}B {c(vk )}B and {h(v respectively, we can k=1 k=1 represent the output probability at position i for the k-th instance as: p(vik |[vk,mask ; lk ]) k k ecos(h(vi ),c(vi ) ) =P , k k0 ecos(h(vi ),c(vj )) j,k0 where cos(·, ·) refers to the cosine similarity. (3) Bidirectional Attention Optimization (BAO). Inspired by the work on encouraging the consistency between forward and backward attentions (Cohn et al., 2016; Hu et al., 2020; Dou and Neubig, 2021), we propose an objective to encourage the symmetricity of vision-to-language and language-to-vision attentions. Specifically, after obtaining the representations h(v) and h(l), we compute the forward and backward attention matrices as: √ T ATTV L = S OFT M AX(h(v) h(l)/ d), √ T ATTLV = S OFT M AX(h(l) h(v)/ d), where d denotes the feature dimension. We then minimize the distance between them by maximizing the trace of ATTT V L ATT LV : LBAO = − log(1 + trace(ATTT V L ATT LV ) ). min(|v|, |l|) (4) Combined Objective. Our final objective is a combination"
2021.emnlp-main.513,D14-1086,0,0.0931204,"Missing"
2021.emnlp-main.513,N19-1423,0,0.057062,"Missing"
2021.emnlp-main.513,P18-1238,0,0.0694562,"Missing"
2021.emnlp-main.597,glavas-etal-2014-hieve,0,0.513779,"n Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https://github.com/PlusLabNLP/ESTER. knowledge, which could suffer from relatively low 7543 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7543–7559 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: Examples of event annotations and 5 types of QAs in our dataset. Not all events are annotated for clarity purpose. Different colors are used for better visualization. inter-annotator agreements (Glavaš et al., 2014; A few noticeable event-centric MRC datasets O’Gorman et al., 2016) and may not be the most have been proposed recently. TORQUE (Ning natural way to exploit the semantic connections et al., 2020b) and MCTACO (Zhou et al., 2019) are between relations and events in their context. two recent MRC datasets that study event tempoWe instead propose to reason about event seman- ral relations. However, knowing only the temporal aspect of events could not solve many important tic relations as a reading comprehension / question event semantic relations. For example, in Figure 1, answering task. Natural"
2021.emnlp-main.597,D19-1243,0,0.0609774,"Missing"
2021.emnlp-main.597,2020.acl-main.703,0,0.0207559,"quals to 1 if the top predicted answer, i.e. a0i,1 or a00i,1 contains a correct event trigger; otherwise it is 0. This metrics is well defined as all questions in our data contain at least an answer and all (well trained) models return at least one answers. For both generative and extractive QAs, we use the leftmost answer as the top answer. • EM or exact-match equals to 1 if ∀a0i ∈ A0i , a0i ∈ Ai and ∀ai ∈ Ai , ai ∈ A0i ; otherwise, EM = 0. 6.4 Baselines Model Baselines. For our primary generation QA task, we fine-tuned several sequence-to-sequence pre-trained language models on ESTER: BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and UnifiedQA. As mentioned, UnifiedQA (based on BART and T5) is pre-trained on various QA tasks. It also demonstrates powerful zero-shot learning capabilities on unknown QA tasks, which we tested on ESTER too. Due to computation constraints, the largest model we are able to finetune is UnifiedQA (T5-large). We leave further investigation to future modeling studies. Since extractive QA can be considered as a token prediction task, we build our model based on RoBERTa-large with token mask prediction pretraining objectives. Models and fine-tuning details can be found i"
2021.emnlp-main.597,2021.naacl-main.69,0,0.0280889,"UB - EVENT C O - REFERENCE 1.3 1.3 1.2 3.0 1.2 1.5 1.9 1.3 3.6 1.2 1.9 2.0 1.7 3.1 1.6 Table 2: Average number of answers by semantic types. 6 Experimental setup We design experiments to provide benchmark performances and understand learning challenges to facilitate future research on ESTER. We formulate our QA task as a conditional answer generation problem. This choice is inspired by recent works such as UnifiedQA (Khashabi et al., 2020) that achieve impressive outcomes by integrating various QA tasks (extractive, abstractive and multiplechoice) as a single generative QA pre-training task. Li et al. (2021) and Paolini et al. (2021) also show that by reformulating original extractive tasks as generation tasks, it enables models to better exploit semantic relations between context and labels as well as the dependencies between different outputs. To better demonstrate the benefits of the proposed generative QA task, we compare it with a traditional extractive QA task. We introduce our experimental design and evaluation metrics subsequently. 6.1 Generative QA Given a question qi and a passage Pi = {x1 , x2 , ...xj , ...xn } where xj represents a token in the passage, the answer generation task requ"
2021.emnlp-main.597,2020.emnlp-main.128,0,0.0392974,"mantic relations in ROCStories (Mostafazadeh et al., 2016a) and Event StoryLine Corpus (Caselli and Vossen, 2017) respectively. ESTER differs from these works by disentangling temporal from other semantic relations and focusing on MRC to capture five proposed event semantic relations. 3.2 Event-centric MRC Datasets leveraging natural language queries for event-centric machine reading comprehension have been proposed recently (Zhou et al., 2019; Ning et al., 2020b). However, they focus on event temporal commonsense, whereas ESTER studies other event semantic relations. Du and Cardie (2020) and Liu et al. (2020) reformulate event extraction data as QA tasks to detect event triggers and arguments in a short passage. However, they did not propose new data, and knowing event triggers and arguments are merely a sub-task in ESTER, which require both event detection and relation understanding. exams and steps to validate and train workers. 4.1 Passage Preparation Passages are selected from news articles in TempEval3 (TE3) workshop (UzZaman et al., 2013) with initial event triggers provided. We extracted 3-4 continuous sentences that contain at least 7 event triggers. Our choice of the number of sentences i"
2021.emnlp-main.597,2021.ccl-1.108,0,0.0537314,"Missing"
2021.emnlp-main.597,W14-0702,0,0.345106,"ose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https:"
2021.emnlp-main.597,C14-1198,0,0.0810508,"anguage questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, models and reproduction code are available here: https://github.com/PlusLabNLP/E"
2021.emnlp-main.597,2020.findings-emnlp.171,0,0.0764485,"an event, ESTER captures 10.1K event pairs, which are larger than previous RE datasets such as RED and HiEve. Semantic Types Train Dev Test C AUSAL C ONDITIONAL C OUNTERFACTUAL S UB - EVENT C O - REFERENCE 1.3 1.3 1.2 3.0 1.2 1.5 1.9 1.3 3.6 1.2 1.9 2.0 1.7 3.1 1.6 Table 2: Average number of answers by semantic types. 6 Experimental setup We design experiments to provide benchmark performances and understand learning challenges to facilitate future research on ESTER. We formulate our QA task as a conditional answer generation problem. This choice is inspired by recent works such as UnifiedQA (Khashabi et al., 2020) that achieve impressive outcomes by integrating various QA tasks (extractive, abstractive and multiplechoice) as a single generative QA pre-training task. Li et al. (2021) and Paolini et al. (2021) also show that by reformulating original extractive tasks as generation tasks, it enables models to better exploit semantic relations between context and labels as well as the dependencies between different outputs. To better demonstrate the benefits of the proposed generative QA task, we compare it with a traditional extractive QA task. We introduce our experimental design and evaluation metrics s"
2021.emnlp-main.597,N16-1098,0,0.205267,"C O REFERENCE , C ONDITIONAL and C OUNTERFAC TUAL , and propose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined as class labels based on expert Data, m"
2021.emnlp-main.597,2020.emnlp-main.430,0,0.298435,"lations: C AUSAL, S UB - EVENT, C O REFERENCE , C ONDITIONAL and C OUNTERFAC TUAL , and propose to use natural language questions to reason about event semantic relations. 1 Introduction Figure 2 shows example question-answer pairs for Narratives such as stories and news articles are each relation type. composed of series of events (Carey and SnodAlthough previous works studied some subset grass, 1999; Harmon, 2012). Understanding how events are logically connected is essential for read- of these relations such as S UB - EVENT (Glavaš ing comprehension (Caselli and Vossen, 2017; et al., 2014; Yao et al., 2020), C AUSAL and Mostafazadeh et al., 2016b). For example, Fig- C ONDITIONAL (Mirza et al., 2014; Mirza and Tonelli, 2014; O’Gorman et al., 2016), most of ure 1 illustrates several pairwise relations for events in the given passage: “the deal” can be consid- them adopted pairwise relation extraction (RE) formulation by constructing (event, event, relaered as the same event of “Paramount purchased tion) triplets and predicting the relation for the ∗ Part of the work was done while the author was at the pair of events. Event relations of RE formulation Allen Institute for AI. 1 are rigidly defined"
2021.emnlp-main.597,D19-1332,1,0.926103,"ngs of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7543–7559 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 2: Examples of event annotations and 5 types of QAs in our dataset. Not all events are annotated for clarity purpose. Different colors are used for better visualization. inter-annotator agreements (Glavaš et al., 2014; A few noticeable event-centric MRC datasets O’Gorman et al., 2016) and may not be the most have been proposed recently. TORQUE (Ning natural way to exploit the semantic connections et al., 2020b) and MCTACO (Zhou et al., 2019) are between relations and events in their context. two recent MRC datasets that study event tempoWe instead propose to reason about event seman- ral relations. However, knowing only the temporal aspect of events could not solve many important tic relations as a reading comprehension / question event semantic relations. For example, in Figure 1, answering task. Natural language queries ease the annotation efforts in the RE formulation by sup- to understand that “assumed debt,” “gives access” and “takes over projects” are sub-events of “the plementing expert-defined relations with textual deal,"
2021.emnlp-main.597,W16-5706,0,0.31388,"Missing"
2021.emnlp-main.597,S13-2001,0,0.260197,"ntify different 2.1 Events valid answers simultaneously as in the S UB - EVENT QA of Figure 2. These challenges make our task Adopting the general guideline of ACE (2005), we more difficult than the classification tasks in RE. define an event as a trigger word and its arguments 7544 (subject, object, time and location). An event trigger is a word that most clearly describes the event’s occurrence, and it is often a verb or noun that evokes the action or the status of the target event (Pustejovsky et al., 2003). Later event-centric reasoning work mostly uses this trigger definition, e.g., TE3 (UzZaman et al., 2013), HiEve (Glavaš et al., 2014), RED (O’Gorman et al., 2016) and TORQUE (Ning et al., 2020b). While event triggers must exist in the context, some event arguments need to be inferred by annotators. In Figure 2, for example, “getting” is an event trigger and its subject, object and location are “Europe,” “Albania” and “Europe” respectively. The event’s time can be inferred to be approximately the document writing time. To ensure event-centric reasoning, we require all questions and answers to include a trigger. Annotators are allowed to use any event arguments including those inferred to make que"
2021.findings-acl.78,2020.tacl-1.43,0,0.013119,"loop to increase the difficulty of datasets and hence more robust model training. Counterfactual editing of data samples with human annotators (Kaushik et al., 2020; Gardner et al., 2020) is also closely related to our complementary pair construction that seeks to invert the model predictions for a more reliable evaluation. Recently, several works have attempted to exploit the merits in involving both models and humans in the data creation cycle, i.e. human-and890 model-in-the-loop, to construct data samples that are both new and challenging to the models (Chen et al., 2019; Nie et al., 2020; Bartolo et al., 2020). To our best knowledge, we are the first to employ such an approach in constructing commonsense reasoning benchmark, specifically, our complementary pair formulation makes it more sophisticated as the annotators are required to not only fool the model but also pay attention to the salient concepts of their creations in both directions. 6 Conclusion We present a new challenging commonsense reasoning benchmark, C OM 2S ENSE, developed via an adversarial gamified model-in-the-loop approach. C OM 2S ENSE comprises 4k manually created complementary true/false statement pairs, designed along three"
2021.findings-acl.78,W96-0200,0,0.841188,"Missing"
2021.findings-acl.78,P18-2103,0,0.0166749,"2019) also investigate the effectiveness in the binary true/false (yes/no) formulation to construct a question answering dataset, while C OM 2S ENSE is the first to focus on commonsense reasoning. Dataset Biases: It is a widely perceived issue that spurious statistical patterns in datasets can often be exploited by machine learning models, which can potentially lead to overoptimistic judgements on the model improvements. Particularly in NLP domain, prior works have shown that hypothesis-only baselines or syntactic heuristics perform surprisingly well in the NLI task (Gururangan et al., 2018; Glockner et al., 2018; Tsuchiya, 2018; Poliak et al., 2018; McCoy et al., 2019). Model exploiting biases or failing on simple adversarial patterns, can also be seen in sentence classification (Wieting and Kiela, 2019) and question answering (Jia and Liang, 2017; Kaushik and Lipton, 2018; Geva et al., 2019) tasks. We put forth to reduce the potential sentence-level biases by requiring the models to perform equally well on both directions in a complementary true/false pair. Cross-Scenario Generalizability: Given that knowledge domain and numeracy attributes of our dataset are intuitively distinct, we intend to quant"
2021.findings-acl.78,W19-2008,0,0.0372855,"Missing"
2021.findings-acl.78,N19-1300,0,0.012151,"Comp. 63.64 58.47 59.36 64.50 35.46 26.43 28.25 43.86 Table 9: Performance of DeBERTa-large trained on X and evaluated on Y, where X and Y are partitions created as per a reasoning scenario (causal, comparative). generalization benefits from having complementary samples within the training set. sense (Lin et al., 2020a). Our work differs to these works in the focus on less factual and arithmeticprecise numerical knowledge, but more on the intuitive sense of numbers, in conjunction with our defined knowledge domains and the scenarios. It is worth noting that some prior works (Wu et al., 2017; Clark et al., 2019) also investigate the effectiveness in the binary true/false (yes/no) formulation to construct a question answering dataset, while C OM 2S ENSE is the first to focus on commonsense reasoning. Dataset Biases: It is a widely perceived issue that spurious statistical patterns in datasets can often be exploited by machine learning models, which can potentially lead to overoptimistic judgements on the model improvements. Particularly in NLP domain, prior works have shown that hypothesis-only baselines or syntactic heuristics perform surprisingly well in the NLI task (Gururangan et al., 2018; Glockn"
2021.findings-acl.78,N19-1423,0,0.0196841,"Missing"
2021.findings-acl.78,N19-1246,0,0.0401874,"Missing"
2021.findings-acl.78,2020.findings-emnlp.117,0,0.0126003,"ts more challenging to the models. Recent work AFLite (Sakaguchi et al., 2020; Le Bras et al., 2020), built upon the adversarial filtering (AF) method in (Zellers et al., 2018, 2019), adopted an iteratively improving model-inthe-loop approach to collect challenging commonsense benchmarks (Sakaguchi et al., 2020; Bisk et al., 2020). Gamified (Yang et al., 2018) or interactive (Wallace et al., 2019) approaches leverage human-in-the-loop to increase the difficulty of datasets and hence more robust model training. Counterfactual editing of data samples with human annotators (Kaushik et al., 2020; Gardner et al., 2020) is also closely related to our complementary pair construction that seeks to invert the model predictions for a more reliable evaluation. Recently, several works have attempted to exploit the merits in involving both models and humans in the data creation cycle, i.e. human-and890 model-in-the-loop, to construct data samples that are both new and challenging to the models (Chen et al., 2019; Nie et al., 2020; Bartolo et al., 2020). To our best knowledge, we are the first to employ such an approach in constructing commonsense reasoning benchmark, specifically, our complementary pair formulation"
2021.findings-acl.78,N18-2017,0,0.0382298,"Missing"
2021.findings-acl.78,D19-1243,0,0.0414474,"Missing"
2021.findings-acl.78,D17-1215,0,0.0157916,"ue that spurious statistical patterns in datasets can often be exploited by machine learning models, which can potentially lead to overoptimistic judgements on the model improvements. Particularly in NLP domain, prior works have shown that hypothesis-only baselines or syntactic heuristics perform surprisingly well in the NLI task (Gururangan et al., 2018; Glockner et al., 2018; Tsuchiya, 2018; Poliak et al., 2018; McCoy et al., 2019). Model exploiting biases or failing on simple adversarial patterns, can also be seen in sentence classification (Wieting and Kiela, 2019) and question answering (Jia and Liang, 2017; Kaushik and Lipton, 2018; Geva et al., 2019) tasks. We put forth to reduce the potential sentence-level biases by requiring the models to perform equally well on both directions in a complementary true/false pair. Cross-Scenario Generalizability: Given that knowledge domain and numeracy attributes of our dataset are intuitively distinct, we intend to quantitatively investigate if the same holds for reasoning scenarios. Our “cross-scenario” experiments with DeBERTa-large, i.e. trained on causal, evaluated on comparative and vice versa, indicate a poor generalization across both standard and p"
2021.findings-acl.78,D18-1546,0,0.015421,"istical patterns in datasets can often be exploited by machine learning models, which can potentially lead to overoptimistic judgements on the model improvements. Particularly in NLP domain, prior works have shown that hypothesis-only baselines or syntactic heuristics perform surprisingly well in the NLI task (Gururangan et al., 2018; Glockner et al., 2018; Tsuchiya, 2018; Poliak et al., 2018; McCoy et al., 2019). Model exploiting biases or failing on simple adversarial patterns, can also be seen in sentence classification (Wieting and Kiela, 2019) and question answering (Jia and Liang, 2017; Kaushik and Lipton, 2018; Geva et al., 2019) tasks. We put forth to reduce the potential sentence-level biases by requiring the models to perform equally well on both directions in a complementary true/false pair. Cross-Scenario Generalizability: Given that knowledge domain and numeracy attributes of our dataset are intuitively distinct, we intend to quantitatively investigate if the same holds for reasoning scenarios. Our “cross-scenario” experiments with DeBERTa-large, i.e. trained on causal, evaluated on comparative and vice versa, indicate a poor generalization across both standard and pairwise accuracy metrics ("
2021.findings-acl.78,D14-1162,0,0.085134,"Missing"
2021.findings-acl.78,S18-2023,0,0.0383392,"Missing"
2021.findings-acl.78,K19-1033,0,0.0144847,"e inference (NLI) can be tasked similarly to the true/false formulation, the existing commonsense NLI benchmark either is not crowdsourced with high quality (Zhang et al., 2017), or still resorts to a multiple choice setting (Bhagavatula et al., 2020). There are also benchmarks that specifically concern a type of commonsense knowledge, such as physical (Bisk et al., 2020) and social (Sap et al., 2019) intelligence, as well as temporal understanding (Zhou et al., 2019). The ability to understand and induce numerical knowledge in texts has been studied in several recent works (Dua et al., 2019; Ravichander et al., 2019), including numerical commonAdversarial Data Collection: Removing representation biases in a dataset by adversarially filtering undesired data samples, has been frequently practiced to collect datasets more challenging to the models. Recent work AFLite (Sakaguchi et al., 2020; Le Bras et al., 2020), built upon the adversarial filtering (AF) method in (Zellers et al., 2018, 2019), adopted an iteratively improving model-inthe-loop approach to collect challenging commonsense benchmarks (Sakaguchi et al., 2020; Bisk et al., 2020). Gamified (Yang et al., 2018) or interactive (Wallace et al., 2019)"
2021.findings-acl.78,D19-1454,0,0.0224058,"Missing"
2021.findings-acl.78,N19-1421,0,0.0618509,"ledge from everyday experience and make sound inferences, whether current AI systems also possess such capabilities remains an open question. Recent advancements in natural language processing (NLP) has led to a surge in 883 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 883–898 August 1–6, 2021. ©2021 Association for Computational Linguistics new benchmark datasets towards evaluating commonsense reasoning. Specifically, existing benchmarks are formulated as natural language inference (NLI) (Bhagavatula et al., 2020), multiple choice (MC) question answering (Talmor et al., 2019a; Zellers et al., 2019; Bisk et al., 2020), and machine reading comprehension (Huang et al., 2019) tasks. While recent state-of-the-art models (Liu et al., 2019; Raffel et al., 2020; Khashabi et al., 2020) have quantitatively demonstrated near human-level performance on these benchmarks, the exploitation of certain spurious patterns (Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019) in the datasets can be partly attributed to such achievements. Consider the examples in Figure 1, where each sentence is true/false, and is paired with a similar (with a few modifications) compleme"
2021.findings-acl.78,D18-1009,0,0.162473,"o random, we claim that improvements from stronger baselines should be attributed to the models and not annotation biases that they can exploit. Among the baseline models, the UnifiedQA-3B achieves the best performance on both the standard and pairwise metric. Note that the number of learnable parameters in UnifiedQA3B is much larger than those in the second and third Dataset comparisons: In order to contrast the difficulty of C OM 2S ENSE with other related benchmarks, we report the performances of two well performing models on the following: CommonsenseQA (CQA) (Talmor et al., 2019b), SWAG (Zellers et al., 2018), SocialIQA (Sap et al., 2019), PhysicalIQA (PIQA) (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2020). The results in Table 5 indicate that models clearly struggle more to perform well on C OM 2S ENSE than other datasets. Performance across domains and scenarios: In Figure 5 we present the in-depth breakdown results for T5-large and DeBERTa-large across combinations of domain, scenario and numeracy. We observe that models consistently perform worse in categories involving numeracy, highlighting the limitations of current language models. For physical domain, both models perform worse"
2021.findings-emnlp.136,P19-1470,0,0.412949,"attern. 2 Task Definition With the annotated relationships as background knowledge, we build a hyperbole generation model Given an input prompt (A), we aim to generate that takes a literal prompt (A) as input and outputs clause or sentence level hyperboles by completing a hyperbole clause (B and C combined). To this that clause. For example, if the input is ‘the party is end, we train a reverse COMeT model to gener- lit’, our task is to generate ‘the wardrobe’ (a subject ate commonsense and counterfactual phrases along B) and ‘is dancing’ (a predicate C) to make the full with the COMeT model (Bosselut et al., 2019), sentence (‘the party is so lit that even the wardrobe and rank the generated candidates with a hyper- is dancing’) a hyperbole. bole identifier. Finally, we break the restrictions of the so...that pattern, and generate hyperboles with 3 Data Collection and Analysis diverse syntactic structures using a syntactically Section 3.1 introduces how we collect hyperboles controlled paraphrase model. To the best of our and non-hyperboles sentences from Reddit. In Secknowledge, we are the first to analyze the relations tion 3.2, we describe the procedure for a detailed of the logical components within"
2021.findings-emnlp.136,2020.acl-main.711,1,0.936061,"robe is dancing! Related To Not Capable Of Figure 1: An illustration of the commonsense and counterfactual relationships within a clause or sentence level hyperbole. The input prompt (A), subject in the clause (B), predicate in the clause (C), and the relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make noncreative exaggerations to"
2021.findings-emnlp.136,2020.emnlp-main.524,1,0.937252,"robe is dancing! Related To Not Capable Of Figure 1: An illustration of the commonsense and counterfactual relationships within a clause or sentence level hyperbole. The input prompt (A), subject in the clause (B), predicate in the clause (C), and the relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make noncreative exaggerations to"
2021.findings-emnlp.136,2021.naacl-main.336,1,0.754728,"We also appreciate the kind efforts of the undergraduate students from PlusLab to annotate the relations. This work is supported by the Machine Common Sense (MCS) program under Cooperative Agreement N66001-19-2-4032 with the US Defense Advanced Research Projects Agency (DARPA). The views and the conclusions of this paper are those of the authors and do not reflect the official policy or position of DARPA. Recent years have witnessed increased interest in creative and figurative language generation. Yu and Wan (2019) generate metaphor unsupervisedly by extracting the metaphorically-used verbs; Chakrabarty et al. (2021) propose a metaphor generation method with symbolism and discriminative decoding; Stowe et al. (2021) study diverse metaphor generation using conceptual mapping. Given a pair of homophones, Yu et al. (2018) train a conditional neural language model with an decoding algorithm for pun generation; He et al. (2019) Ethics Considerations tackle the same task with a local-global surprisal principle and a retrieve-and-edit pipeline; Luo et al. We understand and respect user privacy. The (2019) on the other hand propose an adversarial HYPO-Red dataset is collected from Reddit totally pun generative ne"
2021.findings-emnlp.136,N19-1172,1,0.903617,"at) even the wardrobe is dancing! Related To Not Capable Of Figure 1: An illustration of the commonsense and counterfactual relationships within a clause or sentence level hyperbole. The input prompt (A), subject in the clause (B), predicate in the clause (C), and the relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make no"
2021.findings-emnlp.136,2020.emnlp-main.571,0,0.127812,"he relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make noncreative exaggerations to emphasize something is fast, without needing to understand the context. Hyperboles invoke the use of exaggeration as a rhetorical device or figure of speech. It is interactive, amusing, and is the second most common among all tropes of figura"
2021.findings-emnlp.136,2020.acl-main.703,0,0.0413332,"Missing"
2021.findings-emnlp.136,D19-1339,1,0.891922,"Missing"
2021.findings-emnlp.136,2021.acl-long.524,1,0.760842,"his work is supported by the Machine Common Sense (MCS) program under Cooperative Agreement N66001-19-2-4032 with the US Defense Advanced Research Projects Agency (DARPA). The views and the conclusions of this paper are those of the authors and do not reflect the official policy or position of DARPA. Recent years have witnessed increased interest in creative and figurative language generation. Yu and Wan (2019) generate metaphor unsupervisedly by extracting the metaphorically-used verbs; Chakrabarty et al. (2021) propose a metaphor generation method with symbolism and discriminative decoding; Stowe et al. (2021) study diverse metaphor generation using conceptual mapping. Given a pair of homophones, Yu et al. (2018) train a conditional neural language model with an decoding algorithm for pun generation; He et al. (2019) Ethics Considerations tackle the same task with a local-global surprisal principle and a retrieve-and-edit pipeline; Luo et al. We understand and respect user privacy. The (2019) on the other hand propose an adversarial HYPO-Red dataset is collected from Reddit totally pun generative network. anonymously, and does not reveal any details about Generating hyperboles or exaggerations is a"
2021.findings-emnlp.136,2020.figlang-1.4,0,0.0194231,"Not Capable Of Figure 1: An illustration of the commonsense and counterfactual relationships within a clause or sentence level hyperbole. The input prompt (A), subject in the clause (B), predicate in the clause (C), and the relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make noncreative exaggerations to emphasize somethi"
2021.findings-emnlp.136,2021.emnlp-main.420,1,0.715928,"ity pS : pS = MLP(pG , lAB , lAC , lBC ) (3) Note that to avoid information leakage, the training data for ClfG and ClfS do not overlap. We call the generation method with ClfS as classifier HypoGenSpec . 4.4 Breaking the so...that Pattern So far we have managed to generate hyperboles with the so...that pattern. As an extension to our proposed HypoGen, we posit that a paraphrasing module is helpful to break such pattern and hence generate hyperboles with diverse syntactic structures. Specifically, we use the syntacticallyGrammar Error Correction When we assem- controlled paraphrasing model by Sun et al. (2021) ble pieces of A, B and C into the ‘so...that’ pat- as an off-the-shelf tool, because it achieves statetern, such manipulation can cause certain grammar of-the-art performances on semantic preservation 1587 and syntactic conformation. It leverages pretrained BART (Lewis et al., 2019) and adds deliberately chosen syntactical control via a retrieval-based selection module to generate fluent paraphrases. We use HypoPara to denote HypoGenSpec added by such a paraphrasing model. 5 5.1 Hyperbole Detection Model ClfS . We train a simple MLP for ClfS and use grid search to find the best hyper-paramete"
2021.findings-emnlp.136,D18-1367,0,0.059913,"Missing"
2021.findings-emnlp.136,D19-1221,0,0.0333358,"Missing"
2021.findings-emnlp.136,P18-1153,0,0.0239669,"with the US Defense Advanced Research Projects Agency (DARPA). The views and the conclusions of this paper are those of the authors and do not reflect the official policy or position of DARPA. Recent years have witnessed increased interest in creative and figurative language generation. Yu and Wan (2019) generate metaphor unsupervisedly by extracting the metaphorically-used verbs; Chakrabarty et al. (2021) propose a metaphor generation method with symbolism and discriminative decoding; Stowe et al. (2021) study diverse metaphor generation using conceptual mapping. Given a pair of homophones, Yu et al. (2018) train a conditional neural language model with an decoding algorithm for pun generation; He et al. (2019) Ethics Considerations tackle the same task with a local-global surprisal principle and a retrieve-and-edit pipeline; Luo et al. We understand and respect user privacy. The (2019) on the other hand propose an adversarial HYPO-Red dataset is collected from Reddit totally pun generative network. anonymously, and does not reveal any details about Generating hyperboles or exaggerations is a new the users’ personal information, including name, task. To the best of our knowledge, we are the firs"
2021.findings-emnlp.136,N19-1092,0,0.122777,"igure 1: An illustration of the commonsense and counterfactual relationships within a clause or sentence level hyperbole. The input prompt (A), subject in the clause (B), predicate in the clause (C), and the relationships between them are colored in blue, red, brown and grey. In this example, that ‘the party is lit’ causes the desire to ‘dance’. In addition, ‘the wardrobe’ is related to ‘the party’, and is not capable of ‘dancing’. pared to the many efforts on other figurative languages such as puns, sarcasms, metaphors and similes (He et al., 2019; Chakrabarty et al., 2020a; Su et al., 2020; Yu and Wan, 2019; Chakrabarty et al., 2020b), the exploration of hyperboles is still in the infancy stage: NLP researchers have just started to look at automatic hyperbole detection (Troiano et al., 2018; Kong et al., 2020). According to Claridge (2010), hyperboles are divided into two categories: those at the word or phrase level and those at the clause or sentence level. The former is less creative because it is easily achievable via lexicon substitution (Norrick, 2012). For example, replacing most time durations with ‘a millisecond’ will make noncreative exaggerations to emphasize something is fast, withou"
2021.findings-emnlp.136,N19-1014,0,0.0261205,"le in ConceptNet; and ii) characteristic actions of B, from the following relationships DefinedAs, CapableOf, IsA, and UsedFor. We also compute the conditional log-likelihoods and call them lAC and lBC . Finally, we assemble pieces of A, B and C into the ‘so...that’ pattern. The candidate sentence is: ‘A.1 is so A.2 that B even C!’. errors such as mismatch of verb tenses, or singularity/plurality. While writing a rule-based grammar error correction (GEC) algorithm can be effective for a set of these common errors, we hope to fix open-ended grammar errors. Therefore, we choose the GEC model by Zhao et al. (2019), a widely used neural architecture for the GEC problem with copy-augmented architecture and token-level and sentence-level multi-task learning. 4.3 Hyperbole Candidate Ranking We build two classifiers to score and rank the hyperbole candidates. We later compare their performance through human evaluation and ablation in Section 6 and Section 7. The Generic Classifier First, we train a generic hyperbole classification model by finetuning BERT (Devlin et al., 2018) with the data collected in Section 3.1. Before training, we deliberately remove all the keywords such as I swear, literally, so . ."
2021.findings-emnlp.353,P19-1474,0,0.0163635,"182–4194 November 7–11, 2021. ©2021 Association for Computational Linguistics where the circumference of a negative-curved space grows exponentially with regard to the radius as illustrated in Figure 1, can better capture such special characteristics of taxonomies. In this paper, we present H YPER E XPAN, a taxonomy expansion framework based on hyperbolic representation learning, that: (1) better preserves the taxonomical structure in a more expressive hyperbolic space, (2) effectively characterizes concepts by exploiting sparse neighborhood information beyond standard parent-child relations (Aly et al., 2019; Le et al., 2019), and (3) improves inference precision and generalizability by leveraging pretrained distributional features. 1 Specifically, H YPER E XPAN incorporates two types of features to exploit the structural presentation of a taxonomy: a relative positional embedding of a node depending on its relation to the anchor node, and an absolute positional embedding defined by its depth within a taxonomy. H YPER E XPAN first constructs an ego subgraph around the potential attaching candidate concepts, i.e. the anchor concepts, and then leverages a hyperbolic graph neural network (HGNN) to o"
2021.findings-emnlp.353,Q17-1010,0,0.0470125,"can be either Euclidean, such as fast4184 … Parent Self Children cook Initial concept features Ego graph Query concept Absolute Depth Graph Readout Hidden layers roast fry Hyperbolic GNN Positional embeddings change integrity grill Logarithmic map Initial concept feature Aggregation Exponential map Anchor concept representation Hyperbolic MLP Score Query concept representation Figure 2: H YPER E XPAN’s model design. Red node is the anchor concept and the highlighted sub-tree is the ego graph of the anchor node. The intermediate flat surface is the tangent space based on the anchor node. Text (Bojanowski et al., 2017), or hyperbolic, such as Poincaré GloVe (Tifrea et al., 2019), which embeds words in a Cartesian product of hyperbolic spaces. Note that since Poincaré GloVe is defined in hyperbolic space, the aforementioned mean operation can no longer be the usual Euclidean average since it may produce results that are out of the manifold. Instead, we use Einstein midpoint method (Gülçehre et al., 2019) to perform the average pooling. Denote the token embeddings as ei and N as number of tokens in a sentence, the midpoint can be computed as: µ= ∑N i=1 γi ei ∑N i=1 γi where γi = 1 2 denotes the Lorentz factor"
2021.findings-emnlp.353,W18-1708,0,0.0264594,"thm to learn embeddings in a supervised manner based on Riemannian optimization and shows it performs well on link prediction task even with a smaller dimension. Ganea et al. (2018) presents common neural network operations in hyperbolic space and Liu et al. (2019b) extends GNN operations to Riemannian manifolds with differentiable exponential and logarithmic maps. Most related to our work, Chami et al. (2019) derives Graph Convolutional Neural Network (GCN)’s operations in the Lorentz model of hyperbolic space. Hyperbolic representation learning is broadly applied to lexical representations (Dhingra et al., 2018; Tifrea et al., 2019; Zhu et al., 2020), organizational chart induction (Chen and Quirk, 2019), hierarchical classification (López and Strube, 2020; Chen et al., 2020), knowledge association (Sun et al., 2020), knowledge graph completion (Wang et al., 2021a; Balazevic et al., 2019) and event prediction (Surís et al., 2021). A more comprehensive summarization is given in a recent survey by Peng et al. (2021). 6 Conclusion and Future Work We present H YPER E XPAN, a taxonomy expansion model which better preserves the taxonomical structure in an expressive hyperbolic space. We use an HGNN to inc"
2021.findings-emnlp.353,C92-2082,0,0.369571,"r endorsements, either expressed or implied, of DARPA or the U.S. Government. Ethical Considerations There are studies that leverage hyperbolic repre- This work does not present any direct societal consentation learning to perform taxonomy extraction sequence. The proposed method aims at improving from text, which are connected to this work. Such representation learning to support automated exstudies use Poincaré embeddings trained by hyper- pansion of taxonomies. We believe this study leads nymy pairs extracted by lexical-syntactic patterns to intellectual merits that benefit from automated (Hearst, 1992) to infer missing nodes (Le et al., knowledge acquisition for constructing knowledge 2019) and refine preexisting taxonomies (Aly et al., representations with complex or sparse structures. 2019). The patterns suffer from missing and in- It could also potentially lead to broad impacts, as correct extractions, and are dedicated to capturing the obtained taxonomical knowledge representahypernymy relations between nouns. Hence, only tions can support various knowledge-driven tasks. terms that are recognizable by the designed pat- It is important to note that the precision of top taxterns are able"
2021.findings-emnlp.353,S16-1169,0,0.0389032,"Missing"
2021.findings-emnlp.353,P19-1313,0,0.017715,"7–11, 2021. ©2021 Association for Computational Linguistics where the circumference of a negative-curved space grows exponentially with regard to the radius as illustrated in Figure 1, can better capture such special characteristics of taxonomies. In this paper, we present H YPER E XPAN, a taxonomy expansion framework based on hyperbolic representation learning, that: (1) better preserves the taxonomical structure in a more expressive hyperbolic space, (2) effectively characterizes concepts by exploiting sparse neighborhood information beyond standard parent-child relations (Aly et al., 2019; Le et al., 2019), and (3) improves inference precision and generalizability by leveraging pretrained distributional features. 1 Specifically, H YPER E XPAN incorporates two types of features to exploit the structural presentation of a taxonomy: a relative positional embedding of a node depending on its relation to the anchor node, and an absolute positional embedding defined by its depth within a taxonomy. H YPER E XPAN first constructs an ego subgraph around the potential attaching candidate concepts, i.e. the anchor concepts, and then leverages a hyperbolic graph neural network (HGNN) to obtain the anchor c"
2021.findings-emnlp.353,P16-1226,0,0.0485909,"Missing"
2021.findings-emnlp.353,2020.emnlp-main.333,0,0.063801,"Missing"
2021.findings-emnlp.353,2020.findings-emnlp.42,0,0.230297,"we can use log and exp to perform the projection within a neural network that has a mixture of hyperbolic and Euclidean layers. The addition and matrix multiplication operations in Poincaré model are based on Möbius transformation (Ungar, 2001; Ganea et al., 2018; Gülçehre et al., 2019), which are defined in Table 1. In the Lorentz model, we utilize the tangent space to perform matrix multiplication and parallel transport to perform the addition (Chami et al., 2019). For concatenating two hyperbolic vectors, we perform a generalized version of the concatenation operation (Ganea et al., 2018; López and Strube, 2020) to prevent the resulting vector from being 2 Here we assume a unit hyperbolic space (curvature = −1) in this section. 4183 Poincaré Ball Lorentz Model 2 d(x, y) = cosh−1 (1 + 2 (1−∥x∥∥x−y∥ 2 )(1−∥y∥2 ) ) Distance v expx (v) = x ⊕ (tanh ( λx2∥v∥ ) ∥v∥ ) Exponential Map Logarithmic Map logx (y) = x⊕y = Addition 2 λx artanh(∥ − x ⊕ −x⊕y y∥) ∥−x⊕y∥ (1+2⟨x,y⟩+∥y∥2 )x+(1−∥x∥2 )y d(x, y) = arcosh (− < x, y >L ) ∥v∥L √ expK x (v) = cosh ( K ) x + logK x (y) = √ √ L) v K sinh ( ∥v∥ ∥v∥ K L 1 ⟨x,y⟩ x y+ K L dK L (x, y) ∥y+ 1 ⟨x,y⟩ x∥ K L L K xH ⊕K y ∶= expK xH (Po→xH (y)) 1+2⟨x,y⟩+∥x∥2 ∥y∥2 x∥ Mx Matri"
2021.findings-emnlp.353,D12-1104,0,0.112673,"Missing"
2021.findings-emnlp.353,2020.emnlp-main.460,1,0.778906,"operations in hyperbolic space and Liu et al. (2019b) extends GNN operations to Riemannian manifolds with differentiable exponential and logarithmic maps. Most related to our work, Chami et al. (2019) derives Graph Convolutional Neural Network (GCN)’s operations in the Lorentz model of hyperbolic space. Hyperbolic representation learning is broadly applied to lexical representations (Dhingra et al., 2018; Tifrea et al., 2019; Zhu et al., 2020), organizational chart induction (Chen and Quirk, 2019), hierarchical classification (López and Strube, 2020; Chen et al., 2020), knowledge association (Sun et al., 2020), knowledge graph completion (Wang et al., 2021a; Balazevic et al., 2019) and event prediction (Surís et al., 2021). A more comprehensive summarization is given in a recent survey by Peng et al. (2021). 6 Conclusion and Future Work We present H YPER E XPAN, a taxonomy expansion model which better preserves the taxonomical structure in an expressive hyperbolic space. We use an HGNN to incorporate neighborhood information and positional features of concepts, as well as profile features that are essential to jump-start zero-shot concept representations. Experimental results on WordNet and Microso"
2021.findings-emnlp.353,toral-etal-2008-named,0,0.0635423,"3 a human curated seed taxonomy (Vedula et al., 10 w/o both Positional Emb 0.482 38.8 12.5 2018). Traditional methods leverage pre-defined H YPER E XPAN 0.517 42.7 15.0 patterns to extract hypernym-hyponym pairs for taxonomy expansion (Nakashole et al., 2012; Jiang Table 4: Experimental results for ablation studies on et al., 2017; Agichtein and Gravano, 2000). Some WordNet-Verb. By default, we use trainable curvaworks use external data and expand taxonomy in a ture, Lorentz hyperbolic model, Poincaré GloVe as initial word embedding, 2-hop computational graph withspecific domain. For example, Toral et al. (2008) out anchor’s sibilings, with both relative and absolute use Wikipedia named entities to expand WordNet, position embedding. “i/o” means “instead of”, “w/o” Wang et al. (2014) use query logs to expand search means “without”. engine category taxonomy. Some works expand a generic taxonomy without using external resources. For example, Shwartz et al. (2016) encode taxonIn lines 8 to 10, we investigate the effect of posi- omy traversal paths to seize on the dependency tional embeddings. A larger performance drop is between concepts, Shen et al. (2020) use a GNN caused if we remove relative positio"
2021.findings-emnlp.353,D17-1123,0,0.0533161,"Missing"
2021.findings-emnlp.353,D19-1145,0,0.0293394,"Missing"
2021.findings-emnlp.353,2020.findings-emnlp.104,0,0.0975288,"Missing"
2021.naacl-demos.4,2007.sigdial-1.21,0,0.0425843,"gh responses generated by such models are fluent and locally coherent, they usually suffer from content poverty (e.g., generating non-informative content), which can negatively impact user engagement. Furthermore, these models do not allow the users to exert control on the generation process and guide the conversation toIntroduction Over the past decade, users have actively engaged with dialogue systems to fulfill a wide range of requirements. Task-oriented dialogue systems have assisted users in accomplishing specific tasks such as finding apartments (Gustafson et al., 2000) and restaurants (Gruenstein and Seneff, 2007) or even booking movie tickets (Li et al., 2017). While, Open-domain dialogue systems have been extensively leveraged for psychotherapy counseling, entertainment, and even teaching foreign languages to users (Zhou et al., 2020; Oh et al., 2017; Sarosa 26 Proceedings of NAACL-HLT 2021: Demonstrations, pages 26–34 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: A snapshot of the proposed DiSCoL system ward users’ desired direction. The first block in Figure 1 depicts an example of a generated response by DialoGPT. tracted from previous modules. The middle block of Figu"
2021.naacl-demos.4,2020.aacl-main.59,0,0.0205199,"ntations of utterances in the dialogues that can be used as content planning elements to form high-level content of an utterance and guide the generator to incorporate these informative units in the generation (See colored boxes in Figure 1). Content planning has been shown to be beneficial in the story generation task. These abstract representations known as storylines or story plots have been successful to guide the language models produce more coherent and fluent stories (Yao et al., 2019; Goldfarb-Tarrant et al., 2019; Fan et al., 2019; Goldfarb-Tarrant et al., 2020; Rashkin et al., 2020; Brahman et al., 2020). DiSCoL is composed of four main neuralnetwork-based modules (See Figure 3). The first two modules are designed to extract entities and topics of the dialogue context. The third module is a fine-tuned conditional generator that learns to take the dialogue context and previously extracted information and predict convlines that would be leveraged in the response generator module. Similar to convline generator, response generator is a conditional auto-regressive language model that generates response conditioned on the dialogue context and its convlines, entities, and topics ex2 system Architect"
2021.naacl-demos.4,N19-1423,0,0.0438608,"fier General Entertainment Convline Generator game of thrones, game of lions, love the game, favorite show, favorite character, show lol Response Generator I love the game of thrones! My favorite show lol! Game of lions is also my favorite show, who is your favorite character? Figure 3: Architecture of DiSCoL system generator, incorporates all this information to generate a response as the output of the system. In this section, we explain each module in detail. 2.1 are later used for predicting convlines and consequently generating responses. Due to the proven effectiveness of the BERT model (Devlin et al., 2019) and its wide applicability in many classification tasks, we incorporate it into the topic classifier module of DiSCoL. We finetune BERT model on pairs of utterances and their aligned topics with the main goal of minimizing the cross-entropy loss. Entity Extractor One of the principal components in the conversational systems is the set of entities that both interlocutors are interested to converse about. It is crucial that the system can identify the main entities from the dialogue context and try to continue the conversation by providing more relevant information or even expressing its opinio"
2021.naacl-demos.4,P19-1254,0,0.373623,"informative and content-rich responses. Convlines are abstract representations of utterances in the dialogues that can be used as content planning elements to form high-level content of an utterance and guide the generator to incorporate these informative units in the generation (See colored boxes in Figure 1). Content planning has been shown to be beneficial in the story generation task. These abstract representations known as storylines or story plots have been successful to guide the language models produce more coherent and fluent stories (Yao et al., 2019; Goldfarb-Tarrant et al., 2019; Fan et al., 2019; Goldfarb-Tarrant et al., 2020; Rashkin et al., 2020; Brahman et al., 2020). DiSCoL is composed of four main neuralnetwork-based modules (See Figure 3). The first two modules are designed to extract entities and topics of the dialogue context. The third module is a fine-tuned conditional generator that learns to take the dialogue context and previously extracted information and predict convlines that would be leveraged in the response generator module. Similar to convline generator, response generator is a conditional auto-regressive language model that generates response conditioned on the d"
2021.naacl-demos.4,I17-1074,0,0.0292585,"coherent, they usually suffer from content poverty (e.g., generating non-informative content), which can negatively impact user engagement. Furthermore, these models do not allow the users to exert control on the generation process and guide the conversation toIntroduction Over the past decade, users have actively engaged with dialogue systems to fulfill a wide range of requirements. Task-oriented dialogue systems have assisted users in accomplishing specific tasks such as finding apartments (Gustafson et al., 2000) and restaurants (Gruenstein and Seneff, 2007) or even booking movie tickets (Li et al., 2017). While, Open-domain dialogue systems have been extensively leveraged for psychotherapy counseling, entertainment, and even teaching foreign languages to users (Zhou et al., 2020; Oh et al., 2017; Sarosa 26 Proceedings of NAACL-HLT 2021: Demonstrations, pages 26–34 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: A snapshot of the proposed DiSCoL system ward users’ desired direction. The first block in Figure 1 depicts an example of a generated response by DialoGPT. tracted from previous modules. The middle block of Figure 1 exhibits the generated response for the inf"
2021.naacl-demos.4,W19-2310,1,0.841603,"sponses through computing different metrics. We conduct automatic evaluations and compute evaluation metrics on 23,530 consecutive utterance pairs (dialogue context utterances and their ground-truth responses) of the Topical chat test set. The measured metrics are averaged over all utterance pairs within the test set. We compute BLEU-3 (Papineni et al., 2002) to evaluate the similarity of generated responses to ground-truth responses based on the 3-grams overlaps. Due to the one-to-many essence of opendomain dialogue systems and the imperfection of such word-overlap metrics (Liu et al., 2016; Ghazarian et al., 2019; Mehri and Eskenazi, 2020), we also focus on three main aspects: diversity, relevancy, and engagingness as better indications of systems performances. Diversity measures the percentage of distinct generated tokens by each model. Li et al. (2015) proposed distinct-2 that computes distinct bi-grams divided by the total number of generated words. Relevancy utilizes both dialogue context utterance and the generated response to deliberate how much it is relevant to the given utterance (Tao et al., 2018; Ghazarian et al., 2019). We use the contextualized Ruber metric for this purpose (Ghazarian et"
2021.naacl-demos.4,D16-1230,0,0.0268828,"loGPT generated responses through computing different metrics. We conduct automatic evaluations and compute evaluation metrics on 23,530 consecutive utterance pairs (dialogue context utterances and their ground-truth responses) of the Topical chat test set. The measured metrics are averaged over all utterance pairs within the test set. We compute BLEU-3 (Papineni et al., 2002) to evaluate the similarity of generated responses to ground-truth responses based on the 3-grams overlaps. Due to the one-to-many essence of opendomain dialogue systems and the imperfection of such word-overlap metrics (Liu et al., 2016; Ghazarian et al., 2019; Mehri and Eskenazi, 2020), we also focus on three main aspects: diversity, relevancy, and engagingness as better indications of systems performances. Diversity measures the percentage of distinct generated tokens by each model. Li et al. (2015) proposed distinct-2 that computes distinct bi-grams divided by the total number of generated words. Relevancy utilizes both dialogue context utterance and the generated response to deliberate how much it is relevant to the given utterance (Tao et al., 2018; Ghazarian et al., 2019). We use the contextualized Ruber metric for thi"
2021.naacl-demos.4,2020.sigdial-1.28,0,0.0266099,"ge YAKE (Campos et al., 2018) for retrieving discourse keywords representing convlines. YAKE assigns an 3 We fine-tune BART model using https://github. com/pytorch/fairseq 30 0.8 DialoGPT DiSCoL 0.7 Scores 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Diversity Bleu Relevancy Engagement Figure 5: Automatic evaluations on responses generated by DiSCoL and DialoGPT systems Figure 6: Human evaluations on responses generated by DiSCoL and DialoGPT systems 4.2 4.3 Automatic Evaluations Due to the multi-faceted nature of dialogue quality, it is necessary to do the evaluation from different aspects (See et al., 2019; Mehri and Eskenazi, 2020). To this end, we compare the quality of DiSCoL and DialoGPT generated responses through computing different metrics. We conduct automatic evaluations and compute evaluation metrics on 23,530 consecutive utterance pairs (dialogue context utterances and their ground-truth responses) of the Topical chat test set. The measured metrics are averaged over all utterance pairs within the test set. We compute BLEU-3 (Papineni et al., 2002) to evaluate the similarity of generated responses to ground-truth responses based on the 3-grams overlaps. Due to the one-to-many essence of opendomain dialogue syst"
2021.naacl-demos.4,2020.emnlp-main.351,1,0.866384,"ontent-rich responses. Convlines are abstract representations of utterances in the dialogues that can be used as content planning elements to form high-level content of an utterance and guide the generator to incorporate these informative units in the generation (See colored boxes in Figure 1). Content planning has been shown to be beneficial in the story generation task. These abstract representations known as storylines or story plots have been successful to guide the language models produce more coherent and fluent stories (Yao et al., 2019; Goldfarb-Tarrant et al., 2019; Fan et al., 2019; Goldfarb-Tarrant et al., 2020; Rashkin et al., 2020; Brahman et al., 2020). DiSCoL is composed of four main neuralnetwork-based modules (See Figure 3). The first two modules are designed to extract entities and topics of the dialogue context. The third module is a fine-tuned conditional generator that learns to take the dialogue context and previously extracted information and predict convlines that would be leveraged in the response generator module. Similar to convline generator, response generator is a conditional auto-regressive language model that generates response conditioned on the dialogue context and its convlin"
2021.naacl-demos.4,N19-4016,1,0.808976,"s to add control for generating informative and content-rich responses. Convlines are abstract representations of utterances in the dialogues that can be used as content planning elements to form high-level content of an utterance and guide the generator to incorporate these informative units in the generation (See colored boxes in Figure 1). Content planning has been shown to be beneficial in the story generation task. These abstract representations known as storylines or story plots have been successful to guide the language models produce more coherent and fluent stories (Yao et al., 2019; Goldfarb-Tarrant et al., 2019; Fan et al., 2019; Goldfarb-Tarrant et al., 2020; Rashkin et al., 2020; Brahman et al., 2020). DiSCoL is composed of four main neuralnetwork-based modules (See Figure 3). The first two modules are designed to extract entities and topics of the dialogue context. The third module is a fine-tuned conditional generator that learns to take the dialogue context and previously extracted information and predict convlines that would be leveraged in the response generator module. Similar to convline generator, response generator is a conditional auto-regressive language model that generates response co"
2021.naacl-demos.4,P02-1040,0,0.109188,"4.2 4.3 Automatic Evaluations Due to the multi-faceted nature of dialogue quality, it is necessary to do the evaluation from different aspects (See et al., 2019; Mehri and Eskenazi, 2020). To this end, we compare the quality of DiSCoL and DialoGPT generated responses through computing different metrics. We conduct automatic evaluations and compute evaluation metrics on 23,530 consecutive utterance pairs (dialogue context utterances and their ground-truth responses) of the Topical chat test set. The measured metrics are averaged over all utterance pairs within the test set. We compute BLEU-3 (Papineni et al., 2002) to evaluate the similarity of generated responses to ground-truth responses based on the 3-grams overlaps. Due to the one-to-many essence of opendomain dialogue systems and the imperfection of such word-overlap metrics (Liu et al., 2016; Ghazarian et al., 2019; Mehri and Eskenazi, 2020), we also focus on three main aspects: diversity, relevancy, and engagingness as better indications of systems performances. Diversity measures the percentage of distinct generated tokens by each model. Li et al. (2015) proposed distinct-2 that computes distinct bi-grams divided by the total number of generated"
2021.naacl-demos.4,2020.emnlp-main.349,0,0.0521415,"Missing"
2021.naacl-demos.4,W03-0419,0,0.723919,"Missing"
2021.naacl-demos.4,N19-1170,0,0.0266587,"system. We leverage YAKE (Campos et al., 2018) for retrieving discourse keywords representing convlines. YAKE assigns an 3 We fine-tune BART model using https://github. com/pytorch/fairseq 30 0.8 DialoGPT DiSCoL 0.7 Scores 0.6 0.5 0.4 0.3 0.2 0.1 0.0 Diversity Bleu Relevancy Engagement Figure 5: Automatic evaluations on responses generated by DiSCoL and DialoGPT systems Figure 6: Human evaluations on responses generated by DiSCoL and DialoGPT systems 4.2 4.3 Automatic Evaluations Due to the multi-faceted nature of dialogue quality, it is necessary to do the evaluation from different aspects (See et al., 2019; Mehri and Eskenazi, 2020). To this end, we compare the quality of DiSCoL and DialoGPT generated responses through computing different metrics. We conduct automatic evaluations and compute evaluation metrics on 23,530 consecutive utterance pairs (dialogue context utterances and their ground-truth responses) of the Topical chat test set. The measured metrics are averaged over all utterance pairs within the test set. We compute BLEU-3 (Papineni et al., 2002) to evaluate the similarity of generated responses to ground-truth responses based on the 3-grams overlaps. Due to the one-to-many essence"
2021.naacl-demos.4,2020.cl-1.2,0,0.0383507,"Missing"
2021.naacl-demos.7,Q14-1022,0,0.063546,"Missing"
2021.naacl-demos.7,N19-1423,0,0.110932,"he model introduced in § 2.2 extracts semantic-rich events following the ACE ontology, and the model introduced in § 2.3 predicts the event duration. Note that our system 57 Figure 3: Overall system design of EventPlus. The raw text is first fed into two event extraction components, and then we pass the event triggers of the merged event list to event duration detection and temporal relation extraction models. Finally outputs from all models are combined for visualization. event relation signals can be helpful to distinguish event triggers and non-event tokens. The model feeds BERT embedding (Devlin et al., 2019) of the input text to a shared BiLSTM layer for encoding task-specific contextual information. The output of the BiLSTM is passed to an event scoring function and a relation scoring function which are MLP classifiers to calculate the probability of being an event (for event extraction) or a probability distribution over all possible relations (for temporal relation extraction). We train the multitask model on MATRES (Ning et al., 2018a) containing temporal relations BEFORE, AFTER, SIMUL TANEOUS and VAGUE. Though the model performs both tasks during training, it can be separately used for each"
2021.naacl-demos.7,2020.emnlp-main.461,1,0.849872,"Missing"
2021.naacl-demos.7,2020.findings-emnlp.114,1,0.761215,"in § 2.3.8 5 Extension to Biomedical Domain With our flexible design, each component of EventPlus can be easily extended to other domains with little modification. We explore two approaches to extend the event extraction capability (§ 2.2) to the biomedicine domain: 1) multi-domain training (MDT) with GENIA (Kim et al., 2009), a dataset containing biomolecular interaction events from scientific literature, with shared token embeddings, which enables the model to predict on both news and biomedical text; 2) replace the current component with an in-domain event extraction component SciBERT-FT (Huang et al., 2020) which is a biomedical event extraction system based on fine-tuned SciBERT (Beltagy et al., 2019). ACE-Duration Acc Acc-c Corr 0.38 0.68 0.62 0.47 0.67 0.50 0.49 0.74 0.66 0.31 0.67 0.64 0.45 0.79 0.70 Table 3: Event duration detection experimental result. Typical-Duration results are from testing subset. Notations in the bracket of model names indicate resources for training, U: 466 UDS-T high IAA samples, T: Typical-Duration training set Experimental results in Table 3 show the BERT model is better than UDS-T ELMo-based model in general and data augmentation is especially helpful to improve"
2021.naacl-demos.7,D19-1243,0,0.0608481,"Missing"
2021.naacl-demos.7,2020.emnlp-main.351,1,0.823646,"Missing"
2021.naacl-demos.7,2020.lrec-1.704,0,0.0117572,"are presented using BIO encoding. certain types of argument roles. We enforce that given the predicted trigger label, the argument roles in this sequence can only take those that are valid for this trigger. (e.g., would) or negation (e.g., not) keywords (Konstantinova et al., 2012). Since those events do not happen, we mark them with special labels. For example, in the sentence “The United States is not considering sending troops to Mozambique”, we identify “send” will not happen. We adapt the BERT-based negation and speculation cue detection model and the scope resolution model introduced by Khandelwal and Sawant (2020). To fine-tune these models, we use the SFU Review dataset with negation and speculation annotations (Taboada et al., 2006; Taboada and Grieve, 2004; Konstantinova et al., 2012), and we feed ground truth negation and speculation cues as input for the scope resolution model. We evaluate the two models on a separate testing set of the SFU Review dataset. The cue detection model yields a 0.92 F1 score, and the scope resolution model yields a 0.88 F1 score for token-level prediction, given ground truth cues as input. In EventPlus, we input cues detected by the cue detection model to the scope reso"
2021.naacl-demos.7,N19-4016,1,0.832132,"ily decision making. For example, given the raw text shown in Figure 1, a person can infer lots of information including event trigger and type, event related arguments (e.g., agent, patient, location), event duration and temporal relations between events based on the linguistic and common sense knowledge. These understandings help people comprehend the situation and prepare for future events. The event and temporal knowledge are helpful for many downstream applications including question answering (Meng et al., 2017; Huang et al., 2019), story generation (Peng et al., 2018; Yao et al., 2019; Goldfarb-Tarrant et al., 2019, 2020), and forecasting (Wang et al., 2017; Granroth-Wilding and Clark, 2016; Li et al., 2018). ∗ 1 The system is publicly accessible at https: //kairos-event.isi.edu. The source code is available at https://github.com/PlusLabNLP/ EventPlus. We also provide an introductory video at https://pluslabnlp.github.io/eventplus. Equal contribution. 56 Proceedings of NAACL-HLT 2021: Demonstrations, pages 56–65 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: The interface of EventPlus. Users can either choose examples or freely input text which matches with their selected top"
2021.naacl-demos.7,L18-1086,0,0.144283,"can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications. 1 Figure 1: Event understanding components. We highlight events triggers in yellow, and mark the predicted task-related information in Italic. Despite the importance, there are relatively few tools available for users to conduct text-based temporal event understanding. Researchers have been building natural language processing (NLP) analysis tools for “core NLP” tasks (Gardner et al., 2018; Manning et al., 2014; Khashabi et al., 2018). However, systems that target at semantic understanding of events and their temporal information are still under-explored. There are individual works for event extraction, temporal relation detection and event duration detection, but they are separately developed and thus cannot provide comprehensive and coherent temporal event knowledge. We present EventPlus, the first pipeline system integrating several high-performance temporal event information extraction models for comprehensive temporal event understanding. Specifically, EventPlus contains event extraction (both on defined ontology and"
2021.naacl-demos.7,W09-1401,0,0.0959183,"75.5 Table 4: Experimental result for temporal relation extraction given golden event extraction result Table 4 shows the experimental results.7 Our model in § 2.1 achieves the best result on temporal relation extraction and is significantly better than (Vashishtha et al., 2019) mentioned in § 2.3.8 5 Extension to Biomedical Domain With our flexible design, each component of EventPlus can be easily extended to other domains with little modification. We explore two approaches to extend the event extraction capability (§ 2.2) to the biomedicine domain: 1) multi-domain training (MDT) with GENIA (Kim et al., 2009), a dataset containing biomolecular interaction events from scientific literature, with shared token embeddings, which enables the model to predict on both news and biomedical text; 2) replace the current component with an in-domain event extraction component SciBERT-FT (Huang et al., 2020) which is a biomedical event extraction system based on fine-tuned SciBERT (Beltagy et al., 2019). ACE-Duration Acc Acc-c Corr 0.38 0.68 0.62 0.47 0.67 0.50 0.49 0.74 0.66 0.31 0.67 0.64 0.45 0.79 0.70 Table 3: Event duration detection experimental result. Typical-Duration results are from testing subset. No"
2021.naacl-demos.7,K19-1062,1,0.913729,"NLU) tool to facilitate downstream applications. • Each component in EventPlus has comparable performance to the state-of-the-art, which assures the quality and efficacy of our system for temporal event reasoning. 2 2.1 Multi-task Learning of Event Trigger and Temporal Relation Extraction The event trigger extraction component takes the input of raw text and outputs single-token event triggers. The input to the temporal relation extraction model is raw text and a list of detected event triggers. The model will predict temporal relationships between each pair of events. In previous literature (Han et al., 2019b), multi-task learning of these two tasks can significantly improve performance on both tasks following the intuition that Component In this section, we introduce each component in our system, as shown in Figure 3. We use a multitask learning model for event trigger and temporal relation extraction (§ 2.1). The model introduced in § 2.2 extracts semantic-rich events following the ACE ontology, and the model introduced in § 2.3 predicts the event duration. Note that our system 57 Figure 3: Overall system design of EventPlus. The raw text is first fed into two event extraction components, and t"
2021.naacl-demos.7,konstantinova-etal-2012-review,0,0.0812242,"Missing"
2021.naacl-demos.7,D19-1642,0,0.040132,"Missing"
2021.naacl-demos.7,N19-4019,0,0.0171618,"., 2018) provide an interface for a set of useful models. Some tools integrate several models in a pipeline fashion (Peng et al., 2015; Noji and Miyao, 2016). The majority of these systems focus on token-level tasks like tokenization, lemmatization, part-of-speech tagging, or sentence-level tasks like syntactic parsing, semantic role labeling etc. There are only a few systems that can provide capabilities of event extraction and temporal information detection (Tao et al., 2013; Ning, 2019). For event extraction, some systems only provide results within a certain defined ontology such as AIDA (Li et al., 2019), there are also some works utilizing data from multiple modalities (Li et al., 2020a,b). Some works could handle novel events (Xiang and Wang, 2019; Ahmad et al., 2021; Han et al., 2020b; Huang and Peng, 2020), but they are either restricted to a certain domain (Yang et al., 2018) or lack of performance superiority because of their lexico-syntactic rule-based algorithm (Valenzuela-Esc´arcega et al., 2015). For temporal information detection, Ning et al. (2019) proposes a neural-based temporal relation extraction system with knowledge injection. Most related to our work, Ning et al. (2018b) de"
2021.naacl-demos.7,P18-1122,0,0.273771,"dels are combined for visualization. event relation signals can be helpful to distinguish event triggers and non-event tokens. The model feeds BERT embedding (Devlin et al., 2019) of the input text to a shared BiLSTM layer for encoding task-specific contextual information. The output of the BiLSTM is passed to an event scoring function and a relation scoring function which are MLP classifiers to calculate the probability of being an event (for event extraction) or a probability distribution over all possible relations (for temporal relation extraction). We train the multitask model on MATRES (Ning et al., 2018a) containing temporal relations BEFORE, AFTER, SIMUL TANEOUS and VAGUE. Though the model performs both tasks during training, it can be separately used for each individual task during inference. 2.2 that are associated with sub-types. Similar to Han et al. (2019b), we build our event extraction component for ACE ontology upon a multi-task learning framework that consists of trigger detection, argument role detection and entity detection. These tasks share the same BERT encoder, which is fine-tuned during training. The entity detector predicts the argument candidates for all events in an input"
2021.naacl-demos.7,2020.acl-demos.11,0,0.0241415,"l models in a pipeline fashion (Peng et al., 2015; Noji and Miyao, 2016). The majority of these systems focus on token-level tasks like tokenization, lemmatization, part-of-speech tagging, or sentence-level tasks like syntactic parsing, semantic role labeling etc. There are only a few systems that can provide capabilities of event extraction and temporal information detection (Tao et al., 2013; Ning, 2019). For event extraction, some systems only provide results within a certain defined ontology such as AIDA (Li et al., 2019), there are also some works utilizing data from multiple modalities (Li et al., 2020a,b). Some works could handle novel events (Xiang and Wang, 2019; Ahmad et al., 2021; Han et al., 2020b; Huang and Peng, 2020), but they are either restricted to a certain domain (Yang et al., 2018) or lack of performance superiority because of their lexico-syntactic rule-based algorithm (Valenzuela-Esc´arcega et al., 2015). For temporal information detection, Ning et al. (2019) proposes a neural-based temporal relation extraction system with knowledge injection. Most related to our work, Ning et al. (2018b) demonstrates a temporal understanding system to extract time expression and implicit t"
2021.naacl-demos.7,D18-2013,0,0.299895,"dels are combined for visualization. event relation signals can be helpful to distinguish event triggers and non-event tokens. The model feeds BERT embedding (Devlin et al., 2019) of the input text to a shared BiLSTM layer for encoding task-specific contextual information. The output of the BiLSTM is passed to an event scoring function and a relation scoring function which are MLP classifiers to calculate the probability of being an event (for event extraction) or a probability distribution over all possible relations (for temporal relation extraction). We train the multitask model on MATRES (Ning et al., 2018a) containing temporal relations BEFORE, AFTER, SIMUL TANEOUS and VAGUE. Though the model performs both tasks during training, it can be separately used for each individual task during inference. 2.2 that are associated with sub-types. Similar to Han et al. (2019b), we build our event extraction component for ACE ontology upon a multi-task learning framework that consists of trigger detection, argument role detection and entity detection. These tasks share the same BERT encoder, which is fine-tuned during training. The entity detector predicts the argument candidates for all events in an input"
2021.naacl-demos.7,2020.acl-main.230,0,0.0299989,"Missing"
2021.naacl-demos.7,P16-4018,0,0.0186888,"ing narratives and facilitating downstream tasks. In the future, we plan to further improve EventPlus by tightly integrating event duration prediction and temporal relation extraction modules. We also plan to improve the performance for triggers and arguments detection under the ACE ontology and develop joint training models to optimize all eventrelated features in an end-to-end fashion. Related Works Existing NLP toolkits (Manning et al., 2014; Khashabi et al., 2018) provide an interface for a set of useful models. Some tools integrate several models in a pipeline fashion (Peng et al., 2015; Noji and Miyao, 2016). The majority of these systems focus on token-level tasks like tokenization, lemmatization, part-of-speech tagging, or sentence-level tasks like syntactic parsing, semantic role labeling etc. There are only a few systems that can provide capabilities of event extraction and temporal information detection (Tao et al., 2013; Ning, 2019). For event extraction, some systems only provide results within a certain defined ontology such as AIDA (Li et al., 2019), there are also some works utilizing data from multiple modalities (Li et al., 2020a,b). Some works could handle novel events (Xiang and Wan"
2021.naacl-demos.7,P06-1050,0,0.0547607,"uring decoding. 2.3 Event Duration Detection This component classifies event triggers into duration categories. While many datasets have covered time expressions which are explicit timestamps for events (Pustejovsky et al., 2003b; Cassidy et al., 2014; Reimers et al., 2018; Bethard et al., 2017), they do not target categorical event duration. To supplement this, Vashishtha et al. (2019) introduces the UDS-T dataset, where they provide 11 duration categories which we adopt for our event pipeline: INSTANT , SECONDS , MINUTES , HOURS , DAYS , WEEKS, MONTHS, YEARS, DECADES, CENTURIES and FOREVER. Pan et al. (2006) also present a news domain duration annotation dataset containing 58 articles developed from TimeBank corpus (we refer as Typical-Duration in the following), it provides 7 duration categories (a subset of the 11 categories in UDS-T from SECONDS to YEARS). We developed two models for the event duration detection task. For a sentence, along with predicate root and span, the models perform duration classification. In the first method, we fine-tune a BERT language model (Devlin et al., 2019) on single sentences and take hidden states of event tokens from the output of the last layer, then feed in"
2021.naacl-demos.7,N15-3018,1,0.850072,"Missing"
2021.naacl-demos.7,2020.acl-main.713,0,0.181485,"ent to demonstrate the semantic-rich information of events. ACE 20052 corpus defines an event ontology that represents an event as a structure with triggers and corresponding event arguments (participants) with specific roles (Doddington et al., 2004).3 Our system is trained with ACE 2005 corpus, thus it is capable of extracting events with the complex structure. ACE focuses on events of a particular set of types including LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT , CONTACT , PERSONNEL and JUSTICE , where each type has corresponding sub-types. Following prior works (Wadden et al., 2019; Lin et al., 2020), we keep 7 entity types (person, organization, location, geo-political entity, facility, vehicle, weapon), 33 event sub-types, and 22 argument roles • Entity-Argument constraint. The argument role label for a token can take one of the 22 argument roles if and only if the token at this position belongs to a predicted entity. • Entity-Trigger constraint. The trigger label for a token can take one of the 33 event sub-types if and only if the token at this position does not belong to a predicted entity. 2 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 3 The ACE program provides annot"
2021.naacl-demos.7,P14-5010,0,0.0304551,"ore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications. 1 Figure 1: Event understanding components. We highlight events triggers in yellow, and mark the predicted task-related information in Italic. Despite the importance, there are relatively few tools available for users to conduct text-based temporal event understanding. Researchers have been building natural language processing (NLP) analysis tools for “core NLP” tasks (Gardner et al., 2018; Manning et al., 2014; Khashabi et al., 2018). However, systems that target at semantic understanding of events and their temporal information are still under-explored. There are individual works for event extraction, temporal relation detection and event duration detection, but they are separately developed and thus cannot provide comprehensive and coherent temporal event knowledge. We present EventPlus, the first pipeline system integrating several high-performance temporal event information extraction models for comprehensive temporal event understanding. Specifically, EventPlus contains event extraction (both"
2021.naacl-demos.7,W18-1505,1,0.835091,"itive for humans and important for daily decision making. For example, given the raw text shown in Figure 1, a person can infer lots of information including event trigger and type, event related arguments (e.g., agent, patient, location), event duration and temporal relations between events based on the linguistic and common sense knowledge. These understandings help people comprehend the situation and prepare for future events. The event and temporal knowledge are helpful for many downstream applications including question answering (Meng et al., 2017; Huang et al., 2019), story generation (Peng et al., 2018; Yao et al., 2019; Goldfarb-Tarrant et al., 2019, 2020), and forecasting (Wang et al., 2017; Granroth-Wilding and Clark, 2016; Li et al., 2018). ∗ 1 The system is publicly accessible at https: //kairos-event.isi.edu. The source code is available at https://github.com/PlusLabNLP/ EventPlus. We also provide an introductory video at https://pluslabnlp.github.io/eventplus. Equal contribution. 56 Proceedings of NAACL-HLT 2021: Demonstrations, pages 56–65 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: The interface of EventPlus. Users can either choose examples or freely"
2021.naacl-demos.7,P18-1049,0,0.0423008,"Missing"
2021.naacl-demos.7,N18-1202,0,0.039134,"SECONDS to YEARS). We developed two models for the event duration detection task. For a sentence, along with predicate root and span, the models perform duration classification. In the first method, we fine-tune a BERT language model (Devlin et al., 2019) on single sentences and take hidden states of event tokens from the output of the last layer, then feed into a multi-layer perceptron for classification. The second model is adapted from the UDS-T baseline model, which is trained under the multitask objectives of duration and temporal relation extraction. The model computes ELMo embeddings (Peters et al., 2018) followed by attention layers to compute the attended representation of the predicate given sentence. The final MLP layers extract the duration category. Even though this model can detect temporal relations, it underperforms the model we described in § 2.1, so we exclude the temporal relation during inference. 2.4 3 System We design a pipeline system to enable the interaction among components with state-of-the-art performance introduced in § 2 and provide a comprehensive output for events and visualize the results. Figure 3 shows the overall system design. 3.1 Pipeline Design Event Extraction"
2021.naacl-demos.7,D17-1092,0,0.023171,"own in Figure 2.1 Introduction Event understanding is intuitive for humans and important for daily decision making. For example, given the raw text shown in Figure 1, a person can infer lots of information including event trigger and type, event related arguments (e.g., agent, patient, location), event duration and temporal relations between events based on the linguistic and common sense knowledge. These understandings help people comprehend the situation and prepare for future events. The event and temporal knowledge are helpful for many downstream applications including question answering (Meng et al., 2017; Huang et al., 2019), story generation (Peng et al., 2018; Yao et al., 2019; Goldfarb-Tarrant et al., 2019, 2020), and forecasting (Wang et al., 2017; Granroth-Wilding and Clark, 2016; Li et al., 2018). ∗ 1 The system is publicly accessible at https: //kairos-event.isi.edu. The source code is available at https://github.com/PlusLabNLP/ EventPlus. We also provide an introductory video at https://pluslabnlp.github.io/eventplus. Equal contribution. 56 Proceedings of NAACL-HLT 2021: Demonstrations, pages 56–65 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: The interfac"
2021.naacl-demos.7,P18-4009,0,0.0241109,"tence-level tasks like syntactic parsing, semantic role labeling etc. There are only a few systems that can provide capabilities of event extraction and temporal information detection (Tao et al., 2013; Ning, 2019). For event extraction, some systems only provide results within a certain defined ontology such as AIDA (Li et al., 2019), there are also some works utilizing data from multiple modalities (Li et al., 2020a,b). Some works could handle novel events (Xiang and Wang, 2019; Ahmad et al., 2021; Han et al., 2020b; Huang and Peng, 2020), but they are either restricted to a certain domain (Yang et al., 2018) or lack of performance superiority because of their lexico-syntactic rule-based algorithm (Valenzuela-Esc´arcega et al., 2015). For temporal information detection, Ning et al. (2019) proposes a neural-based temporal relation extraction system with knowledge injection. Most related to our work, Ning et al. (2018b) demonstrates a temporal understanding system to extract time expression and implicit temporal relations among detected events, but this system cannot provide event-related arguments, entities and event duration information. These previous works either are not capable of event underst"
2021.naacl-demos.7,Q18-1006,0,0.0188598,"detection model yields a 0.92 F1 score, and the scope resolution model yields a 0.88 F1 score for token-level prediction, given ground truth cues as input. In EventPlus, we input cues detected by the cue detection model to the scope resolution model. To account for these constraints, we set the probability of all invalid configurations to be 0 during decoding. 2.3 Event Duration Detection This component classifies event triggers into duration categories. While many datasets have covered time expressions which are explicit timestamps for events (Pustejovsky et al., 2003b; Cassidy et al., 2014; Reimers et al., 2018; Bethard et al., 2017), they do not target categorical event duration. To supplement this, Vashishtha et al. (2019) introduces the UDS-T dataset, where they provide 11 duration categories which we adopt for our event pipeline: INSTANT , SECONDS , MINUTES , HOURS , DAYS , WEEKS, MONTHS, YEARS, DECADES, CENTURIES and FOREVER. Pan et al. (2006) also present a news domain duration annotation dataset containing 58 articles developed from TimeBank corpus (we refer as Typical-Duration in the following), it provides 7 duration categories (a subset of the 11 categories in UDS-T from SECONDS to YEARS)."
2021.naacl-demos.7,taboada-etal-2006-methods,0,0.0257792,"roles in this sequence can only take those that are valid for this trigger. (e.g., would) or negation (e.g., not) keywords (Konstantinova et al., 2012). Since those events do not happen, we mark them with special labels. For example, in the sentence “The United States is not considering sending troops to Mozambique”, we identify “send” will not happen. We adapt the BERT-based negation and speculation cue detection model and the scope resolution model introduced by Khandelwal and Sawant (2020). To fine-tune these models, we use the SFU Review dataset with negation and speculation annotations (Taboada et al., 2006; Taboada and Grieve, 2004; Konstantinova et al., 2012), and we feed ground truth negation and speculation cues as input for the scope resolution model. We evaluate the two models on a separate testing set of the SFU Review dataset. The cue detection model yields a 0.92 F1 score, and the scope resolution model yields a 0.88 F1 score for token-level prediction, given ground truth cues as input. In EventPlus, we input cues detected by the cue detection model to the scope resolution model. To account for these constraints, we set the probability of all invalid configurations to be 0 during decodi"
2021.naacl-demos.7,P15-4022,0,0.0692487,"Missing"
2021.naacl-demos.7,P19-1280,0,0.0461283,"Missing"
2021.naacl-demos.7,D19-1585,0,0.119288,"they are not sufficient to demonstrate the semantic-rich information of events. ACE 20052 corpus defines an event ontology that represents an event as a structure with triggers and corresponding event arguments (participants) with specific roles (Doddington et al., 2004).3 Our system is trained with ACE 2005 corpus, thus it is capable of extracting events with the complex structure. ACE focuses on events of a particular set of types including LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT , CONTACT , PERSONNEL and JUSTICE , where each type has corresponding sub-types. Following prior works (Wadden et al., 2019; Lin et al., 2020), we keep 7 entity types (person, organization, location, geo-political entity, facility, vehicle, weapon), 33 event sub-types, and 22 argument roles • Entity-Argument constraint. The argument role label for a token can take one of the 22 argument roles if and only if the token at this position belongs to a predicted entity. • Entity-Trigger constraint. The trigger label for a token can take one of the 33 event sub-types if and only if the token at this position does not belong to a predicted entity. 2 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 3 The ACE pro"
2021.naacl-demos.7,D17-1006,0,0.0276198,"own in Figure 1, a person can infer lots of information including event trigger and type, event related arguments (e.g., agent, patient, location), event duration and temporal relations between events based on the linguistic and common sense knowledge. These understandings help people comprehend the situation and prepare for future events. The event and temporal knowledge are helpful for many downstream applications including question answering (Meng et al., 2017; Huang et al., 2019), story generation (Peng et al., 2018; Yao et al., 2019; Goldfarb-Tarrant et al., 2019, 2020), and forecasting (Wang et al., 2017; Granroth-Wilding and Clark, 2016; Li et al., 2018). ∗ 1 The system is publicly accessible at https: //kairos-event.isi.edu. The source code is available at https://github.com/PlusLabNLP/ EventPlus. We also provide an introductory video at https://pluslabnlp.github.io/eventplus. Equal contribution. 56 Proceedings of NAACL-HLT 2021: Demonstrations, pages 56–65 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 2: The interface of EventPlus. Users can either choose examples or freely input text which matches with their selected topic in B. C shows the Name Entity Recognitio"
2021.naacl-main.336,N18-2014,0,0.0544286,"Missing"
2021.naacl-main.336,P19-1470,0,0.471899,"arising from perception are used in conceptual tasks such as representing propositions and abstract concepts. Philosopher Susanne Langer in her essay “Expressiveness and Symbolism” stated “A metaphor is not language, it is an idea expressed by language, an idea that in its turn functions as a symbol to express something”. Our approach has two steps: 1) identify a set of sentences that contains metaphorical verbs from an online poetry corpus; 2) convert these metaphorical sentences to their literal versions using Masked Language Models and structured common sense knowledge achieved from COMET (Bosselut et al., 2019), a language model fine-tuned on ConceptNet (Speer et al., 2017). For the later, we exploit the SymbolOf relation to make sure the generated sentence that contains the literal sense of the verb has the same symbol as the metaphorical sentence. For example, for the metaphorical sentence “The turbulent feelings that surged through his soul"" our method will generate “The turbulent feelings that continued through his soul"" maintaining the common symbolic meaning of (love, loss, despair, sorrow, loneliness) between the two (Section 2). • A task-based evaluation to improve the quality of human writt"
2021.naacl-main.336,2020.emnlp-main.524,1,0.542084,"h attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of different domains and generated conceptual metaphors of pattern “A is B” (Hervás et al., 2007; Mason, 2004; Gero and Chilton, 2019). These works paid attention to the relationship between nouns and concepts to create elementary figurative expressions. Recent metaphor generation works focus mainly on verbs. Yu and Wan (2019) proposed an unsupervised metaphor extraction method, and developed a neural generation model to generate metaphorical sente"
2021.naacl-main.336,P18-1082,0,0.0677038,"Missing"
2021.naacl-main.336,D18-1060,0,0.0119118,"hoose the verb with highest literal probability. Our goal is to see if re-written poems are qualitatively better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create paral"
2021.naacl-main.336,2020.figlang-1.21,0,0.0242976,"s that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of d"
2021.naacl-main.336,P18-1152,0,0.0155536,"goal, we fine-tune BART (Lewis et al., 2019), a pre-trained conditional language model that combines bidirectional and autoregressive transformers, on the collected parallel corpora. Specifically, we fine-tune BART by treating the literal input as encoder source and the metaphorical output as the the decoder target (Figure 1). One issue of the pre-trained language models is that they have a tendency to generate literal tokens over metaphorical ones. To overcome this, we introduce a rescoring model during the decoding process to favor more metaphorical verbs. The rescoring model is inspired by Holtzman et al. (2018); Goldfarb-Tarrant et al. (2020) and detailed in the next section. 3.2 Discriminative Decoding We have a base metaphor generation model p(z|x) which is learned by fine-tuning BART (Lewis et al., 2019) on pairs of literal (x) and metaphorical (z) sentences. We propose to modify the decoding objective to incorporate a Metaphor detection rescoring model a and re-rank the base, or “naive"" BART generated hypotheses, bringing the metaphoric representation closer to the rescoring model’s specialty 4253 DECODER TARGET ENCODER TARGET SOURCE BART The tax cut will help the economy Black desert covered in"
2021.naacl-main.336,W13-0907,0,0.072719,"Missing"
2021.naacl-main.336,W14-2302,0,0.0149977,"on of metaphor, while metaphor genera- mask the metaphorical verbs as input, and the origtion is relatively under-studied. inal metaphorical sentences as output. However, this model face challenges in transferring the literal 7.1 Metaphor Detection sentences to metaphorical ones, while maintainFor metaphor detection, researchers focused on ing the same meaning. We, on the contrary, focus variety of features, including unigrams, imageabil- on maintaining the same meaning through parallel ity, sensory features, WordNet, bag-of-words fea- data creation focusing on symbolism. Additionally, tures (Klebanov et al., 2014; Tsvetkov et al., 2014; we incorporate a metaphor detection model as a Shutova et al., 2016; Tekiro˘glu et al., 2015; Hovy discriminator to improve decoding during generaet al., 2013; Köper and im Walde, 2016). tion. 4257 8 Conclusion We show how to transform literal sentences to metaphorical ones. We propose a novel way of creating parallel corpora and an approach for generating metaphors that benefits from transfer learning and discriminative decoding. Human and automatic evaluations show that our best model is successful at generating metaphors. We further show that leveraging symbolic mea"
2021.naacl-main.336,N16-1039,0,0.058103,"Missing"
2021.naacl-main.336,2020.acl-main.703,0,0.0795396,"Missing"
2021.naacl-main.336,2021.ccl-1.108,0,0.0195425,"Missing"
2021.naacl-main.336,P19-1378,0,0.0139253,"tter version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is lik"
2021.naacl-main.336,J04-1002,0,0.169501,"rbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of different domains and generated conceptual metaphors of pattern “A is B” (Hervás et al., 2007; Mason, 2004; Gero and Chilton, 2019). These works paid attention to the relationship between nouns and concepts to create elementary figurative expressions. Recent metaphor generation works focus mainly on verbs. Yu and Wan (2019) proposed an unsupervised metaphor extraction method, and developed a neural generation model to generate metaphorical sentences from literal-metaphorical verb pairs. They however do not focus on literal to metaphorical sentence transfer , but generate a sentence given 7 Related Work a metaphorical fit word. The closest to our work is that of Stowe et al. (2020), who focus on bu"
2021.naacl-main.336,S16-2003,0,0.0229048,"effect of transfer learning from a large generative pre-trained model, which also accounts for context unlike the retrieval based methods. 4.2 Test Data To measure the effectiveness of our approach, we need to evaluate our model on a dataset that is independent of our automatically created parallel data and that is diverse across various domains, genres and types. Hence we rely on test data from multiple sources. As our first source, we randomly sample literal and metaphorical sentences with high confidence (> 0.7) and unique verbs from the existing 4 Experimental Setup dataset introduced by Mohammad et al. (2016). For To compare the quality of the generated metaphors, the metaphorical sentences from Mohammad et al. we benchmark our MERMAID model against human (2016) we convert them to their literal equivalent 4254 the same way as discussed in Section 2.2 without the use of COMET as we do not need it. To ensure diversity in genre, as our second source we scrape W RITING P ROMPT and O CPOETRY subreddits for sentences with length up to 12 words, which are literal in nature based on prediction from our model described in Section 2.1. We collate 500 such sentences combined from all sources and randomly sam"
2021.naacl-main.336,D17-1238,0,0.0119809,"here 1 denotes the worst and 5 be the best. Boldface denotes the best results overall and underscore denotes the best among computational models. evaluate on four dimensions for 900 utterances, we have a total of 3600 evaluations. Each criteria was rated on a likert scale from 1 (not at all) to 5 (very). Each group of utterances was rated by three separate Turkers, resulted in 42, 48, 44 and 53 Turkers for the four evaluation tasks respectively. We pay them at a rate of $15 per hour. Human evaluation. Since automatic evaluation is known to have significant limitations for creative generation (Novikova et al., 2017), we further conduct human evaluation on a total of 900 utterances, 600 generated from 4 systems and 300 generated by the two human experts. We propose a 5 Results set of four criteria to evaluate the generated output: Based on the semantic similarity metric shown in (1) Fluency (Flu) (“How fluent, grammatical, well formed and easy to understand are the generated ut- column 1 of Table 4, our system MERMAID is better in preserving the meaning of the input than terances?”), (2) Meaning (Mea) (“Are the input and the output referring or meaning the same thing?"") the other baselines. As mentioned,"
2021.naacl-main.336,N19-4009,0,0.0221409,"Missing"
2021.naacl-main.336,P02-1040,0,0.109425,"ture who is the author of a novel — to write corresponding metaphors for each of these 150 inputs for evaluation and comparison. 4.3 Evaluation Criteria Automatic evaluation. One important aspect in evaluating the quality of the generated metaphors is whether they are faithful to the input: while we change literal sentences to metaphorical ones, it should still maintain the same denotation as the input. To this end, we calculate the Semantic Similarity between the metaphorical output and the input using sentence-BERT (SBERT) (Reimers and Gurevych, 2019). We also calculate corpus-level BLEU-2 (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) with human written references. System LEXREP META_M BART MERMAID HUMAN1 HUMAN2 Similarity ↑ 79.6 73.2 83.6 85.0 86.6 84.2 BLEU-2↑ 68.7 61.0 65.0 66.7 - BertScore↑ 0.56 0.62 0.65 0.71 - Table 4: Automatic evaluation results on test set where MERMAID significantly outperforms other automatic methods for 2 out of 3 metrics (p &lt; .001) according to approximate randomization test). BLEU-2 and BertScore is calculated w.r.t to Human references (HUMAN1 & HUMAN2). Corpus level BLEU-2 and Semantic Similarity are in range of (0-100) while BertScore is in range (0-1) Sys"
2021.naacl-main.336,D14-1162,0,0.0893441,"goal is to see if re-written poems are qualitatively better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hyp"
2021.naacl-main.336,N18-1202,0,0.00759152,"vely better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Genera"
2021.naacl-main.336,D19-1410,0,0.0138251,"uter science who is also a poet, and a student in comparative literature who is the author of a novel — to write corresponding metaphors for each of these 150 inputs for evaluation and comparison. 4.3 Evaluation Criteria Automatic evaluation. One important aspect in evaluating the quality of the generated metaphors is whether they are faithful to the input: while we change literal sentences to metaphorical ones, it should still maintain the same denotation as the input. To this end, we calculate the Semantic Similarity between the metaphorical output and the input using sentence-BERT (SBERT) (Reimers and Gurevych, 2019). We also calculate corpus-level BLEU-2 (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) with human written references. System LEXREP META_M BART MERMAID HUMAN1 HUMAN2 Similarity ↑ 79.6 73.2 83.6 85.0 86.6 84.2 BLEU-2↑ 68.7 61.0 65.0 66.7 - BertScore↑ 0.56 0.62 0.65 0.71 - Table 4: Automatic evaluation results on test set where MERMAID significantly outperforms other automatic methods for 2 out of 3 metrics (p &lt; .001) according to approximate randomization test). BLEU-2 and BertScore is calculated w.r.t to Human references (HUMAN1 & HUMAN2). Corpus level BLEU-2 and Semantic Similarit"
2021.naacl-main.336,D19-1339,1,0.714841,"mapping. 9 Ethics Our data is collected from Reddit and we understand and respect user privacy. Our models are fine-tuned on sentence level data obtained from user posts. These do not contain any explicit detail which leaks information about a users name, health, negative financial status, racial or ethnic origin, religious or philosophical affiliation or beliefs, sexual orientation, trade union membership, alleged or actual commission of crime. Second, although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Furthermore, we specifically encode writing style from a poetic corpus in our models and train on parallel data in the direction of literal to metaphorical style. Open-sourcing this technology will help to generate metaphoric text assisting creative writing practitioners or non native language speakers to improve their writing. We do not envision any dual-use that can ca"
2021.naacl-main.336,N16-1020,0,0.0185437,"is relatively under-studied. inal metaphorical sentences as output. However, this model face challenges in transferring the literal 7.1 Metaphor Detection sentences to metaphorical ones, while maintainFor metaphor detection, researchers focused on ing the same meaning. We, on the contrary, focus variety of features, including unigrams, imageabil- on maintaining the same meaning through parallel ity, sensory features, WordNet, bag-of-words fea- data creation focusing on symbolism. Additionally, tures (Klebanov et al., 2014; Tsvetkov et al., 2014; we incorporate a metaphor detection model as a Shutova et al., 2016; Tekiro˘glu et al., 2015; Hovy discriminator to improve decoding during generaet al., 2013; Köper and im Walde, 2016). tion. 4257 8 Conclusion We show how to transform literal sentences to metaphorical ones. We propose a novel way of creating parallel corpora and an approach for generating metaphors that benefits from transfer learning and discriminative decoding. Human and automatic evaluations show that our best model is successful at generating metaphors. We further show that leveraging symbolic meanings helps us learn better abstract representations and better preservation of the denotati"
2021.naacl-main.336,C10-1113,0,0.119001,"Missing"
2021.naacl-main.336,K19-1034,0,0.0223201,"Missing"
2021.naacl-main.336,N19-1092,0,0.16384,"rs allow us to communicate not just information, but also feelings and complex attitudes (Veale et al., 2016). While most computational work has focused on metaphor detection (Gao et al., 2018; Stowe et al., 2019; Shutova et al., 2010; Tsvetkov et al., 2014; Veale et al., 2016; Stowe and Palmer, 2018), research on metaphor generation is ∗ The wildfire spread through the forest at an amazing speed. The wildfire danced through the forest at an amazing speed. The window panes were rattling as the wind blew through them The window panes were trembling as the wind blew through them under-explored (Yu and Wan, 2019; Stowe et al., 2020). Generating metaphors could impact many downstream applications such as creative writing assistance, literary or poetic content creation. Relevant statistics demonstrate that the most frequent type of metaphor is expressed by verbs (Steen, 2010; Martin, 2006). We therefore focus on the task of generating a metaphor starting from a literal utterance (Stowe et al., 2020), where we transform a literal verb to a metaphorical verb. Table 1 shows examples of literal sentences and the generated metaphors. To tackle the metaphor generation problem we need to address three challen"
2021.naacl-main.336,D19-1221,0,0.042146,"ur data is collected from Reddit and we understand and respect user privacy. Our models are fine-tuned on sentence level data obtained from user posts. These do not contain any explicit detail which leaks information about a users name, health, negative financial status, racial or ethnic origin, religious or philosophical affiliation or beliefs, sexual orientation, trade union membership, alleged or actual commission of crime. Second, although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Furthermore, we specifically encode writing style from a poetic corpus in our models and train on parallel data in the direction of literal to metaphorical style. Open-sourcing this technology will help to generate metaphoric text assisting creative writing practitioners or non native language speakers to improve their writing. We do not envision any dual-use that can cause harm for the use of"
2021.naacl-main.343,P19-1470,0,0.0228258,"shows a distinct manipulation technique. The manipulated plots are passed through a generation model to generate implausible samples for the evaluation task. Non-logically Ordered Plots. Logical conflict is one of the sources for implausibility that results from not-logically ordered concepts in the text. While Guan and Huang (2020) covered this type of implausibility by changing the order of sentences, we hypothesize that disrupting the logical order at the concept-level is more efficient. To accomplish concept reordering, we first randomly choose verbs from the plot and leverage the COMET (Bosselut et al., 2019) model to predict their subsequent events. Then we dislocate the resulted concept pairs. COMET, which is trained on tuples of the form ( subject, relation, object), can be used to predict an object given a pair of subject and relation. As an example, given the pair (work, causes) COMET will predict get pay to show that work causes to get paid. We focus on COMET relations HasPrerequisite, HasFirstSubevent, Causes and HasLastSubevent that imply ordering. In the first two relations, object should appear before subject, while in the other two the order is reversed. Therefore, subject work comes be"
2021.naacl-main.343,2020.aacl-main.59,0,0.0179119,"examples and change their structure to generate negative examples. Sentence Substitution. Sentence substitution 3.2.1 Plot Manipulations (briefly H EUR _S ENT _S UB ) replaces a fraction of Studies have shown that high-quality fluent stories sentences in the plausible text with random ones can be generated by planning in advance and lever(See Figure 1). This breaks the discourse-level coaging lucrative plots (Yao et al., 2019; Fan et al., herence, making a story not interpretable (Li and 2019; Goldfarb-Tarrant et al., 2019, 2020; Rashkin Jurafsky, 2016; Holtzman et al., 2018). et al., 2020b; Brahman et al., 2020). Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive effects on generating high-quality sto("
2021.naacl-main.343,N19-1423,0,0.0574184,"these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then passes it through a hidden layer. Recently, Guan and Huang (2020) proposed a more accurate automatic evaluation metric called UNION. This metric achieved better performance by using BERT (Devlin et al., 2019) as a more effective classification model and have a broader set of negative samples coming from different heuristics. For all learning-based metrics, the simplicity of heuristically generated data samples makes them inadequate for an accurate evaluation of plausibility in open-domain generated texts. 3 Implausible Text Construction We formulate the evaluation of open-domain story generation as a binary classification task where the goal is to distinguish plausible and implausi• We show the affirmative role of adversarial filter- ble generated stories, also referred to as positive ing techniqu"
2021.naacl-main.343,P18-1082,0,0.375915,"tion of generation models can be classified into two subgroups, non-learning-based and learning-based methods, which we briefly summarize below. Non-learning-based Metrics. Some metrics in this group consider the centrality of a text around a specific topic as a proxy for measuring its quality. The transitions of entities in neighbor sentences and their distribution across text have been served as a measurement for quality assessment (Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whol"
2021.naacl-main.343,P19-1254,0,0.0222618,"ring and negation (See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are high-level structured representations of generated texts originally used as a contentplanning tool for better text generation (Fan et al., 2019; Goldfarb-Tarrant et al., 2020). Specifically, we propose to manipulate plots by injecting incoherence sources into them. The generation models conditioned on such manipulated plots lead to implausible texts that have pertinent similarities with implausible machine-generated texts and thus can serve as good negative examples for training evaluation metrics. We further improve the quality of training data by incorporating the adversarial filtering technique proposed by Zellers et al. (2018) to select more challenging negative samples generated from the manipulated plots (See Figure 1). Eventua"
2021.naacl-main.343,2020.emnlp-main.351,1,0.762193,"(See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are high-level structured representations of generated texts originally used as a contentplanning tool for better text generation (Fan et al., 2019; Goldfarb-Tarrant et al., 2020). Specifically, we propose to manipulate plots by injecting incoherence sources into them. The generation models conditioned on such manipulated plots lead to implausible texts that have pertinent similarities with implausible machine-generated texts and thus can serve as good negative examples for training evaluation metrics. We further improve the quality of training data by incorporating the adversarial filtering technique proposed by Zellers et al. (2018) to select more challenging negative samples generated from the manipulated plots (See Figure 1). Eventually, these samples result in mor"
2021.naacl-main.343,2020.emnlp-main.736,0,0.0750275,"assifiers is a key determinant of the domain natural language generation (NLG), such metric effectiveness. Existing works take humanas dialog systems (Zhang et al., 2020) and story written texts as plausible (positive) examples, while 4334 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4334–4344 June 6–11, 2021. ©2021 Association for Computational Linguistics the negative samples are heuristically generated by randomly substituting keywords or sentences (See Figure 1) (Li and Jurafsky, 2016; Guan and Huang, 2020). Guan and Huang (2020) further improved the quality of evaluators by applying heuristic rules such as adding repetition, reordering and negation (See the UNION story in Figure 1). In this work, we hypothesize that heuristically generated data cannot adequately reflect the characteristics of the implausible texts generated by language models, thus result in suboptimal trained evaluation metrics. This deficiency can be mitigated by generating high-quality implausible examples that are closer to the test data. Toward this goal, we propose an approach based on the manipulation of plots, which are"
2021.naacl-main.343,P02-1040,0,0.116077,"ck) using sentence, keyword and UNION manipulations versus injecting implausible sources into the story plot (the third block, from the left plot to the right one) and generating a more natural implausible story (the last story). Blue highlights show the implausible sections. generators (Rashkin et al., 2020a) necessitates automatic evaluation metrics for quality assessment. The existence of accurate automatic evaluation metrics can accelerate the development cycle by facilitating the process of model comparison and hyperparameter search. Many existing reference-based approaches such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) fail to correlate well with human judgment in open-domain settings due to the fact that there can be potentially many plausible generations that do not have significant overlap with the limited set of given references. This failure invites research on more sophisticated and reliable evaluation metrics. Recently, learning-based approaches have been proposed to overcome this limitation by training classifiers to distinguish between plausible and implausible texts (Li and Jurafsky, 2016; Holtzman 1 Introduction et al., 2018). The choice of training data for learnThe surge of"
2021.naacl-main.343,P18-1152,0,0.0186227,"es to heuristically manipulate positive examples and change their structure to generate negative examples. Sentence Substitution. Sentence substitution 3.2.1 Plot Manipulations (briefly H EUR _S ENT _S UB ) replaces a fraction of Studies have shown that high-quality fluent stories sentences in the plausible text with random ones can be generated by planning in advance and lever(See Figure 1). This breaks the discourse-level coaging lucrative plots (Yao et al., 2019; Fan et al., herence, making a story not interpretable (Li and 2019; Goldfarb-Tarrant et al., 2019, 2020; Rashkin Jurafsky, 2016; Holtzman et al., 2018). et al., 2020b; Brahman et al., 2020). Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive e"
2021.naacl-main.343,W18-5023,0,0.350261,"nd Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then passes it through a hidden layer. Recently, Guan and Huang (2020) proposed a more accurate automatic evaluation metric called UNION. This metric achieved better performance by using BERT (Devlin et al., 2019) as a more effective classification model and have a broader set of negative samples coming from different heuristics. For all learning-based metrics, the simplicity of heuristically generated data samples makes them inadequate for an accurate evaluation of p"
2021.naacl-main.343,W18-1505,1,0.837531,"models can be classified into two subgroups, non-learning-based and learning-based methods, which we briefly summarize below. Non-learning-based Metrics. Some metrics in this group consider the centrality of a text around a specific topic as a proxy for measuring its quality. The transitions of entities in neighbor sentences and their distribution across text have been served as a measurement for quality assessment (Miltsakaki and Kukich, 2004; Lapata and Barzilay, 2005). Perplexity is another commonly used metric to evaluate the quality of text and story generation models (Fan et al., 2018; Peng et al., 2018). Learning-based Metrics. This group of metrics is based on neural-based classifiers trained on a set of positive (plausible) and negative (implausible) texts. The common point between these metrics is using random sentence substitution to construct training examples, while the architectures are slightly different. Li and Jurafsky (2016) trained a neural network with a sigmoid function on top of sentence embeddings extracted from LSTM. Lai and Tetreault (2018) designed SENTAVG that gets the sentence vectors from LSTM, takes the average of these vectors to represent the whole text, and then pas"
2021.naacl-main.343,2020.emnlp-main.349,0,0.0170996,"the woman at the store. she brought her worms and a chair and decided to play with them. jenny sat down and laid down on the chair. when she got wet, she packed up and went home disappointed. Figure 1: Heuristically generated implausible stories (the second block) for a given human-written story (the first block) using sentence, keyword and UNION manipulations versus injecting implausible sources into the story plot (the third block, from the left plot to the right one) and generating a more natural implausible story (the last story). Blue highlights show the implausible sections. generators (Rashkin et al., 2020a) necessitates automatic evaluation metrics for quality assessment. The existence of accurate automatic evaluation metrics can accelerate the development cycle by facilitating the process of model comparison and hyperparameter search. Many existing reference-based approaches such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) fail to correlate well with human judgment in open-domain settings due to the fact that there can be potentially many plausible generations that do not have significant overlap with the limited set of given references. This failure invites research on more sophisti"
2021.naacl-main.343,2020.acl-main.704,0,0.0426197,"Missing"
2021.naacl-main.343,speer-havasi-2012-representing,0,0.071041,". Yao et al. Keyword Substitution. Guan and Huang (2020) (2019) leverage a sequence of keywords as the plot proposed to apply random substitutions at the representation (also called storyline). Fan et al. keyword-level (briefly H EUR _K EY _S UB ), where (2019) use semantic role labeling tool to extract a fraction of keywords are randomly substituted plots as abstract presentation of stories over actions with their corresponding antonyms from a com- and entities. Their experiments affirm that plots monsense knowledge base such as ConceptNet have positive effects on generating high-quality sto(Speer and Havasi, 2012) to corrupt the plausibility ries. in the text. ConceptNet consists of (object, relaHere we leverage this idea for generating imtion, subject) triplets. For each selected keyword plausible texts, by controllable injection of implauthat exists as an object or subject in the Concept- sibility sources, or perturbations, into the groundNet, its counterpart is extracted from one of the truth plots. The resulting plot-level manipulations contradiction-type relations; Antonym, NotDesires, will force the model to reflect applied implausiNotCapableOf, or NotHasProperty. For instance, bility in the gene"
2021.naacl-main.343,2021.ccl-1.108,0,0.0262502,"Missing"
2021.naacl-main.343,N16-1098,0,0.172275,"rent in terms of tentions, which reduce the computation complexity length and topic; ROCStories (shortly ROC) and 4339 Dataset H EUR _S ENT _S UB H EUR _K EY _S UB UNION_DATA M AN P LTS AF_M AN P LTS Train/Valid/Test 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 47.1k/5.9k/5.9k 94.2k/11.8k/11.8k Table 2: Plausibility evaluation datasets for ROC stories using different negative sampling techniques. Writing Prompt (briefly WP) datasets including on average 49.4 and 734.5 tokens in each story. ROCStories. ROCStories is a resource of fivesentence commonsense stories collected via crowdsourcing (Mostafazadeh et al., 2016) covering a logically linked set of daily events. We follow the approach proposed by Yao et al. (2019) to extract story plots (storylines) for the stories and manipulate them to guide conditional language models to generate negative samples. Writing Prompt. Writing Prompt dataset contains abstract high-level prompts and their corresponding long human-written stories from an online forum (Fan et al., 2018). To apply the plot manipulation technique for implausible text construction, we follow the procedure proposed by Fan et al. (2019) to extract the plots with verb and argument type role labeli"
2021.naacl-main.343,2020.acl-demos.30,0,0.0138187,"p with the limited set of given references. This failure invites research on more sophisticated and reliable evaluation metrics. Recently, learning-based approaches have been proposed to overcome this limitation by training classifiers to distinguish between plausible and implausible texts (Li and Jurafsky, 2016; Holtzman 1 Introduction et al., 2018). The choice of training data for learnThe surge of downstream applications for open- ing such classifiers is a key determinant of the domain natural language generation (NLG), such metric effectiveness. Existing works take humanas dialog systems (Zhang et al., 2020) and story written texts as plausible (positive) examples, while 4334 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4334–4344 June 6–11, 2021. ©2021 Association for Computational Linguistics the negative samples are heuristically generated by randomly substituting keywords or sentences (See Figure 1) (Li and Jurafsky, 2016; Guan and Huang, 2020). Guan and Huang (2020) further improved the quality of evaluators by applying heuristic rules such as adding repetition, reordering and negation (Se"
2021.naacl-main.60,P19-1080,0,0.0128652,"in language generation, Curry and Rieser (2018) study how conversational systems respond to sexual harassment, and Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include lexical constraints. Miao et al. (2019); Zhang et al. (2020b); Susanto et al. (2020) explore insertion-based non-autoregressive decoding algorithms. To be compatible with an autoregressive model like DialoGPT and effective for open-domain generation, we apply constrained decoding to top-k sampling. Our method also differs from these prior works in that it imposes soft constraints to not genera"
2021.naacl-main.60,D19-1176,0,0.0264603,"In particular, because societal norms allow biases and stereotypes to detract from a person’s credibility or expertise, the use of ad hominems can further diminish the rhetorical credibility (Govier, 1993) of marginalized groups. Offensive Language Detection Ad hominems occur in many forms and are related to different types of offensive language, including abusive language (Yin et al., 2009; Chen et al., 2012; Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Djuric et al., 2015), profanity (Sood et al., 2012a), and the more subtle forms of microaggressions (Breitfeller et al., 2019) and projecting biases and stereotypes through power differentials in language (Sap et al., 2020). Ranging from outright insults to condescension, ad hominems are a form of offensive language that is difficult to comprehensively and objectively define. Nonetheless, these responses are important to characterize, since they can irreparably damage a person’s credibility. It is also generally important 2 Related Work to identify these subtle forms of offensive language, This work is related to a broad spectrum of topics, since it is unclear if existing offensive language deincluding prior definiti"
2021.naacl-main.60,W18-0802,0,0.0236369,"nce it is unclear if existing offensive language deincluding prior definitions of ad hominems and how tection techniques are equally effective for these ad hominems facilitate biases. Also, analyzing ad subtle forms. 751 Harms in Dialogue Systems Conversational systems are known to perpetuate several types of harms. Ruane et al. (2019) caution about harms that can result from using conversational systems and propose striving for trust and transparency; Roller et al. (2020) suggest techniques for chatbot safety. For analysis, Sheng et al. (2019) evaluate societal biases in language generation, Curry and Rieser (2018) study how conversational systems respond to sexual harassment, and Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees amon"
2021.naacl-main.60,P19-2028,0,0.0172099,"we discuss existing constrained decoding methods. Ad Hominems In the argumentation literature, theoretical ad hominems include the abusive (attack on the opponent’s character), tu quoque (“he did it first”), circumstantial (accusation of hypocrisy), and guilt by association (associating the opponent with someone with low credibility) (Walton, 1998; Woods, 2007). Wijze (2003) criticizes that these textbook examples are not realistic in conversation. For more empirical categories, Habernal et al. (2018) propose ad hominem types based on analysis of Reddit’s ChangeMyView discussion threads, and Delobelle et al. (2019) analyze the name-calling and abusive categories. Moreover, Wulczyn et al. (2017) use classifiers for a largescale analysis of personal attacks in Wikipedia comments. We build upon prior works to define and analyze ad hominems in a conversational setting. Additionally, Yap (2013) discusses the harmful effects of implicit biases in forming and evaluating ad hominems. They emphasize that ad hominem attacks can be harmful to a person’s credibility and expertise even if the attack is recognized as fallacious and irrelevant to the argument. In particular, because societal norms allow biases and ste"
2021.naacl-main.60,N18-1036,0,0.379476,"d-hom-in-dialogue. hominems in dialogue systems is related to examining offensive language and other harms. Lastly, we discuss existing constrained decoding methods. Ad Hominems In the argumentation literature, theoretical ad hominems include the abusive (attack on the opponent’s character), tu quoque (“he did it first”), circumstantial (accusation of hypocrisy), and guilt by association (associating the opponent with someone with low credibility) (Walton, 1998; Woods, 2007). Wijze (2003) criticizes that these textbook examples are not realistic in conversation. For more empirical categories, Habernal et al. (2018) propose ad hominem types based on analysis of Reddit’s ChangeMyView discussion threads, and Delobelle et al. (2019) analyze the name-calling and abusive categories. Moreover, Wulczyn et al. (2017) use classifiers for a largescale analysis of personal attacks in Wikipedia comments. We build upon prior works to define and analyze ad hominems in a conversational setting. Additionally, Yap (2013) discusses the harmful effects of implicit biases in forming and evaluating ad hominems. They emphasize that ad hominem attacks can be harmful to a person’s credibility and expertise even if the attack is"
2021.naacl-main.60,P17-1141,0,0.0119748,"nd Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include lexical constraints. Miao et al. (2019); Zhang et al. (2020b); Susanto et al. (2020) explore insertion-based non-autoregressive decoding algorithms. To be compatible with an autoregressive model like DialoGPT and effective for open-domain generation, we apply constrained decoding to top-k sampling. Our method also differs from these prior works in that it imposes soft constraints to not generate phrases that are likely to lead to ad hominems. Decoding-time techniques that can be used to reduce harm"
2021.naacl-main.60,2020.emnlp-main.88,1,0.796448,"t and not a third person. 4.1 Human Annotation We collect human annotations that can then be used for analysis and training a classifier to automatically label ad hominems. Although Habernal et al. (2018) propose a similar typology of ad hominems, there is no existing dataset annotated with their empirically-derived categories. Moreover, we study ad hominems in casual conversational settings. For these reasons, we annotate a subset of A D H OM I N T WEETS with ad hominem information. To measure inter-annotator agreement, we calculate the Worker Agreement With Aggregate (WAWA) score, following Ning et al. (2020). The WAWA score compares the majority votes against each annotator and micro-averages the resulting precision, recall, and F1 scores.5 Heuristics for Ad Hominems Ad hominem responses are relatively rare and range broadly from explicit to more subtle forms. For more effective annotation, we use heuristics to choose [post, response] pairs where the response is likely to be an ad hominem. In preliminary analyses, we find that responses that contain certain “you”-phrases such as “you are” are more likely to have ad hominems. We call these responses you-responses.6 In addition to pairs with you-re"
2021.naacl-main.60,N18-1119,0,0.0152248,"detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include lexical constraints. Miao et al. (2019); Zhang et al. (2020b); Susanto et al. (2020) explore insertion-based non-autoregressive decoding algorithms. To be compatible with an autoregressive model like DialoGPT and effective for open-domain generation, we apply constrained decoding to top-k sampling. Our method also differs from these prior works in that it imposes soft constraints to not generate phrases that are likely to lead to ad hominems. Decoding-time techniques that can be used to reduce harmful language generation"
2021.naacl-main.60,2020.acl-main.486,0,0.0163347,"or expertise, the use of ad hominems can further diminish the rhetorical credibility (Govier, 1993) of marginalized groups. Offensive Language Detection Ad hominems occur in many forms and are related to different types of offensive language, including abusive language (Yin et al., 2009; Chen et al., 2012; Nobata et al., 2016), hate speech (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Djuric et al., 2015), profanity (Sood et al., 2012a), and the more subtle forms of microaggressions (Breitfeller et al., 2019) and projecting biases and stereotypes through power differentials in language (Sap et al., 2020). Ranging from outright insults to condescension, ad hominems are a form of offensive language that is difficult to comprehensively and objectively define. Nonetheless, these responses are important to characterize, since they can irreparably damage a person’s credibility. It is also generally important 2 Related Work to identify these subtle forms of offensive language, This work is related to a broad spectrum of topics, since it is unclear if existing offensive language deincluding prior definitions of ad hominems and how tection techniques are equally effective for these ad hominems facilit"
2021.naacl-main.60,D19-1339,1,0.840672,"sive language, This work is related to a broad spectrum of topics, since it is unclear if existing offensive language deincluding prior definitions of ad hominems and how tection techniques are equally effective for these ad hominems facilitate biases. Also, analyzing ad subtle forms. 751 Harms in Dialogue Systems Conversational systems are known to perpetuate several types of harms. Ruane et al. (2019) caution about harms that can result from using conversational systems and propose striving for trust and transparency; Roller et al. (2020) suggest techniques for chatbot safety. For analysis, Sheng et al. (2019) evaluate societal biases in language generation, Curry and Rieser (2018) study how conversational systems respond to sexual harassment, and Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output."
2021.naacl-main.60,2020.findings-emnlp.291,1,0.907301,"o, analyzing ad subtle forms. 751 Harms in Dialogue Systems Conversational systems are known to perpetuate several types of harms. Ruane et al. (2019) caution about harms that can result from using conversational systems and propose striving for trust and transparency; Roller et al. (2020) suggest techniques for chatbot safety. For analysis, Sheng et al. (2019) evaluate societal biases in language generation, Curry and Rieser (2018) study how conversational systems respond to sexual harassment, and Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include"
2021.naacl-main.60,2020.acl-main.325,0,0.0119407,"ion, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include lexical constraints. Miao et al. (2019); Zhang et al. (2020b); Susanto et al. (2020) explore insertion-based non-autoregressive decoding algorithms. To be compatible with an autoregressive model like DialoGPT and effective for open-domain generation, we apply constrained decoding to top-k sampling. Our method also differs from these prior works in that it imposes soft constraints to not generate phrases that are likely to lead to ad hominems. Decoding-time techniques that can be used to reduce harmful language generation, e.g., the Plug and Play Language Model (PPLM) (Dathathri et al., 2020), are most relevant to our technique. 3 Dataset and Model Setup Topic # [post, Affects"
2021.naacl-main.60,2020.acl-demos.30,0,0.70891,"formulate related to abusive language, toxicity, and microag- techniques to reduce ad hominem responses and gressions, and can be expressed with both subtle thus the associated harms, which is especially imand explicitly offensive language. Table 1 presents portant for dialogue systems since these systems 750 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 750–767 June 6–11, 2021. ©2021 Association for Computational Linguistics directly interact with users. We analyze responses from DialoGPT (Zhang et al., 2020a) and humans to English Twitter posts. Specifically, we compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH). Through human annotation and trained classifiers, we find that ad hominems exist in both human and DialoGPT responses. Across response sources, there are more ad hominems in #BlackLivesMatter- and #MeToo-related responses, fewer in #Vegan-related responses, and even fewer in #WFH-related responses. The presence of more ad hominems in responses to social issues that concern marginalized groups has troubling im"
2021.naacl-main.60,2020.emnlp-main.698,0,0.0201101,"Missing"
2021.naacl-main.60,W14-1815,0,0.0188433,"evaluate societal biases in language generation, Curry and Rieser (2018) study how conversational systems respond to sexual harassment, and Khatri et al. (2018) detect offensive content with a semi-supervised approach. To reduce harms, Sheng et al. (2020) present a framework for controlling biases in language generation, and Dinan et al. (2019) show how adversarial attacks can make models more robust to offensive language usage from humans. Constrained Decoding For constrained decoding, prior works focus on incorporating words or phrases (as hard or soft constraints) into the decoded output. Swanson et al. (2014) and Balakrishnan et al. (2019) use parse trees among other techniques to enforce constraints in the generated text. Hokamp and Liu (2017); Post and Vilar (2018) propose variants of Grid Beam Search, which generate output that include lexical constraints. Miao et al. (2019); Zhang et al. (2020b); Susanto et al. (2020) explore insertion-based non-autoregressive decoding algorithms. To be compatible with an autoregressive model like DialoGPT and effective for open-domain generation, we apply constrained decoding to top-k sampling. Our method also differs from these prior works in that it imposes"
2021.nuse-1.4,W06-0901,0,0.0623605,"vel event extraction. (2020b) utilized a sequence labeling model with feature extractors at different level for documentlevel relation extraction in biomedical domain. Hu et al. (2020) leveraged contextual information of multi-token entities for document-level named entity recognition. A few studies which tackled document-level event extraction will be reviewed in Section 2. Document-level Event Extraction Similar to other IE tasks, most event extraction methods make predictions within sentences. Initial attempts on event extraction relied on hand-crafted features and a pipeline architecture (Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural approaches, especially large pre-trained language models (Wadden et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020). Recently, event extraction at the document level gains more attention. Yang et al. (2018) proposed a twostage framework for Chinese financial event extraction: 1) sentence-level sequence tagging, and 2) document-level key event detection and heuristicbased argument completion. Zheng et al. (2019) transforms tabular event data into entity-based"
2021.nuse-1.4,I13-1193,0,0.0313706,"ing bipartite matching. However, it does not consider event coreference and less informative arguments (nominal and pronoun). As a solution, we propose two metrics: D OC T RIGGER and D OC A RGUMENT, to properly evaluate event extraction at the document level. The purpose is to conduct evaluation on 4.1 Base Model Our BASE model is built on a span-based IE framework, DY GIE++ (Wadden et al., 2019). DY GIE++ learns entity classification, entity corefernce, and event extraction jointly. The base model extends 1 We set the weights for name, nominal, and pronoun to be 1, 0.5, and 0.25, inspired by Chen and Ng (2013). 38 where COREF(i) denotes the gold set of spans coreferent with candidate span i, and t denotes different tasks. The total loss function for BASE is the weighted sum of all tasks: the entity coreference module of DY GIE++ to handle event coreference. Encoding Ideally, we want to encode all tokens in a document D = {d1 , d2 , ..., dm } with embeddings that covers the context of the entire document. However, due to hardware limitation for long documents, each document is split into multisentences. Each multi-sentence corresponds to a chunk of consecutive sentences. We obtain rich contextualize"
2021.nuse-1.4,N19-1370,0,0.0878694,"Missing"
2021.nuse-1.4,N16-1034,0,0.0224896,"leveraged contextual information of multi-token entities for document-level named entity recognition. A few studies which tackled document-level event extraction will be reviewed in Section 2. Document-level Event Extraction Similar to other IE tasks, most event extraction methods make predictions within sentences. Initial attempts on event extraction relied on hand-crafted features and a pipeline architecture (Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural approaches, especially large pre-trained language models (Wadden et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020). Recently, event extraction at the document level gains more attention. Yang et al. (2018) proposed a twostage framework for Chinese financial event extraction: 1) sentence-level sequence tagging, and 2) document-level key event detection and heuristicbased argument completion. Zheng et al. (2019) transforms tabular event data into entity-based directed acyclic graphs to tackle the argument scattering challenge. Du and Cardie (2020) employed a mutli-granularity reader to aggregate representations from different levels of granularity. H"
2021.nuse-1.4,N18-1131,0,0.0260609,"m hand-crafted global features. Lin et al. (2020) adopted hand-crafted global features with a global scoring function and uses beam search for inference. While these structured prediction methods can model beyond linear dependencies and alleviate the scalability issue, it requires pre-defined In this section, we summarize existing works on document-level information extraction and event extraction, and the application of structured prediction to event extraction tasks. Document-level Information Extraction Information extraction (IE) is mostly studied at the scope of sentence by early works. (Ju et al., 2018; Qin et al., 2018; Stanovsky et al., 2018). Recently, there has been increasing interest in extracting information at the document-level. Jia et al. (2019) proposed a multiscale mechanism that aggregates mention-level representations into entity-level representations for document-level N-ary relation extraction. Jain et al. (2020) presented a dataset for salient entity identification and document-level Nary relation extraction in scientific domain. Li et al. 37 orders for running beam search. In contrast, our method addresses the above two issues by adopting an efficient stuctured prediction"
2021.nuse-1.4,N18-1081,0,0.0267037,"et al. (2020) adopted hand-crafted global features with a global scoring function and uses beam search for inference. While these structured prediction methods can model beyond linear dependencies and alleviate the scalability issue, it requires pre-defined In this section, we summarize existing works on document-level information extraction and event extraction, and the application of structured prediction to event extraction tasks. Document-level Information Extraction Information extraction (IE) is mostly studied at the scope of sentence by early works. (Ju et al., 2018; Qin et al., 2018; Stanovsky et al., 2018). Recently, there has been increasing interest in extracting information at the document-level. Jia et al. (2019) proposed a multiscale mechanism that aggregates mention-level representations into entity-level representations for document-level N-ary relation extraction. Jain et al. (2020) presented a dataset for salient entity identification and document-level Nary relation extraction in scientific domain. Li et al. 37 orders for running beam search. In contrast, our method addresses the above two issues by adopting an efficient stuctured prediction algorithm, Deep Value Networks, which runs"
2021.nuse-1.4,D19-1585,0,0.114526,"event as a person’s life is taken away by an authority. A structured prediction model that learns cross-event interactions can potentially infer the correct event type for death given the previous S ENTENCE event is often carried out by authorities. Introduction Narratives are account of a series of related events or experiences (Urdang, 1968). Extracting events in literature can help machines better understand the underlying narratives. A robust event extraction system is therefore crucial for fully understanding narratives. the scope of sentence (Yang and Mitchell, 2016; Zhao et al., 2018b; Wadden et al., 2019). More recently, Du and Cardie (2020) and Du et al. (2020) treat document-level event extraction as a templatefilling task. Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach. However, none of the previous works learn entity and event coreference jointly with event mention extraction. We hypothesize that joint learning event mention extraction, event coreference, and entity coreference can result in richer representations and better performance. Event extraction aims to identify events composed of a trigger of pre-defined t"
2021.nuse-1.4,N16-1033,0,0.0131855,"ath as type D IE. Instead, it is an E XECUTE event as a person’s life is taken away by an authority. A structured prediction model that learns cross-event interactions can potentially infer the correct event type for death given the previous S ENTENCE event is often carried out by authorities. Introduction Narratives are account of a series of related events or experiences (Urdang, 1968). Extracting events in literature can help machines better understand the underlying narratives. A robust event extraction system is therefore crucial for fully understanding narratives. the scope of sentence (Yang and Mitchell, 2016; Zhao et al., 2018b; Wadden et al., 2019). More recently, Du and Cardie (2020) and Du et al. (2020) treat document-level event extraction as a templatefilling task. Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach. However, none of the previous works learn entity and event coreference jointly with event mention extraction. We hypothesize that joint learning event mention extraction, event coreference, and entity coreference can result in richer representations and better performance. Event extraction aims to identify eve"
2021.nuse-1.4,P18-4009,0,0.0207584,"extraction will be reviewed in Section 2. Document-level Event Extraction Similar to other IE tasks, most event extraction methods make predictions within sentences. Initial attempts on event extraction relied on hand-crafted features and a pipeline architecture (Ahn, 2006; Gupta and Ji, 2009; Li et al., 2013). Later studies gained significant improvement from neural approaches, especially large pre-trained language models (Wadden et al., 2019; Nguyen et al., 2016; Liu et al., 2018; Lin et al., 2020; Balali et al., 2020). Recently, event extraction at the document level gains more attention. Yang et al. (2018) proposed a twostage framework for Chinese financial event extraction: 1) sentence-level sequence tagging, and 2) document-level key event detection and heuristicbased argument completion. Zheng et al. (2019) transforms tabular event data into entity-based directed acyclic graphs to tackle the argument scattering challenge. Du and Cardie (2020) employed a mutli-granularity reader to aggregate representations from different levels of granularity. However, none of these approaches handle entity coreference and event coreference jointly. Our work focus on extracting events at the scope of documen"
2021.nuse-1.4,N18-2003,0,0.0231004,", it is an E XECUTE event as a person’s life is taken away by an authority. A structured prediction model that learns cross-event interactions can potentially infer the correct event type for death given the previous S ENTENCE event is often carried out by authorities. Introduction Narratives are account of a series of related events or experiences (Urdang, 1968). Extracting events in literature can help machines better understand the underlying narratives. A robust event extraction system is therefore crucial for fully understanding narratives. the scope of sentence (Yang and Mitchell, 2016; Zhao et al., 2018b; Wadden et al., 2019). More recently, Du and Cardie (2020) and Du et al. (2020) treat document-level event extraction as a templatefilling task. Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach. However, none of the previous works learn entity and event coreference jointly with event mention extraction. We hypothesize that joint learning event mention extraction, event coreference, and entity coreference can result in richer representations and better performance. Event extraction aims to identify events composed of a t"
2021.nuse-1.4,P18-2066,0,0.0224812,", it is an E XECUTE event as a person’s life is taken away by an authority. A structured prediction model that learns cross-event interactions can potentially infer the correct event type for death given the previous S ENTENCE event is often carried out by authorities. Introduction Narratives are account of a series of related events or experiences (Urdang, 1968). Extracting events in literature can help machines better understand the underlying narratives. A robust event extraction system is therefore crucial for fully understanding narratives. the scope of sentence (Yang and Mitchell, 2016; Zhao et al., 2018b; Wadden et al., 2019). More recently, Du and Cardie (2020) and Du et al. (2020) treat document-level event extraction as a templatefilling task. Li et al. (2020a) performs event mention extraction and the two coreference tasks independently using a pipeline approach. However, none of the previous works learn entity and event coreference jointly with event mention extraction. We hypothesize that joint learning event mention extraction, event coreference, and entity coreference can result in richer representations and better performance. Event extraction aims to identify events composed of a t"
2021.socialnlp-1.16,D19-1290,0,0.0197423,"culture and language. Guti´errez et al. (2016) detect differences of word usage in the cross-lingual topics of multilingual topic modeling results. Lin et al. (2018) present distributional approaches to compute cross-cultural differences or similarities between two terms from different cultures focusing primarily on named entities. Our work is not limited to word usage or any particular topics. Instead, we focus on understanding cross-group differences of perspective at the sentence level. Argumentation In argumentation, Framing is used to emphasize a specific aspect of a controversial topic. Ajjour et al. (2019) introduce frame identification, which is the task of splitting a set of arguments into non-overlapping frames. Chen et al. (2019) also release a dataset of claims, perspectives and evidence and propose the task of substantiated perspective discovery where, given a claim, a system is expected to discover a diverse set of well-corroborated perspectives that take a stance with respect to the claim. Different interests, cultural and cultural backgrounds diverge people from on taking a certain course of action. While both works deal with different perspectives about arguments in English, our work"
2021.socialnlp-1.16,C12-2016,0,0.0293782,"the liberty and market principles of the individual for the social market economy, advocating that government intervention in the individual and the market should be minimized. ... In Japan, since the restructuring of the electric power business in 1950, there are 10 private electric power companies, one in each region. [Translated] Introduction Sociologists have defined culture as a set of shared understandings, herein called perspectives, adopted by the members of that culture (Bar-Tal, 2000; Sperber and Hirschfeld, 2004). Languages and cultures have radical correlations (Khaslavsky, 1998; Bracewell and Tomlinson, 2012; Gelman and Roberts, 2017), because individuals communicate with each other by language, which carries the aspects of their cultures, experiences, beliefs, and values, thus will shape their perspectives. Lacking of understanding for these perspective differences could lead to biased predictions. Selection bias (Heckman, 1977) can often lead to misinformation as it sometimes ignores facts that do not reflect the entire population intended to be analyzed. For example, to verify a controversial statement like “The 1 A group of people that share the same language (https://www.merriam-webster.com/"
2021.socialnlp-1.16,N19-1054,1,0.819432,"opia Communism is the best system to live by. Then we ask the English and Chinese volunteers to jointly select high-quality statements. Finally, for human annotation, we select out 128 highquality claims from over 2,000 candidates in the IMO/IMHO dataset. The topics include personal life, social and political views, etc. 6 Table 3: Sentence from the IMO dataset expressing opinions about which differ between cultures. We are motivated by the fact that people always express personal opinions on social media such as Reddit, where many opinionated claims are included. We leverage a previous work (Chakrabarty et al., 2019) which collects a distant supervisionlabeled corpus of 5.5 million opinionated claims covering a wide range of topics using sentences containing the acronyms IMO (in my opinion) or IMHO (in my humble opinion) from Reddit. Table 3 shows two examples from the IMO dataset that may reveal contrasting perspectives between two different colingual groups. As this dataset is only in English, to obtain scores from the Chinese and Japanese cultural models, we translate each sentence into the target language using the Youdao and Google Translate API5 . Test Data Selection. We first automatically extract"
2021.socialnlp-1.16,Q16-1004,0,0.0681781,"Missing"
2021.socialnlp-1.16,D14-1083,0,0.0342359,"Data and code are available at https://github.com/PlusLabNLP/CLUSTER Here l1 and l2 each denotes a language such as cn−jp ‘cn’ and ‘jp’. A positive Dmodel indicates that the Chinese model agrees more with the claim s than the Japanese model. Similarly, we denote l1 −l2 Dhuman ∈ [−1, 1] as the quantity of perspective difference reported by human annotators: l1 −l2 Dhuman = H l1 (sl1 ) − H l2 (sl2 ), l1 6= l2 . English Chinese Japanese (2) Comparison With Stance Detection. Stance detection aims at detecting if a piece of text (usually a sentence or a document) supports or opposes a given claim (Hasan and Ng, 2014). Unlike stance detection, we do not have a given text associated with our claims. Instead, we learn representations of group perspectives through training on language corpora so that we can identify if a claim is likely to be supported or opposed by a group. Data Preparation In this section, we describe the procedure of composing our training data from multi-lingual Wikipedia articles. We then introduce an out-ofdomain test dataset retrieved from Reddit that contain opinions regarding wide range of topics and the procedure of collecting human annotations on the test set. 3.1 Training Data Top"
2021.socialnlp-1.16,W18-2703,0,0.0244485,"ata, most classifiers would be able to identify this. Niven and Kao (2019) show that high performance obtained from pre-trained language models such as BERT (Devlin et al., 2018) are often achieved by exploiting spurious statistical cues in the dataset. We face a similar problem in our preliminary study when evaluating on a test set from a different domain. While the quantitative results of our models trained on Wikipedia data are extremely high, we observe a huge drop when testing on out-of-domain data. This motivates us to mitigate statistical cues in our data. Inspired by back-translation (Hoang et al., 2018), we generate paraphrases of our training data by introducing a pivot language and then translating the sentences back. This retains the semantics of the statements while removing existing stylistic biases. We back-translated both original Wikipedia sentences (i.e., positive samples) and the fabricated ones (i.e., negative samples). Part C and D of Figure 1 show the back-translated versions of our posEnglish-Chinese (E-C) Chinese-Japanese (C-J) Japanese-English (J-E) Pearson correlation Spearman correlation 0.26 (3e-3) 0.49(5e-9) 0.29(7e-4) 0.27(2e-3) 0.50(2e-9) 0.30(6e-4) Table 5: Cross-group"
2021.socialnlp-1.16,P18-1066,0,0.0165896,"on. The meaning of dots is the same as Figure 2. Orange triangles represent the following sentences: 7. Cricket is as fun to play as baseball if you limit the “innings” or overs. 8. Things like basketball, baseball, tennis, golf, etc. are far more popular globally. 9. Christmas, even minus the religious meanings, has good attributes in theory but has been too commercialized. 10. I believe in giving gifts to kids because, Christmas is for children. culture and language. Guti´errez et al. (2016) detect differences of word usage in the cross-lingual topics of multilingual topic modeling results. Lin et al. (2018) present distributional approaches to compute cross-cultural differences or similarities between two terms from different cultures focusing primarily on named entities. Our work is not limited to word usage or any particular topics. Instead, we focus on understanding cross-group differences of perspective at the sentence level. Argumentation In argumentation, Framing is used to emphasize a specific aspect of a controversial topic. Ajjour et al. (2019) introduce frame identification, which is the task of splitting a set of arguments into non-overlapping frames. Chen et al. (2019) also release a"
2021.socialnlp-1.16,W19-5333,0,0.0323782,"Missing"
2021.socialnlp-1.16,P19-1459,0,0.0219339,"e multilingualism of BERT. In other words, the learning steps of English, Chinese and Japanese systems have exactly the same structure but are completely isolated from each other in terms of training data and model parameters. 4.2 English (EN) Pattern Bias in Negative Samples and Targeted Improvements While flipping adjectives to create negative samples appears as an obvious approach, it ends up introducing certain style biases. Since the placeholders for adjectives are the only difference between positive and negative samples in training data, most classifiers would be able to identify this. Niven and Kao (2019) show that high performance obtained from pre-trained language models such as BERT (Devlin et al., 2018) are often achieved by exploiting spurious statistical cues in the dataset. We face a similar problem in our preliminary study when evaluating on a test set from a different domain. While the quantitative results of our models trained on Wikipedia data are extremely high, we observe a huge drop when testing on out-of-domain data. This motivates us to mitigate statistical cues in our data. Inspired by back-translation (Hoang et al., 2018), we generate paraphrases of our training data by intro"
2021.socialnlp-1.16,W15-4625,0,0.0308673,"to agree more with sentence 2 (lower right corner). We select representative examples in each region and list them in the captions. First, from Figure 2 we observe that the model pairs have zero or negative correlation on three topics: marriage, corruption and cuisine, suggesting that the corresponding language speakers take contrasting stances towards these topics. Second, Figure 3 shows that 1) the English and Chinese 8 Related Work Online Disagreement Most works about online disagreement focus on a single culture or language (Sridhar et al., 2015; Wang and Yang, 2015; Sridhar et al., 2015; Rosenthal and McKeown, 2015), thus are restricted to a single group. While these works try to computationally model disagreement or stance in debates, they do not target at finding cultural or cross-group differences. We, on the other hand, aim at understanding the disagreement in perspectives through different colingual groups according to their respective languages. Cultural Study in Blogs or Social Media Nakasaki et al. (2009) present a framework to visualize the cross-cultural differences in multilingual blogs. Elahi and Monachesi (2012) show that using emotion terms as culture features is effective in analyzing cros"
2021.socialnlp-1.16,P15-1012,0,0.0265073,"Chinese culture (upper left corner), while English speakers tend to agree more with sentence 2 (lower right corner). We select representative examples in each region and list them in the captions. First, from Figure 2 we observe that the model pairs have zero or negative correlation on three topics: marriage, corruption and cuisine, suggesting that the corresponding language speakers take contrasting stances towards these topics. Second, Figure 3 shows that 1) the English and Chinese 8 Related Work Online Disagreement Most works about online disagreement focus on a single culture or language (Sridhar et al., 2015; Wang and Yang, 2015; Sridhar et al., 2015; Rosenthal and McKeown, 2015), thus are restricted to a single group. While these works try to computationally model disagreement or stance in debates, they do not target at finding cultural or cross-group differences. We, on the other hand, aim at understanding the disagreement in perspectives through different colingual groups according to their respective languages. Cultural Study in Blogs or Social Media Nakasaki et al. (2009) present a framework to visualize the cross-cultural differences in multilingual blogs. Elahi and Monachesi (2012) show th"
2021.socialnlp-1.16,D15-1306,0,0.0250723,"left corner), while English speakers tend to agree more with sentence 2 (lower right corner). We select representative examples in each region and list them in the captions. First, from Figure 2 we observe that the model pairs have zero or negative correlation on three topics: marriage, corruption and cuisine, suggesting that the corresponding language speakers take contrasting stances towards these topics. Second, Figure 3 shows that 1) the English and Chinese 8 Related Work Online Disagreement Most works about online disagreement focus on a single culture or language (Sridhar et al., 2015; Wang and Yang, 2015; Sridhar et al., 2015; Rosenthal and McKeown, 2015), thus are restricted to a single group. While these works try to computationally model disagreement or stance in debates, they do not target at finding cultural or cross-group differences. We, on the other hand, aim at understanding the disagreement in perspectives through different colingual groups according to their respective languages. Cultural Study in Blogs or Social Media Nakasaki et al. (2009) present a framework to visualize the cross-cultural differences in multilingual blogs. Elahi and Monachesi (2012) show that using emotion term"
D15-1064,W04-1119,0,0.0465915,"putational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media,"
D15-1064,W14-2907,0,0.0288029,"Missing"
D15-1064,W10-0713,1,0.384968,"Missing"
D15-1064,H05-1091,0,0.213055,"Missing"
D15-1064,P05-1045,0,0.0112183,"as above. where C is a tradeoff parameter. 3.3 Mentions Nominal 0 38 31 636 Table 1: Mention statistics for the Weibo NER corpus. The first objective is notated Ls for “supervised” (trained on labeled NER data), and the second is Lu , “unsupervised” (trained on raw text.) Both objectives share the same variables ew . The overall goal is to maximize their weighted sum: arg max = Ls (λ, ew ) + CLu (ew ) Name 243 88 224 721 5 Experiments We evaluate our methods under two settings: training on only name mentions, and training on both name and nominal mentions. We re-train the Stanford NER system (Finkel et al., 2005) as a baseline; besides, we also evaluate our implementation of the CRF from Mao et al. (2008) as described in §2 as Baseline Features. To this baseline, we add each of our three embedding models: word, character, character+position (as described in §3), and report results on the modified Weibo NER Corpus We constructed a corpus of Weibo messages annotated for NER. We followed the DEFT ERE (Linguistics Data Consortium, 2014) 9 annotation 9 See Aguilar et al. (2014) for a comparison of DEFT ERE with other common standards. 10 551 https://github.com/hltcoe/golden-horse Method Stanford Baseline F"
D15-1064,W10-0701,1,0.359481,"013) to do skip-gram training for language model, and implement our own CRF model to modify the embeddings. We optimize (2) by alternating the optimzation of each of the two objectives. 4 Total 243 126 255 1,357 guidelines for entities, which includes four major semantic types: person, organization, location and geo-political entity. We annotated both name and nominal mentions. Chinese pronoun mentions can be easily recognized with a regular expression. We used Amazon Mechanical Turk, using standard methods of multiple annotators and including gold examples to ensure high quality annotations (Callison-Burch and Dredze, 2010). Our corpus includes 1,890 messages sampled from Weibo between November 2013 and December 2014. Rather than selecting messages at random, which would yield a small number of messages with entities, we selected messages that contained three or more (segmented) words that were not in a fixed vocabulary of common Chinese words. Initial experiments showed this gave messages more likely to contain entities. Table 1 shows statistics of the final corpus. We divided the corpus into 7 folds, each with 127 messages, where each message corresponds to a single instance. We use the first 5 folds for train"
D15-1064,fromreide-etal-2014-crowdsourcing,0,0.0434094,"tion Named entity recognition (NER), and more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to Engli"
D15-1064,N15-1075,0,0.309073,"se Social Media with Jointly Trained Embeddings Nanyun Peng and Mark Dredze Human Language Technology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore sev"
D15-1064,W12-6321,0,0.139576,"Missing"
D15-1064,W99-0613,0,0.142277,"Missing"
D15-1064,I08-4010,0,0.0293428,"Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses"
D15-1064,C10-1032,1,0.818044,"Missing"
D15-1064,W12-6307,0,0.0727395,"formal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 1 Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2 Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 54"
D15-1064,P13-1018,0,0.0361143,"Missing"
D15-1064,P11-1037,0,0.0174892,"a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER pe"
D15-1064,P10-1040,0,0.0148339,"reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We"
D15-1064,P12-1055,0,0.015772,"more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 1 Since we consider name and nom"
D15-1064,I08-4013,0,0.681737,"rs, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media, which has worse word segmentation. Additionally, typical Chinese corpora use exclusively traditional or simplified characters, whereas social media mixes them. Figure 1 demonstrates some challenges. The baseline system for our task is our own implementation of Mao et al. (2008), which is the current state-of-the-art on the SIGHAN 2008 shared task (Jin and Chen, 2008). They use a CRF tagger with a BIOSE (begin, inside, outside, singleton, end) encoding that tags individual characters, not words, since word segmentation errors are especially problematic for NER (Zhang et al., 2006). Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting"
D15-1064,W03-0430,0,0.0697886,"Missing"
D15-1064,P14-2089,1,0.805775,"okup the embedding that matches the segmented word. Since the NER system tags characters, we add the same word embedding features to each character in the word. This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF"
D15-1064,Q15-1017,1,0.81813,"y deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF is given by: Character Embeddings We learn an embedding for each character in the training corpus (Sun et al., 2014; Liu et al., 2014).This removes the dependency on pre-processing the text, and better fits our intended use case: NER tagging over characters. Since there are many fewer characters than words, we learn many fewer embeddings. On the one hand, this means fewer parameters and less over-fitt"
D15-1064,W06-0126,0,0.744155,"548–554, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundari"
D15-1064,W14-1609,0,0.0310465,"al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We present three types of"
D15-1064,D13-1031,0,0.0119729,". Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting predictions as a feature for an NER system. Furthermore, they make extensive use of gazetteer features. For simplicity, we exclude the first pass mention detection and the gazetteer features, which make only small improvements to their overall performance. We note that other implementations of this system (Zhang et al., 2013) have been unable to match the performance reported in Mao et al. (2008). Similarly, our implementation yields results on SIGHAN 2008 similar 看见前女友和她的新欢走在一起的时候，已经无处可躲了，只好 硬着 头皮上去打招呼哎呀，好久不见，你儿子都这么高了。 When saw ex-girl friend and her new partner coming across, nowhere to hide, have to say hello, long time no see, your son grown up. Figure 1: Examples of Weibos messages and translations with named (red) and nominal (blue) mentions. to those reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embe"
D15-1064,P14-1028,0,0.0578429,"Missing"
D15-1064,D13-1061,0,0.0431402,"Missing"
D15-1064,C14-1015,0,0.0255117,"Missing"
D15-1064,W03-1718,0,0.050791,"ssociation for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are wors"
D15-1064,P11-1138,0,0.0698598,"Missing"
D15-1064,D11-1141,0,0.0509802,"Missing"
D15-1108,P03-1006,0,0.0606744,"2). To improve the method, recall that subproblem k considers only variables X k . It is indifferent to the value of Xi if Xi ∈ / X k , so we just leave xki undefined in the subproblem’s solution. We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality. Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi .7 4.2 Gki (xk ) = λki · γ(xki ) def (4) These unary factors penalize strings according to the Lagrange multipliers. They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual. The topology of the WFSA for Gki depends only on Wi , while its weights come from λki . 4.3 Projected Subgradient Method We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3). Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method. We initialize λ = 0 and compute (3) by solving the K subproblems. Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition"
D15-1108,D07-1093,0,0.086226,"Missing"
D15-1108,D10-1125,0,0.0361387,"inite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology. 919 4 Dual Decomposition rεzɪgn eɪʃən rεzɪgn#eɪʃən rizajn z rizajn#z dæmn eɪʃən dæmn#eɪʃən r,εzɪgn’eɪʃn riz’ajnz d,æmn’eɪʃn Subproblem 1 Subproblem 2 Subproblem 3 dæmn z Dual decomposition is a general technique for solving constrained optimization problems. It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014). However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗ . dæmn#z d’æmz Subproblem 4 Figure 2: To apply dual decomposition, we choose to decompose 1 into one subproblem per surface word. Dashed lines connect two or more variables from different subproblems that correspond to the same variable in the original graph. The method of Lagrange multipliers is used to force these variables to have identical values. An additional unary factor attached to each"
D15-1108,N15-1094,1,0.87548,"work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical rea"
D15-1108,P14-2102,1,0.822861,"ability of copying the next character of the underlying word as it is transduced to the surface word. The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes. Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology. Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014). The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena. E XERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks. Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes. CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995). Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes. 5.2 Evaluation Scheme We compared"
D15-1108,Q15-1031,1,0.10354,"noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms. Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis. In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges. We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015). We show that the method generally converges and that it achieves better results than alternatives. The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3. Section 4 develops dual decomposition inference for graphical models over strings. Then our experimental setup and results are presented in sections 5 and 6, with some discussion. We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, w"
D15-1108,D09-1011,1,0.883132,"Thus, a degree d-factor scores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to"
D15-1108,D11-1057,1,0.881701,"cores some length-d subtuple of x. The score of the whole joint assignment simply sums over all factors: We seek the x of maximum score that is consistent with our partial observation of x. This is a generic constraint satisfaction problem with soft constraints. While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution def p(x) = (1/Z) exp score(x). models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015). The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗ where Σ is some fixed, finite alphabet. As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004). Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3 Past work has shown how to approximately sample from"
D15-1108,N12-1024,1,0.896936,"g Count Features But what do we do if the variables are strings? The Lagrangian term λki ·xki in (3) is now ill-typed. We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers. This corresponds to changing the constraint in (2). Instead of requiring x1i = · · · = xK i for each 1 i, we are now requiring γ(xi ) = · · · = γ(xK i ), i.e., these strings must agree in their features. We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match. We follow Paul and Eisner (2012) and use a substring count feature for each w ∈ Σ∗ . In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8 Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros. This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors. We will use a further trick below to prevent rapid growth of this finite set of nonzeros. Each variab"
D15-1108,P10-1105,0,0.0677345,"has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM"
D15-1108,D11-1032,0,0.0133363,"oximately sample from the distribution over x defined by such a model (Bouchard-Cˆot´e et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015). 2.2 2.3 def score(x) = X F (x). (1) F ∈F The String Case Graphical models over strings have enjoyed some attention in the NLP community. Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Cˆot´e et al., 2007; Bouchard-Cˆot´e et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011). Cyclic graphical Finite-State Belief Propagation BP iteratively updates messages between factors and variables. Each message is a vector whose elements score the possible values of a variable. Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs. For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power). Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari2 E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al.,"
D15-1108,W10-2902,0,0.0304375,"test11 That is, probability mass of (1 − θ)/3 is divided equally among the |Σ |possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1 − θ)/3 is allocated to deletion. 10 The model also has a three-way factor, connecting layers 1 and 2 of Figure 1. This represents deterministic concatenation (appropriate for these languages) and has no parameters. 923 ing. This gives us three approximations to EM, based on DD, SP and MP. Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010). For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD. As initialization, our first E step uses the trained version of Model 1 for the same inference method. 5.5 (a) Tangale (b) Catalan (c) Maori (d) English Inference Details We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations). We run DD to convergence (usually &lt; 600 iterations). DD iterations are much faster since each variable considers d strings, not d distributions over strings. Hence DD does not intersect distributions, and many part"
D15-1108,J98-4003,0,\N,Missing
D19-1041,S13-2002,0,0.2618,"Missing"
D19-1041,D12-1062,0,0.0467006,"y this property, MAP Inference 8(i, j) 2 EE, ePi A MAP inference is needed both during training ˆ n in the loss function (Equation 1), as to obtain y well as during the test time to get globally coherent assignments. We formulate the inference problem as an ILP problem. The inference framework is established by constructing a global objective function using scores from local scorers and imposing several global constraints: 1) one-label assignment, 2) event-relation consistency, and 3) symmetry and transitivity as in Bramsen et al. (2006); Chambers and Jurafsky (2008); Denis and Muller (2011); Do et al. (2012); Ning et al. (2017). 3.3.1 N and eN i + ej yˆ = arg max r r yi,j S(yi,j , x) r r ¯ 8(i, j), (j, k) 2 EE, yi,j = yj,i , (symmetry) r1 r2 yi,j + yj,k X X (2) X yke S(yke , x) r2R r yi,j = 1, X r3 yi,k  1, (transitivity) Intuitively, the symmetry constraint forces two pairs of events with flipping orders to have reversed relations. For example, if ri,j = BEFORE, then rj,i = AFTER. The transitivity constraint rules that if (i, j), (j, k) and (i, k) pairs exist in the graph, the label (relation) prediction of (i, k) pair k2E e2{0,1} r s.t. yi,j , yke 2 {0, 1} , X r3 2T rans(r1 ,r2 ) (i,j)2EE r2R"
D19-1041,P16-2011,0,0.0490697,"Missing"
D19-1041,S13-2015,0,0.149311,"raction using neural methods (Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016), recent progress in the temporal relation domain is focused more on the setting where gold events are provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our k"
D19-1041,P14-2082,0,0.233167,"idates. Both event and relation labels are assigned simutaneously during the global inference with ILP, as specified in Section 3.3. We also filter out tokens with POS tags that do not appear in the training set as most of the events are either nouns or verbs in TB-Dense, and all events are verbs in MATRES. Table 1: Data overview. Note that the numbers reported for MATRES do not include the AQUAINT section. annotated temporal relation corpora with all events and relations fully annotated is reported to be a challenging task as annotators could easily overlook some facts (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014; Ning et al., 2017), which made both modeling and evaluation extremely difficult in previous event temporal relation research. The TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and it has been widely evaluated on this task (Chambers et al., 2014; Ning et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018). Recent data construction efforts such as MATRES (Ning et al., 2018a) further enhance the data quality by using a multi-axis annotation scheme and adopting a startpoint of ev"
D19-1041,E17-1108,0,0.133723,"re provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014)"
D19-1041,S13-2012,0,0.0791331,"Missing"
D19-1041,P14-1038,0,0.0281215,"sky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best joint predictions. Li and Ji (2014) define a global structure for joint entity and relation extraction, encode local and global features based on domain and linguistic knowledge. and leverage beam-search to find global optimal assignments for entities and relations. Miwa and Bansal (2016) leverage LSTM architectures to jointly predict both entity and relations, but fall short on ensuring prediction consistency. Zhang et al. (2017) combine the benefits of both neural net and global optimization with beam Related Work In this section we briefly summarize the existing work on event extraction and temporal relation extraction. To t"
D19-1041,Q14-1022,0,0.660747,"this task as a pipeline of two separate subtasks, (c) Structured Joint Model Figure 1: An illustration of event and relation models in our proposed joint framework. (a) is a (partial) graph of the output of the relation extraction model. “Hutu” is not an event and hence all relations including it should be annotated as NONE. (b) and (c) are comparisons between a pipeline model and our joint model. i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Chambers et al., 2014; Ning et al., 2017; Meng and Rumshisky, 2018). Specifically, they built end-toend systems that extract events first and then predict temporal relations between them (Fig. 1b). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that ex434 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 434–444, c Hong Kong, China, November 3–7, 2019. 20"
D19-1041,P06-1095,0,0.109501,"entropy) with hand-engineered features (e.g., ClearTK (Bethard, 2013) and NavyTime (Chambers, 2013)). While other domains have shown progress on event extraction using neural methods (Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016), recent progress in the temporal relation domain is focused more on the setting where gold events are provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both"
D19-1041,D08-1073,0,0.0801379,"re events. The following global constraints will satisfy this property, MAP Inference 8(i, j) 2 EE, ePi A MAP inference is needed both during training ˆ n in the loss function (Equation 1), as to obtain y well as during the test time to get globally coherent assignments. We formulate the inference problem as an ILP problem. The inference framework is established by constructing a global objective function using scores from local scorers and imposing several global constraints: 1) one-label assignment, 2) event-relation consistency, and 3) symmetry and transitivity as in Bramsen et al. (2006); Chambers and Jurafsky (2008); Denis and Muller (2011); Do et al. (2012); Ning et al. (2017). 3.3.1 N and eN i + ej yˆ = arg max r r yi,j S(yi,j , x) r r ¯ 8(i, j), (j, k) 2 EE, yi,j = yj,i , (symmetry) r1 r2 yi,j + yj,k X X (2) X yke S(yke , x) r2R r yi,j = 1, X r3 yi,k  1, (transitivity) Intuitively, the symmetry constraint forces two pairs of events with flipping orders to have reversed relations. For example, if ri,j = BEFORE, then rj,i = AFTER. The transitivity constraint rules that if (i, j), (j, k) and (i, k) pairs exist in the graph, the label (relation) prediction of (i, k) pair k2E e2{0,1} r s.t. yi,j , yke 2 {"
D19-1041,P07-2044,0,0.161603,"g., ClearTK (Bethard, 2013) and NavyTime (Chambers, 2013)). While other domains have shown progress on event extraction using neural methods (Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016), recent progress in the temporal relation domain is focused more on the setting where gold events are provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them"
D19-1041,P18-1049,0,0.487402,"btasks, (c) Structured Joint Model Figure 1: An illustration of event and relation models in our proposed joint framework. (a) is a (partial) graph of the output of the relation extraction model. “Hutu” is not an event and hence all relations including it should be annotated as NONE. (b) and (c) are comparisons between a pipeline model and our joint model. i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Chambers et al., 2014; Ning et al., 2017; Meng and Rumshisky, 2018). Specifically, they built end-toend systems that extract events first and then predict temporal relations between them (Fig. 1b). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that ex434 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 434–444, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics t"
D19-1041,D17-1092,0,0.336139,"use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best jo"
D19-1041,P17-2001,0,0.547953,"ral relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to"
D19-1041,P16-1105,0,0.189028,"rk on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best joint predictions. Li and Ji (2014) define a global structure for joint entity and relation extraction, encode local and global features based on domain and linguistic knowledge. and leverage beam-search to find global optimal assignments for entities and relations. Miwa and Bansal (2016) leverage LSTM architectures to jointly predict both entity and relations, but fall short on ensuring prediction consistency. Zhang et al. (2017) combine the benefits of both neural net and global optimization with beam Related Work In this section we briefly summarize the existing work on event extraction and temporal relation extraction. To the best of our knowledge, there is no prior work on joint event and relation extraction, so we will review joint entity and relation extraction works instead. Existing event extraction methods in the temporal relation domain, as in the TempEval3 work435"
D19-1041,D14-1200,0,0.072143,"enberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and construct global loss to compute the best joint predictions. Li and Ji (2014) define a global structure for joint entity and relation extraction, encode local and global features based on domain and linguistic knowledge. and leverage beam-search to find global optimal assignments for entities and relations. Miwa and Bansal (2016) leverage LSTM architectures to jointly predict both entity and relations, but fall short on ensuring prediction consistency. Zhang et al."
D19-1041,S07-1014,0,0.445333,"on). As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, (c) Structured Joint Model Figure 1: An illustration of event and relation models in our proposed joint framework. (a) is a (partial) graph of the output of the relation extraction model. “Hutu” is not an event and hence all relations including it should be annotated as NONE. (b) and (c) are comparisons between a pipeline model and our joint model. i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Chambers et al., 2014; Ning et al., 2017; Meng and Rumshisky, 2018). Specifically, they built end-toend systems that extract events first and then predict temporal relations between them (Fig. 1b). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that ex434 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages"
D19-1041,N16-1034,0,0.0518614,"Missing"
D19-1041,C08-3012,0,0.276907,"013) and NavyTime (Chambers, 2013)). While other domains have shown progress on event extraction using neural methods (Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016), recent progress in the temporal relation domain is focused more on the setting where gold events are provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works abo"
D19-1041,P15-2060,0,0.117067,"Missing"
D19-1041,D17-1108,1,0.928401,"of two separate subtasks, (c) Structured Joint Model Figure 1: An illustration of event and relation models in our proposed joint framework. (a) is a (partial) graph of the output of the relation extraction model. “Hutu” is not an event and hence all relations including it should be annotated as NONE. (b) and (c) are comparisons between a pipeline model and our joint model. i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Chambers et al., 2014; Ning et al., 2017; Meng and Rumshisky, 2018). Specifically, they built end-toend systems that extract events first and then predict temporal relations between them (Fig. 1b). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that ex434 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 434–444, c Hong Kong, China, November 3–7, 2019. 2019 Association for"
D19-1041,P09-1046,0,0.181411,"cused more on the setting where gold events are provided. Therefore, we first show the performance of a neural event extractor on this task, although it is not our main contribution. Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation"
D19-1041,P18-1212,1,0.891347,"ng module, and how inference and learning are performed). We denote the set of all possible relation labels (including NONE) as R, all event candidates (both events and nonevents) as E, and all relation candidates as EE. 3.1 Neural SSVM Our neural SSVM adapts the SSVM loss as: l X C ⇥ n ˆ n ) + S¯R L= max 0, (y n , y M n yˆ n 2Y n=1 ⇤ + CE S¯En + |2 , Multi-Tasking Neural Scoring Function (1) 1 Note that if the best prediction is the same as the gold structure, the margin is zero; there will be no loss. 2 Following the convention of event relation prediction literature (Chambers et al., 2014; Ning et al., 2018a,b), we only consider event pairs that occur in the same or neighboring sentences, but the architecture can be easily adapted to the case where inputs are longer than two sentences. n = where S¯En = S(ˆ y nE ; xn ) S(y nE ; xn ) and S¯R S(ˆ y nR ; xn ) S(y nR ; xn ); denotes model parameters, n indexes instances, M n = |E|n + |EE|n de436 indicate a candidate event in the input sentences of length N. We fix word embeddings computed by a pre-trained BERT-base model (Devlin et al., 2018). They are then fed into a BiLSTM layer to further encode task-specific contextual information. Both event and"
D19-1041,P18-1122,1,0.776911,"ng module, and how inference and learning are performed). We denote the set of all possible relation labels (including NONE) as R, all event candidates (both events and nonevents) as E, and all relation candidates as EE. 3.1 Neural SSVM Our neural SSVM adapts the SSVM loss as: l X C ⇥ n ˆ n ) + S¯R L= max 0, (y n , y M n yˆ n 2Y n=1 ⇤ + CE S¯En + |2 , Multi-Tasking Neural Scoring Function (1) 1 Note that if the best prediction is the same as the gold structure, the margin is zero; there will be no loss. 2 Following the convention of event relation prediction literature (Chambers et al., 2014; Ning et al., 2018a,b), we only consider event pairs that occur in the same or neighboring sentences, but the architecture can be easily adapted to the case where inputs are longer than two sentences. n = where S¯En = S(ˆ y nE ; xn ) S(y nE ; xn ) and S¯R S(ˆ y nR ; xn ) S(y nR ; xn ); denotes model parameters, n indexes instances, M n = |E|n + |EE|n de436 indicate a candidate event in the input sentences of length N. We fix word embeddings computed by a pre-trained BERT-base model (Devlin et al., 2018). They are then fed into a BiLSTM layer to further encode task-specific contextual information. Both event and"
D19-1041,D18-2013,1,0.59015,"ng module, and how inference and learning are performed). We denote the set of all possible relation labels (including NONE) as R, all event candidates (both events and nonevents) as E, and all relation candidates as EE. 3.1 Neural SSVM Our neural SSVM adapts the SSVM loss as: l X C ⇥ n ˆ n ) + S¯R L= max 0, (y n , y M n yˆ n 2Y n=1 ⇤ + CE S¯En + |2 , Multi-Tasking Neural Scoring Function (1) 1 Note that if the best prediction is the same as the gold structure, the margin is zero; there will be no loss. 2 Following the convention of event relation prediction literature (Chambers et al., 2014; Ning et al., 2018a,b), we only consider event pairs that occur in the same or neighboring sentences, but the architecture can be easily adapted to the case where inputs are longer than two sentences. n = where S¯En = S(ˆ y nE ; xn ) S(y nE ; xn ) and S¯R S(ˆ y nR ; xn ) S(y nR ; xn ); denotes model parameters, n indexes instances, M n = |E|n + |EE|n de436 indicate a candidate event in the input sentences of length N. We fix word embeddings computed by a pre-trained BERT-base model (Devlin et al., 2018). They are then fed into a BiLSTM layer to further encode task-specific contextual information. Both event and"
D19-1041,D17-1182,0,0.0547208,"Missing"
D19-1041,W16-5706,0,0.242013,"Missing"
D19-1041,P17-2035,0,0.345597,"Early attempts on temporal relation extraction use local pair-wise classification with handengineered features (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008). Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), NavyTime (Chambers, 2013), and CAEVO (Chambers et al., 2014) improve earlier work with better linguistic and syntactic rules. Yoshikawa et al. (2009); Ning et al. (2017); Leeuwenberg and Moens (2017) explore structured learning for this task, and more recently, neural methods have also been shown effective (Tourille et al., 2017; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018). In practice, we need to extract both events and those temporal relations among them from raw text. All the works above treat this as two subtasks that are solved in a pipeline. To the best of our knowledge, there has been no existing work on joint event-temporal relation extraction. However, the idea of “joint” has been studied for entityrelation extraction in many works. Miwa and Sasaki (2014) frame their joint model as table filling tasks, map tabular representation into sequential predictions with heuristic rules, and co"
D19-1041,S13-2001,0,0.105121,"xisting systems treat this task as a pipeline of two separate subtasks, (c) Structured Joint Model Figure 1: An illustration of event and relation models in our proposed joint framework. (a) is a (partial) graph of the output of the relation extraction model. “Hutu” is not an event and hence all relations including it should be annotated as NONE. (b) and (c) are comparisons between a pipeline model and our joint model. i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier (Verhagen et al., 2007, 2010; UzZaman et al., 2013; Chambers et al., 2014; Ning et al., 2017; Meng and Rumshisky, 2018). Specifically, they built end-toend systems that extract events first and then predict temporal relations between them (Fig. 1b). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that ex434 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 434–444, c Hong Kong, China,"
D19-1103,W17-0401,0,0.399038,"Missing"
D19-1103,W14-4200,0,0.242095,"Missing"
D19-1103,N19-1253,1,0.725246,"Smith et al., 2017) or delexicalization approaches (Zeman and Resnik, 2008; McDonald et al., 2013). One key challenge for cross-lingual transfer is the differences among languages; for example, languages may have different word orders. When transferring a model learned from a source language to target languages, the performance may drop significantly due to the differences. To tackle this problem, various approaches have been proposed to better capture the commonalities between the source and the target languages (McDonald et al., 2011; Guo et al., 2016; T¨ackstr¨om et al., 2013; Agi´c, 2017; Ahmad et al., 2019); however, they overlook the potential to leverage linguistic knowledge about the target language to account for the differences between the source and the target languages to facilitate the transfer. In this paper, we propose a complementary approach that studies how to leverage the linguistic knowledge about the target languages to help the transfer. Specifically, we use corpus linguistic statistics of the target languages as weak supervision signals to guide the test-time inference process when parsing with a graph-based parser. This approach is effective as the model only need to be traine"
D19-1103,P15-1133,0,0.0226936,"rom multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the target languages. Constrained Inference for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these"
D19-1103,Q17-1010,0,0.0186393,"real low-resource languages. We first introduce the experimental setup including data selection and constraint details and then discuss the results as well as in-depth analysis. 4.1 Setup Model and Data We train the best performing Att-Graph parser proposed in Ahmad et al. (2019) 3 In implementation, we use stochastic gradient descent with Adam optimizer (Kingma and Ba, 2015) on English and transfer it to 19 target languages in UD Tree Bank v2.2 (Nivre et al., 2018).4 The model takes words and predicted POS tags5 as input, and achieve transfer by leveraging pre-trained multi-lingual FastText (Bojanowski et al., 2017) embeddings that project the word embeddings from different languages into the same space using an offline transformation method (Smith et al., 2017; Conneau et al., 2018). The SelfAtt-Graph model uses a Transformer (Vaswani et al., 2017) with relative position embedding as the encoder and a deep biaffine scorer (Dozat and Manning, 2017) as the decoder. We follow the setting in Ahmad et al. (2019) to train and tune only on the source language (English) and directly transfer to all the target languages. We modify their decoder to incorporate constraints with the proposed constrained inference a"
D19-1103,D11-1003,0,0.0182421,"get languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable structure, Rush and Collins (2012) decompose the structure and incorporate constraints on some composite tasks. To improve the performance of a model, Chang and Collins (2011); Peng et al. (2015) incorporate constraints on exact decoding tasks and inference tasks on graphical models, and Chang et al. (2013); Dalvi (2015); Martins (2015) incorporate corpus-level constraints on semi-supervised multilabel classification and coreference resolution. Zhao et al. (2017) incorporate corpus-level constraints to avoid amplifying gender bias on visual semantic role labeling and multilabel classification. In contrast to previous work, we incorporate corpuslevel constraints to facilitate dependency parser in the cross-lingual transfer setting. 6 Conclusion We propose to leverag"
D19-1103,P19-1299,0,0.028868,"f the ratio difference between source and target languages. Results show that the performance improvement is highly related to the ratio gap. The Pearson Correlation Coefficient is 0.938. The figure showing the correlation between performance gap (as per UAS) and the corpus statistics ratio gap is in the Appendix Figure 3. 5 Related Work Cross-Lingual Transfer for Parsing Many approaches have been developed to transfer a dependency parser. However, they mainly focus on better capture information from the source language(s). McDonald et al. (2011); Guo et al. (2016); T¨ackstr¨om et al. (2013); Chen et al. (2019) consider transferring a parser trained on multiple source languages. Agi´c (2017); Lin et al. (2019) selects good source languages by comparing part-of-speech tags sequences. Søgaard (2011); T¨ackstr¨om et al. (2013) chooses suitable data points from the source language. Pires et al. (2019) uses multilingual BERT to leverage language features from multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an ani"
D19-1103,P15-1119,0,0.141924,"ce language(s). McDonald et al. (2011); Guo et al. (2016); T¨ackstr¨om et al. (2013); Chen et al. (2019) consider transferring a parser trained on multiple source languages. Agi´c (2017); Lin et al. (2019) selects good source languages by comparing part-of-speech tags sequences. Søgaard (2011); T¨ackstr¨om et al. (2013) chooses suitable data points from the source language. Pires et al. (2019) uses multilingual BERT to leverage language features from multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the target languages. Constrained Inference for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train"
D19-1103,K17-1024,0,0.140706,"Missing"
D19-1103,Q16-1023,0,0.0296348,"Missing"
D19-1103,D16-1180,0,0.0291007,"Missing"
D19-1103,P15-1138,0,0.020709,"features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable structure, Rush and Collins (2012) decompose the structure and incorporate constraints on some composite tasks. To improve the performance of a model, Chang and Collins (2011); Peng et al. (2015) incorporate constraints on exact decoding tasks and inference tasks on graphical models, and Chang et al. (2013); Dalvi (2015); Martins (2015) incorporate corpus-level constraints on semi-supervised multilabel classification and coreference resolution. Zhao et al. (2017) incorporate corpus-level constraints to avoid amplifying gender bias on visual semantic role labeling and multilabel classification. In contrast to previous work, we incorporate corpuslevel constraints to facilitate dependency parser in the cross-lingual transfer setting. 6 Conclusion We propose to leverage corpus-linguistic statistics to guide the inference of cross-lingual dependency parsing. We compile these statistics into corpus-statistic constraints and design"
D19-1103,P05-1012,0,0.0716399,"rees of sentence k. In recent years, neural network approaches (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have been applied to modeling the scoring matrix S (k) and have achieved great performance in dependency parsing. From the probabilistic point of view, if we assume for different i, j, the edge probabilities P (yk (i, j) = 1|wk ) are mutually conditional independent, the probability of a whole parse tree can be written as Y P (yk |wk ) = P (yk (i, j) = 1|wk )yk (i,j) . (2) i,j Our work focuses on the graph-based dependency parser (McDonald et al., 2005) in the zero-shot single-source transfer setting as in Ahmad et al. (2019). However, the proposed algorithms can be extended other transfer settings. Given a trained model, we derive corpus-statistics constraints and apply them to correct errors caused by word order differences between the source and the target language during the inference time. Figure 1 shows an example of how constraints can influence the inference results. In this section, we first give a quick review of the graph-based parser and introduce the notations. We then discuss how to formulate corpuswise constraints based on cor"
D19-1103,P13-2017,0,0.0782545,"Missing"
D19-1103,D11-1006,0,0.640315,"swering (Joty et al., 2017), using a shared multi-lingual word embedding space (Smith et al., 2017) or delexicalization approaches (Zeman and Resnik, 2008; McDonald et al., 2013). One key challenge for cross-lingual transfer is the differences among languages; for example, languages may have different word orders. When transferring a model learned from a source language to target languages, the performance may drop significantly due to the differences. To tackle this problem, various approaches have been proposed to better capture the commonalities between the source and the target languages (McDonald et al., 2011; Guo et al., 2016; T¨ackstr¨om et al., 2013; Agi´c, 2017; Ahmad et al., 2019); however, they overlook the potential to leverage linguistic knowledge about the target language to account for the differences between the source and the target languages to facilitate the transfer. In this paper, we propose a complementary approach that studies how to leverage the linguistic knowledge about the target languages to help the transfer. Specifically, we use corpus linguistic statistics of the target languages as weak supervision signals to guide the test-time inference process when parsing with a grap"
D19-1103,P12-1066,0,0.122837,"ence for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints a"
D19-1103,D10-1120,0,0.021563,"guage. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the target languages. Constrained Inference for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-spe"
D19-1103,P15-2034,0,0.0146672,"s: a) leveraging existing linguistics resources or consulting linguists; b) leveraging a higher-resource language that is similar to the target language (e.g., Finnish and Estonian) to collect the statistics. In this paper, we explore the first option and leverage the WALS features, which provide a reference for word order typology, to estimate the ratios. Compile Constraints From WALS Features. For a particular language, once we collect the corpus-statistics of a pair of POS tags, we can formulate a binary constraint. There are different ways to estimate the corpus-statistics. For exam¨ ple, Ostling (2015) utilizes a small amount of parallel data to estimate the dominant word orders. In this paper, we simply utilize a small subset of WALS features that show the dominant order of some POS pairs (e.g. adjective and noun) in a language. They can be directly compiled into binary constraints. Similarly, we can estimate the ratio for unary constraints based on WALS features. For a particular POS tag, we choose all WALS features related to it to formulate a feature vector f . The mapping from the vector f to the unary constraint ratio r is learnable: for each language with annotated data, we can get a"
D19-1103,D15-1108,1,0.938893,"veraging existing linguistics resources or consulting linguists; b) leveraging a higher-resource language that is similar to the target language (e.g., Finnish and Estonian) to collect the statistics. In this paper, we explore the first option and leverage the WALS features, which provide a reference for word order typology, to estimate the ratios. Compile Constraints From WALS Features. For a particular language, once we collect the corpus-statistics of a pair of POS tags, we can formulate a binary constraint. There are different ways to estimate the corpus-statistics. For exam¨ ple, Ostling (2015) utilizes a small amount of parallel data to estimate the dominant word orders. In this paper, we simply utilize a small subset of WALS features that show the dominant order of some POS pairs (e.g. adjective and noun) in a language. They can be directly compiled into binary constraints. Similarly, we can estimate the ratio for unary constraints based on WALS features. For a particular POS tag, we choose all WALS features related to it to formulate a feature vector f . The mapping from the vector f to the unary constraint ratio r is learnable: for each language with annotated data, we can get a"
D19-1103,P19-1493,0,0.274139,"2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have been applied to modeling the scoring matrix S (k) and have achieved great performance in dependency parsing. From the probabilistic point of view, if we assume for different i, j, the edge probabilities P (yk (i, j) = 1|wk ) are mutually conditional independent, the probability of a whole parse tree can be written as Y P (yk |wk ) = P (yk (i, j) = 1|wk )yk (i,j) . (2) i,j Our work focuses on the graph-based dependency parser (McDonald et al., 2005) in the zero-shot single-source transfer setting as in Ahmad et al. (2019). However, the proposed algorithms can be extended other transfer settings. Given a trained model, we derive corpus-statistics constraints and apply them to correct errors caused by word order differences between the source and the target language during the inference time. Figure 1 shows an example of how constraints can influence the inference results. In this section, we first give a quick review of the graph-based parser and introduce the notations. We then discuss how to formulate corpuswise constraints based on corpus linguistic statistics for guiding the graph-based parser. 2.1 denotes"
D19-1103,P18-1142,0,0.21455,"Missing"
D19-1103,Q17-1020,0,0.162814,"ral previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in v"
D19-1103,N19-1385,0,0.162792,"source languages. Agi´c (2017); Lin et al. (2019) selects good source languages by comparing part-of-speech tags sequences. Søgaard (2011); T¨ackstr¨om et al. (2013) chooses suitable data points from the source language. Pires et al. (2019) uses multilingual BERT to leverage language features from multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the target languages. Constrained Inference for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constra"
D19-1103,W04-2401,0,0.0784584,"007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable structure, Rush and Collins (2012) decompose the structure and incorporate constraints on some composite tasks. To improve the performance of a model, Chang and Collins (2011); Peng et al. (2015) incorporate constraints on exact decoding tasks and inference tasks on graphical models, and Chang et al. (2013); Dalvi (2015); Martins (2015) incorporate corpus-level constraints on semi-supervised multilabel classification and coreference resolution. Zhao et al. (2017) incorporate cor"
D19-1103,P11-1008,0,0.218504,"Missing"
D19-1103,P11-2120,0,0.0621394,"gure showing the correlation between performance gap (as per UAS) and the corpus statistics ratio gap is in the Appendix Figure 3. 5 Related Work Cross-Lingual Transfer for Parsing Many approaches have been developed to transfer a dependency parser. However, they mainly focus on better capture information from the source language(s). McDonald et al. (2011); Guo et al. (2016); T¨ackstr¨om et al. (2013); Chen et al. (2019) consider transferring a parser trained on multiple source languages. Agi´c (2017); Lin et al. (2019) selects good source languages by comparing part-of-speech tags sequences. Søgaard (2011); T¨ackstr¨om et al. (2013) chooses suitable data points from the source language. Pires et al. (2019) uses multilingual BERT to leverage language features from multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the tar"
D19-1103,D18-1034,0,0.0268332,"e performance in a variety of tasks when sufficient training data is available. However, obtaining high-quality annotations for low-resource language tasks is challenging, and this poses great challenges to process low-resource languages. To bridge the gap, crosslingual transfer has been proposed to transfer models trained on high-resource languages (e.g., English) to low-resource languages (e.g., Tamil) to combat the resource scarcity problem. Recent studies have demonstrated successes of transferring models across languages without retraining for NLP tasks, such as named entity recognition (Xie et al., 2018), dependency parsing (Tiedemann, 2015; Agi´c et al., 2014), and question answering (Joty et al., 2017), using a shared multi-lingual word embedding space (Smith et al., 2017) or delexicalization approaches (Zeman and Resnik, 2008; McDonald et al., 2013). One key challenge for cross-lingual transfer is the differences among languages; for example, languages may have different word orders. When transferring a model learned from a source language to target languages, the performance may drop significantly due to the differences. To tackle this problem, various approaches have been proposed to bet"
D19-1103,I08-3008,0,0.151622,"rce languages. To bridge the gap, crosslingual transfer has been proposed to transfer models trained on high-resource languages (e.g., English) to low-resource languages (e.g., Tamil) to combat the resource scarcity problem. Recent studies have demonstrated successes of transferring models across languages without retraining for NLP tasks, such as named entity recognition (Xie et al., 2018), dependency parsing (Tiedemann, 2015; Agi´c et al., 2014), and question answering (Joty et al., 2017), using a shared multi-lingual word embedding space (Smith et al., 2017) or delexicalization approaches (Zeman and Resnik, 2008; McDonald et al., 2013). One key challenge for cross-lingual transfer is the differences among languages; for example, languages may have different word orders. When transferring a model learned from a source language to target languages, the performance may drop significantly due to the differences. To tackle this problem, various approaches have been proposed to better capture the commonalities between the source and the target languages (McDonald et al., 2011; Guo et al., 2016; T¨ackstr¨om et al., 2013; Agi´c, 2017; Ahmad et al., 2019); however, they overlook the potential to leverage ling"
D19-1103,D15-1213,0,0.268059,"hat adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To na"
D19-1103,D17-1323,1,0.877029,"To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable structure, Rush and Collins (2012) decompose the structure and incorporate constraints on some composite tasks. To improve the performance of a model, Chang and Collins (2011); Peng et al. (2015) incorporate constraints on exact decoding tasks and inference tasks on graphical models, and Chang et al. (2013); Dalvi (2015); Martins (2015) incorporate corpus-level constraints on semi-supervised multilabel classification and coreference resolution. Zhao et al. (2017) incorporate corpus-level constraints to avoid amplifying gender bias on visual semantic role labeling and multilabel classification. In contrast to previous work, we incorporate corpuslevel constraints to facilitate dependency parser in the cross-lingual transfer setting. 6 Conclusion We propose to leverage corpus-linguistic statistics to guide the inference of cross-lingual dependency parsing. We compile these statistics into corpus-statistic constraints and design two inference algorithms on top of a graph-based parser based on Lagrangian relaxation and posterior regularization. Experiments"
D19-1103,N13-1126,0,0.296389,"Missing"
D19-1103,W15-2137,0,0.0599466,"n sufficient training data is available. However, obtaining high-quality annotations for low-resource language tasks is challenging, and this poses great challenges to process low-resource languages. To bridge the gap, crosslingual transfer has been proposed to transfer models trained on high-resource languages (e.g., English) to low-resource languages (e.g., Tamil) to combat the resource scarcity problem. Recent studies have demonstrated successes of transferring models across languages without retraining for NLP tasks, such as named entity recognition (Xie et al., 2018), dependency parsing (Tiedemann, 2015; Agi´c et al., 2014), and question answering (Joty et al., 2017), using a shared multi-lingual word embedding space (Smith et al., 2017) or delexicalization approaches (Zeman and Resnik, 2008; McDonald et al., 2013). One key challenge for cross-lingual transfer is the differences among languages; for example, languages may have different word orders. When transferring a model learned from a source language to target languages, the performance may drop significantly due to the differences. To tackle this problem, various approaches have been proposed to better capture the commonalities between"
D19-1103,Q18-1046,0,0.0903621,"ar types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable stru"
D19-1103,D18-1163,0,0.354441,"ar types of arcs in an unsupervised setting. Naseem et al. (2010); Li et al. (2019) train a parser with constraints compiled from the frequency of particular arcs. Compared with the previous work, we focus on crosslingual transfer with word order constraints. Finally, prior studies have noticed that the word order information is significant for parsing and use it as features (Ammar et al., 2016; Naseem et al., 2012; Rasooli and Collins, 2017; Zhang and Barzilay, 2015; Dryer, 2007). T¨ackstr¨om et al. (2013) further propose to decompose these features from models for adapting target languages. Wang and Eisner (2018a) use the statistics of surface part-of-speech (POS) tags of target languages to learn the word order. Wang and Eisner (2018b) use POS tags of target languages together with a similar language, and design a stochastic permutation process to synthetic the word order. However, none of them consider using the word order features as constraints. Incorporating Constraints In NLP Tasks Constraints are widely incorporated in variety of NLP tasks. To name a few, Roth and Yih (2004) propose to formulate constrained inferences in NLP as integer linear programming problems. To solve the intractable stru"
D19-1103,P16-1218,0,0.0645363,"Missing"
D19-1103,W14-1613,0,0.0211856,"rmation from the source language(s). McDonald et al. (2011); Guo et al. (2016); T¨ackstr¨om et al. (2013); Chen et al. (2019) consider transferring a parser trained on multiple source languages. Agi´c (2017); Lin et al. (2019) selects good source languages by comparing part-of-speech tags sequences. Søgaard (2011); T¨ackstr¨om et al. (2013) chooses suitable data points from the source language. Pires et al. (2019) uses multilingual BERT to leverage language features from multiple languages. Ahmad et al. (2019) design an order-free model to take out the order features from the source language. Xiao and Guo (2014); Guo et al. (2015) learn an alignment from source words to target words. Ponti et al. (2018) learn an anisomorphism from the source parsing tree to target. Rasooli and Collins (2019) reorder the source data before training. In contrast, we focus on incorporating linguistic properties in the target languages. Constrained Inference for Parsing Several previous studies show that adding constraints in inference time improves the performance of models. Grave and Elhadad (2015) consider incorporating constraints to promote popular types of arcs in an unsupervised setting. Naseem et al. (2010); Li e"
D19-1339,N18-2003,1,0.848017,"contexts Biases can occur in different textual contexts, some biases manifesting more subtly than others. In this work, we analyze biases that occur in two contexts: those that deal with descriptive levels of respect towards a demographic and those that deal with the different occupations of a demographic. The first four examples in Table 1 are generated text with occupation contexts, and the latter two are generated text with respect contexts. We analyze these two bias contexts because the occupation context has been well-studied in other tasks (Bolukbasi et al., 2016; Rudinger et al., 2018; Zhao et al., 2018; Zhou et al., 2019), and the more descriptive language in respect contexts are a good contrast for the more subtle occupation contexts. For each context, we analyze generated sentences that have been conditioned on content relating to the bias context. Demographics In the process of examining biases in language generation, we need to compare the magnitude of biases across different demographics. Here, we use the term “demographic” to refer to a group of people with the same gender, race, or sexual orientation. Specifically, we examine the groups female and male for gender, Black and White for"
D19-1339,D19-1531,1,0.820085,"occur in different textual contexts, some biases manifesting more subtly than others. In this work, we analyze biases that occur in two contexts: those that deal with descriptive levels of respect towards a demographic and those that deal with the different occupations of a demographic. The first four examples in Table 1 are generated text with occupation contexts, and the latter two are generated text with respect contexts. We analyze these two bias contexts because the occupation context has been well-studied in other tasks (Bolukbasi et al., 2016; Rudinger et al., 2018; Zhao et al., 2018; Zhou et al., 2019), and the more descriptive language in respect contexts are a good contrast for the more subtle occupation contexts. For each context, we analyze generated sentences that have been conditioned on content relating to the bias context. Demographics In the process of examining biases in language generation, we need to compare the magnitude of biases across different demographics. Here, we use the term “demographic” to refer to a group of people with the same gender, race, or sexual orientation. Specifically, we examine the groups female and male for gender, Black and White for race, and gay and s"
D19-1339,S18-2005,0,0.193017,"Missing"
D19-1339,P09-5002,0,0.0110727,"y to find his own voice and to speak clearly. Table 1: Examples of text continuations generated from OpenAI’s medium-sized GPT-2 model, given different prompts Introduction Recent works in machine translation (Prates et al., 2018) and dialogue systems (Henderson et al., 2018) have brought to attention the perpetuation of biases in natural language generation (NLG) systems. In this work, we present a systematic study of biases in open-domain NLG by examining language models. Language models are a fundamental component of NLG that are widely used in downstream tasks such as machine translation (Koehn, 2009), dialogue generation (Serban et al., 2016), and story generation (Yao et al., 2019); as such, biases propagated through the language models will have a profound impact on a variety of other NLG tasks. More generally, NLG systems are at the forefront of developments in humancomputer interaction, and systematic biases in language models have a direct impact on society and broader AI applications. A text is positively or negatively inclined towards a demographic if the text causes the specific demographic to be positively or negatively perceived. When NLP models systematically produce text with"
D19-1339,W16-0429,0,0.0302826,"t in text, we manually construct five placeholder prefix templates for each bias context (Table 2), where the demographic mention in all templates is the placeholder XYZ.4 For each &lt;bias context placeholder prefix template, demographic&gt; pair, we fill in the template with the appropriate demographic (“XYZ worked as” becomes “The woman worked as”), forming complete prefix templates to prompt language generation. Annotation task To select text for annotation, we sample equally from text generated from the different prefix templates. The sentiment and regard annotation guidelines are adapted from Mohammad (2016)’s sentiment annotation guidelines. There are six categories each for sentiment and regard, and both metrics have positive, negative, and neutral categories.5 1. For each &lt;bias context placeholder prefix template, demographic&gt; pair, we generate a complete prefix template, for a total of 60 unique templates. We then use GPT-2 to generate 100 samples per complete prefix template. 2. Each generated sample is truncated so that at most one sentence is in the sample. 3. We use VADER to predict a sentiment score for each generated sample, and for each prefix template, we randomly choose three posgard"
D19-1339,N18-2002,0,0.157462,"Missing"
D19-1625,N16-1071,0,0.0600412,"Missing"
D19-1625,P17-1152,0,0.0299558,"ams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets"
D19-1625,W15-0107,0,0.0304347,"18) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the back of the president’s quaint black armchair there was emblazoned a half-sun, brilliant with its gilded rays. The armchair’s arm is black. vanity mirror white Impossible bench support wooden Unrelated A door to a bathroom half open and a white vanity. In front of me about five feet distance, stood a wooden bench. The vanity’s mirror is white. The bench’s support is wooden. Table 1: Example triples and retrieved premise sentences, with labels"
D19-1625,P17-1025,0,0.0199664,"h provides a focused evaluation, based on a specific task in commonsense reasoning. Gathering and validating data from crowd workers, we evaluate a number of approaches to performing these inferences, a three-way lexical entailment problem. We find that simple word embedding-based models perform adequately, but beneath humans, on this task, with recent large LM approaches (Devlin et al., 2018; Radford et al., 2018) providing only slight improvement over the purely lexical approach. 2 Related Work Other researchers have constructed datasets investigating similar ideas in commonsense reasoning. Forbes and Choi (2017) develop a dataset and methods for inferring physical commonsense knowledge from verb usage, showing it is possible to learn the physical implications of unseen verbs from a small seed set. Zhang et al. (2017) create a large dataset for general commonsense inference in the form of premise-hypothesis pairs, equipped with ordinal labels ranging from “impossible” to “very likely”. We adopt much of their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task invo"
D19-1625,P18-2103,0,0.0129536,"nsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar s"
D19-1625,S13-1035,0,0.0354196,"and retrieved premise sentences, with labels, used for training word embedding-based models and language model fine-tuning. ties, and we expect an approach that leverages this will help solve this task, but we leave it to future work. 3 Candidate collection We seek to annotate examples of (whole, part, adjective) triples with answers to the question: “Does an hadjectivei hwholei have an hadjectivei hparti?” As a major part of our contribution, we provide an annotated dataset that is visually grounded, with relations mined from Visual Genome (Krishna et al., 2017) and Google Syntactic N-grams (Goldberg and Orwant, 2013). We provide an overview here, with details in Appendix A. 3.1 Part–whole relations Visual Genome (VG) is a large dataset of images annotated with objects, their attributes, and the relations between them. We start by considering all relationships in the VG dataset where the predicate is an underspecified has relation. We count the number of images in which a pair of objects appear in a has relation, and keep only those pairs appearing in at least three distinct images. 3.2 Adjectives We gather adjectives from both Google Syntactic N-grams and VG. From Syntactic N-grams, we count the occurrenc"
D19-1625,S18-1117,0,0.0272239,"ting similar ideas in commonsense reasoning. Forbes and Choi (2017) develop a dataset and methods for inferring physical commonsense knowledge from verb usage, showing it is possible to learn the physical implications of unseen verbs from a small seed set. Zhang et al. (2017) create a large dataset for general commonsense inference in the form of premise-hypothesis pairs, equipped with ordinal labels ranging from “impossible” to “very likely”. We adopt much of their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibi"
D19-1625,W17-2810,0,0.0165701,"lar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the bac"
D19-1625,N16-1098,0,0.0313962,"his knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Ca"
D19-1625,S18-1119,0,0.0250216,"e, *Research conducted while author was at USC/ISI the car may now be fast. This knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making"
D19-1625,D16-1244,0,0.089233,"Missing"
D19-1625,P16-1204,0,0.0306828,"et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar systematic and social bias in SNLI, attributing it to the fact that hypothesis sentences were written by crowd workers. T"
D19-1625,D14-1162,0,0.0844078,"abel distribution is shown in Table 2. The dataset has 728 unique part nouns, 873 unique whole nouns, and 553 unique adjectives. 5 Inference baselines We now describe several basic approaches for solving these commonsense inference problems, which we intend as a baseline to be built upon by Word embedding models We approach the problem as categorical classification and train a multi-layer perceptron (MLP) model to classify inputs consisting of word embeddings for the whole, part, and adjective words. The MLP takes as input the concatenation of these three word embeddings, obtained from GloVe (Pennington et al., 2014), and applies a single hidden layer with ReLU activation before the final softmax layer which predicts the class label. 5.2 Adjective projection as NLI As we want to evaluate strong yet simple preexisting language understanding models on this task, we now describe a method for obtaining the direct prediction described above via conversion to a form suitable for inference in the style of the SNLI and MNLI datasets (Bowman et al., 2015; Williams et al., 2018), which consist of premise and hypothesis sentence pairs. We first form simple hypothesis sentences from the tuples using the fixed templat"
D19-1625,N18-1202,0,0.018836,"ls with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al."
D19-1625,P18-2101,0,0.0229796,"nd CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fagarasan et al., 2015) utilizes embeddings to predict real-world perceptual proper6053 Whole Part Adjective Label NLI premise NLI hypothesis armchair arm black Probably On the back of the president’s quaint black armchair there was emblazoned a half-sun, brilliant with its gilded rays. The armchair’s arm is black. vanity mirror white Impossible bench support wooden Unrelated A door to a bathroom half open and a white vanity. In front of me about f"
D19-1625,W17-1609,0,0.0462981,"Missing"
D19-1625,K17-1004,0,0.0175119,"s et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requiring only reasoning over adjective-noun relations. Relatedly, in the shared task to predict sentence endings of ROCStories, Schwartz et al. (2017) show that by incorporating style features, with only the answer choices as input, it is possible to reach near stateof-the-art performance. These results point to implicit bias baked into the data sets. Rudinger et al. (2017) demonstrate similar systematic and social bias in SNLI, attributing it to the fact that hypothesis sentences were written by crowd workers. The SWAG data set was specif6052 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6052–6058, c Hong Kong, China, N"
D19-1625,N18-2049,0,0.119218,"their methodology but for a targeted subset of commonsense reasoning. The SemEval 2018 Task 10 on Capturing Discriminative Attributes (Krebs et al., 2018) describes a similar lexical reasoning task involving triplets of words, though it focuses on finding attributes that distinguish two concepts, while in our work the adjective may well apply to both part and whole. Past work has also evaluated commonsense capabilities in neural models. Pavlick and CallisonBurch (2016) investigate the related problem of entailment in adjective-nouns, and show surprising negative results for neural NLI models. Wang et al. (2018) showed that models based on distributional semantics without explicit external knowledge perform poorly at predicting physical plausibility of actions. Lucy and Gauthier (2017) investigate perceptual properties of distributional embeddings and suggest that part–whole properties like has legs are well encoded by embeddings. This may help explain why the simple word-based MLP models perform well without other sources of context. Rei et al. (2018) introduce an effective neural architecture for learning word-embedding based models for graded lexical entailment. Prior work (Bulat et al., 2016; Fag"
D19-1625,N18-1101,0,0.249084,"dshield, the car remains slow, whereas if the new part is an engine, *Research conducted while author was at USC/ISI the car may now be fast. This knowledge may also help a visual agent reason about unseen objects: it knows a brick house does not have a brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017"
D19-1625,D18-1009,0,0.0129478,"brick door without needing to see the door. The past few years have seen a raft of data sets intended to test our ability to construct models with an understanding of commonsense knowledge. Standout examples are the Stanford Natural Language Inference (SNLI) and related MultiGenre Natural Language Inference (MNLI) corpora (Bowman et al., 2015; Williams et al., 2018), the SemEval-2018 commonsense shared task (Ostermann et al., 2018), the Rochester Story Completion (ROCStories) corpus (Mostafazadeh et al., 2016), and the Situations with Adversarial Generations (SWAG) grounded inference corpus (Zellers et al., 2018). After their release, very large language models (LMs) were able to reach or surpass human-level performance on SNLI (Peters et al., 2018) and SWAG (Devlin et al., 2018). However, researchers have found inadequacies in these datasets and the models trained on them. Despite the strong performance of recent systems on SNLI (e.g., Chen et al., 2017; Parikh et al., 2016), Glockner et al. (2018) show that by making trivial changes to the test set, these methods suffered. Further, Pavlick and Callison-Burch (2016) show that state-of-the-art models for natural language inference fail on a task requi"
D19-1672,D16-1153,0,0.0186096,"mising direction to further improve the model performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi"
D19-1672,Q17-1010,0,0.031467,"a and replace numbers and URLs as “num” and “url” respectively. We use the training data to train our model, the development sets to select the best well-trained model, and the test sets to evaluate performance, where the training data is English and the development and test sets are from the target language. 3.2 Experimental Settings Bilingual Embedding. We use the 300dimensional bilingual word embeddings pretrained by MUSE (Conneau et al., 2018) and 300dimensional randomly initialized character embeddings. Specifically, we first collect monolingual pre-trained word embeddings from fastText (Bojanowski et al., 2017). We then align the embeddings using the MUSE supervised model. We normalize the inputs to MUSE and keep the other hyperparameters as defaults. We merge the aligned word embeddings and remove duplicates. Updating the embedding during training time will change the vector space of partial bilingual embeddings and therefore break the vector alignment. Thus, we freeze the embedding weights during the training step. For OOV (out-of-vocabulary) words, we use a randomly initialized 300 dimension vector within [−0.1, 0.1]. Reverse Translation. We follow the general translation step in Section 2. Speci"
D19-1672,P19-1299,0,0.0136879,"a (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between langua"
D19-1672,L16-1720,0,0.0134871,"ns. to the performance, the entities with two tokens achieve the higher scores, while the entities with more than 2 tokens decrease significantly ranging from 12.59 to 25.04 absolute percentages of F1 scores. The observation indicates that entities longer than two tokens are more difficult to infer. This might encourage us to balance the weight of long entities in our current evaluation method which ignores entity length when datasets have high volumes of long entities. 4 Case Study: Bengali Models CTNER CTNER+ CTNER* CTNER*+ Our Model F1 score 30.47 31.70 46.28 45.70 34.29 and online forums (Cieri et al., 2016). By contrast, the only data source of CoNLL data is from news articles (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). While transfer can help provide universal context clue information across languages there is no substitute for a resource of actual names. Language Spanish Dutch German Bengali The previous cross-lingual settings were only for European languages, which share similar alphabets. However, many languages use non-Latin orthography. In this work, we present a case study on Bengali, which does not use a Latin alphabet. We compare our proposed method with the translation"
D19-1672,I17-2016,0,0.0284126,"cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase"
D19-1672,P18-4003,1,0.876004,"via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between languages that otherwise have different character sets, such as English and Bengali (Hermjakob et al., 2018). Multi-source learning is effective when multiple or similar language resources are available by learning shareable knowledge. For example, training a model on both English and Hindi can significantly improve the model performance on Bengali than only using English (Mayhew et al., 2017). However, there is little prior work with detailed analysis of how cross-lingual NER models transfer knowledge between languages on different levels. In this paper, we focus on a single-source zeroshot transfer setting where we transfer from English to target languages that have no annotated data. In our setti"
D19-1672,N04-1036,0,0.0437452,"Missing"
D19-1672,N16-1030,0,0.0667801,"ation for Computational Linguistics alphabets. Next, we conduct qualitative analyses to answer the following questions on how the model transfers knowledge under the cross-lingual settings: 1) does the source language syntax matter? 2) how do word and character embeddings affect the model transfer? We analyze how F1 scores differ across different entity lengths. Finally, we conduct a case study on Bengali. 2 Model NER models take a sequence of tokens as input and predict a sequence of labels such as person (PER) or location (LOC). In this paper, we adopt the neural architecture NER model from Lample et al. (2016); Ma and Hovy (2016). The model first combines pre-trained word and character embeddings as token representations, then feeds the representations to one layer of a Bidirectional Long Term Short Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997), and finally predicts the outputs via a Conditional Random Field (CRF). We show the model architecture on left of Figure 1. However, languages express the same named entity in different words and characters. To bridge the barriers, we combine three strategies: bilingual embedding, reverse translation, and transliteration. Bilingual Embedding. Word embe"
D19-1672,P18-1074,0,0.0121823,"r languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level kn"
D19-1672,P16-1101,0,0.0128327,"l Linguistics alphabets. Next, we conduct qualitative analyses to answer the following questions on how the model transfers knowledge under the cross-lingual settings: 1) does the source language syntax matter? 2) how do word and character embeddings affect the model transfer? We analyze how F1 scores differ across different entity lengths. Finally, we conduct a case study on Bengali. 2 Model NER models take a sequence of tokens as input and predict a sequence of labels such as person (PER) or location (LOC). In this paper, we adopt the neural architecture NER model from Lample et al. (2016); Ma and Hovy (2016). The model first combines pre-trained word and character embeddings as token representations, then feeds the representations to one layer of a Bidirectional Long Term Short Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997), and finally predicts the outputs via a Conditional Random Field (CRF). We show the model architecture on left of Figure 1. However, languages express the same named entity in different words and characters. To bridge the barriers, we combine three strategies: bilingual embedding, reverse translation, and transliteration. Bilingual Embedding. Word embeddings are usually t"
D19-1672,D17-1269,0,0.0350975,"Missing"
D19-1672,P17-1135,0,0.0290309,"Missing"
D19-1672,P17-1178,1,0.851071,"el performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be ap"
D19-1672,D15-1064,1,0.821467,"we replace English words with target language words if the translation pairs exist in the bilingual dictionaries (Rolston and Kirchhoff, 2016). If a translation pair does not exist for a word, we keep the word unchanged in the training data. We use the pre-trained bilingual embedding to help select the best choice of polysemy. Model parameters. We use 300-dimension hidden states for both character and token level Bi-LSTMs. To prevent overfitting, we apply dropout (Srivastava et al., 2014) with a rate of 0.5 on outputs of the two Bi-LSTMs. We follow the Conditional Random Field (CRF) setup of Peng and Dredze (2015). We then randomly initialize the weights of the layers within [−0.1, 0.1]. We train the model for 200 epochs and optimize the parameters by Stochastic Gradient Descent (SGD) with momentum, gradient clipping, and learning rate decay. We set the learning rate (lr) and the decay rate (dr) as 0.01 and 0.05 respectively. We uplr date the learning rate by (1+(n+1)∗dr) after epoch n. We clip the gradients to the range of [-5.0, 5.0]. We measure performance by F1 score. 3.3 Baselines In this study, we compare our proposed method with three close works under the cross-lingual settings. We compare our"
D19-1672,P19-1015,0,0.0342167,", 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual dictionaries. Bilingual embeddings are a form of bilingual dictionaries; they constitute a projection from one pre-trained language representation into the same vector space as the other such that words with the same meaning have similar representations (Conneau et al., 2018). Phonetic alphabets enable different languages to share the same pronunciation characters so that the character-level knowledge is transferable between languages that otherwise hav"
D19-1672,W09-1119,0,0.0821579,"different European languages, Spanish, Dutch, English, and German. The data contains four types of named entities: person (PER), organization (ORG), location (LOC) and MISC. We use Bengali language data from LDC2015E13 (V2.1). To be consistent, we only keep the PER, ORG and LOC tags and ignore MISC tags from the Bengali corpus. In this study, we choose the BIOSE tag schema instead of standard BIO, where the B, I, O, S, E refer to the beginning, inside, outside, single and end of an entity, respectively. Previous work shows that BIOSE can learn a more expressive model than the BIO schema does (Ratinov and Roth, 2009). We lowercase the data and replace numbers and URLs as “num” and “url” respectively. We use the training data to train our model, the development sets to select the best well-trained model, and the test sets to evaluate performance, where the training data is English and the development and test sets are from the target language. 3.2 Experimental Settings Bilingual Embedding. We use the 300dimensional bilingual word embeddings pretrained by MUSE (Conneau et al., 2018) and 300dimensional randomly initialized character embeddings. Specifically, we first collect monolingual pre-trained word embe"
D19-1672,P19-1336,0,0.0467955,"Missing"
D19-1672,W02-2024,0,0.484143,"may not share the same characters, but some named entities in the multilingual corpora share phonetic similarities (Huang et al., 2004). To map multilingual corpora into the same character space and connect the phonetic similarity between entities, we employ Uroman1 (Hermjakob et al., 2018), which transliterates any language into something roughly pronounceable English spellings, while keeps the English words unchanged. We show some examples of Uroman in Table 1. 3 Experiments In this study, we first evaluate our proposed crosslingual method on the CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). We then conduct ablation studies to examine how the model learns transferable knowledge under the crosslingual settings. Finally, we conduct a case study on Bengali, a low resource language. 6396 1 https://www.isi.edu/˜ulf/uroman.html B-ORG I-ORG E-ORG O S-PER Hidden States Bilingual Embedding …. …. …. …. CRF …. Bi-LSTM …… g r u Translate …… n gruen Uroman grün Tokens Representations Character Bi-LSTM green Token Figure 1: Architecture of our proposed model. Each word is translated from the source. Each token representation contains two concatenated part"
D19-1672,W03-0419,0,0.483696,"Missing"
D19-1672,K16-1022,0,0.0665748,"her improve the model performances. Our results can shed light on future research for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Tra"
D19-1672,I17-2065,1,0.855877,"arch for improving cross-lingual NER. 1 Introduction Named Entity Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation"
D19-1672,D18-1034,0,0.06626,"ty Recognition (NER) is an important NLP task that identifies the boundary and type of named entities (e.g., person, organization, location) in texts. However, for some languages, it is hard to obtain enough labeled data to build a fully supervised learning model. Cross-lingual transfer models, which train on high-resource languages and transfer to target languages are a promising direction for languages with little annotated data (Bharadwaj et al., 2016; Tsai et al., 2016; Pan et al., 2017; Yang et al., 2017; Mayhew et al., 2017; Wang et al., 2017; Cotterell and Duh, 2017; Feng et al., 2018; Xie et al., 2018; Zhou et al., 2019). ∗ The work was done when the first author worked as an intern at USC ISI. Cross-lingual NER models learn transferable knowledge mainly from four sources: translation (Mayhew et al., 2017; Feng et al., 2018), bilingual embeddings (Ni et al., 2017; Xie et al., 2018), phonetic alphabets (Bharadwaj et al., 2016) and multi-source learning (Mayhew et al., 2017; Lin et al., 2018; Zhou et al., 2019; Chen et al., 2019; Rahimi et al., 2019). Translation can be applied at either the sentence level via machine translation or at the word and phrase level via application of bilingual d"
I17-2065,P16-1101,0,0.110712,"ay be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily help out-of-the-box. This corroborates results in the monolingual setting, where it is widely recognized that training task-specific embeddings is helpful for the downstream tasks like NER (Peng and Dredze, 2015; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011;"
I17-2065,N13-1006,0,0.022686,"Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing,"
I17-2065,D11-1006,0,0.0757057,"Missing"
I17-2065,P11-1061,0,0.0382041,"5; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Lan"
I17-2065,P05-1045,0,0.0214667,"ent Tuple #Sentence #Token (6) O Θ = {V0 }, where V0 is the context embedding with size d × v. In the implementation, we use negative sampling to save the computation, since we desire using a large vocabulary to handle as many words as possible for cross-lingual transfer. def 2.3 Data We use the EN-ZH portion of the Wikipedia Comparable Corpora4 . For experiment purposes, we sampled 19K document pairs5 as our comparable corpora C. The NER labeled data on English (S) is obtained by collecting the first paragraph of each English document in C as X (S) , and labeling it with Stanford NER tagger (Finkel et al., 2005) to generate Y (S) .6 For the NER test data in Chinese (T ), we separately sampled 1K documents and collected the first sentence as X (T ) . We ran automatic word segmentation7 and manually labeled X (T ) to generate Y (T ) . The English side of these 1K tuple is treated as held-out data for tuning NER hyperparameters, and is labeled with the same Stanford NER tagger. We use the BIO tagging scheme for 3 basic named-entity types (“LOC” for location, “ORG” for organization and “PER” for person), so the output space is 7 tags. The data statistics are shown in Table 1. The size of BWE’s is about 1"
I17-2065,D16-1135,0,0.0207661,"-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for balancing two Ln and Lm s"
I17-2065,P17-1178,0,0.0456547,"h can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for balancing two Ln and Lm should presumably be"
I17-2065,N15-1157,0,0.110401,"ord embeddings while optimizing a NER objective. This creates word embeddings that are both shared between languages and fine-tuned for the NER task. As a proof of concept, we demonstrate this model on English-toChinese transfer using Wikipedia. … ct 1 (T ) , cK NER Training Sentence y1 , y2 , · · · , yl x 1 , x2 , · · · , x l Figure 1: Our multi-task framework, which trains bilingual word embeddings from comparable corpora while optimizing an NER objective on the high-resource language. The NER part of the model is then tested on a low-resource language. Blunsom, 2014; Vulic and Moens, 2015; Gouws and Søgaard, 2015). A comparable corpus is a collection of document pairs written in different languages but talking about the same topic (e.g. interconnected Wikipedia articles). The advantage of comparable corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily h"
I17-2065,D15-1064,1,0.751953,"corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained BWE’s do not necessarily help out-of-the-box. This corroborates results in the monolingual setting, where it is widely recognized that training task-specific embeddings is helpful for the downstream tasks like NER (Peng and Dredze, 2015; Ma and Hovy, 2016). Second, we propose a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; D"
I17-2065,P08-1001,0,0.037514,"when we replaced the uni4 Conclusion & Future Work We show how a multi-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the Eng"
I17-2065,N16-1030,0,0.0586647,"n different languages into a single document (where S and T words are interspersed), then apply a standard monolingual word embedding algorithm. Evaluation : At test time, given X (T ) – the raw sentences of T , we evaluate the F1 score of Y¯ (T ) predicted by the trained model {V∗ , Λ∗ } against the true label Y (T ) . Note that this model is trained 1 Chinese can be considered a high-resource language for NER, but we use it as a proof-of-concept and do not use any existing Chinese resources. 2 Same word in different languages is treated separately. 3 Besides unigram, we also tried LSTM+CRF (Lample et al., 2016) for longer context. Despite good results in monolingual NER, it did poorly in our cross-lingual experiments. 384 Algorithm 1 Stochastic merging of two documents, where len(c) returns the number of tokens of document c, c[i] is the ith token of document c. Inspired by Lample et al. (2016), for both approaches, words with frequency 1 in the NER data are replaced by OOV with probability 0.5 to so that embedding OOV could be optimized. Input: Comparable Document: c(S) , c(T ) Output: Pseudo-bilingual Document: c 1: c ← []; i1 ← 0; i2 ← 0 len(c(S) ) len(c(S) )+len(c(T ) ) 3: while i1 < len(c(S) )"
I17-2065,R11-1015,0,0.0372364,"Missing"
I17-2065,N12-1052,0,0.0783082,"Missing"
I17-2065,K16-1022,0,0.177793,"We show how a multi-task learning approach can help adapt bilingual word embeddings (BWE’s) to improve cross-lingual transfer. Joint training of BWE’s encourages the BWE’s to be taskspecific, and outperforms the baseline of using pre-trained BWE’s. We showed promising results on the challenging task of cross-lingual NER on comparable corpora, where the target language has no labels. Future work will aim to improve the absolute F1 scores by combining limited labels in the low-resource languages, via exploiting document structure in Wikipedia (Richman and Schone, 2008; Steinberger et al., 2011; Tsai et al., 2016; Ni and Florian, 2016; Pan et al., 2017). While we only focus on the most difficult case where the source language and target languages are not in the same family, and a bilingual dictionary is not available in this paper, it is interesting to study how this technique could be applied when the different levels of supervision are available on various language pairs in the future. negative sampling with 25 samples and subsampling rate of value 1e-4. The dropout rate of 0.3 is decided by the best F1 score of the language-specific NER tagger on the English held-out data. The coefficient α for bal"
I17-2065,P15-2118,0,0.37076,"ntly trains bilingual word embeddings while optimizing a NER objective. This creates word embeddings that are both shared between languages and fine-tuned for the NER task. As a proof of concept, we demonstrate this model on English-toChinese transfer using Wikipedia. … ct 1 (T ) , cK NER Training Sentence y1 , y2 , · · · , yl x 1 , x2 , · · · , x l Figure 1: Our multi-task framework, which trains bilingual word embeddings from comparable corpora while optimizing an NER objective on the high-resource language. The NER part of the model is then tested on a low-resource language. Blunsom, 2014; Vulic and Moens, 2015; Gouws and Søgaard, 2015). A comparable corpus is a collection of document pairs written in different languages but talking about the same topic (e.g. interconnected Wikipedia articles). The advantage of comparable corpora is that they may be more easily acquired in the language and domain of interest. However, cross-lingual transfer on comparable corpora is more difficult than on parallel corpora, due to the difficulty in finding high-quality word translation equivalences. Our contributions are two-fold: First, we investigate cross-lingual transfer on an NER task, and found that pre-trained"
I17-2065,W14-1613,0,0.0209244,"ansfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP corpus, training a NER model from labeled English articles (high-resource) and testing it on Chinese articles (low-resource)1 . The chall"
I17-2065,C16-1045,0,0.071938,"e a multi-task learning framework that utilizes comparable corpora to jointly train BWE’s and the downstream NER task (Figure 1). We experimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taip"
I17-2065,P15-2064,0,0.0403858,"xperimented with a Wikipedia Introduction Cross-lingual transfer is an important technique for building natural language processing (NLP) systems for low-resource languages, where labeled examples are scarce. The main idea is to transfer labels or models from high-resource languages. Representative techniques include (a) projecting labels (or information derived from labels) across parallel corpora (Yarowsky et al., 2011; Das and Petrov, 2011; Che et al., 2013; Zhang et al., 2016), and (b) training universal models using unlexicalized features (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Zirikly and Hagiwara, 2015) or bilingual word embeddings (Xiao and Guo, 2014; Gouws and Søgaard, 2015). Here, we focus on the bilingual word embedding (BWE) approach. In particular, we are interested in leveraging recent advances in learning BWE from comparable corpora (Hermann and ∗ This research was majorly conducted when the author was at Johns Hopkins University. 383 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 383–388, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP corpus, training a NER model from labeled English articles (high-resource) and testing i"
I17-2065,H01-1035,0,\N,Missing
K19-1035,Q17-1010,0,0.0595663,"er, we adopt both cross-entropy objectives for these two types of parsers as in (Dozat and Manning, 2017; Ma et al., 2018). The encoder and the decoder are jointly trained to optimize the probability of the dependency trees (y) given sentences (x): Lp = − log p(y|x). The probability of a tree can be further factorized into the products of the probabilities of each token’s (m) head decision (h(m)) for the graphLd = Ex∼X a [D(G(x)] − Ex∼X b [D(G(x)], 374 Language Families Afro-Asiatic Austronesian IE.Baltic IE.Germanic IE.Indic IE.Latin IE.Romance IE.Slavic Korean Uralic Languages et al., 2017; Bojanowski et al., 2017) with 300 dimensionss or contextualized word representations provided by multilingual BERT6 (Devlin et al., 2019) with 768 dimensions as the word representations. In addition, we use the Gold universal POS tags to form the input representations.7 We freeze the word representations during training to avoid the risk of disarranging the multilingual representation alignments. We select six auxiliary languages8 (French, Portuguese, Spanish, Russian, German, and Latin) for unsupervised language adaptation via adversarial training. We tune the scaling parameter λ in the range of [0.1, 0.01, 0.001] o"
K19-1035,N18-1111,0,0.0311152,"ages with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the language invariant repre"
K19-1035,Q18-1039,0,0.0211044,"Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the language invariant representations learned by previously proposed methods can perform well on a Table 7: Average cross-lingual performance difference between the SelfAtt-Graph parser trained on the source (en) and an auxiliary (x) language and the SelfAttGraph pa"
K19-1035,D11-1005,0,0.0665757,"Missing"
K19-1035,N19-1423,0,0.473239,"2018). The encoder and the decoder are jointly trained to optimize the probability of the dependency trees (y) given sentences (x): Lp = − log p(y|x). The probability of a tree can be further factorized into the products of the probabilities of each token’s (m) head decision (h(m)) for the graphLd = Ex∼X a [D(G(x)] − Ex∼X b [D(G(x)], 374 Language Families Afro-Asiatic Austronesian IE.Baltic IE.Germanic IE.Indic IE.Latin IE.Romance IE.Slavic Korean Uralic Languages et al., 2017; Bojanowski et al., 2017) with 300 dimensionss or contextualized word representations provided by multilingual BERT6 (Devlin et al., 2019) with 768 dimensions as the word representations. In addition, we use the Gold universal POS tags to form the input representations.7 We freeze the word representations during training to avoid the risk of disarranging the multilingual representation alignments. We select six auxiliary languages8 (French, Portuguese, Spanish, Russian, German, and Latin) for unsupervised language adaptation via adversarial training. We tune the scaling parameter λ in the range of [0.1, 0.01, 0.001] on the source language validation set and report the test performance with the best value. For gradient reversal ("
K19-1035,W14-4200,0,0.328542,"Missing"
K19-1035,K15-1012,0,0.0427353,"the auxiliary language has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0.82/0.66 0.82/0.98 bg 0.79/0.47 0.26/-0.07 0.49/0.41 ru 0.96/0.85 0.39/0.06 1.07/1.11 c"
K19-1035,N19-1253,1,0.781609,"ts and analyses are conducted to address the following research questions: • Does encoder trained with adversarial training generate language-agnostic representations? • Does language-agnostic representations improve cross-language transfer? Figure 1: An overview of our experimental model consists of three basic components: (1) Encoder, (2) (Parsing) Decoder, and (3) (Language) Classifier. We also show how parsing and adversarial losses (Lp and Ld ) are back propagated for parameter updates. Experimental results show that the proposed approach consistently outperform a strong baseline parser (Ahmad et al., 2019), with a significant margin in two family of languages. In addition, we conduct experiments to consolidate our findings with different types of input representations and encoders. Our experiment code is publicly available to facilitate future research.1 2 which are fed to the decoder and the classifier to predict the dependency structure and the language identity (id) of that sentence. The encoder and the decoder jointly form the parsing model and we consider two alternatives2 from (Ahmad et al., 2019): “SelfAtt-Graph” and “RNN-Stack”. The “SelfAtt-Graph” parser consists of a modified self-att"
K19-1035,P19-1070,0,0.0522659,"Missing"
K19-1035,D17-1230,0,0.0348251,"of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the language invariant representations learned by previously propos"
K19-1035,D17-1256,0,0.0294794,"et al., 2014; Goodfellow et al., 2015) was initially introduced in computer vision for image classification and received enormous success in improving model’s robustness on input images with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question s"
K19-1035,P17-1001,0,0.0286943,"stness on input images with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the"
K19-1035,P18-1130,1,0.781307,"facilitate future research.1 2 which are fed to the decoder and the classifier to predict the dependency structure and the language identity (id) of that sentence. The encoder and the decoder jointly form the parsing model and we consider two alternatives2 from (Ahmad et al., 2019): “SelfAtt-Graph” and “RNN-Stack”. The “SelfAtt-Graph” parser consists of a modified self-attentional encoder (Shaw et al., 2018) and a graph-based deep bi-affine decoder (Dozat and Manning, 2017), while the “RNN-Stack” parser is composed of a Recurrent Neural Network (RNN) based encoder and a stack-pointer decoder (Ma et al., 2018). We stack a classifier (a linear classifier or a multi-layer Perceptron (MLP)) on top of the encoder to perform the language identification task. The identification task can be framed as either a word- or sentence-level classification task. For the sentence-level classification, we apply average pooling3 on the contextual word representations generated by the encoder to form a fixed-length representation of the input sequence, which is fed to the classifier. For the word-level classification, we perform language classification for each token individually. Training Language-agnostic Encoders W"
K19-1035,P15-1119,0,0.0541009,"o them12 or from the same family. Intuitively, we would assume when the auxiliary language has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0.82/0.66 0.82/0.98"
K19-1035,P14-1126,1,0.860598,"ining significantly improves the overall transfer performances under several different settings. We conduct a careful analysis to evaluate the language-agnostic representations resulted from adversarial training. 1 Introduction Cross-lingual transfer, where a model learned from one language is transferred to another, has become an important technique to improve the quality and coverage of natural language processing (NLP) tools for languages in the world. This technique has been widely applied in many applications, including part-of-speech (POS) tagging (Kim et al., 2017), dependency parsing (Ma and Xia, 2014), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), coreference resolution (Kundu et al., 2018), and question answering (Joty et al., 2017). Noteworthy improvements are achieved on low resource language applications due to cross-lingual transfer learning. 372 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 372–382 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics To verify the proposed approach, we conduct experiments on neural dependency parsers trained on English (source language) and dire"
K19-1035,P19-1311,1,0.833008,"l transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0.82/0.66 0.82/0.98 bg 0.79/0.47 0.26/-0.07 0.49/0.41 ru 0.96/0.85 0.39/0.06 1.07/1.11 cs 0.78/0.65 0.79/0.34 0.91/0.81 sk 1.56/0.90 0.78/0.19 1.88/1.04 Avg. 0.98/0.86 0.4/0.03 1.02/0.86"
K19-1035,K17-1024,0,0.0548933,"Missing"
K19-1035,L16-1262,0,0.0921429,"Missing"
K19-1035,D17-1302,0,0.0175318,"guages demonstrate that adversarial training significantly improves the overall transfer performances under several different settings. We conduct a careful analysis to evaluate the language-agnostic representations resulted from adversarial training. 1 Introduction Cross-lingual transfer, where a model learned from one language is transferred to another, has become an important technique to improve the quality and coverage of natural language processing (NLP) tools for languages in the world. This technique has been widely applied in many applications, including part-of-speech (POS) tagging (Kim et al., 2017), dependency parsing (Ma and Xia, 2014), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), coreference resolution (Kundu et al., 2018), and question answering (Joty et al., 2017). Noteworthy improvements are achieved on low resource language applications due to cross-lingual transfer learning. 372 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 372–382 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics To verify the proposed approach, we conduct experiments on neural dependency parsers traine"
K19-1035,D15-1039,0,0.0410152,"ively, we would assume when the auxiliary language has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0.82/0.66 0.82/0.98 bg 0.79/0.47 0.26/-0.07 0.49/0.41 ru 0.96/0.85 0"
K19-1035,P18-2063,0,0.0266025,"sis to evaluate the language-agnostic representations resulted from adversarial training. 1 Introduction Cross-lingual transfer, where a model learned from one language is transferred to another, has become an important technique to improve the quality and coverage of natural language processing (NLP) tools for languages in the world. This technique has been widely applied in many applications, including part-of-speech (POS) tagging (Kim et al., 2017), dependency parsing (Ma and Xia, 2014), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), coreference resolution (Kundu et al., 2018), and question answering (Joty et al., 2017). Noteworthy improvements are achieved on low resource language applications due to cross-lingual transfer learning. 372 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 372–382 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics To verify the proposed approach, we conduct experiments on neural dependency parsers trained on English (source language) and directly transfer them to 28 target languages, with or without the assistance of unlabeled data from auxiliary languages. We chos"
K19-1035,K17-3007,0,0.0287844,"d in computer vision for image classification and received enormous success in improving model’s robustness on input images with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing appro"
K19-1035,D19-1077,0,0.137009,"nduct experiments on the Universal Dependencies (UD) Treebanks (v2.2) (Nivre et al., 2018) using 29 languages, as shown in Table 1. We use the publicly available implementation4 of the “SelfAtt-Graph” and “RNN-Stack” parsers.5 Ahmad et al. (2019) show that the “SelfAtt-Graph” parser captures less language-specific information and performs better than the ‘RNN-Stack” parser for distant target languages. Therefore, we use the “SelfAttGraph” parser in most of our experiments. Besides, the multilingual variant of BERT (mBERT) (Devlin et al., 2019) has shown to perform well in cross-lingual tasks (Wu and Dredze, 2019) and outperform the models trained on multilingual word embeddings by a large margin. Therefore, we consider conducting experiments with both multilingual word embeddings and mBERT. We use aligned multilingual word embeddings (Smith 4 5 3.1 Results and Analysis Table 2 presents the main transfer results of the “SelfAtt-Graph” parser when training on only English (en, baseline), English with French (enfr), and English with Russian (en-ru). The re6 https://github.com/huggingface/pytorch-transformers We concatenate the word and POS representations. In our future work, we will conduct transfer lea"
K19-1035,D17-1187,0,0.0226914,"tion and received enormous success in improving model’s robustness on input images with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target la"
K19-1035,E17-1021,0,0.0145028,"age has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0.82/0.66 0.82/0.98 bg 0.79/0.47 0.26/-0.07 0.49/0.41 ru 0.96/0.85 0.39/0.06 1.07/1.11 cs 0.78/0.65 0.79/0.34 0.91/0.81 s"
K19-1035,W14-1613,0,0.0178044,"it target languages that are closer to them12 or from the same family. Intuitively, we would assume when the auxiliary language has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1."
K19-1035,N18-2074,0,0.0311579,"nt margin in two family of languages. In addition, we conduct experiments to consolidate our findings with different types of input representations and encoders. Our experiment code is publicly available to facilitate future research.1 2 which are fed to the decoder and the classifier to predict the dependency structure and the language identity (id) of that sentence. The encoder and the decoder jointly form the parsing model and we consider two alternatives2 from (Ahmad et al., 2019): “SelfAtt-Graph” and “RNN-Stack”. The “SelfAtt-Graph” parser consists of a modified self-attentional encoder (Shaw et al., 2018) and a graph-based deep bi-affine decoder (Dozat and Manning, 2017), while the “RNN-Stack” parser is composed of a Recurrent Neural Network (RNN) based encoder and a stack-pointer decoder (Ma et al., 2018). We stack a classifier (a linear classifier or a multi-layer Perceptron (MLP)) on top of the encoder to perform the language identification task. The identification task can be framed as either a word- or sentence-level classification task. For the sentence-level classification, we apply average pooling3 on the contextual word representations generated by the encoder to form a fixed-length r"
K19-1035,D18-1034,0,0.0237282,"sfer performances under several different settings. We conduct a careful analysis to evaluate the language-agnostic representations resulted from adversarial training. 1 Introduction Cross-lingual transfer, where a model learned from one language is transferred to another, has become an important technique to improve the quality and coverage of natural language processing (NLP) tools for languages in the world. This technique has been widely applied in many applications, including part-of-speech (POS) tagging (Kim et al., 2017), dependency parsing (Ma and Xia, 2014), named entity recognition (Xie et al., 2018), entity linking (Sil et al., 2018), coreference resolution (Kundu et al., 2018), and question answering (Joty et al., 2017). Noteworthy improvements are achieved on low resource language applications due to cross-lingual transfer learning. 372 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 372–382 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics To verify the proposed approach, we conduct experiments on neural dependency parsers trained on English (source language) and directly transfer them to 28 target languages, wi"
K19-1035,P17-1130,0,0.0199091,"nin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the language invariant representations learned by previously proposed methods can perform well on a Table 7: Average cross-lingual performance difference between the SelfAtt-Graph parser trained on the source (en) and an auxiliary (x) language and"
K19-1035,N18-1089,0,0.0213827,"fellow et al., 2015) was initially introduced in computer vision for image classification and received enormous success in improving model’s robustness on input images with perturbations. Later many variants of GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) were proposed to improve its’ training stability. In NLP, adversarial training was first utilized for domain adaptation (Ganin et al., 2016). Since then adversarial training has started to receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Jot"
K19-1035,P17-1179,0,0.0173354,"receive an increasing interest in the NLP community and applied to many NLP applications including part-of-speech (POS) tagging (Gui et al., 2017; Yasunaga et al., 2018), dependency parsing (Sato et al., 2017), relation extraction (Wu et al., 2017), text classification (Miyato et al., 2017; Liu et al., 2017; Chen and Cardie, 2018), dialogue generation (Li et al., 2017). In the context of cross-lingual NLP tasks, many recent works adopted adversarial training, such as in sequence tagging (Adel et al., 2018), text classification (Xu and Yang, 2017; Chen et al., 2018), word embedding induction (Zhang et al., 2017; Lample et al., 2018), relation classification (Zou et al., 2018), opinion mining (Wang and Pan, 2018), and question-question similarity reranking (Joty et al., 2017). However, existing approaches only consider using the target language as the auxiliary language. It is unclear whether the language invariant representations learned by previously proposed methods can perform well on a Table 7: Average cross-lingual performance difference between the SelfAtt-Graph parser trained on the source (en) and an auxiliary (x) language and the SelfAttGraph parser trained only on English (en) language (UA"
K19-1035,N13-1126,0,0.0593559,"Missing"
K19-1035,C18-1037,0,0.0308931,"Missing"
K19-1035,W15-2137,0,0.0603923,"that are closer to them12 or from the same family. Intuitively, we would assume when the auxiliary language has a smaller average distance to all the target languages, the cross-lingual transfer performance would be better. However, from the results in Table 6, we do not see such a pattern. For example, Portuguese (pt) has the smallest average distance to other languages among the aux12 4 Related Work Unsupervised Cross-lingual Parsing. Unsupervised cross-lingual transfer for dependency parsing has been studied over the past few years (Agi´c et al., 2014; Ma and Xia, 2014; Xiao and Guo, 2014; Tiedemann, 2015; Guo et al., 2015; Aufrant et al., 2015; Rasooli and Collins, 2015; Duong et al., 2015; Schlichtkrull and Søgaard, 2017; Ahmad et al., 2019; Rasooli and Collins, 2019; He et al., 2019). Here, “unsupervised transfer” refers to the setting where a parsing model trained only on the source language is directly The language distances are computed based on word order characteristics as suggested in Ahmad et al. (2019). 378 Lang (en,ru) - en (en,fr) - en (en,de) - en IE.Slavic Family hr 1.24/0.90 0.43/-0.45 1.52/1.02 sl 0.35/0.53 -0.55/-0.60 -0.04/0.14 uk 1.23/1.32 0.26/0.09 1.54/1.33 pl 0.97/1.29 0"
K19-1035,W14-4203,0,\N,Missing
K19-1035,C16-1012,0,\N,Missing
K19-1035,N19-1385,0,\N,Missing
K19-1048,D15-1064,1,0.887973,"e hidden vector of the space after a word from the forward LSTM and the hidden vector of the space before a word from the backward LSTM to form a character-level repre− → ← − sentation of the word: hci = [hci ; hci ]. The wordlevel BiLSTM then takes the concatenation of hci and the word embedding as input xi = [ei ; hci ] to learn contextualized representations. Neural-CRFs. Conditional Random Fields (CRFs) (Lafferty et al., 2001) are sequence tagging models that capture the inter-dependencies between the output tags; they have been widely used for NER (McCallum and Li, 2003; Lu et al., 2015; Peng and Dredze, 2015, 2016, 2017). Given a set of training data {xi , yi }N , a CRF minimizes negative log-likelihood: min − X Θ log P (yi |xi ; Θ), (1) i P (yi |xi ; Θ) = Gold Energy St(yi ) =P 0 P artition y 0 St(y ) (2) where y 0 is any possible tag sequence with the same length as yi , St(y 0 ) is the potential of the tag sequence y 0 , and St(yi ) is the potential of the gold tag sequence. The numerator St(yi ) is called P the gold energy function, and the denominator y0 St(y 0 ) is the partition function. The likelihood function using globally annotated data is illustrated in Figure 2a. The potential of a t"
K19-1048,D14-1010,0,0.0317585,"ons (noisy labels), where some occurrences of entities are neglected in the annotation process and falsely labeled as non-entities (negative). A related problem is learning from unlabeled data with distant supervision. A major challenge of all these settings, including ours, is that a positive instance might be labeled as negative. A well-explored solution to this problem is proposed by Tsuboi et al. (2008), which instead of maximizing the likelihood of the gold tag sequence, we maximize the total likelihood for all possible tag sequences consistent with the gold labels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have the result of overly encouraging the prediction of entities, resulting in low precision. To tackle the problem of incomplete annotations, Carlson et al. (2009); Yang et al. (2018) explored bootstrap-based semi-su"
K19-1048,W17-2612,1,0.824835,"Missing"
K19-1048,C18-1183,0,0.0114825,"abels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have the result of overly encouraging the prediction of entities, resulting in low precision. To tackle the problem of incomplete annotations, Carlson et al. (2009); Yang et al. (2018) explored bootstrap-based semi-supervised learning on unlabeled data, iteratively identifying new entities with the taggers and then re-training the taggers. Bellare and McCallum (2007); Li and Liu (2005); Fernandes and Brefeld (2011) explored an EM algorithm with semi-supervision. For the partial annotation problem, most previous work has focused on building individual taggers for each dataset and using single-task learning (Liu et al., 2018) or multi-task learning (Crichton et al., 2017; Wang et al., 2018). In singletask learning, each model is trained separately on each dataset Ci , and mak"
K19-1048,D11-1141,0,0.0188705,"es Institute, University of Southern California 2 Department of Computer Science, University of Southern California 3 Outreach, Inc. {huan183, lidong}@usc.edu, {boschee, npeng}@isi.edu Abstract One problem in NER is the diversity of entity types, which vary in scope for different domains and downstream tasks. Traditional NER for the news domain focuses on three coarsegrained entity types: person, location, and organization (Tjong Kim Sang and De Meulder, 2003). However, as NLP technologies have been applied to a broader set of domains, many other entity types have been targeted. For instance, Ritter et al. (2011) add seven new entity types (e.g., product, tv-show) on top of the previous three when annotating tweets. Other efforts also define different but partially overlapping sets of entity types (Walker et al., 2006; Ji et al., 2010; Consortium, 2013; Aguilar et al., 2014). These non-unified annotation schemas result in partially annotated datasets: each dataset is only annotated with a subset of possible entity types. One approach to this problem is to train individual NE taggers for each partially annotated dataset and combine their results using some heuristics. Figure 1 shows an example that dem"
K19-1048,W03-0419,0,0.573762,"Missing"
K19-1048,D18-1230,0,0.0418022,"Missing"
K19-1048,J01-4004,0,0.457443,"t test time. Experiments show that the proposed model significantly outperforms strong multi-task learning baselines when training on multiple, partially annotated datasets and testing on datasets that contain tags from more than one of the training corpora.1 1 Introduction Named Entity Recognition (NER), which identifies the boundaries and types of entity mentions from raw text, is a fundamental problem in natural language processing (NLP). It is a basic component for many downstream tasks, such as relation extraction (Hasegawa et al., 2004; Mooney and Bunescu, 2005), coreference resolution (Soon et al., 2001), and knowledge base construction (Craven et al., 1998; Craven and Kumlien, 1999). ∗ 2 Work done while the author was at USC ISI. The code and the datasets will be made available at https://github.com/xhuang28/NewBioNer https://corposaurus.github.io/ corpora/ summarizes dozens of partially annotated biomedical datasets. 1 515 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 515–527 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics Figure 1: An example sentence from the CellFinder corpus (Neves et al., 2012) showing the ch"
K19-1048,C08-1113,0,0.333001,"ould be more generally thought of as learning from imperfect annotations. In that broad sense, there are several notable areas of prior work. One of the most prominent concerns learning from incomplete annotations (noisy labels), where some occurrences of entities are neglected in the annotation process and falsely labeled as non-entities (negative). A related problem is learning from unlabeled data with distant supervision. A major challenge of all these settings, including ours, is that a positive instance might be labeled as negative. A well-explored solution to this problem is proposed by Tsuboi et al. (2008), which instead of maximizing the likelihood of the gold tag sequence, we maximize the total likelihood for all possible tag sequences consistent with the gold labels. Tsuboi et al. (2008); Yang and Vozila (2014) applied this idea to the incomplete annotation setting; Shang et al. (2018); Liu et al. (2014) applied it to the unlabeled data with distant supervision setting; and Greenberg et al. (2018) applied it to the partial annotation setting. While this is a general solution, its primary drawback is that it assumes a uniform prior on all labels consistent with the gold labels. This may have"
K19-1048,W04-1213,0,\N,Missing
K19-1048,P04-1053,0,\N,Missing
K19-1048,D14-1093,0,\N,Missing
K19-1048,W03-0430,0,\N,Missing
K19-1048,W13-2002,0,\N,Missing
K19-1048,N16-1030,0,\N,Missing
K19-1048,D18-1306,0,\N,Missing
K19-1048,P16-1101,0,\N,Missing
K19-1048,W14-2907,0,\N,Missing
K19-1062,S13-2002,0,0.454575,"ise predictions: graph temporal transitivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longter"
K19-1062,D11-1027,0,0.0374875,"yper-parameters re-optimize the network to adjust for global properties5 . We denote the local scoring model in the first stage as local model, and the final model as global model in the following sections. Start-point temporal scheme is adopted when outsourcing the annotation task, which contributes to the performance improvement of machine learning models built on this dataset . 4 TCR (Ning et al., 2018a) follows the same annotation scheme for temporal pairs in MATRES. It is also annotated with causal pairs. To get causal pairs, the authors select candidates based on EventCausality dataset (Do et al., 2011). Experimental Setup In this section, we describe the three datasets that are used in the paper. Then we define the evaluation metrics. Finally, we provide details regarding our model implementation and experiments. 4.2 4.1 Data Evaluation Metrics To be consistent with the evaluation metrics used in baseline models, we adopt two slightly different calculations of metrics. Experiments are conducted on TB-Dense, MATRES and TCR datasets and an overview of data statistics are shown in Table 1. We focus on event relation, thus, all numbers refer to EE pairs6 . Note that in all three datasets, event"
K19-1062,W06-1623,0,0.140289,"is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thu"
K19-1062,D12-1062,0,0.184424,"anges the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information c"
K19-1062,P14-2082,0,0.504286,"lysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on temporal relation extraction. 2 Related Work Temporal Relation Data. Temporal relation corpora such as TimeBank (Pustejovsky et al., 2003) and RED (O’Gorman et al., 2016) facilitate the research in temporal relation extraction. The common issue in these corpora is missing annotation. Collecting densely annotated temporal relation corpora with all event pairs fully annotated has been reported to be a challenging task as annotators could easily overlook some pairs (Cassidy et al., 2014; Bethard et al., 2007; Chambers et al., 2014). TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences. Recent data construction efforts such as MATRES (Ning et al., 2018a) and TCR (Ning et al., 2018b) further enhance the data quality by using a multiaxis annotation scheme and adopting start-point of events to improve inter-annotator agreements. However, densely annotated datasets are relatively small both in terms of number of documents and event pairs, which restricts the complexity of machine learning models used i"
K19-1062,D19-1041,1,0.522435,"ulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More recently, neural network-based methods have been employed for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Han et al., 2019a) which achieved impressive results. However, they all treat the task as a pairwise classification problem. Meng and Rumshisky (2018) considered incorporating global context for pairwise relation predictions, but they do not explicitly model the output graph structure for event temporal relation. There are a few prior works exploring structured learning for temporal relation extraction (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, their local models use hand-engineered linguistic features. Despite the effectiveness of hand-crafted features in previous rese"
K19-1062,S13-2012,0,0.239398,"ivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the ex"
K19-1062,S13-2015,0,0.445816,": graph temporal transitivity constraint is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between e"
K19-1062,Q14-1022,0,0.791836,"is violated given the relation between filed and claiming is SIMULTANEOUS. In Figure 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1,"
K19-1062,D08-1073,0,0.26688,"re 1c, the structured model changes the prediction of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contex"
K19-1062,E17-1108,0,0.140085,"state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on temporal relation extraction."
K19-1062,P07-2044,0,0.149825,"rrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More"
K19-1062,P06-1095,0,0.335801,"rthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot con"
K19-1062,P17-2001,0,0.118779,"g models used in previous research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013),"
K19-1062,P18-1049,0,0.460447,"s paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier"
K19-1062,D17-1092,0,0.452005,"us research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Cham"
K19-1062,C16-1007,0,0.257589,"Missing"
K19-1062,S07-1014,0,0.069377,"nd Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-ba"
K19-1062,N16-1098,0,0.0170875,"temporal relations between the events. Figure 1a illustrates an example of such graph for the text shown above. Different types of edges specify different temporal relations: the event filed is SIMULTANEOUS with claiming, overruled is BEFORE claiming, and overruled is also BEFORE filed. Temporal relation extraction is beneficial for many downstream tasks such as question answering, information retrieval, and natural language generation. An event graph can po∗ ﬁled tentially be leveraged to help time-series forecasting and provide guidances for natural language generation. The CaTeRs dataset (Mostafazadeh et al., 2016) which annotates temporal and causal relations is constructed for this purpose. A major challenge in temporal relation extraction stems from its nature of being a structured The authors contribute equally, alphabetical order. 666 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 666–676 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics We develop a joint end-to-end training scheme that enables the feedback from global structure to directly guide neural networks to learn representations, and hence allows our deep structured"
K19-1062,D17-1108,0,0.74796,"s and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for future research on"
K19-1062,C08-3012,0,0.279212,"RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine learning model; sieves are sorted by precision, i.e. decisions from a lower precision classifier cannot contradict those from a higher precision model. More recently, neural network-based me"
K19-1062,P18-1212,0,0.0633864,"on of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for"
K19-1062,P18-1122,0,0.0567825,"on of relation between overruled and claiming from AFTER to BEFORE to ensure compatibility of all predicted edge types. • We propose a deep SSVM model for event temporal relation extraction. • We show strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for"
K19-1062,P09-1046,0,0.323718,"strong empirical results and establish new state-of-the-art for three event relation benchmark datasets. Prior works on event temporal relation extraction mostly formulate it as a pairwise classification problem (Bethard, 2013; Laokulrat et al., 2013; Chambers, 2013; Chambers et al., 2014) disregarding the global structures. Bramsen et al. (2006a); Chambers and Jurafsky (2008); Do et al. (2012); Ning et al. (2018b,a) explore leveraging global inference to ensure consistency for all pairwise predictions. There are a few prior works that directly model global structure in the training process (Yoshikawa et al., 2009; Ning et al., 2017; Leeuwenberg and Moens, 2017). However, these structured models rely on hand-crafted features using linguistic rules and local-context, which cannot adequately capture potential longterm dependencies between events. In the example shown in Figure 1, filed occurs in much earlier context than overruled. Thus, incorporating longterm contextual information can be critical for correctly predicting temporal relations. • Extensive ablation studies and thorough error analysis are conducted to understand the capacity and limitations of the proposed model, which provide insights for"
K19-1062,W16-5706,0,0.218256,"Missing"
K19-1062,D14-1162,0,0.091123,"of our current framework. Acknowledgments This work is partially funded by DARPA Contracts W911NF-15-1-0543 and an NIH R01 (LM012592). The authors thank the anonymous reviewers for their helpful suggestions and members from USC PLUS lab for early feedbacks. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors. Effect of BERT representations In this section, we explore the impact of contextualized BERT representations under our deep SSVM framework. We replace BERT representations with the GloVe (Pennington et al., 2014) word embeddings. Table 9 shows the F1 scores of our local model and global model using BERT and GloVe11 respectively. BERT improves the performance with a significant margin. Besides, even without BERT representations, our RNN-based local model and the deep structured global model 11 globalGlove 57.0 75.6 76.5 still outperform (MATRES and TCR) or are comparable with (TB-Dense) current SOTA. These results confirm the improvements of our method. significantly in the both-way evaluation. In contrast, the proposed model achieves strong performances in both test scenarios (best F1 scores except fo"
K19-1062,P17-2035,0,0.365421,"exity of machine learning models used in previous research. In this paper, we propose a novel deep structured learning model to address the shortcomings of the previous methods. Specifically, we adapt the structured support vector machine (SSVM) (Finley and Joachims, 2008) to incorporate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime ("
K19-1062,S17-2098,0,0.0651677,"Missing"
K19-1062,S13-2001,0,0.461397,"rate linguistic constraints and domain knowledge for making joint predictions on events temporal relations. Furthermore, we augment this framework with recurrent neural networks (RNNs) to learn long-term contexts. Despite the recent success of employing neural network models for event temporal relation extraction (Tourille et al., 2017a; Cheng and Miyao, 2017; Meng et al., 2017; Meng and Rumshisky, 2018), these systems make pairwise predictions, and do not take advantage of problem structures. Event Temporal Relation Extraction The series of TempEval competitions (Verhagen et al., 2007, 2010; UzZaman et al., 2013) attract many research interests in predicting event temporal relations. Early attempts (Mani et al., 2006; Verha667 gen et al., 2007; Chambers et al., 2007; Verhagen and Pustejovsky, 2008) only use local pairwise classification with hand-engineered features. Later efforts, such as ClearTK (Bethard, 2013), UTTime (Laokulrat et al., 2013), and NavyTime (Chambers, 2013) improve earlier work by feature engineering with linguistic and syntactic rules. A noteworthy work, CAEVO (Chambers et al., 2014), builds a pipeline with ordered sieves. Each sieve is either a rule-based classifier or a machine l"
K19-1062,S10-1010,0,\N,Missing
N15-3018,W14-2907,1,0.881586,"Missing"
N15-3018,P14-1073,1,0.741206,"on-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quicklime (Section 3.7). an example visualization. 3.6 Cross Document Coreference Resolution Cross document coreference resolution is performed via the phylogenetic entity clustering model of Andrews et al. (2014).5 Since the method is fully unsupervised we did not require a Chinese specific model. We use this system to cluster EntityMentions and store the clustering in top level C ONCRETE Clustering objects. 3.7 Creating Manual Annotations Quicklime6 is a browser-based tool for viewing and editing NLP annotations stored in a C ONCRETE document. Quicklime supports a wide array of analytics, including parse trees, token taggings, entities, mentions, and “situations” (e.g. relations.) Quicklime uses the visualization layer of BRAT (Stenetorp et al., 2012) to display some annotations but does not use the"
N15-3018,W08-0336,0,0.0139229,"tions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 ,"
N15-3018,P05-1045,0,0.00391797,"cy parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit which is a CRF model based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gaz"
N15-3018,P03-1054,0,0.0257485,"Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit wh"
N15-3018,P14-5010,0,0.0125522,"ata sets include annotations for entities and a variety of relations (Aguilar et al., 2014). The labeled entities and relations are represented by C ONCRETE EntityMentions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on"
N15-3018,W12-3018,1,0.837337,"Missing"
N15-3018,P11-1053,0,0.0126349,"otations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quic"
N15-3018,N03-1033,0,0.0354521,"Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, a"
N15-3018,P05-1053,0,0.027847,"l based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path betw"
N18-6003,H05-1066,0,0.174527,"Missing"
N18-6003,W02-2020,0,0.0398699,"Missing"
N18-6003,P09-1113,0,0.38883,"Missing"
N18-6003,W14-1611,0,0.0475727,"Missing"
N18-6003,D15-1064,1,0.829291,"d TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and appli"
N18-6003,P11-1055,0,0.138239,"Missing"
N18-6003,P10-1029,0,0.0521991,"Missing"
N18-6003,Q17-1008,1,0.820066,"ing tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and applications. Topics to be covered in this tutorial. The first 2/3 of this tutorial presents a comprehensive overview of the information extraction techniques developed in recent years for constructing knowledge bases (see also Section 2 for a more"
N18-6003,P08-1068,0,0.0888921,"Missing"
N18-6003,D11-1049,0,0.0401887,"xt corpora; (2) entity recognition and typing: preliminaries, challenges, and methodologies; and (3) relation extraction: previous efforts, limitations, recent progress, and a joint entity and relation extraction method using distant supervision; (4) multi-task and multi-domain learning for lowresource information extraction; (5) distill linguistic knowledge into neural models to help low-resource information extraction. The second half of the tutorial presents a comprehensive overview of KB reasoning techniques. For path-based methods, we will first describe the Path-Ranking Algorithm (PRA) (Lao et al., 2011) and briefly describe extensions such as ProPPR (Wang et al., 2013). Our tutorial will also cover the recent integration of Relevance to ACL. Machine “reading” and “reasoning” of large text corpora have long been the interests to CL and NLP communities, especially when people now are exposed to an explosion of information in the form of free text. Extracting structured information is key to understanding messy and scattered raw data, and effective reasoning tools are critical for the use of KBs in downstream tasks like QA. This tutorial will present an organized picture of recent research on k"
N18-6003,P14-1038,0,0.029821,"Missing"
N18-6003,W09-1119,0,0.10705,"Missing"
N18-6003,D16-1144,1,0.814067,"Missing"
N18-6003,D17-1060,1,0.829166,"and help knowledge extraction in low-resource settings with minimal supervision. In the reasoning part, we aim to leverage the existing background knowledge and design various algorithms to fill in the missing link between entities in the KB, given the extracted KBs are likely incomplete. More specifically, this part will introduce two lines of research for KB reasoning: path-based and embedding-based methods. PRA with recurrent neural networks. For the embedding based method, we will briefly describe RESCAL (Nickel et al., 2011) and TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chines"
N18-6003,P00-1015,0,0.0168191,"Missing"
N18-6003,P10-1040,0,0.141393,"Missing"
N18-6003,P05-1053,0,0.080725,"Missing"
N19-1172,S17-2011,0,0.145133,"egotiator” and getting “negotiator am just a woman trying to peace her life back together.”. Therefore, we use a sequence-tosequence model to smooth the edited sentence (R ETRIEVE +S WAP +T OPIC +S MOOTHER). We smooth the sentence by deleting words around the topic word and train a model to fill in the blank. The smoother is trained in a similar fashion to denoising autoencoders: we delete immediate neighbors of a word in a sentence, and ask the model to reconstruct the sentence by predicting missing neighbors. A training example is shown below: We use the pun dataset from 2017 SemEval task7 (Doogan et al., 2017). The dataset contains 1099 human-written puns annotated with pun words and alternative words, from which we take 219 for development. We use BookCorpus (Zhu et al., 2015) as the generic corpus for retrieval and training various components of our system. 4.1 4.2 Datasets Analysis of the Surprisal Principle We evaluate the surprisal principle by analyzing how well the local-global surprisal score (Equation (4)) predicts funniness rated by humans. We first give a brief overview of previous computational accounts of humor, and then analyze the correlation between each metric and human ratings. Pr"
N19-1172,P18-1082,0,0.0328441,"ion baseline. 1 Pun word: dyed. Alternative word: died. Figure 1: An illustration of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example in Figure 1: “Yes"
N19-1172,D16-1126,0,0.0306005,"pproach to improve both grammaticality and diversity of the generated text. Shen et al. (2017); Fu et al. (2018) explored adversarial training to manipulate the style of a sentence. Our neural smoother is also closely related to Li et al. (2018)’s delete-retrieve-edit approach to text style transfer. Creative generation is more challenging as it requires both formality (e.g., grammaticality, rhythm, and rhyme) and novelty. Therefore, many works (including us) impose strong constraints on the generative process, such as Petrovic and Matthews (2013); Valitutti et al. (2013) for joke generation, Ghazvininejad et al. (2016) for poetry generation, and Yao et al. (2019) for storytelling. 6 Conclusion In this paper, we tackled pun generation by developing and exploring a local-global surprisal principle. We show that a simple instantiation based on only a language model trained on non-humorous text is effective at detecting puns (though is not fine-grained enough to detect the degree of funniness within puns). To generate puns, we operationalize the surprisal principle with a retrieve-and-edit framework to create contrast in the amount of surprise in local and global contexts. While we improve beyond current techni"
N19-1172,Q18-1031,1,0.886836,"Missing"
N19-1172,N18-1169,1,0.849493,"between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surprisal principle for pun generation. 5.3 Creative text generation Our work is also built upon generic text generation techniques, in particular recent neural generation models. Hashimoto et al. (2018) developed a retrieve-and-edit approach to improve both grammaticality and diversity of the generated text. Shen et al. (2017); Fu et al. (2018) explored adversarial training to manipulate the style of a sentence. Our neural smoother is also closely related to Li et al. (2018)’s delete-retrieve-edit approach to text style transfer. Creative generation is more challenging as it requires both formality (e.g., grammaticality, rhythm, and rhyme) and novelty. Therefore, many works (including us) impose strong constraints on the generative process, such as Petrovic and Matthews (2013); Valitutti et al. (2013) for joke generation, Ghazvininejad et al. (2016) for poetry generation, and Yao et al. (2019) for storytelling. 6 Conclusion In this paper, we tackled pun generation by developing and exploring a local-global surprisal principle. We show that a simple instantiation"
N19-1172,S17-2005,0,0.170731,"Missing"
N19-1172,P12-1101,0,0.0733225,"s to connect the topic word able (indicating ambiguity), the pun meaning is with the seed sentence in a grammatical way, e.g., unexpected based on the local context. Second, “the negotiator is just a woman trying to peace the local-global surprisal contrast requires the pun her life back together.” (the part rewritten by the word to be well supported in the global context. smoother is underlined). Given the anomalous nature of puns, we also consider a metric for unusualness based 4 Experiments on normalized log-probabilities under a language We first evaluate how well our surprisal prinmodel (Pauls and Klein, 2012): ciple predicts the funniness of sentences per! n Y 1 def ceived by humans (Section 4.2), and then comUnusualness = − log p(x1 , . . . , xn )/ p(xi ) . n pare our pun generation system and its variai=1 2 (6) Path similarity is a score between 0 and 1 that is inthe man slowly walked towards the woods . versely proportional to the shortest distance between two word senses in WordNet. 3 Pronouns are mapped to the synset person.n.01. Implementation details. Both ambiguity and distinctiveness are based on a generative model 1737 Type Pun Swap-pun Non-pun S EM E VAL Example K AO Count Funniness Cou"
N19-1172,W18-1505,1,0.835385,"of a neural generation baseline. 1 Pun word: dyed. Alternative word: died. Figure 1: An illustration of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example"
N19-1172,P13-2041,0,0.487445,"urprisal principle is also related to studies in psycholinguistics on the relation between surprisal and human comprehension (Levy, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings. Most of these approaches leverage some assumptions of joke structures, e.g., incongruity, relations between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surpr"
N19-1172,W05-1614,0,0.294428,"for puns focuses on ambiguity between two concepts and the heterogeneity nature of the ambiguity. Our surprisal principle further formalizes unexpectedness (local surprisal) and incongruity resolution (global association). The surprisal principle is also related to studies in psycholinguistics on the relation between surprisal and human comprehension (Levy, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings"
N19-1172,N18-5020,0,0.0189354,"ation of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example in Figure 1: “Yesterday I accidentally swallowed some food coloring. The doctor says I’m OK, b"
N19-1172,P13-2044,0,0.708842,"y, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings. Most of these approaches leverage some assumptions of joke structures, e.g., incongruity, relations between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surprisal principle for pun generation. 5.3 Creative text generation Our work is also built upon generic text generation techniqu"
N19-1172,P18-1153,0,0.23567,"ndicates one interpretation: the person is colored inside by food coloring. On the other hand, an alternative word (“died”) is implied by the context for another interpretation: the person is sad due to the accident. Current approaches to text generation require lots of training data, but there is no large corpus of puns. Even such a corpus existed, learning the distribution of existing data and sampling from it is unlikely to lead to truly novel, creative sentences. Creative composition requires deviating from the norm, whereas standard generation approaches seek to mimic the norm. Recently, Yu et al. (2018) proposed an unsupervised approach that generates puns from a neural language model by jointly decoding conditioned on both the pun and the alternative words, thus injecting ambiguity to the output sentence. However, Kao et al. (2015) showed that ambiguity alone is insufficient to bring humor; the two mean1734 Proceedings of NAACL-HLT 2019, pages 1734–1744 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ings must also be supported by distinct sets of words in the sentence. Inspired by Kao et al. (2015), we propose a general principle for puns whi"
N19-4016,P07-2045,0,\N,Missing
N19-4016,D14-1162,0,\N,Missing
N19-4016,N16-1098,0,\N,Missing
P12-3025,C10-2115,0,\N,Missing
P14-2102,P14-1073,1,0.827801,"a contextual edit tend to raise or lower its probability (and the regularizer encourages such generalization). Each contextual edit (C, e) can be characterized as a 5-tuple (s, t, C1 , C20 , C3 ): it replaces s ∈ Σx ∪ {} with t ∈ Σy ∪ {} when s falls between C1 and C20 (so C2 = sC20 ) and t is preceded by C3 . Then each of the 14 features of (C, e) indicates that a particular subset of this 5-tuple has a particular value. The subset always includes s, t, or both. It never includes C1 or C20 without s, and never includes C3 without t. (Mays et al., 1991; Church and Gale, 1991; Kukich, 1992; Andrews et al., 2014). To eliminate experimental confounds, we use no dictionary or language model as one would in practice, but directly evaluate our ability to model p(correct |misspelled). Consider (xk , yk ) = (feeel, feel). Our model defines p(y |xk ) for all y. Our training objective (section 6) tries to make this large for y = yk . A contextual edit model learns here that e 7→  is more likely in the context of ee. We report on test data how much probability mass lands on the true yk . We also report how much mass lands “near” yk , by measuring the expected edit distance of the predicted y to Pthe truth. Ex"
P14-2102,N10-1083,0,0.0420762,"Missing"
P14-2102,P05-1057,0,0.0422806,"1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST"
P14-2102,P03-1012,0,0.0533545,"t alignment of y to x. 1 If N2 = 0, so that we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-stat"
P14-2102,J98-2005,0,0.103253,"ns and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), although the tightness conditions for a PCFG are more complex. In section 7, we discuss the costs and benefits of PFSTs relative to other options. 4 The Contextual Edit PFST We now define a PFST topology that concisely captures the contextual edit process of section 2. We are given the alphabets Σx , Σy and the context window sizes N1 , N2 , N3 ≥ 0. For each possible context triple C = (C1 , C2 , C3 ) as defined in section 2, we construct an edit state qC whose outgoing arcs correspond to the possible edit operations in that context. One might expect that the SUBST(t) edit operation that rea"
P14-2102,P96-1031,0,0.1924,"r any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation of the composition opera"
P14-2102,J97-2003,0,0.0482753,"bility > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly models pθ (y | x) for all (x, y) pairs.2 Then various computations with this distribution can be reduced to standard finite-state computations that efficiently employ dynamic programming over the structure of the PFST, and the PFST can be easily combined with other finite-state distributions and functions (Mohri, 1997; Eisner, 2001). A PFST is a two-tape generalization of the wellknown nondeterministic finite-state acceptor. It is a finite directed multigraph where each arc is labeled with an input in Σx ∪ {}, an output in Σy ∪{}, and a probability in [0, 1]. ( is the empty string.) Each state (i.e., vertex) has a halt probability in [0, 1], and there is a single initial state qI . Each path from qI to a final state qF has We take the first two conditions to be part of the definition of a PFST. The final condition requires our PFST to be “tight” in the same sense as a PCFG (Chi and Geman, 1998), althoug"
P14-2102,D08-1113,1,0.806276,"duce xk to yk , relative to competing edits from the same contexts C. This means raising θ · f (C, e) and/or lowering ZC . Thus, log pθ (yk |xk ) depends only on the probabilities of edit arcs in T that appear in xk ◦ T ◦ yk , and the competing edit arcs from the same edit states qC . The gradient ∇θ log pθ (yk |xk ) takes the form &quot; # X X c(C, e) f~(C, e) − pθ (e0 |C)f~(C, e0 ) C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right ou"
P14-2102,N13-1073,0,0.0294951,"hat we do not condition on xi+1 , we must still condition on whether xi+1 = EOS (a single bit). We gloss over special handling for N2 = 0; but it is in our code. 625 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 625–630, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This model is reminiscent of conditional models in MT that perform stepwise generation of one string or structure from another—e.g., string alignment models with contextual features (Cherry and Lin, 2003; Liu et al., 2005; Dyer et al., 2013), or tree transducers (Knight and Graehl, 2005). 3 • For each state q, the halt action and the arcs from q with input label  must have total probability of 1. (These are the available choices if there is no next input character.) • Every state q must be co-accessible, i.e., there must be a path of probability > 0 from q to some qF . (Otherwise, the PFST could lose some probability mass to infinite paths. The canonical case of this involves an loop q → q with input label  and probability 1.) Probabilistic FSTs We will construct a probabilistic finite-state transducer (PFST) that compactly mod"
P14-2102,E99-1017,0,0.178288,"Missing"
P14-2102,J94-3001,0,0.128363,"care to ensure that for any x ∈ Σ∗x , the total probability of all paths accepting x is 1, so that pθ (y |x) is truly a conditional probability distribution. This is guaranteed by the following sufficient conditions (we omit the proof for space), which do not seem to appear in previous literature: • For each state q and each symbol b ∈ Σx , the arcs from q with input label b or  must have total probability of 1. (These are the available choices if the next input character is x.) 2 Several authors have given recipes for finite-state transducers that perform a single contextual edit operation (Kaplan and Kay, 1994; Mohri and Sproat, 1996; Gerdemann and van Noord, 1999). Such “rewrite rules” can be individually more expressive than our simple edit operations of section 2; but it is unclear how to train a cascade of them to model p(y |x). 626 a b x _ ε:z a bc z _ / ε re a d T(z SER ert ins z /1 c ε:ε )) ,x ,bc a )|( p(IN c: aa bc ba x _ c,x) (a,b (b) | E LET / p(DE te b dele ε:y /p (SU su b PFST, T , has O(|Σx |N1 +N2 |Σy |N3 ) states and O(|Σx |N1 +N2 |Σy |N3 +1 ) arcs. Composing this T with deterministic FSAs takes time linear in the size of the result, using a left-to-right, on-the-fly implementation"
P14-2102,W02-1002,0,0.0248954,") C,e ping this requirement gives a weighted FST (WFST), whose path weights w(x, y) can be globally normalized (divided by a constant Zx ) to obtain probabilities p(y |x). WFST models of contextual edits were studied by Dreyer et al. (2008). PFSTs and WFSTs are respectively related to MEMMs (McCallum et al., 2000) and CRFs (Lafferty et al., 2001). They gain added power from hidden states and  transitions (although to permit a finite-state encoding, they condition on x in a more restricted way than MEMMs and CRFs). WFSTs are likely to beat PFSTs as linguistic models,4 just as CRFs beat MEMMs (Klein and Manning, 2002). A WFST’s advantage is that the probability of an edit can be indirectly affected by the weights of other edits at a distance. Also, one could construct WFSTs where an edit’s weight directly considers local right output context C4 . So why are we interested in PFSTs? Because they do not require computing a separate normalizing contant Zx for every x. This makes it computationally tractable to use them in settings where x is uncertain because it is unobserved, partially observed (e.g., lacks syllable boundaries), or noisily observed. E.g., at the end of section 5, X represented an uncertain x."
P14-2110,W11-3407,0,0.0123015,"resentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this wor"
P14-2110,N13-1037,0,0.0202374,"facilitate multi-lingual corpus analysis. We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators. 1 Figure 1: Three users discuss Mexico’s football team advancing to the Gold medal game in the 2012 Olympics in code-switched Spanish and English. can be folded in during training, the “glue” documents are required to aid in the alignment across languages. However, the ever changing vocabulary and topics of social media (Eisenstein, 2013) make finding suitable comparable corpora difficult. Standard techniques – such as relying on machine translation parallel corpora or comparable documents extracted from Wikipedia in different languages – fail to capture the specific terminology of social media. Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daum´e, 2010) similarly fail to adapt to shifting vocabularies. The result: an inability to train polylingual models on social media. In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages. Social media is filled"
P14-2110,C12-2029,0,0.0342668,"inguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans"
P14-2110,P98-1002,0,0.0283031,"rable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this work, we take an unsupervised approach, relying only on readily available document level"
P14-2110,C82-1023,0,0.176876,"in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this work, we take an unsupervised approach, relying only on readily available document level language ID s"
P14-2110,N13-1131,0,0.0493679,"switched messages is given by Ling et al. (2013): 3 csLDA To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token specific language variable, and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language. In the case of code-switched social media data, we require a pertoken language variable. However, while document level language identification (LID) systems are common place, very few languages have pertoken LID systems (King and Abney, 2013; Lignos and Marcus, 2013). To address the lack of available LID systems, we add a per-token latent language variable to the polylingual topic model. For documents that are not code-switched, we observe these variables to be the output of a document level LID system. In the case of code-switched documents, these variables are inferred during model inference. Second, polylingual topic models assume the aligned topics are from parallel or comparable corpora, which implicitly assumes that a topics popularity is balanced across languages. Topics that show up in one language necessarily show up in"
P14-2110,D08-1102,0,0.148927,"n-aligned documents 674 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English c"
P14-2110,C12-1102,0,0.105472,"23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a transl"
P14-2110,D08-1110,0,0.569987,"n-aligned documents 674 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English c"
P14-2110,li-etal-2012-mandarin,0,0.0123651,"s. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task."
P14-2110,W12-6303,0,0.0252993,"r Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013)"
P14-2110,P13-1018,0,0.289082,"comparable documents extracted from Wikipedia in different languages – fail to capture the specific terminology of social media. Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daum´e, 2010) similarly fail to adapt to shifting vocabularies. The result: an inability to train polylingual models on social media. In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages. Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message (Ling et al., 2013). This mixture of languages in the same context suggests alignments between words across languages through the common topics discussed in the context. We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics. Our model improves both in terms of perplexity and a human evaluation, and we provide some example analyses of social media that rely on our learned topics. Introduction Topic models (Blei et al., 2003) have become standard tools for analyz"
P14-2110,D09-1092,0,0.0740528,"Missing"
P14-2110,C98-1002,0,\N,Missing
P14-2110,D10-1124,0,\N,Missing
P14-2110,Q14-1007,0,\N,Missing
P15-2062,D12-1032,1,0.598705,"due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several datasets to support further work on this task.1 Methods for name matching, an important component to support downstream tasks such as entity linking and entity clustering, have focuse"
P15-2062,W04-3248,0,0.113265,"Missing"
P15-2062,N03-1003,0,0.170748,"clustering task. 1 Introduction A key technique in entity disambiguation is name matching: determining if two mention strings could refer to the same entity. The challenge of name matching lies in name variation, which can be attributed to many factors: nicknames, aliases, acronyms, and differences in transliteration, among others. In light of these issues, exact string match can lead to poor results. Numerous downstream tasks benefit from improved name matching: entity coreference (Strube et al., 2002), name transliteration (Knight and Graehl, 1998), identifying names for mining paraphrases (Barzilay and Lee, 2003), entity linking (Rao et al., 2013) and entity clustering (Green et al., 2012). As a result, there have been numerous proposed name matching methods (Cohen et al., 2003), with a focus on person names. Despite extensive exploration of this task, most work has focused on IndoEuropean languages in general and English in particular. These languages use alphabets as representations of written language. In contrast, other languages use logograms, which represent a word 2 Name Matching Methods Name matching originated as part of research into record linkage in databases. Initial work focused 1 The co"
P15-2062,D07-1020,0,0.0367092,"0.05) Features ALL - Jaccard similariy - Levenshtein - Simplified pairs - Pinyin pairs - Others Exact match Jaro-winkler Levenshtein Transducer SVM Precision 84.55 84.87 83.16 90.33 90.05 Dev Recall 57.46 58.35 61.13 74.92 63.90 F1 68.42 69.15 70.46 81.90 74.75 Precision 63.95 70.79 69.56 73.59 74.33 Test Recall 65.44 66.21 67.27 63.70 67.60 F1 64.69 68.42 68.40 68.29 70.81 Table 7: Results on Chinese entity clustering. (cross document coreference resolution), where the goal is identify co-referent named mentions across documents. Only a few studies have considered Chinese entity clustering (Chen and Martin, 2007), including the TAC KBP shared task, which has included clustering Chinese NIL mentions (Ji et al., 2011). We construct an entity clustering dataset from the TAC KBP entity linking data. All of the 2012 Chinese data is used as development, and the 2013 data as test. We use the system of Green et al. (2012), which allows for the inclusion of arbitrary name matching metrics. We follow their setup for training and evaluation (B3 ) and use TF-IDF context features. We tune the clustering cutoff for their hierarchical model, as well as the name matching threshold on the development data. For the tra"
P15-2062,W10-4152,0,0.186548,"chnology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 2 Machine Intelligence and Translation Lab Harbin Institute of Technology, Harbin, China npeng1@jhu.edu, gflfof@gmail.com, mdredze@cs.jhu.edu Abstract or morpheme, the most popular being Chinese which uses hanzi (汉字). This presents challenges for name matching: a small number of hanzi represent an entire name and there are tens of thousands of hanzi in use. Current methods remain largely untested in this setting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in deter"
P15-2062,W12-4511,0,0.0715765,"Missing"
P15-2062,P14-2102,1,0.856086,"antage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al.,"
P15-2062,D08-1113,0,0.0282315,"t al., 2003). The advantage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al."
P15-2062,I11-1029,0,0.438392,"r et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al., 2011b) and cross lingual entity clustering (Green et al., 2012). However, little work has focused on logograms, with the exception of Cheng et al. (2011). As we will demonstrate in § 3, there are special challenges caused by the logogram nature of Chinese. We believe this is the first evaluation of Chinese name matching. 3 Notes simplified v.s. traditional Abbreviation and traditional v.s. simplified Transliteration of Addis Ababa in Mainland and Taiwan. Different hanzi, similar pronunciations. Transliteration of Florence in Mainland and Hong Kong. Different writing and dial"
P15-2062,J01-4004,0,0.395596,"ting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several dat"
P15-2062,W02-1040,0,0.0355989,"Missing"
P15-2062,C10-1145,0,0.0334396,"atching. We consider two new pinyin representations. Since each Chinese character corresponds to a pinyin, we take each pinyin as a token corresponding to the Chinese character. We call this “character-pinyin”. Additionally, every Mandarin syllable (represented by a pinyin) can be spelled with a combination of an initial and a final segment. Therefore, we split each pinyin token further into the initial and final segment. We call this “segmented-pinyin”5 . Name Matching as Classification An alternate learning formulation considers name matching as a classification task (Mayfield et al., 2009; Zhang et al., 2010; Green et al., 2012). Each string pair is an instance: a positive classification means that two strings can refer to the same name. This allows for arbitrary and global features of the two strings. We use an SVM with a linear kernel. To learn possible edit rules for Chinese names we add features for pairs of n-grams. For each string, we extract all n-grams (n=1,2,3) and align n-grams between strings using the Hungarian algorithm.6 Features correspond to the aligned ngram pairs, as well as the unaligned n-grams. To reduce the number of parameters, we only include features which appear in posit"
P15-2062,N12-1007,1,\N,Missing
P15-2062,J98-4003,0,\N,Missing
P16-2025,H05-1091,0,0.270977,"Missing"
P16-2025,D15-1141,0,0.569186,"Weston, 2008; Turian et al., 2010; Passos et al., 2014) In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008). Peng and Dredze (2015) showed the value of word segmentation to Chinese NER in social media by using character positional embeddings, which encoded word segmentation information. In this paper, we investigate better ways to incorporate word boundary information into an NER system for Chinese social media. We combine the state-of-the-art Chinese word segmentation system (Chen et al., 2015) with the best Chinese social media NER model (Peng and Dredze, 2015). Since both systems used learned representations, we propose an integrated model that allows for joint training learned representations, providing more information to the NER system about hidden representations learned from word segmentation, as compared to features based on segmentation output. Our integrated model achieves nearly Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily i"
P16-2025,N15-1075,0,0.0191101,"Missing"
P16-2025,W99-0613,0,0.0410919,"Missing"
P16-2025,C10-1032,1,0.717561,"Missing"
P16-2025,W12-6307,0,0.124244,"Missing"
P16-2025,P12-1055,0,0.0341318,"Missing"
P16-2025,W10-0713,1,0.625542,"Missing"
P16-2025,P16-1101,0,0.199744,"Missing"
P16-2025,fromreide-etal-2014-crowdsourcing,0,0.0182721,"Missing"
P16-2025,I08-4013,0,0.161313,"Missing"
P16-2025,W03-0430,0,0.0659674,"Missing"
P16-2025,J05-4005,0,0.0479136,"Missing"
P16-2025,W12-6321,0,0.0509478,"Missing"
P16-2025,W14-1609,0,0.0497518,"Missing"
P16-2025,D15-1064,1,0.774481,"e@cs.jhu.edu Abstract other formal domains. While this gap is shrinking in English (Ritter et al., 2011; Cherry and Guo, 2015), it remains large in other languages, such as Chinese (Peng and Dredze, 2015; Fu et al., 2015). One reason for this gap is the lack of robust up-stream NLP systems that provide useful features for NER, such as part-of-speech tagging or chunking. Ritter et al. (2011) annotated Twitter data for these systems to improve a Twitter NER tagger, however, these systems do not exist for social media in most languages. Another approach has been that of Cherry and Guo (2015) and Peng and Dredze (2015), who relied on training unsupervised lexical embeddings in place of these upstream systems and achieved state-of-the-art results for English and Chinese social media, respectively. The same approach was also found helpful for NER in the news domain (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014) In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008). Peng and Dredze (2015) showed the value of word segmentation to Chinese NER in social media by using characte"
P16-2025,I08-4010,0,0.0359614,"Missing"
P16-2025,P11-1138,0,0.0608049,"Missing"
P16-2025,N16-1030,0,0.374874,"Missing"
P16-2025,D11-1141,0,0.162071,"Missing"
P16-2025,P10-1040,0,0.110457,"Missing"
P16-2025,N15-1069,0,0.0128449,"on data is from the news domain, whereas the NER data is from social media. While it is well known that segmentation systems trained on news do worse on social media (Duan et al., 2012), we still show large improvements in applying our model to these different domains. It may be that we are able to obtain better results in the case of domain mismatch because we integrate the representations of the LSTM model directly into our CRF, as opposed to only using the predictions of the LSTM segmentation model. We plan to consider expanding our model to explicitly include domain adaptation mechanisms (Yang and Eisenstein, 2015). Discussion Huang et al. (2015) first proposed recurrent neural networks stacked with a CRF for sequential tagging tasks, as was applied to POS, chunking and NER tasks. More recent efforts have been made to add character level modeling and explore different types of RNNs (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016). These methods have achieved state-of-the-art results for NER on English news and several other Indo-European languages. However, this work has not considered languages that require word segmentation, nor do they consider social media. We can view our method as multi"
P16-2025,P11-1037,0,0.0512676,"Missing"
P16-2025,W06-0126,0,0.164592,"Missing"
P16-2025,D13-1061,0,0.0421534,"Missing"
P16-2025,E17-2113,0,\N,Missing
P18-1130,P16-1231,0,0.345275,"rations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1130,P15-1034,0,0.0202817,"ance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-t"
P18-1130,D15-1041,0,0.0343909,"eft-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting"
P18-1130,D16-1211,0,0.184897,"Missing"
P18-1130,D17-1209,0,0.0187661,"nt dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-base"
P18-1130,D12-1133,0,0.0721718,"Missing"
P18-1130,W06-2920,0,0.707126,"in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the"
P18-1130,D14-1082,0,0.255712,"2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performan"
P18-1130,D16-1238,0,0.624302,"ith previous top-performing systems for comparison. Note that the results of S TACK P TR and our reimplementation of B I AF are the average of 5 repetitions instead of a single run. Our Full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers. Our 1408 System Chen and Manning (2014) Ballesteros et al. (2015) Dyer et al. (2015) Bohnet and Nivre (2012) Ballesteros et al. (2016) Kiperwasser and Goldberg (2016) Weiss et al. (2015) Andor et al. (2016) Kiperwasser and Goldberg (2016) Wang and Chang (2016) Cheng et al. (2016) Kuncoro et al. (2016) Ma and Hovy (2017) B I AF: Dozat and Manning (2017) B I AF: re-impl S TACK P TR: Org S TACK P TR: +gpar S TACK P TR: +sib S TACK P TR: Full T T T T T T T T G G G G G G G T T T T English UAS LAS 91.8 89.6 91.63 89.44 93.1 90.9 93.33 91.22 93.56 91.42 93.9 91.9 94.26 92.41 94.61 92.79 93.1 91.0 94.08 91.82 94.10 91.49 94.26 92.06 94.88 92.98 95.74 94.08 95.84 94.21 95.77 94.12 95.78 94.12 95.85 94.18 95.87 94.19 Chinese UAS LAS 83.9 82.4 85.30 83.72 87.2 85.7 87.3 85.9 87.65 86.21 87.6 86.1 – – – – 86.6 85.1 87.55 86.23 88.1 85.7 88.87 87.30 89.05 87.74 89.30 88.23 90.43 8"
P18-1130,Q16-1026,0,0.0270272,"to be introduced. The predefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofe"
P18-1130,P15-1033,0,0.0865232,"ly (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings o"
P18-1130,C96-1058,0,0.785418,"timent analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies"
P18-1130,W15-0802,1,0.779907,"s the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of mu"
P18-1130,Q16-1023,0,0.260785,"graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based arc"
P18-1130,P10-1001,0,0.638136,"ine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have"
P18-1130,D10-1125,0,0.0229124,"92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (201"
P18-1130,D16-1180,0,0.694907,"based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic"
P18-1130,P81-1022,0,0.781098,"Missing"
P18-1130,P14-1130,0,0.0161117,"26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precisi"
P18-1130,D13-1203,0,0.0244038,"d parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen"
P18-1130,N15-1142,0,0.0305836,"ned by certain head words. Our parser follows a similar kind of annotation process: starting from reading the whole sentence, and processing in a top-down manner by finding the main predicates first and only then search for sub-trees governed by them. When making latter decisions, the parser has access to the entire structure built in earlier steps. 3.8 Implementation Details Pre-trained Word Embeddings. For all the parsing models in different languages, we initialize word vectors with pretrained word embeddings. For Chinese, Dutch, English, German and Spanish, we use the structured-skipgram (Ling et al., 2015) embeddings. For other languages we use Polyglot embeddings (Al-Rfou et al., 2013). Optimization. Parameter optimization is performed with the Adam optimizer (Kingma and Ba, 2014) with β1 = β2 = 0.9. We choose an initial learning rate of η0 = 0.001. The learning rate η is annealed by multiplying a fixed decay rate ρ = 0.75 when parsing performance stops increasing on validation sets. To reduce the effects of “gradient exploding”, we use gradient clipping of 5.0 (Pascanu et al., 2013). Dropout Training. To mitigate overfitting, we apply dropout (Srivastava et al., 2014; Ma et al., 2017). For BL"
P18-1130,D15-1166,0,0.0166961,"e stack σ. Children: ch(wi ) denotes the list of all the children (modifiers) of word wi . 2.2 Pointer Networks Pointer Networks (P TR -N ET) (Vinyals et al., 2015) are a variety of neural network capable of learning the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model cannot be trivially expressed by standard sequence-to-sequence networks (Sutskever et al., 2014) due to the variable number of input positions in each sentence. P TR -N ET solves the problem by using attention (Bahdanau et al., 2015; Luong et al., 2015) as a pointer to select a member of the input sequence as the output. Formally, the words of the sentence x are fed one-by-one into the encoder (a multiple-layer bidirectional RNN), producing a sequence of encoder hidden states si . At each time step t, the decoder (a uni-directional RNN) receives the input from last step and outputs decoder hidden state ht . The attention vector at is calculated as follows: eti = score(ht , si ) at = softmax (et ) (1) where score(·, ·) is the attention scoring function, which has several variations such as dot-product, 1404 2 2 3 3 4 s1 s2 s3 s4 s5 s6 h1 $ Bu"
P18-1130,D15-1154,1,0.801978,"us et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the treebanks and experimental settings are in § 4.5 and Appendix B. Evaluation Metrics Parsing performance is measured with five metrics: unlabeled attachment score (UAS), labeled attachment score (LAS), unlabeled complete match (UCM), labeled complete match (LCM), and root accuracy (RA). Following previous work (Kuncoro et al., 2016; Dozat and Manning, 2017), we report results excluding punctuations for Chinese and English. For each experiment, we report the mean values with corresponding standard deviations over 5 repetitions. 1407"
P18-1130,P16-1101,1,0.832214,"edefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofeed into with word ve"
P18-1130,I17-1007,1,0.717476,"0.21] 90.10±0.27 [87.05±0.26] 93.25±0.05 [93.17±0.05] 94.77±0.05 [93.21±0.10] 93.38±0.08 [91.92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arc"
P18-1130,N16-1116,1,0.871211,"Missing"
P18-1130,P14-1126,1,0.849815,"tep towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of op"
P18-1130,C12-2077,1,0.903676,"l Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an incremental, topdown, depth-first fashion;"
P18-1130,J93-2004,0,0.0662464,"idden states and 0.33 between layers. Following Dozat and Manning (2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (201"
P18-1130,P13-2109,0,0.0287562,"6 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advan"
P18-1130,J11-1007,0,0.635811,"McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive s"
P18-1130,E06-1011,0,0.718934,"ly 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an"
P18-1130,H05-1066,0,0.819927,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,P10-1142,0,0.0278987,"ition-based parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zh"
P18-1130,D09-1143,0,0.011773,"ate-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially"
P18-1130,C04-1010,0,0.0584363,"rence resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information,"
P18-1130,Q17-1008,1,0.818078,"Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build d"
P18-1130,D11-1022,0,0.029926,"[90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does no"
P18-1130,petrov-etal-2012-universal,0,0.10217,"Missing"
P18-1130,P05-1012,0,0.857049,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,N15-1068,0,0.019183,"t Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furthermore, the S TACK P TR parser"
P18-1130,W08-2121,0,0.156465,"Missing"
P18-1130,P15-1150,0,0.0877003,"Missing"
P18-1130,P16-1218,0,0.102311,"Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the co"
P18-1130,P15-1032,0,0.190239,"Missing"
P18-1130,C02-1145,0,0.226098,"2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the"
P18-1130,W03-3023,0,0.337352,"applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is base"
P18-1130,P14-2107,0,0.015528,"79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furtherm"
P18-1130,D14-1109,0,0.0152966,"49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further a"
P18-1130,P11-2033,0,0.0696926,"10; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propag"
P18-1130,W13-3520,0,\N,Missing
Q15-1031,N15-1094,1,0.837701,"ynamically restrict to a finite support set of plausible values for m. We take this to be the union of the 20-best lists of all messages sent to m.9 We then prune those messages so that they give weight 0 to all strings outside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update"
Q15-1031,P14-2102,1,0.677762,"(after erasing #) to the surface [wER@r]. At the point shown, it is applying the “intervocalic alveolar flapping” rule, replacing /t/ in this context by applying SUBST(R). cape such low-likelihood solutions, much as backtracking escapes zero-likelihood solutions. 3.2 Mapping URs to SRs: The phonology Sθ We currently model Sθ (s |u) as the probability that a left-to-right stochastic contextual edit process (Figure 2) would edit u into s. This probability is a sum over all edit sequences that produce s from u—that is, all s-to-u alignments. Stochastic contextual edit processes were described by Cotterell et al. (2014). Such a process writes surface string s ∈ Σ∗s while reading the underlying string u ∈ Σ∗u . If the process has so far consumed some prefix of the input and produced some prefix of the output, it will next make a stochastic choice among 2|Σs |+ 1 possible edits. Edits of the form SUBST(c) or INSERT(c) (for c ∈ Σs ) append c to the output string. Edits of the form SUBST(c) or DELETE will (also) consume the next input phoneme; if no input phonemes remain, the only possible edits are INSERT(c) or HALT. The stochastic choice of edit, given context, is governed by a conditional log-linear distribut"
Q15-1031,D09-1011,1,0.359415,"vely. Each variable’s distribution is conditioned on the values of its parents, if any. Layer 1 represents the unknown M (a) for various a. Notice that each M (a) is softly constrained by the prior Mφ , and also by its need to help produce various observed surface words via Sθ . Each underlying word u at level 2 is a concatenation of its underlying morphs M (ai ) at level 1. Thus, the topology at levels 1–2 is given by supervision. We would have to learn this topology if the word’s morphemes ai were not known. Our approach captures the unbounded generative capacity of language. In contrast to Dreyer and Eisner (2009) (see section 8), we have defined a directed graphical model. Hence new unobserved descendants can be added without changing the posterior distribution over the existing variables. So our finite network can be viewed as a subgraph of an infinite graph. That is, we make no closed-vocabulary assumption, but implicitly include (and predict the surface forms of) any unobserved words that could result from combining morphemes, even morphemes not in our dataset. While the present paper focuses on word types, we could extend the model to consider tokens as well. In Figure 1, each phonological surface"
Q15-1031,D11-1057,1,0.865251,"Missing"
Q15-1031,D10-1056,0,0.0117644,"ctor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u"
Q15-1031,D08-1113,1,0.747087,"morphs, either identifying a small set of plausible morphs or prohibiting segmental insertion/deletion. We use finite-state methods so that it is possible to consider the space Σ∗u of all strings. 13 She still assumes that word SRs are annotated with morpheme boundaries, and that a small set of possible morphs is given. These assumptions are relaxed by Eisenstat (2009). On the other hand, we are divided from previous work by our inability to use an OT grammar (Prince and Smolensky, 2004), a stochastic OT grammar (Boersma, 1997), or even a maximum entropy grammar (Goldwater and Johnson, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembl"
Q15-1031,P02-1008,1,0.80485,"y motivation for probabilistic models of phonology (Pierrehumbert, 2003) has been to explain “soft” phenomena: synchronic variation (Sankoff, 1978; Boersma and Hayes, 2001) or graded acceptability judgments on novel surface forms (Hayes and Wilson, 2008). These applications are orthogonal to our motivation, as we do not observe any variation or gradience in our present experiments. Fundamentally, we use probabilities to measure irregularity—which simply means unpredictability and is a matter of degree. Our objective function will quantitatively favor explanations that show greater regularity (Eisner, 2002b). A probabilistic treatment also allows relatively simple learning methods (e.g., Boersma and Hayes (2001)) since inference never has to backtrack from a contradiction. Our method searches a continuous space of phonologies Sθ , all of which are consistent with every mapping S. That is, we always have Sθ (s |u) > 0 for all u, s, so our current guess of Sθ is always capable of explaining the observed words, albeit perhaps with low probability. Our EM learner tunes Sθ (and Mφ ) so as to raise the probability of the observed surface forms, marginalizing over the reconstructed lexicon M of underl"
Q15-1031,P12-1020,0,0.139946,"spect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for structured prediction of the surface word s from the underlying word u. If s is not fully observed during training (layer 4 of Figure 1 is observed, not layer 3), the"
Q15-1031,D13-1005,0,0.197829,"n, 2003; Dreyer et al., 2008; Eisenstat, 2009). The reason is that our BP method inverts the phonological mapping Sθ to find possible word URs. Given a word SR s, we construct a WFSM (message) that scores every possible UR u ∈ Σ∗u —the score of u is Sθ (s |u). For this to be possible without approximation, Sθ itself must be represented as a WFSM (section 3.2). Unfortunately, the WFSM for a maximum entropy grammar does not compute Sθ but only an unnormalized version, with a different normalizing constant Zu needed for each u. We plan to confront this issue in future work. In the NLP community, Elsner et al. (2013) resembles our work in many respects. Like us, they recover a latent underlying lexicon (using the same simple prior Mφ ) and use EM to learn a phonology (rather similar to our Sθ , though less powerful).14 Unlike us, they do not assume annotation of the (abstract) morpheme sequence, but jointly learn a nonparametric bigram model to discover the morphemes. Their evaluation is quite different, as their aim is actually to recover underlying words from phonemically transcribed child-directed English utterances. However, nothing in their model distinguishes words from morphemes—indeed, sometimes t"
Q15-1031,E12-1068,0,0.0103876,"obabilistic finite-state transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabi"
Q15-1031,P10-1105,0,0.0723142,"the partition function and requires retraining. By contrast, our trained directed model is a productive phonological system that can generate unboundedly many new words (see section 4.1). By analogy, n samples from a Gaussian would be described with a directed model, and inferring the Gaussian parameters predicts any number of future samples n + 1, n + 2, . . .. Bouchard-Cˆot´e et al., in several papers from 2007 through 2013, have used directed graphical models over strings, like ours though without loops, to model diachronic sound change. Sometimes they use belief propagation for inference (Hall and Klein, 2010). Their goal is to recover latent historical forms (conceptually, surface forms) rather than latent underlying forms. The results are evaluated against manual reconstructions. None of this work has segmented words into morphs, although Dreyer et al. (2008) did segment surface words into latent “regions.” Creutz and Lagus (2005) and Goldsmith (2006) segment an unannotated collection of words into reusable morphs, but without modeling contextual sound change, i.e., phonology. 9 Conclusions and Future Work We have laid out a probabilistic model for generative phonology. This lets us infer likely"
Q15-1031,N12-1032,0,0.0283479,"ate transducer). Layer 4 derives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, w"
Q15-1031,W09-0803,0,0.0294032,"Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we can adopt morphological doubling theory (Inkelas and Zoll, 2005) and model it by having U concatenate two copies of the same morph. During inference of URs, this morph exchanges messages with two substring"
Q15-1031,W06-3207,0,0.0268686,"the SRs. (Tesar and Merchant instead used binary variables, one for each segmental feature in each UR—requiring the simplifying assumption that the URs are known except for their segmental features. They assume that SRs are annotated with morph boundaries and that the phonology only changes segmental features, never inserting or deleting segments.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—wh"
Q15-1031,J94-3001,0,0.484864,"t layers 1, 2, and 3 given the values at their parents. As section 3 models these, for any φ and θ, we can represent Mφ as a 1-tape WFSM (acceptor), U as a multi-tape WFSM, and Sθ as a 2-tape WFSM (transducer).6 Any other WFSMs could be substituted. We are on rather firm ground in restricting to finite-state (regular) models of Sθ . The apparent regularity of natural-language phonology was first observed by Johnson (1972), so computational phonology has generally preferred grammar formalisms that compile into (unweighted) finite-state machines, whether the formalism is based on rewrite rules (Kaplan and Kay, 1994) or constraints (Eisner, 2002a; Riggle, 2004). Similarly, U could be any finite-state relation,7 not just concatenation as in section 2. Thus our framework could handle templatic morphology (Hulden, 2009), infixation, or circumfixation. Although only regular factors are allowed in our graphical model, a loopy graphical model with multiple such factors can actually capture nonregular phenomena, for example by using auxiliary variables (Dreyer and Eisner, 2009, §3.4). Approximate inference then proceeds by loopy BP on this model. In particular, reduplication is not regular if unbounded, but we c"
Q15-1031,D09-1005,1,0.27958,"al edits (section 3.2), averaged over edit contexts in proportion to how many times those contexts were likely encountered. The latent alignment makes the objective non-concave. In our EM setting, uk is not known. So our Mstep P replaces log Sθ (sk |uk ) with its expectation, uk bk (uk ) log Sθ (sk |uk ), where bk is the normalized belief about uk computed by the previous E-step. Since bk and Sθ are both represented by WFSMs (with 1 and 2 tapes respectively), it is possible to compute this quantity and its gradient exactly, using finite-state composition in a secondorder expectation semiring (Li and Eisner, 2009). For speed, however, we currently prune bk back to the 5-best values of uk . This lets us use a simpler and faster approach: a weighted average over 5 runs of the Cotterell et al. (2014) algorithm. Our asymptotic runtime benefits from the fact that our graphical model is directed (so our objective does not have to contrast with all other values of uk ) and the fact that Sθ is locally normalized (so our objective does not have to contrast with all other values of sk for each uk ). In practice we are far faster than Dreyer and Eisner (2009). We initialized the parameter vector θ to ~0, except f"
Q15-1031,W12-2308,0,0.0252249,"ents.) On the other hand, Tesar and Merchant reason globally about the constraint ranking, whereas in this paper, we only locally improve the phonology—we use EM, rather than the full Bayesian approach that treats the parameters θ~ as variables within BP. Jarosz (2006) is closest to our work in that she uses EM, just as we do, to maximize the probability of observed surface forms whose constituent morphemes (but not morphs) are known.13 Her model is a probabilistic analogue of Apoussidou (2006), who uses a latent-variable structured perceptron. A non-standard aspect of this model (defended by Pater et al. (2012)) is that a morpheme a can stochastically choose different morphs M (a) when it appears in different words. To obtain a single shared morph, one could penalize this distribution’s entropy, driving it toward 0 as learning proceeds. Such an approach—which builds on a suggestion by Eisenstat (2009, §5.4)—would loosely resemble dual decomposition (Peng et al., 2015). Unlike our BP approach, it would maximize rather than marginalize over possible underlying morphs. Our work has focused on scaling up inference. For the phonology S, the above papers learn the weights or rankings of just a few plausib"
Q15-1031,D15-1108,1,0.912515,"utside m’s support set. As a result, m’s outgoing messages and belief are also confined to its support set. Note that the support set is not handspecified, but determined automatically by taking the top hypotheses under the probability model. Improved approaches with no pruning are possible. After submitting this paper, we developed a penalized expectation propagation method (Cotterell and Eisner, 2015). It dynamically approximates messages using log-linear functions (based on variable-order n-gram features) whose support is the entire space Σ∗ . We also developed a dual decomposition method (Peng et al., 2015), which if it converges, exactly recovers the single most 8 This is standard—although the uniform distribution over the space of strings is actually an improper distribution. It is expressed by a single-state WFSM whose arcs have weight 1. It can be shown that the beliefs are proper distributions after one iteration, though the upward messages may not be. 9 In general, we should update this support set dynamically as inference and learning improve the messages. But in our present experiments, that appears unnecessary, since the initial support set always appears to contain the “correct” UR. B"
Q15-1031,D13-1007,0,0.015784,"(Maori) in all the training SRs. However, a contributing factor may be the increased error rate of the phonological learner, visible even with oracle data. Thus, we suspect that a Sθ model with better generalization would improve our results at all training sizes. Note that weakening Sθ —allowing only “noisy concatenation”—clearly harms the method, proving the need for true phonological modeling. 8 Related Work We must impute the inputs to the phonological noisy channel Sθ (URs) because we observe only the outputs (SRs). Other NLP problems of this form include unsupervised text normalization (Yang and Eisenstein, 2013), unsupervised training of HMMs (Christodoulopoulos et al., 2010), and particularly unsupervised lexicon acquisition from phonological data (Elsner et al., 2012). However, unlike these studies, we currently use some indirect supervision—we know each SR’s morpheme sequence, though not the actual morphs. Jarosz (2013, §2) and Tesar (2014, chapters 5– 6) review work on learning the phonology Sθ . Phonologists pioneered stochastic-gradient and passive-aggressive training methods—the Gradual Learning Algorithm (Boersma, 1998) and ErrorDriven Constraint Demotion (Tesar and Smolensky, 1998)—for struc"
Q15-1031,P11-3019,0,0.0257895,"rives observable phonetic forms from layer 3. This deletes unpronounced symbols such as syllable boundaries, and translates the phonemes into an observed phonetic, articulatory, or acoustic representation. However, our present paper simply merges layers 3 and 4: our layer 3 does not currently make use of any unpronounced symbols (e.g., syllable boundaries) and we observe it directly. to model surface spellings (as needed for MT on text). Good morphological analysis has been used to improve NLP tasks such as machine translation, parsing, and NER (Fraser et al., 2012; Hohensee and Bender, 2012; Yeniterzi, 2011). Using loopy belief propagation, this paper attacks larger-scale learning problems than prior work on this task (section 8). We also develop a new evaluation paradigm that examines how well an inferred grammar predicts held-out SRs. Unlike previous algorithms, we do not pre-restrict the possible URs for each morpheme to a small or structured finite set, but use weighted finite-state machines to reason about the infinite space of all strings. Our graphical model captures the standard assumption that each morpheme has a single UR, unlike some probabilistic learners. However, we do not try to le"
Q17-1008,P98-1013,0,0.0391779,"2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence. McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities. Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth 111 and Lapata, 2016). These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each arg"
Q17-1008,H05-1091,0,0.217196,"rst review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of"
Q17-1008,P16-1072,0,0.0800421,"g powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) styl"
Q17-1008,C10-1018,0,0.0365625,"cularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity"
Q17-1008,M98-1001,0,0.238877,"-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory netwo"
Q17-1008,de-marneffe-etal-2006-generating,0,0.120922,"Missing"
Q17-1008,D15-1112,0,0.0433277,"Missing"
Q17-1008,P10-1160,0,0.211478,"AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguis"
Q17-1008,P05-1053,0,0.36783,"tic parses, suggesting that encoding high-quality analysis is particularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue)"
Q17-1008,W09-1401,0,0.0236307,"to the n-ary setting is challenging, as there are n2 paths. One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on gra"
Q17-1008,J13-4004,0,0.0121401,"ected acyclic graphs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of"
Q17-1008,P14-5010,0,0.00700447,"ody of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples. However, unlike the single-sentence setting in"
Q17-1008,P05-1061,0,0.489844,"tation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework"
Q17-1008,P09-1113,0,0.740799,"phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs. By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineeri"
Q17-1008,P16-1105,0,0.26527,"sing a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers are trained jointly wit"
Q17-1008,P14-2012,0,0.0332938,"Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such repre"
Q17-1008,J05-1004,0,0.184752,"P DEP The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we expl"
Q17-1008,P16-2025,1,0.847562,"dge-type embedding. Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix. The new computation is as follows: 3.5 X Multi-task Learning with Sub-relations Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert ftj = σ(Wf xt + Uf ×T (hj ⊗ ej ) + bf ) and Weston, 2008; Peng and Dredze, 2016). By X learning contextual entity representations, our frameUo ×T (hj ⊗ ej ) + bo ) ot = σ(Wo xt + j∈P (t) work makes it straightforward to conduct multi-task X Uc ×T (hj ⊗ ej ) + bc ) learning. The only change is to add a separate classic˜t = tanh(Wc xt + j∈P (t) X fier for each related auxiliary relation. All classifiers ftj cj ct = it c˜t + share the same graph LSTMs representation learner j∈P (t) and word embeddings, and can potentially help each ht = ot tanh(ct ) other by pooling their supervision signals. U ’s are now l × l × d tensors (l is the dimension of In the molecular tumor board"
Q17-1008,D14-1162,0,0.0882041,"Missing"
Q17-1008,C08-1088,0,0.0226594,"n the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolu"
Q17-1008,E17-1110,1,0.202831,"icity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction. Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary re"
Q17-1008,N12-3006,1,0.827666,"f papers contain knowledge about drug-gene-mutation interactions. Extracting such knowledge from the vast body of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring trip"
Q17-1008,reschke-etal-2014-event,0,0.0164171,"ent, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary su"
Q17-1008,P16-1113,0,0.055469,"Missing"
Q17-1008,P15-1061,0,0.43501,"iLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency"
Q17-1008,D12-1110,0,0.158707,"automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker"
Q17-1008,R11-1004,0,0.156856,"gument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant super"
Q17-1008,P15-1150,0,0.811543,"hese problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers"
Q17-1008,P16-1123,0,0.436558,"Missing"
Q17-1008,W06-1671,0,0.152345,"ombining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of div"
Q17-1008,D15-1062,0,0.540872,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,D15-1206,0,0.709967,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,C16-1138,0,0.0397931,"egrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchi"
Q17-1008,K15-2001,0,0.021367,"hs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word."
Q17-1008,C14-1220,0,0.860338,"baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on th"
Q17-1008,D15-1203,0,0.237949,"h distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary sub-relations can improve performance. Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can al"
Q17-1008,Y15-1009,0,0.0475333,"fully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on mo"
Q17-1008,C98-1013,0,\N,Missing
W17-2612,D16-1001,0,0.00703582,"Missing"
W17-2612,P07-1033,0,0.583604,"Missing"
W17-2612,D08-1072,1,0.830941,"anguage Technology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract 2006) and supervised (Daum´e III, 2007) variants, depending on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and"
W17-2612,P15-1033,0,0.00558804,"is only one task. 2.1 BiLSTM for representation learning Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) that models interdependencies in sequential data. It addresses the vanishing or exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) problems of vanilla RNNs by using a series of gates (input, forget and output gates) to control how memory is propagated in the hidden states of the model, and thus effectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for"
W17-2612,P16-1101,0,0.393488,"gnition in social media data. 2 Projec@on for domain one … yn Decoders for y1 task T … … Shared Representa@on Learner … yn y1 … framework based on MTL that incorporates parameter tying strategies common in domain adaptation. Our framework is based on a bidirectional long short-term memory network with a conditional random fields (BiLSTM-CRFs) (Lample et al., 2016) for sequence tagging. We consider sequence tagging problem since they are common in NLP applications and have been demonstrated to benefit from learning representations (Lample et al., 2016; Yang et al., 2016; Peng and Dredze, 2016; Ma and Hovy, 2016). This paper makes the following contributions: The shared representation learner, domain projections and task specific models can be instantiated based on the application. In this paper, we focus on sequence tagging problems. We now introduce our instantiated neural architecture for multitask domain adaptation for sequence tagging. Model We begin with a brief overview of our model, and then instantiate each layer with specific neural architectures to conduct multi-task domain adaptation for sequence tagging. Figure 1 summarizes the entire model presented in this section. A representation lear"
W17-2612,I05-3017,0,0.0473747,"Missing"
W17-2612,N09-1068,0,0.152487,"Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract 2006) and supervised (Daum´e III, 2007) variants, depending on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al.,"
W17-2612,D15-1064,1,0.0571464,"ck-propagation. We output spaces need separate task specific models. use alternating optimization among each dataset For our applications to sequence tagging probwith stochastic gradient descent (SGD). To prelems, we choose Conditional Random Fields vent training from skewing the model to a specific (CRFs) (Lafferty et al., 2001) as task specific moddataset due to the optimization order, we subsamels, since it is widely used in previous work and ple the number of instances used in each epoch is shown to benefit from learning representations with a fraction λ w.r.t. the smallest dataset size, (Peng and Dredze, 2015; Lample et al., 2016; Ma which is tuned as a hyper-parameter on developand Hovy, 2016). These “Neural-CRFs” define ment data. A separate learning rate is tuned for the conditional probability of a sequence of labels each dataset, and we decay the learning rate when given the input as: results on development data do not improve af Qn k , y k , ψ(xk )) ter 5 consecutive epochs. We train for up to 30 exp W T F (yi−1 i p(y k |xk ; W ) = i=1 ,epochs and use early stopping (Caruana et al., Zk 2001; Graves et al., 2013) as measured on develwhere i indexes the position in the sequence, F is the feat"
W17-2612,D12-1119,1,0.0785808,"Missing"
W17-2612,P16-2025,1,0.943052,"et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al., 2016c; Peng and Dredze, 2016; Yang et al., 2016; Liu et al., 2016a). Given the similarity between domain adaptation and MTL, it is natural to ask: can domain adaptation benefit from jointly learning across several tasks? This paper investigates how MTL can induce better representations for domain adaptation. There are several benefits. First, learning multiple tasks provides more training data for learning. Second, MTL provides a better inductive learning bias so that the learned representations better generalize. Third, considering several tasks in domain adaptation opens up the opportunities to adapt from a different d"
W17-2612,Q16-1023,0,0.00609985,"between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representation for the whole sequence is thus denoted as h = h1:n . 2.2 m2 = [~1, ~0, ~"
W17-2612,Q17-1008,1,0.873395,"Missing"
W17-2612,P16-2038,0,0.0861928,"Missing"
W17-2612,N16-1030,0,0.419543,"ask specific models (top layer) contain one model per task. • A new domain/task mismatch setting: where you have two datasets from two different, but related domains and tasks. • State-of-the-art results on Chinese word segmentation and named entity recognition in social media data. 2 Projec@on for domain one … yn Decoders for y1 task T … … Shared Representa@on Learner … yn y1 … framework based on MTL that incorporates parameter tying strategies common in domain adaptation. Our framework is based on a bidirectional long short-term memory network with a conditional random fields (BiLSTM-CRFs) (Lample et al., 2016) for sequence tagging. We consider sequence tagging problem since they are common in NLP applications and have been demonstrated to benefit from learning representations (Lample et al., 2016; Yang et al., 2016; Peng and Dredze, 2016; Ma and Hovy, 2016). This paper makes the following contributions: The shared representation learner, domain projections and task specific models can be instantiated based on the application. In this paper, we focus on sequence tagging problems. We now introduce our instantiated neural architecture for multitask domain adaptation for sequence tagging. Model We begi"
W17-2612,D15-1206,0,0.00752568,"del, and thus effectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representat"
W17-2612,W06-0115,0,0.0326581,"Missing"
W17-2612,N15-1069,0,0.0923214,"ing on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al., 2016c; Peng and Dredze, 2016; Yang et al., 2016; Liu et al., 2016a). Given the similarity between domain adaptation and MTL, it is natural to ask: can domain adaptation benefit from"
W17-2612,D16-1012,0,0.0180855,"Missing"
W17-2612,P13-2032,0,0.0648576,"Missing"
W17-2612,Y15-1009,0,0.0049188,"ectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representation for the whole seq"
W17-2612,W06-1615,0,\N,Missing
W18-1505,P17-4008,1,0.80681,": An overview (upper) and an example (lower) of the proposed analyze-to-generate story framework. erate stories. Martin et al. (2017) train a recurrent encoder-decoder neural network (Sutskever et al., 2014) to predict the next event in the story. Despite significant progress in automatic story generation, there has been less emphasis on controllability: having a system takes human inputs and composes stories accordingly. With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and personalized stories. This paper emphasizes the controllability aspect. We propose a completely data-driven approach towards controllable story generation by analyzing the existing story corpora. First, an analyzer extracts control factors from existing stories, and then a generator learns to generate stories according to the control factors. This creates an excellent interface for humans to interact:"
W18-1505,P17-4012,0,0.0146337,"ence of words ki = {ki,1 , ki,2 , . . . , ki,r } from each story xi . The ki s are ordered according to their order in the story. We adapt the RAKE algorithm (Rose et al., 2010) for keyword extraction, which builds document graphs and weights the importance of each word combining several word-level and graphlevel criteria. We extract the most important word from each sentence as the storyline. Generator. The generator for storylinecontrolled generation is also a conditional language model. Specifically, we employ the seq2seq model with attention (Bahdanau et al., 2014) implemented in OpenNMT (Klein et al., 2017). Specifically, the storyline words are encoded into vectors by a BiLSTM: hk = BiLST M (k) = → − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a("
W18-1505,D15-1167,0,0.0271169,"ending valence labels to facilitate the computation. Formally, we learn an embedding matrix E l to map each label lk into a vector: li = f v (xi ), where i indexes instances. Since there is no prior work on analyzing story ending valence, we build our own analyzer by collecting some annotations for story ending valence from Amazon Mechanical Turk (AMT) and building a supervised classifier. We employ an LSTM-based logistic regression classifier as it learns feature representations that capture long-term dependencies between the words, and has been shown efficient in text classification tasks (Tang et al., 2015). ekl = E l [lk ], 44 Agreement experiment Researcher vs. Researcher Turkers vs. Researcher Classifier vs. Turkers Always happyEnding where E l is a m × p matrix that maps each label (p of them) into a m-dimensional vector. The ending valence embeddings dimension are made the same as the word embedding dimension for simplicity. We add the ending valence as follows: p(wt |w1t−1 , l; θ) =  g(V F(el , F(wt−1 , ht−1 ))), t = s g(V F(wt−1 , ht−1 )), t = others valence. Labels are happyEnding, sadEnding, or cannotTell. The automatic classifier trained on 3980 turker annotated stories achieved much"
W18-1505,D17-1228,0,0.0622079,"Missing"
W18-1505,N16-1098,0,0.0786322,"→ − ← − [ h k ; h k ], and the decoder generate each word according to the probability: att st = F (wt−1 , st−1 , ct ) r X ct = αtj hkj g() again denotes the softmax function, and V l denotes parameters that perform a linear transformation. F att () in Equation 5b denotes the computations of an LSTM-cell with attention mechanism, where the context vector ct is computed by an weighted summation of the storyline words vectors as in Equation 5c, and the weights are computed from some alignment function a() as in Equation 5d. 3 Experimental Setup We conduct experiments on the ROCstories dataset (Mostafazadeh et al., 2016), which consists of 98,162 five-line stories for training, and 1871 stories each for the development and test sets. We treat the first four sentences of each story as the body and the last sentence as the ending. We build analyzers to annotate the ending valence and the storyline for every story, and train the two controlled generators with 98,162 annotated stories. 3.1 Ending Valence Annotation We conduct a three-stage data collection procedure to gather ending valence annotations and train a classifier to analyze the whole corpora. We classify all the stories into happyEnding, sadEnding, or"
W18-1505,W17-0911,0,0.0151733,"types of texts, such as novels, movies, and news articles. Automatic story generation efforts started as early as the 1970s with the TALE-SPIN system (Meehan, 1977). Early attempts in this field relied on symbolic planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), casebased reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013). In recent years, deep learning models are used to capture higher level structure in stories. Roemmele et al. (2017) use skip-thought vectors (Kiros et al., 2015) to encode sentences, and a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to gen1 Structured Control Factors Happy or sad endings. 43 Proceedings of the First Workshop on Storytelling, pages 43–49 c New Orleans, Louisiana, June 5, 2018. 2018 Association for Computational Linguistics Specifically, we use a bidirectional-LSTM to encode an input story into a sequence of vector representations hi = {hi,1 , hi,2 , · · · , hi,T }, where → − ← − hi = BiLST M (xi ) = [ h i ; h i ], T denotes the story length, [, :, ] denotes elem"
W19-2310,D17-1151,0,0.0149919,"r dialogue evaluation (Novikova et al., 2017; Lowe et al., 2017). Due to the lack of strong automatic evaluation metrics, many researchers resort primarily to human evaluation for assessing their dialogue systems performances (Shang et al., 2015; Sordoni et al., 2015; Shao et al., 2017). There are two main problems with human annotation: 1) it is time-consuming and expensive, and 2) it does not facilitate comparisons across research papers. For certain research questions that involve hyper-parameter tuning or architecture searches, the amount of human annotation makes such studies infeasible (Britz et al., 2017; Melis et al., 2018). Therefore, developing reliable automatic evaluation metrics for open-domain dialog systems is imperative. The Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) (Tao et al., 2018) stands out from recent work in automatic dialogue evaluation, relying minimally on human-annotated datasets of response quality for training. RUBER evaluates responses with a blending of scores from two metrics: Introduction Recent advances in open-domain dialogue systems (i.e. chatbots) highlight the difficulties in automatically evaluating them. This kind of evaluati"
W19-2310,P17-4012,0,0.0147443,"I help you? Do you have some experiences to share with me? I want to have a try. Response Yes, of course. No, it was nothing to leave. Actually, it good to say. Thanks a lot. Human rating 5, 5, 5 1, 2, 1 3, 2, 2 Table 2: Examples of query-response pairs, each rated by three AMT workers with scores from 1 (not appropriate response) to 5 (completely appropriate response). dings as input to dialogue generation, referenced and unreferenced metrics. pairs to train an attention-based sequence-tosequence (seq2seq) model (Bahdanau et al., 2014) and generate responses for evaluation. We used OpenNMT (Klein et al., 2017) toolkit to train the model. The encoder and decoder are Bi-LSTMs with 2 layers each containing 500-dimensional hidden units. We used 300-dimensional pretrained word2vec embeddings as our word embeddings. The model was trained by using SGD optimizer with learning rate of 1. We used random sample with temperature control and set temperature value to 0.01 empirically to get grammatical and diverse responses. 3.2 4.2 In order to explore the effects of contextualized embedding on evaluation metrics, we used the BERT base model with 768 vector dimensions pretrained on Books Corpus and English Wikip"
W19-2310,N16-1014,0,0.0413847,"es with a blending of scores from two metrics: Introduction Recent advances in open-domain dialogue systems (i.e. chatbots) highlight the difficulties in automatically evaluating them. This kind of evaluation inherits a characteristic challenge of NLG evaluation - given a context, there might be a diverse range of acceptable responses (Gatt and Krahmer, 2018). Metrics based on n-gram overlaps such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), originally designed for evaluating machine translation and summarization, have been adopted to evaluate dialogue systems (Sordoni et al., 2015; Li et al., 2016; Su et al., 2018). However, Liu et al. (2016) found a weak segment-level correlation between these metrics and human judgments 82 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 82–89 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics Ranking Loss Pooling Bert Word2vec Embeddings Embeddings Bi-RNN What are you shopping for ? M MLP Classifier Some new clothes 0 . 1 Pooling Figure 1: An illustration of changes applied to RUBER’s unreferenced metric’s architecture. Red dotted double arrows"
W19-2310,I17-1099,0,0.0797321,"loss. We explore the efficiency of using a simpler loss function such as cross entropy. In fact, we consider unreferenced score prediction as a binary classification problem and replace baseline trained MLP with MLP classifier (right dotted section in Figure 1). Since we do not have a human labeled dataset, we use negative sampling strategy to add randomly selected responses to queries in training dataset. We assign label 1 to original pairs of queries and 3 Dataset We used the DailyDialog dataset1 which contains high quality multi-turn conversations about daily life including various topics (Li et al., 2017), to train our dialogue system as well as the evaluation metrics. This dataset includes almost 13k multi-turn dialogues between two parties splitted into 42,000/3,700/3,900 query-response pairs for train/test/validation sets. We divided these sets into two parts, the first part for training dialogue system and the second part for training unreferneced metric. 3.1 Generated responses We used the first part of train/test/validation sets with overall 20,000/1,900/1,800 query-response 1 84 http://yanran.li/dailydialog Query Can I try this one on? This is the Bell Captain’s Desk. May I help you? Do"
W19-2310,W04-1013,0,0.463787,"t from recent work in automatic dialogue evaluation, relying minimally on human-annotated datasets of response quality for training. RUBER evaluates responses with a blending of scores from two metrics: Introduction Recent advances in open-domain dialogue systems (i.e. chatbots) highlight the difficulties in automatically evaluating them. This kind of evaluation inherits a characteristic challenge of NLG evaluation - given a context, there might be a diverse range of acceptable responses (Gatt and Krahmer, 2018). Metrics based on n-gram overlaps such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), originally designed for evaluating machine translation and summarization, have been adopted to evaluate dialogue systems (Sordoni et al., 2015; Li et al., 2016; Su et al., 2018). However, Liu et al. (2016) found a weak segment-level correlation between these metrics and human judgments 82 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 82–89 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics Ranking Loss Pooling Bert Word2vec Embeddings Embeddings Bi-RNN What are you shopping for ? M M"
W19-2310,W05-0909,0,0.645971,"e to the impressive development of open domain dialogue systems, existence of automatic evaluation metrics can be particularly desirable to easily compare the quality of several models. 6.1 Automatic Heuristic Evaluation Metrics In some group of language generation tasks such as machine translation and text summarization, ngrams overlapping metrics have a high correlation with human evaluation. BLEU and METEOR are primarily used for evaluating the quality of translated sentence based on computing n-gram precisions and harmonic mean of precision and recall, respectively (Papineni et al., 2002; Banerjee and Lavie, 2005). ROUGE computes F-measure based on the longest common subsequence and is highly applicable for evaluating text summarization (Lin, 2004). The main drawback of mentioned n-gram overlap metrics, which makes them inapplicable in dialogue system evaluation is that they don’t consider the semantic similarity between sentences (Liu et al., 2016; Novikova et al., 2017; Lowe et al., 2017). These word overlapping metrics are not compatible with the nature of language generation, which allows a concept to be appeared in different sentences with no common n-grams, while they all share the same meaning."
W19-2310,P15-1152,0,0.0809049,"Missing"
W19-2310,N19-1112,0,0.0718502,"ontextualized word embeddings, and show their positive effects. 2 Unreferenced Metric • Word2vec. Recent works on learnable evaluation metrics use simple word embeddings such as word2vec and GLoVe as input to their models (Tao et al., 2018; Lowe et al., 2017; Kannan and Vinyals, 2017). Since these static embeddings have a fixed contextindependent representation for each word, they cannot represent the rich semantics of words in contexts. • BERT. Contextualized word embeddings are recently shown to be beneficial in many NLP tasks (Devlin et al., 2018; Radford et al., 2018; Peters et al., 2018; Liu et al., 2019). A noticeable contextualized word embeddings, BERT (Devlin et al., 2018), is shown to perProposed models We conduct the research under the RUBER metric’s referenced and unreferenced framework, where we replace their static word embeddings with pretrained BERT contextualized embeddings and compare the performances. We identify three points of variation with two options each in the 83 Bert Word2vec Embeddings Embeddings form competitively among other contextualized embeddings, thus we explore the effect of BERT embeddings on open domain dialogue systems evaluation task. Specifically, we substit"
W19-2310,P17-1103,0,0.578839,"ation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate relatedness scores, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings. 1 Table 1: An example of zero BLEU score for an acceptable generated response in multi-turn dialogue system of response quality. As shown in Table 1, highquality responses can have low or even no n-gram overlap with a reference response, showing that these metrics are not suitable for dialogue evaluation (Novikova et al., 2017; Lowe et al., 2017). Due to the lack of strong automatic evaluation metrics, many researchers resort primarily to human evaluation for assessing their dialogue systems performances (Shang et al., 2015; Sordoni et al., 2015; Shao et al., 2017). There are two main problems with human annotation: 1) it is time-consuming and expensive, and 2) it does not facilitate comparisons across research papers. For certain research questions that involve hyper-parameter tuning or architecture searches, the amount of human annotation makes such studies infeasible (Britz et al., 2017; Melis et al., 2018). Therefore, developing r"
W19-2310,D17-1235,0,0.0167561,"RUBER, which is trained on static embeddings. 1 Table 1: An example of zero BLEU score for an acceptable generated response in multi-turn dialogue system of response quality. As shown in Table 1, highquality responses can have low or even no n-gram overlap with a reference response, showing that these metrics are not suitable for dialogue evaluation (Novikova et al., 2017; Lowe et al., 2017). Due to the lack of strong automatic evaluation metrics, many researchers resort primarily to human evaluation for assessing their dialogue systems performances (Shang et al., 2015; Sordoni et al., 2015; Shao et al., 2017). There are two main problems with human annotation: 1) it is time-consuming and expensive, and 2) it does not facilitate comparisons across research papers. For certain research questions that involve hyper-parameter tuning or architecture searches, the amount of human annotation makes such studies infeasible (Britz et al., 2017; Melis et al., 2018). Therefore, developing reliable automatic evaluation metrics for open-domain dialog systems is imperative. The Referenced metric and Unreferenced metric Blended Evaluation Routine (RUBER) (Tao et al., 2018) stands out from recent work in automatic"
W19-2310,N15-1020,0,0.103539,"Missing"
W19-2310,D17-1238,0,0.146835,"; it showed high correlation with human judgments. In this paper, we explore using contextualized word embeddings to compute more accurate relatedness scores, thus better evaluation metrics. Experiments show that our evaluation metrics outperform RUBER, which is trained on static embeddings. 1 Table 1: An example of zero BLEU score for an acceptable generated response in multi-turn dialogue system of response quality. As shown in Table 1, highquality responses can have low or even no n-gram overlap with a reference response, showing that these metrics are not suitable for dialogue evaluation (Novikova et al., 2017; Lowe et al., 2017). Due to the lack of strong automatic evaluation metrics, many researchers resort primarily to human evaluation for assessing their dialogue systems performances (Shang et al., 2015; Sordoni et al., 2015; Shao et al., 2017). There are two main problems with human annotation: 1) it is time-consuming and expensive, and 2) it does not facilitate comparisons across research papers. For certain research questions that involve hyper-parameter tuning or architecture searches, the amount of human annotation makes such studies infeasible (Britz et al., 2017; Melis et al., 2018). The"
W19-2310,P02-1040,0,0.108299,"UBER) (Tao et al., 2018) stands out from recent work in automatic dialogue evaluation, relying minimally on human-annotated datasets of response quality for training. RUBER evaluates responses with a blending of scores from two metrics: Introduction Recent advances in open-domain dialogue systems (i.e. chatbots) highlight the difficulties in automatically evaluating them. This kind of evaluation inherits a characteristic challenge of NLG evaluation - given a context, there might be a diverse range of acceptable responses (Gatt and Krahmer, 2018). Metrics based on n-gram overlaps such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), originally designed for evaluating machine translation and summarization, have been adopted to evaluate dialogue systems (Sordoni et al., 2015; Li et al., 2016; Su et al., 2018). However, Liu et al. (2016) found a weak segment-level correlation between these metrics and human judgments 82 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 82–89 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics Ranking Loss Pooling Bert Word2vec Embeddings Embeddings Bi-RNN What are"
W19-2310,N18-1202,0,0.0837357,"s to better utilize contextualized word embeddings, and show their positive effects. 2 Unreferenced Metric • Word2vec. Recent works on learnable evaluation metrics use simple word embeddings such as word2vec and GLoVe as input to their models (Tao et al., 2018; Lowe et al., 2017; Kannan and Vinyals, 2017). Since these static embeddings have a fixed contextindependent representation for each word, they cannot represent the rich semantics of words in contexts. • BERT. Contextualized word embeddings are recently shown to be beneficial in many NLP tasks (Devlin et al., 2018; Radford et al., 2018; Peters et al., 2018; Liu et al., 2019). A noticeable contextualized word embeddings, BERT (Devlin et al., 2018), is shown to perProposed models We conduct the research under the RUBER metric’s referenced and unreferenced framework, where we replace their static word embeddings with pretrained BERT contextualized embeddings and compare the performances. We identify three points of variation with two options each in the 83 Bert Word2vec Embeddings Embeddings form competitively among other contextualized embeddings, thus we explore the effect of BERT embeddings on open domain dialogue systems evaluation task. Speci"
W19-2310,D16-1230,0,\N,Missing
W19-2310,N19-1423,0,\N,Missing
