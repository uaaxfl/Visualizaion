2020.acl-main.11,D18-1547,0,0.093333,"Missing"
2020.acl-main.11,2020.nlp4convai-1.5,1,0.86053,"Missing"
2020.acl-main.11,N19-1423,0,0.0488617,"t-filling as a fully intentagnostic span extraction problem. Instead of using rules to constrain the co-occurrence of slots and intents, we identify a slot as either a single span of text or entirely absent. This makes our approach more flexible than prior work; it is fully independent of other system components. Regardless, we can explicitly capture turn-by-turn context by adding an input feature denoting whether a slot was requested for this dialog turn (see Figure 1). Pretrained Representations. Large-scale pretrained models have shown compelling benefits in a plethora of NLP applications (Devlin et al., 2019; Liu et al., 2019): such models drastically lessen the amount of required task/domain-specific training data with in-domain fine-tuning. This is typically achieved by adding a task-specific output layer to a large pretrained encoder and then finetuning the entire model (Xie et al., 2019). However, this process requires a fine-tuned model for each slot or domain, rather than a single model shared across all slots and domains. This adds a large memory and computational overhead and makes the approach impractical in real-life applications. Therefore, we propose to keep the pretrained encoder mod"
2020.acl-main.11,W17-5526,0,0.10359,"Missing"
2020.acl-main.11,N18-3018,0,0.0291388,"the slots date, ∗ Both authors contributed equally to the work. The work of TF was done during an internship at PolyAI. time and number of guests with correct values given by the user (e.g. tomorrow, 8pm, 3 people) in order to proceed with a booking. A particular challenge is to deploy slot-filling systems in low-data regimes (i.e., few-shot learning setups), which is needed to enable quick and wide portability of conversational agents. Scarcity of in-domain data has typically been addressed using domain adaption from resource-rich domains, e.g. through multitask learning (Jaech et al., 2016; Goyal et al., 2018) or ensembling (Jha et al., 2018; Kim et al., 2019). In this work, we approach slot-filling as a turnbased span extraction problem similar to Rastogi et al. (2019): in our Span-ConveRT model we do not restrict values to fixed categories, and simultaneously allow the model to be entirely independent of other components in the dialog system. In order to facilitate slot-filling in resource-lean settings, our main proposal is the effective use of knowledge coded in representations transferred from large general-purpose conversational pretraining models, e.g., the ConveRT model trained on a large R"
2020.acl-main.11,H90-1021,0,0.137106,"Missing"
2020.acl-main.11,P19-1536,1,0.860982,"Missing"
2020.acl-main.11,2021.ccl-1.108,0,0.143695,"Missing"
2020.acl-main.11,P16-1101,0,0.048028,"d mean, for example, we would need 100 fine-tuned encoders running in production to support 100 different slots. As the encoder models have both high memory and runtime requirements, this would drastically increase the running costs of a conversational system. 108 My name is Joseph Schmoe tions, we can leverage conversational cues from over 700M conversational turns for the few-shot span extraction task.3 Span ConveRT: Final Model. We now describe our model architecture, illustrated in Figure 2. Our approach builds on established sequence tagging models using Conditional Random Fields (CRFs) (Ma and Hovy, 2016; Lample et al., 2016). We propose to replace the LSTM part of the model with fixed ConveRT embeddings.4 We take contextualized subword embeddings from ConveRT, giving a sequence of the same length as the subwordtokenized sentence. For sequence tagging, we train a CNN and CRF on top of these fixed subword representations. We concatenate three binary features to the subword representations to emphasize important textual characteristics: (1) whether the token is alphanumeric, (2) numeric, or (3) the start of a new word. In addition, we concatenate the character length of the token as another int"
2020.acl-main.11,P19-1373,0,0.0874358,"0.36 0.42 Homes_1 1 (2064) 1/2 (1032) 1/4 (516) 1/8 (258) 1/16 (129) 0.98 0.96 0.95 0.92 0.88 0.95 0.90 0.88 0.82 0.69 0.97 0.94 0.87 0.80 0.70 RentalCars_1 1 (874) 1/2 (437) 1/4 (218) 1/8 (109) 1/16 (54) 0.91 0.87 0.81 0.75 0.62 0.89 0.83 0.69 0.59 0.31 0.89 0.82 0.74 0.56 0.38 full fine-tuning), have recently been established in other dialog tasks such as intent detection (Henderson et al., 2019a; Casanueva et al., 2020; Bunk et al., 2020). In general, our findings also call for investing more effort in investigating different pretraining strategies that are better aligned to target tasks (Mehri et al., 2019; Henderson et al., 2019a; Humeau et al., 2020). Error Analysis. To better understand the performance of Span-ConveRT on the RESTAURANTS 8 K data set, we also conducted a manual error analysis, comparing it with the best performing baseline model, V-CNN-CRF. In Appendix C we lay out the types of errors that occur in a generic span extraction task and investigate the distribution of these types of errors across slots and models. We show that when trained in the high-data setting the distribution is similar between the two models, suggesting that gains from Span-ConveRT are across all types of e"
2020.acl-main.11,E17-1042,0,0.0691475,"Missing"
2020.acl-main.11,W14-4339,0,0.0420804,"ents are finding success in a wide range of well-defined tasks such as customer support, restaurant, train or flight bookings (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017; Budzianowski et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), and also in domains such as healthcare (Laranjo et al., 2018) or entertainment (Fraser et al., 2018). Scaling conversational agents to support new domains and tasks, and particular system behaviors is a highly challenging and resource-intensive task: it critically relies on expert knowledge and domain-specific labeled data (Williams, 2014; Wen et al., 2017b,a; Liu et al., 2018; Zhao et al., 2019). Slot-filling is a crucial component of any taskoriented dialog system (Young, 2002, 2010; Bellegarda, 2014). For instance, a conversational agent for restaurant bookings must fill all the slots date, ∗ Both authors contributed equally to the work. The work of TF was done during an internship at PolyAI. time and number of guests with correct values given by the user (e.g. tomorrow, 8pm, 3 people) in order to proceed with a booking. A particular challenge is to deploy slot-filling systems in low-data regimes (i.e., few-shot learning se"
2020.acl-main.257,Q17-1002,0,0.0183207,"t activate selectively or more strongly for a particular function such as modalityspecific or category-specific semantics (such as objects/actions, abstract/concrete, animate/inanimate, animals, fruits/vegetables, colours, body parts, countries, flowers, etc.) (Warrington, 1975; Warrington and McCarthy, 1987; McCarthy and Warrington, 1988). This indicates a function-specific 2873 division of lower-level semantic processing. Singlespace distributional word models have been found to partially correlate to these distributed brain activity patterns (Mitchell et al., 2008; Huth et al., 2012, 2016; Anderson et al., 2017), but fail to explain the full spectrum of fine-grained word associations humans are able to make. Our work has been partly inspired by this literature. Compositional Distributional Semantics. Partially motivated by similar observations, prior work frequently employs tensor-based methods for composing separate tensor spaces (Coecke et al., 2010): there, syntactic categories are often represented by tensors of different orders based on assumptions on their relations. One fundamental difference is made between atomic types (e.g., nouns) versus compositional types (e.g., verbs). Atomic types are"
2020.acl-main.257,J10-4006,0,0.0840012,"th neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed da"
2020.acl-main.257,D08-1007,0,0.0581951,"Missing"
2020.acl-main.257,P09-1068,0,0.0571221,"additionally evaluate our models on a number of other established datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this t"
2020.acl-main.257,P10-1046,0,0.060836,"usibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc ,"
2020.acl-main.257,D14-1082,0,0.00762763,"igh similarity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,W09-0211,0,0.0610713,"Missing"
2020.acl-main.257,D14-1004,0,0.0502357,"Missing"
2020.acl-main.257,W16-1605,0,0.0209594,"ile effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enab"
2020.acl-main.257,Q17-1010,0,0.285232,"V), I(O)). The space is optimised such that vectors for plausible SVO compositions will be close. Note that one word can have several vectors, for example chicken can occur both as S and O. Introduction Word representations are in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The"
2020.acl-main.257,J10-4007,0,0.031176,"Missing"
2020.acl-main.257,P19-1318,0,0.0205895,"actorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vectors encode knowledge that is often complementary to what is coded in word vectors. Rei et al. (2018) and Vuli´c and Mrkˇsi´c (2018) described related task-dependent neural nets for mapping word embeddings into relation-specific spaces for scoring lexical entailment. In this work, we propose a task-independent approach and extend it to work with a variable number of relations. Neuroscience. Theories from cognitive linguistics and neuroscience reveal that single-space repre"
2020.acl-main.257,W15-1106,0,0.122987,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,N15-1003,0,0.119856,"ing to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propo"
2020.acl-main.257,D11-1129,0,0.0855066,"Missing"
2020.acl-main.257,W11-2507,0,0.311832,"SVO study (V) researcher (S), scientist (S), subject (O), art (O) eat (V) food (O), cat (S), dog (S) need (V) help (O), implementation (S), support (O) Table 1: Nearest neighbours in a function-specific space trained for the SVO structure. In the Joint SVO space (bottom) we show nearest neighbors for verbs (V) from the two other subspaces (O and S). Mann and Ruhlen, 2011). In language, this event understanding information is typically captured by the SVO structures and, according to the cognitive science literature, is well aligned with how humans process sentences (McRae et al., 1997, 1998; Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014); it reflects the likely distinct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation"
2020.acl-main.257,P14-2050,0,0.194288,"and Motivation Representation Learning. Standard word representation models such as skip-gram negative sampling (SGNS) (Mikolov et al., 2013b,a), Glove (Pennington et al., 2014), or FastText (Bojanowski et al., 2017) induce a single word embedding space capturing broad semantic relatedness (Hill et al., 2015). For instance, SGNS makes use of two vector spaces for this purpose, which are referred to as Aw and Ac . SGNS has been shown to approximately correspond to factorising a matrix M = Aw ATc , where elements in M represent the co-occurrence strengths between words and their context words (Levy and Goldberg, 2014b). Both matrices represent the same vocabulary: therefore, only one of them is needed in practice to represent each word. Typically only Aw is used while Ac is discarded, or the two vector spaces are averaged to produce the final space. Levy and Goldberg (2014a) used dependencybased contexts, resulting in two separate vector spaces; however, the relation types were embedded into the vocabulary and the model was trained only in one direction. Camacho-Collados et al. (2019) proposed to learn separate sets of relation vectors in addition to standard word vectors and showed that such relation vec"
2020.acl-main.257,P16-1020,0,0.018132,"nal solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspecific vector spaces which enable a better model of associations between concepts and consequently improved event representations by encoding the relevant information directly into the parameters for each word during training. Word vectors offer several advantages over tensors: a large reduction in parameters and fixed dimensionality across concepts. This facilitates their reuse and transfer across different tasks. For this reason, we find our multidirection"
2020.acl-main.257,N16-1118,0,0.0466753,"Missing"
2020.acl-main.257,J15-4004,1,0.911699,"in ubiquitous usage across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as i"
2020.acl-main.257,C12-2054,0,0.0226717,"tandalone: their meaning is independent from other types. On the other hand, verbs are compositional as they rely on their subjects and objects for their exact meaning. Due to this added complexity, the compositional types are often represented with more parameters than the atomic types, e.g., with a matrix instead of a vector. The goal is then to compose constituents into a semantic representation which is independent of the underlying grammatical structure. Therefore, a large body of prior work is concerned with finding appropriate composition functions (Grefenstette and Sadrzadeh, 2011a,b; Kartsaklis et al., 2012; Milajevs et al., 2014) to be applied on top of word representations. Since this approach represents different syntactic structures with tensors of varying dimensions, comparing syntactic constructs is not straightforward. This compositional approach thus struggles with transferring the learned knowledge to downstream tasks. State-of-the-art compositional models (Tilk et al., 2016; Weber et al., 2018) combine similar tensor-based approaches with neural training, leading to task-specific compositional solutions. While effective for a task at hand, the resulting models rely on a large number of"
2020.acl-main.257,P08-1028,0,0.227251,"Missing"
2020.acl-main.257,K16-1008,0,0.0310574,"tablished datasets (Sayeed et al., 2016). Event Similarity (3 Variables: SVO). A standard task to measure the plausibility of SVO structures (i.e., events) is event similarity (Grefenstette and Sadrzadeh, 2011a; Weber et al., 2018): the goal is to score similarity between SVO triplet pairs and correlate the similarity scores to humanelicited similarity judgements. Robust and flexible event representations are important to many core areas in language understanding such as script learning, narrative generation, and discourse understanding (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Modi, 2016; Weber et al., 2018). We evaluate event similarity on two benchmarking data sets: GS199 (Grefenstette and Sadrzadeh, 2011a) and KS108 (Kartsaklis and Sadrzadeh, 2014). GS199 contains 199 pairs of SV O triplets/events. In the GS199 data set only the V is varied, while S and O are fixed in the pair: this evaluation prevents the model from relying only on simple lexical overlap for similarity computation.2 KS108 contains 108 event pairs for the same task, but is specifically constructed without any lexical overlap between the events in each pair. For this task function-specific representations a"
2020.acl-main.257,Q17-1022,1,0.909677,"Missing"
2020.acl-main.257,L16-1262,0,0.0649244,"Missing"
2020.acl-main.257,D14-1162,0,0.108435,"arity score of 6.53, whereas ’river meet sea’ and ’river satisfy sea’ have been given a low score of 1.84. Accuracy Using an example from Sayeed et al. (2016), the human participants were asked “how common is it for a {snake, monster, baby, cat} to frighten someone/something” (agent role) as opposed to “how common is it for a {snake, monster, baby, cat} to be frightened by someone/something” (patient role). 2877 Training Data. We parse the ukWaC corpus (Baroni et al., 2009) and the British National Corpus (BNC) (Leech, 1992) using the Stanford Parser with Universal Dependencies v1.4 (Chen and Manning, 2014; Nivre et al., 2016) and extract cooccurring subjects, verbs and objects. All words are lowercased and lemmatised, and tuples containing non-alphanumeric characters are excluded. We also remove tuples with (highly frequent) pronouns as subjects, and filter out training examples containing words with frequency lower than 50. After preprocessing, the final training corpus comprises 22M SVO triplets in total. Table 2 additionally shows training data statistics when training in the 2-group setup (SV and VO) and in the 4-group setup (when adding indirect objects: SVO+iO). We report the number of e"
2020.acl-main.257,K15-1026,1,0.838922,"across all areas of natural language processing (NLP) (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Standard approaches rely on the distributional hypothesis (Harris, 1954; Sch¨utze, 1993) and learn a single word vector space based on word co-occurrences in large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). This purely context-based training produces general word representations that capture the broad notion of semantic relatedness and conflate a variety of possible semantic relations into a single space (Hill et al., 2015; Schwartz et al., 2015). However, this mono-faceted view of meaning is a well-known deficiency in NLP applications (Faruqui, 2016; Mrkˇsi´c et al., 2017) as it fails to distinguish between fine-grained word associations. In this work we propose to learn a joint functionspecific word vector space that accounts for the different roles and functions a word can take in text. The space can be trained for a specific structure, such as SVO, and each word in a particular role will have a separate representation. Vectors for plausible SVO compositions will then be optimized to lie close together, as illustrated by Figure 1."
2020.acl-main.257,D16-1017,0,0.246401,"nct storage and processing of objects (typically nouns) and actions (typically verbs) in the brain (Caramazza and Hillis, 1991; Damasio and Tranel, 1993). The quantitative results are reported on two established test sets for compositional event similarity (Grefenstette and Sadrzadeh, 2011a; Kartsaklis and Sadrzadeh, 2014). This task requires reasoning over SVO structures and quantifies the plausibility of the SVO combinations by scoring them against human judgments. We report consistent gains over established word representation methods, as well as over two recent tensor-based architectures (Tilk et al., 2016; Weber et al., 2018) which are designed specifically for solving the event similarity task. Furthermore, we investigate the generality of our approach by also applying it to other types of structures. We conduct additional experiments in a 4-role setting, where indirect objects are also modeled, along with a selectional preference evaluation of 2-role SV and VO relationships (Chambers and Jurafsky, 2010; Van de Cruys, 2014), yielding the highest scores on several established benchmarks. 2 Background and Motivation Representation Learning. Standard word representation models such as skip-gram"
2020.acl-main.257,N18-1103,1,0.908435,"Missing"
2020.acl-main.257,P18-2101,1,0.90757,"Missing"
2020.acl-main.257,P99-1014,0,0.453354,"Missing"
2020.acl-main.257,W16-2518,0,0.142282,"itional solutions. While effective for a task at hand, the resulting models rely on a large number of parameters and are not robust: we observe deteriorated performance on other related compositional tasks, as shown in Section 6. Multivariable (SVO) Structures in NLP. Modeling SVO-s is important for tasks such as compositional event similarity using all three variables, and thematic fit modeling based on SV and VO associations separately. Traditional solutions are typically based on clustering of word co-occurrence counts from a large corpus (Baroni and Lenci, 2010; Greenberg et al., 2015a,b; Sayeed et al., 2016; Emerson and Copestake, 2016). More recent solutions combine neural networks with tensor-based methods. Van de Cruys (2014) present a feedforward neural net trained to score compositions of both two and three groups with a max-margin loss. Grefenstette and Sadrzadeh (2011a,b); Kartsaklis and Sadrzadeh (2014); Milajevs et al. (2014); Edelstein and Reichart (2016) employ tensor compositions on standard single-space word vectors. Hashimoto and Tsuruoka (2016) discern compositional and non-compositional phrase embeddings starting from HPSG-parsed data. Objectives. We propose to induce functionspe"
2020.acl-main.618,P15-1165,0,0.0732183,"Missing"
2020.acl-main.618,P18-1072,1,0.867314,"Missing"
2020.acl-main.618,D19-1449,1,0.869192,"Missing"
2020.acl-main.675,P15-1165,0,0.0243921,"Missing"
2020.acl-main.675,P18-1072,1,0.9198,"Missing"
2020.acl-main.675,P11-2084,1,0.842277,"Missing"
2020.acl-main.675,D19-1449,1,0.902946,"Missing"
2020.acl-main.675,P16-1024,1,0.782186,"Missing"
2020.acl-main.675,2020.acl-main.536,0,0.0902746,"e., languages with nonisomorphic monolingual spaces). 1 Introduction and Motivation Induction of cross-lingual word embeddings (CLWEs) (Vuli´c et al., 2011; Mikolov et al., 2013; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018) has been one of the key mechanisms for enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks. Even though CLWEs are recently being contested in cross-lingual downstream transfer by pretrained multilingual language models (Pires et al., 2019; Conneau et al., 2020; Artetxe et al., 2019; Wu and Dredze, 2019; Wu et al., 2020), they are still paramount in word-level translation, that is, bilingual lexicon induction (BLI). While earlier work focused on joint induction of multilingual embeddings from multilingual corpora, relying on word- (Klementiev et al., 2012; Koˇcisk`y et al., 2014; Gouws and Søgaard, 2015), sentence- (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015; Coulmance et al., 2015; Levy et al., 2017), or document-level (Søgaard et al., 2015; Mogadala and Rettinger, 2016; Vuli´c and Moens, 2016) alignments, most recent efforts focus on post-hoc alignment of independently trained monolingu"
2020.acl-main.675,D19-1077,0,0.0182767,"nt language pairs (i.e., languages with nonisomorphic monolingual spaces). 1 Introduction and Motivation Induction of cross-lingual word embeddings (CLWEs) (Vuli´c et al., 2011; Mikolov et al., 2013; Xing et al., 2015; Smith et al., 2017; Artetxe et al., 2018) has been one of the key mechanisms for enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks. Even though CLWEs are recently being contested in cross-lingual downstream transfer by pretrained multilingual language models (Pires et al., 2019; Conneau et al., 2020; Artetxe et al., 2019; Wu and Dredze, 2019; Wu et al., 2020), they are still paramount in word-level translation, that is, bilingual lexicon induction (BLI). While earlier work focused on joint induction of multilingual embeddings from multilingual corpora, relying on word- (Klementiev et al., 2012; Koˇcisk`y et al., 2014; Gouws and Søgaard, 2015), sentence- (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015; Coulmance et al., 2015; Levy et al., 2017), or document-level (Søgaard et al., 2015; Mogadala and Rettinger, 2016; Vuli´c and Moens, 2016) alignments, most recent efforts focus on post-hoc alignment of independently"
2020.acl-main.675,N15-1104,0,0.15939,"Missing"
2020.acl-main.675,C00-2137,0,0.110738,"t groups of language pairs identifies I NSTA M AP as particularly beneficial for pairs of distant languages (setups No-EN and HARD) and languages with least reliable monolingual vectors (TR, HR). For example, 3 Competing models – VecMap, RCSLS, and BLISS – all come with much larger sets of hyperparameters. 4 For some pairs other configurations yield slightly better results: for simplicity, we report the results with base configuration K = 70; T = 4 for all pairs. 5 We provide detailed results for each of 28 language language pairs in the supplemental material. 6 Non-parametric shuffling test (Yeh, 2000) with the Bonferroni correction: α &lt; 0.05 in comparison with VecMap and α &lt; 0.01 in comparison with other models. while I NSTA M AP alone and IM ◦ VM yield gains of 0.9 and 2.6 points, respectively, w.r.t. VecMap across ALL language pairs, these gaps widen to 1.5 and 3.5 points on most challenging language pairs (HARD). In contrast, BLISS, a model specifically tailored to improve the mappings between non-isomorphic spaces, appears to be robust only on pairs of close languages (e.g., HR-RU) and pairs involving EN (setup EN-*). It exhibits barely any improvement over the baseline orthogonal proj"
2020.cl-4.5,E17-1088,0,0.0206243,"-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steerin"
2020.cl-4.5,D18-1214,0,0.0617967,"Missing"
2020.cl-4.5,P18-1073,0,0.159528,"to principal component analysis) from the input distributional word vectors, since they do not contribute toward distinguishing the actual semantic meaning of different words. The method contains a single (tunable) hyperparameter ddA , which denotes the number of the dominating directions to remove from the initial representations. Previous work has verified the usefulness of ABTT in several English lexical semantic tasks such as semantic similarity, word analogies, and concept categorization, as well as in sentence-level text classification tasks (Mu, Bhat, and Viswanath 2018). (3) UNCOVEC (Artetxe et al. 2018) adjusts the similarity order of an arbitrary input word embedding space, and can emphasize either syntactic or semantic information in the transformed vectors. In short, it transforms the input space X into an adjusted space XWα through a linear map Wα controlled by a single hyperparameter α. The nth -order similarity transformation of the input word vector space X (for which n = 1) can be obtained as M n (X) = M 1 (XW (n − 1)/2 ), with Wα = QΓ α , where Q and Γ are the matrices obtained via eigendecomposition of X T X = QΓQT . Γ is a diagonal matrix containing eigenvalues of X T X; Q is an o"
2020.cl-4.5,P98-1013,0,0.103027,"Missing"
2020.cl-4.5,J82-2005,0,0.629873,"Missing"
2020.cl-4.5,L18-1618,0,0.021365,"999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 2008). For instance, the two senses that are distinguished in English as time and weather are co-lexified in Croatian: the word vrijeme is used in both cases. 854 Vuli´c et al. Multi-SimLex resolution of translat"
2020.cl-4.5,N18-1083,0,0.0292232,"ially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents"
2020.cl-4.5,J19-2006,0,0.0292702,"es to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scale (i.e., gradience/strength of semantic similarity) (Budanitsky and Hirst 2006; Hill, Reichart, and Korhonen 2015). For any pair of words, this relation measures whether (and to what extent) their referents share the same (func"
2020.cl-4.5,E17-2036,0,0.0144067,"2) Source: SemEval-17: Task 2 (henceforth SEMEVAL-500; Camacho-Collados et al. 2017). We start from the full data set of 500 concept pairs to extract a total of 334 concept pairs for English Multi-SimLex a) which contain only single-word concepts, b) which are not named entities, c) where POS tags of the two concepts are the same, d) where both concepts occur in the top 250K most frequent word types in the English Wikipedia, and e) which do not already occur in SimLex-999. The original concepts were sampled as to span all the 34 domains available as part of BabelDomains (Camacho-Collados and Navigli 2017), which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample. 3) Source: CARD-660 (Pilehvar et al. 2018). Sixty-seven word pairs are taken from this data set focused on rare word similarity, applying the same selection criteria a to e utilized for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus (Baroni et al. 2009). CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia), and slang (2mrw), which might be difficult to tr"
2020.cl-4.5,S17-2002,0,0.0611659,"uli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 849 Computational Linguistics Volume 46, Numbe"
2020.cl-4.5,D14-1082,0,0.0211604,"Missing"
2020.cl-4.5,2020.acl-main.747,0,0.168947,"Missing"
2020.cl-4.5,D18-1269,0,0.383476,"the coverage also to languages that are resourcelean and/or typologically diverse (e.g., Welsh, Kiswahili, as in this work). Multilingual Data Sets for Natural Language Understanding. The Multi-SimLex initiative and corresponding data sets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual BERT (Devlin et al. 2019) or XLM (Conneau and Lample 2019) are typically probed on XNLI test data (Conneau et al. 2018b) for crosslingual natural language inference. XNLI was created by translating examples from the English MultiNLI data set, and projecting its sentence labels (Williams, Nangia, and Bowman 2018). Other recent multilingual data sets target the task of question answering based on reading comprehension: i) MLQA (Lewis et al. 2019) includes 7 languages; ii) XQuAD (Artetxe, Ruder, and Yogatama 2019) 10 languages; and iii) TyDiQA (Clark et al. 2020) 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English data set, TyDiQA was built independen"
2020.cl-4.5,Q19-1041,1,0.810306,"ingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 diffe"
2020.cl-4.5,C18-1323,0,0.354998,"6), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish [Ercan and Yıldız 2018], 500 in Farsi [Camacho-Collados et al. 2017], or 300 in Finnish [Venekoski and Vankka 2017]) and coverage (e.g., all data sets that originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity data set spanning 1,888 concept pairs (see §4). 1 This lexical relation is, somewhat imprecisely, also termed true or pure semantic similarity (Hill, Reichart, and Korhonen 2015; Kiela, Hill, and Clark 2015); see the ensuing discussion in §2.1. 8"
2020.cl-4.5,D19-1006,0,0.0554256,"n 2019). 875 Computational Linguistics Volume 46, Number 4 Impact of Unsupervised Post-Processing. First, the results in Table 12 suggest that applying dimension-wise mean centering to the initial vector spaces has positive impact on word similarity scores in all test languages and for all models, both static and contextualized (see the + MC rows in Table 12). Mimno and Thompson (2017) show that distributional word vectors have a tendency toward narrow clusters in the vector space (i.e., they occupy a narrow cone in the vector space and are therefore anisotropic [Mu, Bhat, and Viswanath 2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vu"
2020.cl-4.5,N15-1184,0,0.266878,"trinsic evaluations of specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This make"
2020.cl-4.5,N18-2029,1,0.762404,"Missing"
2020.cl-4.5,P19-1070,1,0.912198,"Missing"
2020.cl-4.5,Q16-1002,1,0.880245,"Missing"
2020.cl-4.5,J15-4004,1,0.93809,"Missing"
2020.cl-4.5,D18-1043,0,0.0596882,"Missing"
2020.cl-4.5,2020.acl-main.560,0,0.0211952,"guiding future developments in multilingual lexical semantics and representation learning—available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages. 848 Vuli´c et al. Multi-SimLex 1. Introduction The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world’s languages (Snyder and Barzilay 2010; Adams et al. 2017; Ponti et al. 2019a; Joshi et al. 2020). The necessity to guide and advance multilingual and crosslingual NLP through annotation efforts that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zema"
2020.cl-4.5,D18-1330,0,0.0361587,"Missing"
2020.cl-4.5,W14-1503,0,0.0323243,"wski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney (2012), Kiela and Clark (2014), and Melamud et al. (2016) demonstrated that different choices of hyperparameters in WE algorithms (such as context window) emphasize different relations in the resulting representations. Likewise, Agirre et al. (2009) and Levy and Goldberg (2014) discovered that WEs learned from texts annotated with syntactic information mirror similarity better than simple local bag-of-words neighborhoods. The failure of WEs to capture semantic similarity, in turn, affects model performance in several NLP applications where such knowledge is crucial. In particular, Natural Language Understanding tasks such"
2020.cl-4.5,W16-1607,0,0.0601778,"Missing"
2020.cl-4.5,kipper-etal-2004-extending,0,0.0569229,"Missing"
2020.cl-4.5,D19-1279,0,0.0717479,"Missing"
2020.cl-4.5,2020.emnlp-main.363,1,0.891144,"Missing"
2020.cl-4.5,P15-1027,0,0.0827227,"Missing"
2020.cl-4.5,P14-2050,0,0.0586512,"their associated meaning confounds the two distinct relations (Hill, Reichart, and Korhonen 2015; Schwartz, Reichart, and Rappoport 2015; Vuli´c et al. 2017b). As a result, distributional methods obscure a crucial facet of lexical meaning. This limitation also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications (Collobert et al. 2011; Chen and Manning 2014; Melamud et al. 2016, inter alia). In particular, it involves both static WEs learned from co-occurrence patterns (Mikolov et al. 2013; Levy and Goldberg 2014; Bojanowski et al. 2017) and contextualized WEs learned from modeling word sequences (Peters et al. 2018; Devlin et al. 2019, inter alia). As a result, in the induced representations, geometrical closeness (measured, e.g., through cosine distance) conflates genuine similarity with broad relatedness. For 852 Vuli´c et al. Multi-SimLex instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. Similar to work on distributional representations that predated the WE era (Sahlgren 2006), Turney"
2020.cl-4.5,2020.emnlp-main.484,0,0.0477066,"Missing"
2020.cl-4.5,D18-1521,0,0.0264898,"Missing"
2020.cl-4.5,D17-1308,0,0.0696529,"Missing"
2020.cl-4.5,N19-1386,0,0.0334837,"Missing"
2020.cl-4.5,Q17-1022,1,0.934218,"Missing"
2020.cl-4.5,L18-1381,0,0.0518909,"Missing"
2020.cl-4.5,D18-1169,0,0.15982,"me prominent English word pair data sets such as WordSim-353 (Finkelstein et al. 2002), MEN (Bruni, Tran, and Baroni 2014), or Stanford Rare Words (Luong, Socher, and Manning 2013) did not discriminate between similarity and relatedness, the importance of this distinction was established by Hill, Reichart, and Korhonen (2015) (see again the discussion in §2.1) through the creation of SimLex-999. This inspired other similar data sets that focused on different lexical properties. For instance, SimVerb-3500 (Gerz et al. 2016) provided similarity ratings for 3,500 English verbs, whereas CARD-660 (Pilehvar et al. 2018) aimed at measuring the semantic similarity of infrequent concepts. Semantic Similarity Data Sets in Other Languages. Motivated by the impact of data sets such as SimLex-999 and SimVerb-3500 on representation learning in English, a line of related work put focus on creating similar resources in other languages. The dominant approach is translating and reannotating the entire original English SimLex-999 data set, as done previously for German, Italian, and Russian (Leviant and Reichart 2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venek"
2020.cl-4.5,P19-1493,0,0.0740204,"Missing"
2020.cl-4.5,J19-3005,1,0.889644,"Missing"
2020.cl-4.5,D18-1026,1,0.925849,"Missing"
2020.cl-4.5,Q17-1020,0,0.0167721,"that follow crosslingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation"
2020.cl-4.5,D18-1299,0,0.0140965,"i.e., the distributional information).1 Data sets that quantify the strength of semantic similarity between concept pairs such as SimLex-999 (Hill, Reichart, and Korhonen 2015) or SimVerb-3500 (Gerz et al. 2016) have been instrumental in improving models for distributional semantics and representation learning. Discerning between semantic similarity and relatedness/association is not only crucial for theoretical studies on lexical semantics (see §2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking (Mrkˇsi´c et al. 2017; Ren et al. 2018), spoken language understanding (Kim et al. 2016; Kim, de Marneffe, and Fosler-Lussier 2016), text simplification (Glavaˇs and Vuli´c 2018; Ponti et al. 2018b; Lauscher et al. 2019), and dictionary and thesaurus construction (Cimiano, Hotho, and Staab 2005; Hill et al. 2016). Despite the proven usefulness of semantic similarity data sets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian (Leviant and Reichart 2015), whereas some language types and low-resource languages typically lack similar evaluation data. Eve"
2020.cl-4.5,Q19-1044,1,0.813505,"elines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project (Nivre et al. 2019). The latest version of UD (as of July 2020) covers about 90 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD’s reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing (Zeman et al. 2018; Kondratyuk and Straka 2019; Doitch et al. 2019) and crosslingual parser transfer (Rasooli and Collins 2017; Lin et al. 2019; Rotman and Reichart 2019), the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages’ syntactic (dis)similarities (Chen and Gerdes 2017; Bjerva and Augenstein 2018; Bjerva et al. 2019; Ponti et al. 2018a; Pires, Schlinger, and Garrette 2019). Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic data sets for 12 different languages, focused on the fundamental lexical relation of semantic similarity on a continuous scal"
2020.cl-4.5,L18-1152,0,0.223909,"2015), Hebrew and Croatian (Mrkˇsi´c et al. 2017), and Polish (Mykowiecka, Marciniak, and Rychlik 2018). Venekoski and Vankka (2017) applied this process only to a subset of 300 concept pairs from the English SimLex-999. On the other hand, Camacho-Collados et al. (2017) sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by Ercan and Yıldız (2018) for Turkish, by Huang et al. (2019) for Mandarin Chinese, and by Sakaizawa and Komachi (2018) for Japanese. Netisopakul, Wohlgenannt, and Pulich (2019) translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, Barzegar et al. (2018) translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the 3 More formally, colexification is a phenomenon when different meanings can be expressed by the same word in a language (Franc¸ois 20"
2020.cl-4.5,P19-1072,0,0.0344182,"Missing"
2020.cl-4.5,K15-1026,1,0.904015,"Missing"
2020.cl-4.5,P18-1072,1,0.919137,"Missing"
2020.cl-4.5,N16-1161,0,0.0133804,"For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in XLM -100 but absent in 886 Vuli´c et al. Multi-SimLex M - BERT help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases (Littell et al. 2017) and from neural architectures (Malaviya, Neubig, and Littell 2017). Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work (Tsvetkov et al. 2016; Ammar et al. 2016; Ponti et al. 2019b) might extend the coverage to more resourcelean languages. Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual data sets for both word-level semantic similarity and sentence-level natural language understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI (Conneau et al. 2018b) for natural language inference or"
2020.cl-4.5,P19-1490,1,0.894602,"Missing"
2020.cl-4.5,Q15-1025,0,0.0155434,"f specific WE models as a proxy for their reliability for downstream applications (Collobert and Weston 2008; Baroni and Lenci 2010; Hill, Reichart, and Korhonen 2015); intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs (Faruqui et al. 2015; Wieting et al. 2015; Mrkˇsi´c et al. 2017; Lauscher et al. 2019; Kamath et al. 2019, inter alia) in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties. 2.3 Similarity and Language Variation: Semantic Typology In this work, we tackle the concept of (true and gradient) semantic similarity from a multilingual perspective. Although the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This makes the comparison of s"
2020.cl-4.5,2020.acl-main.536,0,0.0322002,"Missing"
2020.cl-4.5,D19-1077,0,0.0931228,". Because the concept pairs in Multi-SimLex are lowercased, 12 We also tested another encoding method where we fed pairs instead of single words/concepts into the pretrained encoder. The rationale is that the other concept in the pair can be used as a disambiguation signal. However, this method consistently led to sub-par performance across all experimental runs. 873 Computational Linguistics Volume 46, Number 4 we use the uncased version of M - BERT.13 M - BERT comprises all Multi-SimLex languages, and its evident ability to perform crosslingual transfer (Pires, Schlinger, and Garrette 2019; Wu and Dredze 2019; Wang et al. 2020) also makes it a convenient baseline model for crosslingual experiments later in §8. The second multilingual model we consider, XLM -100,14 is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a 1,280-dimensional representation. In contrast to M - BERT, XLM -100 drops the next-sentence prediction objective and adds a crosslingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the first H = 4 hidden layers in all experiments.15 Besides M - BERT and XLM, covering multiple lang"
2020.cl-4.5,K18-2001,0,0.0637127,"Missing"
2020.cl-4.5,P19-1307,0,0.0177035,"2018; Ethayarajh 2019]), and are prone to the undesired effect of hubness (Radovanovi´c, Nanopoulos, and Ivanovi´c 2010; Lazaridou, Dinu, and Baroni 2015).18 Applying dimension-wise mean centering has the effect of spreading the vectors across the hyperplane and mitigating the hubness issue, which consequently improves wordlevel similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks (Suzuki et al. 2013), bilingual lexicon induction with crosslingual word embeddings (Artetxe, Labaka, and Agirre 2018a; Zhang et al. 2019; Vuli´c et al. 2019), and for modeling lexical semantic change (Schlechtweg et al. 2019). However, to the best of our knowledge, the results summarized in Table 12 are the first evidence that also confirms its importance for semantic similarity in a wide array of languages. In sum, as a general rule of thumb, we suggest always mean-centering representations for semantic tasks. The results further indicate that additional post-processing methods such as ABTT and UNCOVEC on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages t"
2020.cl-4.5,K19-1021,1,0.900219,"Missing"
2020.cl-4.5,C98-1013,0,\N,Missing
2020.cl-4.5,J10-4006,0,\N,Missing
2020.cl-4.5,P94-1019,0,\N,Missing
2020.cl-4.5,J06-1003,0,\N,Missing
2020.cl-4.5,N09-1003,0,\N,Missing
2020.cl-4.5,D14-1034,1,\N,Missing
2020.cl-4.5,W13-3512,0,\N,Missing
2020.cl-4.5,D15-1242,0,\N,Missing
2020.cl-4.5,P15-2001,0,\N,Missing
2020.cl-4.5,kamholz-etal-2014-panlex,0,\N,Missing
2020.cl-4.5,N15-1104,0,\N,Missing
2020.cl-4.5,N16-1060,1,\N,Missing
2020.cl-4.5,Q17-1010,0,\N,Missing
2020.cl-4.5,P16-1024,1,\N,Missing
2020.cl-4.5,J17-4004,1,\N,Missing
2020.cl-4.5,E17-1016,1,\N,Missing
2020.cl-4.5,E17-2002,0,\N,Missing
2020.cl-4.5,P17-1042,0,\N,Missing
2020.cl-4.5,P18-1004,1,\N,Missing
2020.cl-4.5,P18-1142,1,\N,Missing
2020.cl-4.5,D18-1027,0,\N,Missing
2020.cl-4.5,D18-1024,0,\N,Missing
2020.cl-4.5,K18-1028,0,\N,Missing
2020.cl-4.5,N19-1391,0,\N,Missing
2020.cl-4.5,N19-1131,0,\N,Missing
2020.cl-4.5,K17-1013,1,\N,Missing
2020.cl-4.5,N19-1423,0,\N,Missing
2020.cl-4.5,N18-1101,0,\N,Missing
2020.cl-4.5,P19-4007,1,\N,Missing
2020.cl-4.5,W19-4310,1,\N,Missing
2020.cl-4.5,D19-1449,1,\N,Missing
2020.cl-4.5,D19-1288,1,\N,Missing
2020.cl-4.5,D19-1226,1,\N,Missing
2020.cl-4.5,D19-1165,0,\N,Missing
2020.cl-4.5,K19-1004,1,\N,Missing
2020.cl-4.5,D19-2007,1,\N,Missing
2020.cl-4.5,W17-0228,0,\N,Missing
2020.coling-main.118,2020.acl-srw.36,0,0.0330834,"Missing"
2020.coling-main.118,Q17-1010,0,0.34049,"r).1 We transform the constraints from C into a BERT-compatible input format and feed them as additional training examples for the model. The encoding of a constraint is then forwarded to the relation classifier, which predicts whether the input word pair represents a valid lexical relation. From Linguistic Constraints to Training Instances. We start from a set of linguistic constraints C = {(w1 , w2 )i }Ni=1 and an auxiliary static word embedding space Xaux ∈ Rd . The space Xaux can be obtained via any standard static word embedding model such as Skip-Gram (Mikolov et al., 2013) or fastText (Bojanowski et al., 2017) (used in this work). Each constraint c = (w1 , w2 ) corresponds to a true/positive relation of semantic similarity, and thus represents a positive training example for the model (label 1). For each positive example c, we create corresponding negative examples following prior work on specialization of static embeddings (Wieting et al., 2015; Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). We first group positive constraints from C into mini-batches B p of size k. For each positive example c = (w1 , w2 ), we create two negatives cˆ1 = (wˆ 1 , w2 ) and cˆ2 = (w1 , wˆ 2 ) such that wˆ 1 is the wor"
2020.coling-main.118,S17-2001,0,0.0785084,"Missing"
2020.coling-main.118,2020.acl-main.747,0,0.0992258,"Missing"
2020.coling-main.118,N19-1423,0,0.606051,"arity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on makin"
2020.coling-main.118,I05-5002,0,0.0570753,"Missing"
2020.coling-main.118,N15-1184,0,0.133257,"Missing"
2020.coling-main.118,P15-2011,1,0.904732,"Missing"
2020.coling-main.118,P18-1004,1,0.919583,"Missing"
2020.coling-main.118,P19-1476,1,0.878044,"Missing"
2020.coling-main.118,J15-4004,1,0.777108,"tances from a batch B p of k positive training instances. Next, we transform each instance (i.e., a pair of words) into a “BERT-compatible” format, i.e., into a sequence of WordPiece (Wu et al., 2016) tokens.2 We split both w1 and w2 into WordPiece tokens, insert the special separator token (with a randomly initialized embedding) before and after the tokens of w2 and prepend the whole sequence with BERT’s sequence start token, as shown in this example for the constraint (mended, regenerated):3 1 As the goal is to inform the BERT model on the relation of true semantic similarity between words (Hill et al., 2015), according to prior work on static word embeddings, the sets of both synonym pairs and direct hyponym-hypernym pairs are useful to boost the model’s ability to capture true semantic similarity, which in turn has a positive effect on downstream language understanding applications. See the work of Hill et al. (2015) and Vuli´c (2018) for further details regarding the relationship between direct hyponym-hypernym pairs and true semantic similarity. 2 We use the same 30K WordPiece vocabulary as Devlin et al. (2019). Sharing WordPieces helps our word-level task as lexico-semantic relationships are"
2020.coling-main.118,P14-2075,0,0.0658816,"emantic similarity between the fastText vectors (Bojanowski et al., 2017) of the original word w and the candidate ci , and (4) word frequency of ci in the top 12 million texts of Wikipedia and in the Children’s Book Test corpus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and"
2020.coling-main.118,W18-3003,0,0.0138404,"with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words dur"
2020.coling-main.118,W19-4310,1,0.883945,"Missing"
2020.coling-main.118,D15-1242,0,0.0229554,"ely researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external co"
2020.coling-main.118,2020.deelio-1.5,1,0.878646,"Missing"
2020.coling-main.118,C18-1205,0,0.0281817,"al knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specializ"
2020.coling-main.118,P15-1145,0,0.0248839,"em. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rat"
2020.coling-main.118,N16-1018,0,0.0427133,"Missing"
2020.coling-main.118,Q17-1022,1,0.931397,"Missing"
2020.coling-main.118,Q16-1030,0,0.0174702,"clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words found in the external constraints, but rather the entire embeddi"
2020.coling-main.118,L16-1491,0,0.0237812,"pus.8 Based on the individual features, we next rank the candidates in C and consequently, obtain a set of ranks for each ci . The best candidate is chosen according to its average rank across all features. In our experiments, we fix the number of candidates k to 6. Evaluation Data. We run the evaluation on three standard datasets for lexical simplification: (1) LexMTurk (Horn et al., 2014). The dataset consists of 500 English instances, which are collected from Wikipedia. The complex word and the simpler substitutions were annotated by 50 crowd workers on Amazon Mechanical Turk. (2) BenchLS (Paetzold and Specia, 2016) is a merge of LexMTurk and LSeval (De Belder and Moens, 2010) containing 929 sentences. The latter dataset focuses on text simplification for children. The authors of BenchLS applied additional corrections over the instances of the two datasets. (3) NNSeval (Paetzold and Specia, 2017) is an English dataset focused on text simplification for non-native speakers and consists in total of 239 instances. Similar to BenchLS, the dataset is based on LexMTurk, but filtered for a) instances that contain a complex target word for non-native speakers, and b) simplification candidates that were found to"
2020.coling-main.118,N18-1202,0,0.0455618,"or the word-level semantic similarity, yields better performance than the lexically blind “vanilla” BERT on several language understanding tasks. Concretely, LIBERT outperforms BERT in 9 out of 10 tasks of the GLUE benchmark and is on a par with BERT in the remaining one. Moreover, we show consistent gains on 3 benchmarks for lexical simplification, a task where knowledge about word-level semantic similarity is paramount, as well as large gains on lexical reasoning probes. 1 Introduction Unsupervised pretraining models, such as GPT and GPT-2 (Radford et al., 2018; Radford et al., 2019), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2019) yield state-of-the-art performance on a wide range of natural language processing tasks. All these models rely on language modeling (LM) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent resea"
2020.coling-main.118,D19-1005,0,0.0962985,"e text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wietin"
2020.coling-main.118,D19-1250,0,0.0534456,"Missing"
2020.coling-main.118,2020.emnlp-main.617,1,0.883586,"Missing"
2020.coling-main.118,D18-1026,1,0.91771,"Missing"
2020.coling-main.118,D19-1226,1,0.894989,"Missing"
2020.coling-main.118,D16-1264,0,0.10254,"Missing"
2020.coling-main.118,D18-1299,0,0.0170595,"n the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28th International Conference on Computational Linguistics, pages 1371–1383 Barcelona, Spain (Online), December 8-13, 2020 simplification (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019). Existing specialization methods are, however, not directly applicable to unsupervised pretraining models because they are either (1) tied to a particular training objective of a static word embedding model, or (2) predicated"
2020.coling-main.118,K15-1026,0,0.0583297,"rch threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracki"
2020.coling-main.118,D13-1170,0,0.0139938,"Missing"
2020.coling-main.118,N18-1103,1,0.886385,"Missing"
2020.coling-main.118,N18-1048,1,0.919448,"Missing"
2020.coling-main.118,W18-3018,1,0.88785,"Missing"
2020.coling-main.118,W18-5446,0,0.34382,"ances in the training batch: LLRC = − ∑ ln yˆ k · yk . (2) k where y ∈ {[0, 1], [1, 0]} is the true relation label for a word-pair training instance. 4 Language Understanding Evaluation To isolate the effects of injecting linguistic knowledge into BERT, we train base BERT and LIBERT in the same setting: the only difference is that we additionally update the parameters of LIBERT’s Transformer encoder based on the gradients of the LRC loss LLRC from Eq. (2). In the first set of experiments, we probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordN"
2020.coling-main.118,Q19-1040,0,0.0209636,"lization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification, predicting if sentences from linguistic publications are grammatically acceptable; 4 We acknowledge that training the models on larger corpora would likely lead to better absolute downstream scores; however, the main goal of this work is not to achieve state-of-the-art downstream performance, but to compare the base model against its lexically informed counterpart. 5 Note again that similar to work of Vuli´c (2018), both WordNet synonyms and direct hyponym-hypernym pairs are treated exactly the same: as positive examples for the relation of true semantic simil"
2020.coling-main.118,Q15-1025,0,0.524219,", 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resources to static word embeddings (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018, inter alia) in order to emphasize a particular lexical relation in a specialized embedding space. For instance, lexically informed word vectors specialized for pure semantic similarity result in substantial gains in a number of downstream tasks where such similarity plays an important role, e.g., in dialog state tracking (Mrkˇsi´c et al., 2017; Ren et al., 2018) or for lexical This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1371 Proceedings of the 28"
2020.coling-main.118,N18-1101,0,0.139902,"Missing"
2020.coling-main.118,P14-2089,0,0.0618712,"ations is an extensively researched problem. For instance, clearly discerning between true/pure semantic similarity and broader conceptual relatedness in static embeddings benefits a range of natural language understanding tasks such as dialog state tracking (Mrkˇsi´c et al., 2017), text simplification (Glavaˇs and Vuli´c, 2018), and spoken language understanding (Kim et al., 2016). The most widespread solution relies on the use of specialization algorithms to enrich word embeddings with external lexical knowledge and steer them towards a desired lexical relation. Joint specialization models (Yu and Dredze, 2014; Kiela et al., 2015; Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) jointly train word embedding models from scratch and enforce the external constraints with an auxiliary objective. On the other hand, retrofitting models are postprocessors that fine-tune pretrained word embeddings by gauging pairwise distances according to the external constraints (Faruqui et al., 2015; Wieting et al., 2015; Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Jo and Choi, 2018; Lengerich et al., 2018). More recently, retrofitting models have been extended to specialize not only words foun"
2020.coling-main.118,D14-1161,0,0.0234709,"e probe the usefulness of injecting semantic similarity knowledge on the well-known suite of GLUE tasks (Wang et al., 2018), while we also present the results on lexical simplification, another task that has been shown to benefit from lexico-semantic similarity specialization (Glavaˇs and Vuli´c, 2018), later in §5. 4.1 Experimental Setup Pretraining Data. We minimize BERT’s original objective LMLM + LNSP on training examples coming from English Wikipedia.4 We obtain the set of constraints C for the LLRC term from the body of previous work on semantic specialization of static word embeddings (Zhang et al., 2014; Vuli´c et al., 2018; Ponti et al., 2018). In particular, we collect 1,023,082 synonymy pairs from WordNet (Miller, 1995) and Roget’s Thesaurus (Kipfer, 2009) and 326,187 direct hyponym-hypernym pairs (Vuli´c and Mrkˇsi´c, 2018) from WordNet, and use them as positive instances for the binary classifier (LRC).5 Fine-Tuning (Downstream) Tasks. We evaluate BERT and LIBERT on the the following tasks from the GLUE benchmark (Wang et al., 2018), where sizes of training, development, and test datasets for each task are provided in Table 1: CoLA (Warstadt et al., 2019): Binary sentence classification"
2020.coling-main.118,P19-1139,0,0.144719,"M) objectives that exploit the knowledge encoded in large text corpora. BERT (Devlin et al., 2019), as one of the current state-of-the-art models, is pretrained on a joint objective consisting of two parts: (1) masked language modeling (MLM), and (2) next sentence prediction (NSP). Through both of these objectives, BERT still consumes only the distributional knowledge encoded by word co-occurrences. While several concurrent research threads are focused on making BERT optimization more robust (Liu et al., 2019) or on imprinting external world knowledge on its representations (Sun et al., 2019; Zhang et al., 2019; Sun et al., 2020; Liu et al., 2020; Peters et al., 2019; Wang et al., 2020, inter alia), no study yet has been dedicated to mitigating a severe limitation that contextualized representations and unsupervised pretraining inherited from static word embeddings: every model that relies on distributional patterns has a tendency to conflate together pure lexical semantic similarity with broader topical relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). In the past, a plethora of models have been proposed for injecting linguistic constraints (i.e., lexical knowledge) from external resource"
2020.coling-main.118,2020.acl-main.201,0,0.015309,"ther the entire embedding space. In explicit retrofitting models (Glavaˇs and Vuli´c, 2018; Glavaˇs and Vuli´c, 2019), a (deep, non-linear) specialization function is directly learned from external constraints. Post-specialization models (Vuli´c et al., 2018; Ponti et al., 2018; Kamath et al., 2019; Biesialska et al., 2020), instead, propagate lexico-semantic information to unseen words by imitating the transformation undergone by seen words during the initial specialization. This family of models can also transfer specialization across languages (Glavaˇs and Vuli´c, 2018; Ponti et al., 2019; Zhang et al., 2020). The goal of this work is to move beyond similarity-based specialization of static word embeddings only. We present a novel methodology for enriching unsupervised pretraining models such as BERT (Devlin et al., 2019) with readily available discrete lexico-semantic knowledge, and measure the benefits of such semantic specialization on similarity-oriented downstream applications. 1372 2.2 Injecting Knowledge into Unsupervised Pretraining Models Unsupervised pretraining models do retain some of the limitations of static word embeddings. First, they still conflate separate lexico-semantic relatio"
2020.coling-main.345,P14-1126,0,0.0570066,"Missing"
2020.coling-main.345,D11-1006,0,0.0370746,"merges parses by all 42 parsers, but uses oracle performance as parser weights; E NS -A LL – ensembles all 42 parsers, with equal weights. An exception is the MSP model which is not an ensemble model, but rather trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact tha"
2020.coling-main.345,D19-1103,0,0.0113354,"lumn, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data"
2020.coling-main.345,K19-1029,0,0.0286747,"Missing"
2020.coling-main.345,P12-1066,0,0.0306445,"and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we als"
2020.coling-main.345,petrov-etal-2012-universal,0,0.639447,"2020; Lauscher et al., 2020). Therefore, cross-lingual transfer of dependency parsers has profiled as the most viable strategy to use parsing technology in resource-low languages (McDonald et al., 2011; Søgaard, 2011; Kondratyuk ¨ un et al., 2020). Delexicalized transfer is conceptually the least demanding option and Straka, 2019; Ust¨ in terms of language-specific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Com"
2020.coling-main.345,P11-1157,0,0.0929328,"Missing"
2020.coling-main.345,P18-1142,1,0.928731,"Missing"
2020.coling-main.345,J19-3005,1,0.844979,"Missing"
2020.coling-main.345,D15-1039,0,0.0451007,"Missing"
2020.coling-main.345,Q17-1020,0,0.0720177,"ific resource requirements. The only provision, in order to transfer the parser trained on a delexicalized treebank of a resource-rich language, is a POS tagger in a low-resource target language based on the Universal POS (UPOS) tagset (Petrov et al., 2012). Delexicalized transfer is nowadays used primarily as a simple yet competitive baseline for more sophisticated transfer models when porting parsing technology in a new language. However, in realistic truly low-resource setups, one cannot guarantee additional resources such as parallel sentences (Ma and Xia, 2014; Rasooli and Collins, 2015; Rasooli and Collins, 2017; Wang et al., 2019; Zhang et al., 2019), word alignments (Lacroix et al., 2016), sufficiently large monolingual corpus in the target language (Mulcaire et al., 2019), and language coverage This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 3886 Proceedings of the 28th International Conference on Computational Linguistics, pages 3886–3898 Barcelona, Spain (Online), December 8-13, 2020 in massively multilingual language models which are used as the basis for modern parsers (Kondratyuk ¨ un et al.,"
2020.coling-main.345,P15-2040,0,0.277129,"Missing"
2020.coling-main.345,N06-2033,0,0.09635,"determine a threshold τ ∈ [0, 1] that defines the set of “good enough” parsers, in relative terms w.r.t. the performance of the best parser. The sets of parsers whose trees are to be merged are obtained as follows: {iILPS }τ (j) = {i|∀i : yˆi,j ≥ max(ˆ yi,j ) · τ }, (8) {iSBPSILPS }τ = {i |∀i : y¯i ≥ max(¯ yi ) · τ }. (9) where Eq (8) refers to the pure ILPS setting, and Eq. (9) refers to the SBPSILPS setting. 3.4 Reparsing After selecting multiple parsers in the ensemble settings, we need to merge their produced parse trees into a final tree. Such a step is commonly referred to as reparsing (Sagae and Lavie, 2006). Here we resort to a standard reparsing procedure in which we: (1) merge the trees produced by individual parsers into a weighted graph G – the parser i contributes to an edge with the weight wi = yˆi,j (for pure ILPS; for SBPSILPS , wi = y¯i ) if the parser i predicted that edge, and with wi = 0 otherwise; (2) induce the Maximum Spanning Tree (MST) of G (Edmonds, 1967) as the final parse of the input UPOS-sentence (see again Figure 2). 4 Experimental Setup Data. We perform all experiments on the UD v2.3 dataset,7 as it contains a wide array of both resourcerich languages with large treebanks"
2020.coling-main.345,D18-1545,0,0.0710174,"Missing"
2020.coling-main.345,C12-2115,0,0.20997,"ance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin et al. (2019): they learn to score and rank languages in order to predict the top transfer languages. However, contrary to their work, our approach doe"
2020.coling-main.345,P11-2120,0,0.0388067,"ve treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given dataset, instance-based treatment closes the gap to the best achievable result given an array of pretrained parsers, as we also show in §2. Early efforts in this line of research include data point selection where language models are used to capture the prevalent syntactic structure of a language and score potential training instances such that a multi-source parser is trained on the mixture of training instances that are most similar to the test language instances (Søgaard, 2011). Instead of instance selection one can also apply instance reweighting in accordance to their similarity to the test language (Søgaard and Wulff, 2012). Regardless of whether we attempt to align languages on an instance-level or on a treebank-level there is a need for a similarity measure between languages. Prior work relied on existing manually curated resources such as the URIEL ˇ database (Littell et al., 2017), using the KL-Divergence on POS-trigrams (Rosa and Zabokrtsk´ y, 2015), or handcrafted features derived from the datasets at hand. Our work is most similar to the recent work of Lin"
2020.coling-main.345,N13-1126,0,0.0427885,"Missing"
2020.coling-main.345,2020.emnlp-main.180,0,0.0293142,"Missing"
2020.coling-main.345,D19-1102,0,0.0129897,"¨ un et al., 2020). Thus, delexicalized transfer still remains a widely useful baseline and Straka, 2019; Ust¨ and plausible option (Johannsen et al., 2016; Agi´c, 2017). Cross-lingual transfer comes in two main flavors. We either (1) choose the best parser from a set of available parsers, trained on treebanks of various resource-rich languages (single-best parser selection, SBPS) or (2) use the parser trained on a mixture of treebanks of (ideally related) resource-rich languages (multi-source parser transfer, MSP). Other transfer paradigms, like data augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), assume the existence of at least a small treebank for a target language, violating the assumption of a (treebank-wise) fully low-resource target language. Both SBPS and MSP rely on some measure of structural alignment between languages in order to select either the single best source language parser (SBPS) or a set of (syntactically related) source languages (MSP). Existing solutions rely on measures like the Kullback–Leibler (KL) divergence between sourceˇ and target-language distributions of POS trigrams (Rosa and Zabokrtsk´ y, 2015), which can be unreliable for small target language corpo"
2020.coling-main.345,Q16-1035,0,0.0200005,"er languages. However, contrary to their work, our approach does not employ a model to learn the ranking, but transforms the labels to directly reflect the ranking when we train the scoring model. In addition, we stress the importance of instance-based learning for cross-lingual parser transfer in particular. Another core difference is that our approach is an end-to-end system without external resources or handcrafted static features. Instead, our framework relies on trainable parser embeddings that encode the necessary features in a single representation. From another viewpoint, the work of (Wang and Eisner, 2016; Wang and Eisner, 2018a; Wang and Eisner, 2018b) explores the potential of synthesizing and reordering delexicalized POS sequences to come up with better parser transfer without unrealistic assumptions on target-language resources. Their work in synthetic delexicalization is compatible with ours as it lends itself entirely to instance-based parsing. Finally, the line of work by Ammar et al. (2016) in learning monolithic models over multiple languages, 3894 and its continuation for zero-shot learning by Kondratyuk and Straka (2019) also promises to abstract away from language boundaries, but s"
2020.coling-main.345,Q18-1046,0,0.0679321,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D18-1163,0,0.0518162,"w-resource languages with small test treebanks. For our experiments, we select 42 languages with the largest treebanks as our resource-rich source languages for training, and a set of 20 typologically diverse low-resource 7 https://universaldependencies.org/ 3891 Figure 3: Performance (UAS) for single-parser se- Figure 4: Performance (UAS) for ensemble (i.e., lection models, micro- and macro- averaged, respec- few-parser selection) models, micro- and macrotively, across 20 test languages. averaged, respectively, across 20 test languages. languages for testing.8 Following established practice (Wang and Eisner, 2018b), at inference we use gold UPOS-tags of test treebanks for all models in comparison.9 ILPS Hyperparameters are optimized via fixed-split cross-validation on our training set (see §3.1). We set the embedding size for both parser embeddings and UPOS-tag embeddings, as well as the hidden size of the feed-forward Transformer layers to 256. The transformer encoder has NT = 3 layers with 8 attention heads in each layer. We update the model in mini-batches of 16 examples, using Adam (Kingma and Ba, 2015) with the default parameters: β1 = 0.9, β2 = 0.999, and  = 10−8 , with an initial learning rate"
2020.coling-main.345,D19-1575,0,0.0388791,"Missing"
2020.coling-main.345,D15-1213,0,0.017293,"her trains a single parser on the concatenation of all training treebanks. Ma & Mi: average performance across 20 languages, macro- and micro-averaged scores, respectively. The best result in each column, not considering oracle scores, is in bold. 6 Related Work Parsing languages with no training data has been a very active topic of research for nearly a decade since the pivotal works by McDonald et al. (2011) and Petrov et al. (2012). Many diverse approaches are explored along the lines of model transfer, annotation projection, machine translation (T¨ackstr¨om et al., 2013; Guo et al., 2015; Zhang and Barzilay, 2015; Tiedemann and Agi´c, 2016; Rasooli and Collins, 2017), and selective sharing based on language typology (Naseem et al., 2012) and structural similarity (Ponti et al., 2018; Meng et al., 2019). However, vast majority of prior work involves bulk evaluation, whereby transfer parsers are validated by mean accuracy on test data. Such evaluation protocols stand in contrast with the fact that languages exhibit high variance in syntactic structure, which calls for a sensitive treatment of every sentence. While an oracle single-source parser may be appropriate for the majority of sentences in a given"
2020.coling-main.345,D19-1092,0,0.0217857,"Missing"
2020.coling-main.345,P15-1119,0,\N,Missing
2020.coling-main.345,N16-1121,0,\N,Missing
2020.coling-main.345,P16-2091,1,\N,Missing
2020.coling-main.345,W17-0401,1,\N,Missing
2020.coling-main.345,K17-3002,0,\N,Missing
2020.coling-main.345,E17-2002,0,\N,Missing
2020.coling-main.345,N19-1423,0,\N,Missing
2020.coling-main.416,P19-1509,0,0.0234685,"al aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single"
2020.coling-main.416,2020.acl-main.407,0,0.0838667,"tured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach no"
2020.coling-main.416,2020.acl-main.747,0,0.0845909,"Missing"
2020.coling-main.416,N19-1423,0,0.0219833,"ew-shot machine translation, and inductive biases for language. To all of these we cannot do full justice given space constraints. Pretraining for Transfer Learning. Unsupervised pretraining on large collections of unlabelled text yields general-purpose contextualized word representations (Peters et al., 2018; Howard and Ruder, 2018) 4717 that are beneficial across a range of downstream NLP tasks. The current dominant paradigm is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-age"
2020.coling-main.416,2020.acl-main.143,0,0.106931,"an inform models of language. Inductive Biases for Language. Finally, a series of recent works has investigated how to construct neural models that are inductively biased towards learning new natural languages. This endeavour is motivated both by the need of sample efficiency and concerns of cognitive realism, as children can acquire language from limited stimuli (Chomsky, 1978). In particular, neural weights reflecting linguistic universals in phonotactics can be learned via approximate Bayesian inference (Ponti et al., 2019b) or meta-learning (McCoy et al., 2020). Papadimitriou and Jurafsky (2020) found that recurrent models pretrained on non-linguistic data with latent structure (such as music or code) facilitate natural language tasks. To our knowledge, we are the first to propose grounded communication as a non-linguistic source for 4718 pretraining, based on the hypothesis that modal and functional knowledge is a crucial inductive bias for fast and effective language acquisition. 3 Model Architecture The proposed method comprises the standard two stages of transfer learning. First, as detailed in § 3.1, we pretrain two speaker-listener agents via emergent communication on image ref"
2020.coling-main.416,P15-2139,0,0.0284187,"follow a simple architecture from prior work (Houlsby et al., 2019), and comprise linear layers with residual connections and dropout, as illustrated in Figure 1. 3.3 Regularisation with Annealing During fine-tuning, we also add to the objective an annealed regulariser for the encoder-decoder parameters (which, on the other hand, does not apply to the adapter module). These parameters are initialised using the parameters w? transferred from the EC agents. We can then define a regularisation term that prevents the parameters w from drifting away from their initialisation w? during fine-tuning (Duong et al., 2015): R = α kw − w? k2 (7) where α is a positive real-valued tunable hyper-parameter denoting the strength of the regularisation penalty. Note that this amounts to placing a prior N (w? , Iα−1 ) on the encoder-decoder parameters. However, the contribution of the log-prior in Eq. (7) to the posterior probability of the parameters should stay fixed, whereas the contribution of the negative log-likelihood in Eq. (6) should grow linearly with the number of examples. In other words, the likelihood should be able to overwhelm the prior in the limit of infinite data. For this reason, the importance of th"
2020.coling-main.416,I17-1014,0,0.0609495,"Missing"
2020.coling-main.416,W16-3210,0,0.0602061,"Missing"
2020.coling-main.416,D19-1384,0,0.104738,"raw images offers a favourable inductive bias for natural language tasks. In particular, we experiment with initialising an encoder-decoder model for few-shot neural machine translation with parameters pretrained on emergent communication. In the past, emergent communication has mostly attracted theoretical interest as a tool to shed light on cooperative behaviours, the compositional properties of emergent communication protocols (Lazaridou et al., 2017; Havrylov and Titov, 2017; Cao et al., 2018; Li and Bowling, 2019; Kaji´c et al., 2020), and natural language evolution (Kottur et al., 2017; Graesser et al., 2019). To our knowledge, this is the first preliminary study on deploying artificial languages from emergent communication in natural language applications. Conversely, our method also constitutes an extrinsic evaluation protocol to probe the properties of different emergent languages. The underlying assumption is that they should facilitate downstream tasks only to the extent that they share common characteristics with natural languages. In particular, we run in-depth analyses on the impact that the rate of communication success and maximum sequence length have on NMT performance. For the sake of"
2020.coling-main.416,N18-1032,0,0.020273,"idou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and N"
2020.coling-main.416,D18-1398,0,0.0253552,"idou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and N"
2020.coling-main.416,P18-1031,0,0.0573029,"Missing"
2020.coling-main.416,2020.tacl-1.5,0,0.0124059,"e given space constraints. Pretraining for Transfer Learning. Unsupervised pretraining on large collections of unlabelled text yields general-purpose contextualized word representations (Peters et al., 2018; Howard and Ruder, 2018) 4717 that are beneficial across a range of downstream NLP tasks. The current dominant paradigm is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some share"
2020.coling-main.416,D14-1086,0,0.0310622,"neau et al., 2020) or few examples in a target resourcepoor language (Lauscher et al., 2020). However, even raw texts required for pretraining are scant (Kornai, 2013): for instance, Wikipedia dumps cover 278 languages out of the 7,097 spoken world-wide (Eberhard et al., 2020). For this reason, we push the idea of cross-lingual knowledge transfer even further, exploring and profiling a setting where not even raw natural language data for a target language are available for unsupervised pretraining. In their stead, we exploit artificial languages emerging from a referential game on raw images (Kazemzadeh et al., 2014; Lazaridou et al., 2017). In particular, we encourage agents to cooperate in identifying images among distractors by communicating over vocabularies whose meanings are unknown. The key intuition is that, whereas lexicalisation is mostly arbitrary (Saussure, 1916), communication grounded in a real-world environment does constrain what languages are likely or This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 4716 Proceedings of the 28th International Conference on Computational Linguistics, pages"
2020.coling-main.416,2020.blackboxnlp-1.2,0,0.0979359,"y, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable to truly resource-lean languag"
2020.coling-main.416,2005.mtsummit-papers.11,0,0.0920641,"dal MT, contains multilingual captions for ≈ 30k images. We discard images and run text-only fine-tuning and evaluation on English-German (EN - DE) and English-Czech (EN - CS) in both directions. We rely on the default training set of 29,000 pairs of parallel sentences, which we also subsample to simulate true few-shot scenarios: we randomly select 500, 1,000, and 10,000 sentence pairs for the lower-resource setups. In all experimental runs, we use the original validation set spanning 1,014 sentence pairs and the default test set spanning 1,000 pairs. We also run experiments on Europarl data (Koehn, 2005) from OPUS (Tiedemann, 2009) for two language pairs: English-Romanian (EN - RO) and English-French (EN - FR), again in both directions. We retain only sentences with a length between 5 and 15 words to construct data sets whose average sentence length is similar to that of Multi30k. We then randomly sample 10,000 parallel sentences as our (largest) training set, while two other disjoint random samples of 1,500 sentence pairs are used for validation and test, respectively. As with Multi30k, we again sample 500 and 1,000 training instances from the full set of 10k examples to simulate few-shot se"
2020.coling-main.416,D17-1321,0,0.0615366,"Missing"
2020.coling-main.416,D18-1549,0,0.0534502,"Missing"
2020.coling-main.416,2020.emnlp-main.363,1,0.828492,"Missing"
2020.coling-main.416,2020.acl-main.685,0,0.0680796,"ameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable to truly resource-lean languages but also substantially superior in performance on a same dataset such as English-German Multi30k (see § 5). Another strand of recent research (Lowe et al., 2019; Lowe et al., 2020; Lazaridou et al., 2020) aims at enhancing emergent communication success by encouraging agents to imitate natural language data supplied at the beginning of training. Our work goes the opposite direction and investigates whether an emergent communication protocol pretrained without any human language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a;"
2020.coling-main.416,2020.acl-main.465,0,0.0186262,"m is training a Transformer-based deep model (Vaswani et al., 2017) relying on masked language modeling or a similar objective, as proposed in the omnipresent BERT model (Devlin et al., 2019) and its extensions (Liu et al., 2019; Conneau and Lample, 2019; Song et al., 2019; Joshi et al., 2020), and then fine-tuning the model further on a downstream task (Wang et al., 2019). Often this approach exploits large textual data and deep models spanning even billions of parameters (Conneau et al., 2020; Raffel et al., 2019; Brown et al., 2020). In this work, we refrain from chasing task leaderboards (Linzen, 2020) and posit a fundamental question about language learning instead. Emergent Communication. The functional aspect of language (Clark, 1996) can be captured by artificial multi-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (L"
2020.coling-main.416,2020.tacl-1.47,0,0.0224364,"uman language data can benefit downstream NLP applications such as machine translation. Few-shot Neural Machine Translation. Our work addresses the problem of few-shot machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and Nishida, 2017; Elliott and K´ad´ar, 2017; Lee et al., 2018). On the contrary, we ground our neural model on visual knowledge acquired from agent interactions without any observation of human language, and then fine-tune our model on translation tasks even with as few as 500 to 1, 000 t"
2020.coling-main.416,2020.findings-emnlp.397,1,0.769368,"i-agent games (Kirby, 2002; Mordatch and Abbeel, 2018), in which agents have to communicate about some shared input space (e.g., images). A common emergent communication protocol has been adopted in a large body of recent research: a speaker encodes a piece of information into a sequence of discrete symbols (emergent language) and a listener then aims to decipher the sequence and recover the original piece of information (Lazaridou et al., 2017; Havrylov and Titov, 2017; Lazaridou et al., 2018; Bouchacourt and Baroni, 2018; Chaabouni et al., 2019; Li and Bowling, 2019; Chaabouni et al., 2020; Luna et al., 2020; Kharitonov and Baroni, 2020, inter alia). The present work is partly inspired by the work of Lee et al. (2018), who train agents to communicate about images with their natural language captions and use their parameters as encoder-decoders for machine translation. However, this framework relies on the availability of natural language captions (whereas we use only artificial languages emerging from raw images). Moreover, it does not cast EC as pretraining followed by NMT few-shot fine-tuning; rather, it learns a model in a single stage. These differences make our approach not only applicable t"
2020.coling-main.416,N18-1202,0,0.0826799,"Missing"
2020.coling-main.416,2020.emnlp-demos.7,1,0.810541,"Missing"
2020.coling-main.416,2020.emnlp-main.617,1,0.869992,"Missing"
2020.coling-main.416,D18-1039,0,0.0218539,"t machine translation with limited parallel data. Differently from previous methods (Lample et al., 2018b; Lample et al., 2018a; Lample et al., 2018c; Gu et al., 2018a; Artetxe et al., 2018), our approach does not draw upon auxiliary language data for pretraining, which usually consists of machine translation tasks on other languages (Gu et al., 2018b) or domains (Sharaf et al., 2020), multilingual training (Aharoni et al., 2019; Liu et al., 2020), language model pretraining on monolingual data (Conneau and Lample, 2019; Siddhant et al., 2020), back-translation techniques on monolingual data (Platanios et al., 2018; Edunov et al., 2018), leveraging bilingual dictionaries (Duan et al., 2020), treebanks (Ponti et al., 2018), or image captions (Nakayama and Nishida, 2017; Elliott and K´ad´ar, 2017; Lee et al., 2018). On the contrary, we ground our neural model on visual knowledge acquired from agent interactions without any observation of human language, and then fine-tune our model on translation tasks even with as few as 500 to 1, 000 training instances. We rely on few-shot MT as a standard, well-known, and sound testbed to empirically validate the crucial question of this work, that is, whether emergent"
2020.coling-main.416,P18-1142,1,0.88487,"Missing"
2020.coling-main.416,J19-3005,1,0.856711,"Missing"
2020.coling-main.416,D19-1288,1,0.864033,"Missing"
2020.coling-main.416,W18-6319,0,0.0148031,"MT model is the standard seq2seq model whose architecture is exactly the same as our proposed model, but now with randomly initialised parameters (rather than transferred from EC). We extensively search the hyper-parameter space of the baseline model (Sennrich and Zhang, 2019) and adopt Adam optimiser with learning rate of 0.001, β1 = 0.9, β2 = 0.999,  = 1e-08, a dropout rate of 0.2, a batch size of 128, a hidden-state size of 512, an embedding size of 256, and a max sequence length of 80. For all models, we rely on beam search with beam size 12 for decoding. The evaluation metric is BLEU-4 (Post, 2018). 5 Results and Analysis In what follows, we report the NMT results of our proposed model on all language pairs. We then perform an ablation study highlighting the individual contributions—of the customised adapter layer, the strategies for annealing the regulariser, and emergent communication pretraining—to the final results. Finally, we assess the impact of the rate of communication success and maximum sequence length on downstream NMT performances. Main Results. The BLEU scores of the model leveraging both EC pretraining and adapters are shown in Table 1 for the Multi30k dataset, and in Tab"
2020.coling-main.416,P19-1021,0,0.0201136,"on the scores on the EN - DE validation set (in the 1k training setup) and fixed to those values in all other experiments and for all other language pairs. For a fair comparison, the other hyper-parameters for fine-tuning are set identically to the NMT baseline introduced in the next paragraph. NMT Baseline and Evaluation Details. The baseline NMT model is the standard seq2seq model whose architecture is exactly the same as our proposed model, but now with randomly initialised parameters (rather than transferred from EC). We extensively search the hyper-parameter space of the baseline model (Sennrich and Zhang, 2019) and adopt Adam optimiser with learning rate of 0.001, β1 = 0.9, β2 = 0.999,  = 1e-08, a dropout rate of 0.2, a batch size of 128, a hidden-state size of 512, an embedding size of 256, and a max sequence length of 80. For all models, we rely on beam search with beam size 12 for decoding. The evaluation metric is BLEU-4 (Post, 2018). 5 Results and Analysis In what follows, we report the NMT results of our proposed model on all language pairs. We then perform an ablation study highlighting the individual contributions—of the customised adapter layer, the strategies for annealing the regulariser"
2020.coling-main.416,P16-1162,0,0.028224,"(EN - FR), again in both directions. We retain only sentences with a length between 5 and 15 words to construct data sets whose average sentence length is similar to that of Multi30k. We then randomly sample 10,000 parallel sentences as our (largest) training set, while two other disjoint random samples of 1,500 sentence pairs are used for validation and test, respectively. As with Multi30k, we again sample 500 and 1,000 training instances from the full set of 10k examples to simulate few-shot settings. For each language pair, we lowercase and tokenise the data using byte-pair encoding (BPE) (Sennrich et al., 2016). Our BPE vocabularies are derived from all 29,000 training pairs (for the Multi30k language pairs) and 10,000 training pairs (for the Europarl language pairs). We again use Adam in the same configuration as EC pretraining, except for setting the dropout rate to 0.2. The hyper-parameters of the annealed regulariser are set to α = 5 and λ = 0.998 based on the scores on the EN - DE validation set (in the 1k training setup) and fixed to those values in all other experiments and for all other language pairs. For a fair comparison, the other hyper-parameters for fine-tuning are set identically to t"
2020.coling-main.416,2020.ngt-1.5,0,0.0482746,"Missing"
2020.coling-main.416,D19-1077,0,0.0203283,"s and extrinsic evaluation of artificial languages. 1 Introduction Zero-shot and few-shot learning are notoriously challenging for neural networks (Bottou and Bousquet, 2008; Vinyals et al., 2016; Ravi and Larochelle, 2017). However, they are a prerequisite for natural language processing in most languages, which suffer from the paucity of annotated data (Ponti et al., 2019a). State-of-the-art models rely on knowledge transfer, whereby an encoder is pretrained via language modeling on texts from multiple languages, and subsequently ‘fine-tuned’ on labelled examples of resource-rich languages (Wu and Dredze, 2019; Conneau et al., 2020) or few examples in a target resourcepoor language (Lauscher et al., 2020). However, even raw texts required for pretraining are scant (Kornai, 2013): for instance, Wikipedia dumps cover 278 languages out of the 7,097 spoken world-wide (Eberhard et al., 2020). For this reason, we push the idea of cross-lingual knowledge transfer even further, exploring and profiling a setting where not even raw natural language data for a target language are available for unsupervised pretraining. In their stead, we exploit artificial languages emerging from a referential game on raw ima"
2020.coling-main.416,D19-1143,0,0.0264866,"r. A second MLP2 is used by 1 Note that we use each listener module as an MT encoder and each speaker module as a decoder. In addition, we train two separate agents because vocabulary sizes of SRC and TRG languages are different and we adopt disjoint input embeddings. 2 We will experiment with Transformer-based architectures (Vaswani et al., 2017) in future work. Our choice of GRU is also partially motivated by recent results in few-shot MT showing on-par or even slightly stronger performance of recurrent networks over Transformers when only a small number of parallel sentences are available (Zhou et al., 2019). 3 Another common approach is based on reinforcement learning, but recent work suggests that it is less effective and converges more slowly than Gumbel-Softmax for EC tasks (Havrylov and Titov, 2017; Lee et al., 2018). 4719 Speaker to project each GRU hidden state—one for each time step—into vectors with dimensionality equal to the predefined vocabulary size of the emergent language. Image Inference. Given the input image, the generated message describing the image, and K confounding images, Listeners must now guess the correct input image among the distractors. To do so, a second GRU layer d"
2020.coling-main.423,N09-1003,0,0.0866242,"ings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datas"
2020.coling-main.423,2020.emnlp-main.618,0,0.013873,"ining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multiling"
2020.coling-main.423,D14-1034,1,0.80299,"er languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter SpA-Verb) comprises verb classes and unmatched coverage of nearly 30k verb similarity scores. In this work, we demonstrate that their large-scale dataset creation methodology based on spatial arrangement (SpAM) can be extended to o"
2020.coling-main.423,Q17-1010,0,0.0123746,"nge of direction). Whereas in Italian and English, verbs describing motion towards the speaker/listener form a distinct cluster. These preliminary analyses suggest that the collected semantic multi-arrangement data may support many other, fine-grained and in-depth lexical-typological analyses in future work, e.g., focusing on cross-lingual comparisons of the organisation of different semantic fields and examination of the most salient meaning dimensions underlying a given conceptual space. 4 Evaluation Evaluation is focused on two types of representation architectures: static word embeddings (Bojanowski et al., 2017) and more recently proposed large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabil"
2020.coling-main.423,N19-1423,0,0.404351,"he performance of large language-specific pretraining models with their multilingual equivalent on semantic clustering and lexical similarity, across different domains of verb meaning. We release the data from both phases as a large-scale multilingual resource, comprising 85 verb classes and nearly 130k pairwise similarity scores, offering a wealth of possibilities for further evaluation and research on multilingual verb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of lan"
2020.coling-main.423,C18-1323,0,0.017561,"or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently int"
2020.coling-main.423,D16-1235,1,0.79979,"Missing"
2020.coling-main.423,J15-4004,1,0.908022,"uced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets"
2020.coling-main.423,S13-2049,0,0.0308016,"Missing"
2020.coling-main.423,kipper-etal-2006-extending,1,0.609614,"n Word similarity has been widely used as a go-to intrinsic evaluation task, in which rankings of similarity scores computed between word embeddings produced by representation models are compared against ranked human similarity judgments. The dataset design involving sets of word pairs and their associated rating on a discrete scale has been particularly common, due to its reliance on non-expert native speaker judgments, quicker and cheaper to obtain than the large expert-curated lexical-semantic or semanticsyntactic resources such as WordNet (Fellbaum, 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these d"
2020.coling-main.423,D19-1279,0,0.0158964,"erb semantics. 1 Introduction Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned represen"
2020.coling-main.423,J99-4009,0,0.0289894,"which optimises the evidence collected for the dissimilarity estimates (see Figure 1). The final representational dissimilarity matrix (RDM) estimate is produced by statistically combining the evidence from multiple subsequent 2D arrangements and contains a dissimilarity estimate for each pairing of words in the set (see Kriegeskorte and Mur (2012) for the details). The dissimilarities collected for each Phase 1 class are then normalised to ensure inter-class consistency in the final dataset. The main advantages of the spatial arrangement method lie in its intuitiveness, rooted in psychology (Lakoff and Johnson, 1999; Gärdenfors, 2004; Casasanto, 2008), and flexibility, due to the reliance on fluid item placements simultaneously expressing multi-way similarity judgments, rather than discrete numerical scores. By repeatedly considering subsets of items, the users reflect on relative differences in meaning between different configurations of words, which decreases bias from placement error, order of presentation and judgment context. The two-phase design offers a practical advantage for porting the method to other languages. The approach starts from a verb sample, rather than a set of word pairs, which allo"
2020.coling-main.423,K19-1004,1,0.892956,"Missing"
2020.coling-main.423,2020.lrec-1.705,1,0.859868,"Missing"
2020.coling-main.423,L18-1008,0,0.0142817,"large pretrained encoders (Devlin et al., 2019). We compare their ability to capture word-level semantics across languages and domains of verb meaning. We also contrast the performance of language-specific BERT models with their massively multilingual counterpart (Devlin et al., 2019), and examine the impact of computing word-level representations in context, rather than by feeding items to a pretrained model in isolation. Representation Models. We evaluate FAST T EXT (FT) as a representative non-contextualised word embedding model with proven representation capabilities on diverse NLP tasks (Mikolov et al., 2018) and coverage of 157 languages. For multi-word expressions, we compute their representations by averaging the vectors of their constituent words. We contrast the performance of FT vectors with the omnipresent state-of-the-art BERT model (Devlin et al., 2019). We derive word-level BERT representations of words and multi-word expressions in two different ways: (a) in isolation and (b) in context. In method (a), we follow the steps of Liu et al. (2019) by (1) feeding each item to the pretrained model in isolation, (2) averaging 3 The easier, higher-IAA classes tend to include verbs whose meanings"
2020.coling-main.423,Q17-1022,1,0.893661,"Missing"
2020.coling-main.423,2020.acl-main.720,0,0.011818,"recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in"
2020.coling-main.423,P19-1493,0,0.0239074,"s in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of t"
2020.coling-main.423,2020.tacl-1.54,0,0.012083,"ular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to address this deficit by releasing a multilingual resource targeting verb semantics in a typologically diverse selection of languages where no such datasets have hitherto been available. The motivation behind the specific focus on verbs is twofold: (i) the importance of accurate and nuanced representation of verb meaning in light of their pivotal role in sentence structure and the still subpar verbal reasoning ability of SOTA models (Rogers et al., 2020), and (ii) the scarcity of verb data in evaluation datasets currently available. To this end, we employ a recently proposed two-phase data collection method (Majewska et al., 2020) combining semantic clustering (Phase 1) and finer-grained spatial arrangements of words based on their similarity (Phase 2), and evaluate its cross-lingual applicability. Using cross-lingual mappings, This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 4810 Proceedings of the 28th International Conference on Computationa"
2020.coling-main.423,L18-1152,0,0.0326317,"(Kipper Schuler, 2005; Kipper et al., 2006). In English, examples include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). Analogous datasets have been created in other languages, either through translation from an existing English dataset (e.g., from SimLex: German, Italian, and Russian (Leviant and Reichart, 2015), Hebrew and Croatian (Mrkši´c et al., 2017) and Polish (Mykowiecka et al., 2018)), or from a new set of concept pairs (e.g., Turkish (Ercan and Yıldız, 2018), Mandarin Chinese (Huang et al., 2019), Japanese (Sakaizawa and Komachi, 2018)). While these datasets are dominated by nouns (e.g., SimLex includes 222 verb pairs), verb-oriented datasets are harder to come by. In English, these include datasets of Yang and Powers (2006) (130 verb pairs), Baker et al. (2014) (143 verb pairs), Gerz et al. (2016) (3,500 verb pairs). A recent multilingual word similarity dataset, Multi-SimLex (Vuli´c et al., 2020), extends coverage of verb semantic similarity to 469 verb pairs in 12 languages, including Mandarin Chinese, Finnish, and Polish. Another recently introduced large-scale English verb resource of Majewska et al. (2020) (hereafter"
2020.coling-main.423,C10-1119,1,0.808447,"ased for ZH, JA (BERT- BASE with and without whole word masking (+WWM)), PL, FI, and IT (BERT- BASE and BERT- BASE - XXL trained on a larger Italian corpus), available in the Transformers repository (Wolf et al., 2019).4 4.1 Semantic Verb Clustering First, we evaluate the models on semantic clustering, where the task is to group the starting verb sample (Table 1, N verbs) into clusters based on semantic similarity. For each vector collection, we apply the spectral clustering algorithm (Meila and Shi, 2001; Yu and Shi, 2003), shown to produce strong results in previous work on verb clustering (Sun et al., 2010; Scarton et al., 2014; Vuli´c et al., 2017), and evaluate the produced groupings against the Phase 1 classes in each language using standard clustering evaluation metrics, modified purity (M P UR) (i.e., mean precision of induced verb clusters) and weighted class accuracy (WACC), calculated as follows: P M P UR = C∈Clust,nprev(C) >1 ntest_verbs nprev(C) P (1) WACC = ndom(C) ntest_verbs C∈Gold (2) where (1) each cluster C from the set of all KClust automatically induced clusters Clust is associated with its prevalent Phase 1 class, and nprev(C) is the number of verbs in an induced cluster C ap"
2020.coling-main.423,D17-1270,1,0.90543,"Missing"
2020.coling-main.423,2020.cl-4.5,1,0.881421,"Missing"
2020.coling-main.423,D19-1575,0,0.0122398,"Many recent efforts in semantic modeling have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most lan"
2020.coling-main.423,D19-1077,0,0.0176768,"ng have focused on unsupervised pretraining to extend the benefits offered by recently proposed text encoders (Devlin et al., 2019) to new languages and domains. In these approaches, general language representations are learned from large volumes of unlabeled text, and subsequently leveraged in downstream systems by means of fine-tuning on a given supervised task. The release of large multilingual pretrained encoders (Devlin et al., 2019; Conneau and Lample, 2019) boosted the state of the art on a range of multilingual tasks (Kondratyuk and Straka, 2019; Wang et al., 2019; Pires et al., 2019; Wu and Dredze, 2019; Hu et al., 2020; Artetxe et al., 2020; Qiu et al., 2020; Mueller et al., 2020). In parallel, the number of language-specific pretrained architectures available has also been steadily growing, with the advantage of being more attuned to the properties of the language in question (Virtanen et al., 2019; Nozza et al., 2020). The ease of incorporating these powerful encoders into downstream task pipelines has made them widely popular. However, there is a disproportionate shortage of resources allowing for probing of the learned representations in most languages. The aim of this work is to addres"
2020.coling-main.559,S19-2007,0,0.127781,"Missing"
2020.coling-main.559,R19-1132,0,0.0175161,"(Zampieri et al., 2020) introduced a multilingual data set for 5 languages (English, Arabic, Danish, Hebrew, Turkish), which was expanded to German and Italian by Casula (2020). The 1 In the actual experiments, we do not assess if the raw text is considered abusive - the criterion for sentence inclusion is that it simply contains at least one cue word that is considered abusive - e.g., stupid, racist, hate, fool, kill, ridiculous. 6351 HatEval shared task (Basile et al., 2019) spans only English and German, and other works (Steinberger et al., 2017; Sohn and Lee, 2019; Ousidhoum et al., 2019; Steimel et al., 2019; Stappen et al., 2020; Corazza et al., 2020, inter alia) similarly target only major European languages such as French, German, Italian, Czech, and Spanish.2 As indicated by Stappen et al. (2020), annotated evaluation data for more diverse and resource-poor languages is a prerequisite to develop portable and widely reachable abusive language detection methodology. With XH ATE -999, we make a step towards reaching out also to such languages. In the cross-lingual settings, Steinberger et al. (2017) train separate detection models for several languages, but link results via named entities and di"
2020.coling-main.559,steinberger-etal-2017-cross,0,0.0242634,"es, realized mostly through shared tasks. The recent OffenseEval task (Zampieri et al., 2020) introduced a multilingual data set for 5 languages (English, Arabic, Danish, Hebrew, Turkish), which was expanded to German and Italian by Casula (2020). The 1 In the actual experiments, we do not assess if the raw text is considered abusive - the criterion for sentence inclusion is that it simply contains at least one cue word that is considered abusive - e.g., stupid, racist, hate, fool, kill, ridiculous. 6351 HatEval shared task (Basile et al., 2019) spans only English and German, and other works (Steinberger et al., 2017; Sohn and Lee, 2019; Ousidhoum et al., 2019; Steimel et al., 2019; Stappen et al., 2020; Corazza et al., 2020, inter alia) similarly target only major European languages such as French, German, Italian, Czech, and Spanish.2 As indicated by Stappen et al. (2020), annotated evaluation data for more diverse and resource-poor languages is a prerequisite to develop portable and widely reachable abusive language detection methodology. With XH ATE -999, we make a step towards reaching out also to such languages. In the cross-lingual settings, Steinberger et al. (2017) train separate detection models"
2020.coling-main.559,K19-1088,0,0.0133012,"who experiment with multi-task learning for domain transfer on three data sets. In a similar vein, Karan and Šnajder (2018) employ frustratingly easy domain adaptation (Daumé III, 2007) to experiment with domain transfer on a wide range of abusive language data sets. Some cross-domain approaches rely on term analysis, e.g., Wiegand et al. (2018a) start from a manually constructed sample of abusive terms and augment it automatically to aid domain adaptation, while Rizoiu et al. (2019) aim to construct task-agnostic representations of abusive language. This stands in contrast with insights from Swamy et al. (2019), which suggest that the high variation in abusive language typically precludes wide generalisations and domain adaptation. The work of Pamungkas and Patti (2019) is closest to ours, as they provide some preliminary experiments on domain transfer across languages, mostly indicating its complexity, key challenges, and usefulness of available abusive language lexicons. However, they focus on readily available and unaligned data sets in major European languages (English, German, Italian, Spanish), do not provide direct comparisons across languages, now enabled by XH ATE -999, and do not investiga"
2020.coling-main.559,R15-1086,0,0.0716638,"Missing"
2020.coling-main.559,D19-1449,1,0.879871,"Missing"
2020.coling-main.559,N16-2013,0,0.0108282,"modeling (MLM) (i.e., the so-called intermediate MLM-ing) on automatically extracted “hateful” raw text in the target languages.1 We show that this additional language and domain adaptation of the base massively multilingual model can yield further performance gains: we obtain higher scores than MLM-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grain"
2020.coling-main.559,W17-3012,0,0.0548048,"-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been p"
2020.coling-main.559,W16-5618,0,0.0275167,"he so-called intermediate MLM-ing) on automatically extracted “hateful” raw text in the target languages.1 We show that this additional language and domain adaptation of the base massively multilingual model can yield further performance gains: we obtain higher scores than MLM-ing on randomly sampled raw text of the same size, confirming that both language adaptation and adaptation to abusive language are required to boost transfer performance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical"
2020.coling-main.559,2020.semeval-1.213,0,0.0309202,"ant. While such methods cannot completely replace human moderators, they are very helpful as assistance tools, offering moderation suggestions, thus partially automating and expediting human moderation work. The focus of abusive language detection is still predominantly on a single language – English, and single-domain setups (e.g., Twitter). However, some recent initiatives have aspired to broaden the scope of abusive detection methodology to other languages, showcasing the usefulness of cross-lingual transfer for the task (Sohn and Lee, 2019; Stappen et al., 2020; Pamungkas and Patti, 2019; Wiedemann et al., 2020, inter alia). Another line of research (Wiegand et al., 2018a; Karan and Šnajder, 2018; Waseem et al., 2018, inter alia) focuses on benefits of cross-domain transfer in monolingual settings. An interesting aspect, currently lacking in prior work, is the interaction of cross-lingual and cross-domain settings. Furthermore, except for some notable exceptions discussed in §2, previous work in cross-lingual setups is still tied to resource-rich and typologically similar languages (e.g., English, German, Spanish, Italian) (Stappen et al., 2020). We aim to fill both these gaps by introducing XH ATE"
2020.coling-main.559,N18-1095,0,0.232338,"ors, they are very helpful as assistance tools, offering moderation suggestions, thus partially automating and expediting human moderation work. The focus of abusive language detection is still predominantly on a single language – English, and single-domain setups (e.g., Twitter). However, some recent initiatives have aspired to broaden the scope of abusive detection methodology to other languages, showcasing the usefulness of cross-lingual transfer for the task (Sohn and Lee, 2019; Stappen et al., 2020; Pamungkas and Patti, 2019; Wiedemann et al., 2020, inter alia). Another line of research (Wiegand et al., 2018a; Karan and Šnajder, 2018; Waseem et al., 2018, inter alia) focuses on benefits of cross-domain transfer in monolingual settings. An interesting aspect, currently lacking in prior work, is the interaction of cross-lingual and cross-domain settings. Furthermore, except for some notable exceptions discussed in §2, previous work in cross-lingual setups is still tied to resource-rich and typologically similar languages (e.g., English, German, Spanish, Italian) (Stappen et al., 2020). We aim to fill both these gaps by introducing XH ATE -999, a multilingual data set annotated for abusive language"
2020.coling-main.559,N19-1060,0,0.0283851,"similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been pointed out that existing abusive language data sets are biased towards certain types of abuse (Jurgens et al., 2019; Vidgen and Derczynski, 2020) and domains/topics (Wiegand et al., 2019). In this work, we combine three different abusive language variants – hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), and attack (Wulczyn et al., 2017) – spanning three distinct data sources (comments under Fox News stories, Twitter/Facebook posts, and Wikipedia edit messages, respectively) into an integrated and cross-language aligned multilingual evaluation resource. Multilingual and Cross-Lingual Abusive Language Detection. There is a growing body of work on abusive language detection for other languages, realized mostly through shared tasks. The recent OffenseEval task"
2020.coling-main.559,N19-1144,0,0.0358884,"erformance. 2 Related Work and Motivation Variants of Abusive Language. Abusive language appears in many flavors, including sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2019), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), cyberbullying (Van Hee et al., 2015; Sprugnoli et al., 2018), misogyny (Fersini et al., 2018), obscenity, threats, and insults. Waseem et al. (2017) proposed a systematic typology of toxic language. Another typology focusing more on the nature of targets of abusive texts was proposed by Zampieri et al. (2019). A similar scheme, expanded to include the personal sentiments of annotators, was introduced by Ousidhoum et al. (2019). A very fine-grained hierarchical annotation scheme including 81 different types of annotations was used to label the data set of Fortuna et al. (2019). Furthermore, Founta et al. (2018) propose an iterative crowdsourcing-based approach to derive a set of high-quality abusive language labels. Recently, it has been pointed out that existing abusive language data sets are biased towards certain types of abuse (Jurgens et al., 2019; Vidgen and Derczynski, 2020) and domains/topi"
2020.emnlp-demos.7,S17-2001,0,0.0526606,"Missing"
2020.emnlp-demos.7,2020.acl-main.747,0,0.143648,"Missing"
2020.emnlp-demos.7,N19-1423,0,0.0455398,"state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in lowresource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml. 1 Introduction Recent advances in NLP leverage transformerbased language models (Vaswani et al., 2017), pretrained on large amounts of text data (Devlin et al., 2019; Liu et al., 2019; Conneau et al., 2020). These models are fine-tuned on a target task and achieve state-of-the-art (SotA) performance for most natural language understanding tasks. Their performance has been shown to scale with their size (Kaplan et al., 2020) and recent models have reached ∗ 1 *Equal contribution. https://github.com/huggingface/transformers 46 Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 46–54 c November 16-20, 2020. 2020 Association for Computational Linguistics hance transformers with adapter modules that can be combined with existing SotA models with min"
2020.emnlp-demos.7,N16-1181,0,0.0279735,"l´ 2 Aishwarya Kamath , Ivan Vuli´c4 , Sebastian Ruder5 , Kyunghyun Cho2,3 , Iryna Gurevych1 1 Technical University of Darmstadt 2 New York University 3 CIFAR Associate Fellow 4 University of Cambridge 5 DeepMind AdapterHub.ml Abstract billions of parameters (Raffel et al., 2019; Brown et al., 2020). While fine-tuning large pre-trained models on target task data can be done fairly efficiently (Howard and Ruder, 2018), training them for multiple tasks and sharing trained models is often prohibitive. This precludes research on more modular architectures (Shazeer et al., 2017), task composition (Andreas et al., 2016), and injecting biases and external information (e.g., world or linguistic knowledge) into large models (Lauscher et al., 2019; Wang et al., 2020). Adapters (Houlsby et al., 2019) have been introduced as an alternative lightweight fine-tuning strategy that achieves on-par performance to full fine-tuning (Peters et al., 2019) on most tasks. They consist of a small set of additional newly initialized weights at every layer of the transformer. These weights are then trained during fine-tuning, while the pre-trained parameters of the large model are kept frozen/fixed. This enables efficient parame"
2020.emnlp-demos.7,I05-5002,0,0.0149582,"Missing"
2020.emnlp-demos.7,D19-1165,0,0.313909,"e been trained for particular tasks, domains, and languages. This opens up the possibility of building on and combining information from many more sources than was previously possible, and makes research such as intermediate task training (Pruksachatkun et al., 2020), composing information from many tasks (Pfeiffer et al., 2020a), and training models for very low-resource languages (Pfeiffer et al., 2020b) much more accessible. representations in intermediate layers of the pretrained model. Current work predominantly focuses on training adapters for each task separately (Houlsby et al., 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a,b), which enables parallel training and subsequent combination of the weights. In NLP, adapters have been mainly used within deep transformer-based architectures (Vaswani et al., 2017). At each transformer layer l, a set of adapter parameters Φl is introduced. The placement and architecture of adapter parameters Φ within a pre-trained model is non-trivial and may impact their efficacy: Houlsby et al. (2019) experiment with different adapter architectures, empirically validating that a two-layer feed-forward neural network with a bottleneck works well. While this down-"
2020.emnlp-demos.7,W07-1401,0,0.115017,"Missing"
2020.emnlp-demos.7,2020.acl-main.740,0,0.0195176,"no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracted and open-sourced (®) and visualized (¯). Pre-trained adapters are downlo"
2020.emnlp-demos.7,2020.acl-main.467,0,0.0786619,"Missing"
2020.emnlp-demos.7,P18-1031,1,0.821346,"tely, the necessity of sampling heuristics due to skewed data set sizes no longer arises. By separating knowledge extraction and composition, adapters mitigate the two most common pitfalls of multi-task learning, catastrophic forgetting and catastrophic interference. Overcoming these problems together with the availability of readily available trained task-specific adapters enables researchers and practitioners to leverage information from specific tasks, domains, or languages that is often more relevant for a specific application—rather than more general pretrained counterparts. Recent work (Howard and Ruder, 2018; Phang et al., 2018; Pruksachatkun et al., 2020; Gururangan et al., 2020) has shown the benefits of such information, which was previously only available by fully fine-tuning a model on the data of interest prior to task-specific fine-tuning. 3 & pr Loa e- di tra ng Θ ined mod ad el ap te Φ’ rs el ers od apt m d ng a di new Θ a Lo ing d Φ ad 1 & 5 Finding adapters 4 2 Training adapters 3 Φ’ Θ,Φ’ Extracting and uploading adapters 6 Inference Figure 1: The AdapterHub Process graph. Adapters Φ are introduced into a pre-trained transformer Θ (step ¬) and are trained (). They can then be extracte"
2020.emnlp-demos.7,D16-1264,0,0.0151158,"Missing"
2020.emnlp-demos.7,N19-5004,1,0.857724,"Missing"
2020.emnlp-demos.7,W19-4302,1,0.887479,"Missing"
2020.emnlp-demos.7,2020.emnlp-main.617,1,0.706042,"Missing"
2020.emnlp-demos.7,D13-1170,0,0.0449199,"020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of additional newly introduced parameters Φ within a large pre-trained model with parameters Θ. The parameters Φ are learnt on a target task while keeping Θ fixed; Φ thus learn to encode task-specific 2 Layer normalization learns to normalize the inputs across the features. This is usually done by introducing a new set of features for mean and variance. 47 Full RTE (Wang et al., 2018) MRPC (Dolan and Brockett, 2005) STS-B (Cer et al., 2017) CoLA (Warstadt et al., 2019) SST-2 (Socher et al., 2013) QNLI (Rajpurkar et al., 2016) MNLI (Williams et al., 2018) QQP (Iyer et al., 2017) 66.2 90.5 88.8 59.5 92.6 91.3 84.1 91.4 Pfeif. Houl. 70.8 89.7 89.0 58.9 92.2 91.3 84.1 90.5 CRate 69.8 91.5 89.2 59.1 92.8 91.2 84.1 90.8 64 16 2 Base #Params Size Large #Params Size 0.2M 0.9M 7.1M 0.9Mb 3.5Mb 28Mb 0.8M 3.1M 25.2M 3.2Mb 13Mb 97Mb Table 2: Number of additional parameters and compressed storage space of the adapter of Pfeiffer et al. (2020a) in (Ro)BERT(a)-Base and Large transformer architectures. The adapter of Houlsby et al. (2019) requires roughly twice as much space. CRate refers to the adap"
2020.emnlp-demos.7,P19-1355,0,0.0848206,"Missing"
2020.emnlp-demos.7,W18-5446,0,0.202668,"scalability, modularity, and composition. We now provide a few use-cases for adapters to illustrate their usefulness in practice. Task-specific Layer-wise Representation Learning. Prior to the introduction of adapters, in order to achieve SotA performance on downstream tasks, the entire pre-trained transformer model needs to be fine-tuned (Peters et al., 2019). Adapters have been shown to work on-par with full fine-tuning, by adapting the representations at every layer. We present the results of fully fine-tuning the model compared to two different adapter architectures on the GLUE benchmark (Wang et al., 2018) in Table 1. The adapters of Houlsby et al. (2019, Figure 3c) and Pfeiffer et al. (2020a, Figure 3b) comprise two and one down- and up-projection Adapters While the predominant methodology for transfer learning is to fine-tune all weights of the pre-trained model, adapters have recently been introduced as an alternative approach, with applications in computer vision (Rebuffi et al., 2017) as well as the NLP domain (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020; Pfeiffer et al., 2020a,b). 2.1 Why Adapters? Adapter Architecture Adapters are neural modules with a small amount of"
2020.emnlp-demos.7,Q19-1040,0,0.0324564,"Missing"
2020.emnlp-demos.7,N18-1101,0,0.0345727,"Missing"
2020.emnlp-main.185,2020.acl-main.747,0,0.168246,"Missing"
2020.emnlp-main.185,D18-1269,0,0.379567,"hkin et al., 2018; Sap et al., 2019, inter alia). Unfortunately, the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be"
2020.emnlp-main.185,W17-1504,0,0.0993792,"rted across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but also ranking what is most plausible. 2 The only exception is direct translation of the 272 paired English Winograd Schema Challenge instances to Japanese (Shibata et al., 2015), French (Amsili and Seminck, 2017), and Portuguese (Melo et al., 2020). 2362 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2362–2376, c November 16–20, 2020. 2020 Association for Computational Linguistics PREMISE qu en th en Sipasqa cereal mikhunanpi kuruta tarirqan. The girl found a bug in her cereal. ตาของฉันแดงและบวม My eyes became red and puffy. R C CHOICE 1 Payqa pukunman n˜ uq˜nuta churakurqan. She poured milk in the bowl. ฉันรองไห I was sobbing. CHOICE 2 Payqa manam mikhuyta munarqanchu. She lost her appetite. ฉันหัวเราะ I was laughing. Table 1: Examples of forward (Resu"
2020.emnlp-main.185,2020.emnlp-main.618,0,0.023636,"Missing"
2020.emnlp-main.185,N19-1423,0,0.197445,"ce in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such unseen languages. In particular, we investigate the resource-lean scenarios where either some monolingual data or a bilingual dictionary with English (or both) are available for the target language. In summary, we offer the following contributions. 1) We create the first large-scale multilingual evaluation set for commonsense reasoning, spanning 11 languages, and"
2020.emnlp-main.185,2020.acl-main.421,0,0.58447,", the extensive efforts related to this thread of research have so far been limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonse"
2020.emnlp-main.185,P18-1231,0,0.0284989,"ltiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to m"
2020.emnlp-main.185,2020.acl-main.653,0,0.124033,"Missing"
2020.emnlp-main.185,D19-1243,0,0.0381807,"Missing"
2020.emnlp-main.185,E17-2002,0,0.162717,"Missing"
2020.emnlp-main.185,2020.acl-main.560,0,0.0659782,"lar is still missing. In order to address this gap, we develop a novel dataset, XCOPA (see examples in Table 1), by carefully translating and re-annotating the validation and test sets of English COPA into 11 target languages. A key design choice is the selection of a typologically diverse sample of languages. In particular, we privilege variety over the abundance in digital texts. Since resource-rich languages tend to belong to a few families and areas, samples inspired by this criterion are highly biased and not indicative of true models’ performance (Gerz et al., 2018; Ponti et al., 2019a; Joshi et al., 2020; Lauscher et al., 2020). Following this guiding principle, we select 11 languages from 11 distinct families, and 5 geographical macro-areas (Africa, Eurasia, Papunesia, North America, and South America). We leverage XCOPA to benchmark a series of state-of-the-art pretrained multilingual models, including XLM - R (Conneau et al., 2020), MBERT (Devlin et al., 2019), and multilingual USE (Yang et al., 2019a). Two XCOPA languages (i.e., Southern Quechua and Haitian Creole) are out-of-sample for the pretrained models: this naturally raises the question of how to adapt the pretrained models to such"
2020.emnlp-main.185,kamholz-etal-2014-panlex,0,0.029351,"HT and QU by concatenating their respective Wikipedia dumps with their respective text from the JW300 corpus (Agi´c and Vuli´c, 2019). In total, the training size is 5,710,426 tokens for HT, and 2,263,134 tokens for QU. 2) S. Sentences in English (EN). This could prevent (catastrophic) forgetting of the source language while fine-tuning, which presumably may occur with T only. We create the English corpus of comparable size to HT and QU corpora by randomly sampling 200K sentences from EN Wikipedia. 3) D. A bilingual EN – HT and EN – QU dictionary. The dictionaries were extracted from PanLex (Kamholz et al., 2014): we retain the 5k most reliable word translation pairs according to the available PanLex confidence scores. We create a synthetic corpus from the dictionary (termed D-corpus henceforth) by concatenating each translation pair from the dictionary into a quasi-sentence. 4) T-REP. T data with all occurrences of target language terms from the 5K dictionary replaced with their English translations. HT QU CO-ZS Even massively multilingual encoders like MBERT and XLM-R, pretrained on corpora of over 100 languages, cover only a fraction of the world’s 7,000+ languages. In fact, the majority of the wor"
2020.emnlp-main.185,W15-2137,0,0.0276317,"are hidden (Niven and Kao, 2019). Finally, in §4.4 we explore several strategies to adapt massively multilingual models to new languages not observed during pretraining, such as Quechua and Haitian Creole. 4.1 Baselines We evaluate baselines in several combinations of experimental setups based on: 1) different methods for cross-lingual transfer, either based on model transfer or machine translation; 2) different multilingual pretrained encoders; 3) different sources of training and validation data. Cross-lingual Transfer Methods. We consider two high-level methods for cross-lingual transfer (Tiedemann, 2015; Ponti et al., 2019a): 1) multilingual model transfer (MuMoTr), whereby a Transformer-based encoder is pretrained on multiple languages in an unsupervised fashion, and subsequently trained on English annotated data for multiple-choice classification, therefore enabling zero-shot generalisation to the other languages. 2) translate test (TrTe), whereby target test data7 are translated into English via Google Translate. This includes all languages except for QU, for which the service is not available. Multilingual Encoders. For model transfer, we evaluate the following state-of-the-art pretraine"
2020.emnlp-main.185,2020.cl-4.5,1,0.86898,"Missing"
2020.emnlp-main.185,N18-1101,0,0.0568428,"written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languages. A standard and pragmatic approach to multilingual dataset creation is translation from an existing (English) dataset, e.g., Multi-SimLex from the extended English SimLex-999 (Hill et al., 2015), XNLI from MultiNLI (Williams et al., 2018), XQuAD from SQuAD (Rajpurkar et al., 2016), and PAWS-X from PAWS (Zhang et al., 2019). TyDiQA, however, was built independently in each language. Finally, a large number of tasks has been recently integrated into unified multilingual evaluation suites, XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020). 6 Conclusion and Future Work We presented the Cross-lingual Choice of Plausible Alternatives (XCOPA), a multilingual evaluation benchmark for causal commonsense reasoning. All XCOPA instances are aligned across 11 languages, which enables cross-lingual comparisons. The language selection"
2020.emnlp-main.185,2020.acl-demos.12,0,0.139316,"Missing"
2020.emnlp-main.185,D19-1382,0,0.392467,"limited only to the English language.2 Such a narrow scope not only curbs the development of natural language understanding tools in other languages (Bender, 2011; Ponti et al., 2019a), but also exacerbates the Anglocentric bias in modeling commonsense reasoning. In fact, the expectations about typical situations do vary across cultures (Thomas, 1983). Datasets that cover multiple languages for other natural understanding tasks, such as language inference (Conneau et al., 2018), question answering (Lewis et al., 2020; Artetxe et al., 2020a; Clark et al., 2020), and paraphrase identification (Yang et al., 2019b) have received increasing attention. In fact, the requirement to generalise to new languages encourages the development of more versatile language understanding models, which can be ported across different grammars and lexica. These efforts have recently culminated in the integration of several multilingual tasks into the XTREME evaluation suite (Hu et al., 2020). However, a compre1 Moreover, there are often multiple legitimate chains of sentences that can be invoked in between premises and hypotheses. In short, commonsense reasoning does not just involve understanding what is possible, but"
2020.emnlp-main.185,D19-1454,0,0.0645931,"Missing"
2020.emnlp-main.185,L18-1560,0,0.0311419,"ers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document classification (MLDoc; Schwenk and Li, 2018), sentiment analysis (Barnes et al., 2018), and natural language inference (XNLI; Conneau et al., 2018). Other recent multilingual sets target the QA task based on reading comprehenseen scripts (e.g., both HT and QU are written in Latin script). 13 Abductive reasoning is inference to the most plausible explanation of incomplete observations (Peirce, 1960). sion: MLQA (Lewis et al., 2020) in 7 languages, XQuAD (Artetxe et al., 2020b) in 10; and TyDiQA (Clark et al., 2020) in 11 typologically diverse languages. Further, PAWS-X (Yang et al., 2019b) evaluates paraphrase identification in 6 languag"
2020.emnlp-main.185,D18-1009,0,0.027514,"Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available, e.g., lexical semantic similarity (Multi-SimLex; Vuli´c et al., 2020), document cl"
2020.emnlp-main.185,P19-1472,0,0.0271269,"ational modeling of commonsense reasoning is the Winograd Schema Challenge (WSC; Levesque et al., 2012; Morgenstern and Ortiz, 2015). WSC consists in a pronoun coreference resolution task with paired instances, and has been recently expanded into the WinoGrande dataset (Sakaguchi et al., 2020) through crowd-sourcing. Several recent evaluation sets target particular aspects of commonsense, e.g., abductive reasoning (Bhagavatula et al., 2020),13 intents and reactions to events (Rashkin et al., 2018), social (Sap et al., 2019) and physical (Bisk et al., 2020) interactions, or visual commonsense (Zellers et al., 2019a). Others, e.g., CommonsenseQA (Talmor et al., 2019), SWAG (Zellers et al., 2018), and HellaSWAG (Zellers et al., 2019b) are cast as openended multiple-choice problems where the most sensible option is chosen. Another line of evaluation involves commonsense-enabled reading comprehension and question answering (Ostermann et al., 2018; Zhang et al., 2018; Huang et al., 2019). Multilingual Evaluation of Natural Language Understanding. While the above commonsense reasoning datasets are limited to English, several multilingual datasets for other natural language understanding tasks are available,"
2020.emnlp-main.185,J15-4004,1,\N,Missing
2020.emnlp-main.185,P11-1132,0,\N,Missing
2020.emnlp-main.185,J19-3005,1,\N,Missing
2020.emnlp-main.185,D18-1029,1,\N,Missing
2020.emnlp-main.185,N19-1131,0,\N,Missing
2020.emnlp-main.185,P19-1310,1,\N,Missing
2020.emnlp-main.185,N19-5004,0,\N,Missing
2020.emnlp-main.185,N19-1421,0,\N,Missing
2020.emnlp-main.185,D19-1288,1,\N,Missing
2020.emnlp-main.186,D19-1572,0,0.0168032,"fectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training domain, or training data size and quality (Søgaard et al., 2018; Arora et al., 2019; Vuli´c et al., 2020). In future research we also plan an in-depth study of these factors and their relation to our spectral analysis. We believe that the main insights from this study will inform and guide different cross-lingual transfer learning methods and scenarios in future work. Thes"
2020.emnlp-main.186,P19-1070,1,0.899741,"Missing"
2020.emnlp-main.186,D18-1330,0,0.0777051,"Missing"
2020.emnlp-main.186,kamholz-etal-2014-panlex,0,0.0256814,"Missing"
2020.emnlp-main.186,D19-1167,0,0.0284714,"the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and learn transfer functions can be estimated through the differences between the monolingual embeddings’ spectra. We therefore discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sor"
2020.emnlp-main.186,D18-1543,0,0.0348147,"Missing"
2020.emnlp-main.186,E17-2002,0,0.396657,"the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–20, 2020. 2020 Association for Computational Linguistics resource requirements, which makes it widely applicable across"
2020.emnlp-main.186,P12-1066,0,0.0250075,"1 Introduction The effectiveness of joint multilingual modeling and cross-lingual transfer in cross-lingual NLP is critically impacted by the actual languages in consideration (Bender, 2011; Ponti et al., 2019). Characterizing, measuring, and understanding this cross-language variation is often the first step towards the development of more robust multilingually applicable NLP technology (O’Horan et al., 2016; Bjerva et al., 2019; Ponti et al., 2019). For instance, selecting suitable source languages is a prerequisite for successful cross-lingual transfer of dependency parsers or POS taggers (Naseem et al., 2012; Ponti et al., 2018; de Lhoneux et al., 2018). In another example, with all other factors kept similar (e.g., training data size, domain similarity), the quality of machine translation also depends heavily on the properties and language proximity of the actual language pair (Kudugunta et al., 2019). In this work, we contribute to this research endeavor by proposing a suite of spectral-based measures that capture the degree of isomorphism (Søgaard et al., 2018) between the monolingual embedding spaces of two languages. Our main hypothesis is that the potential to align two embedding spaces and"
2020.emnlp-main.186,P19-1018,0,0.418289,"re discuss representative statistics of the spectrum of an embedding space (i.e., the set of the singular values of the embedding matrix), such as its condition number or its sorted list of singular values. We then derive measures for the isomorphism between two embedding spaces based on these statistics. To validate our hypothesis, we perform an extensive empirical evaluation with a range of crosslingual NLP tasks. This analysis reveals that our proposed spectrum-based isomorphism measures better correlate and explain greater variance than previous isomorphism measures (Søgaard et al., 2018; Patra et al., 2019). In addition, our measures also outperform standard approaches based on linguistic information (Littell et al., 2017), The first part of our empirical analysis targets bilingual lexicon induction (BLI), a cross-lingual task that received plenty of attention, in particular as a case study to investigate the impact of crosslanguage variation on task performance (Søgaard et al., 2018; Artetxe et al., 2018). Its popularity stems from its simple task formulation and reduced 2377 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2377–2390, c November 16–2"
2020.emnlp-main.186,P19-1493,0,0.0231134,"tistic of effective condition number in §2.1. Our study is also the first to compare language distance measures that are based on discrete linguistic information (Littell et al., 2017) with measures of isomorphism (i.e., our spectral-based measures, IS, GH), which can also be used as proxy language distance measures. Our findings, suggesting that it is possible to effectively combine these two types of language distance measures, call for further research that will advance our understanding of: 1) what knowledge is captured in monolingual and cross-lingual embedding spaces (Gerz et al., 2018; Pires et al., 2019; Artetxe et al., 2020); 2) how that knowledge complements or overlaps with linguistic knowledge compiled into lexical-semantic and typological databases (Dryer and Haspelmath, 2013; Wichmann et al., 2018; Ponti et al., 2019); and 3) how to use the combined knowledge for more effective transfer in cross-lingual NLP applications (Ponti et al., 2018; Eisenschlos et al., 2019). The differences in embedding spaces of different languages do not only depend on linguistic properties of the languages in consideration, but also on other factors such as the chosen training algorithm, underlying training"
2020.emnlp-main.186,J19-3005,1,0.889996,"Missing"
2020.emnlp-main.186,P18-1142,1,0.89642,"Missing"
2020.emnlp-main.186,W19-4328,0,0.0612639,"Missing"
2020.emnlp-main.186,P18-1072,1,0.760711,"Missing"
2020.emnlp-main.186,D19-1449,1,0.902371,"Missing"
2020.emnlp-main.186,2020.emnlp-main.257,1,0.862162,"Missing"
2020.emnlp-main.257,W13-3520,0,0.254793,"which has so far been largely attributed only to inherent typological differences. In fact, the amount of data used to induce the monolingual embeddings is predictive of the quality of the aligned cross-lingual word embeddings, as evaluated on bilingual lexicon induction (BLI). Consider, for motivation, Figure 1; it shows the performance of a state-of-the-art alignment method— RCSLS with iterative normalisation (Zhang et al., 2019)—on mapping English embeddings onto embeddings in other languages, and its correlation (ρ = 0.72) with the size of the tokenised target language Polyglot Wikipedia (Al-Rfou et al., 2013). We investigate to what extent the amount of data available for some languages and corresponding training conditions provide a sufficient explanation for the variance in reported results; that is, whether it is the full story or not: The answer is ’almost’, 3178 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3178–3192, c November 16–20, 2020. 2020 Association for Computational Linguistics that is, its interplay with inherent typological differences does have a crucial impact on the ‘alignability’ of monolingual vector spaces. We first discuss cur"
2020.emnlp-main.257,D19-1328,1,0.761399,"IM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully manually translated and double-checked by a native Spanish speaker. There are no duplicates. We also report BLI results on the PanLex test lexicons (Vuli´c et al., 2019). For English–Spanish, we create training dictionaries of sizes 1k and 5k based on PanLex (Kamholz et al., 2014) following Vuli´c et al. (2019). We exclude all words from ALL -FREQ from the training set. For EN–JA / AR BLI experiments, we rely on the standard training and test dictionaries from the MUSE benchmark (Conneau et"
2020.emnlp-main.257,N13-1090,0,0.737461,"sv hu he vi de pl ja ru zh uk 20M lt 50M 100M 200M # of tokens in Wikipedia 500M Figure 1: Performance of a state-of-the-art BLI model mapping from English to a target language and the size of the target language Wikipedia are correlated. Linear fit shown as a blue line (log scale). Word embeddings have been argued to reflect how language users organise concepts (Mandera et al., 2017; Torabi Asr et al., 2018). The extent to which they really do so has been evaluated, e.g., using semantic word similarity and association norms (Hill et al., 2015; Gerz et al., 2016), and word analogy benchmarks (Mikolov et al., 2013c). If word embeddings reflect more or less languageindependent conceptual organisations, word embeddings in different languages can be expected to be near-isomorphic. Researchers have exploited this to learn linear transformations between such spaces (Mikolov et al., 2013a; Glavaˇs et al., 2019), which have been used to induce bilingual dictionaries, as well as to facilitate multilingual modeling and cross-lingual transfer (Ruder et al., 2019). In this paper, we show that near-isomorphism arises only with sufficient amounts of training. This is of practical interest for applications of linear"
2020.emnlp-main.257,D18-1047,0,0.147371,"For clarity, the corresponding isomorphism scores (and impact of training duration on isomorphism of vector spaces) over the same training snapshots for Spanish are shown in Figure 5. (c) We again report scores without and with self-learning on EN– AR / JA BLI evaluation sets from the MUSE benchmark with 1k seed translation pairs. The results with 5k seed pairs for EN–AR / JA are available in the appendix. Dashed lines without any marks show isomorphism scores (computed by RSIM; higher is better) computed across different AR and JA snapshots. 2019) whose neighbourhoods may be more isomorphic (Nakashole, 2018). We thus create new evaluation dictionaries for English–Spanish that consist of words in different frequency bins: we sample EN words for 300 translation pairs respectively from (i) the top 5k words of the full English Wikipedia (HFREQ); (ii) the interval [10k, 20k] (MFREQ); (iii) the interval [20k, 50k] (LFREQ). The entire dataset (ALL -FREQ; 900 pairs) consists of (i) + (ii) + (iii). We exclude named entities as they are over-represented in many test sets (Kementchedjhieva et al., 2019) and include nouns, verbs, adjectives, and adverbs in all three sets. All 900 words have been carefully ma"
2020.emnlp-main.257,D18-1268,0,0.0306659,"een a source and a target embedding space (Mikolov et al., 2013a). Such mapping-based approaches assume that the monolingual embedding spaces are isomorphic, i.e., that one can be transformed into the other via a linear transformation (Xing et al., 2015; Artetxe et al., 2018a). Recent unsupervised approaches rely even more strongly on this assumption: They assume that the structures of the embedding spaces are so similar that they can be aligned by minimising the distance between the transformed source language and the target language embedding space (Zhang et al., 2017; Conneau et al., 2018; Xu et al., 2018; Alvarez-Melis and Jaakkola, 2018; Hartmann et al., 2019). 2.1 Quantifying Isomorphism We employ measures that quantify isomorphism in three distinct ways—based on graphs, metric spaces, and vector similarity. Eigenvector similarity (Søgaard et al., 2018) Eigenvector similarity (EVS) estimates the degree of isomorphism based on properties of the nearest neighbour graphs of the two embedding spaces. We first length-normalise embeddings in both embedding spaces and compute the nearest neighbour graphs on a subset of the top most frequent N words. We then calculate the Laplacian matrices L1 and"
2020.emnlp-main.257,P16-1023,0,0.146189,"Missing"
2020.emnlp-main.257,J15-4004,0,\N,Missing
2020.emnlp-main.257,D14-1162,0,\N,Missing
2020.emnlp-main.257,kamholz-etal-2014-panlex,0,\N,Missing
2020.emnlp-main.257,N15-1104,0,\N,Missing
2020.emnlp-main.257,Q17-1010,0,\N,Missing
2020.emnlp-main.257,D16-1099,0,\N,Missing
2020.emnlp-main.257,P17-1042,0,\N,Missing
2020.emnlp-main.257,D17-1207,0,\N,Missing
2020.emnlp-main.257,D18-1056,1,\N,Missing
2020.emnlp-main.257,P19-1492,0,\N,Missing
2020.emnlp-main.257,P19-1018,0,\N,Missing
2020.emnlp-main.257,D19-1449,1,\N,Missing
2020.emnlp-main.257,D15-1243,0,\N,Missing
2020.emnlp-main.363,2020.emnlp-main.618,0,0.0336263,"Missing"
2020.emnlp-main.363,2020.acl-main.421,0,0.517326,"1 Data and Web Science Group, University of Mannheim, Germany 2 Language Technology Group, University of Oslo, Norway 3 Language Technology Lab, University of Cambridge, UK 1 {anne,goran}@informatik.uni-mannheim.de, 2 vinitr@ifi.uio.no, 3 iv250@cam.ac.uk Abstract predictions in resource-lean languages? In the most extreme scenario, termed zero-shot cross-lingual transfer, not a single labeled instance exists for a target language. Recent work has placed much emphasis on this scenario exactly; in theory, it offers the widest portability across the world’s 7,000+ languages (Pires et al., 2019; Artetxe et al., 2020b; Lin et al., 2019; Cao et al., 2020; Hu et al., 2020). The current mainstay of cross-lingual transfer in NLP are approaches based on continuous crosslingual representation spaces such as cross-lingual word embeddings (CLWEs) (Ruder et al., 2019) and, most recently, massively multilingual transformer networks (MMTs), pretrained on multilingual corpora with language modeling (LM) objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). The latter have de facto become the default language transfer paradigm, with multiple studies reporting their unparalleled transfer per"
2020.emnlp-main.363,2020.acl-main.658,0,0.103664,"1 Data and Web Science Group, University of Mannheim, Germany 2 Language Technology Group, University of Oslo, Norway 3 Language Technology Lab, University of Cambridge, UK 1 {anne,goran}@informatik.uni-mannheim.de, 2 vinitr@ifi.uio.no, 3 iv250@cam.ac.uk Abstract predictions in resource-lean languages? In the most extreme scenario, termed zero-shot cross-lingual transfer, not a single labeled instance exists for a target language. Recent work has placed much emphasis on this scenario exactly; in theory, it offers the widest portability across the world’s 7,000+ languages (Pires et al., 2019; Artetxe et al., 2020b; Lin et al., 2019; Cao et al., 2020; Hu et al., 2020). The current mainstay of cross-lingual transfer in NLP are approaches based on continuous crosslingual representation spaces such as cross-lingual word embeddings (CLWEs) (Ruder et al., 2019) and, most recently, massively multilingual transformer networks (MMTs), pretrained on multilingual corpora with language modeling (LM) objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). The latter have de facto become the default language transfer paradigm, with multiple studies reporting their unparalleled transfer per"
2020.emnlp-main.363,Q19-1038,0,0.0330511,"e reliable MT hinges on availability of large parallel corpora, transfer via multilingual KBs (Camacho-Collados et al., 2016; Mrkˇsi´c et al., 2017) is impaired by the limited KB coverage and inaccurate entity linking (Moro et al., 2014; Raiman and Raiman, 2018). Therefore, recent years have seen a surge of language transfer methods based on continuous representation spaces. The previous state-of-the-art, cross-lingual word embeddings (CLWEs) (Mikolov et al., 2013; Ammar et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Glavaˇs et al., 2019; Vuli´c et al., 2019) and sentence embeddings (Artetxe and Schwenk, 2019), have most recently been replaced by massively multilingual transformers (MMTs) pretrained with LM objectives (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020). 2.2 Massively Multilingual Transformers Multilingual BERT (mBERT). At BERT’s (Devlin et al., 2019) core is a multi-layer transformer network (Vaswani et al., 2017), parameters of which are pretrained using masked language modeling (MLM) and next sentence prediction (NSP). In MLM, some tokens are masked out and they need to be recovered from the context; NSP predicts adjacency of sentences in text, informing the tra"
2020.emnlp-main.363,D19-1279,0,0.036118,"languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-shot transfer (see §4), we reserve 10 documents as the development set for our experiments and evaluate on the remaining 38 articles.4 Fine-tuning. For higher-level tasks, we perform standard downstream fine-tuning of LM-pretrained mBERT and XLM-R. For lower-level tasks, we instead freeze the transformer and train only taskspecific classifiers.5,6 We add the following task-specific architectures on top of MMTs: for DEP we add the biaffine parsing head (Dozat and Manning, 2017; Kondratyuk and Straka, 2019); for POS, we attach a simple 4 As a general note, while the effects of “translationese” might have some impact on the absolute numbers (Artetxe et al., 2020a), they are not prominent enough to have any impact on the relative trends in the reported results (e.g., zeroshot vs. few-shot performance). For both XNLI and XQuAD, the translations were done completely manually and not via post-editing of MT (which would pose a higher “translationese” risk). Moreover, having an independently created test set in each language would impede comparability across languages. 5 This gave slightly better perfo"
2020.emnlp-main.363,2020.findings-emnlp.150,0,0.106535,"Missing"
2020.emnlp-main.363,E17-2002,0,0.35783,"a particular task in consideration for transfer performance? We conduct all analyses across five different tasks, which we roughly divide into two groups: (1) “lowlevel” tasks (POS-tagging, dependency parsing, and NER); and (2) “high-level” language understanding (LU) tasks (NLI and QA). We show that transfer performance in both zero-shot and fewshot scenarios largely depends on the “task level”. (Q3) Can we (even) predict transfer performance? Running a simple regression on available transfer results, we show that we can (roughly) predict the transfer performance from (1) language proximity (Littell et al., 2017) for low-level tasks; (2) combination of language proximity and size of targetlanguage pretraining corpora for high-level tasks. (Q4) Should we focus more on few-shot transfer scenarios and quick annotation cycles? Complementing the efforts on improving zeroshot transfer (Cao et al., 2020), we point to fewshot transfer as a very effective mechanism for improving target-language performance. Similar to the seminal “pre-neural” work of Garrette and Baldridge (2013), our results suggest that only several hours (or even minutes) of annotation work can “buy” substantial performance gains for lowres"
2020.emnlp-main.363,J19-3005,1,0.781718,"Missing"
2020.emnlp-main.363,2020.acl-main.467,0,0.0550715,"Missing"
2020.emnlp-main.363,P19-1015,0,0.0356261,"E covers all 40 languages, but much smaller language subsets. 3 We leave an even more general analysis that combines transfer both across tasks (Pruksachatkun et al., 2020; Glavaˇs and Vuli´c, 2020) and across languages for future work. 4485 darin) Chinese (ZH), Finnish (FI), Hebrew (HE), Hindi (HI), Italian (IT), Japanese (JA), Korean (KO), Russian (RU), Swedish (SV), and Turkish (TR). Part-of-speech Tagging (POS). Again, we use UD and obtain the Universal POS-tag (UPOS) annotations from the same treebanks as with DEP. Named Entity Recognition (NER). We resort to the NER WikiANN dataset from Rahimi et al. (2019). We experiment with the same set of 12 target languages as in DEP and POS. Cross-lingual Natural Language Inference (XNLI). We evaluate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe"
2020.emnlp-main.363,D16-1264,0,0.219834,"luate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe et al., 2020b), created by translating the 240 dev paragraphs (from 48 documents) and corresponding 1,190 QA pairs of SQuAD v1.1 (Rajpurkar et al., 2016) to 11 languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-shot transfer (see §4), we reserve 10 documents as the development set for our experiments and evaluate on the remaining 38 articles.4 Fine-tuning. For higher-level tasks, we perform standard downstream fine-tuning of LM-pretrained mBERT and XLM-R. For lower-level tasks, we instead freeze the transformer and train only taskspecific classifiers.5,6 We add the following task-specific architectures on top of MMTs: for DEP we add the biaffine parsing head (Dozat and Manning,"
2020.emnlp-main.363,W19-6204,0,0.0672448,"Missing"
2020.emnlp-main.363,P19-1355,0,0.0488697,"Missing"
2020.emnlp-main.363,D19-5901,0,0.0231081,"ances. 4.2 Cost of Language Transfer Gains As shown in §4.1, moving to few-shot transfer can massively improve performance and reduce the gaps observed with zero-shot transfer, especially for low-resource languages. While additional finetuning on few target-language examples is computationally cheap, data annotation may be expensive, especially for minor languages. What are the annotation costs, and how do they translate into performance gains? Table 5 provides ballpark estimates for our five evaluation tasks; the estimates are based on annotation costs from the literature (Hovy et al., 2014; Tratz, 2019; Bontcheva et al., 2017; Marelli et al., 2014; Rajpurkar et al., 2016). We explain these cost-to-gain conversion estimates in more detail in Appendix C). A provocative high-level question that calls for further discussion in future work can be framed as: are GPU hours effectively more costly13 than data annotations are in the long run? While MMTs are extremely useful as general-purpose models of language, their potential for some (target) languages can be quickly unlocked by pairing them with a small number of annotated target-language exam13 Conclusion Research on zero-shot language transfer"
2020.emnlp-main.363,D19-1449,1,0.855998,"Missing"
2020.emnlp-main.363,2020.emnlp-main.257,1,0.891424,"Missing"
2020.emnlp-main.363,N18-1101,0,0.0405413,"ebrew (HE), Hindi (HI), Italian (IT), Japanese (JA), Korean (KO), Russian (RU), Swedish (SV), and Turkish (TR). Part-of-speech Tagging (POS). Again, we use UD and obtain the Universal POS-tag (UPOS) annotations from the same treebanks as with DEP. Named Entity Recognition (NER). We resort to the NER WikiANN dataset from Rahimi et al. (2019). We experiment with the same set of 12 target languages as in DEP and POS. Cross-lingual Natural Language Inference (XNLI). We evaluate on the XNLI corpus (Conneau et al., 2018) created by translating dev and test portions of the English Multi-NLI dataset (Williams et al., 2018) into 14 languages by professional translators (French (FR), Spanish (ES), German (DE), Greek (EL), Bulgarian (BG), Russian (RU), Turkish (TR), Arabic (AR), Vietnamese (VI), Thai (TH), Chinese (ZH), Hindi (HI), Swahili (SW), and Urdu (UR)). Cross-lingual Question Answering (XQuAD). We rely on the XQuAD dataset (Artetxe et al., 2020b), created by translating the 240 dev paragraphs (from 48 documents) and corresponding 1,190 QA pairs of SQuAD v1.1 (Rajpurkar et al., 2016) to 11 languages (ES, DE, EL, RU, TR, AR, VI, TH, ZH, and HI ). In order to allow for a comparison between zero-shot and few-s"
2020.emnlp-main.363,W09-3002,0,\N,Missing
2020.emnlp-main.363,Q14-1019,0,\N,Missing
2020.emnlp-main.363,P14-2062,0,\N,Missing
2020.emnlp-main.363,N13-1014,0,\N,Missing
2020.emnlp-main.363,marelli-etal-2014-sick,0,\N,Missing
2020.emnlp-main.363,Q17-1022,1,\N,Missing
2020.emnlp-main.363,P17-1042,0,\N,Missing
2020.emnlp-main.363,D17-1269,0,\N,Missing
2020.emnlp-main.363,C18-1071,0,\N,Missing
2020.emnlp-main.363,N19-1423,0,\N,Missing
2020.emnlp-main.363,2020.acl-main.560,0,\N,Missing
2020.emnlp-main.586,2020.acl-main.421,0,0.0383607,", are reported for EN and DE. 4 Results and Discussion A summary of the results is shown in Figure 2 for LSIM, in Figure 3a for BLI, in Figure 3b for CLIR, in Figure 4a and Figure 4b for RELP, and in Figure 4c for WA. These results offer multiple axes of comparison, and the ensuing discussion focuses on the central questions Q1-Q3 posed in §1.6 Monolingual versus Multilingual LMs. Results across all tasks validate the intuition that languagespecific monolingual LMs contain much more lexical information for a particular target language than massively multilingual models such as mBERT or XLM-R (Artetxe et al., 2020). We see large drops between MONO.* and MULTI.* configurations even for very high-resource languages (EN and DE), and they are even more prominent for FI and TR. Encompassing 100+ training languages with limited model capacity, multilingual models suffer from the “curse of multilinguality” (Conneau et al., 2020): they must trade off monolingual lexical information coverage (and consequently monolingual performance) for a wider language coverage.7 How Important is Context? Another observation that holds across all configurations concerns the usefulness of providing contexts drawn from external"
2020.emnlp-main.586,Q19-1004,0,0.020417,"et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across dif"
2020.emnlp-main.586,Q17-1010,0,0.807302,"r layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edm"
2020.emnlp-main.586,C18-1140,0,0.0236558,"fastText (Bojanowski et al., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning meth"
2020.emnlp-main.586,2020.nlp4convai-1.5,1,0.835928,"Missing"
2020.emnlp-main.586,D19-1627,0,0.0202607,"19; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overa"
2020.emnlp-main.586,2020.acl-main.493,0,0.0265766,"Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are"
2020.emnlp-main.586,2020.acl-main.747,0,0.0810209,"Missing"
2020.emnlp-main.586,D19-1572,0,0.019846,"match configurations that average subword embeddings from multiple contexts (AOC-10 and AOC100). However, it is worth noting that 1) perfor5 Note that RELP is structurally different from the other four tasks: instead of direct computations with word embeddings, called metric learning or similarity-based evaluation (Ruder et al., 2019), it uses them as features in a neural architecture. 6 Full results are available in the appendix. 7 For a particular target language, monolingual performance can be partially recovered by additional in-language monolingual training via masked language modeling (Eisenschlos et al., 2019; Pfeiffer et al., 2020). In a side experiment, we have also verified that the same holds for lexical information coverage. 7225 Spearman ρ correlation Spearman ρ correlation 0.55 0.45 0.35 0.25 0.15 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.55 0.45 0.35 0.25 0.15 L ≤ 12 L≤2 (a) English L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (b) Finnish 0.4 Spearman ρ correlation 0.65 Spearman ρ correlation L≤4 0.55 0.45 0.35 L≤2 L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers 0.3 0.2 0.1 0.0 L≤2 L ≤ 12 (c) Mandarin Chinese L≤4 L≤6 L ≤ 8 L ≤ 10 Average over layers L ≤ 12 (d) Russian Figure 2: Spearman’s ρ c"
2020.emnlp-main.586,D19-1006,0,0.406652,"gers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and evaluation data are readily available. We dissect the pipeline for extracting lexical representations, and divide it into crucial components, including: the underlying source LM, the selection of subword tokens, external corpora, and which Transformer layers to average o"
2020.emnlp-main.586,D18-1029,1,0.828082,"Missing"
2020.emnlp-main.586,N18-2029,1,0.893221,"Missing"
2020.emnlp-main.586,P19-1070,1,0.894028,"Missing"
2020.emnlp-main.586,P19-1356,0,0.0244048,"word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretr"
2020.emnlp-main.586,D18-1176,0,0.0193142,"l., 2017). This study has only scratched the surface of this research avenue. In future work, we plan to investigate how domains of external corpora affect AOC configurations, and how to sample representative contexts from the corpora. We will also extend the study to more languages, more lexical semantic probes, and other larger underlying LMs. The difference in performance across layers also calls for more sophisticated lexical representation extraction methods (e.g., through layer weighting or attention) similar to meta-embedding approaches (Yin and Sch¨utze, 2016; Bollegala and Bao, 2018; Kiela et al., 2018). Given the current large gaps between monolingual and multilingual LMs, we will also focus on lightweight methods to enrich lexical content in multilingual LMs (Wang et al., 2020; Pfeiffer et al., 2020). Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. The work of Goran Glavaˇs and Robert Litschko is supported by the Baden-W¨urttemberg Stiftung (AGREE grant of the Eliteprogramm). References Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. A robust self-learning method for fully unsuperv"
2020.emnlp-main.586,2005.mtsummit-papers.11,0,0.115125,"esented in the respective fastText (FT) vectors, which were trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017). The equivalent vocabulary coverage allows a direct comparison to fastText vectors, which we use as a baseline static WE method in all evaluation tasks. To retain the same vocabulary across all configurations, in AOC variants we back off to the related ISO variant for words that have zero occurrences in external corpora. For all AOC vector variants, we leverage 1M sentences of maximum sequence length 512, which we randomly sample from external corpora: Europarl (Koehn, 2005) for EN, DE, FI, available via OPUS (Tiedemann, 2009); the United Nations Parallel Corpus for RU and ZH (Ziemski et al., 2016), and monolingual TR WMT17 data (Bojar et al., 2017). Evaluation Tasks. We carry out the evaluation on five standard and diverse lexical semantic tasks: Task 1: Lexical semantic similarity (LSIM) is the most widespread intrinsic task for evaluation of traditional word embeddings (Hill et al., 2015). The evaluation metric is the Spearman’s rank correlation between the average of human-elicited semantic similarity scores for word pairs and the cosine similarity between th"
2020.emnlp-main.586,2020.acl-main.375,0,0.0252215,"Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019) and Vuli´c et al. (2020) suggest that there is a wealth of lexical knowledge available within the parameters of BERT and other LMs, a systematic empirical study across different languages is currently lacking. We present such a study, spanning six typologically diverse languages for which comparable pretrained BERT models and e"
2020.emnlp-main.586,D14-1162,0,0.0919991,"aim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter"
2020.emnlp-main.586,N18-1202,0,0.25049,"e that translation pairs indeed obtain similar representations (Q4), but the similarity depends on the extraction configuration, as well as on the typological distance between the two languages. 2 Lexical Representations from Pretrained Language Models Classical static word embeddings (Bengio et al., 2003; Mikolov et al., 2013b; Pennington et al., 2014) are grounded in distributional semantics, as they infer the meaning of each word type from its co-occurrence patterns. However, LM-pretrained Transformer encoders have introduced at least two levels of misalignment with the classical approach (Peters et al., 2018; Devlin et al., 2019). First, representations are assigned to word tokens and are affected by the current context and position within a sentence (Mickus et al., 2020). Second, tokens may correspond to subword strings rather than complete word forms. This begs the question: do pretrained encoders still retain a notion of lexical concepts, abstracted from their instances in texts? Analyses of lexical semantic information in large pretrained LMs have been limited so far, focusing only on the English language and on the task of word sense disambiguation. Reif et al. (2019) showed that senses are"
2020.emnlp-main.586,2020.emnlp-main.617,1,0.88287,"Missing"
2020.emnlp-main.586,2020.acl-main.420,0,0.0210954,"ame token tends not to be self-similar across different contexts (Ethayarajh, 2019; Mickus et al., 2020). As a consequence, we hypothesise that abstract, type-level information could be codified in lower layers instead. However, given the absence of a direct equivalent to a static word type embedding, we still need to establish how to extract such type-level information. In prior work, contextualised representations (and attention weights) have been interpreted in the light of linguistic knowledge mostly through probes. These consist in learned classifier predicting annotations like POS tags (Pimentel et al., 2020) and word senses (Peters et al., 2018; Reif et al., 2019; Chang and Chen, 2019), or linear transformations to a space where distances mirror dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between"
2020.emnlp-main.586,2020.emnlp-main.185,1,0.865778,"Missing"
2020.emnlp-main.586,N19-1112,0,0.283984,"ults indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers. 1 Introduction and Motivation Language models (LMs) based on deep Transformer networks (Vaswani et al., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al.,"
2020.emnlp-main.586,K19-1004,1,0.869925,"Missing"
2020.emnlp-main.586,2021.ccl-1.108,0,0.114032,"Missing"
2020.emnlp-main.586,2020.tacl-1.54,0,0.0262276,"., 2017), pretrained on unprecedentedly large amounts of text, offer unmatched performance in virtually every NLP task (Qiu et al., 2020). Models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019c), and T5 (Raffel et al., 2019) replaced task-specific neural architectures that relied on static word embeddings (WEs; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), where each word is assigned a single (type-level) vector. While there is a clear consensus on the effectiveness of pretrained LMs, a body of recent research has aspired to understand why they work (Rogers et al., 2020). State-of-the-art models are “probed” to shed light on whether they capture task-agnostic linguistic knowledge and structures (Liu et al., 2019a; Belinkov and Glass, 2019; Tenney et al., 2019); e.g., they have been extensively probed for syntactic knowledge (Hewitt and Manning, 2019; Jawahar et al., 2019; Kulmizev et al., 2020; Chi et al., 2020, inter alia) and morphology (Edmiston, 2020; Hofmann et al., 2020). In this work, we put focus on uncovering and understanding how and where lexical semantic knowledge is coded in state-of-the-art LMs. While preliminary findings from Ethayarajh (2019)"
2020.emnlp-main.586,D19-1374,0,0.0238678,"BLI, the respective scores are 0.486 and 0.315 with ISO, and 0.503 and 0.334 with AOC-10. Similar observations hold for FI and ZH LSIM, and also in the RELP task. In RELP, it is notable that ‘BERT-based’ embeddings can recover more lexical relation knowledge than standard FT vectors. These findings reveal that pretrained LMs indeed implicitly capture plenty of lexical type-level knowledge (which needs to be ‘recovered’ from the models); this also suggests why pretrained LMs have been successful in tasks where this knowledge is directly useful, such as NER and POS tagging (Tenney et al., 2019; Tsai et al., 2019). Finally, we also note that gains with AOC over ISO are much more pronounced for the under-performing MULTI.* configurations: this indicates that MONO models store more lexical information even in absence of context. How Important are Special Tokens? The results reveal that the inclusion of special tokens [CLS] and [SEP] into type-level embedding extraction deteriorates the final lexical information contained in the embeddings. This finding holds for different languages, underlying LMs, and averaging across various layers. The NOSPEC configurations consistently outperform their ALL and WITHCL"
2020.emnlp-main.586,2020.emnlp-main.14,0,0.0225443,"or dependency tree structures (Hewitt and Manning, 2019).1 In this work, we explore several unsupervised word-level representation extraction strategies and configurations for lexico-semantic tasks (i.e., probes), stemming from different combinations of the components detailed in Table 1 and illustrated in Figure 1. In particular, we assess the impact of: 1) encoding tokens with monolingual LM-pretrained Transformers vs. with their mas1 The interplay between the complexity of a probe and its accuracy, as well as its effect on the overall procedure, remain controversial (Pimentel et al., 2020; Voita and Titov, 2020). 7223 Component Source LM Label Short Description MONO Language-specific (i.e., monolingually pretrained) BERT Multilingual BERT, pretrained on 104 languages (with shared subword vocabulary) MULTI ISO Context AOC -M NOSPEC Subword Tokens ALL WITHCLS Layerwise Avg Each vocabulary word w is encoded in isolation, without any external context Average-over-context: average over word’s encodings from M different contexts/sentences Special tokens [CLS] and [SEP] are excluded from subword embedding averaging Both special tokens [CLS] and [SEP] are included into subword embedding averaging [CLS] is in"
2020.emnlp-main.586,2020.cl-4.5,1,0.881404,"Missing"
2020.emnlp-main.586,D19-1575,0,0.0186957,"Averaging across layers bottom-to-top (i.e., from L0 to L12 ) is beneficial across the board, but we notice that scores typically saturate or even decrease in some tasks and languages when we include 8 For this reason, we report the results of tions only in the NOSPEC setting. AOC configurahigher layers into averaging: see the scores with *.AVG(L≤10) and *.AVG(L≤12) configurations, e.g., for FI LSIM; EN/DE RELP, and summary BLI and CLIR scores. This hints to the fact that two strategies typically used in prior work, either to take the vectors only from the embedding layer L0 (Wu et al., 2020; Wang et al., 2019) or to average across all layers (Liu et al., 2019b), extract suboptimal word representations for a wide range of setups and languages. The sweet spot for n in *.AVG(L≤n) configurations seems largely task- and language-dependent, as peak scores are obtained with different n-s. Whereas averaging across all layers generally hurts performance, the results strongly suggest that averaging across layer subsets (rather than selecting a single layer) is widely useful, especially across bottom-most layers: e.g., L ≤ 6 with MONO.ISO.NOSPEC yields an average score of 0.561 in LSIM, 0.076 in CLIR, and 0.4"
2020.emnlp-main.617,D16-1264,0,0.11385,"Missing"
2020.emnlp-main.617,D19-1454,0,0.0261635,"anguages, it achieves competitive performance even for high-resource languages and on more challenging tasks. These evaluations also hint at the modularity of the adapter-based MAD-X approach, which holds promise of quick adaptation to more tasks: we use exactly the same languagespecific adapters in NER, CCR, and QA for languages such as English and Mandarin Chinese that appear in all three evaluation language samples. 7 Further Analysis Impact of Invertible Adapters We also analyse the relative performance difference of MAD-X 80 60 F1 ting from Ponti et al. (2020a)—fine-tuning first on SIQA (Sap et al., 2019) and on the English COPA training set—and report other possible settings in the appendix. Target language adaptation outperforms XLM-RBase while MAD-XBase achieves the best scores. It shows gains in particular for the two unseen languages, Haitian Creole (ht) and Quechua (qu). Performance on the other languages is also generally competitive or better. 40 Language qu cdo ilo 20 0 0 20k 40k xmf mi mhr 60k Number of iterations Epochs 25 50 100 tk gn 80k 100k Figure 4: Cross-lingual NER performance of MAD-X transferring from English to the target languages with invertible and language adapters tra"
2020.emnlp-main.617,N19-1423,0,\N,Missing
2020.emnlp-main.617,D19-1572,1,\N,Missing
2020.emnlp-main.617,W19-4330,0,\N,Missing
2020.emnlp-main.617,D19-1165,0,\N,Missing
2020.findings-emnlp.196,D18-1547,1,0.902657,"Missing"
2020.findings-emnlp.196,2020.nlp4convai-1.5,1,0.874545,"Missing"
2020.findings-emnlp.196,D18-2029,0,0.103609,"Missing"
2020.findings-emnlp.196,K18-1048,0,0.120064,"y assist users in accomplishing well-defined tasks such as finding and booking restaurants, hotels, and flights (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), with further use in tourist information (Budzianowski et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), entertainment (Fraser et al., 2018), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). They are also key Response Selection is a task of selecting the most appropriate response given the dialog history (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Chaudhuri et al., 2018). This task is central to retrieval-based dialog systems, which typically encode the context and a 1 Retrieval-based dialog is popular because posing dialog as response selection (Gunasekara et al., 2019) simplifies system design (Boussaha et al., 2019). Unlike modular or end-toend task-oriented systems, retrieval-based ones do not rely on dedicated modules for language understanding, dialog management, and generation. They mitigate the requirements for explicit task-specific semantics hand-crafted by domain experts (Henderson et al., 2014; Mrkˇsi´c et al., 2015, 2017). 2161 Findings of the As"
2020.findings-emnlp.196,W19-4330,0,0.220327,"ning the proposed dual-encoder model on the entire Reddit costs only 85 USD) and quicker development cycles offer new opportunities for more researchers and practitioners to tap into the construction of neural task-based dialog systems. 3 github.com/PolyAI-LDN/ conversational-datasets 2162 similarity score in task-based dialog has been introduced by Henderson et al. (2019b), which closely follows a related line of work focused on modelling sentence pairs for short text retrieval (Kannan et al., 2016; Henderson et al., 2017), bilingual text mining and representation learning (Guo et al., 2018; Chidambaram et al., 2019), and question answering (Humeau et al., 2020). In what follows in §2.1, we: 1) introduce ConveRT, our novel single-context dual-encoder architecture; and 2) briefly outline the quantization method. Finally, we show how to extend ConveRT into a multi-context dual encoder that works with additional context inputs (§2.2). More Compact Response Selection Model We propose ConveRT – Conversational Representations from Transformers – a compact dual-encoder pretraining architecture, leveraging subword representations, transformer-style blocks, and quantization, as illustrated in Figure 1. ConveRT sat"
2020.findings-emnlp.196,2020.acl-main.11,1,0.74962,"Missing"
2020.findings-emnlp.196,N19-1423,0,0.676664,"Inigo Casanueva, Nikola Mrkˇsi´c, Pei-Hao Su, Tsung-Hsien Wen and Ivan Vuli´c matt@poly-ai.com PolyAI Limited, London, UK Abstract components of intelligent virtual assistants such as Siri, Alexa, Cortana, and Google Assistant. Data-driven task-oriented dialog systems require domain-specific labelled data: annotations for intents, explicit dialog states, and mentioned entities (Williams, 2014; Wen et al., 2017b,a; Ramadan et al., 2018; Liu et al., 2018; Zhao et al., 2019b). This makes the scaling and maintenance of such systems very challenging. Transfer learning on top of pretrained models (Devlin et al., 2019; Liu et al., 2019, inter alia) provides one avenue for reducing the amount of annotated data required to train models capable of generalization. Pretrained models making use of languagemodel (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, response selection provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled conversational data (Henderson et al., 2019b; Mehri et al., 2019). Response selecti"
2020.findings-emnlp.196,W18-5708,0,0.0649133,"of applications. They assist users in accomplishing well-defined tasks such as finding and booking restaurants, hotels, and flights (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), with further use in tourist information (Budzianowski et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), entertainment (Fraser et al., 2018), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). They are also key Response Selection is a task of selecting the most appropriate response given the dialog history (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Chaudhuri et al., 2018). This task is central to retrieval-based dialog systems, which typically encode the context and a 1 Retrieval-based dialog is popular because posing dialog as response selection (Gunasekara et al., 2019) simplifies system design (Boussaha et al., 2019). Unlike modular or end-toend task-oriented systems, retrieval-based ones do not rely on dedicated modules for language understanding, dialog management, and generation. They mitigate the requirements for explicit task-specific semantics hand-crafted by domain experts (Henderson et al., 2014; Mrkˇsi´c et al., 2015, 2017)"
2020.findings-emnlp.196,W17-5526,0,0.060077,"Missing"
2020.findings-emnlp.196,H90-1021,0,0.392646,"Missing"
2020.findings-emnlp.196,W19-4101,1,0.900113,"Missing"
2020.findings-emnlp.196,W14-4337,1,0.805544,"Missing"
2020.findings-emnlp.196,P19-1536,1,0.899779,"Missing"
2020.findings-emnlp.196,W19-4107,0,0.39909,"f languagemodel (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, response selection provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled conversational data (Henderson et al., 2019b; Mehri et al., 2019). Response selection is also directly applicable to retrieval-based dialog systems, a popular and elegant approach to framing dialog (Wu et al., 2017; Weston et al., 2018; Mazar´e et al., 2018; Gunasekara et al., 2019; Henderson et al., 2019b).1 General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization in the dual encoder to build a lightweight mem"
2020.findings-emnlp.196,W18-6317,0,0.0249327,"training (pretraining the proposed dual-encoder model on the entire Reddit costs only 85 USD) and quicker development cycles offer new opportunities for more researchers and practitioners to tap into the construction of neural task-based dialog systems. 3 github.com/PolyAI-LDN/ conversational-datasets 2162 similarity score in task-based dialog has been introduced by Henderson et al. (2019b), which closely follows a related line of work focused on modelling sentence pairs for short text retrieval (Kannan et al., 2016; Henderson et al., 2017), bilingual text mining and representation learning (Guo et al., 2018; Chidambaram et al., 2019), and question answering (Humeau et al., 2020). In what follows in §2.1, we: 1) introduce ConveRT, our novel single-context dual-encoder architecture; and 2) briefly outline the quantization method. Finally, we show how to extend ConveRT into a multi-context dual encoder that works with additional context inputs (§2.2). More Compact Response Selection Model We propose ConveRT – Conversational Representations from Transformers – a compact dual-encoder pretraining architecture, leveraging subword representations, transformer-style blocks, and quantization, as illustrat"
2020.findings-emnlp.196,N18-1187,0,0.0412009,"Missing"
2020.findings-emnlp.196,2021.ccl-1.108,0,0.23534,"Missing"
2020.findings-emnlp.196,D18-1298,0,0.0428444,"Missing"
2020.findings-emnlp.196,P18-2069,0,0.0472661,"Missing"
2020.findings-emnlp.196,P19-1373,0,0.0752579,"retrained models (Devlin et al., 2019; Liu et al., 2019, inter alia) provides one avenue for reducing the amount of annotated data required to train models capable of generalization. Pretrained models making use of languagemodel (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, response selection provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled conversational data (Henderson et al., 2019b; Mehri et al., 2019). Response selection is also directly applicable to retrieval-based dialog systems, a popular and elegant approach to framing dialog (Wu et al., 2017; Weston et al., 2018; Mazar´e et al., 2018; Gunasekara et al., 2019; Henderson et al., 2019b).1 General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effe"
2020.findings-emnlp.196,N18-2074,0,0.0222445,"zes- M 1 of dimensionality [47, 512] and M 2 of dimensionality [11, 512]. An embedding at position i is added to: Mi1 mod 47 + Mi2 mod 11 .5 5 Note that since 47 and 11 are coprime, this gives 47·11 = 517 different possible positional encodings. Similar to the original (non-learned) positional encodings from Vaswani et al. (2017), the rationale behind this choice of positional encoding is to allow the model to generalize to unseen sequence lengths. 2163 The next layers closely follow the original Transformer architecture with some notable differences. First, we set maximum relative attention (Shaw et al., 2018) in the six layers to the following respective values: [3, 5, 48, 48, 48, 48].6 This also helps the architecture to generalize to long sequences and distant dependencies: earlier layers are forced to group together meanings at the phrase level before later layers model larger patterns (Singh et al., 2019). We use single-headed attention throughout the network.7 Before going into a softmax, we add a bias to the attention scores that depends only on the relative positions: αij → αij + Bn−i+j where B is a learned bias vector. This helps the model understand relative positions, but is much more co"
2020.findings-emnlp.196,P15-2130,1,0.897394,"Missing"
2020.findings-emnlp.196,Q17-1022,1,0.918789,"Missing"
2020.findings-emnlp.196,D19-6106,0,0.018067,"from Vaswani et al. (2017), the rationale behind this choice of positional encoding is to allow the model to generalize to unseen sequence lengths. 2163 The next layers closely follow the original Transformer architecture with some notable differences. First, we set maximum relative attention (Shaw et al., 2018) in the six layers to the following respective values: [3, 5, 48, 48, 48, 48].6 This also helps the architecture to generalize to long sequences and distant dependencies: earlier layers are forced to group together meanings at the phrase level before later layers model larger patterns (Singh et al., 2019). We use single-headed attention throughout the network.7 Before going into a softmax, we add a bias to the attention scores that depends only on the relative positions: αij → αij + Bn−i+j where B is a learned bias vector. This helps the model understand relative positions, but is much more computationally efficient than computing full relative positional encodings (Shaw et al., 2018). Again, it also helps the model generalize to longer sequences. Six Transformer blocks use a 64-dim projection for computing attention weights, a 2,048-dim kernel (feed-forward 1 in Figure 1), and 512-dim embeddi"
2020.findings-emnlp.196,P19-1355,0,0.123297,"m the general-domain response selection model pretrained on Reddit. Similar to Henderson et al. (2019b), we choose Reddit for pretraining due to: 1) its organic conversational structure; and 2) its unmatched size, as the public repository of Reddit data comprises 727M (input, response) pairs.3 Dual-Encoder for Response Selection. A dualencoder neural architecture for response selection 2 Finally, our more compact neural response selection architecture is well aligned with the recent socially-aware initiatives on reducing costs and improving fairness and inclusion in NLP research and practice (Strubell et al., 2019; Mirzadeh et al., 2019; Schwartz et al., 2019). Cheaper training (pretraining the proposed dual-encoder model on the entire Reddit costs only 85 USD) and quicker development cycles offer new opportunities for more researchers and practitioners to tap into the construction of neural task-based dialog systems. 3 github.com/PolyAI-LDN/ conversational-datasets 2162 similarity score in task-based dialog has been introduced by Henderson et al. (2019b), which closely follows a related line of work focused on modelling sentence pairs for short text retrieval (Kannan et al., 2016; Henderson et al., 20"
2020.findings-emnlp.196,W18-1819,0,0.0198252,"ary V contains 31,476 subword tokens. During training and inference, if we encounter an OOV character it is treated as a subword token, where its ID is computed using a hash function, and it gets assigned to one of 1,000 additional “buckets” reserved for the OOVs. We therefore reserve parameters (i.e., embeddings) for the 31,476 subwords from V and for the additional 1,000 OOV-related buckets. At training and inference, after the initial word-level tokenization on UTF8 punctuation and word boundaries, input text x is split into subwords following a simple left-to-right greedy prefix matching (Vaswani et al., 2018). We tokenize all responses y during training in exactly the same manner. Input and Response Encoder Networks. The subword embeddings then go through a series of transformations on both the input and the response side. The transformations are based on the standard 4 In the actual implementation, we use the same subword tokenization as Vaswani et al. (2018). We run it for 4 iterations and retain only subwords occurring at least 250 times, containing no more than 20 UTF8 characters, also disallowing more than 4 consecutive digits. hTx hy hx rx x6 Transformer layers 2.1 Transfer encodings (e.g.,"
2020.findings-emnlp.196,N19-1123,0,0.0477463,"Missing"
2020.findings-emnlp.196,P18-1103,0,0.106141,"Missing"
2020.findings-emnlp.196,D13-1096,0,0.034839,"ms or conversational agents, have found use in a wide range of applications. They assist users in accomplishing well-defined tasks such as finding and booking restaurants, hotels, and flights (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), with further use in tourist information (Budzianowski et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), entertainment (Fraser et al., 2018), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). They are also key Response Selection is a task of selecting the most appropriate response given the dialog history (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Chaudhuri et al., 2018). This task is central to retrieval-based dialog systems, which typically encode the context and a 1 Retrieval-based dialog is popular because posing dialog as response selection (Gunasekara et al., 2019) simplifies system design (Boussaha et al., 2019). Unlike modular or end-toend task-oriented systems, retrieval-based ones do not rely on dedicated modules for language understanding, dialog management, and generation. They mitigate the requirements for explicit task-specific semantics hand-crafted by domain"
2020.findings-emnlp.196,E17-1042,1,0.900173,"Missing"
2020.findings-emnlp.196,W18-5713,0,0.0282284,"eralization. Pretrained models making use of languagemodel (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, response selection provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled conversational data (Henderson et al., 2019b; Mehri et al., 2019). Response selection is also directly applicable to retrieval-based dialog systems, a popular and elegant approach to framing dialog (Wu et al., 2017; Weston et al., 2018; Mazar´e et al., 2018; Gunasekara et al., 2019; Henderson et al., 2019b).1 General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-level parameterization"
2020.findings-emnlp.196,W14-4339,0,0.0631013,"Missing"
2020.findings-emnlp.196,P17-1046,0,0.0588684,"ls capable of generalization. Pretrained models making use of languagemodel (LM) based learning objectives have become prevalent across the NLP research community. When it comes to dialog systems, response selection provides a more suitable pretraining task for learning representations that can encapsulate conversational cues. Such models can be pretrained using large corpora of natural unlabelled conversational data (Henderson et al., 2019b; Mehri et al., 2019). Response selection is also directly applicable to retrieval-based dialog systems, a popular and elegant approach to framing dialog (Wu et al., 2017; Weston et al., 2018; Mazar´e et al., 2018; Gunasekara et al., 2019; Henderson et al., 2019b).1 General-purpose pretrained sentence encoders such as BERT are not ideal for real-world conversational AI applications; they are computationally heavy, slow, and expensive to train. We propose ConveRT (Conversational Representations from Transformers), a pretraining framework for conversational tasks satisfying all the following requirements: it is effective, affordable, and quick to train. We pretrain using a retrieval-based response selection task, effectively leveraging quantization and subword-l"
2020.findings-emnlp.196,W18-3022,0,0.0283203,"se in a wide range of applications. They assist users in accomplishing well-defined tasks such as finding and booking restaurants, hotels, and flights (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), with further use in tourist information (Budzianowski et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), entertainment (Fraser et al., 2018), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). They are also key Response Selection is a task of selecting the most appropriate response given the dialog history (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Chaudhuri et al., 2018). This task is central to retrieval-based dialog systems, which typically encode the context and a 1 Retrieval-based dialog is popular because posing dialog as response selection (Gunasekara et al., 2019) simplifies system design (Boussaha et al., 2019). Unlike modular or end-toend task-oriented systems, retrieval-based ones do not rely on dedicated modules for language understanding, dialog management, and generation. They mitigate the requirements for explicit task-specific semantics hand-crafted by domain experts (Henderson et al., 2014; Mrkˇsi´c"
2020.lrec-1.705,N09-1003,0,0.89788,"sed in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013;"
2020.lrec-1.705,N19-1050,0,0.0169763,"he dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al.,"
2020.lrec-1.705,W16-2519,0,0.0178536,"verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one ta"
2020.lrec-1.705,P98-1013,0,0.705002,"ding (Jackendoff, 1972; Levin, 1993; McRae et al., 1997; Altmann and Kamide, 1999; Resnik and Diab, 2000; Ferretti et al., 2001; Sauppe, 2016, inter alia). The demand for verb-specific resources to support NLP has been recognised in recent years, as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased"
2020.lrec-1.705,D14-1034,1,0.83064,"g and spatial arrangements of words. Despite their wide usefulness, most available datasets used for intrinsic evaluation in distributional semantics are restricted in size and coverage, many do not distinguish similarity and relatedness, and only a few target verbs in particular. The prominent word pair datasets include WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009), comprising 353 noun pairs, and SimLex-999 (Hill et al., 2015), comprising 999 word pairs out of which 222 are verb pairs. Verb-only datasets include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relie"
2020.lrec-1.705,W11-2501,0,0.0337348,"e which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, generate similarity ratings for over 8 times as many verb pair"
2020.lrec-1.705,P14-1023,0,0.0147114,"e still limited, and few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-h"
2020.lrec-1.705,W16-2502,0,0.0255245,"i are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous"
2020.lrec-1.705,Q17-1010,0,0.0301069,"huler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et"
2020.lrec-1.705,P12-1015,0,0.111911,"as reflected in the publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 201"
2020.lrec-1.705,J06-1003,0,0.0402948,"inantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate cat"
2020.lrec-1.705,W16-2506,0,0.0148593,"ontext of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments,"
2020.lrec-1.705,D16-1235,1,0.902677,"Missing"
2020.lrec-1.705,W16-2507,0,0.0140033,"ample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting semantic factors. At the same time, degrees of similarity and subtle meaning distinctions between words are very difficult to quantify and translate onto a discrete scale without context or points of reference. This leads to inconsistencies in annotations by the same rater or across raters. By allowing repeated multi-wise, relative continuous similarity judgments, SpAM addresses shortcoming"
2020.lrec-1.705,N16-2002,0,0.0123547,"relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on analogy (Mikolov et al., 2013; Gladkova et al., 2016) and semantic relation datasets (Baroni and Lenci, 2011). The largest verb-focused dataset currently available, SimVerb, is a result of a crowd-sourcing effort involving over 800 raters, each completing the pairwise similarity rating task for 79 verb pairs. In this paper, we present an alternative novel approach which allows an annotator to implicitly express multiple pairwise similarity judgments by a single mouse drag, instead of having to consider each word pair independently. This lets us scale up the data collection and, starting from the same set of verbs as those used in SimVerb, genera"
2020.lrec-1.705,J15-4004,1,0.936771,"publication of a large verb similarity dataset for English, SimVerb3500 (hereafter SimVerb) (Gerz et al., 2016). However, the need for high-quality, wide-coverage lexical resources targeting verb semantics has by no means been satisfied. Rich lexical resources encoding information about verbs’ semantic properties such as FrameNet (Baker et al., 1998) or VerbNet (Kipper Schuler, 2005; Kipper et al., 2006) are still unavailable for most languages, and evaluation datasets dedicated to or dominated by nouns are by far predominant (Finkelstein et al., 2002; Agirre et al., 2009; Bruni et al., 2012; Hill et al., 2015). Therefore, we propose methodology aimed at alleviating the evaluation data scarcity problem and overcoming the bottleneck of manual gold standard creation. We present a novel approach to obtaining semantic similarity data by means of a two-phase design consisting in (1) bottom-up semantic clustering of verbs into relatednessbased classes and (2) spatial similarity judgments obtained via a multi-arrangement method so far employed only in psychology and cognitive neuroscience research and with visual stimuli (Kriegeskorte and Mur, 2012; Mur et al., 2013; Charest et al., 2014). We show how it c"
2020.lrec-1.705,S13-2049,0,0.448125,"om the final sample due to their very low frequency, resulting in a 825-verb sample. 5751 4.1. Figure 2: The rough clustering task layout (zoomed in). Verbs can be dragged onto the ‘new category’ circle to create a new grouping, onto ‘copy’ to create a duplicate label, or ‘Trash’ to dispose of the unwanted duplicate. Participants The rough clustering task was first tested by two native English speakers. They produced clusters with an encouraging degree of overlap. It was computed using the B-Cubed metric (Bagga and Baldwin, 1998) extended by Amig´o et al. (2009) to overlapping clusters and by Jurgens and Klapaftis (2013) to fuzzy clusters, as used in related work (Jurgens and Klapaftis, 2013; Majewska et al., 2018). B-Cubed, based on precision and recall, estimates the overlap between two clusterings X and Y at the item level. Let U represent the collection of items, Xi the set of clusters containing item i in clustering X, Yi the set of clusters containing i in clustering Y ; let Ji be the set of items in Xi but excluding i and Ki be the set of items in Yi but excluding i. B-Cubed precision P and recall R are defined as: P = 1 X 1 X min(|Xi ∩ Xj |, |Yi ∩ Yj |) |U |i∈U |Ji |j∈J |Xi ∩ Xj | i numerous as the bi"
2020.lrec-1.705,kipper-etal-2006-extending,1,0.837416,"Missing"
2020.lrec-1.705,N16-1095,0,0.0182253,"ts include YP-130 (Yang and Powers, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synony"
2020.lrec-1.705,P17-2074,0,0.121665,"rs, 2006) (130 verb pairs) and the dataset of Baker et al. (2014) (143 verb pairs). A resource aimed at addressing the problem of insufficient verb-specific evaluation data is SimVerb (Gerz et al., 2016), providing pairwise similarity scores for 3,500 verb pairs. Although pairwise rating datasets have been ubiquitous in intrinsic evaluation, alternative annotation approaches and dataset types have been proposed to address some of their limitations. These include best-worst scaling (Louviere and Woodworth, 1991; Louviere et al., 2015; Avraham and Goldberg, 2016; Kiritchenko and Mohammad, 2016; Kiritchenko and Mohammad, 2017; Asaadi et al., 2019), which relies on relative judgments of several items to decide which displays a given property to the highest and which to the lowest degree, and paired comparisons (Dalitz and Bednarek, 2016) (where annotators choose which of the two items has more of a given property). Models have also been evaluated on synonym detection datasets gathered via English as foreign or second language tests (Landauer and Dumais, 1997; Turney, 2001) and word games (Jarmasz and Szpakowicz, 2003), composed of 5-word tuples (one target word and 4 potential synonyms, only one correct), and on an"
2020.lrec-1.705,J99-4009,0,0.887765,"se combinations possible. However, in SpAM a subject arranges multiple stimuli simultaneously in a two-dimensional space (e.g. on a computer screen), expressing (dis)similarity through the relative positions of items within that space. The inter-stimulus Euclidean distances represent pairwise dissimilarities. This set-up ensures that all stimuli are considered in the context of the entire sample. Each placement simultaneously communicates similarity relationship of the item to all other items in the set. SpAM taps into the spatial nature of humans’ mental representation of concept similarity (Lakoff and Johnson, 1999; 5750 G¨ardenfors, 2004; Casasanto, 2008). It allows for a freer, intuitive expression of similarity judgments as continuous distances, rather than requiring assignment of discrete numerical ratings. The latter, although ubiquitous in intrinsic evaluation of distributional semantic models, have a number of limitations (Batchkarov et al., 2016; Faruqui et al., 2016; Gladkova and Drozd, 2016; Kiritchenko and Mohammad, 2017). Human ratings of isolated pairs of words are likely to be biased by word frequency, prototypicality, order of presentation and speed of association, rather than reflecting"
2020.lrec-1.705,P14-2050,0,0.0227907,"few and far between. Rich expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhar"
2020.lrec-1.705,L18-1153,1,0.89982,"Missing"
2020.lrec-1.705,W16-2523,0,0.024829,"e in legibility. Furthermore, the two-phase set-up handles ambiguity by permitting copying verb labels to capture different senses in Phase 1. The rough clustering phase guarantees that each verb label is presented in the context of related verbs in the arena in Phase 2, a necessary prerequisite for meaningful similarity judgments in psychology (Turner et al., 1987).2 The actual sense is implied by the surrounding words: this helps avoid mismatches in similarity judgments between participants for ambiguous verbs. What is more, this avoids the common problem of ambiguous low similarity scores (Milajevs and Griffiths, 2016) that conflate similarity judgments on antonyms (vanish - appear) and completely unrelated notions (fry - appear), and focuses on judgments between comparable concepts. 3.3. Data To test the scaling-up potential of our approach and to enable direct comparisons with the standard pairwise similarity rating methods, we select the 827 verbs from SimVerb (Gerz et al., 2016) as our item sample.3 The sample presents a challenge due to its size (i.e., it is almost nine times as 2 Following Turner et al. (1987), ‘stimuli can only be compared in so far as they have already been categorised as identical,"
2020.lrec-1.705,Q17-1022,1,0.925243,"Missing"
2020.lrec-1.705,D14-1162,0,0.0815648,"expert-created resources such as WordNet (Miller, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as"
2020.lrec-1.705,D18-1169,0,0.013689,"arena (inhale - exhale and sink - swim). This tendency is also illustrated by the RDM in Figure 3: separate clusters are formed by verbs such as raise, rise, grow and diminish, decline, lower, and finish is kept separate from begin and start. Crucially, our spatial approach records simultaneous judgments on multiple related words, which helps improve judgment consistency (e.g., word pairs holding analogous relations have similar scores) and allows making subtle distinctions based on varying degrees of similarity by means of 9 The ρSV scores are promising compared to the ρ = 0.612 SimVerb IAA (Pilehvar et al., 2018), despite the fact that the easy cases of verb pairs involving very disparate verbs (in different classes) are not included in our results. 10 There are exceptions: positive (e.g. love) and negative (e.g. hate) emotion verbs form two different classes; there are also separate groupings with ‘construction’ and ‘destruction’ verbs. See Table 1. 5754 600 5000 500 4000 400 Frequency Frequency 6000 3000 300 2000 200 1000 100 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 SpA-Verb dissimilarity score 1 2 3 4 5 6 7 SimVerb similarity score 8 9 10 Figure 4: Score distributio"
2020.lrec-1.705,K15-1026,0,0.0190291,"er, 1995; Fellbaum, 1998), VerbNet (Kipper Schuler, 2005; Kipper et al., 2006), or FrameNet (Baker et al., 1998) encode a wealth of semantic, syntactic and predicate-argument information for English words, but are expensive and time-consuming to create. Crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce evaluation benchmarks. Semantic models have been predominantly evaluated on datasets consisting of human similarity ratings collected for sets of word pairs (Baroni et al., 2014; Levy and Goldberg, 2014; Pennington et al., 2014; Dhillon et al., 2015; Schwartz et al., 2015; Wieting et al., 2016; Bojanowski et al., 2017; Mrkˇsi´c et al., 2017). Various views of what constitutes ‘semantic similarity’ between words have been adopted, and it is undecided what kind of meaning relationship word embeddings should capture. The term semantic relatedness has been employed to refer to words linked by any kind of semantic relation (Budanitsky and Hirst, 2001; Budanitsky and Hirst, 2006; Turney and Pantel, 2010), including synonyms (baffle-perplex), meronyms and holonyms (finger-hand) or antonyms (softhard). Similarity defined as association, i.e., the mental activation of"
2020.lrec-1.705,J06-3003,0,0.103961,".e., the mental activation of a term when another is presented (Chiarello et al., 1990; Lemaire and Denhiere, 2006), e.g., knife-murder, has been estimated in terms of frequency of co-occurrence of words in language (and the physical world) (Turney, 2001; Turney and Pantel, 2010; McRae et al., 2012; Bruni et al., 2012). In contrast to associative relatedness, a concept of semantic similarity defined in terms of shared superordinate category (Lupker, 1984; Resnik, 1995) (taxonomical similarity (Turney and Pantel, 2010)) or shared semantic features (Tversky, 1977; Frenck-Mestre and Bueno, 1999; Turney, 2006) has been proposed. Here, similarity is quantified in terms of degree of overlap in semantic properties, e.g., shared function or physical features, with synonyms occupying the top region of the similarity scale (e.g. fiddleviolin (Cruse, 1986)). In this work, we reserve the term (semantic) similarity for this latter definition of closeness of meaning, and distinguish it from the more general relatedness, which also includes association, as in previous work (Resnik, 1995; Resnik and Diab, 2000; Agirre et al., 2009; Hill et al., 2015; Gerz et al., 2016). We explore how this distinction is captu"
2020.lrec-1.705,D16-1157,0,0.0274248,"Missing"
2020.nlp4convai-1.5,W19-4330,0,0.125918,"Missing"
2020.nlp4convai-1.5,N19-1423,0,0.372961,"well as conversational systems in general) to support new target domains and tasks is a very challenging and resource-intensive process (Wen et al., 2017; Rastogi et al., 2019). The need for expert domain knowledge and domain-specific labeled data still impedes quick and wide deployment of intent detectors. In other words, one crucial challenge is enabling effective intent detection in low-data scenarios typically met in commercial systems, with only several examples available per intent (i.e., the so-called few-shot learning setups). Transfer learning on top of pretrained sentence encoders (Devlin et al., 2019; Liu et al., 2019b, inter alia) has now established as the mainstay paradigm aiming to mitigate the bottleneck with scarce in-domain data. However, directly applying the omnipresent sentence encoders such as BERT to intent detection may be sub-optimal. 1) As shown by Henderson et al. (2019b), pretraining on a general language-modeling (LM) objective for conversational tasks is less effective than conversational pretraining based on the response selection task and conversational data (Henderson et al., 2019c; Mehri et al., 2019). 2) Fine-tuning BERT and its variants is very resource-intensive"
2020.nlp4convai-1.5,W17-5526,0,0.0857054,"Missing"
2020.nlp4convai-1.5,H90-1021,0,0.726607,"Missing"
2020.nlp4convai-1.5,W19-4101,1,0.893284,"Missing"
2020.nlp4convai-1.5,D19-1542,0,0.0198838,"; Radford et al., 2019). Their core strength lies in the fact that, through consuming large general-purpose corpora during pretraining, they require smaller amounts of domain-specific training data to adapt to a particular task and/or domain (Ruder et al., 2019). The adaptation is typically achieved by adding a task-specific output layer to a large pretrained sentence encoder, and then fine-tuning the entire model (Devlin et al., 2019). However, the fine-tuning process is computationally intensive (Zafrir et al., 2019; Henderson et al., 2019b), and still requires sufficient taskspecific data (Arase and Tsujii, 2019; Sanh et al., 2019). As such, the standard full fine-tuning approach is both unsustainable in terms of resource consumption (Strubell et al., 2019), as well as suboptimal for few-shot scenarios. Dual Sentence Encoders and Conversational Pretraining. A recent branch of sentence encoders moves beyond the standard LM-based pretraining objective, and proposes an alternative objective: conversational response selection, typically on Reddit data (Al-Rfou et al., 2016; Henderson et al., 2019a). As empirically validated by Henderson et al. (2019c); Mehri et al. (2019), conversational (instead of LM-b"
2020.nlp4convai-1.5,W17-5522,0,0.106469,"ANKING 77 (ours) Table 1: Intent detection datasets: key statistics. which requires fine-grained decisions, see Table 2 (e.g., reverted top-up vs. failed top-up). Furthermore, as other examples from Table 2 suggest, it is not always possible to rely on the semantics of individual words to capture the correct intent.3 BANKING 77 4 In spite of the crucial role of intent detection in any task-oriented conversational system, publicly available intent detection datasets are still few and far between, even for English. The previous standard datasets such as Web Apps, Ask Ubuntu, the Chatbot Corpus (Braun et al., 2017) or SNIPS (Coucke et al., 2018) are limited to only a small number of classes (< 10), which oversimplifies the intent detection task and does not emulate the true environment of commercial systems. Therefore, more recent work has recognized the need for improved and more challenging intent detection datasets. 1) The dataset of Liu et al. (2019a), dubbed HWU 64, contains 25,716 examples for 64 intents in 21 domains. 2) The dataset of Larson et al. (2019), dubbed CLINC 150, spans 150 intents and 23,700 examples across 10 domains. However, the two recent English datasets are multi-domain, and the"
2020.nlp4convai-1.5,P19-1536,1,0.885438,"Missing"
2020.nlp4convai-1.5,D18-1547,1,0.919779,"Missing"
2020.nlp4convai-1.5,D19-1131,0,0.204585,"Missing"
2020.nlp4convai-1.5,P19-1355,0,0.119456,"smaller amounts of domain-specific training data to adapt to a particular task and/or domain (Ruder et al., 2019). The adaptation is typically achieved by adding a task-specific output layer to a large pretrained sentence encoder, and then fine-tuning the entire model (Devlin et al., 2019). However, the fine-tuning process is computationally intensive (Zafrir et al., 2019; Henderson et al., 2019b), and still requires sufficient taskspecific data (Arase and Tsujii, 2019; Sanh et al., 2019). As such, the standard full fine-tuning approach is both unsustainable in terms of resource consumption (Strubell et al., 2019), as well as suboptimal for few-shot scenarios. Dual Sentence Encoders and Conversational Pretraining. A recent branch of sentence encoders moves beyond the standard LM-based pretraining objective, and proposes an alternative objective: conversational response selection, typically on Reddit data (Al-Rfou et al., 2016; Henderson et al., 2019a). As empirically validated by Henderson et al. (2019c); Mehri et al. (2019), conversational (instead of LM-based) pretraining aligns better with conversational tasks such as dialog act prediction or next utterance generation. Pretraining on response select"
2020.nlp4convai-1.5,2021.ccl-1.108,0,0.274072,"Missing"
2020.nlp4convai-1.5,D19-1129,0,0.062916,"Missing"
2020.nlp4convai-1.5,E17-1042,0,0.114963,"Missing"
2020.nlp4convai-1.5,P19-1373,0,0.44838,"etups). Transfer learning on top of pretrained sentence encoders (Devlin et al., 2019; Liu et al., 2019b, inter alia) has now established as the mainstay paradigm aiming to mitigate the bottleneck with scarce in-domain data. However, directly applying the omnipresent sentence encoders such as BERT to intent detection may be sub-optimal. 1) As shown by Henderson et al. (2019b), pretraining on a general language-modeling (LM) objective for conversational tasks is less effective than conversational pretraining based on the response selection task and conversational data (Henderson et al., 2019c; Mehri et al., 2019). 2) Fine-tuning BERT and its variants is very resource-intensive as it assumes the adaptation of the full large model. Moreover, in few-shot setups fine-tuning may result in overfitting. From a commercial perspective, these properties lead to extremely slow, cumbersome, and expensive development cycles. Therefore, in this work we propose to use efficient dual sentence encoders such as Universal Sentence Encoder (USE) (Cer et al., 2018) and ConveRT (Henderson et al., 2019b) to support intent detection. These models are in fact neural Abstract Building conversational systems in new domains and"
2020.nlp4convai-1.5,N18-1202,0,0.112799,"Missing"
2020.nlp4convai-1.5,W19-4302,0,0.0432518,"Missing"
2020.nlp4convai-1.5,N19-5004,0,0.0268862,"repository: github.com/PolyAI-LDN/polyai-models. The BANKING 77 dataset is available at: github. com/PolyAI-LDN/task-specific-datasets. 2 Methodology: Intent Detection with Dual Sentence Encoders Pretrained Sentence Encoders. Large-scale pretrained models have benefited a wide spectrum of NLP applications immensely (Devlin et al., 2019; Liu et al., 2019b; Radford et al., 2019). Their core strength lies in the fact that, through consuming large general-purpose corpora during pretraining, they require smaller amounts of domain-specific training data to adapt to a particular task and/or domain (Ruder et al., 2019). The adaptation is typically achieved by adding a task-specific output layer to a large pretrained sentence encoder, and then fine-tuning the entire model (Devlin et al., 2019). However, the fine-tuning process is computationally intensive (Zafrir et al., 2019; Henderson et al., 2019b), and still requires sufficient taskspecific data (Arase and Tsujii, 2019; Sanh et al., 2019). As such, the standard full fine-tuning approach is both unsustainable in terms of resource consumption (Strubell et al., 2019), as well as suboptimal for few-shot scenarios. Dual Sentence Encoders and Conversational Pr"
2020.nlp4convai-1.5,D19-1676,0,\N,Missing
2020.repl4nlp-1.7,P19-1070,1,0.874475,"Missing"
2020.repl4nlp-1.7,P19-4007,1,0.895056,"Missing"
2020.repl4nlp-1.7,D18-1330,0,0.113978,"uli´c♦ Anna Korhonen♦ Goran Glavaš♣ ♦ Language Technology Lab, TAL, University of Cambridge ♣ Data and Web Science Group, University of Mannheim {iv250,alk23}@cam.ac.uk goran@informatik.uni-mannheim.de Abstract sources is the main reason for popularity of the socalled projection-based CLWE methods (Mikolov et al., 2013a; Artetxe et al., 2016, 2018a). These models align two independently trained monolingual word vector spaces post-hoc, using limited bilingual supervision in the form of several hundred to several thousand word translation pairs (Mikolov et al., 2013a; Vuli´c and Korhonen, 2016; Joulin et al., 2018; Ruder et al., 2018). Some models even align the monolingual spaces using only identical strings (Smith et al., 2017; Søgaard et al., 2018) or numerals (Artetxe et al., 2017). The most recent work focused on fully unsupervised CLWE induction: they extract seed translation lexicons relying on topological similarities between monolingual spaces (Conneau et al., 2018; Artetxe et al., 2018a; Hoshen and Wolf, 2018; Alaux et al., 2019). In this work, we do not focus on projection itself: rather, we investigate a transformation of input monolingual word vector spaces that facilitates the projection"
2020.repl4nlp-1.7,kamholz-etal-2014-panlex,0,0.0195343,"Missing"
2020.repl4nlp-1.7,N19-1162,0,0.0281812,"rs as well as in different BLI setups and with different CLWE methods. In future work, we will test other unsupervised post-processors, and also probe similar methods that inject external lexical knowledge into monolingual word vectors towards improved BLI. We also plan to probe if similar gains still hold with recently proposed more sophisticated self-learning methods (Karan et al., 2020), non-linear mappingbased CLWE methods (Glavaš and Vuli´c, 2020; Mohiuddin and Joty, 2020). Another idea is to also apply a similar principle to contextualised word representations in cross-lingual settings (Schuster et al., 2019; Liu et al., 2019). Mikel Artetxe, Gorka Labaka, Iñigo Lopez-Gazpio, and Eneko Agirre. 2018c. Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation. In Proceedings of CoNLL, pages 282–291. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the ACL, 5:135–146. Alexis Conneau, Guillaume Lample, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018. Word translation without parallel data. In Proceedings of ICLR. Yerai Doval, Jose Camacho-Colla"
2020.repl4nlp-1.7,2020.acl-main.618,1,0.838234,"Missing"
2020.repl4nlp-1.7,C12-1089,0,0.0444421,"I setups), and in combination with two different projection methods. 1 Introduction Cross-lingual word embeddings (CLWEs) are a mainstay of modern cross-lingual NLP (Ruder et al., 2019b). CLWE models induce a shared cross-lingual vector space in which words with similar meanings obtain similar vectors regardless of their language. Their usefulness has been attested in tasks such as bilingual lexicon induction (BLI) (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Litschko et al., 2018), machine translation (Artetxe et al., 2018b; Lample et al., 2018), document classification (Klementiev et al., 2012), and many others (Ruder et al., 2019b). Importantly, CLWEs are one of the central mechanisms for facilitating transfer of language technologies for low-resource languages, which often lack sufficient bilingual signal for obvious transfer via machine translation. Lack of language re45 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 45–54 c July 9, 2020. 2020 Association for Computational Linguistics formation on both monolingual spaces before any standard projection-based CLWE framework yields consistent BLI gains for a wide array of languages. We run"
2020.repl4nlp-1.7,J98-1004,0,0.762655,"each self-learning iteration k, a dictionary D(k) is first used to learn the joint space (k) (k) Y (k) = XW x ∪ ZW z . The mutual crosslingual nearest neighbours in Y (k) are then used to extract the new dictionary D(k+1) . Relying on mutual nearest neighbours partially removes the noise, leading to better performance. For more technical Unsupervised Monolingual Post-processing. We now outline the simple post-processing method of Artetxe et al. (2018c) used in this work, and then extend it to the bilingual setup. The core idea is to generalise the notion of first-and second-order similarity (Schütze, 1998)2 to nth-order similarity. Let us define the (standard, first-order) similarity matrix of the source language space X as M1 (X) = XX T (similar for Z). The second-order similarity can then be defined as M2 (X) = XX T XX T , where it holds M2 (X) = M1 (M1 (X)); the nth-order similarity is then Mn (X) = (XX T )n . The embeddings of words wi and wj are given by the rows i and j of each Mn matrix. We are then looking for a general linear transformation that adjusts the similarity order of input 1 Recent empirical studies (Glavaš et al., 2019; Vuli´c et al., 2019) show that, under fair evaluation,"
2020.repl4nlp-1.7,K19-1004,1,0.879568,"Missing"
2020.repl4nlp-1.7,D19-1449,1,0.839786,"Missing"
2020.repl4nlp-1.7,P16-1024,1,0.882173,"Missing"
2020.semeval-1.2,P18-1073,0,0.0246037,"traints in target languages by translating EN constraints to target languages via Google Translate. A similar approach of automatic constraint translation has already been proven very effective in the context of symmetric similarity-based specialization of embedding spaces for low-resource languages (Ponti et al., 2019). This way, Wang et al. (2020) obtain an LE-specialized embedding space for each language. Following that, in the second step, they learn a linear projection mapping between the LE-specialized monolingual spaces with the VecMap tool for inducing bilingual word embedding spaces (Artetxe et al., 2018). Recent comparative evaluations (Glavaˇs et al., 2019; Vuli´c et al., 2019a) rendered VecMap as one of the most robust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similari"
2020.semeval-1.2,I13-1095,0,0.0123381,"-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is"
2020.semeval-1.2,Q17-1010,0,0.00755369,"bust algorithms for inducing cross-lingual embedding spaces. The word translations obtained with Google Translate when translating EN constraints are also forwarded to VecMap as supervision for inducing bilingual embedding spaces. 5 Official Evaluation We now report the official results of our evaluation. We first describe the baselines (Section 5.1) and then show the performances for all submitted runs (Section 5.2).5 5.1 Baselines For the Dist track we use simple cosine similarity between distributional word vectors as a baseline. To this end, we use the 300-dimensional FastText embeddings (Bojanowski et al., 2017) trained on Wikipedias of respective languages.6 For the cross-lingual (sub)tasks we induce the bilingual embedding spaces via the simple Procrustes alignment (Smith et al., 2017), using 5K word translation dictionaries, as described in Glavaˇs et al. (2019). Since LE is an asymmetric relation and cosine similarity is a symmetric measure, we did not expect this baseline to be particularly competitive and expected most participants to outperform it. For the Any track, we used GLEN, our recent neural explicit specialization model for LE (Glavaˇs and Vuli´c, 2019) as a competitive baseline. GLEN"
2020.semeval-1.2,S16-1168,0,0.059972,"lly validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014"
2020.semeval-1.2,D15-1075,0,0.0445674,"ded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degr"
2020.semeval-1.2,P15-2001,0,0.0651471,"Missing"
2020.semeval-1.2,S17-2002,0,0.336887,"we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates a perfect LE relation between the concepts"
2020.semeval-1.2,D18-1269,0,0.0318256,"n annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014;"
2020.semeval-1.2,N13-1073,0,0.0750154,"resume something similar to be the case with LE and the proposed 4lang graphs – it is inherently difficult to create a reliable LE score based on paths and distances in a symbolic representation that is a (directed) graph. Team UAlberta (Hauer et al., 2020). The approach of UAlberta for cross-lingual binary LE detection combines sentence-level translations (i.e., parallel corpora), distributional word vectors (i.e., word embeddings) and multilingual lexical resources. Their base method, dubbed BITEXT, mines candidates for cross-lingual LE from parallel corpora – they simply run the FastAlign (Dyer et al., 2013) word alignment algorithm and assume that the LE relation holds between all aligned pairs of words. As clarified by the authors, this will, in most cases, extract cross-lingual synonyms, which, strictly speaking, do satisfy the LE relation; also, in some cases, the alignments will be established between close (e.g., first order) hyponymy-hypernymy pairs – in this case, however, the bitext alignment of words alone does not suggest the direction of the LE relation. The authors simply declare any pair of words from our cross-lingual datasets to stand in the LE relation if they find this pair in t"
2020.semeval-1.2,ehrmann-etal-2014-representing,0,0.0286213,"re of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online),"
2020.semeval-1.2,E17-1056,1,0.777636,"all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) sugg"
2020.semeval-1.2,N15-1184,0,0.0325169,"Missing"
2020.semeval-1.2,P14-1113,0,0.0244977,"has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 202"
2020.semeval-1.2,P05-1014,0,0.0147749,"Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models an"
2020.semeval-1.2,D16-1235,1,0.900545,"Missing"
2020.semeval-1.2,P18-1004,1,0.899108,"Missing"
2020.semeval-1.2,P19-1476,1,0.837489,"Missing"
2020.semeval-1.2,P19-1070,1,0.894937,"Missing"
2020.semeval-1.2,D17-1185,1,0.900423,"Missing"
2020.semeval-1.2,P16-1193,0,0.0238705,"SQ). We offered two different evaluation tracks. In the distributional (Dist) track we allowed only for fully distributional systems, capturing LE only on the basis of unannotated corpora. In contrast, the Any track invited systems that exploit any kind of additional external resources, including lexico-semantic networks. Overall, we did not observe any empirically confirmed strong systems in the Dist track, further corroborating the findings from prior work that building LE-oriented vectors distributionally is more difficult than for some other relations such as broader semantic relatedness (Henderson and Popa, 2016). However, several runs submitted to the Any track pushed the state of the art both in binary LE detection and graded LE prediction, for most of the languages and language pairs in our evaluation. 2 Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in"
2020.semeval-1.2,J15-4004,1,0.896991,"Data We started from the LE datasets we previously created and published (Vuli´c et al., 2017; Vuli´c et al., 2019b), covering four languages (EN, DE, IT, HR) and extended those datasets to two new languages (TR, SQ). For completeness, we describe the details of the annotation process and the creation of final multilingual and cross-lingual datasets for the shared task. Starting Point: Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the graded LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016; Camacho-Collados et al., 2017), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymy-hypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the graded LE “To what degree is X a type of Y?” question to human subjects, with each pair rated by at least 10 raters: the score of 6 in"
2020.semeval-1.2,W19-4310,1,0.900241,"Missing"
2020.semeval-1.2,P15-2020,1,0.902688,"Missing"
2020.semeval-1.2,S15-1019,0,0.138539,"in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of concepts (which can be both unigrams and multi-word expressions, i.e., phrases) in order to induce the directed graphs conforming to the 4lang formalism (Kornai et al., 2015). 4lang graphs are directed graphs with concepts as nodes and three types of edges: 0 0 edges of type 0 denote attribution (cat → − four-legged), lexical entailment (cat → − mammal), or 0 unary predication (cat → − meow); edges of type 1 and 2 denote relations between the predicate and its 1 2 subject and object, respectively (e.g., cat ← − catch → − mouse).4 Kov´acs et al. (2020) first extract definitions from Wiktionary using language-specific templates. Each definition is then transformed into a 4lang graph with the help of a language-specific Universal Dependencies (Nivre et al., 2016) par"
2020.semeval-1.2,P19-1313,0,0.0113187,"a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License."
2020.semeval-1.2,Q15-1016,0,0.0406784,"rediction for SQ) and each language-pair in (3) and (4) (e.g., binary LE detection for HR-TR) instantiates one concrete subtask. We allowed participants to submit their predictions for an arbitrary set of subtasks. Moreover, the participants were allowed to tackle only graded LE prediction or only binary LE detection. Evaluation Metrics. For each graded LE prediction subtask, we measured the alignment of predictions and gold LE scores using the Spearman’s Rank Correlation Coefficient (Spearman ρ), which is in line with previous work on similar concept pair scoring datasets (Hill et al., 2015; Levy et al., 2015; Vuli´c et 27 al., 2017, inter alia). For the binary LE detection subtasks we resorted to the standard F1 measure. 4 Participating Systems We now describe in more detail the approaches adopted by the three teams who submitted their system description papers.3 Team BMEAUT (Kov´acs et al., 2020). The BMEAUT method for LE detection and prediction is a rule-based approach that exploits Wiktionary definitions (Meyer and Gurevych, 2012) and relies on dependency parsing and semantic graphs. In the first step, the authors apply the dict to 4lang tool (Recski et al., 2016) on Wiktionary definitions of"
2020.semeval-1.2,S10-1002,0,0.040472,"egree is X a type of Y?”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluati"
2020.semeval-1.2,W13-0904,0,0.0173071,"c lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relation holds between concepts (“To which degree is X a type of Y?”).1 The graded nature of the"
2020.semeval-1.2,Q17-1022,1,0.903619,"Missing"
2020.semeval-1.2,S13-2005,0,0.0319147,"”).1 The graded nature of the LE relation has been empirically validated in human annotations crowdsourced for the HyperLex dataset (Vuli´c et al., 2017). Its creation catalyzed research on models for predicting graded LE (Nguyen et al., 2017; Nickel and Kiela, 2017; Vuli´c and Mrkˇsi´c, 2018; Tifrea et al., 2019; Le et al., 2019). Multilingual and Cross-Lingual Lexical Entailment. Despite its potential for a variety of crosslingual and multilingual applications such as multilingual taxonomy construction, machine translation, and multilingual natural language inference (Mihalcea et al., 2010; Negri et al., 2013; Ehrmann et al., 2014; Fu et al., 2014; Bordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barc"
2020.semeval-1.2,D18-1026,1,0.886103,"Missing"
2020.semeval-1.2,D19-1226,1,0.869316,"Missing"
2020.semeval-1.2,W16-1622,0,0.0342201,"Missing"
2020.semeval-1.2,P18-2057,0,0.0129808,"died in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the su"
2020.semeval-1.2,E14-4008,0,0.0287191,"Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared ta"
2020.semeval-1.2,P16-1226,0,0.0129442,"ection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reaso"
2020.semeval-1.2,E17-1007,0,0.0156047,"graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical enta"
2020.semeval-1.2,P06-1101,0,0.114198,"em runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction. 1 Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vaguen"
2020.semeval-1.2,N18-1056,0,0.334657,".org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE holds between concepts"
2020.semeval-1.2,N18-1103,1,0.907335,"Missing"
2020.semeval-1.2,J17-4004,1,0.90587,"Missing"
2020.semeval-1.2,N18-1048,1,0.882981,"Missing"
2020.semeval-1.2,D19-1449,1,0.894355,"Missing"
2020.semeval-1.2,P19-1490,1,0.629745,"Missing"
2020.semeval-1.2,N16-1142,0,0.0991043,"http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this paper. Our shared task had a broad scope aiming to cover reasoning over lexical entailment from multiple perspectives. Namely, the subtasks covered both monolingual and cross-lingual setups as well as both binary LE detection and graded LE prediction (i.e., prediction of a degree to which LE"
2020.semeval-1.2,2020.semeval-1.31,0,0.482762,"quire a bilingual word embedding space, merely two monolingual word embedding spaces: sets X and Y are obtained by thresholding monolingual word embedding similarities (the threshold value is tuned on the development portions of our LE sets). Finally, all possible pairs (xi , yj ) ∈ X × Y are considered to stand in the LE relation. Finally, in the third run, the authors couple the bitext-based FastText aligner with the BABEL A LIGN algorithm, which aligns concepts across languages based on BabelNet (Navigli and Ponzetto, 2012), a massively multilingual lexico-semantic network. Team SHIKEBLCU (Wang et al., 2020). The approach of Wang et al. (2020) extends the wellestablished line of work based on specializing (i.e., fine-tuning) distributional word vectors for lexical relations, be it symmetric semantic similarity (Faruqui et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2018; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018) or the asymmetric LE relation (Vuli´c and Mrkˇsi´c, 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019; Vuli´c et al., 2019b), using constraints from external lexico-semantic resources like WordNet for supervision. At the core of the approach is the Lexical-Entailment Attract Re"
2020.semeval-1.2,C14-1212,0,0.0213805,"ordea et al., 2016; Conneau et al., 2018, inter alia), LE detection, especially its graded variant, has so far been predominantly studied in monolingual settings (Geffet and Dagan, 2005; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 For instance, chess is perceived as a less typical sport than basketball, but it is definitely a more typical sport than sitting. 24 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 24–35 Barcelona, Spain (Online), December 12, 2020. Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016; Shwartz et al., 2017; Glavaˇs and Ponzetto, 2017; Roller et al., 2018, inter alia) with most models and evaluations, unsurprisingly, focusing on English. Existing work on multilingual and cross-lingual LE (Vyas and Carpuat, 2016; Upadhyay et al., 2018; Glavaˇs and Vuli´c, 2019; Kamath et al., 2019) has been rather limited and focused dominantly on major and mutually similar languages and binary LE detection. Shared Task. Aiming to catalyze the development of models for predicting LE, we organized the shared task described in this"
2020.semeval-1.2,N18-1101,0,0.0246649,"Introduction Lexical entailment (LE; hyponymy-hypernymy or is-a relation) is a core asymmetric lexico-semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a crucial building block of lexico-semantic networks and knowledge bases such as WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2012) or ConceptNet (Speer et al., 2017). The ability to reason about concept-level entailment supports a plethora of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Faralli et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), and text generation (Biran and McKeown, 2013) or metaphor detection (Mohler et al., 2013). Binary and Graded Lexical Entailment. For this task, we follow the definition of lexical entailment as thoroughly discussed in Vuli´c et al. (2017, Section 2), namely as a taxonomical asymmetric hyponymy–hypernymy or is-a relation. Although commonly treated as a binary relation (“Is X a type of Y?”), cognitive theories of concept (proto)typicality and category vagueness (Rosch, 1975; Kamp and Partee, 1995) suggest that LE is rather a graded relation: humans can perceive the degree to which the LE relat"
2020.semeval-1.3,C18-1139,0,0.0425101,"imLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differe"
2020.semeval-1.3,2020.semeval-1.37,0,0.0306792,"h Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, especially in the English Subtask 2. However, in order to optimise their system, they made many more submissions than allowed in the competition; we therefore leave them out of the official ranking. With a more multilingual approach, BabelEncoding (Costella Pessutto et al., 2020) proposed a solution in which they translated the contexts and target words to many languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings"
2020.semeval-1.3,2020.lrec-1.720,1,0.566046,"Missing"
2020.semeval-1.3,2020.semeval-1.38,0,0.0192666,"l at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they achieved a second place in the English Subtask 1 and won the Finnish Subtask 1. 8 Conclusion We resented the SemEval-2020 Task on Graded Word Similarity in Context and introduced our new dataset CoSimLex. We provided the motivation behind their design choices and described the annotation process. The task received a good number of submissions and system description papers (15 and"
2020.semeval-1.3,S17-2002,1,0.894409,"me word, and labelled as to whether the word sense in the two examples/contexts is the same or different. This forces engagement with the context; it also creates a task in which context-independent models like word2vec “would perform no better than a random baseline”; and inter-rater agreement scores are much more healthy. However, as the dataset focuses on discrete word senses, it cannot capture graded effects of context. These datasets are also available only in English. Multi-lingual similarity datasets exist: in SemEval2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity, Camacho-Collados et al. (2017) used five different languages, and even used pairs in which each word was presented in a different language. A more recent Multi-SimLex dataset (Vuli´c et al., 2020) comprises similarity ratings for 1,888 concept pairs aligned across 13 typologically diverse languages. However, the pairs in both datasets were annotated out of context, preventing analysis of contextual effects. 38 3 Task Description Our dataset is based on pairs of words from SimLex-999 (Hill et al., 2015). Each instance is a naturallyoccurring context, taken from Wikipedia, in which both words in the pair appear, labelled wit"
2020.semeval-1.3,2020.semeval-1.35,0,0.0334679,"al influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categories, and significantly improving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created"
2020.semeval-1.3,P19-4007,1,0.829844,"e to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts"
2020.semeval-1.3,2020.semeval-1.5,0,0.204677,"Missing"
2020.semeval-1.3,P19-1285,0,0.02456,"ty scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask"
2020.semeval-1.3,N19-1423,0,0.0961329,"w dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in iso"
2020.semeval-1.3,J13-3003,0,0.173635,"luation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; sp"
2020.semeval-1.3,2020.semeval-1.34,0,0.0283968,"roving the non fine-tuned model from a ρ=0.715 and 0.661 per subtask, to a ρ=0.760 and 0.718 respectively. Ferryman’s focus (Chen et al., 2020) was clearly the English Subtask 1, which they won with a modification of BERT in which they fed the TF-IDF score of the words to the model, thus incorporating information about the general importance of words. The system does very well at predicting the change between contexts, but surprisingly poorly at predicting similarity itself, ending last in the English Subtask 2 and second from the last in Croatian and Slovene. The starting point of CitiusNLP (Gamallo, 2020) was the idea that, even if BERT seems to be able to encode syntactic structure, it doesn’t seem to make use of it. They created a linguistically motivated system that relied in dependency to create predictions. However, its performance was considerably worse than BERT’s and their actual submissions are based on a standard BERT model. Finally, the Will_Go team (Bao et al., 2020) looked at different ways to measure similarity between embeddings, mixing euclidean distance with the most common cosine similarity and several others not 44 described in their paper. The combination works well, they a"
2020.semeval-1.3,2020.semeval-1.17,0,0.0328571,"nally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed prediction"
2020.semeval-1.3,2020.semeval-1.16,0,0.0416405,"rom the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores. The approach, even if very successful, seems to rely on having out of context human annotations, perhaps not realistic in the general case. The fact that the system did very poorly in Subtask 1, which asked to predict change, seems to indicate much of the success is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, su"
2020.semeval-1.3,J15-4004,0,0.527307,"used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential"
2020.semeval-1.3,P12-1092,0,0.573882,"eam tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of S"
2020.semeval-1.3,2020.emnlp-main.283,0,0.0166095,"submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The"
2020.semeval-1.3,P19-1569,0,0.0310581,"notator against the average of the rest. JUSTMasters is not part of the official ranking since they were able to optimise their system with more than the competition’s limit of 9 submissions. neither filled the form nor submitted a system description paper do not appear in the official rankings (Tables 2 and 3). We will discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new"
2020.semeval-1.3,2020.semeval-1.33,0,0.0312792,"set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the similarity scores themselves. The biggest weakness of the approach is their reliance on linguistic resources that don’t exist for most languages other than English. Related to these systems, the submission by MineriaUNAM (Gomez-Adorno et al., 2020) won the English Subtask 2. They pr"
2020.semeval-1.3,2020.semeval-1.36,0,0.0172085,"ss is coming from the human annotations. A related strategy could perhaps be used with embeddings or computed predictions instead of human scores. The next group focused on testing a variety of models and parameters. BRUMS (Hettiarachchi and Ranasinghe, 2020) worked with ELMo, BERT, Flair (Akbik et al., 2018), Transformer-XL (Dai et al., 2019) and XLNet (Yang et al., 2019). Their final submission made use of stacked embeddings proposed by Akbik et al. (2018). They won the Finnish Subtask 2, ended second in the two Slovene ones and performed very well in the two English ones. The Hitachi team (Morishita et al., 2020) looked at BERT and XML-R. Their main insight was that for every language, the layers from the center to the end where always the best performing ones, however while BERT performed best in the last layer, XLM-R did in the center one, suggesting their inner structure is organised differently. They won the Slovene Subtask 1, finished second in the two Croatian subtasks and performed competitively in the English ones. To conclude with this group JUSTMasters (Al-Khdour et al., 2020) tested several models, parameters and their own strategy to combine models. They achieved very good performance, esp"
2020.semeval-1.3,Q17-1022,1,0.867881,"Missing"
2020.semeval-1.3,N18-1202,0,0.17532,"system description papers. A new dataset (CoSimLex) was created for evaluation in this task: it contains pairs of words, each annotated within two short text passages. Systems beat the baselines by significant margins, but few did well in more than one language or subtask. Almost every system employed a Transformer model, but with many variations in the details: WordNet sense embeddings, translation of contexts, TF-IDF weightings, and the automatic creation of datasets for fine-tuning were all used to good effect. 1 Introduction Contextualised word embeddings, produced by models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have quickly become the standard in NLP systems. They deliver impressive performance in language modeling and downstream tasks; but there are few resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datas"
2020.semeval-1.3,N19-1128,1,0.870958,"ew resources available which allow intrinsic evaluation in terms of the properties of the embeddings themselves, or their ability to model human perception of meaning, and how these depend on context. For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reflect human similarity judgements. However these datasets treat pairs of words in isolation, and thus cannot tell us much about the effect of context. The few resources that work with context, like SCWS (Huang et al., 2012), WiC (Pilehvar and Camacho-Collados, 2019), and WSim (Erk et al., 2013), focus on word sense and discrete effects, thus missing the more graded effects that context has on words in general, and that approaches like ELMo and BERT would seem well suited to model. Further, USim (Erk et al., 2013) focuses on separate sentential contexts only in the English language. The goal of SemEval-2020 Task 3: Graded Word Similarity in Context, was to move towards filling that gap. We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words"
2020.semeval-1.3,2020.semeval-1.18,0,0.0318793,"languages and then used a weighted combination of monolingual pretrained non contextualised embeddings and BERT embeddings. Their idea is that the translation not only brings new resources but the process itself can produce useful information, for example to disambiguate. The approach works very well for the less resourced languages, being clearly the best system in that category, in both Subtask 1 and 2. Their system won Subtask 1 and 2 for Croatian (by a healthy margin) and 2 for Slovene, ending third in the Slovene Subtask 1 and third and second in the two Finnish ones. The MultiSem team (Soler and Apidianaki, 2020) collected 5 different datasets in order to fine-tune their BERT models, most of them automatically generated from previous datasets to increase contextual influence. As an example, ukWaC-subs was created by substituting target words by either: a correct substitute; a word that could be the right substitute in other circumstances but it is not in this context; or a random word. The datasets included WiC, which when used to fine tune the model resulted in the best performance for Subtask1, giving them a third place. The approach works very well, giving a very consistent performance in all categ"
2020.semeval-1.3,2020.semeval-1.19,0,0.0366991,"l discuss here the results of the remaining 11 systems. First, we describe a group of systems designed around sense embeddings created using WordNet (Miller, 1995) as a guide. The most successful was the submission by LMMS. They employed a similar strategy to the one set out in (Loureiro and Jorge, 2019), creating pretrained embeddings for each sense in WordNet, this time using XLM-R (Conneau et al., 2019) and SemCor augmented with their own UWA dataset (Loureiro and Camacho-Collados, 2020). This approach achieved second place in the English Subtask 1 and fourth in the English Subtask 2. UZH (Tang, 2020) submitted (after the competition had ended) a system based on the original BERT sense embeddings created for (Loureiro and Jorge, 2019) but improved their performance by combining them with contextualised embeddings. Finally for this group AlexU-AUX-BERT (Mahmoud and Torki, 2020) created new sense embeddings for the competition 43 target words. In order to do so they sourced additional contexts for the top WordNet synsets. Their system scored third in the English Subtask 2. The pretrained WordNet sense embedding proved highly successful in this task, especially in Subtask 2, predicting the si"
2020.semeval-1.3,2020.lrec-1.582,1,0.835923,"Missing"
2020.semeval-1.3,W17-0228,0,0.0235541,"ilarity of words and the effect that context has on it. Good context-independent models could theoretically give reasonably competitive results in this task, however we still expect context-dependent models to have a considerable advantage. 4 Dataset CoSimLex (Armendariz et al., 2020) is based on pairs of words from SimLex-999 (Hill et al., 2015); the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case. For Croatian and Finnish we use existing translations of SimLex-999 (Mrkši´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019). In the case of Slovene, we have produced our own new translation,1 following Mrkši´c et al. (2017)’s methodology for Croatian. The dataset consists of 340 pairs in English, 112 in Croatian, 111 in Slovene and 24 in Finnish. Each pair is rated within two different contexts, giving a total of 1174 scores of contextual similarity. This poses a difficult task: to find suitable, organically occurring contexts; this task is even more challenging for languages with less resources, and as a result the selection of pairs is different for each language. Each line of CoSimLex is made of"
2020.semeval-1.3,2020.cl-4.5,1,0.887547,"Missing"
2021.acl-long.151,W19-3805,0,0.0517219,"Missing"
2021.acl-long.151,2020.acl-tutorials.2,0,0.0608011,"Missing"
2021.acl-long.151,2020.acl-main.485,0,0.0620893,"Missing"
2021.acl-long.151,N19-3002,0,0.0282129,"presentations of t1 and t2 from the output LM layer (i.e., output embeddings of t1 and t2),12 and cos denotes the cosine similarity. ADD forces the output representations of target terms from the dominant group (e.g., christian) to be equally distant to the representation of a stereotypical attribute for the minoritized group (e.g., dangerous) as the representations of corresponding target terms denoting the minoritized group (e.g., muslim). Similar to LMD, for all occurrences of a ∈ A1 , the final loss is the weighted sum of LLM and LADD , see Eq. (2). 4.3 Hard Debiasing Loss (HD) Similar to Bordia and Bowman (2019), we next devise a loss based on the idea of hard debiasing from Bolukbasi et al. (2016). We compute this loss in two steps: (1) identification of the bias subspace, and (2) neutralization of the attribute words w.r.t. to the previously identified bias subspace. (1) Bias Subspace Identification. We start from the same set of manually curated target term pairs P as in LMD and ADD. Let t be the output vector of some term t from the LM head. We then obtain partial bias vectors bi for pairs (t1i , t2i ) ∈ P by computing the differences between t1i and t2i : bi = (t1i − t2i )/2. We then stack the p"
2021.acl-long.151,W19-3655,0,0.0712183,"Missing"
2021.acl-long.151,2020.acl-main.488,0,0.0834776,"Missing"
2021.acl-long.151,2020.coling-main.446,0,0.0863191,"Missing"
2021.acl-long.151,N19-1063,0,0.0270244,"ammer as woman is to homemaker”, Bolukbasi et al. (2016) first drew attention 15 Two exceptions, which requires further investigation are DST performance drops of LMD when debiasing for Race and of ADD when debiasing for Gender. to the issue. Caliskan et al. (2017) presented the Word Embedding Association Test (WEAT), quantifying the bias between two sets of target terms towards two sets of attribute terms. Subsequent work proposed extensions to further embedding models (Liang et al., 2020a,b) and languages (e.g., McCurdy and Serbetci, 2020; Lauscher and Glavaˇs, 2019; Lauscher et al., 2020b; May et al., 2019), analyses of the proposed measures (e.g., Gonen and Goldberg, 2019; Ethayarajh et al., 2019), more comprehensive evaluation frameworks (Lauscher et al., 2020a), new debiasing approaches (Dev and Phillips, 2019; Karve et al., 2019) and task-specific bias measures and resources for tasks like coreference resolution (Zhao et al., 2018), machine translation (Stanovsky et al., 2019) and natural language inference (Dev et al., 2020). In our work, we similarly acknowledge the importance of understanding bias w.r.t. downstream tasks, but focus on dialog systems, for which the landscape of research ef"
2021.acl-long.151,P17-1163,0,0.0746699,"Missing"
2021.acl-long.151,P19-1164,0,0.022803,"owards two sets of attribute terms. Subsequent work proposed extensions to further embedding models (Liang et al., 2020a,b) and languages (e.g., McCurdy and Serbetci, 2020; Lauscher and Glavaˇs, 2019; Lauscher et al., 2020b; May et al., 2019), analyses of the proposed measures (e.g., Gonen and Goldberg, 2019; Ethayarajh et al., 2019), more comprehensive evaluation frameworks (Lauscher et al., 2020a), new debiasing approaches (Dev and Phillips, 2019; Karve et al., 2019) and task-specific bias measures and resources for tasks like coreference resolution (Zhao et al., 2018), machine translation (Stanovsky et al., 2019) and natural language inference (Dev et al., 2020). In our work, we similarly acknowledge the importance of understanding bias w.r.t. downstream tasks, but focus on dialog systems, for which the landscape of research efforts is surprisingly scarce. (2) Bias in Language Generation. Dialog systems crucially depend on natural language generation (NLG) models. Yeo and Chen (2020) experimented with gender bias in word embeddings for NLG. Sheng et al. (2019) introduce the notion of a regard for a demographic, and compile a data set and devise a bias classification model based on that notion. Webster"
2021.acl-long.151,P19-1159,0,0.0152656,"(see Tables 4 and 5).15 Interestingly, while LMD drastically increases the perplexity on Reddit utterances (Figure 1b; see LMP in §3) this does not have negative consequences on DST and CRG. To summarize, from the benchmarked debiasing methods, HD and CDA are able to significantly reduce the bias and preserve conversational capabilities; Our results suggest that the dialog performance would remain unaffected even if HD and CDA are to be applied more than once, in order to mitigate multiple bias types. 6 Related Work For a comprehensive overview of work on bias in NLP, we refer the reader to (Sun et al., 2019; Blodgett et al., 2020; Shah et al., 2020). Here, we provide (1) a brief overview of bias measures and mitigation methods and their usage in (2) language generation and, specifically, in (3) dialog. (1) Bias in NLP. Resources, measures, and mitigation methods largely target static word embedding models: with their famous analogy “man is to computer programmer as woman is to homemaker”, Bolukbasi et al. (2016) first drew attention 15 Two exceptions, which requires further investigation are DST performance drops of LMD when debiasing for Race and of ADD when debiasing for Gender. to the issue."
2021.acl-long.243,2020.emnlp-main.367,0,0.649627,"e model capacity (Artetxe et al., 2020; Pfeiffer et al., 2020b; Chau et al., 2020) or through additional training for particular language pairs (Pfeiffer et al., 2020b; Ponti et al., 2020). Another observation concerns substantially reduced crosslingual and monolingual abilities of the models for resource-poor languages with smaller pretraining data (Wu and Dredze, 2020; Hu et al., 2020; Lauscher et al., 2020). Those languages remain underrepresented in the subword vocabulary and the model’s shared representation space despite oversampling. Despite recent efforts to mitigate this issue (e.g., Chung et al. (2020) propose to cluster and merge the vocabularies of similar languages, before defining a joint vocabulary across all languages), the multilingual LMs still struggle with balancing their parameters across many languages. Monolingual versus Multilingual LMs. New monolingual language-specific models also emerged for many languages, following BERT’s architecture and pretraining procedure. There are monolingual BERT variants for Arabic (Antoun et al., 2020), French (Martin et al., 2020), Finnish (Virtanen et al., 2019), Dutch (de Vries et al., 2019), to name only a few. Pyysalo et al. (2020) released"
2021.acl-long.243,2020.tacl-1.30,0,0.117373,"ng) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thus also satisfying C3. We achieve task diversity and generalizability by selecting a combination of tasks driven by lowe"
2021.acl-long.243,2020.acl-main.747,0,0.0639935,"Missing"
2021.acl-long.243,D18-1269,0,0.0249882,"9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBERT (Antoun et al., 2020) BERT (D"
2021.acl-long.243,D18-1029,1,0.883569,"Missing"
2021.acl-long.243,2021.eacl-main.270,1,0.685166,"Missing"
2021.acl-long.243,2020.acl-main.560,0,0.0242538,"ir comparisons. 3.1 Language and Task Selection Our selection of languages has been guided by several (sometimes competing) criteria: C1) typological diversity; C2) availability of pretrained monolingual BERT models; C3) representation of the languages in standard evaluation benchmarks for a sufficient number of tasks. Regarding C1, most high-resource languages belong to the same language families, thus sharing a majority of their linguistic features. Neglecting typological diversity inevitably leads to poor generalizability and language-specific biases (Gerz et al., 2018; Ponti et al., 2019; Joshi et al., 2020). Following recent work in multilingual NLP that pays particular attention to typological diversity (Clark et al., 2020; Hu et al., 2020; Ponti et al., 2020, inter alia), we experiment with a language sample covering a broad spectrum of language properties. Regarding C2, for computational tractability, we only select languages with readily available BERT models. Unlike prior work, which typically lacks either language (R¨onnqvist et al., 2019; Zhang et al., 2020) or task diversity (Wu and Dredze, 2020; Vuli´c et al., 2020), we ensure that our experimental framework takes both into account, thu"
2021.acl-long.243,2020.emnlp-main.363,1,0.857757,"Missing"
2021.acl-long.243,2020.acl-main.653,0,0.0385977,". Finally, we select a set of 9 languages from 8 language families, as listed in Table 1.3 We evaluate mBERT and monolingual BERT models on five downstream NLP tasks: named entity recognition (NER), sentiment analysis (SA), question answering (QA), universal dependency parsing (UDP), and part-of-speech tagging (POS).4 3 Note that, since we evaluate monolingual performance and not cross-lingual transfer performance, we require training data in the target language. Therefore, we are unable to leverage many of the available multilingual evaluation data such as XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), or XNLI (Conneau et al., 2018). These evaluation sets do not provide any training portions for languages other than English. Additional information regarding our selection of pretrained models is available in Appendix A.1. 4 Information on which datasets are associated with which language and the dataset sizes (examples per split) are provided in Appendix A.4. 3120 Language ISO Language Family Pretrained BERT Model Arabic English Finnish Indonesian Japanese Korean Russian Turkish Chinese AR Afroasiatic Indo-European Uralic Austronesian Japonic Koreanic Indo-European Turkic Sino-Tibetan AraBE"
2021.acl-long.243,N19-1392,0,0.0619148,"verified the scores, nor have they performed a controlled impartial comparison. Vuli´c et al. (2020) probed mBERT and monolingual BERT models across six typologically diverse languages for lexical semantics. They show that pretrained monolingual BERT models encode significantly more lexical information than mBERT. Zhang et al. (2020) investigated the role of pretraining data size with RoBERTa, finding that the model learns most syntactic and semantic features on corpora spanning 10M–100M word tokens, but still requires massive datasets to learn higher-level semantic and commonsense knowledge. Mulcaire et al. (2019) compared monolingual and bilingual ELMo (Peters et al., 2018) LMs across three downstream tasks, finding that contextualized representations from the bilingual models can improve monolingual task performance relative to their monolingual counterparts.2 However, it is unclear how their findings extend to massively multilingual LMs potentially suffering from the curse of multilinguality. R¨onnqvist et al. (2019) compared mBERT to monolingual BERT models for six languages (German, English, Swedish, Danish, Norwegian, Finnish) on three different tasks. They find that mBERT lags behind its monolin"
2021.acl-long.410,Q17-1010,0,0.0713222,"nt mapping-based approach (Mikolov et al., 2013a; Smith et al., 2017) with the V EC M AP framework (Artetxe et al., 2018). We run main BLI evaluations for 10 language pairs spanning EN, DE, RU, FI, TR.7 Word Vocabularies and Baselines. We extract decontextualized type-level WEs in each language both from the original BERTs (termed BERT- REG)4 and the L EX F IT-ed BERT models for exactly the same vocabulary. Following Vuli´c et al. (2020), the vocabularies cover the top 100K most frequent words represented in the respective fastText (FT) vectors, trained on lowercased monolingual Wikipedias by Bojanowski et al. (2017).5 The equivalent vocabulary coverage allows for a direct comparison of all WEs regardless of the induction/extraction method; this also includes the FT Task 3: Lexical Relation Prediction (RELP). We assess the usefulness of lexical knowledge in WEs to learn relation classifiers for standard lexical relations (i.e., synonymy, antonymy, hypernymy, meronymy, plus no relation) via a state-ofthe-art neural model for RELP which learns solely based on input type-level WEs (Glavaš and Vuli´c, 2018). We use the WordNet-based evaluation data of Glavaš and Vuli´c (2018) for EN, DE, ES; they contain 10K"
2021.acl-long.410,2020.acl-main.431,0,0.547689,"ll pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correc"
2021.acl-long.410,P13-1133,0,0.0173955,"LI fall under similarity-based evaluation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display larg"
2021.acl-long.410,D18-2029,0,0.136669,"Missing"
2021.acl-long.410,2020.conll-1.17,0,0.026315,"g decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and"
2021.acl-long.410,2020.emnlp-main.367,0,0.0318772,"0.24 to 0.60 for FI ; BLI scores for EN - FI rise from 0.21 to 0.37), it cannot match the absolute performance peaks of L EX F IT-ed monolingual BERTs. Storing the knowledge of 100+ languages in its limited parameter budget, mBERT still cannot capture monolingual knowledge as accurately as language-specific BERTs (Conneau et al., 2020). However, we believe that its performance with L EX F IT may be further improved by leveraging recently proposed multilingual LM adaptation strategies that mitigate a mismatch between shared multilingual and language-specific vocabularies (Artetxe et al., 2020; Chung et al., 2020; Pfeiffer et al., 2020); we leave this for future work. Layerwise Averaging. A consensus in prior work (Tenney et al., 2019; Ethayarajh, 2019; Vuli´c et al., 2020) points that out-of-context lexical knowledge in pretrained LMs is typically stored in bottom Transformer layers (see Table 5). However, Table 5 also reveals that this does not hold after L EX F ITing: the tuned model requires knowledge from all layers to extract effective decontextualized WEs and reach peak task scores. Effectively, this means 11 When sampling all reduced sets, we again deliberately excluded all words occurring in"
2021.acl-long.410,2020.acl-main.747,0,0.116681,"Missing"
2021.acl-long.410,N19-1423,0,0.259071,"cal tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than aut"
2021.acl-long.410,ehrmann-etal-2014-representing,0,0.112194,". sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in tasks such as RELP and LexSIMP, again hi"
2021.acl-long.410,D19-1006,0,0.449664,"the era of large neural models. 1 Word embedding extraction BERT LexFit loss u 1. SOFTMAX 2. MNEG 3. MSIM w v Pooling Pooling BERT BERT (w, v) = (dormant, asleep) Step 1: Lexical ﬁne-tuning Figure 1: Illustration of the full pipeline for obtaining decontextualized word representations, based on lexically fine-tuning pretrained LMs via dual-encoder networks (Step 1, §2.1), and then extracting the representations from their (fine-tuned) layers (Step 2, §2.2). Introduction Probing large pretrained encoders like BERT (Devlin et al., 2019) revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´"
2021.acl-long.410,N15-1184,0,0.0368031,"or static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Ganitkevitch et al., 2013) into WEs. Thus, it accentuates relationships of pure semantic similarity in the re5269 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge,"
2021.acl-long.410,N13-1092,0,0.115153,"Missing"
2021.acl-long.410,D16-1235,1,0.882037,"Missing"
2021.acl-long.410,2020.acl-main.247,0,0.0241888,"FTMAX SOFTMAX Table 4: LexSIMP results (Accuracy ×100). BERTs. However, there are differences across their task performance: the ranking-based MNEG and MSIM variants display stronger performance on similarity-based ranking lexical tasks such as LSIM and BLI. The classification-based SOFTMAX objective is, as expected, better aligned with the RELP task, and we note slight gains with its ternary variant which leverages extra antonymy knowledge. This finding is well aligned with the recent findings demonstrating that task-specific pretraining results in stronger (sentence-level) task performance (Glass et al., 2020; Henderson et al., 2020; Lewis et al., 2020). In our case, we show that task-specific lexical fine-tuning can reshape the underlying LM’s parameters to not only act as a universal word encoder, but also towards a particular lexical task. The per-epoch time measurements from Table 1 validate the efficiency of L EX F IT as a post-training fine-tuning procedure. Previous approaches that attempted to inject lexical information (i.e., word senses and relations) into large LMs (Lauscher et al., 2020; Levine et al., 2020) relied on joint LM (re)training from scratch: it is effectively costlier than"
2021.acl-long.410,P15-2011,1,0.825395,"age.8 Task 4: Lexical Simplification (LexSIMP) aims to automatically replace complex words (i.e., specialized terms, less-frequent words) with their simpler in-context synonyms, while retaining grammaticality and conveying the same meaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types o"
2021.acl-long.410,N18-2029,1,0.905965,"Missing"
2021.acl-long.410,P18-1004,1,0.888961,"Missing"
2021.acl-long.410,P19-1070,1,0.911229,"Missing"
2021.acl-long.410,2020.findings-emnlp.196,1,0.890486,"Missing"
2021.acl-long.410,J15-4004,1,0.91951,"a wealth of lexical knowledge (Ethayarajh, 2019; Vuli´c et al., 2020). If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli´c et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020). However, both static and contextualized WEs ultimately learn solely from the distributional word co-occurrence signal. This source of signal is known to lead to distortions in the induced representations by conflating meaning based on topical relatedness rather than authentic semantic similarity (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017). This also creates a ripple effect on downstream applications, where model performance may suffer (Faruqui, 2016; Mrkši´c et al., 2017; Lauscher et al., 2020). Our work takes inspiration from the methods to correct these distortions and complement the distributional signal with structured information, which were originally devised for static WEs. In particular, the process known as semantic specialization (or retrofitting) injects information about lexical relations from databases like WordNet (Beckwith et al., 1991) or the Paraphrase Database (Gan"
2021.acl-long.410,P14-2075,0,0.0258555,"eaning as the more complex input text (Paetzold and Specia, 2017). Therefore, discerning between semantic similarity (e.g., synonymy injected via L EX F IT) and broader relatedness is critical for LexSIMP (Glavaš and Vuli´c, 2018). We adopt the standard LexSIMP evaluation protocol used in prior research on static WEs (Ponti et al., 2018, 2019). 1) We use Light-LS (Glavaš and Štajner, 2015), a languageagnostic LexSIMP tool that makes simplifications in an unsupervised way based solely on word similarity in an input (static) WE space; 2) we rely on standard LexSIMP benchmarks, available for EN (Horn et al., 2014), IT (Tonelli et al., 2016), and ES (Saggion, 2017); and 3) we report the standard Accuracy scores (Horn et al., 2014).9 Important Disclaimer. We note that the main purpose of the chosen evaluation tasks and experimental protocols is not necessarily achieving state-ofthe-art performance, but rather probing the vectors in different lexical tasks requiring different types of lexical knowledge,10 and offering fair and insightful comparisons between different L EX F IT variants, as well as against standard static WEs (fastText) and non-tuned BERT-based static WEs. 4 Results and Discussion The main"
2021.acl-long.410,2021.eacl-main.10,0,0.0341408,"ss diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2), we will also probe al"
2021.acl-long.410,kamholz-etal-2014-panlex,0,0.011838,"valuation tasks (Ruder et al., 2019). sults demonstrate that the inexpensive lexical finetuning procedure can indeed turn large pretrained LMs into effective decontextualized word encoders, and this can be achieved for a reasonably wide spectrum of languages for which such pretrained LMs exist. What is more, L EX F IT for all nonEN languages has been run with noisy automatically translated lexical constraints, which holds promise to support even stronger static L EX F ITbased WEs with human-curated data in the future, e.g., extracted from multilingual WordNets (Bond and Foster, 2013), PanLex (Kamholz et al., 2014), or BabelNet (Ehrmann et al., 2014). The results give rise to additional general implications. First, they suggest that the pretrained LMs store even more lexical knowledge than thought previously (Ethayarajh, 2019; Bommasani et al., 2020; Vuli´c et al., 2020); the role of L EX F IT finetuning is simply to ‘rewire’ and expose that knowledge from the LM through (limited) lexical-level supervision. To further investigate the ‘rewiring’ hypothesis, in §4.1, we also run L EX F IT with a drastically reduced amount of external knowledge. BERT- REG vectors display large gains over FT vectors in task"
2021.acl-long.410,2020.coling-main.118,1,0.90184,"Missing"
2021.acl-long.410,2021.emnlp-main.109,1,0.821601,"Missing"
2021.acl-long.410,K19-1004,1,0.90017,"Missing"
2021.acl-long.410,P17-1163,0,0.0702013,"Missing"
2021.acl-long.410,Q17-1022,1,0.929258,"Missing"
2021.acl-long.410,N15-1100,0,0.0289612,"diversity of the selection. The final test languages are English (EN), German (DE), Spanish (ES), Finnish (FI), Italian (IT), Polish (PL), Russian (RU), and Turkish (TR). For comparability across languages, we use monolingual uncased BERT Base models for all languages (N = 12 Transformer layers, 12 attention heads, hidden layer dimensionality is 768), available (see the appendix) via the HuggingFace repository (Wolf et al., 2020). External Lexical Knowledge. We use the standard collection of EN lexical constraints from previous work on (static) word vector specialization (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018; Ponti et al., 2018, 2019). It covers the lexical relations from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009); it comprises 1,023,082 synonymy (Psyn ) word pairs and 380,873 antonymy pairs (Pant ). For all other languages, we rely on non-curated noisy lexical constraints, obtained via an automatic word translation method by Ponti et al. (2019); see the original work for the details of the translation procedure. L EX F IT: Technical Details. The implementation is based on the SBERT framework (Reimers and Gurevych, 2019), using the suggested settings: AdamW"
2021.acl-long.410,D18-1026,1,0.887916,"Missing"
2021.acl-long.410,D19-1226,1,0.877527,"Missing"
2021.acl-long.410,D19-1410,0,0.221705,"rence on Natural Language Processing, pages 5269–5283 August 1–6, 2021. ©2021 Association for Computational Linguistics fined representations (Faruqui et al., 2015; Mrkši´c et al., 2017; Ponti et al., 2019, inter alia). Our goal is to create representations that take advantage of both 1) the expressivity and lexical knowledge already stored in pretrained language models (LMs) and 2) the precision of lexical finetuning. To this effect, we develop L EX F IT, a versatile lexical fine-tuning framework, illustrated in Figure 1, drawing a parallel with universal sentence encoders like SentenceBERT (Reimers and Gurevych, 2019).1 Our working hypothesis, extensively evaluated in this paper, is as follows: pretrained encoders store a wealth of lexical knowledge, but it is not straightforward to extract that knowledge. We can expose this knowledge by rewiring their parameters through lexical fine-tuning, and turn the LMs into universal (decontextualized) word encoders. Compared to prior attempts at injecting lexical knowledge into large LMs (Lauscher et al., 2020), our L EX F IT method is innovative as it is deployed post-hoc on top of already pretrained LMs, rather than requiring joint multi-task training. Moreover, L"
2021.acl-long.410,2020.tacl-1.54,0,0.031043,"demonstrate the usefulness of L EX F IT: we report large gains over WEs extracted from vanilla LMs and over traditional WE models across 8 languages and 4 lexical tasks, even with very limited and noisy external lexical knowledge, validating the rewiring hypothesis. The code is available at: https://github.com/cambridgeltl/lexfit. 2 From Language Models to (Decontextualized) Word Encoders The motivation for this work largely stems from the recent work on probing and analyzing pretrained language models for various types of knowledge they might implicitly store (e.g., syntax, world knowledge) (Rogers et al., 2020). Here, we focus on their lexical semantic knowledge (Vuli´c et al., 2020; Liu et al., 2021), with an aim of extracting high-quality static word embeddings from the parameters of the input LMs. In what follows, we describe lexical fine-tuning via dual-encoder networks (§2.1), followed by the WE extraction pro1 These approaches are connected as they are both trained via contrastive learning on dual-encoder architectures, but they provide representations for a different granularity of meaning. cess from the fine-tuned layers of pretrained LMs (§2.2), see Figure 1. 2.1 L EX F IT: Methodology Our"
2021.acl-long.410,2021.acl-long.243,1,0.826374,"Missing"
2021.acl-long.410,2020.semeval-1.1,0,0.0402676,"trum of lexical tasks across diverse languages in controlled evaluations, thus directly questioning the practical usefulness of the traditional WE models in modern NLP. Besides inducing better static WEs for lexical tasks, following the line of lexical probing work (Ethayarajh, 2019; Vuli´c et al., 2020), our goal in this work was to understand how (and how much) lexical semantic knowledge is coded in pretrained LMs, and how to ‘unlock’ the knowledge from the LMs. We hope that our work will be beneficial for all lexical tasks where static WEs from traditional WE models are still largely used (Schlechtweg et al., 2020; Kaiser et al., 2021). Despite the extensive experiments, we only scratched the surface, and can indicate a spectrum of future enhancements to the proof-of-concept L EX F IT framework beyond the scope of this work. We will test other dual-encoder loss functions, including finer-grained relation classification tasks (e.g., in the SOFTMAX variant), and hard (instead of random) negative examples (Wieting et al., 2015; Mrkši´c et al., 2017; Lauscher et al., 2020; Kalantidis et al., 2020). While in this work, for simplicity and efficiency, we focused on fully decontextualized ISO setup (see §2.2),"
2021.acl-long.410,K15-1026,0,0.0675206,"Missing"
2021.acl-long.410,P18-1072,1,0.903042,"Missing"
2021.acl-long.410,2020.cl-4.5,1,0.903622,"Missing"
2021.acl-long.410,N18-1048,1,0.933808,"Missing"
2021.acl-long.447,D19-1607,0,0.0452479,"Missing"
2021.acl-long.447,2020.emnlp-main.40,0,0.467333,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,2020.emnlp-main.369,0,0.514216,"widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially improve model performance of the target language with negligible annotation costs (Garrette and Baldridge, 2013; Hedderich et al., 2020). In this work, however, we demonst"
2021.acl-long.447,P19-1493,0,0.364814,"s, we make our sampled few shots publicly available.1 1 Introduction Multilingual pretrained encoders like multilingual BERT (mBERT; Devlin et al. (2019)) and XLMR (Conneau et al., 2020) are the top performers in crosslingual tasks such as natural language inference (Conneau et al., 2018), document classification (Schwenk and Li, 2018; Artetxe and Schwenk, 2019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English f"
2021.acl-long.447,2020.acl-main.467,0,0.054412,"Missing"
2021.acl-long.447,P19-1459,0,0.0154384,"he clock didn’t even work one minute ... Visually, however, very nice.”) Pretrained multilingual encoders are shown to learn and store “language-agnostic” features (Pires et al., 2019; Zhao et al., 2020); §5.3 shows that source-training mBERT on EN substantially benefits other languages, even for difficult semantic tasks like PAWSX. Conditioning on such languageagnostic features, we expect that the buckets should lead to good understanding and reasoning capabilities for a target language. However, plain few-shot finetuning still relies heavily on unintended shallow lexical cues and shortcuts (Niven and Kao, 2019; Geirhos et al., 2020) that generalize poorly. Other open research questions for future work arise: How do we overcome this excessive reliance on lexical features? How can we leverage language-agnostic features with few shots? Our standardized buckets, baseline results, and analyses are the initial step towards researching and answering these questions. 5.5 Target-Adapting Methods SotA few-shot learning methods (Chen et al., 2019; Wang et al., 2020; Tian et al., 2020; Dhillon et al., 2020) from computer vision consist of two stages: 1) training on base-class images, and 2) few-shot finetuning"
2021.acl-long.447,P19-1015,0,0.0280507,"in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for each target (i.e., non-English) language of a task to get a reliable estimation of model performance. CLS Tasks. For MLDoc and MARC, e"
2021.acl-long.447,N18-1101,0,0.0283863,"l selection in this stage. 4 Experimental Setup We consider three types of tasks requiring varying degrees of semantic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to a"
2021.acl-long.447,D19-1077,0,0.12812,"019), and argument mining (ToledoRonen et al., 2020). They enable transfer learning through language-agnostic representations in crosslingual setups (Hu et al., 2020). A widely explored transfer scenario is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019), * Equal contribution. Code and resources are available at https://github. com/fsxlt 1 where a pretrained encoder is finetuned on abundant task data in the source language (e.g., English) and then directly evaluated on target-language test data, achieving surprisingly good performance (Wu and Dredze, 2019; Hu et al., 2020). However, there is evidence that zero-shot performance reported in the literature has large variance and is often not reproducible (Keung et al., 2020a; Rios et al., 2020); the results in languages distant from English fall far short of those similar to English (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) stress the importance of few-shot crosslingual transfer instead, where the encoder is first finetuned on a source language and then further finetuned with a small amount (10–100) of examples (few shots) of the target language. The few shots substantially im"
2021.acl-long.447,2020.repl4nlp-1.16,0,0.269393,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.emnlp-main.362,0,0.385788,"dapting 40 times using different 1-shot buckets in German (DE) and Spanish (ES). Second, for a fixed 1-shot bucket, we repeat the same experiment 40 times using random seeds in {0 . . . 39}. Figure 1 presents the dev set performance distribution of the 40 runs with 40 random seeds (top) and 40 1-shot buckets (bottom). With exactly the same training data, using different random seeds yields a 1–2 accuracy difference of FS-XLT (Figure 1 top). A similar phenomenon has been observed in finetuning monolingual encoders (Dodge et al., 2020) and multilingual encoders with ZS-XLT (Keung et al., 2020a; Wu and Dredze, 2020b; Xia et al., 2020); we show this observation also holds for FS-XLT. The key takeaway is that varying the buckets is a more severe problem. It causes much larger variance (Figure 1 bottom): The maximum accuracy difference is ≈6 for DE MARC and ≈10 for ES MLDoc. This can be due to the fact that difficulty of individual examples varies in a dataset (Swayamdipta et al., 2020), resulting in different amounts of information encoded in buckets. This large variance could be an issue when comparing different few-shot learning algorithms. The bucket choice is a strong confounding factor that may obscu"
2021.acl-long.447,2020.findings-emnlp.29,0,0.094877,"Missing"
2021.acl-long.447,2020.emnlp-main.608,0,0.0774621,"Missing"
2021.acl-long.447,D19-1382,0,0.0227255,"tic and syntactic knowledge transfer: Sequence classification (CLS), namedentity recognition (NER), and part-of-speech tagging (POS) in up to 40 typologically diverse languages (cf., Appendix §B). 4.1 Datasets and Selection of Few Shots For the CLS tasks, we sample few shots from four multilingual datasets: News article classification (MLDoc; Schwenk and Li (2018)); Amazon review classification (MARC; Keung et al. (2020b)); natural language inference (XNLI; Conneau et al. (2018); Williams et al. (2018)); and crosslingual paraphrase adversaries from word scrambling (PAWSX; Zhang et al. (2019); Yang et al. (2019)). We use treebanks in Universal Dependencies (Nivre et al., 2020) for POS, and WikiANN dataset (Pan et al., 2017; Rahimi et al., 2019) for NER. Table 1 reports key information about the datasets. We adopt the conventional few-shot sampling strategy (Fei-Fei et al., 2006; Koch et al., 2015; Snell et al., 2017), and conduct “N -way K-shot” sampling from the datasets; N is the number of classes and K refers to the number of shots per class. A group of N -way K-shot data is referred to as a bucket. We set N equal to the number of labels |T |. Following Wang et al. (2020), we sample 40 buckets for"
2021.acl-long.447,2020.emnlp-main.660,0,0.0765103,"Missing"
2021.acl-long.447,N18-1109,0,0.0145892,"e task instead of between different tasks. Few-shot learning was first explored in computer vision (Miller et al., 2000; Fei-Fei et al., 2006; Koch et al., 2015); the aim there is to learn new concepts with only few images. Methods like prototypical networks (Snell et al., 2017) and modelagnostic meta-learning (MAML; Finn et al. (2017)) have also been applied to many monolingual (typically English) NLP tasks such as relation classification (Han et al., 2018; Gao et al., 2019), namedentity recognition (Hou et al., 2020a), word sense disambiguation (Holla et al., 2020), and text classification (Yu et al., 2018; Yin, 2020; Yin et al., 2020; Bansal et al., 2020; Gupta et al., 2020). However, recent few-shot learning methods in computer vision consisting of two simple finetuning stages, first on base-class images and then on new-class few shots, have been shown to outperform MAML and achieve SotA scores (Wang et al., 2020; Chen et al., 2020; Tian et al., 2020; Dhillon et al., 2020). Inspired by this work, we compare various fewshot finetuning methods from computer vision in the context of FS-XLT. Task Performance Variance. Deep neural networks’ performance on NLP tasks is bound to exhibit large varian"
2021.acl-long.447,N19-1131,0,0.034028,"Missing"
2021.acl-long.541,P98-1013,0,0.268885,"semanticsyntactic properties, provide a mapping between the verbs’ senses and the morpho-syntactic realisation of their arguments (Jackendoff, 1992; Levin, 1993). The potential of verb classifications lies in their predictive power: for any given verb, a set of rich semantic-syntactic properties can be inferred based on its class membership. In this work, we explicitly harness this rich linguistic knowledge to aid pretrained LMs in capturing regularities in the properties of verbs and their arguments. We select two major English lexical databases – VerbNet (Kipper Schuler, 2005) and FrameNet (Baker et al., 1998) – as sources of verb knowledge at the semantic-syntactic interface, each representing a different lexical framework. 6953 VerbNet (VN) (Kipper Schuler, 2005; Kipper et al., 2006), the largest available verb-focused lexicon, organises verbs into classes based on the overlap in their semantic properties and syntactic behaviour; it builds on the premise that a verb’s predicateargument structure informs its meaning (Levin, 1993). Each entry provides a set of thematic roles and selectional preferences for the verbs’ arguments; it also lists the syntactic contexts characteristic for the class membe"
2021.acl-long.541,2020.acl-main.463,0,0.0134269,"ased encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syn"
2021.acl-long.541,W11-4606,0,0.0169673,", It freed him of guilt), there exists a subset of verbs participating in a syntactic frame NP V NP S_ING (‘free-80-1’), within which there exists an even more constrained subset of verbs appearing with prepositional phrases headed specifically by the preposition from (e.g., The scientist purified the water from bacteria). 3 For instance, descriptions of transactions will include the same frame elements Buyer, Seller, Goods, Money in most languages. Indeed, English FN has inspired similar projects in other languages: e.g., Spanish (Subirats and Sato, 2004), Japanese (Ohara, 2012), and Danish (Bick, 2011). Adapter Architecture. Instead of directly finetuning all parameters of the pretrained Transformer, we opt for storing verb knowledge in a separate set of adapter parameters, keeping the verb knowledge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general languag"
2021.acl-long.541,Q17-1010,0,0.0124165,"data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the need for external event-related knowledge and transfer learning approaches, such as the ones introduced in this work. Semantic Specialisation. Representation spaces induced through self-supervised objectives from large corpora, be it the word embedding spaces (Mikolov et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al.,"
2021.acl-long.541,P17-1038,0,0.0238949,"raph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only em"
2021.acl-long.541,P15-1017,0,0.0313724,"ns capable of making fine-grained predictions in the face of data scarcity. Traditional event extraction methods relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through"
2021.acl-long.541,2020.acl-main.747,0,0.041835,"do not exist for a vast majority of languages. Given the inherent cross-lingual nature of verb classes and semantic frames (see English (EN) Spanish (ES) Chinese (ZH) Arabic (AR) VerbNet FrameNet 181,882 96,300 60,365 70,278 57,335 36,623 21,815 24,551 Table 1: Number of positive verb pairs in English, and in each target language obtained via VTRANS (§2.4). §2.1), we investigate the potential for verb knowledge transfer from English to target languages, without any manual target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three"
2021.acl-long.541,D19-2007,1,0.874739,"Missing"
2021.acl-long.541,P18-1048,0,0.0213021,"atures (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferg"
2021.acl-long.541,C16-1114,0,0.0203051,"verbs) may trigger the same type of event, and conversely, the same word (verb) can evoke differ5 We provide more details about the frameworks and their corresponding annotation schemes in Appendix A. 6 E.g., in the sentence: “The rules can also affect small businesses, which sometimes pay premiums tied to employees’ health status and claims history.”, affect and pay are event triggers of type STATE and OCCURRENCE, respectively. 7 The ACE annotations distinguish 34 trigger types (e.g., Business:Merge-Org, Justice:Trial-Hearing, Conflict:Attack) and 35 argument roles. Following previous work (Hsi et al., 2016), we conflate eight time-related argument roles - e.g., ‘Time-At-End’, ‘Time-Before’, ‘Time-At-Beginning’ - into a single ‘Time’ role in order to alleviate training data sparsity. ent types of event schemata depending on the context. Adopting these tasks for evaluation thus tests whether leveraging fine-grained curated knowledge of verbs’ semantic-syntactic behaviour can improve pretrained LMs’ reasoning about event-triggering predicates and their arguments. Model Configurations. For each task, we compare the performance of the underlying “vanilla” BERT-based model (see §2.3) against its varia"
2021.acl-long.541,D18-1330,0,0.0375193,"Missing"
2021.acl-long.541,D17-1206,0,0.0158938,"edge 4 We also experimented with sentence-level tasks: we fed (a) pairs of sentence examples from VN/FN in a binary classification setup (e.g., Jackie leads Rose to the store. – Jackie escorts Rose.); and (b) individual sentences in a multi-class classification setup (predicting the correct VN class/FN frame). These variants, however, led to weaker performance. 6954 separate from the general language knowledge acquired in pretraining. This (1) allows downstream training to flexibly combine the two sources of knowledge, and (2) bypasses the issues with catastrophic forgetting and interference (Hashimoto et al., 2017; de Masson d&apos;Autume et al., 2019). We adopt the standard efficient adapter architecture of Pfeiffer et al. (2020a,c). In each Transformer layer l, we insert a single adapter (Adapterl ) after the feed-forward sub-layer. The adapter itself is a two-layer feed-forward neural network with a residual connection, consisting of a down-projection D ∈ Rh×m , a GeLU activation (Hendrycks and Gimpel, 2016), and an upprojection U ∈ Rm×h , where h is the hidden size of the Transformer model and m is the dimensionality of the adapter: Adapterl (hl , rl ) = Ul (GeLU(Dl (hl ))) + rl ; where rl is the residu"
2021.acl-long.541,W19-4310,1,0.899186,"Missing"
2021.acl-long.541,2020.deelio-1.5,1,0.869674,"Missing"
2021.acl-long.541,K19-1061,0,0.0389188,"Missing"
2021.acl-long.541,miltsakaki-etal-2004-penn,0,0.027807,"s et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et al., 2004) and to ground a text in real-world facts. Verbs (with their arguments) are prominently used for expressing events (with their participants). Thus, fine-grained knowledge about verbs, e.g., the syntactic patterns in which they partake and the semantic frames, may help pretrained encoders to achieve a deeper understanding of text and improve their performance in event-oriented downstream tasks. There already exist some expert-curated computational resources that organise verbs into classes based on their syntactic-semantic properties (Jackendoff, 1992; Levin, 1993). In particular, here we consi"
2021.acl-long.541,2020.tacl-1.54,0,0.0262688,"knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reasoning about events.1 Identifying tokens in the text that mention events and classifying the temporal and causal relations among them is crucial to understand the structure of a story or dialogue (Carlson et al., 2002; Miltsakaki et"
2021.acl-long.541,S13-2001,0,0.0809782,"of BEARING with an additional participant, a BOWL. 6952 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6952–6969 August 1–6, 2021. ©2021 Association for Computational Linguistics We hypothesise that complementing pretrained LMs with verb knowledge should benefit model performance in downstream tasks that involve event extraction and processing. We first put this hypothesis to the test in English monolingual event identification and classification tasks from the TempEval (UzZaman et al., 2013) and ACE (Doddington et al., 2004) datasets. We report modest but consistent improvements in the former, and significant performance boosts in the latter, thus verifying that verb knowledge is indeed paramount for a deeper understanding of events and their structure. Moreover, expert-curated resources are not available for most of the languages spoken worldwide. Therefore, we also investigate the effectiveness of transferring verb knowledge across languages; in particular, from English to Spanish, Arabic and Chinese. The results demonstrate the success of the transfer techniques, and also shed"
2021.acl-long.541,C08-3012,0,0.14324,"Missing"
2021.acl-long.541,N18-1048,1,0.897103,"Missing"
2021.acl-long.541,D17-1270,1,0.878618,"Missing"
2021.acl-long.541,2021.acl-long.410,1,0.836059,"Missing"
2021.acl-long.541,2020.emnlp-main.586,1,0.842521,"Missing"
2021.acl-long.541,D19-1585,0,0.0136948,"r. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that"
2021.acl-long.541,2021.findings-acl.121,0,0.0656224,"Missing"
2021.acl-long.541,2020.semeval-1.31,0,0.455707,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,N19-1105,0,0.0867627,"dden state, output of the subsequent layer normalisation. 2.3 Downstream Fine-Tuning for Event Tasks The next step is downstream fine-tuning for event processing tasks. We experiment with (1) tokenlevel event trigger identification and classification and (2) span extraction for event triggers and arguments (a sequence labeling task); see §3. For the former, we mount a classification head – a simple single-layer feed-forward softmax regression classifier – on top of the Transformer augmented with VN-/FN-Adapters. For the latter, we follow the architecture from prior work (M’hamdi et al., 2019; Wang et al., 2019) and add a CRF layer (Lafferty et al., 2001) on top of the sequence of Transformer’s outputs (for subword tokens). For all tasks, we propose and evaluate two different fine-tuning regimes: (1) full fine-tuning, where we update both the original Transformer’s parameters and VN-/FN-Adapters (see 2a in Figure 1); and (2) task-adapter (TA) fine-tuning, where we keep both Transformer’s original parameters and VN/FN-Adapters frozen, while stacking a new trainable task adapter on top of the VN-/FN-Adapter in each Transformer layer (see 2b in Figure 1). 2.4 Cross-Lingual Transfer Creation of curated r"
2021.acl-long.541,2020.emnlp-main.129,0,0.280266,"l target-language adjustments. Massively multilingual LMs, such as multilingual BERT (mBERT) (Devlin et al., 2019) or XLMR (Conneau et al., 2020) have become the de facto standard mechanisms for zero-shot (ZS) crosslingual transfer. In our first transfer approach: we fine-tune mBERT first on the English verb knowledge, then on English task data, and then simply make task predictions for the target language input. The second approach, dubbed VTRANS, is inspired by the work on cross-lingual transfer of semantic specialisation for static word embeddings (Glava´s et al., 2019; Ponti et al., 2019; Wang et al., 2020b). In brief (with full details in Appendix C), starting from a set of positive pairs from English VN/FN, VTRANS involves three steps: (1) automatic translation of verbs in each pair into the target language, (2) filtering of the noisy target language pairs by means of a transferred relation prediction model trained on the English examples, and (3) training the verb adapters injected into the pretrained model, now with the translated and filtered target-language verb pairs. For the monolingual target-language FN-/VN-Adapter training, we follow the protocol used for English, see §2.2. 3 Experim"
2021.acl-long.541,Q15-1025,0,0.0310524,"two verbs belong to the same VN class or FN frame. We extract training instances from FN and VN independently. This allows for a separate analysis of the impact of verb knowledge from each resource. We generate positive training instances by extracting all unique verb pairings from the set of members of each main VN class/FN frame (e.g., walk–march), resulting in 181,882 instances created from VN and 57,335 from FN. We then generate k = 3 negative examples per positive example by combining controlled and random sampling. In controlled sampling, we follow prior work on semantic specialisation (Wieting et al., 2015; Glavaš and Vuli´c, 2018b; Lauscher et al., 2020b). For each positive example p = (w1 , w2 ) in the training batch B, we create two negatives pˆ1 = (w ˆ1 , w2 ) and pˆ2 = (w1 , w ˆ2 ); w ˆ1 is the verb from batch B other than w1 that is closest to w2 in terms of their cosine similarity in an auxiliary static word embedding space Xaux ∈ Rd ; conversely, w ˆ2 is the verb from B other than w2 closest to w1 . We additionally create one negative instance pˆ3 = (w ˆ1 ,w ˆ2 ) by randomly sampling w ˆ1 and w ˆ2 from batch B, not considering w1 and w2 . We ensure that the negatives are not present in"
2021.acl-long.541,D19-1582,0,0.0124374,"relied on hand-crafted, language-specific features (Ahn, 2006; Gupta and Ji, 2009; Llorens et al., 2010; Hong et al., 2011; Li et al., 2013; Glavaš and Šnajder, 2015) (e.g., POS tags, entity knowledge), which limited their generalisation ability and effectively prevented language transfer. More recent approaches commonly resorted to word embedding input and neural text encoders such as recurrent nets (Nguyen et al., 2016; Duan et al., 2017; Sha et al., 2018) and convolutional nets (Chen et al., 2015; Nguyen and Grishman, 2015), 6959 as well as graph neural networks (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and boot"
2021.acl-long.541,P19-1522,0,0.103672,"ore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge. Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge. 1 Introduction Large Transformer-based encoders, pretrained with self-supervised language modeling (LM) objectives, form the backbone of state-of-the-art models for most NLP tasks (Devlin et al., 2019; Yang et al., 2019b; Liu et al., 2019). Recent probes showed that they implicitly extract a non-negligible amount of linguistic knowledge from text corpora in an unsupervised fashion (Hewitt and Manning, 2019; Vuli´c et al., 2020; Rogers et al., 2020, inter alia). In downstream tasks, however, they often rely on spurious correlations and superficial cues (Niven and Kao, 2019) rather than a deep understanding of language meaning (Bender and Koller, 2020), which is detrimental to both generalisation and interpretability (McCoy et al., 2019). In this work, we focus on a specific facet of linguistic knowledge: reas"
2021.acl-long.541,P14-2089,0,0.036386,"et al., 2013; Bojanowski et al., 2017) or those spanned by LM-pretrained Transformers (Devlin et al., 2019; Liu et al., 2019), encode only distributional knowledge. A large body of work focused on semantic specialisation of such distributional spaces by injecting lexico-semantic knowledge from external resources (e.g., WordNet (Fellbaum, 1998), BabelNet (Navigli and Ponzetto, 2010) or ConceptNet (Liu and Singh, 2004)) in the form of lexical constraints (Faruqui et al., 2015; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018b; Kamath et al., 2019; Vuli´c et al., 2021). Joint specialisation models (Yu and Dredze, 2014; Lauscher et al., 2020b; Levine et al., 2020, inter alia) train the representation space from scratch on the large corpus, but augment the selfsupervised training objective with an additional objective based on external lexical constraints. Lauscher et al. (2020b) add to the Masked LM (MLM) and next sentence prediction (NSP) pretraining objectives of BERT (Devlin et al., 2019) an objective that predicts pairs of (near-)synonyms, aiming to improve word-level semantic similarity in BERT’s representation space. In a similar vein, Levine et al. (2020) add the objective that predicts WordNet super"
2021.acl-long.541,Y18-1097,0,0.00983903,"s (Nguyen and Grishman, 2018; Yan et al., 2019) and adversarial networks (Hong et al., 2018; Zhang et al., 2019). Most recent empirical advancements in event trigger and argument extraction tasks stem from fine-tuning of LM-pretrained Transformer networks (Yang et al., 2019a; Wang et al., 2019; M’hamdi et al., 2019; Wadden et al., 2019; Liu et al., 2020). Limited training data nonetheless remains an obstacle, especially when facing previously unseen event types. The alleviation of such data scarcity issues was attempted through data augmentation – automatic data annotation (Chen et al., 2017; Zheng, 2018; Araki and Mitamura, 2018) and bootstrapping for training data generation (Ferguson et al., 2018; Wang et al., 2019). The recent release of the large English event detection dataset MAVEN (Wang et al., 2020c), with annotations of event triggers only, partially remedies for English data scarcity. MAVEN also demonstrates that even the state-of-the-art Transformer models fail to yield satisfying event detection performance in the general domain. The fact that it is unlikely to expect datasets of similar size for other event extraction tasks and especially for other languages only emphasises the"
2021.acl-short.72,W19-1909,0,0.0585707,"Missing"
2021.acl-short.72,Q19-1038,0,0.0287965,"AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for language understanding tasks have been achieved (Artetxe and Schwenk, 2019; Hu et al., 2020). However, it remains unclear whether such transfer techniques can be used to improve domain-specific NLP applications and mitigate the gap between knowledge-enhanced models in resource-rich versus resource-poor languages. In this paper, we thus investigate the current performance gaps in the BEL task beyond English, and propose several cross-lingual transfer techniques to improve domain-specialised representations and BEL in resource-lean languages. In particular, we first present a novel crosslingual BEL (XL - BEL) task and its corresponding evaluation benchmark in 10 typol"
2021.acl-short.72,2020.emnlp-main.253,1,0.890817,"rt. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension o"
2021.acl-short.72,W19-5403,0,0.0578786,"Missing"
2021.acl-short.72,2020.coling-main.118,1,0.788893,"Missing"
2021.acl-short.72,2020.acl-main.423,0,0.0348157,"nowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a f"
2021.acl-short.72,2021.naacl-main.334,1,0.851504,"get languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are available only for a few languages, hindering the development of domainspecific NLP models in all other languages. Simultaneously, exciting breakthroughs in crosslingual transfer for l"
2021.acl-short.72,2021.ccl-1.108,0,0.0946509,"Missing"
2021.acl-short.72,I11-1029,0,0.0423863,"our XL - BEL benchmark. The statistics of the benchmark are available in Table 1. We also convert word and phrase translations into the same format (§2.1), where each ‘class’ now contains only two examples. For a translation pair (xp , xq ), we create a unique pseudo-label yxp ,xq and produce two new name-label instances (xp , yxp ,xq ) and (xq , yxp ,xq ),4 and proceed as in §2.1. This allows us to easily combine domainspecific knowledge with general translation knowledge within the same S AP framework. 3 The XL - BEL Task and Evaluation Data A general cross-lingual entity linking (EL) task (McNamee et al., 2011; Tsai and Roth, 2016) aims to map a mention of an entity in free text of any language to a controlled English vocabulary, typically obtained from a knowledge graph (KG). In this work, we propose XL - BEL, a cross-lingual biomedical EL task. Instead of grounding entity mentions to English-specific ontologies, we use UMLS as a language-agnostic KG: the XL - BEL task requires a model to associate a mention in any language to a (language-agnostic) CUI in UMLS. XL - BEL thus serves as an ideal evaluation benchmark for biomedical entity representations: it challenges the capability of both 1) repre"
2021.acl-short.72,W19-5006,0,0.018804,"ta, and pretrained models are available online at: github.com/cambridgeltl/sapbert. 2 Methodology Learning Background and Related Work. biomedical entity representations is at the core of BioNLP, benefiting, e.g., relational knowledge discovery (Wang et al., 2018) and literature search (Lee et al., 2016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks."
2021.acl-short.72,2020.emnlp-main.365,0,0.0598287,"Missing"
2021.acl-short.72,2020.acl-main.335,0,0.204153,"016). In the current era of contextualised representations based on Transformer architectures (Vaswani et al., 2017), biomedical text encoders are pretrained via Masked Language Modelling (MLM) on diverse biomedical texts such as PubMed articles (Lee et al., 2020; Gu et al., 2020), clinical notes (Peng et al., 2019; Alsentzer et al., 2019), and even online health forum posts (Basaldella et al., 2020). However, it has been empirically verified that naively applying MLMpretrained models as entity encoders does not perform well in tasks such as biomedical entity linking (Basaldella et al., 2020; Sung et al., 2020). Recently, Liu et al. (2021) proposed S AP (SelfAlignment Pretraning), a fine-tuning method that leverages synonymy sets extracted from UMLS to improve B ERT’s ability to act as a biomedical entity encoder. Their S AP B ERT model currently achieves SotA scores on all major English BEL benchmarks. In what follows, we first outline the S AP procedure, and then discuss the extension of the method to include multilingual UMLS synonyms (§2.1), and then introduce another S AP extension which combines domain-specific synonyms with generaldomain translation data (§2.2). Training Examples. Given a min"
2021.acl-short.72,P19-1139,0,0.0265228,"available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data. 1 Introduction Recent work has demonstrated that it is possible to combine the strength of 1) Transformer-based encoders such as B ERT (Devlin et al., 2019; Liu et al., 2019), pretrained on large general-domain data with 2) external linguistic and world knowledge (Zhang et al., 2019; Levine et al., 2020; Lauscher et al., 2020). Such expert human-curated knowledge is crucial for NLP applications in specialised domains such as biomedicine. There, Liu et al. (2021) recently proposed self-alignment pretraining (S AP), a technique to fine-tune B ERT on phraselevel synonyms extracted from the Unified Medical Language System (UMLS; Bodenreider 2004).1 Their S AP B ERT model currently holds state-of-theart (SotA) across all major English biomedical entity linking (BEL) datasets. However, this approach is not widely applicable to other languages: abundant external resources are a"
2021.conll-1.44,2021.eacl-main.140,0,0.027205,"different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings"
2021.conll-1.44,D14-1110,0,0.0244753,"C’s effects on embedding properties such as isotropy. ings of M IRRORW I C, and its impact on the contextual representation space. We release our code at github.com/cambridgeltl/MirrorWiC. 2 Related Work and Background Word-in-Context Representations. Modelling context influence on lexical meaning and creating context-aware word representations is a longstanding research goal in lexical semantics. One direction is to create discrete sense embeddings according to a fixed sense inventory such as WordNet. These embeddings can be created from the attributes in the sense inventory such as glosses (Chen et al., 2014) or from the knowledge structure (Camacho-Collados et al., 2016). We point to Camacho-Collados and Pilehvar (2018) for a thorough survey on sense embeddings. Such sense representations require a fixed and discrete sense inventory and might not be sensitive enough to the the dynamic and fluid nature of contextual changes. More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benc"
2021.conll-1.44,N19-1423,0,0.195405,"e two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tail"
2021.conll-1.44,J13-3003,0,0.0344753,"ever, even if the items in the pair happen to have similar meanings, our learning objective still instructs the model to push them away from each other. Our rationale and decision here are based on the following: (1) Such false negative pairs can act as a regularisation; and (2) in essence, one could argue that all distinct word-in-context instances have slightly different meanings since sense is a continuous function of word and context. 4 Experimental Setup tasks: Usim and CoSimLex; two word-in-context classification tasks: WiC and WiC-TSV; and oneshot Word Sense Disambiguation (WSD). Usim (Erk et al., 2013) measures the similarity between two instances of the same word occurring in two different sentential contexts. CoSimLex (Armendariz et al., 2020) measures the change in similarity between two different words appearing in two different contexts: paragraphs. We follow the standard evaluation protocol, computing the cosine similarity of the contextual word representations and comparing them against human-elicited scores via Spearman’s rank correlation (ρ). The WiC classification task (Pilehvar and Camacho-Collados, 2019) challenges a model to make a binary decision on whether or not the same tar"
2021.conll-1.44,D19-1006,0,0.0403436,"Missing"
2021.conll-1.44,2021.emnlp-main.552,0,0.0564678,"Missing"
2021.conll-1.44,W19-0423,0,0.0266549,"they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance"
2021.conll-1.44,D19-1533,0,0.0166081,"We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs a"
2021.conll-1.44,2021.acl-long.197,0,0.062921,"Missing"
2021.conll-1.44,2020.acl-main.423,0,0.187269,"states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/feature space (Garí Soler et al., 2019; Wiedemann et al., 2019; Reif et al., 2019; Garí Soler and Apidianaki, 2021). While off-the-shelf PLMs already offer a useful contextualised lexical semantic space, their contextualised representation"
2021.conll-1.44,2021.naacl-main.334,1,0.803834,"Missing"
2021.conll-1.44,2021.emnlp-main.109,1,0.847856,"Missing"
2021.conll-1.44,2020.emnlp-main.333,1,0.928285,"ao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other words, the contextual representations, typically extracted as the averages over the top four layers of a base PLM (Liu et al., 2020; Garí Soler and Apidianaki, 2021), can be seen as a by-product of training a language model, and are not directly optimised for contextual sensitivity. Inspired by the previous work on adaptive fine-tuning for word and sentence representations (Liu et al., 2021b), we propose a simple self-supervised technique termed M IRRORW I C: it rewires input PLMs to provide improved word-incontext (WiC) representations. Unlike prior work on fine-tuning towards improving WiC representations, our M IRRORW I C procedure disposes of any sense labels, annotated task data, and any external knowledge, and elici"
2021.conll-1.44,2021.emnlp-main.571,1,0.82908,"Missing"
2021.conll-1.44,2021.ccl-1.108,0,0.0776108,"Missing"
2021.conll-1.44,P19-1569,0,0.0183662,"More recently, PLMs provide dynamic and continuous contextual representations, not tied to predefined sense inventories, computed as a function of both the target word and its context. The use of PLMs has resulted in further progress on a range of context-aware evaluation benchmarks (Pilehvar and Camacho-Collados, 2019; Wang et al., 2019; Raganato et al., 2020). A body of work has aimed to enrich context-aware and sense information in the PLMs by injecting such knowledge (e.g., sense annotations from predefined sense inventories) at pretraining stage (Levine et al., 2020) or during inference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciti"
2021.conll-1.44,2020.scil-1.35,0,0.0874921,"Missing"
2021.conll-1.44,2020.coling-main.602,0,0.0154305,"M IRROR W I C method, based on contrastive learning, for eliciting better word-in-context (WiC) representations from pretrained language models. We augment a randomly selected WiC instance with random span masking and apply dropout to the hidden states to create two slightly different representations of the base instance. These two representations form a positive pair for contrastive fine-tuning. During fine-tuning, we pull the representations of each positive pair closer together, while at the same time pushing away representations of other WiC instances, serving as negative examples. 2020; Pedinotti and Lenci, 2020). As a consequence, they usually fall far behind the performance of the same PLM fine-tuned with (i) sense annotations (Hadiwinoto et al., 2019; Blevins and Zettlemoyer, 2020) or (ii) external (e.g., WordNet) knowledge (Levine et al., 2020). Introduction Pretrained Language Models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) provide dynamic contextual representations; they induce token-level lexical representations that capture the impact of the word’s context on its embedding. Recent studies have assessed the PLMs by probing into their off-the-shelf representation/"
2021.conll-1.44,N19-1128,0,0.0323547,"Missing"
2021.conll-1.44,E17-1010,0,0.0271206,"a binary decision on whether or not the same target word has the same meaning in two different contexts. The WiC-TSV (TSV) task (Breit et al., 2021) extends the original WiC to multiple domains with three different subtasks. In TSV-1, the task is to decide if the intended sense of the target word in the context matches the target sense described by the definition. In TSV-2, the model must identify if the intended sense (in the context) is the hyponym of the provided hypernyms. TSV-3 combines the previous two subtasks (see Breit et al. (2021) for further details). The WSD task (Navigli, 2009; Raganato et al., 2017) requires a system to select the correct label for a given target word in context from a candidate set of all possible meanings for this target word. To evaluate the feature space of the models in WSD, we create a one-shot setting where we provide one context example3 per label and perform nearest neighbour search over contextual word representations from the candidate labels. We directly test the models on the concatenated ALL test set from Raganato et al. (2017) without access to training and development data. We also perform multilingual and cross-lingual evaluation on XL-WiC (Raganato et a"
2021.conll-1.44,2020.emnlp-main.584,0,0.0500662,"Missing"
2021.conll-1.44,2021.acl-short.73,0,0.0937812,"Missing"
2021.conll-1.44,D19-1410,0,0.288673,"ic space, their contextualised representation spaces suffer from instability and anisotropy (Mickus et al., ∗ Due to the fat-tailed nature of pandemic risk… However, PLMs have been shown to actually store more lexical and sentence-level information than what can be directly extracted from their offthe-shelf variants. In simple words, this knowledge must be ‘unlocked’ or exposed via additional adaptive fine-tuning (Ruder, 2021). For instance, while off-the-shelf PLMs are not directly effective as universal sentence encoders, it is possible to convert them into such encoders through supervised (Reimers and Gurevych, 2019a; Feng et al., 2020; Liu et al., 2021a) or self-supervised fine-tuning (Carlsson et al., 2021; Liu et al., 2021b; Gao et al., 2021) based on the contrastive learning paradigm. The fundamental limitation of extracting conEqual contribution. 562 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 562–574 November 10–11, 2021. ©2021 Association for Computational Linguistics j textual features/representations directly from the layers of the off-the-shelf PLMs is the mismatch between their (pre)training objectives and the feature extraction method. In other"
2021.conll-1.44,2020.emnlp-main.586,1,0.844463,"Missing"
2021.conll-1.44,2021.acl-long.393,0,0.0325412,"nference (Loureiro and Jorge, 2019). Other work has attempted at combining/ensembling multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive finen"
2021.conll-1.44,2021.acl-long.402,0,0.0262613,"multiple contextualised and static type-level embeddings to refine the contextualised representation space (Liu et al., 2020; Xu et al., 2020). Inducing Text Representations from PLMs via Self-Supervision. Recently, there has been growing interest in learning completely unsupervised sentence representations from PLMs using conContributions. 1) We present a simple yet ex- trastive learning techniques (Carlsson et al., 2021; tremely effective unsupervised M IRRORW I C tech- Liu et al., 2021b; Gao et al., 2021; Yan et al., 2021; nique for eliciting contextual lexical knowledge. Kim et al., 2021; Zhang et al., 2021). Similar to 2) Our experiments on a range of English, mul- the supervised approaches such as Sentence-BERT tilingual, and cross-lingual context-sensitive lex- (Reimers and Gurevych, 2019b) or SapBERT (Liu ical benchmarks demonstrate that M IRRORW I C et al., 2021a), the idea is to transform an input PLM achieves consistent and substantial improvements into an effective sentence encoder via additional over different baseline PLMs, indicating its robust- fine-tuning. During self-supervised contrastive fineness and wide applicability. 3) We offer extensive tuning, the model learns from identical"
2021.eacl-main.270,2020.acl-main.493,0,0.221541,"er to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We t"
2021.eacl-main.270,2020.acl-main.747,0,0.10477,"Missing"
2021.eacl-main.270,D18-1269,0,0.163913,"l language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBER"
2021.eacl-main.270,de-marneffe-etal-2006-generating,0,0.0180012,"Missing"
2021.eacl-main.270,N19-1423,0,0.369971,"n, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU ta"
2021.eacl-main.270,2020.tacl-1.3,0,0.0177432,"languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-"
2021.eacl-main.270,W18-6008,0,0.0150566,"applied on BERT’s contextualized word vectors, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al. (2020) extend the analysis to multilingual BERT, finding that its representation subspaces may recover trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps"
2021.eacl-main.270,N19-1419,0,0.038581,"directly on representations from transformer’s output layer, eliminating the head- and dependendantbased feed-forward mapping. Despite this simplification, our biaffine parser produces DP results comparable to current state-of-the-art parsers. Syntactic BERTology. The substantial body of syntactic probing work shows that BERT (Devlin et al., 2019) (a) encodes text in a hierarchical manner (i.e., it encodes some implicit underlying syntax) (Lin et al., 2019); and (b) captures specific shallow syntactic information (parts-of-speech and syntactic chunks) (Tenney et al., 2019; Liu et al., 2019a). Hewitt and Manning (2019) find that linear transformations, when applied on BERT’s contextualized word vectors, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al. (2020) extend the analysis to multilingual BERT, finding that its representation subspaces may recover trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds"
2021.eacl-main.270,J07-3004,0,0.0177526,"ed syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin"
2021.eacl-main.270,N19-1075,0,0.0253781,"s (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence"
2021.eacl-main.270,Q16-1023,0,0.0343826,"nce and phrase structure tree) into an LSTMbased student pretrained on a much larger corpus. They show that distillation helps the student in structured prediction tasks, but their downstream evaluation does not involve LU tasks. Their subsequent work (Kuncoro et al., 2020) replaces the RNN student with BERT (Devlin et al., 2019): syntactic distillation again helps structured prediction, but hurts (slightly) the performance on LU tasks from the GLUE benchmark (Wang et al., 2018). Transformer-Based Dependency Parsing. Building on the success of preceding neural parsers (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Dozat and Manning (2017) proposed a biaffine parsing head on top of a Bi-LSTM encoder: contextualized word vectors are fed to two feedforward networks, producing dependent- and headspecific token representations, respectively. Arc and relation scores are produced via biaffine products between these dependent- and head-specific representation matrices. Finally, the Edmonds algorithm induces the optimal tree from pairwise arc predictions. Most recent DP work (Kondratyuk ¨ un et al., 2020) replaces the and Straka, 2019; Ust¨ Bi-LSTM encoder with multilingual BERT’s transformer, reporting state-"
2021.eacl-main.270,P03-1054,0,0.161861,"sults, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et"
2021.eacl-main.270,D19-1279,0,0.349309,"fore and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models? 1 Introduction Structural analysis of sentences, based on a variety of syntactic formalisms (Charniak, 1996; Taylor et al., 2003; De Marneffe et al., 2006; Hockenmaier and Steedman, 2007; Nivre et al., 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of"
2021.eacl-main.270,2020.acl-main.375,0,0.068379,"and modeling, but rather to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to sta"
2021.eacl-main.270,P19-1337,0,0.0676021,"training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream target languages, before the downstream fine-tuning on source language (English) data. While intermediate parsing training is obviously not the only way of bringing syntactic knowledge to downstream tasks (Kuncoro et al., 2019; Swayamdipta et al., 2019; Kuncoro et al., 2020), it is arguably the most straightforward way of injecting syntactic signal in the context of the predominant pretraining-fine-tuning paradigm that has, nonetheless, not been investigated up to this point. Other methods of bringing syntactic signal to downstream tasks such as knowledge distillation (Kuncoro et al., 2020) and pre-training on shallow trees instead of sequences (Swayamdipta et al., 2019) have failed to demonstrate significant gains on higher-level LU tasks. Our results also render supervised UD parsing largely inconsequential to LU"
2021.eacl-main.270,2020.tacl-1.50,0,0.17146,"-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream target languages, before the downstream fine-tuning on source language (English) data. While intermediate parsing training is obviously not the only way of bringing syntactic knowledge to downstream tasks (Kuncoro et al., 2019; Swayamdipta et al., 2019; Kuncoro et al., 2020), it is arguably the most straightforward way of injecting syntactic signal in the context of the predominant pretraining-fine-tuning paradigm that has, nonetheless, not been investigated up to this point. Other methods of bringing syntactic signal to downstream tasks such as knowledge distillation (Kuncoro et al., 2020) and pre-training on shallow trees instead of sequences (Swayamdipta et al., 2019) have failed to demonstrate significant gains on higher-level LU tasks. Our results also render supervised UD parsing largely inconsequential to LU. We observe limited and inconsistent gains only"
2021.eacl-main.270,2020.emnlp-main.185,1,0.890601,"Missing"
2021.eacl-main.270,P14-2050,0,0.010901,"et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for a"
2021.eacl-main.270,2020.acl-main.467,0,0.0274302,"brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-tuning for the target task. Phang et al. (2018) show that intermediate NLI training of BERT on the Multi-NLI dataset (Williams et al., 2018) benefits several language understanding tasks. Subsequent work (Wang et al., 2019; Pruksachatkun et al., 2020) investigated many combinations of intermediate and target LU tasks, failing to identify any universally beneficial intermediate task. In this work we use DP as an intermediate training task (IPT) for LM-pretrained transformers. 3 Methodology Biaffine Parser. Our parsing model, illustrated in Figure 1, consists of a biaffine attention layer applied directly on the transformer’s output (BERT, RoBERTa, mBERT, or XLM-R). We first obtain word-level vectors by averaging transformed representations of their constituent subwords, produced by the transformer. Let X ∈ RN ×H denote the encoding of a sen"
2021.eacl-main.270,W19-4825,0,0.103726,"o invalidate the admirable efforts on syntactic annotation and modeling, but rather to make an empirically driven step towards a deeper understanding of the relationship between LU and formalised syntactic knowledge, and the extent of its impact to modern semantic LU and applications. (RQ) Is explicit structural language information, provided in the form of a widely adopted syntactic formalism (Universal Dependencies, UD) (Nivre et al., 2016) and injected in a supervised manner into LM-pretrained transformers beneficial for transformers’ downstream LU performance? While existing body of work (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Kulmizev et al., 2020; Chi et al., 2020) probes transformers for structural phenomena, our work is more pragmatically motivated. We directly evaluate the effect of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Connea"
2021.eacl-main.270,N19-1112,0,0.353879,", 2016, 2020, inter alia), has been the beating heart of NLP pipelines for decades (Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Gold"
2021.eacl-main.270,2021.ccl-1.108,0,0.052308,"Missing"
2021.eacl-main.270,P19-1334,0,0.050602,"Missing"
2021.eacl-main.270,2020.lrec-1.497,0,0.0489337,"Missing"
2021.eacl-main.270,N18-1202,0,0.0418824,"pe that these empirical findings will shed new light on the relationship between supervised parsing (and manually labeled treebanks) and LU with transformer networks, and guide further similar investigations in future work, in order to fully understand the impact of formal syntactic knowledge on LU performance with modern neural architectures. 2 Related Work Bringing Explicit Syntax to LMs. Previous work has attempted to enrich language models with explicit syntactic knowledge in ways other than intermediate parsing training. Swayamdipta et al. (2019) modify the pretraining objective of ELMo (Peters et al., 2018) to learn from shallowly parsed (i.e., chunked) corpora. They, however, report no notable improvements on downstream tasks. Kuncoro et al. (2019) propose to distil the knowledge from a Recurrent NN Grammar (RNNG) teacher trained on a small syntactically annotated corpus (by modeling the joint probability of surface sequence and phrase structure tree) into an LSTMbased student pretrained on a much larger corpus. They show that distillation helps the student in structured prediction tasks, but their downstream evaluation does not involve LU tasks. Their subsequent work (Kuncoro et al., 2020) rep"
2021.eacl-main.270,2020.emnlp-demos.7,1,0.914258,"Missing"
2021.eacl-main.270,D19-1454,0,0.0442123,"Missing"
2021.eacl-main.270,2020.emnlp-main.180,0,0.182024,"Missing"
2021.eacl-main.270,D19-1221,0,0.0184478,"r trees also for other languages. They also provide evidence that clusters of head–dependency pairs roughly correspond to UD relations. Similarly, Kulmizev et al. (2020) show that BERT’s latent syntax corresponds more to UD trees than to shallower SUD (Gerdes et al., 2018) structures. Despite the evident similarity between BERT’s latent syntax and formalisms such as UD, there is ample evidence that BERT insufficiently leverages syntax in downstream tasks: it often produces similar predictions for syntactically valid as well as for structurally corrupt sentences (e.g., with random word order) (Wallace et al., 2019; Ettinger, 2020; Zhao et al., 2020). Intermediate Training. Sometimes called Supplementary Training on Intermediate Labeled-data Tasks (STILT) (Phang et al., 2018), intermediate Relation classifier Arc classifier X X’ concat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) be"
2021.eacl-main.270,P19-1439,0,0.0198921,"ncat concat [root] brown + jumps + ... Word-level average pooling ... Transformer (BERT / RoBERTa) [CLS] The quick br ##own fox jump ##s ... [SEP] Figure 1: Architecture of our transformer-based biaffine dependency parser. training is a transfer learning setup in which one trains an LM-pretrained transformer on one or more supervised tasks (ideally with large training sets) before final fine-tuning for the target task. Phang et al. (2018) show that intermediate NLI training of BERT on the Multi-NLI dataset (Williams et al., 2018) benefits several language understanding tasks. Subsequent work (Wang et al., 2019; Pruksachatkun et al., 2020) investigated many combinations of intermediate and target LU tasks, failing to identify any universally beneficial intermediate task. In this work we use DP as an intermediate training task (IPT) for LM-pretrained transformers. 3 Methodology Biaffine Parser. Our parsing model, illustrated in Figure 1, consists of a biaffine attention layer applied directly on the transformer’s output (BERT, RoBERTa, mBERT, or XLM-R). We first obtain word-level vectors by averaging transformed representations of their constituent subwords, produced by the transformer. Let X ∈ RN ×H"
2021.eacl-main.270,W18-5446,0,0.184779,"(Klein and Manning, 2003; Chen and Manning, 2014; Dozat and Manning, 2017; Kondratyuk and Straka, 2019), establishing Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk rather strong common belief that high-level semantic language understanding (LU) crucially depends on explicit syntax. The unprecedented success of neural language learning models based on transformer networks (Vaswani et al., 2017), trained on unlabeled corpora via language modeling (LM) objectives (Devlin et al., 2019; Liu et al., 2019b; Clark et al., 2020, inter alia) on a wide variety of LU tasks (Wang et al., 2018; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019;"
2021.eacl-main.270,N18-1101,0,0.327694,"t of infusing structural language information from UD treebanks, via intermediate dependency parsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual tra"
2021.eacl-main.270,D19-1382,0,0.10091,"arsing (DP) training, on transformers’ performance in downstream LU. To this end, we couple a pretrained transformer with a biaffine parser similar to Dozat and Manning (2017), and train the model (i.e., fine-tune the transformer) for DP. Our parser on top of RoBERTa (Liu et al., 2019b) and XLM-R (Conneau et al., 2020) produces DP results which are comparable to state of the art. We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) (Williams et al., 2018; Conneau et al., 2018), paraphrase identification (Zhang et al., 2019b; Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019; Ponti et al., 2020). We quantify the contribution of explicit syntax by comparing LU performance of the transformer exposed to intermediate parsing training (IPT) and its counterpart directly fine-tuned for the downstream task. We investigate the effects of IPT (1) monolingually, by fine-tuning English transformers, BERT and RoBERTa, on an English UD treebank and for (2) downstream zero-shot language transfer, by fine-tuning massively multilingual transformers (MMTs) – mBERT and XLM-R (Conneau et al., 2020) – on treebanks of downstream targ"
2021.eacl-main.270,N19-1118,0,0.338711,"; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence tackling human lang"
2021.eacl-main.270,E17-1063,0,0.0191432,"he representation of syntactic dependants and X0 as the representation of dependency heads. We then directly compute the arc and relation scores as biaffine products of X and X0 : &gt; &gt; Yarc = XWarc X0 + Barc ; Yrel = XWrel X0 + Brel RH×H RH×H×R where Warc ∈ and Wrel ∈ denote, respectively, the arc classification matrix and relation classification tensor (with R as the number of relations); Barc and Brel denote the corresponding bias parameters. We greedily select the dependency head for each word by finding the maximal score in each row of Yarc : while this is not guaranteed to produce a tree, Zhang et al. (2017) show that in most cases it does.3 Our arc prediction loss is the cross-entropy loss with sentence words (plus the root node) as categorical labels: this implies a different number of labels for different sentences. We compute the relation prediction loss as a cross-entropy loss over gold arcs. Our final loss is the sum of the arc loss and relation loss. Note that, in comparison with the original biaffine parser (Dozat and Manning, 2017) and its other transformer-based variants (Kondratyuk and ¨ un et al., 2020), we feed wordStraka, 2019; Ust¨ level representations derived from the transformer"
2021.eacl-main.270,N19-1131,0,0.354988,"; Hu et al., 2020), however, questions this widely accepted assumption. The question of necessity of supervised parsing for LU and NLP in general has been raised before. More than a decade ago, Bod (2007) questioned the superiority of supervised parsing over unsupervised induction of syntactic structures in the context of statistical machine translation. Nonetheless, the NLP community has since still managed to find sufficient evidence for the usefulness of explicit syntax in higher-level LU tasks (Levy and Goldberg, 2014; Cheng and Kartsaklis, 2015; Bastings et al., 2017; Kasai et al., 2019; Zhang et al., 2019a, inter alia). However, we believe that the massive improvements brought about by the LM-pretrained transformers – unexposed to any explicit syntactic signal – warrant a renewed scrutiny of the utility of supervised parsing for high-level language understanding.1,2 The research question we address in this work can be summarized as follows: 1 Disclaimer 1: In this work, we make a clear distinction between Computational Linguistics (CL), i.e., the area of linguistics leveraging computational methods for analyses of human languages and NLP, the area of artificial intelligence tackling human lang"
2021.eacl-main.270,2020.acl-main.151,1,0.861869,"Missing"
2021.emnlp-main.109,2020.emnlp-main.253,1,0.647974,"similarities between xi and all other strings besides xi (the negatives).2 3 Experimental Setup Evaluation Tasks: Lexical. We evaluate on domain-general and domain-specific tasks: word similarity and biomedical entity linking (BEL). For the former, we rely on the Multi-SimLex evaluation set (Vuli´c et al., 2020a): it contains human-elicited word similarity scores for multiple languages. For the latter, we use NCBI-disease (NCBI, Do˘gan et al. 2014), BC5CDR-disease, BC5CDR-chemical (BC5-d, BC5-c, Li et al. 2016), AskAPatient (Limsopatham and Collier, 2016) and COMETA (stratified-general split, Basaldella et al. 2020) as our evaluation datasets. The first three datasets are in the scientific domain (i.e., the data have been extracted from scientific papers), while the latter two 2 We also experimented with another state-of-the-art contrastive learning scheme proposed by Liu et al. (2021). There, hard triplet mining combined with multi-similarity loss (MS loss) is used as the learning objective. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social me"
2021.emnlp-main.109,S14-2010,0,0.0705321,"Missing"
2021.emnlp-main.109,S16-1081,0,0.0605945,"Missing"
2021.emnlp-main.109,D15-1075,0,0.308897,"2021). 1 Introduction In order to address this gap, recent work has Transfer learning with pretrained Masked Lan- trained dual-encoder networks on labelled exterguage Models (MLMs) such as BERT (Devlin et al., nal resources to convert MLMs into universal lan2019) and RoBERTa (Liu et al., 2019) has been guage encoders. Most notably, Sentence-BERT widely successful in NLP, offering unmatched per- (SBERT, Reimers and Gurevych 2019) further formance in a large number of tasks (Wang et al., trains BERT and RoBERTa on Natural Language 2019a). Despite the wealth of semantic knowledge Inference (NLI, Bowman et al. 2015; Williams et al. stored in the MLMs (Rogers et al., 2020), they do 2018) and sentence similarity data (Cer et al., 2017) not produce high-quality lexical and sentence em- to obtain high-quality universal sentence embedbeddings when used off-the-shelf, without further dings. Recently, SapBERT (Liu et al., 2021) self1442 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1442–1459 c November 7–11, 2021. 2021 Association for Computational Linguistics aligns phrasal representations of the same meaning using synonyms extracted from the UMLS (Bodenreider,"
2021.emnlp-main.109,S12-1051,0,0.0536384,"e. InfoNCE and triplet mining + MS loss work mostly on par, with slight gains of one variant in some tasks, and vice versa. For simplicity and brevity, we report the results only with InfoNCE. 1444 are in the social media domain (i.e., extracted from online forums discussing health-related topics). We report Spearman’s rank correlation coefficients (ρ) for word similarity; accuracy @1/@5 is the standard evaluation measure in the BEL task. Evaluation Tasks: Sentence-Level. Evaluation on the intrinsic sentence textual similarity (STS) task is conducted on the standard SemEval 20122016 datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS Benchmark (STS-b, Cer et al. 2017), SICK-Relatedness (SICK-R, Marelli et al. 2014) for English; STS SemEval-17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-li"
2021.emnlp-main.109,S13-1004,0,0.0904508,"Missing"
2021.emnlp-main.109,Q16-1028,0,0.0521718,"orms standard dropout and is even worse than not using dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we r"
2021.emnlp-main.109,P18-1073,0,0.0275264,"17 data is used for Spanish and Arabic (Cer et al., 2017), and we also evaluate on Russian STS.3 We report Spearman’s ρ rank correlation. Evaluation in the question-answer entailment task is conducted on QNLI (Rajpurkar et al., 2016; Wang et al., 2019b). It contains 110k English QA pairs with binary entailment labels.4 Evaluation Tasks: Cross-Lingual. We also assess the benefits of Mirror-BERT on cross-lingual representation learning, evaluating on cross-lingual word similarity (CLWS, Multi-SimLex is used) and bilingual lexicon induction (BLI). We rely on the standard mapping-based BLI setup (Artetxe et al., 2018), and training and test sets from Glavaš et al. (2019), reporting accuracy @1 scores (with CSLS as the word retrieval method, Lample et al. 2018). Mirror-BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021"
2021.emnlp-main.109,D18-2029,0,0.115897,"Missing"
2021.emnlp-main.109,N19-1423,0,0.116457,"Missing"
2021.emnlp-main.109,ehrmann-etal-2014-representing,0,0.0845129,"shing away the negatives (§2.3). 2.1 Training Data through Self-Duplication The key to success of dual-network representation learning (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020; Liu et al., 2021, inter alia) is the construction of positive and negative pairs. While negative pairs can be easily obtained from randomly sampled texts, positive pairs usually need to be manually annotated. In practice, they are extracted from labelled task data (e.g., NLI) or knowledge bases that store relations such as synonymy or hypernymy (e.g., PPDB, Pavlick et al. 2015; BabelNet, Ehrmann et al. 2014; WordNet, Fellbaum 1998; UMLS). Mirror-BERT, however, does not rely on any external data to construct the positive examples. In a nutshell, given a set of non-duplicated strings X , we assign individual labels (yi ) to each string and build a dataset D = {(xi , yi )|xi ∈ X , yi ∈ {1, . . . , |X |}}. We then create self-duplicated training data D0 simply by repeating every element in D. In other words, let X = {x1 , x2 , . . .}. We then have D = {(x1 , y1 ), (x2 , y2 ), . . .} and D0 = {(x1 , y1 ), (x1 , y 1 ), (x2 , y2 ), (x2 , y 2 ), . . .} where x1 = x1 , y1 = y 1 , x2 = x2 , y2 = y 2 , . ."
2021.emnlp-main.109,D19-1006,0,0.084111,"dropout at all. As the only difference between controlled and standard dropout is the augmented features for positive pairs in the latter case, this suggests that the gain from +Mirror indeed stems from the data augmentation effect rather than from regularisation. Mirror-BERT Improves Isotropy? (Fig. 7). We argue that the gains with Mirror-BERT largely stem from its reshaping of the embedding space geometry. Isotropy (i.e., uniformity in all orientations) of the embedding space has been a favourable property for semantic similarity tasks (Arora et al., 2016; Mu and Viswanath, 2018). However, Ethayarajh (2019) shows that (off-the-shelf) MLMs’ representations are anisotropic: they reside in a narrow cone in the vector space and the average cosine similarity of (random) data points is extremely high. Sentence embeddings induced from MLMs without fine-tuning thus suffer from spatial anistropy (Li et al., 2020; Su et al., 2021). Is Mirror-BERT then improving isotropy of the embedding space?14 To investigate this claim, we inspect (1) the distributions of cosine similarities and (2) an isotropy score, as defined by Mu and Viswanath (2018). First, we randomly sample 1,000 sentence pairs from the Quora Qu"
2021.emnlp-main.109,2021.emnlp-main.552,0,0.385338,"Missing"
2021.emnlp-main.109,2021.acl-long.72,0,0.0257597,"ry of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. Very recently, there has been a growing interest in using individual raw sentences for self- as well as on biomedical entity linking. The large gains over base ML"
2021.emnlp-main.109,N16-1162,1,0.835972,"cross-lingual tasks. Self-supervised text representations have a large body of literature. Here, due to space constraints, we provide a highly condensed summary of the most related work. Even prior to the emergence of large pretrained LMs (PLMs), most representation models followed the distributional hypothe- 6 Conclusion sis (Harris, 1954) and exploited the co-occurrence statistics of words/phrases/sentences in large cor- We proposed Mirror-BERT, a simple, fast, selfpora (Mikolov et al., 2013a,b; Pennington et al., supervised, and highly effective approach that trans2014; Kiros et al., 2015; Hill et al., 2016; Lo- forms large pretrained masked language models (MLMs) into universal lexical and sentence engeswaran and Lee, 2018). Recently, DeCLUTR (Giorgi et al., 2021) follows the distributional hy- coders within a minute, and without any external pothesis and formulates sentence embedding train- supervision. Mirror-BERT, based on simple unsupervised data augmentation techniques, demoning as a contrastive learning task where span pairs sampled from the same document are treated as pos- strates surprisingly strong performance in (wordlevel and sentence-level) semantic similarity tasks, itive pairs. V"
2021.emnlp-main.109,J15-4004,1,0.738393,"BERT: Training Resources. For finetuning (general-domain) lexical representations, we use the top 10k most frequent words in each language. For biomedical name representations, we randomly sample 10k names from the UMLS. In sentence-level tasks, for STS, we sample 10k sentences (without labels) from the training set of the STS Benchmark; for Spanish, Arabic and Russian, we sample 10k sentences from the WikiMatrix dataset (Schwenk et al., 2021). For QNLI, we sample 10k sentences from its training set. Training Setup and Details. The hyperparameters of word-level models are tuned on SimLex-999 (Hill et al., 2015); biomedical models are tuned on COMETA (zero-shot-general split). Sentencelevel models are tuned on the dev set of STS-b. τ in Eq. (1) is 0.04 (biomedical and sentence-level models); 0.2 (word-level). Dropout rate p is 0.1. Sentence-level models use a random span masking 3 github.com/deepmipt/deepPavlovEval We follow the setup of Li et al. (2020) and adapt QNLI to an unsupervised task by computing the AUC scores (on the development set, ≈5.4k pairs) using 0/1 labels and cosine similarity scores of QA embeddings. 4 lang.→ EN FR ET AR ZH RU ES PL avg. fastText .528 .560 .447 .409 .428 .435 .488"
2021.emnlp-main.109,P19-1070,1,0.91152,"Missing"
2021.emnlp-main.109,2021.acl-long.197,0,0.292243,"Missing"
2021.emnlp-main.109,2021.eacl-main.270,1,0.773582,"Missing"
2021.emnlp-main.109,P19-1580,0,0.029665,"Probing the impact of dropout. Table 7: Ablation study: (i) replacing dropout with drophead; (ii) the synergistic effect of dropout and random span masking in the English STS tasks. v1 == v¯ 1 controlled dropout controlled dropout dropout( v1 ) == dropout( v¯ 1 ) Figure 6: Under controlled dropout, if two strings are identical, they will have an identical set of dropout masks throughout the encoding process. other augmentation types work as well? Recent work points out that pretrained MLM are heavily overparameterised and most Transformer heads can be pruned without hurting task performance (Voita et al., 2019; Kovaleva et al., 2019; Michel et al., 2019). Zhou et al. (2020) propose a drophead method: it randomly prunes attention heads at MLM training as a regularisation step. We thus evaluate a variant of Mirror-BERT where the dropout layers are replaced with such dropheads:12 this results in even stronger STS performance, cf. Tab. 7. In short, this hints that the Mirror-BERT framework might benefit from other data and feature augmentation techniques in future work.13 Regularisation or Augmentation? (Tab. 8). When using dropout, is it possible that we are simply observing the effect of adding/remov"
2021.emnlp-main.109,2020.cl-4.5,1,0.887829,"Missing"
2021.emnlp-main.109,2021.acl-long.410,1,0.821601,"Missing"
2021.emnlp-main.109,2020.emnlp-main.586,1,0.883623,"Missing"
2021.emnlp-main.571,D18-1025,0,0.0242468,"system performance. The SemEval7158 2021 shared task MCL-WiC does focus on crosslingual WiC, but covers only five high-resource languages from three language families (English, French, Chinese, Arabic, Russian). Both XL-WiC and MCL-WiC mainly focus on common words and do not include less frequent concepts (e.g., named entities). Further, their language coverage and data availability are heavily skewed towards Indo-European languages. There are several other ‘non-WiC’ datasets designed to evaluate cross-lingual context-aware lexical representations. Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018) challenges a model to predict graded similarity of crosslingual word pairs given sentential context, one in each language. In the Bilingual Token-level Sense Retrieval (BTSR) task (Liu et al., 2019), given a query word in a source language context, a system must retrieve a meaning-equivalent target language word within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as gene"
2021.emnlp-main.571,W16-2501,1,0.796206,"d within a target language context.10 However, both BCWS and BTSR are again very restricted in terms of language coverage: BCWS covers only one language pair (EN-ZH), while BTSR contains two pairs (EN-ZH/ES). Further, they provide only test data: as such, they can merely be used as general intrinsic probes for pretrained models, but cannot support fine-tuning experiments and cannot fully expose the relevance of information available in pretrained models for downstream applications. This is problematic as intrinsic tasks in general do not necessarily correlate well with downstream performance (Chiu et al., 2016; Glavaš et al., 2019). AM2 I C O vs. Entity Linking. Our work is related to the entity linking (EL) task (Rao et al., 2013; Cornolti et al., 2013; Shen et al., 2014) similarly to how the original WiC (based on WordNet knowledge) is related to WSD. EL systems must map entities in context to a predefined knowledge base (KB). While WSD relies on the WordNet sense inventory, the EL task focuses on KBs such as Wikipedia and DBPedia. When each entity mention is mapped to a unique Wiki page, this procedure is termed wikification (Mihalcea and Csomai, 2007). The cross-lingual wikification task (Ji et"
2021.emnlp-main.571,2020.acl-main.747,0,0.527258,"e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference"
2021.emnlp-main.571,N19-1423,0,0.168088,"lable in different languages, i.e., it does not support cross-lingual assessments. Further, 4) the current WiC datasets offer low human upper bounds and inflated (even superhuman) performance for some languages.1 This is due to superficial cues where 5) many examples in the current WiC datasets can be resolved relying either on the target word alone without any context or on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 P"
2021.emnlp-main.571,P19-1070,1,0.872566,"Missing"
2021.emnlp-main.571,N18-2017,0,0.037706,"Missing"
2021.emnlp-main.571,N18-1170,0,0.0535351,"Missing"
2021.emnlp-main.571,D17-1215,0,0.0513424,"Missing"
2021.emnlp-main.571,K19-1004,1,0.870059,"Missing"
2021.emnlp-main.571,Q14-1019,0,0.0357826,"r on the context alone, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologi"
2021.emnlp-main.571,S13-2040,0,0.0330702,"emonstrate the challenging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide wh"
2021.emnlp-main.571,D12-1128,0,0.0920929,"Missing"
2021.emnlp-main.571,P19-1459,0,0.0209984,"ty of AM2 I C O. For each dataset, 7153 we recruit two annotators who each validate a random sample of 100 examples, where 50 examples are shared between the two samples and are used to compute inter-rater agreement.3 2.2 2.3 Adversarial Examples Another requirement is assessing to which extent models can grasp the meaning of a target word based on the (complex) interaction with its context. However, recently it was shown that SotA pretrained LMs exploit superficial cues while solving language understanding tasks due to spurious correlations seeping into the datasets (Gururangan et al., 2018; Niven and Kao, 2019). This hinders generalizations beyond the particular datasets and makes the models brittle to minor changes in the 3 The annotators were recruited via two crowdsourcing platforms, Prolific and Proz, depending on target language coverage. The annotators were native speakers of the target language, fluent in English, and with an undergraduate degree. 4 E.g., ‘China’ can be linked to the page ‘Republic of China (1912–1949)’ and to the page ‘Empire of China (1915–1916)’. XL-WiC MCL-WiC AM2 I C O 7,466 7,466 4,130 4,130 17 14,510 1,676 7,255 1,201 22.7 3600 2000 2766 2072 26.13 13,074 8,570 9,868 8"
2021.emnlp-main.571,N19-1128,0,0.042815,"ent in other datasets, namely the Georgian alphabet and the Bengali script (a Northern Indian abugida), for a total of 8 distinct scripts. 3 Experimental Setup We now establish a series of baselines on AM2 I C O to measure the gap between current SotA models and human performance. XLM-R6 (Conneau et al., 2020), available in the HuggingFace repository (Wolf et al., 2020). Classification. Given two contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize th"
2021.emnlp-main.571,2020.emnlp-main.185,1,0.926148,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,J19-3005,1,0.863194,"Missing"
2021.emnlp-main.571,E17-1010,0,0.0897936,"e, which eludes evaluation honing in on the interplay between target words and their corresponding contexts. In order to address these limitations and provide Pretrained language models (LMs) such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) offer a natural way to distinguish different word meanings in context without performing explicit sense disambiguation. This property of “meaning contextualization” is typically evaluated either via standard entity linking (Rao et al., 2013; Shen et al., 2014) and Word Sense Disambiguation (WSD) tasks (Navigli, 2009; Moro et al., 2014; 1 Raganato et al., 2017) or, recently, via the Word-inIn turn, this might give a false impression that some lanContext (WiC) evaluation paradigm (Pilehvar and guages in the task are ’solved’ by the current pretrained LMs. 7151 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7151–7162 c November 7–11, 2021. 2021 Association for Computational Linguistics a more comprehensive evaluation framework, we present AM2 I C O (Adversarial and Multilingual Meaning in Context), a novel multilingual and cross-lingual WiC task and resource. It covers a typologically diverse set of 15 la"
2021.emnlp-main.571,2020.emnlp-main.584,0,0.272024,"contextualized representations ei,src and ei,trg for a pair of target words, two setups to make prediction are considered: the first, metric-based, is a non-parametric setup. In particular, we follow Pilehvar and CamachoCollados (2019) and score the distance δ between the representations via cosine similarity. A threshold t from the development set is set via grid search across 0.02 intervals in the [0, 1] interval. Therefore, if δ(ei,src , ei,trg ) ≥ t the pair is classified as negative, and positive otherwise. On the other hand, the fine-tuning setup is parametric: following Raganato et al. (2020), we train a logistic regression classifier that takes the concatenation of the contextualized representations [ei,src ⊕ ei,trg ] as input.7 The entire model (both the encoder and the classifier) is then fine-tuned to minimize the cross-entropy loss of the training set examples with Adam (Kingma and Ba, 2015). We perform grid search for the learning rate in [5e−6, 1e−5, 3e−5], and train for 20 epochs selecting the checkpoint with the best performance on the dev set. Cross-lingual Transfer. In addition to supervised learning, we also carry out cross-lingual transfer experiments where data split"
2021.emnlp-main.571,D19-1077,0,0.0167166,"6.3. However, this is still insufficient to equalize performances across the board, as the latter group of languages continues to lag behind: between DE and UR remains a gap of 7.2 points. We speculate that the reason behind this asymmetry is the fact that in addition to being resource-poor, UR, KK, BN, and KA are also typologically distant from languages where most of the examples are concentrated. Overall, these findings suggest that leveraging multiple sources is better than a single one by virtue of the transfer capabilities of massively multilingual encoders, as previously demonstrated (Wu and Dredze, 2019; Ponti et al., 2021). Few-shot Transfer. To study the differences between training on `s and `t with controlled train data size, we plot the model performance on two target languages (RU and JA) as a function of the amount of available examples across different transfer conditions in Figure 2. Comparing supervised learning (based on target language data) with zeroshot learning (based on DE data), it emerges how the former is always superior if the number of examples is the same. However, zero-shot learning may eventually surpass the peak performance of supervised learning by taking advantage"
2021.emnlp-main.571,2020.lrec-1.723,0,0.0355104,"ging nature of AM2 I C O. The results reveal that current SotA pretrained encoders substantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the sam"
2021.emnlp-main.571,P18-1072,1,0.894256,"Missing"
2021.emnlp-main.571,N16-1072,0,0.164299,"stantially lag behind human performance, and the largest gaps are observed for low-resource languages and languages dissimilar to English. 1 Introduction Camacho-Collados, 2019; Raganato et al., 2020). Although monolingual evaluation in English is still predominant, a need has been recognized to construct similar resources for other languages to support cross-lingual evaluation and model diagnostics. This includes multilingual and crosslingual WSD benchmarks (Navigli and Ponzetto, 2012; Navigli et al., 2013; Scarlini et al., 2020; Barba et al., 2020, inter alia), cross-lingual entity linking (Tsai and Roth, 2016; Raiman and Raiman, 2018; Upadhyay et al., 2018) and, most recently, multilingual WiC (termed XL-WiC) spanning 12 languages (Raganato et al., 2020). This most recent WiC evaluation approach is particularly attractive as 1) it bypasses the dependence on modeling predefined ontologies (entity linking) and explicit sense inventories (WSD), and 2) it is framed as a simple binary classification task: for a target word w appearing in two different contexts c1 and c2 , the system must decide whether w conveys the same meaning in both contexts, or not. However, the current WiC evaluation still allows"
2021.emnlp-main.571,D18-1270,0,0.0460866,"Missing"
2021.emnlp-main.571,2020.cl-4.5,1,0.894831,"Missing"
2021.emnlp-main.591,H90-1021,0,0.609383,"Missing"
2021.emnlp-main.591,2020.nlp4convai-1.5,1,0.883147,"Missing"
2021.emnlp-main.591,2020.findings-emnlp.196,1,0.857812,"Missing"
2021.emnlp-main.591,D18-2029,0,0.0690498,"Missing"
2021.emnlp-main.591,W19-4330,0,0.333444,"g, pages 7468–7475 c November 7–11, 2021. 2021 Association for Computational Linguistics evaluation resource for ID from spoken data. It originates from the use of a commercial voice assistant and real-life industry needs: it covers 14 intents in the banking domain in 14 different language varieties, making it the most comprehensive multilingual ID dataset to date. 2) We present a systematic evaluation and comparison of current state-of-the-art multilingual and cross-lingual ID models, which rely on machine translation and current cutting-edge multilingual sentence encoders, multilingual USE (Chidambaram et al., 2019) and LaBSE (Feng et al., 2020). 3) We provide additional analyses to further profile the potential and current ID gaps in multilingual voice-based contexts, including augmentation with data from a similar domain, target-only versus multilingual training, and aggregations of n-best ASR hypotheses. Our results demonstrate that strong ID results can be achieved for all languages represented in MI N DS-14, but we also indicate the crucial importance of in-domain model fine-tuning and fewshot learning, reporting strong gains over zero-shot transfer models. In hope to motivate and inspire further wo"
2021.emnlp-main.591,D19-1131,0,0.0366961,"Missing"
2021.emnlp-main.591,N19-1423,0,0.106766,"other content that might contain private c) Slavic: Russian (RU), Polish (PL), and Czech or sensitive information. (CS); and d) Asian languages: Korean (KO) and 4 The dataset is open-sourced to the research community Chinese (ZH). The choice of languages was driven to facilitate the progress of multilingual NLU research, there by (a) the number of native speakers and (b) the are no IP-related issues. 7469 3 Multilingual ID: Methodology tilingual encoder, and supports 109 languages. A standard transfer learning paradigm (Ruder et al., 2019) fine-tunes a pretrained language model such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019a) on the annotated task data. For the intent classification task in particular, Casanueva et al. (2020) have recently shown that full finetuning of the large pretrained model is not needed at all. In contrast, they propose a more efficient feature-based approach to intent detection. Here, fixed universal sentence encoders such as USE (Cer et al., 2018; Chidambaram et al., 2019) or ConveRT (Henderson et al., 2020) are used “off-the-shelf” to encode utterances, and a standard multi-layer perceptron (MLP) classifier is then learnt on top of the sentence encodings. Ca"
2021.emnlp-main.591,2020.emnlp-main.650,0,0.0160236,"particular to support a multitude of new dialogue tasks and domains is a challenging, timeconsuming, and resource-intensive process (Wen et al., 2017; Rastogi et al., 2019). This problem is further exacerbated in multilingual setups: it is extremely expensive to annotate sufficient task data in each of more than 7,000 languages (Bellomaria et al., 2019; Xu et al., 2020). As a consequence, the current ID work has been largely constrained only to English, and standard ID benchmarks also exist only in English (Hemphill et al., 1990; Larson et al., 2019; Liu et al., 2019b; Casanueva et al., 2020; Larson et al., 2020, inter alia). The need to widen the reach of dialogue technology to other languages has been recognised only recently, and thus even text-based multilingual ID datasets are still few and far between: Schuster et al. (2019) provide NLU data in three languages (English, Spanish, Thai), while a more recent MultiATIS++ dataset (Xu et al., 2020) manually translates the ATIS dataset (Hemphill et al., 1990) from English to 8 target languages, extending the work of Upadhyay et al. (2018) which translated portions of the English ATIS data to Hindi and Turkish.2 Despite these efforts, there are still p"
2021.emnlp-main.591,D19-1224,0,0.041406,"Missing"
2021.emnlp-main.591,2021.naacl-main.201,0,0.0244688,"Both authors equally contributed to this work. For instance, in the banking domain utterances referring to cash withdrawal or currency exchange rates should be classified to the respective intent classes (Casanueva et al., 2020). An error in intent detection is typically the first point of failure for any task-oriented dialogue system. 1 2 Further, reaching beyond the English language, other languages often exhibit different typological (e.g., morphosyntactic) and lexical properties, potentially requiring additional language-specific adaptations of English-trained models (Ponti et al., 2019; Hedderich et al., 2021). 7468 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7468–7475 c November 7–11, 2021. 2021 Association for Computational Linguistics evaluation resource for ID from spoken data. It originates from the use of a commercial voice assistant and real-life industry needs: it covers 14 intents in the banking domain in 14 different language varieties, making it the most comprehensive multilingual ID dataset to date. 2) We present a systematic evaluation and comparison of current state-of-the-art multilingual and cross-lingual ID models, which rely on mac"
2021.emnlp-main.591,P19-1441,0,0.165458,"systems in general and intent detectors in particular to support a multitude of new dialogue tasks and domains is a challenging, timeconsuming, and resource-intensive process (Wen et al., 2017; Rastogi et al., 2019). This problem is further exacerbated in multilingual setups: it is extremely expensive to annotate sufficient task data in each of more than 7,000 languages (Bellomaria et al., 2019; Xu et al., 2020). As a consequence, the current ID work has been largely constrained only to English, and standard ID benchmarks also exist only in English (Hemphill et al., 1990; Larson et al., 2019; Liu et al., 2019b; Casanueva et al., 2020; Larson et al., 2020, inter alia). The need to widen the reach of dialogue technology to other languages has been recognised only recently, and thus even text-based multilingual ID datasets are still few and far between: Schuster et al. (2019) provide NLU data in three languages (English, Spanish, Thai), while a more recent MultiATIS++ dataset (Xu et al., 2020) manually translates the ATIS dataset (Hemphill et al., 1990) from English to 8 target languages, extending the work of Upadhyay et al. (2018) which translated portions of the English ATIS data to Hindi and Turk"
2021.emnlp-main.591,2020.emnlp-main.185,1,0.895323,"Missing"
2021.emnlp-main.591,J19-3005,1,0.854785,"Missing"
2021.emnlp-main.591,D19-1410,0,0.0168103,"-the-art multilingual sentence encoders, but remind the reader that decoupling MLP from the encoder allows for a wider exploration of other available multilingual sentence encoders (Reimers and Gurevych, 2020; Litschko et al., 2021, inter alia). In what follows, we provide only brief descriptions of each encoder in our evaluation; for more details we refer the reader to the original work. mUSE (Yang et al., 2020) is a multilingual version of the Universal Sentence Encoder (USE) model for English (Cer et al., 2018). It relies on a standard dual-encoder neural framework (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020), features 16 languages, and learns a shared cross-lingual semantic space via translationbridging tasks (Chidambaram et al., 2019). LaBSE. Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) (Devlin et al., 2019) using a dual-encoder framework (Yang et al., 2019) with larger embedding capacity (i.e., it provides a shared multilingual vocabulary of 500k subwords).5 LaBSE is the current state-of-the-art mul5 In addition to the multi-task training objective of mUSE, We keep pretrained sentence encoders fixed durin"
2021.emnlp-main.591,2020.emnlp-main.365,0,0.0149829,"e feature-based approach to intent classification yields performance on-par with the full-model finetuning, while offering improved training efficiency. Therefore, due to the large number of executed experiments and comparisons in this work, and preliminary results which corroborated the findings from prior work (Casanueva et al., 2020), we opt for this efficient approach to ID. We evaluate two widely used state-of-the-art multilingual sentence encoders, but remind the reader that decoupling MLP from the encoder allows for a wider exploration of other available multilingual sentence encoders (Reimers and Gurevych, 2020; Litschko et al., 2021, inter alia). In what follows, we provide only brief descriptions of each encoder in our evaluation; for more details we refer the reader to the original work. mUSE (Yang et al., 2020) is a multilingual version of the Universal Sentence Encoder (USE) model for English (Cer et al., 2018). It relies on a standard dual-encoder neural framework (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020), features 16 languages, and learns a shared cross-lingual semantic space via translationbridging tasks (Chidambaram et al., 2019). LaBSE. Language-agnostic BER"
2021.emnlp-main.591,E17-1042,1,0.872902,"Missing"
2021.emnlp-main.591,2020.emnlp-main.410,0,0.017637,"orean (KO) and 4 The dataset is open-sourced to the research community Chinese (ZH). The choice of languages was driven to facilitate the progress of multilingual NLU research, there by (a) the number of native speakers and (b) the are no IP-related issues. 7469 3 Multilingual ID: Methodology tilingual encoder, and supports 109 languages. A standard transfer learning paradigm (Ruder et al., 2019) fine-tunes a pretrained language model such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019a) on the annotated task data. For the intent classification task in particular, Casanueva et al. (2020) have recently shown that full finetuning of the large pretrained model is not needed at all. In contrast, they propose a more efficient feature-based approach to intent detection. Here, fixed universal sentence encoders such as USE (Cer et al., 2018; Chidambaram et al., 2019) or ConveRT (Henderson et al., 2020) are used “off-the-shelf” to encode utterances, and a standard multi-layer perceptron (MLP) classifier is then learnt on top of the sentence encodings. Casanueva et al. (2020) demonstrate that the feature-based approach to intent classification yields performance on-par with the full-mo"
2021.emnlp-main.591,2020.acl-demos.12,0,0.0376615,"comparisons in this work, and preliminary results which corroborated the findings from prior work (Casanueva et al., 2020), we opt for this efficient approach to ID. We evaluate two widely used state-of-the-art multilingual sentence encoders, but remind the reader that decoupling MLP from the encoder allows for a wider exploration of other available multilingual sentence encoders (Reimers and Gurevych, 2020; Litschko et al., 2021, inter alia). In what follows, we provide only brief descriptions of each encoder in our evaluation; for more details we refer the reader to the original work. mUSE (Yang et al., 2020) is a multilingual version of the Universal Sentence Encoder (USE) model for English (Cer et al., 2018). It relies on a standard dual-encoder neural framework (Henderson et al., 2019; Reimers and Gurevych, 2019; Humeau et al., 2020), features 16 languages, and learns a shared cross-lingual semantic space via translationbridging tasks (Chidambaram et al., 2019). LaBSE. Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) (Devlin et al., 2019) using a dual-encoder framework (Yang et al., 2019) with larger embedding capacity (i.e., it p"
2021.emnlp-main.591,N19-5004,0,0.0228504,"xtremely long utterances. We also manually removed all personal names and other content that might contain private c) Slavic: Russian (RU), Polish (PL), and Czech or sensitive information. (CS); and d) Asian languages: Korean (KO) and 4 The dataset is open-sourced to the research community Chinese (ZH). The choice of languages was driven to facilitate the progress of multilingual NLU research, there by (a) the number of native speakers and (b) the are no IP-related issues. 7469 3 Multilingual ID: Methodology tilingual encoder, and supports 109 languages. A standard transfer learning paradigm (Ruder et al., 2019) fine-tunes a pretrained language model such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019a) on the annotated task data. For the intent classification task in particular, Casanueva et al. (2020) have recently shown that full finetuning of the large pretrained model is not needed at all. In contrast, they propose a more efficient feature-based approach to intent detection. Here, fixed universal sentence encoders such as USE (Cer et al., 2018; Chidambaram et al., 2019) or ConveRT (Henderson et al., 2020) are used “off-the-shelf” to encode utterances, and a standard multi-layer perce"
2021.emnlp-main.591,N19-1380,0,0.0228374,"ngual setups: it is extremely expensive to annotate sufficient task data in each of more than 7,000 languages (Bellomaria et al., 2019; Xu et al., 2020). As a consequence, the current ID work has been largely constrained only to English, and standard ID benchmarks also exist only in English (Hemphill et al., 1990; Larson et al., 2019; Liu et al., 2019b; Casanueva et al., 2020; Larson et al., 2020, inter alia). The need to widen the reach of dialogue technology to other languages has been recognised only recently, and thus even text-based multilingual ID datasets are still few and far between: Schuster et al. (2019) provide NLU data in three languages (English, Spanish, Thai), while a more recent MultiATIS++ dataset (Xu et al., 2020) manually translates the ATIS dataset (Hemphill et al., 1990) from English to 8 target languages, extending the work of Upadhyay et al. (2018) which translated portions of the English ATIS data to Hindi and Turkish.2 Despite these efforts, there are still prominent gaps remaining: 1) a large number of (even major) languages is still uncovered, 2) there are no multilingual data for specialized and well-defined domains such as e-banking, and 3) most importantly, all intent dete"
2021.emnlp-main.800,2021.findings-emnlp.410,1,0.87244,"Missing"
2021.emnlp-main.800,2020.acl-main.421,1,0.820989,"The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix for the target language (Artetxe et al., 2020) or adds new tokens to the pretrained vocabulary. While the former has only been applied to high-resource languages, the latter approaches have been limited to languages with seen scripts (Chau et al., 2020; Müller et al., 2021) and large pretraining corpora (Wang et al., 2020). Another line of work adapts the embedding layer as well as other layers of the model via adapters (Pfeiffer et al., 2020b; Üstün et al., 2020). Such methods, however, cannot be directly applied to languages with unseen scripts. In this work, we first empirically verify that the original tokenizer and the original embed"
2021.emnlp-main.800,D19-1165,0,0.0261306,"ained multilingual model such as mBERT or XLM-R is 1) fine-tuning it on labelled data of a downstream task in a source language and then 2) applying it directly to perform inference in a target language (Hu et al., 2020). However, as the model must balance between many languages in its representation space, it is not suited to excel at a specific language at inference time without further adaptation (Pfeiffer et al., 2020b). Adapters for Cross-lingual Transfer. Adapterbased approaches have been proposed as a remedy (Rebuffi et al., 2017, 2018; Houlsby et al., 2019; Stickland and Murray, 2019; Bapna and Firat, 2019; Pfeiffer et al., 2020a, 2021). In the crosslingual setups, the idea is to increase the multilingual model capacity by storing language-specific knowledge of each language in dedicated parameters (Pfeiffer et al., 2020b; Vidoni et al., 2020). We start from MAD-X (Pfeiffer et al., 2020b), a state-of-the-art adapter-based framework for crosslingual transfer. For completeness, we provide a brief overview of the framework in what follows. MAD-X comprises three adapter types: language, task, and invertible adapters; this enables learning language and task-specific transformations in a modular and"
2021.emnlp-main.800,2020.findings-emnlp.118,0,0.510962,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.367,0,0.255907,"embedding matrix X whereas F stores token-specific information. G only needs to be pretrained once and can be used and fine-tuned for every new language. To this end, we simply learn new low-dimensional embeddings 0 0 F0 ∈ R|V |×D with the pretraining task, which are up-projected with G and fed to the model. MFC KM EANS -∗. When C > 1, each token is associated with one of C up-projection matrices. Grouping tokens and using a separate up-projection matrix per group may help balance sharing information across typologically similar languages with learning a robust representation for each token (Chung et al., 2020). We propose two approaches to automatically learn such a clustering. In our first, pipeline-based approach, we first cluster X into C clusters using KMeans. For each cluster, we then factorize the subset of embeddings Xc associated with the c-th cluster separately using Semi-NMF equivalently as for MF1 -∗. For a new language, we learn new low-dim em0 0 beddings F0 ∈ R|V |×D and a randomly initialized 0 matrix Z ∈ R|V |×C , which allows us to compute 0 the cluster assignment matrix I0 ∈ R|V |×C . Specifically, for token v, we obtain its cluster assignment as arg max of z0v,· . As arg max is no"
2021.emnlp-main.800,2020.emnlp-main.358,0,0.0272702,"ddings X0 ∈ R|V |×D for all V 0 vocabulary items where D is the dimensionality of the existing embeddings X ∈ R|V |×D , and only initialize special tokens (e.g. [CLS], [SEP]) with their pretrained representations. We train the new embeddings of the X0 with the pretraining task. This approach, termed EL- RAND, was proposed by Artetxe et al. (2020): they show that it allows learning aligned representations for a new language but only evaluate on high-resource languages. The shared special tokens allow the model to access a minimum amount of lexical information, which can be useful for transfer (Dufter and Schütze, 2020). Beyond this, this approach leverages knowledge from the existing embedding matrix only implicitly to the extent that the higher-level hidden representations are aligned to the lexical representations. 3.2 Initialization with Lexical Overlap 0 , and us denote this vocabulary subset with Vlex 0 0 0 Vrand = V  Vlex . In particular, we initialize the embeddings of all lexically overlapping tokens 0 with their pretrained representaX0lex from Vlex tions from the original matrix X, while the tokens 0 from Vrand receive randomly initialized embeddings X0rand . We then fine-tune all target language"
2021.emnlp-main.800,P19-1070,1,0.848463,"able parameters and yield more efficient model adaptation. Our approach, based on matrix factorization and language clusters, extracts relevant information from the pretrained embedding matrix. 4) We show that our methods outperform previous approaches with both resourcerich and resource-poor languages. They substantially reduce the gap between random and lexicallyoverlapping initialization, enabling better model adaption to unseen scripts. The code for this work is released at github.com/ Adapter-Hub/UNKs_everywhere. surpassed (static) cross-lingual word embedding spaces (Ruder et al., 2019; Glavas et al., 2019) as the state-of-the-art paradigm for cross-lingual transfer in NLP (Pires et al., 2019; Wu and Dredze, 2019; Wu et al., 2020; Hu et al., 2020; K et al., 2020). However, recent studies have also indicated that even current state-of-the-art models such as XLM-R (Large) still do not yield reasonable transfer performance across a large number of target languages (Hu et al., 2020). The largest drops are reported for resource-poor target languages (Lauscher et al., 2020), and (even more dramatically) for languages not covered at all during pretraining (Pfeiffer et al., 2020b). Standard Cross-Lingua"
2021.emnlp-main.800,2021.eacl-main.270,1,0.788219,"default implementation provided by Bauckhage et al. (2011).9 We train for 3,000 update steps and leverage the corresponding matrices F and G as initialization for the new vocabulary. We choose the reduced embedding dimensionality D0 = 100. F is only used when initializing the (lower-dimensional) embedding matrix with lexically overlapping representations. transfer. We train all the models with a batch size of 16 on high resource languages. For NER we use learning rates 2e − 5 and 1e − 4 for full fine-tuning and adapter-based training, respectively. For DP, we use a transformer-based variant (Glavas and Vulic, 2021) of the standard deep biaffine attention dependency parser (Dozat and Manning, 2017) and train with learning rates 2e − 5 and 5e − 4 for full fine-tuning and adapter-based training respectively. Masked Language Modeling. For MLM pretraining we leverage the entire Wikipedia corpus of the respective language. We train for 200 epochs or ∼100k update steps, depending on the corpus size. The batch size is 64; the learning rate is 1e − 4. 5 Results and Discussion The main results are summarised in Table 3a for NER, and in Table 3b for DP. First, our novel MAD-X 2.0 considerably outperforms the MADX"
2021.emnlp-main.800,2020.emnlp-main.363,1,0.903259,"Missing"
2021.emnlp-main.800,2021.naacl-main.38,0,0.0340559,"nguage vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a new embedding matrix fo"
2021.emnlp-main.800,L16-1262,0,0.0926263,"Missing"
2021.emnlp-main.800,2020.lrec-1.497,0,0.0513677,"Missing"
2021.emnlp-main.800,P17-1178,0,0.0893718,"Missing"
2021.emnlp-main.800,2020.emnlp-demos.7,1,0.921743,"Figure 2. Pearson’s ρ correlation scores between the lexical overlap and 2 An alternative approach based on transliteration (Müller proportion of UNKs (see Table 1) and NER perforet al., 2021) side-steps script adaptation but relies on languagespecific heuristics, which are not available for most languages. mance are 0.443 and −0.798, respectively. 10188 Method EL- RAND EL- LEX MFC ∗ - RAND MFC ∗ - LEX Special tokens X X X X Lexical overlap Latent semantic concepts Language clusters New params # of new params Reference X X X0 X0 F0 , I0 F0 , I0 7.68M 7.68M 1M + C·10k 1M + C·10k Artetxe et al. (2020) Chau et al. (2020); Wang et al. (2020) Ours Ours X X X X Table 2: Overview of our methods and related approaches together with the pretrained knowledge they utilize. We calculate the number of new parameters per language with V 0 = 10k, D = 768, and D0 = 100. We do not include up-projection matrices G as these are learned only once and make up a comparatively small number of parameters. Recent approaches such as invertible adapters (Pfeiffer et al., 2020b) that adapt embeddings in the pretrained multilingual vocabulary may be able to deal with lesser degrees of lexical overlap. Still, they ca"
2021.emnlp-main.800,2020.emnlp-main.617,1,0.343661,"en mBERT’s and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can yield improvements for low-resource languages written in scripts covered by the pretrained model. Amharic የእግዚአብሔርን የሚሰጡዋቸውን የእግዚአብሔር ቤተክርስቲያን ኢትዮጵያውያን Tibetan Divehi ཉྩཡོནིརོདྷཨེཝ ެގާޔްއިރޫހްމުޖްލުސީއަރ བཱདཱིམཧཱཤྲམཎ ެވެއަވަންނަގިއަޑަވެނެދ བཱདཱིམཧཱཤྲམཎ އޓްސޮއ ި ޯރތާއ ު ަްސިޓިއ ཧཱུཕཊསྭཱཧཱ ލސީއަރ ު ްާޔްއިރޫހްމުޖ ཧཱུཧཱུཕཊ ާވިއަފްނެގިއަ ަޑވި ބިލ Figure 1: Example tokens of unseen scripts. training corpora (Pfeiffer et al., 2020b; Müller et al., 2021; Ansell et al., 2021). The most extreme challenge is dealing with unseen languages with unseen በእግዚአብሔር བྷྱཿཤཱནྟིཾ ެވެންނުތާރަފްތާނަކ scripts (i.e., the scripts are notުޖrepresented in the እግዚአብሔርን བཻཌཱུརྱའི ެވެ ކެއާޔްއިރޫހްމ pretraining data; see Figure 1), where the pretrained ከእግዚአብሔር ེདྷརྨཱཧེཏ ިވެންނެގިއަޑަވި ބިލ ེཏུནྟེ entirely ކްނާޔަބ ް ީނަވިއަފare used models የመጀመሪያውን are bound toཝཱཧfail ifޮށthey ትምህርታቸውን ྲེའུའི further model އަޑަވެވްނަފުއadaptation. ިަގ off-the-shelf withoutསྤany Existing work focuses on the embedding layer and learns either a n"
2021.emnlp-main.800,P19-1493,0,0.0619527,"Missing"
2021.emnlp-main.800,P19-1015,0,0.109665,"Missing"
2021.emnlp-main.800,2021.acl-long.243,1,0.644693,"Massachusetts Encyclopedia Pennsylvania Jacksonville Turkmenistan Melastomataceae munisipalidad Internasional internasional International establecimiento vicepresidente Internacional internacional Independencia University Therefore suffering existence practice languages language formula disease control government Chinese govern system ation International Bangladesh wikipedia Australia Zimbabwe Table 5: Longest lexically overlapping (sub)words. model (e.g. Georgian (ka), Urdu (ur), and Hindi (hi)), the proposed methods outperform MAD-X 2.0 for all tasks. This is in line with contemporary work (Rust et al., 2021), which emphasizes the importance of tokenizer quality for the downstream task. Consequently, for unseen languages with under-represented scripts, the performance gains are even larger, e.g., we see large improvements for Min Dong (cdo), Mingrelian (xmf), and Sindhi (sd). For unseen languages with the Latin script, our methods perform competitively (e.g. Maori (mi), Ilokano (ilo), Guarani (gn), and Wolof (wo)): this empirically confirms that the Latin script is adequately represented in the original vocabulary. The largest gains are achieved for languages with unseen scripts (e.g. Amharic (am)"
2021.emnlp-main.800,P18-1072,1,0.88992,"Missing"
2021.emnlp-main.800,2020.emnlp-main.180,0,0.115408,"Missing"
2021.emnlp-main.88,2020.nlp4convai-1.5,1,0.864046,"Missing"
2021.emnlp-main.88,D18-2029,0,0.0604642,"Missing"
2021.emnlp-main.88,W19-4330,0,0.0141885,"2018; Wieting and Gimpel, to store a wealth of semantic knowledge. 2018) into a sentence encoder which excels at senHowever, 1) they are not effective as sentence similarity and retrieval tasks (Marelli et al., tence encoders when used off-the-shelf, and 2014; Cer et al., 2017). This transformation pro2) thus typically lag behind conversationally pretrained (e.g., via response selection) encess supports the creation of other similar universal coders on conversational tasks such as intent sentence encoders in monolingual and multilingual detection (ID). In this work, we propose C ON settings (Chidambaram et al., 2019; Wieting et al., V F I T, a simple and efficient two-stage proce2020; Feng et al., 2020), and is typically based on dure which turns any pretrained LM into a dual-encoder architectures. universal conversational encoder (after Stage 1 Another parallel research thread aims at learning C ONV F I T-ing) and task-specialised sentence conversational encoders: it validates the benefits encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not of masked language modeling (MLM) pretraining required, and that LMs can be quickly transon naturally conversational data (W"
2021.emnlp-main.88,2021.acl-long.295,0,0.0835563,"Missing"
2021.emnlp-main.88,2021.emnlp-main.552,0,0.0746489,"Missing"
2021.emnlp-main.88,2020.coling-main.559,1,0.928607,"RoBERTa in two separate stages via dual-encoder networks (“zoomed-in” parts; grey blocks denote tunable parameters), and performs intent detection with the C ONV F I T-ed models via similarity-based inference. Stage 1 (S1): adaptive conversational fine-tuning, §2.1; Stage 2 (S2): task-tailored conversational fine-tuning (for intent detection), §2.2. Dashed lines denote baseline/ablation variants which skip one of the two stages: (i) we can directly task-tune the sentence encoder with the task data (Stage 2) without running Stage 1, or (ii) we can skip Stage 2, and similar to Casanueva et al. (2020), learn an MLP classifier on top of the conversational representations from Stage 1. (a) RoBERTa (no fine-tuning) (b) RoBERTa (after S1) (c) RoBERTa (after S1 and S2) Figure 2: t-SNE plots (van der Maaten and Hinton, 2012) of encoded utterances from the ID test set of BANKING 77 (i.e., all examples are effectively unseen by the encoder models at training) associated with a selection of 12 intents, demonstrating the effects of gradual “representation specialisation funnel”. The encoded utterances are created via mean-pooling based on (a) the original RoBERTa LM; (b) RoBERTa after Stage 1 (i.e.,"
2021.emnlp-main.88,2020.acl-main.740,0,0.051893,"Missing"
2021.emnlp-main.88,2020.acl-main.11,1,0.880619,"Missing"
2021.emnlp-main.88,N19-1423,0,0.53,"as Reddit (Henderson inference on the standard ID evaluation sets: et al., 2019a), again via dual-encoder architectures. C ONV F I T-ed LMs achieve state-of-the-art ID Inspired by these two research threads, we pose performance across the board, with particular the following two crucial questions: gains in the most challenging, few-shot setups. (Q1) Is it necessary to conduct full-scale expen1 Introduction and Motivation sive conversational pretraining? In other words, is it possible to simply and quickly ’rewire’ existPretrained Transformer-based (masked) language models (LMs) such as BERT (Devlin et al., 2019) ing MLM-pretrained encoders as conversational encoders via, e.g., response ranking fine-tuning on or RoBERTa (Liu et al., 2019b), coupled with (much) smaller-scale datasets? task-specific fine-tuning, offer unmatched state-ofthe-art performance in a wide array of standard (Q2) If we frame conversational tasks such as language understanding and conversational tasks intent detection as semantic similarity tasks in(Wang et al., 2019a; Mehri et al., 2020). However, stead of their standard classification-based formupretrained LMs do not produce coherent and effec- lation, is it also possible to fr"
2021.emnlp-main.88,H90-1021,0,0.28565,"Missing"
2021.emnlp-main.88,W19-4101,1,0.864984,"Missing"
2021.emnlp-main.88,2020.findings-emnlp.196,1,0.869365,"Missing"
2021.emnlp-main.88,2021.naacl-main.264,1,0.854613,"Missing"
2021.emnlp-main.88,P19-1536,1,0.899243,"Missing"
2021.emnlp-main.88,2021.naacl-main.334,0,0.0778499,"Missing"
2021.emnlp-main.88,2021.emnlp-main.109,1,0.842734,"Missing"
2021.emnlp-main.88,2021.ccl-1.108,0,0.0820952,"Missing"
2021.emnlp-main.88,2020.findings-emnlp.307,0,0.0157737,"he underlying semantic class/intent. 5 We use the online version of the loss that updates the loss focusing on hard negative pairs (i.e., negatives that are close by cosine in the current semantic space) and hard positives which are far apart in the current space. This typically results in quicker convergence and slightly better performance. 6 The benefits of similarity-based classification were recently validated also in other NLP tasks such as cross-lingual abusive content detection (Sarwar et al., 2021), language modeling (Khandelwal et al., 2020; Guu et al., 2020), and question answering (Kassner and Schütze, 2020), among others. 1154 Dataset Intents Examples Domains BANKING 77 CLINC 150 HWU 64 77 150 64 13,083 23,700 25,716 1 (banking) 10 21 Table 1: Intent detection datasets: key statistics. 3 Experimental Setup Input LMs. We experiment with several popular Transformer-based (Vaswani et al., 2017) LMs as input (see Figure 1), aiming to validate the robustness of C ONV F I T, as well as to analyse the impact of LM pretraining on the final task performance: (i) BERT (Devlin et al., 2019) (labeled BERT henceforth); (ii) RoBERTa (ROB), as an improved variant of BERT, LM-pretrained with more data (Liu et a"
2021.emnlp-main.88,marelli-etal-2014-sick,0,0.0966209,"Missing"
2021.emnlp-main.88,2020.emnlp-main.55,0,0.0863323,"Missing"
2021.emnlp-main.88,2021.naacl-main.237,0,0.134105,"oce2020; Feng et al., 2020), and is typically based on dure which turns any pretrained LM into a dual-encoder architectures. universal conversational encoder (after Stage 1 Another parallel research thread aims at learning C ONV F I T-ing) and task-specialised sentence conversational encoders: it validates the benefits encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not of masked language modeling (MLM) pretraining required, and that LMs can be quickly transon naturally conversational data (Wu et al., 2020; formed into effective conversational encoders Mehri et al., 2021), as well as the benefits of transwith much smaller amounts of unannotated fer learning for conversational tasks which goes data; 2) pretrained LMs can be fine-tuned beyond MLM as the pretraining objective (Mehri into task-specialised sentence encoders, optiet al., 2019; Coope et al., 2020; Henderson and mised for the fine-grained semantics of a parVuli´ c, 2021, inter alia). In particular, response ticular task. Consequently, such specialised selection as a suitable pretraining task (Al-Rfou sentence encoders allow for treating ID as a simple semantic similarity task based on interet al., 201"
2021.emnlp-main.88,D19-1131,0,0.039597,"Missing"
2021.emnlp-main.88,2020.coling-main.118,1,0.79174,"Missing"
2021.emnlp-main.88,P19-1373,0,0.0582766,"Missing"
2021.emnlp-main.88,Q17-1022,1,0.88271,"Missing"
2021.emnlp-main.88,2021.acl-long.410,1,0.84379,"Missing"
2021.emnlp-main.88,2020.emnlp-main.586,1,0.835331,"Missing"
2021.emnlp-main.88,N18-1101,0,0.0662745,"Missing"
2021.emnlp-main.88,2020.emnlp-main.66,0,0.0170445,"9; Wieting et al., V F I T, a simple and efficient two-stage proce2020; Feng et al., 2020), and is typically based on dure which turns any pretrained LM into a dual-encoder architectures. universal conversational encoder (after Stage 1 Another parallel research thread aims at learning C ONV F I T-ing) and task-specialised sentence conversational encoders: it validates the benefits encoder (after Stage 2). We demonstrate that 1) full-blown conversational pretraining is not of masked language modeling (MLM) pretraining required, and that LMs can be quickly transon naturally conversational data (Wu et al., 2020; formed into effective conversational encoders Mehri et al., 2021), as well as the benefits of transwith much smaller amounts of unannotated fer learning for conversational tasks which goes data; 2) pretrained LMs can be fine-tuned beyond MLM as the pretraining objective (Mehri into task-specialised sentence encoders, optiet al., 2019; Coope et al., 2020; Henderson and mised for the fine-grained semantics of a parVuli´ c, 2021, inter alia). In particular, response ticular task. Consequently, such specialised selection as a suitable pretraining task (Al-Rfou sentence encoders allow for treatin"
2021.emnlp-main.88,2020.emnlp-main.410,0,0.267672,"RoBERTa in two separate stages via dual-encoder networks (“zoomed-in” parts; grey blocks denote tunable parameters), and performs intent detection with the C ONV F I T-ed models via similarity-based inference. Stage 1 (S1): adaptive conversational fine-tuning, §2.1; Stage 2 (S2): task-tailored conversational fine-tuning (for intent detection), §2.2. Dashed lines denote baseline/ablation variants which skip one of the two stages: (i) we can directly task-tune the sentence encoder with the task data (Stage 2) without running Stage 1, or (ii) we can skip Stage 2, and similar to Casanueva et al. (2020), learn an MLP classifier on top of the conversational representations from Stage 1. (a) RoBERTa (no fine-tuning) (b) RoBERTa (after S1) (c) RoBERTa (after S1 and S2) Figure 2: t-SNE plots (van der Maaten and Hinton, 2012) of encoded utterances from the ID test set of BANKING 77 (i.e., all examples are effectively unseen by the encoder models at training) associated with a selection of 12 intents, demonstrating the effects of gradual “representation specialisation funnel”. The encoded utterances are created via mean-pooling based on (a) the original RoBERTa LM; (b) RoBERTa after Stage 1 (i.e.,"
2021.emnlp-main.88,2020.acl-demos.12,0,0.032949,"a Stage 1 C ON V F I T-ing; intent detection is performed via standard feature-based MLP classification on top of the sentence encodings as in prior work. SotA Sentence Encoders. We evaluate three widely used state-of-the-art sentence encoders in the standard feature-based MLP classification approach to intent detection:13 (i) ConveRT (Henderson et al., 2020) is a dual sentence encoder pretrained with the conversational response selection task (Henderson et al., 2019b) on the full Reddit data (Al-Rfou et al., 2016; Henderson et al., 2019a); (ii) multilingual Universal Sentence Encoder (mUSE) (Yang et al., 2020) is a multilingual and better-performing version of the USE model for English (Cer et al., 2018), which again relies on a standard dual-encoder framework (Henderson et al., 2019b; Humeau et al., 2020) and is pretrained on massive amounts of data; (iii) Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) (Devlin et al., 2019) into a sentence encoder using a dual-encoder framework 12 Very similar results are observed with k = 3 and k = 5. For more technical details regarding each sentence encoder, we refer the reader to the original w"
2021.emnlp-main.88,W18-3022,0,0.0872029,"s well as the benefits of transwith much smaller amounts of unannotated fer learning for conversational tasks which goes data; 2) pretrained LMs can be fine-tuned beyond MLM as the pretraining objective (Mehri into task-specialised sentence encoders, optiet al., 2019; Coope et al., 2020; Henderson and mised for the fine-grained semantics of a parVuli´ c, 2021, inter alia). In particular, response ticular task. Consequently, such specialised selection as a suitable pretraining task (Al-Rfou sentence encoders allow for treating ID as a simple semantic similarity task based on interet al., 2016; Yang et al., 2018; Henderson et al., pretable nearest neighbours retrieval. We vali2019b; Humeau et al., 2020) learns representations date the robustness and versatility of the C ON that organically capture conversational cues from V F I T framework with such similarity-based conversational text data such as Reddit (Henderson inference on the standard ID evaluation sets: et al., 2019a), again via dual-encoder architectures. C ONV F I T-ed LMs achieve state-of-the-art ID Inspired by these two research threads, we pose performance across the board, with particular the following two crucial questions: gains in th"
2021.emnlp-main.88,2020.emnlp-main.411,0,0.435735,"; Casanueva et al., 2020; Feng et al., 2020): a standard ID approach stacks a Multi-Layer Perceptron (MLP) classifier on top of the fixed sentence vectors t, and fine-tunes only the MLP parameters (Casanueva et al., 2020; Gerz et al., 2021). However, the output of S1 can also be further fed as the input encoding for C ONV F I T’s Stage 2 (Figure 1). 2.2 Stage 2: Task-Based Sentence Encoders Stage 2 fine-tuning is inspired by metric-based meta-learning (Vinyals et al., 2016; Musgrave et al., 2020) and exemplar-based (also termed prototype-based) learning (Snell et al., 2017; Sung et al., 2018; Zhang et al., 2020), which is especially suited for few-shot scenarios. We assume the existence of Na annotated in-task examples see (Henderson et al., 2019a). The intuition is that sentences which elicit similar responses should obtain similar sentence encodings (Yang et al., 2018). 2 We also experimented with another SotA loss function, the triplet-based multi-similarity loss (Wang et al., 2019b; Liu et al., 2021a), without any substantial performance differences. 1153 {(x1 , y1 ), . . . , (xNa , yNa )}: e.g., x-s are text sentences with y-s being their intent labels/classes; let us assume that there are Nc cl"
2021.emnlp-main.88,2021.acl-long.447,1,0.82232,"Missing"
2021.findings-acl.431,2020.emnlp-main.40,0,0.0242914,"lopment data in the target the training portions of Russian GSD, PUD, and SynTagRus treebanks. 5 Note that the number of climbs sl needed to reach some hierarchy level depends on the language l: e.g., the hierarchy level joining Tagalog (tl) with Scottish, Irish, and Welsh ({gd, ga, cy}) is reached in sl = 1 climbs from Tagalog, sl = 2 climbs from Scottish and sl = 3 climbs from Irish and Welsh. language. Model selection based on the development set of the source language, on the other hand, overfits the model to the source language, which may hurt effectiveness of the cross-lingual transfer (Keung et al., 2020; Chen and Ritter, 2020). For test treebanks with a respective development portion, T OWER uses that development set for model selection. For low-resource languages l without development treebanks, we compile a proxy development set Dl = ∪{Dk }K k=1 by collecting all development treebanks Dk from the hierarchy level closest to l that encompasses at least one treebank with a development set.6 Intuitively, the more syntactically similar Dl is to l, the more beneficial the model selection based on Dl will be for performance on l, the optimal model checkpoint w.r.t. l should be closer to the model"
2021.findings-acl.431,D19-1279,0,0.0524103,"e leaf node of the target language. We stop ‘climbing’ (i.e., select the set of source treebanks subsumed by the current hierarchy level), when the relative decrease in linguistic similarity of the training sample w.r.t the target language outweighs the increase in size of the training sample. We additionally exploit the linguistic similarity between the target language and its closest sources with existing development treebanks to inform a model selection (that is, early-stopping) heuristic. T OWER substantially outperforms stateof-the-art multilingual parsers – UDPipe (Straka, 2018), UDify (Kondratyuk and Straka, 2019), and ¨ un et al., 2020) on low-resource lanUDapter (Ust¨ guages, while offering comparable performance for high-resource languages. 2 Climbing the T OWER of Treebanks Constructing the T OWER. We start by hierarchically clustering the set of 89 languages from Universal Dependencies 3 based on their syntactic collection of N source treebanks. 2 For the vast majority of world languages there does not exist a single manually annotated syntactic tree. 3 We worked with the UD version 2.5. Figure 1: Part of the syntax-based hierarchical clustering of UD languages (ISO 639-1 codes). similarity. To th"
2021.findings-acl.431,D19-1277,0,0.0232923,"Missing"
2021.findings-acl.431,2020.tacl-1.50,0,0.0256652,"pretrained language models (PLMs) (Devlin Ivan Vuli´c University of Cambridge Language Technology Lab iv250@cam.ac.uk et al., 2019; Liu et al., 2019; Brown et al., 2020) and their end-to-end fine-tuning for downstream tasks has reduced the downstream relevance of supervised syntactic parsing. What is more, there is more and more evidence that PLMs implicitly acquire rich syntactic knowledge through large-scale pretraining (Hewitt and Manning, 2019; Chi et al., 2020) and that exposing them to explicit syntax from human-coded treebanks does not offer significant language understanding benefits (Kuncoro et al., 2020; Glavaˇs and Vuli´c, 2021). In order to implicitly acquire syntactic competencies, however, PLMs need language-specific corpora at the scale at which it can only be obtained for a tiny portion of world’s 7,000+ languages. For the remaining vast majority of languages – with limited-size monolingual corpora – explicit syntax still provides valuable linguistic bias for more sample-efficient learning in downstream NLP tasks. Reliable syntactic parsing requires annotated treebanks of reasonable size: this prerequisite is, unfortunately, satisfied for even fewer languages. Despite the multi-year, w"
2021.findings-acl.431,2020.coling-main.345,1,0.895755,"Missing"
2021.findings-acl.431,E17-2002,0,0.0923696,"ata augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), violate the zero-shot transfer by assuming a small target-language treebank – a requirement unfulfilled for most world languages.2 In this work, we propose a simple and effective heuristic for selecting a good set of source treebanks for any given low-resource target language. In our approach, named T OWER, we first hierarchically cluster all Universal Dependencies (UD) languagues. To this end, we compute syntactic similarity of languages by comparing manually coded vectors of their syntactic properties from the URIEL database (Littell et al., 2017). We then iteratively ‘climb’ that language hierarchy level by level, starting from the leaf node of the target language. We stop ‘climbing’ (i.e., select the set of source treebanks subsumed by the current hierarchy level), when the relative decrease in linguistic similarity of the training sample w.r.t the target language outweighs the increase in size of the training sample. We additionally exploit the linguistic similarity between the target language and its closest sources with existing development treebanks to inform a model selection (that is, early-stopping) heuristic. T OWER substanti"
2021.findings-acl.431,2021.ccl-1.108,0,0.0316735,"Missing"
2021.findings-acl.431,2020.lrec-1.497,0,0.035975,"Missing"
2021.findings-acl.431,2020.emnlp-demos.7,1,0.826128,"Missing"
2021.findings-acl.431,D18-1039,0,0.0281471,"8.0 bm br tr (IMST) 74.3 59.3 44.5 39.8 38.4 sv (TB) Tower 69.2 68.4 58.5 0 UDApter ja (GSD) 69.6 70.0 fo 36.7 35.0 16.4 19.2 19.4 17.3 18.6 40.1 22.2 8.0 kpv myv pcm sa tl 12.1 16.2 wbp 22.2 29.3 33.8 AVG Figure 2: LAS performance of UDify, UDapter and T OWER on 12 high-resource treebanks (top figure), and 11 low-resource languages (bottom figure). 2019) with the deep biaffine parser (Dozat and Manning, 2017) and trains on all UD treebanks; (2) ¨ un et al., 2020) extends mBERT with UDapter (Ust¨ adapter parameters (Houlsby et al., 2019; Pfeiffer et al., 2020) that are contextually generated (Platanios et al., 2018) from URIEL vectors – the parameters of the adapter generator are trained on treebanks of 13 diverse resource-rich languages selected by Kulmizev et al. (2019). We additionally quantify the contributions of T OWER’s heuristic components (TBS and MS, see §2) by evaluating variants in which we (1) remove TBS and train on the closest language with training data (-TBS), (2) remove MS and just select the model checkpoint that performs best on the proxy dev set Dl (-MS), and (3) remove both TBS and MS (-TBS-MS). Training and Optimization Details. We limit input sequences to 128 subword tokens. We us"
2021.findings-acl.431,P15-2040,0,0.0290423,"ustive search over all possible subsets of source treebanks is not only computationally intractable1 but also 1 One can create 2N − 1 different training sets from a 4878 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4878–4888 August 1–6, 2021. ©2021 Association for Computational Linguistics uninformative in true zero-shot scenarios in which there is no development treebank (i.e., any syntactically annotated data) for the target language. Most existing transfer methods therefore either (1) choose one (or a few) best source languages for each target language (Rosa and Zabokrtsky, 2015; Agi´c, 2017; Lin et al., 2019; Litschko et al., 2020) or (2) train a single multilingual parser on all available treebanks; such parsers, based on pretrained multilingual encoders, currently produce best results in low-resource parsing (Kondratyuk ¨ un et al., 2020). Other transand Straka, 2019; Ust¨ fer approaches, e.g., based on data augmentation (S¸ahin and Steedman, 2018; Vania et al., 2019), violate the zero-shot transfer by assuming a small target-language treebank – a requirement unfulfilled for most world languages.2 In this work, we propose a simple and effective heuristic for selec"
2021.findings-acl.431,D18-1545,0,0.0350568,"Missing"
2021.findings-emnlp.410,P18-1073,0,0.0529912,"Missing"
2021.findings-emnlp.410,2020.acl-main.421,1,0.808072,"ponent inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans- this work, we adopt the competitive and lightweight fer methods of prior work. (so-called bottleneck) adapter variant of Pfeiffer In experiments on zero-shot cross-lingual trans- et al. (2021a). There, only one adapter module, 4763 consisting of a successive down-projection and up-projection, is injected per Transformer layer"
2021.findings-emnlp.410,N19-1191,0,0.133549,"ross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to train a language adapter on the unlabeled data for each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of"
2021.findings-emnlp.410,N16-1101,0,0.0319479,"he nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimensional language embeddings λ(l) ∈ Rdl . These are used by the generator g, a hyper-network (Ha et al., 2017) component4 with its own parameterization φ, to produce the language-specific parameterization of the main model: θ (l) = gφ (λ(l) ). While g can in principle be any differentiable function (i.e., arbitrarily deep neural model), in practice it is typically set to a simple linear projection (i.e., φ = W ): gW (λ(l) ) , W λ(l) , (2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameter"
2021.findings-emnlp.410,P19-1070,1,0.898419,"Missing"
2021.findings-emnlp.410,2021.eacl-main.270,1,0.690338,"Missing"
2021.findings-emnlp.410,2020.acl-main.560,0,0.0116496,"e generation of task-agnostic LAs that can support downstream cross-lingual transfer for arbitrary NLP tasks. 4 A hyper-network is a neural model that generates the parameters of another (main) neural model. 5 Training MAD-G on 95 languages with dl = 32 (this work) achieves roughly a threefold saving in parameter size. 4764 3 MAD-G: Methodology MAD-G aims to enable resource-efficient adaptation of MMTs to a wide range of previously unseen, radically resource-poor languages,6 and contribute in this manner to more sustainable (Strubell et al., 2019; Moosavi et al., 2020) and more inclusive NLP (Joshi et al., 2020). We couple (i) the computational efficiency of the light-weight adapters (cf. Section 2.1) and (ii) knowledge sharing and zero-shot language transfer capabilities of CPG (cf. Section 2.2), with (iii) external linguistic (i.e., typological) knowledge (Ponti et al., 2019a) towards supporting arbitrary NLP tasks for (even radically) resource-poor languages. MAD-G mitigates important limitations of prior work. Unlike Üstün et al. (2020), we generate taskagnostic LAs, (re)usable across NLP tasks. Unlike the MAD-X framework (Pfeiffer et al., 2020b), which trains LAs independently for each language"
2021.findings-emnlp.410,2020.emnlp-main.363,1,0.824906,"Missing"
2021.findings-emnlp.410,E17-2002,0,0.408844,"rs can be leveraged in typically achieved through the use of adapter layers arbitrary downstream tasks (Pfeiffer et al., 2020b). (Houlsby et al., 2019; Pfeiffer et al., 2020b). MAD-G shares information across languages (i) In particular, a language adapter is a light-weight at the level of hidden representations by sharing component inserted into a MMT such as mBERT the parameters of the adapter generator as well as (Devlin et al., 2019) or XLM-R (Conneau et al., (ii) at the typological level by conditioning on fea- 2020) with the purpose of specializing the MMT tures from the URIEL database (Littell et al., 2017). for a particular language, in order to either (a) supThe latter additionally enables zero-shot transfer port a new language not covered by the MMT’s to unseen languages. Further, we propose a vari- original multilingual pretraining (Pfeiffer et al., ant of MAD-G in which we generate adapters also 2020b; Artetxe et al., 2020) or (b) recover/improve conditioned on their Transformer layer position the performance for a particular (resource-rich) lan(see Section 3.2), allowing MAD-G to be much guage (Bapna and Firat, 2019; Rust et al., 2021). In more parameter-efficient than adapter-based trans-"
2021.findings-emnlp.410,2021.eacl-main.39,1,0.794583,"amed entity recognition (NER) on the MasakhaNER dataset for African languages (Adelani et al., 2021). For POS and DP, we evaluate on a substantial subset of all UD languages with available treebanks.8 We discern between three language groups in evaluation, with some examples in Table 1: (i) mBERT-seen languages are those included in mBERT’s pretraining; (ii) MAD-G-seen languages were not part of mBERT’s pretraining but are included in MAD8 For POS and DP, we omit only (i) languages with scripts unseen in mBERT’s pretraining, where mBERT’s tokenizer predominantly produces unknown (UNK) tokens (Pfeiffer et al., 2021b), (ii) languages lacking any information in URIEL, and (iii) languages whose treebanks have missing fields. For MasakhaNER, we evaluate on all dataset languages except Amharic, as Amharic also uses a script unseen by mBERT. G training; and (iii) unseen languages are those not included in mBERT pretraining nor in MAD-G training. 4.1 Baselines and MAD-G Variants mBERT is an MMT pretrained on the Wikipedias of 104 languages. We use mBERT as the base MMT for MAD-G. XLM-R is a state-of-the-art MMT pretrained on the CommonCrawl data of 100 languages (Conneau et al., 2020).9 We evaluate them in the"
2021.findings-emnlp.410,2020.emnlp-demos.7,1,0.70459,"Missing"
2021.findings-emnlp.410,2020.emnlp-main.617,1,0.622952,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.800,1,0.87244,"Missing"
2021.findings-emnlp.410,P19-1493,0,0.0388325,"te that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The gen"
2021.findings-emnlp.410,D18-1039,0,0.123067,"each language (Pfeiffer et al., 2020b) via masked language modeling (MLM). However, this generally requires substantial amounts of monolingual data, which prevents adapters from serving under-resourced languages where such additional language-specific capacity would be most useful. To address this deficiency, we propose multilingual adapter generation (MAD-G), a novel paradigm that enables the generation of adapters for low-resource languages by sharing information across languages. Instead of learning separate adapters for each language, MAD-G leverages contextual parameter generation (CPG; Platanios et al., 2018a; Ponti et al., 2019b), that is, it learns a single model that can generate a language adapter for an arbitrary target language. At the core of MAD-G is a contextual parameter generator which 1 mBERT and XLM-R have been trained on corpora from Multilingual NLP has witnessed large ad104 and 100 languages, respectively. According to Glottolog vances, with cross-lingual word embedding spaces (Hammarström et al., 2017), however, there are over 7,000 (Mikolov et al., 2013; Artetxe et al., 2018; Glavaš languages spoken around the world. 4762 Findings of the Association for Computational Linguistics"
2021.findings-emnlp.410,2020.emnlp-main.185,1,0.891273,"Missing"
2021.findings-emnlp.410,J19-3005,1,0.858703,"Missing"
2021.findings-emnlp.410,D19-1288,1,0.883413,"Missing"
2021.findings-emnlp.410,2021.emnlp-main.626,1,0.768969,"genealogical ties to high(er)-resource languages. CPG is a technique introduced by Platanios et al. (2018a) to address these drawbacks. While originally conceived for neural machine translation (NMT), CPG can be applied to any neural model f parameterized by θ, for which we aim to learn parameterizations for a number of different contexts; in multilingual NLP, these “contexts” are languages. In the instance-per-language approach, an independent parameterization θ (l) , l ∈ {1, . . . , nl }, is learned for each of the nl languages of interest. 2 According to Pfeiffer et al. (2020a, 2021a) and Rücklé et al. (2021), such an architecture with a single adapter per Transformer layer is more parameter-efficient while performing on par with the architecture of Houlsby et al. (2019) with two adapters per Transformer layer (one after the multi-head attention sublayer and one after the feed-forward sublayer). 3 Other examples include the training of language-specific pretrained language models (Rust et al., 2021) as well as language pair-specific encoder–decoder models for machine translation (Luong et al., 2016; Firat et al., 2016). In CPG, the only language-specific parameters that we learn are the low-dimens"
2021.findings-emnlp.410,2021.acl-long.243,1,0.823183,"Missing"
2021.findings-emnlp.410,2021.findings-acl.106,1,0.880987,"2) where W ∈ Rnp ×dl is a learnable weight matrix, np being the number of parameters of f . The total number of parameters learned when training nl independent models is nl np , whereas the number of parameters in the W matrix is dl np . Therefore, neglecting the small number of parameters dedicated to language embeddings, the CPG approach uses fewer parameters when dl < nl .5 More importantly, in multilingual training the generator matrix W is shared across all languages, which enables knowledge sharing across languages and leads to improved transfer performance. Platanios et al. (2018b) and Ponti et al. (2021a) opt for randomly initializing language embeddings λ(l) and learning them end-to-end. Specified like this, however, CPG cannot generalize to languages unseen in training, as it would lack embeddings for those languages at inference. To support generalization to arbitrary new languages, one must ground language embeddings in some external language representation, available for many languages. To this end, Ponti et al. (2019b) exploit typological language vectors from the URIEL database (Littell et al., 2017) directly as language embeddings to generate a full set of model parameters. In a simi"
2021.findings-emnlp.410,2020.emnlp-main.180,0,0.119503,"Missing"
2021.findings-emnlp.410,D19-1077,0,0.021338,"sfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed as a parameter-efficient means to extend multilingual models to underrepresented languages (Bapna and Firat, 2019; Üstün et al., 2020). The general practice is to tr"
2021.findings-emnlp.410,2021.naacl-main.41,0,0.0352696,"offers substantial benefits for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G’s transfer performance can be further improved via: (i) multi-source training, i.e., by generating and combining adapters of multiple languages with available taskspecific training data; and (ii) by further finetuning generated MAD-G adapters for languages with monolingual data. 1 Introduction et al., 2019) and, more recently, massively multilingual Transformers (MMTs) like mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), and mT5 (Xue et al., 2021) as main vehicles of cross-lingual transfer. Although MMTs display impressive (zero-shot) cross-lingual transfer abilities (Pires et al., 2019; Wu and Dredze, 2019), their performance has been shown to drop when the target language is typologically distant to the source language, or the size of its pretraining data is limited (Hu et al., 2020; Lauscher et al., 2020). In addition, their coverage of the world’s languages—and consequently the range of language technology applications they can support—remains insufficient.1 Adapters (Rebuffi et al., 2017; Houlsby et al., 2019) have been proposed a"
2021.naacl-main.264,D18-1547,0,0.060573,"Missing"
2021.naacl-main.264,W19-4101,1,0.86003,"Missing"
2021.naacl-main.264,2020.nlp4convai-1.5,1,0.87835,"Missing"
2021.naacl-main.264,2020.findings-emnlp.196,1,0.912342,"Missing"
2021.naacl-main.264,D18-2029,0,0.0960672,"Missing"
2021.naacl-main.264,W14-4337,1,0.838333,"Missing"
2021.naacl-main.264,2020.acl-main.747,0,0.225668,"wed by the existing cloze task using Reddit data, is well aligned models of few-shot slot labeling is transfer learnwith its intended usage on sequence labeling ing (Ruder et al., 2019): 1) they rely on representatasks. This enables learning domain-specific tions from models pretrained on large data collecslot labelers by simply fine-tuning decoding tions in a self-supervised manner on some general layers of the pretrained general-purpose seNLP tasks such as (masked) language modeling quence labeling model, while the majority of the pretrained model’s parameters are kept (Devlin et al., 2019; Conneau et al., 2020; Brown frozen. We report state-of-the-art performance et al., 2020) or response selection (Henderson et al., of ConVEx across a range of diverse domains 2019b, 2020; Cer et al., 2018); and then 2) add adand data sets for dialog slot-labeling, with the ditional task-specific layers for modeling the input largest gains in the most challenging, few-shot sequences. However, we detect several gaps with setups. We believe that ConVEx’s reduced the existing setup, and set to address them in this pretraining times (i.e., only 18 hours on 12 work. First, recent work in NLP has validated that a GPUs) a"
2021.naacl-main.264,2020.acl-main.11,1,0.867596,"Missing"
2021.naacl-main.264,N19-1423,0,0.661355,"minant paradigm followed by the existing cloze task using Reddit data, is well aligned models of few-shot slot labeling is transfer learnwith its intended usage on sequence labeling ing (Ruder et al., 2019): 1) they rely on representatasks. This enables learning domain-specific tions from models pretrained on large data collecslot labelers by simply fine-tuning decoding tions in a self-supervised manner on some general layers of the pretrained general-purpose seNLP tasks such as (masked) language modeling quence labeling model, while the majority of the pretrained model’s parameters are kept (Devlin et al., 2019; Conneau et al., 2020; Brown frozen. We report state-of-the-art performance et al., 2020) or response selection (Henderson et al., of ConVEx across a range of diverse domains 2019b, 2020; Cer et al., 2018); and then 2) add adand data sets for dialog slot-labeling, with the ditional task-specific layers for modeling the input largest gains in the most challenging, few-shot sequences. However, we detect several gaps with setups. We believe that ConVEx’s reduced the existing setup, and set to address them in this pretraining times (i.e., only 18 hours on 12 work. First, recent work in NLP has va"
C12-1166,J93-2003,0,0.0929656,"al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009),"
C12-1166,J93-1001,0,0.254266,"pora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead of data augmentation. The method is directed towards extraction of only highly reliable translation pairs from parallel data of limited size. It is based on the idea of sub-corpora sampling from the original corpus. For inst"
C12-1166,W93-0301,0,0.15569,"or analysis, we have detected that both IBM Model 1 and LLR provide a wrong translation of the Dutch word beschouwen (consider), since both models retrieve the English word as as the first translation candidate (due to a very high frequency of the collocation consider as). Other examples of the same type include the Dutch word integreren (integrate) which is translated as into, betwijfelen (doubt) which is translated as whether, or an Italian example of the verb entrare (enter) which is translated as into. Our BLE model, on the other hand, provides correct translations for all these examples. Dagan et al. (1993) noted that collocates often tend to cause confusion among algorithms for bilingual lexicon extraction. More examples include the Dutch word opinie (opinion), translated as public by IBM Model 1 and LLR (due to a high frequency of the collocation public opinion), the Dutch word cirkels (circles), translated as concentric, or the Italian word pensionabile (pensionable), translated as age. All these examples are again correctly translated by our model for lexicon extraction. In order to test the hypothesis that our lexicon extraction model does not suffer from the problem of learning indirect as"
C12-1166,J93-1003,0,0.45469,"hms built upon the same idea. 2.3.3 Properties of the Algorithm Reducing corpora size provides several benefits. First, establishing associations between translation candidates is much easier when we deal with low-frequency words - we reduce our problem to a binary decision problem. According to the specified criteria for extraction, two words are simply considered to be a translation pair, or they are not. By employing the criteria that rely on raw frequency counts as distributional evidences, we remove the need of an association measure based on hypothesis testing such as the G 2 statistic (Dunning, 1993; Agresti, 2002) or a similarity-based measure such as the Dice coefficient (Dice, 1945), which are often unreliable when dealing with low-frequency words (Manning and Schütze, 1999). The SampLEX algorithm is symmetric and non-directional. The final output of the algorithm provides translation pairs along with their counts obtained after training. We can easily transform them into word translation probabilities to build word translation tables similar to those of IBM Model 1. Since the algorithm is symmetric, we can obtain both source-to-target and target-to-source word translation probabiliti"
C12-1166,W04-3208,0,0.0270937,"statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons"
C12-1166,P98-1069,0,0.125863,"ieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almo"
C12-1166,P08-1088,0,0.031167,"ey, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexico"
C12-1166,J93-1006,0,0.0943796,"it is easier to establish translational equivalence for low-frequency words. 2.2 Criteria for Extraction of Translation Pairs Given is a source language S, a target language T , and a corpus C of N aligned item pairs C = {(I1S , I1T ), (I2S , I2T ), . . . , (I NS , I NT )}, where, depending on the corpus type, item pairs may be sentences, paragraphs, chunks, documents, etc. For parallel corpora, the item pairs are pairs of sentences. The goal is to extract potential translation candidates from the item-aligned set using only internal distributional evidences. Internal evidences, according to Kay and Röscheisen (1993), represent information derived only from the given corpora themselves. Our criteria for establishing translational equivalence between words are derived from this trivial case: Imagine the scenario where a source word w1S occurs only once on the source side of the corpus C , in a source item I Sj . There is a target word w2T occurring in a target item I jT (which is aligned to I Sj ) and the word w2T also occurs only once on the target side of the corpus C . Additionally, there does not exist another source word w aS such that it occurs only once on the source side of the corpus and, at the s"
C12-1166,2005.mtsummit-papers.11,0,0.012472,"nce. Also, by building sub-corpora of smaller sizes from the original large corpus, we perform an implicit disambiguation - a word occurring only once or twice in a small sub-corpus cannot bear more meanings in that sub-corpus, although it might have more meanings in the large superset corpus. 3 Experimental Setup In this section, we present datasets used for training, training setup of the SampLEX method and state-of-the-art models for bilingual lexicon extraction from parallel data often used in real-life applications. 2727 3.1 Training 3.1.1 Training Collections We work with Europarl data (Koehn, 2005) for Dutch-English and Italian-English language pairs, retrieved from the website2 of the OPUS project (Tiedemann, 2009). We use subsets of the corpora, comprising the first 300, 000 sentence pairs. For Dutch-English, there are 76, 762 unique Dutch words, and 37, 138 unique English words. For Italian-English, there are 68, 710 unique Italian words and 37, 391 unique English words. The unbalance between the number of unique vocabulary words is mostly due to a richer morphological system in Italian and the noun compounding phenomenon in Dutch. Since we also want to test and evaluate the behavior"
C12-1166,C10-1070,0,0.0197607,"construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from"
C12-1166,E09-1057,0,0.0981246,"ls (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead"
C12-1166,J00-2004,0,0.0642524,"ient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Ki"
C12-1166,W03-0301,0,0.0997297,"Missing"
C12-1166,W01-1411,0,0.0390315,"ey (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Kilgarriff, 2001; Padó and Lapata, 2007), which is a four-cell matrix for each pair of words (w1S , w2T ) (see Table 1). w2T ¬w2T w1S k m ¬w1S l n Table 1: The contigency table for a pair of words (w1S , w2T ). The contingency table records that source word w1S and target word w2T co-occur in k aligned item/sentences pairs, and w1S occurs in m aligned pairs in which w2T is not present. Similarly, w2T occurs in l aligned pairs in which w1S is not present, and n is the number of aligned pairs that"
C12-1166,moore-2002-fast,0,0.043971,"t to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize st"
C12-1166,P04-1066,0,0.288762,"tool for bilingual lexicon extraction from parallel data (e.g., Venugopal et al. (2003), Munteanu and Marcu (2005), Munteanu and Marcu (2006), Lefever et al. (2009)). We use standard GIZA++ settings and train IBM Model 1 with 5 iterations 2 http://opus.lingfil.uu.se/Europarl3.php We have also tried to use word translation probabilities from the higher order IBM Models, but we have not detected any major difference in results on the task of bilingual word lexicon extraction. 3 2728 (IBM1-i5) and 20 iterations (IBM1-i20) of the EM algorithm, as often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-"
C12-1166,W04-3243,0,0.425853,"tool for bilingual lexicon extraction from parallel data (e.g., Venugopal et al. (2003), Munteanu and Marcu (2005), Munteanu and Marcu (2006), Lefever et al. (2009)). We use standard GIZA++ settings and train IBM Model 1 with 5 iterations 2 http://opus.lingfil.uu.se/Europarl3.php We have also tried to use word translation probabilities from the higher order IBM Models, but we have not detected any major difference in results on the task of bilingual word lexicon extraction. 3 2728 (IBM1-i5) and 20 iterations (IBM1-i20) of the EM algorithm, as often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-"
C12-1166,P07-1084,0,0.0208928,"anslation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from paral"
C12-1166,J05-4003,0,0.373784,"and-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of"
C12-1166,P06-1011,0,0.491432,"lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains"
C12-1166,J03-1002,0,0.233415,"cnih rjeˇcnika, empirijsko prevo¯ denje rijeˇci, uzorkovanje potkorpusa, smanjivanje koliˇcine podataka, niskofrekventne rijeˇci. Proceedings of COLING 2012: Technical Papers, pages 2721–2738, COLING 2012, Mumbai, December 2012. 2721 1 Introduction Bilingual word lexicons serve as an invaluable and indispensable source of knowledge for both end users (as an aid for translators or other language specialists) and many natural language processing tasks, such as dictionary-based cross-language information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi"
C12-1166,J07-2002,0,0.0139445,"ure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Kilgarriff, 2001; Padó and Lapata, 2007), which is a four-cell matrix for each pair of words (w1S , w2T ) (see Table 1). w2T ¬w2T w1S k m ¬w1S l n Table 1: The contigency table for a pair of words (w1S , w2T ). The contingency table records that source word w1S and target word w2T co-occur in k aligned item/sentences pairs, and w1S occurs in m aligned pairs in which w2T is not present. Similarly, w2T occurs in l aligned pairs in which w1S is not present, and n is the number of aligned pairs that involve neither w1S nor w2T . The final formula for the log-likelihood ratio is then defined as: L LR(w1S , w2T ) = G 2 (k, l, m, n) = 2(k"
C12-1166,P11-1133,0,0.223311,"ns manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003"
C12-1166,P95-1050,0,0.147782,"rmation retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or"
C12-1166,P99-1067,0,0.113752,"al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusive"
C12-1166,J03-3002,0,0.0301959,"word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead of data augmentation. The method is directed towards extraction of only highly reliable translation pairs from parallel data of limited size. It is based on the idea of sub-corpora sampling from the original corpus. For instance, given an initial corpus C of 4 data items {I1 , I2 , I3 , I4 }, the construction of, say, a sub-corpus SC = {I2 , I4 } may be observed as: (1) sampling items I2 , I4 ∈ C for SC (hence the term sub-corpora sampling) or (2) removing data items I1 , I3 from the or"
C12-1166,P10-1011,0,0.0246365,"ous domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignme"
C12-1166,D12-1003,0,0.0125501,"rom parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as t"
C12-1166,E03-1026,0,0.163137,"often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and M"
C12-1166,J07-1003,0,0.0136774,"sually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues"
C12-1166,P03-1041,0,0.225583,"tions trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, the"
C12-1166,P11-2084,1,0.895272,"Missing"
C12-1166,E12-1046,1,0.886432,"Missing"
C12-1166,C98-1066,0,\N,Missing
C12-1166,C10-1003,0,\N,Missing
C16-1123,Q16-1031,0,0.0570178,"ons of atomic features, this work presents a hierarchical architecture that enables discarding chosen feature combinations. This allows the model to integrate prior typological knowledge, while ignoring uninformative combinations of typological and dependency features. At the same time, it capitalises on the automatisation of feature construction inherent to tensor models to generate combinations of informative typology-based features, further enhancing the added value of typological priors. Another successful integration of externally-defined typological information in parsing is the work of Ammar et al. (2016). They present a multilingual parser trained on a concatenation of syntactic treebanks of multiple languages. To reduce the adverse impact of contradicting syntactic information in treebanks of typologically distinct languages, while still maintaining the benefits of additional training data for cross-linguistically consistent syntactic patterns, the parser encodes a language-specific bias for each given input language. This bias is based on the identity of the language and its WBO features as used in (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015). Differently from p"
C16-1123,D14-1034,1,0.783502,"ly carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed"
C16-1123,W13-2710,0,0.0153812,"g to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subseque"
C16-1123,P10-1131,0,0.177593,"nder, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In"
C16-1123,W14-1603,1,0.869447,"alues for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of stu"
C16-1123,K15-1010,1,0.845013,"nguage model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typological Information and NLP: What’s Next? § 4.2 surveyed the current uses of typological information in NLP. Here we discuss several future research avenues that might benefit from tighter integration"
C16-1123,P07-1036,0,0.0446587,"respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of"
C16-1123,D14-1082,0,0.0308785,"e type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evalua"
C16-1123,N09-1009,0,0.190277,"jority of other languages they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: languag"
C16-1123,D11-1005,0,0.0221498,"ifferent languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This pa"
C16-1123,W02-1001,0,0.0226129,"r open challenges for typologically-driven NLP is the construction of principled mechanisms for the integration of typological knowledge in machine learning-based algorithms. Here, we briefly discuss a few traditional machine learning frameworks which support encoding of expert information, and as such hold promise for integrating typological information in NLP. Encoding typological knowledge into machine learning requires mechanisms that can bias learning (parameter estimation) and inference (prediction) of the model towards predefined knowledge. Algorithms such as the structured perceptron (Collins, 2002) and structured SVM (Taskar et al., 2004) iterate between an inference step and a parameter update step with respect to gold training labels. The inference step is a natural place for encoding external knowledge through constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Inde"
C16-1123,J81-4005,0,0.764144,"Missing"
C16-1123,P11-1061,0,0.0239916,"ardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological info"
C16-1123,P07-1009,0,0.67981,"Missing"
C16-1123,de-marneffe-etal-2014-universal,0,0.088799,"Missing"
C16-1123,P16-1038,0,0.0439992,"al POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource rich to resource poor languages. 1303 Berzak et al. (2015) use typological classifications to study language learning. Formalizing the theory of “Contrastive Analysis” which aims to analyse learning difficulties in a foreign language by comparing native and foreign language structures, they build a regression model that predicts language-specific grammatical error distributions by comparing typological features in the native and foreign languages. 5 Typologica"
C16-1123,N15-1184,0,0.0326505,"Missing"
C16-1123,C10-1044,0,0.311097,"see Table 1 for feature coverage of other typological databases). The integration of information from different databases is challenging due to differences in feature taxonomies as well as information overlap across repositories. Furthermore, available typological classifications contain different feature types, including nominal, ordinal and interval variables, and features that mix several types of values. This property hinders systematic and efficient encoding of such features in NLP models – a problem which thus far has only received a partial solution in the form of feature binarisation (Georgi et al., 2010). Further, typological databases are constructed manually using limited resources, and do not contain information on the distribution of feature values within a given language. This results in incomplete feature characterisations, as well as inaccurate generalisations. For example, WALS encodes only the dominant noun-adjective ordering for French, although in some cases this language also permits the adjective-noun ordering. Other aspects of typological databases may require feature pruning and preprocessing prior to use. For example, some features in WALS, such as feature 81B “Languages with"
C16-1123,P11-1149,1,0.703578,"and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss t"
C16-1123,N13-1113,1,0.819373,"d, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such a"
C16-1123,W04-3229,0,0.0329235,"uage structure), despite their great diversity. Captured in typological classifications at the level of generalisation useful 1299 for NLP, such information can be used to support multilingual NLP in a variety of ways (Bender, 2011). For example, it can be used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common"
C16-1123,P14-1006,0,0.0284841,"ual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment betwee"
C16-1123,P11-1057,0,0.0519073,"natural language, regardless of language-specific variation; ii) comprehensive systematisation of all possible variation in different languages. The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased u"
C16-1123,C12-1089,0,0.0412053,"Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually im"
C16-1123,I08-2093,0,0.266592,"gical information relating to word order. The predicted typological features, when evaluated against 1301 WALS, achieve high accuracy. This method not only extends WALS word order documentation to hundreds of new languages, but also quantifies the frequency of different word orders across languages – information that is not available in manually crafted typological repositories. Typological information can also be extracted from Interlinear Glossed Text (IGT). Such resources contain morphological segmentation, glosses and English translations of example sentences collected by field linguists. Lewis and Xia (2008) and Bender et al. (2013) demonstrate that IGT can be used to extract typological information relating to word order, case systems and determiners for a variety of languages. Another line of work seeks to increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluste"
C16-1123,P08-1099,0,0.0349195,"hrough constraints. It biases the prediction of the model to agree with external knowledge, which, in turn, affects both the training process and the final model prediction. As typological information often reflects tendencies rather than strict rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingua"
C16-1123,W12-0209,0,0.0637077,"Missing"
C16-1123,D11-1006,0,0.132738,"l., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a trans"
C16-1123,N16-1018,0,0.0371221,"Missing"
C16-1123,D10-1120,0,0.0614546,"pervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald"
C16-1123,P12-1066,0,0.127638,"e used to define the similarity between two languages with respect to the linguistic information one hopes to transfer; it can also help to define the optimal degree, level and method of transfer. For example, direct transfer of POS tagging is more likely to succeed when languages are sufficiently similar in terms of morphology in particular (Hana et al., 2004; Wisniewski et al., 2014). Typological information has been used to guide language transfer mostly in the areas of part-of-speech tagging and parsing, e.g. (Cohen and Smith, 2009; McDonald et al., 2011; Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2012; T¨ackstr¨om et al., 2013). Section 4 surveys such works in more detail. Multilingual Joint Learning Another approach involves learning information for multiple languages simultaneously, with the idea that the languages will be able to support each other (Snyder, 2010; Navigli and Ponzetto, 2012). This can help in the challenging but common scenario where none of the languages involved has adequate resources. This applies even with English, where annotations needed for training basic tools are primarily available only for newspaper texts and a handful of other domains. In some areas of NLP, e"
C16-1123,D12-1128,0,0.183062,"The field of linguistic typology offers valuable resources for nearing both of these theoretical ideals: it studies and classifies world’s languages according to their structural and functional features, with the aim of explaining both the common properties and the structural diversity of languages. Many of the current popular solutions to multilingual NLP: transfer of information from resource-rich to resource-poor languages (Pad´o and Lapata, 2005; Khapra et al., 2011; Das and Petrov, 2011; T¨ackstr¨om et al., 2012, inter alia), joint multilingual learning (Snyder, 2010; Cohen et al., 2011; Navigli and Ponzetto, 2012, inter alia), and development of universal models (de Marneffe et al., 2014; Nivre et al., 2016, inter alia), either assume or explicitly make use of information related to linguistic typology. While previous work has recognised the role of linguistic typology (Bender, 2011), no systematic survey of typological information resources and their use in NLP to date has been published. Given the growing need for multilingual NLP and the increased use of typological information in recent work, such a survey would be highly valuable in guiding further development. This paper provides such a survey f"
C16-1123,Q16-1030,0,0.052552,"uages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are commonly constructed manually by linguists. Despite the progress made in recent years in the digitisatio"
C16-1123,P15-2034,0,0.163715,"Missing"
C16-1123,H05-1108,0,0.170353,"Missing"
C16-1123,petrov-etal-2012-universal,0,0.245026,"2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperfo"
C16-1123,P16-1140,0,0.0255051,"rieval and knowledge transfer. The extent to which multilingual WEs capture word meaning across languages has been recently evaluated in (Leviant and Reichart, 2015) with the conclusion that multilingual training usually improves the alignment between the induced WEs and the meaning of the participating words in each of the involved languages. Naturally, as these models become more established and better understood, the challenge of external knowledge encoding becomes more prominent. Recent work has examined the ability to map from word embeddings to interpretable typological representations (Qian et al., 2016). Furthermore, a number of works (Faruqui et al., 2015; Rothe and Sch¨utze, 2015; Osborne et al., 2016; Mrkˇsi´c et al., 2016) proposed means through which external knowledge from structured knowledge bases and specialised linguistic resources can be encoded in these models. The success of these works suggests that more extensive integration of external linguistic knowledge in general, and typological knowledge in particular, is likely to play a key role in the future development of WE representations. 1304 Can NLP Support Typology Construction? As discussed in §4, typological resources are co"
C16-1123,N12-1008,1,0.912594,"t constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal feature"
C16-1123,D10-1067,1,0.759201,"es they are lacking altogether. Since resource creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual"
C16-1123,D11-1001,0,0.0132249,"mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs)"
C16-1123,P15-2040,0,0.0407258,"linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although parallel data can be used to giv"
C16-1123,P15-1173,0,0.0417279,"Missing"
C16-1123,D10-1001,0,0.014986,"t rules, soft constraints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learnin"
C16-1123,D12-1131,1,0.832511,"raints are helpful. Ultimately, an efficient mechanism for encoding soft constraints into the inference step is needed. Indeed, several modeling approaches have been proposed that do exactly this: constraint-driven learning (CODL) (Chang et al., 2007), posterior regularisation (PR) (Ganchev et al., 2010), generalized expectation (GE) (Mann and McCallum, 2008), and dual decomposition (Globerson and Jaakkola, 2007), among others. Such approaches have been applied successfully to various NLP tasks where external knowledge is available. Examples include POS tagging and parsing (Rush et al., 2010; Rush et al., 2012), information extraction (Riedel and McCallum, 2011; Reichart and Barzilay, 2012), and discourse analysis (Guo et al., 2013), among others. In addition to further extensions to the modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued"
C16-1123,P11-2120,0,0.0651153,"fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially developed (Petrov et al., 2012). Although para"
C16-1123,song-xia-2014-modern,0,0.0275956,"structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise these similarities for nearest neighbor prediction of typological features. The method evaluates competitively compared to baselines in the spirit of (Georgi et al., 2010) which rely on existing typological documentation of the target language for determining its nearest neighbors. In addition, a number of studies learned typological information tailored to the particular task and data at hand (i.e. task-based development). For example, Song and Xia (2014) process Ancient Chinese using Modern Chinese parsing resources. They manually identify and address statistical patterns in variation between monolingual corpora in each language, and ultimately optimise the model performance by selectively using only the Modern Chinese features which correspond to Ancient Chinese features. Although automatically-learned typological classifications have not been used frequently to date, they hold great promise for extending the use of typological information in NLP. Furthermore, such work offers an additional axis of interaction between linguistic typology and"
C16-1123,D11-1117,0,0.0144064,"ce creation is expensive and cannot be realistically carried out for all tasks in all languages, much recent research in multilingual NLP has investigated ways of overcoming the resource problem. One avenue of research that aims to solve this problem has been unsupervised learning, which exploits unlabelled data that is now available in multiple languages. Over the past two decades increasingly sophisticated unsupervised methods have been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of unive"
C16-1123,N12-1052,0,0.0971877,"Missing"
C16-1123,N13-1126,0,0.219991,"Missing"
C16-1123,L16-1011,0,0.044567,"increase the coverage of typological information using existing information in typological databases. Daum´e III and Campbell (2007) and Bakker (2008) use existing WALS features to learn typological implications of the kind pioneered by Greenberg (1963). Such rules can then be used to predict unknown feature values for new languages. Georgi et al. (2010) use documented WALS features to cluster languages, and subsequently predict new feature values using nearest-neighbour projection. A classifier-based approach for predicting new feature values from documented WALS information is presented in (Takamura et al., 2016). Coke et al. (2016) predict word order typological features by combining documented typological and genealogical features with the multilingual alignment approach discussed above. An alternative approach for learning typological information uses English as a Second Language (ESL) texts (Berzak et al., 2014). This work demonstrates that morphosyntactic typological similarities between languages are largely preserved in second language structural usage. It leverages this observation to approximate typological similarities between languages directly from ESL usage patterns and further utilise th"
C16-1123,N16-1161,0,0.058165,"nological Modeling and Language Learning Besides dependency parsing, several other areas have started integrating typological information in various forms. A number of such works revolve around the task of POS tagging. For example, in Zhang et al. (2012), the previously discussed WBO features were used to inform mappings from language-specific to a universal POS tagset. In (Zhang et al., 2016), WBO feature values are used to evaluate the quality of a multilingual POS tagger. Another application area which benefited from integration of typological knowledge are phonological models of text. In (Tsvetkov et al., 2016) a multilingual neural phoneme-based language model is trained on several languages using a shared phonological inventory. The model is conditioned on the identity of the language at hand, as well as its phonological features obtained from a concatenation of phonological features from WALS, PHOIBLE and Ethnologue, extracted from URIEL. The resulting model subsumes and outperforms monolingually trained models for phone sequence prediction. Deri and Knight (2016) use URIEL to obtain phone and language similarity metrics, which are used for adjusting Grapheme to Phoneme (G2P) models from resource"
C16-1123,P10-1040,0,0.0148561,"he modeling approaches surveyed in §4.2, these type of frameworks could expedite principled integration of typological information in NLP. Typologies and Multilingual Representation Learning While the traditional models surveyed above assume a predefined feature representation and focus on generating the best prediction of the output labels, a large body of recent NLP research has focused on learning dense real-valued vector representations —i.e., word embeddings (WEs). WEs serve as pivotal features in a range of downstream NLP tasks such as parsing, named entity recognition, and POS tagging (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The extensions of WE models in bilingual and multilingual settings (Klementiev et al., 2012; Hermann and Blunsom, 2014; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia) abstract over language-specific features and attempt to represent words from several languages in a languageagnostic manner such that similar words (regardless of the actual language) obtain similar representations. Such multilingual WEs facilitate cross-lingual learning, information retrieval and knowledge transfer. The extent to which multilingual WEs capture word"
C16-1123,D14-1187,0,0.0449846,"Missing"
C16-1123,I08-3008,0,0.365977,"ransfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning, it does not achieve optimal performance. One potential reason for this is that the tagset used by a POS tagger may not fit a target language which exhibits significantly different morphological features to a source language for which the tagset was initially develope"
C16-1123,D15-1213,0,0.349778,"al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petrov et al., 2012; Zhang and Barzilay, 2015). It has been particularly popular in recent research on dependency parsing, where a variety of methods have been explored. For example, most work for resource-poor languages has combined delexicalised parsing with cross-lingual transfer (e.g. (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011; Rosa and Zabokrtsky, 2015)). Here, a delexicalised parser is first trained on a resource-rich source language, with both languages POS-tagged using the same tagset, and then applied directly to a resource-poor target language. While such a transfer approach outperforms unsupervised learning,"
C16-1123,D12-1125,1,0.921529,"been developed and applied to a variety of tasks and in some cases also to multiple languages (Cohen and Smith, 2009; Reichart and Rappoport, 2010; Snyder, 2010; Spitkovsky et al., 2011; Goldwasser et al., 2011; Baker et al., 2014, inter alia). However, while purely unsupervised approaches are appealing in side-stepping the resource problem, their relatively low performance has limited their practical usefulness (T¨ackstr¨om et al., 2013). More success has been gained with solutions that use some form of supervision or guidance to enable NLP for less-resourced languages (Naseem et al., 2010; Zhang et al., 2012; T¨ackstr¨om et al., 2013, inter alia). In what follows, we consider three such solutions: language transfer, joint multilingual learning, and the development of universal models. We discuss the guidance employed in each, paying particular attention to typological guidance. Language Transfer This very common approach exploits the fact that rich linguistic resources do exist for some languages. The idea is to use them for less-resourced languages via data (i.e. parallel corpora) and/or model transfer. This approach has been explored widely in NLP (Hwa et al., 2005; McDonald et al., 2011; Petro"
C16-1123,N16-1156,0,0.153027,"databases reviewed in §2; and ii) automatic learning. The two 1300 methods have been used independently and in combination, and both are based on the assumption (be it explicit or implicit) that typological relations may be fruitfully used in NLP. Manual Extraction from Linguistic Resources Manually crafted linguistic resources – in particular the WALS database – have been the most commonly used sources of typological information in NLP. To date, syntactic parsing (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Ammar et al., 2016) and POS tagging (Zhang et al., 2012; Zhang et al., 2016) were the predominant areas for integration of structural information from such databases. In the context of these tasks, the most frequently used features related to word ordering according to coarse syntactic categories. Additional areas with emerging research which leverages externally-extracted typological features are phonological modeling (Tsvetkov et al., 2016; Deri and Knight, 2016) and language learning (Berzak et al., 2015). While information obtained from typological databases has been successfully integrated in several NLP tasks, a number of challenges remain. Perhaps the most cruc"
C16-1123,L16-1262,0,\N,Missing
D13-1168,J10-4006,0,0.0293563,"le 2: IT-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number of dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB. are therefore not always able to push the real crosslingual synonyms higher in the ranked list of semantically similar words, while the window-based bootstrapping approach is better tailored to model the relation of cross-lingual synonymy, i.e., to extract one-to-one translation pairs (as reflected in Acc1 scores). A similar conclusion for monolingual settings is drawn by Baroni and Lenci (2010). (iv) Since our bootstrapping approach utilizes ResponseBC or TopicBC as a preprocessing step, it is obvious that the approach leads to an increased complexity. On top of the initial complexity of ResponseBC and TopicBC, the bootstrapping method requires |V S ||V T |comparisons at each iteration, but given the fact that each wiS ∈ V S may be processed independently of any other wjS ∈ V S in each iteration, the bootstrapping method is trivially parallelizable. That makes the method computationally feasible even for vocabularies larger than the ones reported in the paper. 1621 4.2 Is Confidence"
D13-1168,P11-1061,0,0.0250393,"retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context feat"
D13-1168,P11-2071,0,0.0295884,"Missing"
D13-1168,D12-1001,0,0.0292423,"le source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional v"
D13-1168,R11-1018,0,0.0377545,"Missing"
D13-1168,W04-3208,0,0.0640861,"er to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a sm"
D13-1168,P98-1069,0,0.0380592,"ch as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context features. The context features are typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck"
D13-1168,P04-1067,0,0.252236,"Missing"
D13-1168,P08-1088,0,0.793174,"ational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon. The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn and Knight, 2002; Peirsman and Pad´o, 2010), and it spans the initial bilingual vector space. The space is then gradually enriched with new dimensions/axes during the bootstrapping procedure. The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as S"
D13-1168,E12-1029,0,0.0130607,"constraint should ensure a relative reliability of translation pairs. In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs. We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg. 1), and choose only the best B candidates from the pool in each iteration (step 4) as done in (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Huang and Riloff, 2012). By picking only a subset of the B most confident candidates in each iteration, we hope to further prevent a possibility of semantic drift, i.e., “poisoning” the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space. In this paper, we investigate 3 different confidence estimation functions:3 (1) Absolute similarity score. Confidence of a translation pair CF (wiS , T C(wiS )) is simply the absolute similarity value sim(wiS , T C(wiS )) (2) M-Best confidence function. It contrasts the score of the translation candidate with the average scor"
D13-1168,C10-2055,0,0.0142854,"representations in the N -dimensional bilingual vector space. The cross-lingual similarity is computed following the standard procedure (Gaussier et al., 2004): (1) For each source word wiS ∈ V S , build its N dimensional context vector cv(wiS ) that consists of association scores scSk (cSk ), that is, we compute the strength of association with the “source” part of each dimension ck that constitutes the N -dimensional bilingual space. The association is dependent on the co-occurrence of wiS and cSk in a predefined context. Various functions such as the log-likelihood ratio (LLR) (Rapp, 1999; Ismail and Manandhar, 2010), TF-IDF (Fung and Yee, 1998), or pointwise mutual information (PMI) (Bullinaria and Levy, 2007; Shezaf and Rappoport, 2010) are typically used as weighting functions to quantify the strength of the association. (2) Repeat step (1) for each target word wjT ∈ V T and build context vectors cv(wjT ) that consist of scores scTk (cTk ). (3) Since cSk and cTk address the same dimension ck in the bilingual vector space for each k = 1, . . . , N , we are able to compute the similarity between cv(wiS ) and cv(wjT ) using any similarity measure such as the Jaccard index, the Kullback-Leibler or the Jens"
D13-1168,P10-1026,0,0.0200566,"Missing"
D13-1168,W02-0902,0,0.221947,"sume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon. The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn and Knight, 2002; Peirsman and Pad´o, 2010), and it spans the initial bilingual vector space. The space is then gradually enriched with new dimensions/axes during the bootstrapping procedure. The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as Spanish-Portuguese or Croatian-Slovene (Fiˇser and Ljubeˇsi´c, 2011), but it still lacks further generalization to more distant language pairs. The main goal of this paper is to shed new light on the bootstrapping approaches to bilingual lexicon extraction, and to construct a language pair agnost"
D13-1168,N10-1087,0,0.0157337,"at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones. For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter. 2.2 Initializing Bilingual Vector Spaces frequent symmetric pairs as seeds. Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure. However, Peirsman and Pad´o (2011) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces. Here, we present two different strategies of initializing the bilingual vector space. Identical words and cognates. Previous work relies exclusively on identical and similarly spelled words to build the initial set of dimensions Z0 (Koehn and Knight, 2002; Peirsman and Pad´o, 2010; Fiˇser and Ljubeˇsi´c, 2011). This strategy yields promising results for closely similar lang"
D13-1168,C10-1070,0,0.0141025,"ck-Leibler, Jensen-Shannon) (Lee, 1999; Turney and Pantel, 2010). In this paper, we present results obtained by positive pointwise mutual information (PPMI) (Niwa and Nitta, 1994) as a weighting function, which is a standard choice in vector space semantics (Turney and Pantel, 2010), and (combined with cosine) yields the best results over a group of semantic tasks according to (Bullinaria and Levy, 2007). We use a smoothed version of PPMI as presented in (Pantel and Lin, 2002; Turney and Pantel, 2010). Again, based on the results reported in the relevant literature (Bullinaria and Levy, 2007; Laroche and Langlais, 2010; Turney and Pantel, 2010), we opt for the cosine similarity as a standard choice for SF . We have also experimented with different window sizes ranging from 3 to 15 in both directions around the pivot word, but we have not detected any major qualitative difference in the results and their interpretation. Therefore, all results reported in the paper are obtained by setting the window size to 6. 4 4.1 Results and Discussion Are Seeds Important? In recent work, Peirsman and Pad´o (2010; 2011) report that “the size and quality of the (seed) lex1618 icon are not of primary importance given that th"
D13-1168,P99-1004,0,0.0935525,"typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors (Lee, 1999). When operating with 2 languages, the context features cannot be compared directly. Therefore, in order to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. T"
D13-1168,P09-1045,0,0.0606458,"y of the space to increase at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones. For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter. 2.2 Initializing Bilingual Vector Spaces frequent symmetric pairs as seeds. Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure. However, Peirsman and Pad´o (2011) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces. Here, we present two different strategies of initializing the bilingual vector space. Identical words and cognates. Previous work relies exclusively on identical and similarly spelled words to build the initial set of dimensions Z0 (Koehn and Knight, 2002; Peirsman and Pad´o, 2010; Fiˇser and Ljubeˇsi´c, 2011). This strategy yields promising result"
D13-1168,J00-2004,0,0.0133663,"mework We assume that we are solely in possession of a (non-parallel) bilingual corpus C that is composed of a sub-corpus CS given in the source language LS , and a sub-corpus CT in the target language LT . All word types that occur in CS constitute a set V S . All word types in CT constitute a set V T . The goal is to build a bilingual vector space using only corpus C. Assumption 1. Dimensions of the bilingual vector space are one-to-one word translation pairs. For instance, dimensions of a Spanish-English space are pairs like (perro, dog), (ciencia, science), etc. The one-to-one constraint (Melamed, 2000), although not valid in general, simplifies the construction of the bootstrapping procedure. Z denotes the set of translation pairs that are the dimensions of the space. Computing cross-lingual word similarity in a bilingual vector space. Now, assume that our bilingual vector space consists of N one-to-one word translation pairs ck = (cSk , cTk ), k = 1, . . . , N . For each word wiS ∈ V S , we compute the similarity of that word with each word wjT ∈ V T by computing the similarity between their context vectors cv(wiS ) and cv(wjT ), which are actually their representations in the N -dimension"
D13-1168,D09-1092,0,0.173224,"of limited use for other language pairs. High-frequency seeds. Another problem with using only identical words and cognates as seeds lies in the fact that many of them might be infrequent in the corpus, and as a consequence the expressiveness of a bilingual vector space might be limited. On the other hand, high-frequency words offer a lot of evidence in the corpus that could be exploited in the bootstrapping approach. In order to induce initial translation pairs, we rely on the framework of multilingual probabilistic topic modeling (MuPTM) (BoydGraber and Blei, 2009; De Smet and Moens, 2009; Mimno et al., 2009; Zhang et al., 2010), that does not require a bilingual lexicon, it operates with nonparallel data, and is able to produce highly confident translation pairs for high-frequency words (Mimno et al., 2009; Vuli´c and Moens, 2013).2 Therefore, we can construct the initial seed lexicon as follows: (1) Train a multilingual topic model on the corpus. (2) Obtain one-to-one translation pairs using any of the MuPTM-based models of cross-lingual similarity, e.g., (Vuli´c et al., 2011; Vuli´c and Moens, 2013). (3) Retain only symmetric translation pairs. This step ensures that only highly confident pair"
D13-1168,C94-1049,0,0.138401,"ails about the methods in the relevant literature. These two models also serve as our baseline models, and our goal is to test whether we are able to obtain bilingual lexicons of higher quality using bootstrapping that starts from the output of these models. Weighting and similarity functions. We have experimented with different families of weighting (e.g., PMI, LLR, TF-IDF, chi-square) and similarity functions (e.g., cosine, Dice, Kullback-Leibler, Jensen-Shannon) (Lee, 1999; Turney and Pantel, 2010). In this paper, we present results obtained by positive pointwise mutual information (PPMI) (Niwa and Nitta, 1994) as a weighting function, which is a standard choice in vector space semantics (Turney and Pantel, 2010), and (combined with cosine) yields the best results over a group of semantic tasks according to (Bullinaria and Levy, 2007). We use a smoothed version of PPMI as presented in (Pantel and Lin, 2002; Turney and Pantel, 2010). Again, based on the results reported in the relevant literature (Bullinaria and Levy, 2007; Laroche and Langlais, 2010; Turney and Pantel, 2010), we opt for the cosine similarity as a standard choice for SF . We have also experimented with different window sizes ranging"
D13-1168,J03-1002,0,0.0115179,"ping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets. 1 Introduction Bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction"
D13-1168,N10-1135,0,0.221083,"Missing"
D13-1168,P11-1133,0,0.0215868,"idence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki-ES-EN), (ii) 18, 898 Italian-English Wikipedia article pairs (Wiki-IT-EN).4 Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we use TreeTagger (Schmid, 1994) for POS-tagging and lemmatization of the corpora, and then retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Our final vocabularies consist of 9, 439 Spanish nouns and 4 Vuli´c and Moens (2013) also worked with Dutch-English (NL-EN), but we have decided to leave out the results obtained for that language pair due to space constraints, high similarity between the two languages, and the fact that the results obtained for that language pair are qual"
D13-1168,P99-1067,0,0.108932,"abeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context features. The context features are typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck , where scS1"
D13-1168,P10-1011,0,0.0331304,"d procedure (Gaussier et al., 2004): (1) For each source word wiS ∈ V S , build its N dimensional context vector cv(wiS ) that consists of association scores scSk (cSk ), that is, we compute the strength of association with the “source” part of each dimension ck that constitutes the N -dimensional bilingual space. The association is dependent on the co-occurrence of wiS and cSk in a predefined context. Various functions such as the log-likelihood ratio (LLR) (Rapp, 1999; Ismail and Manandhar, 2010), TF-IDF (Fung and Yee, 1998), or pointwise mutual information (PMI) (Bullinaria and Levy, 2007; Shezaf and Rappoport, 2010) are typically used as weighting functions to quantify the strength of the association. (2) Repeat step (1) for each target word wjT ∈ V T and build context vectors cv(wjT ) that consist of scores scTk (cTk ). (3) Since cSk and cTk address the same dimension ck in the bilingual vector space for each k = 1, . . . , N , we are able to compute the similarity between cv(wiS ) and cv(wjT ) using any similarity measure such as the Jaccard index, the Kullback-Leibler or the Jensen-Shannon divergence, the cosine measure, or others (Lee, 1999; Cha, 2007). The similarity score for two words wiS and wjT"
D13-1168,D07-1070,0,0.0610707,"r words in the ranked list. The larger the difference, the more confidence we have in the translation candidate. Given a word wiS ∈ V S and a ranked list RLM (wiS ), the 3 A symmetrized version of the confidence functions is computed as the geometric mean of source-to-target and target-tosource confidence scores. average score of the best M words is computed as: simM (wiS ) = 1 M X sim(wiS , wjT ) wjT ∈RLM (wiS ) The final confidence score is then: CF (wiS , T C(wiS )) = sim(wiS , T C(wiS )) − simM (wiS ) (3) Entropy-based confidence function. We adapt the well-known entropy-based confidence (Smith and Eisner, 2007; Tu and Honavar, 2012) to this particular task. First, we need to define a distribution: S p(wjT |wiS ) =P T esim(wi ,wj ) S wlT ∈V T T) esim(wi ,wl The confidence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipe"
D13-1168,Q13-1001,0,0.0493112,"Missing"
D13-1168,N13-1126,0,0.00893387,"Missing"
D13-1168,D12-1003,0,0.289249,"features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors (Lee, 1999). When operating with 2 languages, the context features cannot be compared directly. Therefore, in order to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore,"
D13-1168,W02-1028,0,0.0291222,"T C(wiS ) = wjT and T C(wjT ) = wiS . This symmetry constraint should ensure a relative reliability of translation pairs. In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs. We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg. 1), and choose only the best B candidates from the pool in each iteration (step 4) as done in (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Huang and Riloff, 2012). By picking only a subset of the B most confident candidates in each iteration, we hope to further prevent a possibility of semantic drift, i.e., “poisoning” the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space. In this paper, we investigate 3 different confidence estimation functions:3 (1) Absolute similarity score. Confidence of a translation pair CF (wiS , T C(wiS )) is simply the absolute similarity value sim(wiS , T C(wiS )) (2) M-Best confidence function. It contrasts the scor"
D13-1168,D12-1121,0,0.014041,"st. The larger the difference, the more confidence we have in the translation candidate. Given a word wiS ∈ V S and a ranked list RLM (wiS ), the 3 A symmetrized version of the confidence functions is computed as the geometric mean of source-to-target and target-tosource confidence scores. average score of the best M words is computed as: simM (wiS ) = 1 M X sim(wiS , wjT ) wjT ∈RLM (wiS ) The final confidence score is then: CF (wiS , T C(wiS )) = sim(wiS , T C(wiS )) − simM (wiS ) (3) Entropy-based confidence function. We adapt the well-known entropy-based confidence (Smith and Eisner, 2007; Tu and Honavar, 2012) to this particular task. First, we need to define a distribution: S p(wjT |wiS ) =P T esim(wi ,wj ) S wlT ∈V T T) esim(wi ,wl The confidence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki"
D13-1168,P11-2052,0,0.0339269,"Missing"
D13-1168,N13-1011,1,0.838398,"Missing"
D13-1168,P11-2084,1,0.789296,"Missing"
D13-1168,N01-1026,0,0.0748191,"cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vecto"
D13-1168,P10-1115,0,0.0488981,"Missing"
D13-1168,P09-1007,0,0.010773,"e as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word b"
D13-1168,C98-1066,0,\N,Missing
D14-1040,J12-1002,0,0.0134765,"rm way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context"
D14-1040,P11-1061,0,0.0131515,"nd Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these mode"
D14-1040,W11-2203,0,0.0217513,"n the Spanish sentence ”She was unable to find a match in her pocket to light up a cigarette.”, it is clear that the strength of semantic similarity should change in context as only cerilla exhibits a strong semantic similarity to match within this particular sentential context. Following this intuition, in this paper we investigate models of cross-lingual semantic similarity in context. The context-sensitive models of similarity target to re-rank the lists of semantically similar words based on the co-occurring contexts of words. Unlike prior work (e.g., (Ng et al., 2003; Prior et al., 2011; Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cross-lingual concepts (Haghighi et al.,"
D14-1040,P11-2071,0,0.0540797,"Missing"
D14-1040,D10-1115,0,0.0326918,"hen compute the similarity between words and sets of words given in the same latent semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk"
D14-1040,D12-1050,0,0.0166891,"semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to p"
D14-1040,D10-1113,0,0.0535594,", and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic space spanned by latent cross-lingual concepts in both monolingual (Dinu and Lapata, 2010a; Dinu and Lapata, 2010b) and multilingual settings (Vuli´c et al., 2011) typically proceed in the following manner. Latent language-independent concepts (e.g., cross-lingual topics or latent word senses) are estimated on a large corpus. The K-dimensional vector representation of the word w1S ∈ V S is: vec(w1S ) = [P (z1 |w1S ), . . . , P (zK |w1S )] By using all words occurring with w1S in a context set (e.g., a sentence) to build the set Con(w1S ), we do not make any distinction between “informative and “uninformative” context words. However, some context words bear more contextual informat"
D14-1040,C10-2029,0,0.19433,", and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic space spanned by latent cross-lingual concepts in both monolingual (Dinu and Lapata, 2010a; Dinu and Lapata, 2010b) and multilingual settings (Vuli´c et al., 2011) typically proceed in the following manner. Latent language-independent concepts (e.g., cross-lingual topics or latent word senses) are estimated on a large corpus. The K-dimensional vector representation of the word w1S ∈ V S is: vec(w1S ) = [P (z1 |w1S ), . . . , P (zK |w1S )] By using all words occurring with w1S in a context set (e.g., a sentence) to build the set Con(w1S ), we do not make any distinction between “informative and “uninformative” context words. However, some context words bear more contextual informat"
D14-1040,J93-2003,0,0.070668,"oven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in the target language in isolation or invariably, that is, they do not explicitly idenWe propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words an"
D14-1040,D12-1001,0,0.0145737,"S models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingua"
D14-1040,P14-1006,0,0.0280069,"e properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent semantic space (and again assume the uniform topic prior P"
D14-1040,D13-1205,0,0.0129757,"models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ran"
D14-1040,P12-1092,0,0.0246423,"istic topic models output probability scores P (wiS |zk ) and P (wjT |zk ) for each wiS ∈ V S and wjT ∈ V T and each zk ∈ P S Z, and it holds wiS ∈V S P (wi |zk ) = 1 and P T wT ∈V T P (wj |zk ) = 1. The scores are then Defining Context. Given an occurrence of a word w1S , we build its context set Con(w1S ) = {cw1S , . . . , cwrS } that comprises r words from V S that co-occur with w1S in a defined contextual scope or granularity. In this work we do not investigate the influence of the context scope (e.g., document-based, paragraph-based, window-based contexts). Following the recent work from Huang et al. (2012) in the monolingual setting, we limit the contextual scope to the sentential context. However, we emphasize that the proposed models are designed to be fully functional regardless of the actual chosen context granularity. e.g., when operating in the sentential context, Con(w1S ) consists of words occurring in the same sentence with the particular instance of w1S . Following Mitchell and Lapata (2008), for the sake of simplicity, we impose the bag-of-words assumption, and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Inves"
D14-1040,P04-1067,0,0.161912,"Missing"
D14-1040,J00-4006,0,0.0197136,"to their respective similarity scores and the best scoring candidate may be selected as the best translation of an occurrence of the word w1S given its local context. Since the contextual knowledge is integrated directly into the estimation of probability P (zk |w1S , Con(w1S )), we name this context-aware CLSS model the Direct-Fusion model. Model II: Smoothed-Fusion. The next model follows the modeling paradigm established within the framework of language modeling (LM), where the idea is to “back off” to a lower order Ngram in case we do not possess any evidence about a higher-order N-gram (Jurafsky and Martin, 2000). The idea now is to smooth the representation of a word in the latent semantic space induced only by the words in its local context with the out-of-context type-based representation of that word induced directly from a large training corpus. In other words, the modulated probability score P 0 (zk |w1S ) from eq. (5) is calculated as: vec(w1S , Con(w1S )) = [P 0 (z1 |w1S ), . . . , P 0 (zK |w1S )] (5) where P 0 (zK |w1S ) denotes the recalculated (or modulated) probability score for the conditional concept/topic distribution of w1S after observing its context Con(w1S ). For an illustration of"
D14-1040,P10-1026,0,0.0441824,"Missing"
D14-1040,D11-1129,0,0.0492216,"Missing"
D14-1040,P12-1073,0,0.0127399,"ne language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in the target language in isolation or invariably, that is, they do not explicitly idenWe propose the first"
D14-1040,C12-1089,0,0.0226263,"ence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent semantic space (and again assume the uniform topic prior P (zk )), we finally obtain the following formula: text units beyond words is similar to (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), but unlike them, we do not need high-quality sentence-aligned parallel data to induce bilingual text representations. Moreover, this work on compositionality in multilingual settings is only preliminary (e.g., we treat phrases and sentences as bags-of-words), and in future work we will aim to include syntactic information in the composition models as already done in monolingual settings (Socher et al., 2012; Hermann and Blunsom, 2013). Intuition behind the Approach. Going back to our novel CLSS models in context, these models rely on the re"
D14-1040,P08-1088,0,0.27697,"Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cross-lingual concepts (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013) induced from non-parallel data. While all the previous models in this framework are context-insensitive models of semantic similarity, we demonstrate how to build context-aware models of semantic similarity within the same probabilistic framework which relies on the same shared set of latent concepts. The main contributions of this paper are: 2 Towards Cross-Lingual Semantic Similarity in Context Latent Cross-Lingual Concepts. Latent crosslingual concepts/senses may be interpreted as language-independent semantic"
D14-1040,W02-0902,0,0.0220362,"sh, Italian and Dutch from our test set accompanied by the sets of their respective possible senses/translations in English. All corpora are theme-aligned comparable corpora, i.e, the aligned document pairs discuss similar themes, but are in general not direct translations (except for Europarl). By training on Wiki+EP-NL-EN we want to test how the training corpus of higher quality affects the estimation of latent cross-lingual concepts that span the shared latent semantic space and, consequently, the overall results in the task of suggesting word translations in context. Following prior work (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we retain only nouns that occur at least 5 times in the corpus. We record lemmatized word forms when available, and original forms otherwise. We use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in English (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible translations) in each of the"
D14-1040,D09-1124,0,0.0285564,"similar to the model from O and Korhonen (2011) in the monolingual setting, one may try to introduce dependency-based contexts (Pad´o and Lapata, 2007) and incorporate the syntax-based knowledge in the context-aware CLSS modeling. It is also worth studying other models that induce latent semantic concepts from multilingual data (see sect. 2) within this framework of context-sensitive CLSS modeling. One may also investigate a similar approach to contextsensitive CLSS modeling that could operate with explicitly defined concept categories (Gabrilovich and Markovitch, 2007; Cimiano et al., 2009; Hassan and Mihalcea, 2009; Hassan and Mihalcea, 2011; McCrae et al., 2013). Acc1 0.75 0.7 0.65 ES-EN IT-EN NL-EN (Wiki) 0.6 NL-EN (Wiki+EP) 0.55 1 2 3 4 5 6 7 8 9 10 11 All Size of the ranked context Figure 2: The influence of the size of sorted context on the accuracy of word translation in context. The model is Cue+Smoothed-Fusion. also investigate the utility of context sorting and pruning, and its influence on the overall results in our evaluation task. Therefore, we have conducted experiments with sorted context sets that were pruned at different positions, ranging from 1 (only the most similar word to w1S in a s"
D14-1040,2005.mtsummit-papers.11,0,0.0517389,"st is then the suggested correct translation for that particular occurrence of w1S after observing its local context Con(w1S ). Training Data. We use the following corpora for inducing latent cross-lingual concepts/topics, i.e., for training our multilingual topic model: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki-ES-EN), (ii) a collection of 18, 898 Italian-English Wikipedia article pairs, (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (Wiki-NL-EN), and (iv) the Wiki-NLEN corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005) (Wiki+EP-NL-EN). The corpora were previously used in (Vuli´c and Moens, 2013). No explicit use is made of sentence-level alignments in Europarl. (10) where λ2 is the interpolation parameter. Since this model computes the similarity with each target word separately for the source word in isolation and its local context, and combines the ob354 Sentence in Italian 1. I primi calci furono prodotti in legno ma recentemente... 2. In caso di osteoporosi si verifica un eccesso di rilascio di calcio dallo scheletro... 3. La crescita del calcio femminile professionistico ha visto il lancio di competizi"
D14-1040,P14-2037,0,0.0837968,"Missing"
D14-1040,P13-1088,0,0.0147807,"uniform topic prior P (zk )), we finally obtain the following formula: text units beyond words is similar to (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), but unlike them, we do not need high-quality sentence-aligned parallel data to induce bilingual text representations. Moreover, this work on compositionality in multilingual settings is only preliminary (e.g., we treat phrases and sentences as bags-of-words), and in future work we will aim to include syntactic information in the composition models as already done in monolingual settings (Socher et al., 2012; Hermann and Blunsom, 2013). Intuition behind the Approach. Going back to our novel CLSS models in context, these models rely on the representations of words and their contexts in the same latent semantic space spanned by latent cross-lingual concepts/topics. The models differ in the way the contextual knowledge is fused with the out-of-context word representations. The key idea behind these models is to represent a word w1S in the latent semantic space as a distribution over the latent cross-lingual concepts, but now with an additional modulation of the representation after taking its local context into account. The mo"
D14-1040,D11-1097,0,0.0397802,"Missing"
D14-1040,J03-1002,0,0.0118243,"pts Induced from Comparable Data Ivan Vuli´c and Marie-Francine Moens Department of Computer Science KU Leuven, Belgium {ivan.vulic|marie-francine.moens}@cs.kuleuven.be Abstract output of the CLSS models is a key resource in the models of dictionary-based cross-lingual information retrieval (Ballesteros and Croft, 1997; Lavrenko et al., 2002; Levow et al., 2005; Wang and Oard, 2006) or may be utilized in query expansion in cross-lingual IR models (Adriani and van Rijsbergen, 1999; Vuli´c et al., 2013). These CLSS models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petr"
D14-1040,J07-2002,0,0.0161936,"Missing"
D14-1040,P99-1004,0,0.0211846,"modulation of the representation after taking its local context into account. The modulated word representation in the semantic space spanned by K latent cross-lingual concepts is then: Q P (w1S |zk ) rj=1 P (cwjS |zk ) P 0 (zk |w1S ) ≈ PK Qr S S j=1 P (cwj |zl ) l=1 P (w1 |zl ) The ranking of all words w2T ∈ V T according to their similarity to w1S may be computed by detecting the similarity score between their representation in the K-dimensional latent semantic space and the modulated source word representation as given by eq. (5) and eq. (7) using any of the existing similarity functions (Lee, 1999; Cha, 2007). The similarity score Sim(w1S , w2T , Con(w1S )) between some w2T ∈ V T represented by its vector vec(w2T ) and the observed word w1S given its context Con(w1S ) is computed as: sim(w1S , w2T , Con(w1S ))    = SF vec w1S , Con(w1S ) , vec w2T (8) where SF denotes a similarity function. Words are then ranked according to their respective similarity scores and the best scoring candidate may be selected as the best translation of an occurrence of the word w1S given its local context. Since the contextual knowledge is integrated directly into the estimation of probability P (zk |w"
D14-1040,N10-1135,0,0.184797,"Missing"
D14-1040,D13-1179,0,0.0224558,"e monolingual setting, one may try to introduce dependency-based contexts (Pad´o and Lapata, 2007) and incorporate the syntax-based knowledge in the context-aware CLSS modeling. It is also worth studying other models that induce latent semantic concepts from multilingual data (see sect. 2) within this framework of context-sensitive CLSS modeling. One may also investigate a similar approach to contextsensitive CLSS modeling that could operate with explicitly defined concept categories (Gabrilovich and Markovitch, 2007; Cimiano et al., 2009; Hassan and Mihalcea, 2009; Hassan and Mihalcea, 2011; McCrae et al., 2013). Acc1 0.75 0.7 0.65 ES-EN IT-EN NL-EN (Wiki) 0.6 NL-EN (Wiki+EP) 0.55 1 2 3 4 5 6 7 8 9 10 11 All Size of the ranked context Figure 2: The influence of the size of sorted context on the accuracy of word translation in context. The model is Cue+Smoothed-Fusion. also investigate the utility of context sorting and pruning, and its influence on the overall results in our evaluation task. Therefore, we have conducted experiments with sorted context sets that were pruned at different positions, ranging from 1 (only the most similar word to w1S in a sentence is included in the context set Con(w1S ))"
D14-1040,P02-1027,0,0.0207223,"ingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in"
D14-1040,P11-1133,0,0.0202546,"nied by the sets of their respective possible senses/translations in English. All corpora are theme-aligned comparable corpora, i.e, the aligned document pairs discuss similar themes, but are in general not direct translations (except for Europarl). By training on Wiki+EP-NL-EN we want to test how the training corpus of higher quality affects the estimation of latent cross-lingual concepts that span the shared latent semantic space and, consequently, the overall results in the task of suggesting word translations in context. Following prior work (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we retain only nouns that occur at least 5 times in the corpus. We record lemmatized word forms when available, and original forms otherwise. We use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in English (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible translations) in each of the 3 languages, and have manually extracted 24 sente"
D14-1040,D09-1092,0,0.527947,"Missing"
D14-1040,D10-1114,0,0.0419217,"Missing"
D14-1040,P08-1028,0,0.0688078,"fined contextual scope or granularity. In this work we do not investigate the influence of the context scope (e.g., document-based, paragraph-based, window-based contexts). Following the recent work from Huang et al. (2012) in the monolingual setting, we limit the contextual scope to the sentential context. However, we emphasize that the proposed models are designed to be fully functional regardless of the actual chosen context granularity. e.g., when operating in the sentential context, Con(w1S ) consists of words occurring in the same sentence with the particular instance of w1S . Following Mitchell and Lapata (2008), for the sake of simplicity, we impose the bag-of-words assumption, and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic s"
D14-1040,P10-1093,0,0.0173152,", P (zK |Con(w1S ))] We can then compute the similarity between words and sets of words given in the same latent semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Sin"
D14-1040,P03-1058,0,0.0111603,"atch when observed in isolation, given the Spanish sentence ”She was unable to find a match in her pocket to light up a cigarette.”, it is clear that the strength of semantic similarity should change in context as only cerilla exhibits a strong semantic similarity to match within this particular sentential context. Following this intuition, in this paper we investigate models of cross-lingual semantic similarity in context. The context-sensitive models of similarity target to re-rank the lists of semantically similar words based on the co-occurring contexts of words. Unlike prior work (e.g., (Ng et al., 2003; Prior et al., 2011; Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cr"
D14-1040,D12-1110,0,0.0822891,"ective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent seman"
D14-1040,C08-1125,0,0.0541665,"Missing"
D14-1040,N01-1026,0,0.0133772,"dge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention re"
D14-1040,Q13-1001,0,0.025178,"Missing"
D14-1040,N13-1126,0,0.0221236,"Missing"
D14-1040,P10-1115,0,0.386221,"ng set and do not take into account any contextual information. They provide only out-of-context word representations and are therefore able to deliver only context-insensitive models of similarity. factorization (Lee and Seung, 1999; Gaussier and Goutte, 2005; Ding et al., 2008) on concatenated documents in aligned document pairs. Other more recent models include matching canonical correlation analysis (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011) and multilingual probabilistic topic models (Ni et al., 2009; De Smet and Moens, 2009; Mimno et al., 2009; Boyd-Graber and Blei, 2009; Zhang et al., 2010; Fukumasu et al., 2012). Due to its inherent language pair independent nature and state-of-the-art performance in the tasks such as bilingual lexicon extraction (Vuli´c et al., 2011) and cross-lingual information retrieval (Vuli´c et al., 2013), the description in this paper relies on the multilingual probabilistic topic modeling (MuPTM) framework. We draw a direct parallel between latent cross-lingual concepts and latent cross-lingual topics, and we present the framework from the MuPTM perspective, but the proposed framework is generic and allows the usage of all other models that are able t"
D14-1040,D12-1003,0,0.0126294,"suggesting word translations in context. Evaluation Procedure. Our task is to present the system a list of possible translations and let the system decide a single most likely translation given the word and its sentential context. Ground truth thus contains one word, that is, one correct translation for each sentence from the evaluation dataset. We have manually annotated the correct translation for the ground truth1 by inspecting the discourse in Wikipedia articles and the interlingual Wikipedia links. We measure the performance of all models as Top 1 accuracy (Acc1 ) (Gaussier et al., 2004; Tamura et al., 2012). It denotes the number of word instances from the evaluation dataset whose top proposed candidate in the ranked list of translation candidates from T is exactly the correct translation for that word instance as given by ground truth over the total number of test word instances (360 in each test dataset). Parameters. We have tuned λ1 and λ2 on the development sets. We set λ1 = λ2 = 0.9 for all language pairs. We use sorted context sets (see sect. 2) and perform a cut-off at M = 3 most descriptive context words in the sorted context sets for all models. In the following section we discuss the u"
D14-1040,P09-1007,0,0.013475,"., 2013). These CLSS models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the"
D14-1040,P11-2052,0,0.0201485,"Missing"
D14-1040,N13-1011,1,0.854231,"Missing"
D14-1040,P11-2084,1,0.813579,"Missing"
D15-1015,N09-1003,0,0.0221124,"l similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-M EAN and CNN-M AX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interIt is clear that the per-image similarity metrics perform better on genuine similarity, as measured by SimLex-999, than on relate"
D15-1015,P98-1069,0,0.0757273,"aning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 199"
D15-1015,P04-1067,0,0.0822964,"Missing"
D15-1015,R11-1055,0,0.188877,"Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al.,"
D15-1015,P08-1088,0,0.297869,"when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some langu"
D15-1015,D14-1032,0,0.0322433,"ulti-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (F"
D15-1015,D09-1092,0,0.0138313,"n induction models to learn translations from comparable data (see sect. 3.3). We do not necessarily expect visual methods to outperform linguistic ones, but it is instructive to see the comparison. We compare our visual models against the current state-of-the-art lexicon induction model using comparable data (Vuli´c and Moens, 2013b). This model induces translations from comparable Wikipedia data in two steps: (1) It learns a set of highly reliable one-to-one translation pairs using a shared bilingual space obtained by applying the multilingual probabilistic topic modeling (MuPTM) framework (Mimno et al., 2009). (2) These highly reliable one-to-one translation pairs serve as dimensions of a word-based bilingual semantic space (Gaussier et al., 2004; Tamura et al., 2012). The model then bootstraps from the high-precision seed lexicon of translations and learns new dimensions of the bilingual space until convergence. This model, which we call B OOTS TRAP, obtains the current best results on the evaluation dataset. For more details about the bootstrapping model and its comparison against other approaches, we refer to Vuli´c and Moens (2013b). Table 4 shows the results for the language pairs in the V UL"
D15-1015,D14-1005,1,0.742838,"orming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sou"
D15-1015,P14-2135,1,0.898971,"edge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use"
D15-1015,J03-1002,0,0.00373644,"isual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Assoc"
D15-1015,W02-0902,0,0.108859,"in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003),"
D15-1015,P99-1067,0,0.0295644,"oehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual s"
D15-1015,J99-4009,0,0.872954,"n down by language, are shown in Table 6. B ERGSMA 500 has a lower average image dispersion score in general, and thus is more concrete than V ULIC 1000. It also has less variance. This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one. When examining individual languages in the datasets, we note that the worst performing language on V ULIC 1000 is Italian, which is also the most abstract dataset, with the highest average image dispersion score and the lowest variance. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), but in a more complex way, since abstract concepts express more varied situations (Barsalou and Wiemer-Hastings, 2005). Using an image resource like Google Images that has full coverage for almost any word, means that we can retrieve what we might call “associated” images (such as images of voters for words like democracy) as opposed to “extensional” images (such as images of cats for cat). This explains why we still obtain good performance on the more abstract V ULIC 1000 dataset, in some cases outperforming linguistic methods: even abstract concepts can have a clear visual representation,"
D15-1015,P14-1132,0,0.0141507,"ata (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also h"
D15-1015,D13-1115,0,0.258685,"Missing"
D15-1015,I11-1162,0,0.0481252,"isition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other e"
D15-1015,W02-2026,0,0.0492709,"014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language p"
D15-1015,P10-1011,0,0.0134096,"aches work by mapping language pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic m"
D15-1015,lin-etal-2010-new,0,0.0150856,"ambiamento (change) in Italian. Using the two evaluation datasets can potentially provide Evaluations Test Sets. Bergsma and Van Durme’s primary evaluation dataset consists of a set of five hundred matching lexical items for fifteen language pairs, based on six languages. (The fifteen pairs results from all ways of pairing six languages). The data is publicly available online.1 In order to get the five hundred lexical items, they first rank nouns by the conditional probability of them occurring in the pattern “{image,photo,photograph,picture} of {a,an} ” in the web-scale Google N-gram corpus (Lin et al., 2010), and take the top five hundred words as their English lexicon. For each item 1 1 n 2 http://www.clsp.jhu.edu/˜sbergsma/LexImg/ 151 http://people.cs.kuleuven.be/˜ivan.vulic/software/ Figure 2: Example images for the languages in the Bergsma and Van Durme dataset. Method P@1 P@5 P@20 MRR B&VD Visual-Only B&VD Visual + NED 31.1 48.0 41.4 59.5 53.7 68.7 0.367 0.536 CNN-AVG M AX CNN-M AX M AX CNN-M EAN CNN-M AX 56.7 42.8 50.5 51.4 69.2 60.0 62.7 64.9 77.4 64.5 71.1 74.8 0.658 0.529 0.586 0.608 4 We evaluate the four similarity metrics on the B ERGSMA 500 dataset and compare the results to the syst"
D15-1015,D12-1130,0,0.113474,"ith using multiple layers from the same network in an attempt to improve performance. 1 There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequenc"
D15-1015,W13-3523,0,0.0364082,"ver, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test the ability of purely visual data to induce shared bilingual spaces and to consequently learn bilingual word correspondences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is remove"
D15-1015,P14-1068,0,0.101262,"(Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004)."
D15-1015,P13-1056,0,0.0195593,"3 A Purely Visual Approach to Bilingual Lexicon Learning Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques"
D15-1015,Q14-1017,0,0.0416793,"ures from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 200"
D15-1015,D12-1003,0,0.136163,"ge pairs to a shared bilingual space and ex148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al.,"
D15-1015,N13-1011,1,0.87388,"Missing"
D15-1015,D13-1168,1,0.763446,"Missing"
D15-1015,P11-2084,1,0.863258,"Missing"
D15-1015,N10-1011,0,\N,Missing
D15-1015,J15-4004,0,\N,Missing
D15-1015,C98-1066,0,\N,Missing
D16-1235,N09-1003,0,0.316379,"rb similarity due to their small size or narrow coverage of verbs. In particular, a number of word pair evaluation sets are prominent in the distributional semantics 1 In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 a"
D16-1235,P98-1013,0,0.378783,"s similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6 The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties. The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena. Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7 all. For each such class we sampled additional verb types until the class was represented by 3 or 4 member verbs (chosen randomly).8 Following that, we sampled at least 2 verb pairs for each previously ’under-represented’ VN class by pairing 2 member verbs from each such class. This procedure resulted in 81 additional pairs, now 3,153 in total. (Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed"
D16-1235,D14-1034,1,0.389034,"resentative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource. 3 The SimVerb-3500 Data Set Design Motivation Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in n"
D16-1235,P14-1023,0,0.0211925,"rage of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge"
D16-1235,P15-2076,0,0.0948827,"SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation"
D16-1235,N15-1184,0,0.117935,"Missing"
D16-1235,W16-2506,0,0.0632045,"oying additional databases or linguistic resources. The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72. The same model obtains the highest score on SV-3500 (ρ = 0.628), with a clear gap to IAA-1 of 0.84. We attribute these differences in 2178 performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs. Development Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets. Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others. The lack of standard splits stems mostly from small size and poor coverage – issues which we have solved with SimVerb-3500. Our development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb). The test set includes the remaining 3,000 verb pairs. The performa"
D16-1235,J15-4004,1,0.907142,"emantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics representation lear"
D16-1235,kipper-etal-2004-extending,0,0.102139,"Missing"
D16-1235,P14-2050,0,0.13367,"idual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2). Vector Space Models We compare the performance of prominent representation models on SimVerb-3500. We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use sparse binary vectors built from linguistic resources (NonEval set IAA-1 IAA-2 A LL T EXT WS IM (203) S IM L EX (999) 0.67 0.65 0.67 0.78 0.79 SGNS-BOW 0.74 Paragram+CF 0.79 SGNS-BOW 0.56 SymPat+SGNS SL-222 (222) 0.72 - 0.73 Paragram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL"
D16-1235,W13-3512,0,0.701559,"play a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types. Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling. Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different 2173 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, c Austin, Texas, November 1-5, 2016. 2016 Association for Computatio"
D16-1235,N16-1018,0,0.138051,"Missing"
D16-1235,D07-1042,0,0.100538,"Missing"
D16-1235,J05-1004,0,0.62211,"to verb semantics research, we introduce SimVerb-3500 – an extensive intrinsic evaluation resource that is unprecedented in both size and coverage. SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008). This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005). Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures. In § 2, we discuss previous evaluation resources targeting verb similarity. We present the new SimVerb-3500 data set along with our design choices and the pair selection process in § 3, while the annotation process is detailed in § 4. In § 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performan"
D16-1235,D14-1162,0,0.0789657,"Missing"
D16-1235,D10-1114,0,0.0113012,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,N10-1013,0,0.0390423,"g models. One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs. This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns. More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015). In future work we aim to apply such methods to the task of verb acquisition. Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes. Better understanding of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in lang"
D16-1235,K15-1026,1,0.904651,"v et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993). Verbs play a key role at almost every level of linguistic analysis. Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation, One factor behind the lack of more nuanced word represent"
D16-1235,P14-1068,0,0.0149921,"are scored for relatedness which has some overlap with similarity. SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators. For a broader discussion see (Hill et al., 2015). 2174 literature. Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively). Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena. Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015). Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015). These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena. In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pa"
D16-1235,P10-1040,0,0.0211468,"methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning. 1 Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015). These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010). Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary. This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015). Introduction Verbs are famously both complex and variable. They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and"
D16-1235,Q15-1025,0,0.0741812,"gram+CF 0.58 SymPat S IM V ERB (3500) 0.84 0.86 0.63 Paragram+CF 0.36 SGNS-DEPS Table 2: An overview of word similarity evaluation benchmarks. A LL is the current best reported score on each data set across all models (including the models that exploit curated knowledge bases and hand-crafted lexical resources, see supplementary material). T EXT denotes the best reported score for a model that learns solely on the basis of distributional information. All scores are Spearman’s ρ correlations. Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkši´c et al., 2016)). Descriptions of these models are in the supplementary material. Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500. The correlation between the two data sets calculated on the shared pairs is ρ = 0.91. This proves, as expected, that the ratings are consistent across the two data sets. Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222. Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for"
D16-1235,C10-1011,0,\N,Missing
D16-1235,N03-1033,0,\N,Missing
D16-1235,W08-1301,0,\N,Missing
D16-1235,C98-1013,0,\N,Missing
D16-1235,P06-1038,0,\N,Missing
D16-1235,petrov-etal-2012-universal,0,\N,Missing
D16-1235,N13-1092,0,\N,Missing
D16-1235,P15-2070,0,\N,Missing
D16-1235,N16-1060,1,\N,Missing
D16-1235,P16-2084,1,\N,Missing
D16-1235,C12-1059,0,\N,Missing
D16-1235,P13-2109,0,\N,Missing
D17-1270,E17-1088,0,0.0338438,"into cross-lingual BABEL synsets (and is currently available for 271 languages). The wide and steadily growing coverage of languages in BabelNet means that our proposed framework promises to support the transfer of VerbNet-style information to numerous target languages (with increasingly high accuracy). To establish that the proposed transfer approach is in fact independent of the chosen cross-lingual information source, we also experiment with another cross-lingual dictionary: PanLex (Kamholz et al., 2014), which was used in prior work on crosslingual word vector spaces (Duong et al., 2016; Adams et al., 2017). This dictionary currently covers around 1,300 language varieties with over 12 million expressions, thus offering support also for low-resource transfer settings.7 VerbNet constraints are extracted from the English VerbNet class structure in a straightforward manner. For each class V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in Tab. 1. Note that Ver"
D17-1270,W13-3520,0,0.0596742,"language: training/test data and constraints. Coverage refers to the percentage of test verbs represented in the target language vocabularies. vectors on ptWaC (Wagner Filho et al., 2016), FI vectors on fiWaC (Ljubeši´c et al., 2016), and PL vectors on the Araneum Polonicum Maius Web corpus (Benko, 2014). Note that we do not utilise any VerbNet-specific knowledge in the target language to induce and further specialise these word vectors. Source EN vectors were taken directly from the work of Levy and Goldberg (2014): they are trained with SGNS on the cleaned and tokenised Polyglot Wikipedia (Al-Rfou et al., 2013) containing ∼75M sentences, ∼1.7B word tokens and a vocabulary of ∼180k words after lowercasing and frequency cut-off. To measure the importance of the starting source language space as well as to test if syntactic knowledge on the source side may be propagated to the target space, we test two variant EN vector spaces: SGNS with (a) BOW contexts and the window size 2 (SGNS-BOW2); and (b) dependencybased contexts (SGNS-DEPS) (Padó and Lapata, 2007; Levy and Goldberg, 2014). Linguistic Constraints We experiment with the following constraint types: (a) monolingual synonymy constraints in each tar"
D17-1270,aparicio-etal-2008-ancora,0,0.173239,"f generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accu"
D17-1270,P98-1013,0,0.896649,"verse languages with high accuracy. The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the resources for Spanish and Catalan (Aparicio et al., 2008), 3 The usefulness of VerbNet is further accentuated by available mappings (Loper et al., 2007) to a number of other verb resources such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005). 4 For example, Levin (1993) notes that verbs in Warlpiri manifest analogous behavior to English with respect to the conative alternation. In another example, Polish verbs have the same patterns as EN verbs in terms of the middle construction. 2548 Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). Other VerbNets were created semi-automatically, with the help of other lexical resources, e.g., for French (Pradet et al., 2014) and Brazilian Portuguese (Scarton and Aluısio, 2012). These approaches involved substantial amounts of specialised lin"
D17-1270,D14-1034,1,0.854054,"rs: SGNS-BOW2. lingual information again leads to large gains with the cross-lingual transfer model, as is evident from Fig. 3. This suggests that the proposed approach does not depend on a particular source of information - it can be used with any general-purpose bilingual dictionary. We mark slight improvements for 3/6 target languages when comparing the results with the ones from Fig. 2a. The new state-of-the-art F-1 scores are 0.79 for FR and 0.74 for PT. Verb Classification vs. Semantic Similarity An interesting question originating from prior work on verb representation learning, e.g., (Baker et al., 2014) touches upon the correlation between verb classification and semantic similarity. Due to the availability of VerbNet constraints and a recent similarity evaluation set (SimVerb-3500; it contains human similarity ratings for 3,500 verb pairs) (Gerz et al., 2016), we perform the analysis on English: the results are summarised in Tab. 4. They clearly indicate that cross-lingual synonymy constraints are useful for both relationship types (compare the scores with XLing), with strong gains over the nonspecialised distributional space. However, the inclusion of VerbNet information, while boosting cl"
D17-1270,P14-1023,0,0.0382331,"ssing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically diverse languages. Based on this finding, we propose an automatic approach which exploits readily available annotations for English to facilitate efficient, large-scale development of VerbNets for a wide set of target languages. Recently, unsupervised methods for inducing distributed word vector space representations or word embeddings (Mikolov et al., 2013a) have been successfully applied to a plethora of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014, i.a.). These methods offer an elegant way to learn directly from large corpora, bypassing the feature engineering step and the dependence on mature NLP pipelines (e.g., POS taggers, parsers, extraction of subcategorisation frames). In this work, we demonstrate how these models can be used to support automatic verb class induction. Moreover, we show that these models offer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the best of our knowledge, this proposition has not"
D17-1270,D16-1136,0,0.0430305,"e which groups words into cross-lingual BABEL synsets (and is currently available for 271 languages). The wide and steadily growing coverage of languages in BabelNet means that our proposed framework promises to support the transfer of VerbNet-style information to numerous target languages (with increasingly high accuracy). To establish that the proposed transfer approach is in fact independent of the chosen cross-lingual information source, we also experiment with another cross-lingual dictionary: PanLex (Kamholz et al., 2014), which was used in prior work on crosslingual word vector spaces (Duong et al., 2016; Adams et al., 2017). This dictionary currently covers around 1,300 language varieties with over 12 million expressions, thus offering support also for low-resource transfer settings.7 VerbNet constraints are extracted from the English VerbNet class structure in a straightforward manner. For each class V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in"
D17-1270,ehrmann-etal-2014-representing,0,0.0341319,"have a similar effect on verb classification, which relies on the similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages? (Q2) Can one boost verb classification for lowerresource languages by exploiting general-purpose cross-lingual resources such as BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) or bilingual dictionaries such as PanLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional ve"
D17-1270,P12-1090,0,0.597422,"Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Pro"
D17-1270,N15-1184,0,0.157158,"Missing"
D17-1270,D16-1235,1,0.899174,"Missing"
D17-1270,P06-1117,0,0.0515276,"f, 1972; Gruber, 1976; Levin, 1993, inter alia). Lexical resources which capture the variability of verbs are instrumental for many Natural Language Processing (NLP) applications. One of the richest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin"
D17-1270,J15-4004,1,0.774578,"ffer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the best of our knowledge, this proposition has not been investigated in previous work. There has been little work on assessing the suitability of embeddings for capturing rich syntacticsemantic phenomena. One challenge is their reliance on the distributional hypothesis (Harris, 1954), which coalesces fine-grained syntacticsemantic relations between words into a broad relation of semantic relatedness (e.g., coffee:cup) (Hill et al., 2015; Kiela et al., 2015). This property has an adverse effect when word embeddings are used in downstream tasks such as spoken language understanding (Kim et al., 2016a,b) or dialogue state tracking (Mrkši´c et al., 2016, 2017a). It could have a similar effect on verb classification, which relies on the similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic"
D17-1270,W02-1016,0,0.184628,"Missing"
D17-1270,W11-0110,0,0.0374549,"verbs are instrumental for many Natural Language Processing (NLP) applications. One of the richest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English"
D17-1270,P06-1017,0,0.0515279,"understand) (accept, reject) (repent, rue) (reject, discourage) (encourage, discourage) (reject, discourage) (disprefer, understand) Given the initial distributional or specialised collection of target language vectors Vt , we apply an offthe-shelf clustering algorithm on top of these vectors in order to group verbs into classes. Following prior work (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009; Sun et al., 2010), we employ the MNCut spectral clustering algorithm (Meila and Shi, 2001), which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces (Chen et al., 2006; von Luxburg, 2007; Scarton et al., 2014, i.a.). Again, following prior work (Sun et al., 2010, 2013), we estimate the number of clusters KClust using the self-tuning method of Zelnik-Manor and Perona (2004). This algorithm finds the optimal number by minimising a cost function based on the eigenvector structure of the word similarity matrix. We refer the reader to the relevant literature for further details. Table 1: Example pairwise ATTRACT constraints extracted from three VerbNet classes in English. a) cross-lingual (translation) links between languages, and b) available VerbNet annotation"
D17-1270,kamholz-etal-2014-panlex,0,0.0919158,"e similarity in syntactic-semantic properties of verbs within a class. In summary, we explore three important questions in this paper: (Q1) Given their fundamental dependence on the distributional hypothesis, to what extent can unsupervised methods for inducing vector spaces facilitate the automatic induction of VerbNet-style verb classes across different languages? (Q2) Can one boost verb classification for lowerresource languages by exploiting general-purpose cross-lingual resources such as BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) or bilingual dictionaries such as PanLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional vector spaces (Mikolov et al., 2013b; Levy and Goldberg, 2014) fro"
D17-1270,P14-1097,0,0.0547583,"ferring lexical resources from resource-rich to resourcepoor languages using general-purpose cross-lingual dictionaries and bilingual vector spaces as means of transfer within a semantic specialisation framework. However, we believe that the proposed basic framework may be upgraded and extended across several research paths in future work. First, in the current work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBan"
D17-1270,D15-1242,0,0.299366,"Missing"
D17-1270,W16-1607,0,0.152578,"Missing"
D17-1270,D12-1048,0,0.0181924,"chest verb resources currently available for English is VerbNet (Kipper et al., 2000; Kipper, 2005).1 Based on the work of Levin (1993), this largely hand-crafted taxonomy organises verbs into classes on the basis of their shared syntacticsemantic behaviour. Providing a useful level of generalisation for many NLP tasks, VerbNet has been used to support semantic role labelling (Swier and Stevenson, 2004; Giuglea and Moschitti, 2006), semantic parsing (Shi and Mihalcea, 2005), word sense disambiguation (Brown et al., 2011), discourse parsing (Subba and Di Eugenio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which"
D17-1270,kipper-etal-2006-extending,1,0.70061,"ss V N i from the 273 VerbNet classes, we simply take the set of all ni verbs CLi = {v1,i , v2,i , . . . , vni ,i } associated with that class, including its subclasses, and generate all unique pairs (vk , vl ) so that vk , vl ∈ CLi and vk 6= vl . Example VerbNet pairwise constraints are shown in Tab. 1. Note that VerbNet classes in practice contain verb instances standing in a variety of lexical relations, including synonyms, antonyms, troponyms, hypernyms, and the class membership is determined on the basis of connections between the syntactic patterns and the underlying semantic relations (Kipper et al., 2006, 2008). 7 Similar to BabelNet, the translations in PanLex were derived from various sources such as glossaries, dictionaries, and automatic inference from other languages. This results in a high-coverage lexicon containing a certain amount of noise. 3 Clustering Algorithm Experimental Setup Languages We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI). All statistics regarding the source and size of training and test data, and linguistic constraints for each target language are summarised in Tab. 2. Automa"
D17-1270,P14-2050,0,0.473593,"anLex (Kamholz et al., 2014) to construct better word vector spaces for these languages? (Q3) Based on the stipulated cross-linguistic validity of VerbNet-style classification, can one exploit rich sets of readily available annotations in one language (e.g., the full English VerbNet) to automatically bootstrap the creation of VerbNets for other languages? In other words, is it possible to exploit a cross-lingual vector space to transfer VerbNet knowledge from a resource-rich to a resource-lean language? To investigate Q1, we induce standard distributional vector spaces (Mikolov et al., 2013b; Levy and Goldberg, 2014) from large monolingual corpora in English and six target languages. As expected, the results obtained with this straightforward approach show positive trends, but at the same time reveal its limitations for all the languages involved. Therefore, the focus of our work shifts to Q2 and Q3. The problem of inducing VerbNetoriented embeddings is framed as vector space specialisation using the available external resources: BabelNet or PanLex, and (English) VerbNet. Formalised as an instance of post-processing semantic specialisation approaches (Faruqui et al., 2015; Mrkši´c et al., 2016), our proce"
D17-1270,P08-1050,0,0.0262482,"nd-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even"
D17-1270,W14-0405,0,0.0376627,"Missing"
D17-1270,K16-1006,0,0.0361737,"ype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the specialisation and clustering algorithms may also result in improved verb class induction. 5 Conclusion We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vector space specialisation framework, utilised to directly model the assumption of cross-linguistic validity of VerbNet-style classifications. Our results indicate strong improvements in verb classification accuracy across all six target languages explored. All automatically"
D17-1270,P02-1027,0,0.0579786,"rate that a constraints-driven fine-tuning framework can specialise word embeddings to reflect VerbNet-style relations which rely not only on verb sense similarity, but also on similarity in syntax, selectional preferences, and diathesis alternations. arguments.3 The current version of VerbNet (v3.2) contains 8,537 distinct English verbs grouped into 273 VerbNet main classes. The inter-relatedness of syntactic behaviour and meaning of verbs is not limited to English (Levin, 1993). The basic meaning components underlying verb classes are said to be cross-linguistically valid (Jackendoff, 1992; Merlo et al., 2002)4 and therefore the classification has a strong cross-lingual dimension. A recent investigation of Majewska et al. (2017) show that it is possible to manually translate VerbNet classes and class members to different, typologically diverse languages with high accuracy. The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the"
D17-1270,P08-3010,0,0.032561,"source development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically div"
D17-1270,N16-1018,1,0.903553,"Missing"
D17-1270,P17-1163,1,0.690358,"Missing"
D17-1270,Q17-1022,1,0.879777,"Missing"
D17-1270,N16-1060,0,0.0177637,"ised in Tab. 2. Automatic approaches to verb class induction have been tried out in prior work for FR and PT. To the best of our knowledge, our cross-lingual study is the first aiming to generalise an automatic induction method to more languages using an underlying methodology which is language-pair independent. Initial Vector Space: Training Data and Setup All target language vectors were trained on large monolingual running text using the same setup: 300-dimensional word vectors, the frequency cutoff set to 100, bag-of-words (BOW) contexts, and the window size of 2 (Levy and Goldberg, 2014; Schwartz et al., 2016). All tokens were lowercased, and all numbers were converted to a placeholder symbol &lt;NUM&gt;.8 FR and IT word vectors were trained on the standard frWaC and itWaC corpora (Baroni et al., 2009), and vectors for other target languages were trained on the corpora of similar style and size: HR vectors were trained on the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), PT 8 Other SGNS parameters were also set to standard values (Baroni et al., 2014; Vuli´c and Korhonen, 2016): 15 epochs, 15 negative samples, global learning rate: .025, subsampling rate: 1e − 4. Similar trends in results persist with d"
D17-1270,D14-1113,0,0.0300192,"t work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the"
D17-1270,C08-1082,0,0.0356657,"lassification approach requires an initial gold standard (Sun et al., 2010): these have been developed for FR (Sun et al., 2010), PT (Scarton et al., 2014), IT, PL, HR, and FI (Majewska et al., 2017). They were created using the methodology of Sun et al. (2010), based on the EN gold standard of Sun et al. (2008) which contains 17 fine-grained Levin classes with 12 member verbs each. For instance, the class PUT-9.1 in French contains verbs such as accrocher, déposer, mettre, répartir, réintégrer, etc. Evaluation Measures We use standard evaluation measures from prior work on verb clustering (Ó Séaghdha and Copestake, 2008; Sun and Korhonen, 2009; Sun et al., 2010; Falk et al., 2012, i.a.). The mean precision of induced verb clusters labelled modified purity (M P UR) is computed as: P M P UR = C∈Clust,nprev(C) &gt;1 nprev(C) #test_verbs (4) Here, each cluster C from the set of all KClust induced clusters Clust is associated with its prevalent class/cluster from the gold standard, and the number of verbs in an induced cluster C taking this prevalent class is labelled nprev(C) . All other verbs not taking the prevalent class are considered errors.10 #test_verbs denotes the total number of test verb instances. The se"
D17-1270,J07-2002,0,0.0510243,"EN vectors were taken directly from the work of Levy and Goldberg (2014): they are trained with SGNS on the cleaned and tokenised Polyglot Wikipedia (Al-Rfou et al., 2013) containing ∼75M sentences, ∼1.7B word tokens and a vocabulary of ∼180k words after lowercasing and frequency cut-off. To measure the importance of the starting source language space as well as to test if syntactic knowledge on the source side may be propagated to the target space, we test two variant EN vector spaces: SGNS with (a) BOW contexts and the window size 2 (SGNS-BOW2); and (b) dependencybased contexts (SGNS-DEPS) (Padó and Lapata, 2007; Levy and Goldberg, 2014). Linguistic Constraints We experiment with the following constraint types: (a) monolingual synonymy constraints in each target language extracted from BabelNet (Mono-Syn); (b) cross-lingual ENTARGET constraints from BabelNet; (c) crosslingual EN-TARGET constraints plus EN VerbNet constraints (see Sect. 2.1 and Fig. 1). Unless stated otherwise, we use BabelNet as the default source of cross-lingual constraints for (b) and (c). Vector Space Specialisation The PARAGRAM model’s parameters are adopted directly from prior work (Wieting et al., 2015) without any additional"
D17-1270,J05-1004,0,0.580512,"The practical usefulness of VerbNet style classification both within and across languages has been limited by the fact that few languages boast resources similar to the English VerbNet. Some VerbNets have been developed completely manually from scratch, aiming to capture properties specific to the language in question, e.g., the resources for Spanish and Catalan (Aparicio et al., 2008), 3 The usefulness of VerbNet is further accentuated by available mappings (Loper et al., 2007) to a number of other verb resources such as WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), and PropBank (Palmer et al., 2005). 4 For example, Levin (1993) notes that verbs in Warlpiri manifest analogous behavior to English with respect to the conative alternation. In another example, Polish verbs have the same patterns as EN verbs in terms of the middle construction. 2548 Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). Other VerbNets were created semi-automatically, with the help of other lexical resources, e.g., for French (Pradet et al., 2014) and Brazilian Portuguese (Scarton and Aluısio, 2012). These approaches involved substantial amounts of specialised linguistic and translation work. Finall"
D17-1270,S16-2012,0,0.0669443,"es from resource-rich to resourcepoor languages using general-purpose cross-lingual dictionaries and bilingual vector spaces as means of transfer within a semantic specialisation framework. However, we believe that the proposed basic framework may be upgraded and extended across several research paths in future work. First, in the current work we have operated with standard single-sense/single-prototype representations, thus effectively disregarding the problem of verb polysemy. While several polysemy-aware verb classification models for English were developed recently (Kawahara et al., 2014; Peterson et al., 2016), the current lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005),"
D17-1270,D16-1174,0,0.0350193,"Missing"
D17-1270,pradet-etal-2014-adapting,0,0.0400988,"Missing"
D17-1270,D07-1043,0,0.0306227,"from English (as a resource-rich language) to the resource-lean target language (providing an answer to question Q3, Sect. 1). These improvements are visible across all target languages, empirically demonstrating the cross-lingual nature of VerbNetstyle classifications. Second, using cross-lingual constraints alone (XLing) yields strong gains over initial distributional spaces (answering Q1 and Q2). Fig. 2 also shows that cross-lingual similarity constraints are more beneficial than the monolingual ones, despite a larger total number of the monolin11 We have also experimented with V-measure (Rosenberg and Hirschberg, 2007), another standard evaluation measure which balances between homogeneity (precision) and completeness (recall); we do not report these scores for brevity as similar trends in results are observed. gual constraints in each language (see Tab. 2). This suggests that such cross-lingual similarity links are strong implicit indicators of class membership. Namely, target language words which map to the same source language word are likely to be synonyms and consequently end up in the same verb class in the target language. However, the crosslingual links are even more useful as means for transferring"
D17-1270,J06-2001,0,0.0498789,"). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element (Jackendoff, 1992; Levin, 1993). In support of this argument, Majewska et al. (2017) have shown that English VerbNet has high translatability across different, 2546 1 http://verbs.colorado.edu/∼mpalmer/projects/verbnet.html Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D17-1270,N09-1064,0,0.05965,"Missing"
D17-1270,D09-1067,1,0.895498,"relearn) (read, study) (cram, relearn) (read, assimilate) (learn, assimilate) (read, relearn) (break, dissolve) (crash, crush) (shatter, split) (break, rip) (crack, smash) (shred, splinter) (snap, tear) (accept, understand) (accept, reject) (repent, rue) (reject, discourage) (encourage, discourage) (reject, discourage) (disprefer, understand) Given the initial distributional or specialised collection of target language vectors Vt , we apply an offthe-shelf clustering algorithm on top of these vectors in order to group verbs into classes. Following prior work (Brew and Schulte im Walde, 2002; Sun and Korhonen, 2009; Sun et al., 2010), we employ the MNCut spectral clustering algorithm (Meila and Shi, 2001), which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces (Chen et al., 2006; von Luxburg, 2007; Scarton et al., 2014, i.a.). Again, following prior work (Sun et al., 2010, 2013), we estimate the number of clusters KClust using the self-tuning method of Zelnik-Manor and Perona (2004). This algorithm finds the optimal number by minimising a cost function based on the eigenvector structure of the word similarity matrix. We refer the reader to the relevant literature"
D17-1270,P13-2129,1,0.893049,"Missing"
D17-1270,C10-1119,1,0.355161,"genio, 2009), information extraction (Mausam et al., 2012), text mining applications (Lippincott et al., 2013; Rimell et al., 2013), research into human language acquisition (Korhonen, 2010), and other tasks. This benefit for English NLP has motivated the development of VerbNets for languages such as Spanish and Catalan (Aparicio et al., 2008), Czech (Pala and Horák, 2008), and Mandarin (Liu and Chiang, 2008). However, end-to-end manual resource development using Levin’s methodology is extremely time consuming, even when supported by translations of English VerbNet classes to other languages (Sun et al., 2010; Scarton et al., 2014). Approaches which aim to learn verb classes automatically offer an attractive alternative. However, existing methods rely on carefully engineered features that are extracted using sophisticated language-specific resources (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012, i.a.), ranging from accurate parsers to pre-compiled subcategorisation frames (Schulte im Walde, 2006; Li and Brew, 2008; Messiant, 2008). Such methods are limited to a small set of resource-rich languages. It has been argued that VerbNet-style classification has a strong cross-lingual element"
D17-1270,P10-1040,0,0.115591,"n Empirical Methods in Natural Language Processing, pages 2546–2558 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics even typologically diverse languages. Based on this finding, we propose an automatic approach which exploits readily available annotations for English to facilitate efficient, large-scale development of VerbNets for a wide set of target languages. Recently, unsupervised methods for inducing distributed word vector space representations or word embeddings (Mikolov et al., 2013a) have been successfully applied to a plethora of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014, i.a.). These methods offer an elegant way to learn directly from large corpora, bypassing the feature engineering step and the dependence on mature NLP pipelines (e.g., POS taggers, parsers, extraction of subcategorisation frames). In this work, we demonstrate how these models can be used to support automatic verb class induction. Moreover, we show that these models offer the means to exploit inherent cross-lingual links in VerbNet-style classification in order to guide the development of new classifications for resource-lean languages. To the bes"
D17-1270,P16-2084,1,0.915141,"Missing"
D17-1270,Q15-1025,0,0.366752,"t encodes the cross-linguistic validity of Levin-style verb classifications into the vector-space specialisation framework (Sect. 2.1) driven by linguistic constraints. A standard clustering algorithm is then run on top of the VerbNetspecialised representations using vector dimensions as features to learn verb clusters (Sect. 2.2). Our approach attains state-of-the-art verb classification performance across all six target languages. 2.1 Vector Space Specialisation Specialisation Model Our departure point is a state-of-the-art specialisation model for fine-tuning vector spaces termed PARAGRAM (Wieting et al., 2015).5 The PARAGRAM procedure injects similarity constraints between word pairs in order to make their vector space representations more similar; we term these the ATTRACT constraints. Let V = Vs t Vt be the vocabulary consisting of the source language and target language vocabularies Vs and Vt , respectively. Let C be the set of word pairs standing in desirable lexical relations; these include: 1) verb pairs from the same VerbNet class (e.g. (en_transport, en_transfer) from verb class SEND -11.1); and 2) the cross-lingual synonymy 5 The original PARAGRAM model as well as other finetuning models f"
D17-1270,N16-1096,0,0.0305242,"lack of polysemyaware evaluation sets in other languages impedes this line of research. Evaluation issues aside, one idea for future work is to use the ATTRACT-R EPEL specialisation framework for sense-aware crosslingual transfer relying on recently developed multisense/prototype word representations (Neelakantan et al., 2014; Pilehvar and Collier, 2016, inter alia). Another challenge is to apply the idea from this work to enable cross-lingual transfer of other structured lexical resources available in English such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbKB (Wijaya and Mitchell, 2016). Other potential research avenues include porting the approach to other typologically diverse languages and truly low-resource settings (e.g., with only limited amounts of parallel data), as well as experiments with other distributional spaces, e.g. (Melamud et al., 2016). Further refinements of the specialisation and clustering algorithms may also result in improved verb class induction. 5 Conclusion We have presented a novel cross-lingual transfer model which enables the automatic induction of VerbNet-style verb classifications across multiple languages. The transfer is based on a word vect"
D18-1026,W13-3520,0,0.0562518,"− Xs ||F = UV&gt; W UΣV&gt; = SVD(Xt X&gt; s ) (7) where ||· ||F is the Frobenius norm. After mapping the original target embeddings into the shared space with this method, we post-specialize them with the function outlined in §2.2, learnt on the source language. This yields the specialized target vectors ˆ t = G(W ˆ Xt ; θG ). Y 3 Experimental Setup Distributional Vectors. We estimate the robustness of adversarial post-specialization by experimenting with three widely used collections of distributional English vectors. 1) SGNS - W 2 vectors are trained on the cleaned and tokenized Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli"
D18-1026,P14-2131,0,0.06586,"Missing"
D18-1026,Q17-1010,0,0.552254,"xical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre"
D18-1026,D14-1082,0,0.00851156,"plification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move bey"
D18-1026,P16-1156,0,0.0192147,"straints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applications. Comparing to their work, we show that a more sophisticated adversarial post-specia"
D18-1026,N15-1184,0,0.2529,"Missing"
D18-1026,N13-1092,0,0.0607536,"Missing"
D18-1026,D16-1235,1,0.92545,"Missing"
D18-1026,P15-2011,1,0.950221,"nstream NLP applications (Faruqui, 2016). Such models are versatile as they can be applied to arbitrary distributional spaces, but they have a major drawback: they locally update only vectors of words present in linguistic constraints (i.e., seen words), whereas vectors of all other (i.e., unseen) words remain intact (see Figure 1). Both authors equally contributed to this work. 1 For instance, it is difficult to discern synonyms from antonyms in distributional vector spaces: this has a negative impact on language understanding tasks such as statistical dialog modeling or text simplification (Glavaš and Štajner, 2015; Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016) 282 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 282–293 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Source Target posed model in two downstream tasks: lexical text simplification and dialog state tracking. Finally, we demonstrate that, by coupling our adversarial specialization model with any unsupervised model for inducing bilingual vector spaces, such as the algorithm proposed by Conneau et al. (2018), we can successfully per"
D18-1026,W14-4337,0,0.114879,"Missing"
D18-1026,J15-4004,1,0.949832,"P), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al.,"
D18-1026,P14-2075,0,0.121359,"Missing"
D18-1026,N15-1070,0,0.0340618,"lexical resources. An overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not ti"
D18-1026,D15-1242,0,0.222314,"Missing"
D18-1026,P17-1163,1,0.900293,"Missing"
D18-1026,P14-2050,0,0.556242,"onal space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the"
D18-1026,P15-1145,0,0.126883,"ss, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et"
D18-1026,Q17-1022,1,0.887228,"Missing"
D18-1026,P16-2074,0,0.299861,"twork (i.e., the generator). We show that the proposed adversarial model yields state-of-the-art performance on standard word similarity benchmarks, outperforming the post-specialization model of Vuli´c et al. (2018). We further demonstrate the effectiveness of the proMethodology The post-specialization procedure (Vuli´c et al., 2018) is a two-step process. First, a subspace of vectors for words observed in external resources is fine-tuned using any off-the-shelf specialization model, such as the original retrofitting model (Faruqui et al., 2015), counter-fitting (Mrkši´c et al., 2016), dLCE (Nguyen et al., 2016), or state-of-theart ATTRACT- REPEL ( AR ) specialization (Mrkši´c et al., 2017; Vuli´c et al., 2017). We outline the initial specialization algorithms in §2.1. In the second step, the initial specialization is propagated to the entire vocabulary, including words not observed in the resources, relying on an adversarial architecture augmented with a distance loss. This adversarial post-specialization model, compatible with any specialization model, is described in §2.2. Finally, in §2.3, we introduce a cross-lingual zero-shot specialization model which transfers the specialization to a target l"
D18-1026,K16-1006,0,0.0212333,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,N15-1100,0,0.536844,"(Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as (Melamud et al., 2"
D18-1026,N16-1118,0,0.0700653,"onsistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervise"
D18-1026,Q16-1030,0,0.390812,"results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al"
D18-1026,D14-1162,0,0.0930116,"a full target distributional space without any lexical knowledge in the target language and without any bilingual data. 1 Introduction Word representation learning is a mainstay of modern Natural Language Processing (NLP), and its usefulness has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et"
D18-1026,P15-1173,0,0.0417519,"ed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, outperforming less versatile joint models and yielding state-of-theart results on language understanding tasks (Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017). By design, these methods fine-tune only vectors of words seen in external resources. Vuli´c et al. (2018) suggest that specializing the full vocabulary is beneficial for downstream applica"
D18-1026,K15-1026,0,0.222497,"ess has been proven across a wide spectrum of NLP applications (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b, inter alia). Standard distributional word vector models ∗ are grounded in the distributional hypothesis (Harris, 1954), that is, they leverage information about word co-occurrences in large text corpora (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014; Bojanowski et al., 2017). This dependence on contextual signal results in a well-known tendency to conflate semantic similarity with other types of semantic association (Hill et al., 2015; Schwartz et al., 2015; Vuli´c et al., 2017) in the induced word vector spaces.1 A common remedy is to move beyond purely unsupervised word representation learning, in a process referred to as semantic specialization or retrofitting. Specialization methods exploit lexical knowledge from external resources, such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013) to refine the semantic properties of pre-trained vectors and specialize the distributional spaces for a particular relation, e.g., synonymy (i.e., true similarity) (Faruqui et al., 2015; Mrkši´c et al., 2017) or hypernymy (Nic"
D18-1026,P16-1157,0,0.102759,"Missing"
D18-1026,N18-1048,1,0.24712,"Missing"
D18-1026,P16-1024,1,0.923484,"Missing"
D18-1026,N18-1103,1,0.824775,"Missing"
D18-1026,P17-1006,1,0.860576,"Missing"
D18-1026,K17-1013,1,0.898667,"Missing"
D18-1026,E17-1042,1,0.840593,"Missing"
D18-1026,Q15-1025,0,0.637092,"n overview of the proposed methodology from this section is provided in Figure 1. 2.1 Initial Specialization Linguistic Constraints. Adopting the nomenclature from Mrkši´c et al. (2017), post-processing models are generally guided by two broad sets of constraints: 1) ATTRACT constraints specify which words should be close to each other in the finetuned vector space (e.g. synonyms like graceful and amiable); 2) REPEL constraints describe which words should be pulled away from each other (e.g. antonyms like innocent and sinful). Earlier postprocessors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT constraints, and are thus not suited to model both aspects contributing to the specialization process. We first outline the state-of-the-art ATTRACTREPEL specialization model (Mrkši´c et al., 2017) 283 which leverages both sets of constraints. Here, we again stress two important aspects relevant to our post-specialization model: a) all initial specialization models fine-tune only representations for the subspace of words seen in the external constraints, while all other words remain unaffected by specialization; b) post-specialization is not tied to ATTRACT- REPEL in"
D18-1026,P14-2089,0,0.199538,"he different ways concepts are lexicalized across languages, as studied by semantic typology (Ponti et al., 2018). Nonetheless, in the long run, these transfer results hold promise to support the specialization of vector spaces even for resource-lean languages, and their applications. 5 Related Work Vector Space Specialization. Specialization methods embed external information into vector spaces. Some of them integrate external linguistic constraints into distributional training and jointly optimize distributional and non-distributional objectives: they modify the prior or the regularization (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016). Other models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). They offer a portable, flexible, and lightweight approach to incorporating external knowledge into arbitrary vector spaces, ou"
D18-1026,D14-1161,0,0.205358,"Polyglot Wikipedia (Al-Rfou et al., 2013) using Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) by Levy and Goldberg (2014) with bag-ofwords contexts (window size is 2). 2) GLOVE - CC are GloVe vectors trained on the Common Crawl (Pennington et al., 2014). 3) FASTTEXT are vectors trained on Wikipedia with a SGNS variant that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). All vectors are 300-dimensional.4 Constraints and Initial Specialization. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015; Vuli´c et al., 2018). These constraints, extracted from WordNet (Fellbaum, 1998) and Roget’s Thesaurus (Kipfer, 2009), comprise a total of 1,023,082 synonymy/ATTRACT word pairs and 380,873 antonymy/REPEL pairs. Note that the sets of constraints cover only a fraction of the full distributional vocabulary, providing direct motivation for post-specialization methods 4 3 See the recent survey papers on cross-lingual word embeddings and their typology (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016; Ruder et al., 2017) 286 Experiments with other standard word vectors, such as"
D18-1026,P17-1179,0,0.031446,"distance can be formulated as the mean squared error between the input and the target (Pathak et al., 2016), their feature maps (Li and Wand, 2016), both (Zhu et al., 2016), or a loss calculated on feature maps of a deep convolutional network (Ledig et al., 2017). In the textual domain, adversarial models have been proven to support domain adaptation (Ganin et al., 2016) and language transfer (Chen et al., 2016) by learning domain/language-invariant latent features. Adversarial training also powers unsupervised mapping between monolingual vector spaces to learn cross-lingual word embeddings (Zhang et al., 2017; Conneau et al., 2018). In this work, we show how to apply adversarial techniques to the problem of vector specialization, which has a substantial impact on language understanding tasks. 6 Conclusion and Future Work We have presented adversarial post-specialization, a novel model supported by adversarial training which specializes word vectors for the full vocabulary of the input distributional vector space, including words unseen in external lexical resources. We have also introduced a method for zero-shot specialization of word vectors in languages without any external resources. The benefi"
D18-1029,E17-1088,0,0.119612,"e standard fixed-vocabulary assumption. MIN=5: only words with corpus frequency above 5 are retained in the final fixed vocabulary V ; 10K: V comprises the 10k most frequent words. improves the perplexity measure, it actually makes the models less useful, especially in morphologically rich languages, as exemplified in Table 1. Our goal is to get a clear picture on how different typological features and the corresponding corpus frequency distributions affect LM performance, without the influence of the unrealistic fixed-vocabulary assumption. Therefore, we work in the full-vocabulary LM setup (Adams et al., 2017; Grave et al., 2017). This means that we explicitly decide to retain also infrequent words in the modeled data: V contains all words occurring at least once in the training set, only unseen words from test data are treated as OOVs. We believe that this setup leads to an evaluation that pinpoints the crucial limitations of standard LM architectures.2 Why Not Open Vocabulary Setup? Recent neural LM architectures have also focused on handling large vocabularies and unseen words using character-aware modeling (Luong and Manning, 2016; Jozefowicz et al., 2016; Kawakami et al., 2017, inter alia). T"
D18-1029,W13-3520,0,0.0177648,"nly for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We start from the Polyglot Wikipedia (PW) project (Al-Rfou et al., 2013) which provides cleaned and tokenised Wikipedia data in 40 languages. However, the majority of the PW languages are similar from the perspective of genealogy (26/40 are Indo-European), geography (28/40 are Western European), and typology (26/40 are fusional). Consequently, the PW set is not a representative sample of the world’s languages. To amend this limitation, we source additional languages with the data coming from the same domain, Wikipedia, considering candidates in descending order of corpus size cleaned and preprocessed by the Polyglot tokeniser (Al-Rfou et al., 2013). Since fusional"
D18-1029,K17-2002,0,0.0277995,"sis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder, 2018) across a wide variety of syntactic and"
D18-1029,D08-1078,0,0.02949,"rts preliminary results from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stil"
D18-1029,D15-1042,0,0.0182386,"ically diverse languages is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marc"
D18-1029,Q18-1032,1,0.863129,"Missing"
D18-1029,P17-1109,0,0.0426337,", as shown in §4. 4 Data Selection of Languages. Our selection of test languages is guided by the following goals: a) we have to ensure the coverage of typological properties from §3, and b) we want to analyse a large set of languages which extends and surpasses other work in the LM literature (see §2). Since cross-lingual NLP aims at modeling extant languages rather than possible languages (including, e.g., extinct ones), creating a balanced sample is challenging. In fact, attested languages, intended as a random variable, are extremely sparse and not independent-and-identically-distributed (Cotterell and Eisner, 2017). First, available and reliable data exist only for a fraction of the world’s languages. Second, these data are biased because their features may not stem from the underlying distribution, i.e., from what is naturally possible/frequent, but rather 319 can be inherited by genealogical relatedness or borrowed by areal proximity (Bakker, 2010). To mitigate these biases, theoretical works resorted to stratification approaches, where each subgroup of related languages is sampled independently. maximizing their diversity (Dryer, 1989, inter alia). We perform our selection in the same spirit. We star"
D18-1029,P13-2121,0,0.0214948,"Missing"
D18-1029,N18-2085,0,0.180154,"et statistical models in terms of variation in language structures (Ponti et al., 2017). ∗ Both authors equally contributed to this work. In order to evaluate how cross-lingual structural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light"
D18-1029,D17-1030,0,0.0229719,"n to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent s"
D18-1029,K15-1017,0,0.0360047,", but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al., 2018). Given the recent success and improved performance with LM-based pre-training methodology (Peters et al., 2018; Howard and Ruder"
D18-1029,P18-1031,0,0.0568651,"word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German"
D18-1029,P17-1137,0,0.184332,"ders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary assumption in fact ign"
D18-1029,2005.mtsummit-papers.11,0,0.0412215,"gium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a comprehensive evaluation set that systematically covers a wide and balanced spectrum of"
D18-1029,P16-1100,0,0.0493417,"Missing"
D18-1029,P11-1015,0,0.00869074,"representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs"
D18-1029,J93-2004,0,0.0652462,"2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese,"
D18-1029,D16-1209,0,0.0331062,"Missing"
D18-1029,N12-1043,0,0.0266202,"18 Conference on Empirical Methods in Natural Language Processing, pages 316–327 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ing a short overview of multilingual LM and its possible setups (§2), we describe the cross-lingual variation in morphological systems and propose a novel typologically diverse dataset for LM in §3. We outline the data in §4 and benchmarked language models in §5. Finally, we discuss the results in light of linguistic typology in §6. 2 To the best of our knowledge, the largest datasets used in previous work are from (Müller et al., 2012; Cotterell et al., 2018) and amount to 21 languages from the Europarl data (Koehn, 2005). Despite the large coverage of languages, these sets are still restricted only to the languages of the European Union. On the other hand, the most typologically diverse dataset thus far was released by Vania and Lopez (2017). It includes 10 languages representing some morphological systems. This short survey of related work demonstrates a clear tendency towards extending LM evaluation to other languages, abandoning English-centric assumptions, and focusing on language-agnostic LM architectures. However, a"
D18-1029,C16-1123,1,0.845009,"Missing"
D18-1029,N09-2056,0,0.100996,"lts from prior work (Botha and Blunsom, 2014; Adams et al., 2017; Cotterell et al., 2018), and is also backed by insights from linguistic theory on variance of language complexity in general and variance of morphological complexity in specific (McWhorter, 2001; Evans and Levinson, 2009). More broadly and along the same line, earlier research in statistical machine translation (SMT) has also shown that typological factors such as the amount of reordering, the morphological complexity, as well as genealogical relatedness of languages are crucial in predicting success in SMT (Birch et al., 2008; Paul et al., 2009; Daiber, 2018). Our results indicate that the artificial fixed323 5 Unfortunately no values are available in WALS for the feature of flexivity besides a limited domain. vocabulary assumption from prior work produces overly optimistic perplexity scores, and its limitation is even more pronounced in morphologically rich languages, which inherently contain a large number of infrequent words due to their productive morphological systems. The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the"
D18-1029,N18-1202,0,0.182197,".wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech"
D18-1029,D17-1010,0,0.0372516,". The typical solution to collect more data (Jozefowicz et al., 2016; Kawakami et al., 2017) mitigates this effect to a certain extent, but stills suffers from the Zipfian hypothesis (1949), and it cannot be guaranteed for resource-poor languages where obtaining sufficient monolingual data is also a challenge (Adams et al., 2017). Therefore, another solution is to resort to other sources of information which are not purely contextual/distributional. For instance, a promising line of current and future research is to (learn to) exploit subword-level patterns captured in an unsupervised manner (Pinter et al., 2017; Herbelot and Baroni, 2017) or integrate existing morphological generation and inflection tools and regularities (Cotterell et al., 2015; Vuli´c et al., 2017; Bergmanis et al., 2017) into language models to reduce data sparsity, and improve language modeling for morphologically rich languages. For instance, a recent enhancement of the Char-CNN-LSTM language model that enforces similarity between parameters of morphologically related words leads to large perplexity gains across a large number of languages, with the most prominent gains reported for morphologically complex languages (Gerz et al"
D18-1029,S17-1003,1,0.846376,"Missing"
D18-1029,E17-2025,0,0.0127292,"ternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on the fixed-vocabulary assumption: they use a spe"
D18-1029,D15-1044,0,0.0129894,"is still missing. The novel dataset we discuss in this paper aims at bridging this gap (see §4). Multilingual Language Modeling A language model computes a probability distribution over sequences of word tokens, and is typically trained to maximise the likelihood of word input sequences. The LM objective is expressed as: P (w1 , ...wn ) = Y P (wi |w1 , ...wi−1 ) (1) i wi is a word token with the index i in the sequence. LM is considered a central task in NLP and language understanding, with applications in speech recognition (Mikolov et al., 2010), text summarisation (Filippova et al., 2015; Rush et al., 2015), and information retrieval (Ponte and Croft, 1998; Zamani and Croft, 2016). The importance of language modeling has been accentuated even more in representation learning recently, where it is used as a novel form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and"
D18-1029,P17-1184,0,0.256638,"tructural variation hinders the design of effective generalpurpose algorithms, we propose the task of language modeling (LM) as a testbed. In particular, we opt for a full-vocabulary setup where no word encountered at training time is treated as an unknown symbol, in order to a) ensure a fair comparison across languages with different word frequency rates and b) avoid setting an arbitrary threshold on vocabulary size (Cotterell et al., 2018). Although there has recently been a tendency towards expanding test language samples, the datasets considered in previous works (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017; Cotterell et al., 2018) are not entirely adequate yet to represent the typological variation and to ground cross-lingual generalisations empirically. Hence, we test several LM architectures (including n-gram, neural, and character-aware models) on a novel and wider set of 50 languages sampled according to stratification principles. Through this large-scale multilingual analysis, we shed new light on the current limitations of standard LM models and offer support to further developments in multilingual NLP. In particular, we demonstrate that the previous fixedvocabulary"
D18-1029,P17-1006,1,0.88688,"Missing"
D18-1029,P16-1125,0,0.0132887,"form of unsupervised pre-training (and an alternative to static word embeddings) for the benefit of a variety of NLP applications (Peters et al., 2018; Howard and Ruder, 2018). Related Work: Datasets and Evaluation. Language modeling is predominantly tested on English and other Western European languages. Standard English LM benchmarks are the Penn Treebank (PTB) (Marcus et al., 1993) and the 1 Billion Word Benchmark (BWB) (Chelba et al., 2013). Datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation in English (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). For multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for Czech, French, Spanish, German, and Russian from the 2013 Workshop on Statistical Machine Translation (WMT) data (Bojar et al., 2013). Kim et al. (2016) reuse these datasets and add Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Fixed vs. Full Vocabulary Setup. A majority of language models rely on"
D19-1226,Q17-1010,0,0.0982944,"revious state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog"
D19-1226,S17-2001,0,0.0533888,"Missing"
D19-1226,P19-1070,1,0.721523,"Missing"
D19-1226,P15-2011,1,0.869561,"Missing"
D19-1226,W16-2501,1,0.82432,", word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing sp"
D19-1226,J15-4004,1,0.915895,"Missing"
D19-1226,P14-2075,0,0.0774785,"pecialization on LS, we employ ˇ Light-LS (Glavaˇs and Stajner, 2015), a languageagnostic LS tool that makes simplifications based on word similarities in a given vector space. The quality of similarity-based information encoded in the vector space encode is thus expected to directly correlate with the performance of Light-LS. We use LS datasets for Italian (IT) (Tonelli et al., 2016), Spanish (ES) (Saggion et al., 2015; Saggion, 2017), and Portuguese (PT) (Hartmann et al., 2018) to evaluate the specialized spaces in those languages. We rely on the standard LS evaluation metric of Accuˇ racy (Horn et al., 2014; Glavaˇs and Stajner, 2015): it quantifies both the quality and frequency of replacements as a number of correct simplifications divided by the total number of complex words. Results and Analysis. The results are reported in Table 3. As shown in previous work (Vuli´c et al., 2018; Ponti et al., 2018), retrofitting (CLSRI-AR) and the cross-lingual post-specialization transfer (X-PS) are substantially better in the LS task than the original distributional space. However, our full CLSRI-PS model results in substantial boosts in the 2213 LS any additional input for the lexical prediction step (i."
D19-1226,N15-1184,0,0.302339,"Missing"
D19-1226,D18-1330,0,0.0952862,"Missing"
D19-1226,W19-4310,1,0.811783,"Missing"
D19-1226,D15-1242,0,0.242713,"Missing"
D19-1226,W16-1607,0,0.262024,"Missing"
D19-1226,N18-2029,1,0.870858,"Missing"
D19-1226,P18-1004,1,0.752226,"Missing"
D19-1226,N16-1018,0,0.139801,"Missing"
D19-1226,C18-1205,0,0.107783,"and translation of incorrect senses of Ls words. We thus subsequently refine the noisy set of target constraints by having a state-of-the-art neural model for lexico-semantic relation prediction (Glavaˇs and Vuli´c, 2018a), trained on the Ls constraints, discern valid from invalid Lt constraints. Following that, we perform monolingual retrofitting and post-specialization in the target language Lt , as outlined in § 3.2. The Lt distributional vectors can be specialized with the cleaned Lt constraints using any off-the-shelf retrofitting model (Faruqui et al., 2015; Mrkˇsi´c et al., 2016; 2208 Lengerich et al., 2018, inter alia). In this work we opt for the best-performing retrofitting model ATTRACT- REPEL ( AR ) (Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b). AR specializes only the words seen in the cleaned Lt constraints. As the final step, we generalize AR’s specialization to the entire target vocabulary with a post-specialization model (Ponti et al., 2018) that learns the global specialization function from pairs of distributional and ARspecialized vectors of words from Lt constraints. A visual summary of our transfer model is presented in Figure 1. Our proposed CLSRI specialization conceptually diff"
D19-1226,P14-2050,0,0.0346927,"og state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., betwe"
D19-1226,C18-1172,0,0.0157486,"ext simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better downstream performance (Mrkˇsi´c et al., 2016). Moreover, while the joint models are tightly coupled to a concrete word embedding objective, retrofitting models can be applied on top"
D19-1226,P15-1145,0,0.0602024,"ntic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical"
D19-1226,N16-1118,0,0.020871,". The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can br"
D19-1226,P18-2018,1,0.880048,"Missing"
D19-1226,Q17-1022,1,0.892103,"Missing"
D19-1226,L18-1381,0,0.208203,"Missing"
D19-1226,C16-1123,1,0.872206,"Missing"
D19-1226,N15-1100,0,0.0535031,"., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the l"
D19-1226,Q16-1030,0,0.0603068,"rity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of retrofitting models specializes only the vectors of words seen in lexical constraints, the latter yield better dow"
D19-1226,D14-1162,0,0.0845732,"mantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensi"
D19-1226,N18-1202,0,0.0147967,"ialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation s"
D19-1226,J19-3005,1,0.873216,"Missing"
D19-1226,P17-1163,0,0.0403913,"Missing"
D19-1226,D18-1026,1,0.750703,"Missing"
D19-1226,D18-1299,0,0.171899,"Missing"
D19-1226,Q15-1025,0,0.0752505,"(Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Ponti et al., 2018) or lexical entailment (Nguyen et al., 2017; Vuli´c and Mrkˇsi´c, 2018) over other types of semantic association in the word vector space. The best-performing specialization models (cf. Mrkˇsi´c et al. 2017; Ponti et al. 2018) are executed as vector space post-processors. In short, these techniques force the distributional vectors to conform to external linguistic constraints (e.g., synonymy, meronymy, lexical entailment) extracted from structured external resources (e.g., WordNet, BabelNet) to emphasize the particular relation. As post-processors th"
D19-1226,K15-1026,1,0.801861,"ally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages. 1 Introduction Due to their dependence on the distributional hypothesis (Harris, 1954), that is, word co-occurrence information in large corpora, distributional word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, lexical entailment, cohyponymy, meronymy) and the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). This property can propagate undesired effects to language understanding applications such as statistical dialog modeling or text simplification (Faruqui, 2016; Chiu et al., 2016; Mrkˇsi´c et al., 2017): for instance, the inability to distinguish between synonymy and antonymy (e.g., between cheap pubs and expensive restaurants) can break task-oriented dialog or a recommendation system (Mrkˇsi´c et al., 2016; Kim et al., 2016b). Semantic specialization techniques are therefore leveraged to stress a relation of interest such as semantic similarity (Wieting et al., 2015;"
D19-1226,S17-2016,0,0.0528375,"Missing"
D19-1226,P18-1072,1,0.903026,"Missing"
D19-1226,P14-2089,0,0.035718,"d vectors; semantic specialization of such spaces for a particular lexicosemantic relation (e.g., semantic similarity or lexical entailment) benefits a number of tasks, e.g., dialog state tracking (Mrkˇsi´c et al., 2017; Ponti et al., 2018), spoken language understanding (Kim et al., 2016b,a), text simplification (Glavaˇs and Vuli´c, 2018b; Ponti et al., 2018), and cross-lingual transfer of resources (Vuli´c et al., 2017a). Specialization methods inject external lexical knowledge into a distributional space, tailoring vectors for a particular relation of interest. Joint specialization models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017, inter alia) use external constraints to modify the training objective of word embedding models (Mikolov et al., 2013; Dhillon et al., 2015; Liu et al., 2018b,a) and train specialized vectors from scratch. In contrast, retrofitting (also known as postprocessing) methods tune the pre-trained distributional vectors post-hoc based on the provided external constraints. Despite the fact that joint models specialize the entire space, whereas the first generation of"
D19-1226,D14-1161,0,0.211456,"Missing"
D19-1226,P18-1135,0,0.0126936,"ble 2. Several findings emerge from the results. First, as already confirmed in prior work (Vuli´c et al., 2018; Ponti et al., 2018), vectors specialized for semantic similarity are indeed important for DST: we observe improvements with all specialized vectors. The highest gains are observed with the full CSLRIPS model. This confirms two main intuitions: 1) our proposed specialization transfer via lexical induction in the target language is more robust than 15 Note that the original NBT framework in the English DST task has been recently surpassed by more intricate taskspecific architectures (Zhong et al., 2018; Ren et al., 2018), but its lightweight design coupled with its strong dependence on input word vectors still makes it a convenient means to evaluate the effects of different specialization methods. Lexical Simplification Lexical simplification (LS) aims to automatically replace complex words (i.e., specialized terms, words used less frequently and known to fewer speakers) with their simpler in-context synonyms: the simplified text must be grammatical and retain the meaning of the original text. Lexical simplification critically depends on discerning semantic similarity from other types of se"
D19-1226,W17-0228,0,0.123249,"Missing"
D19-1226,N18-1048,1,0.885485,"Missing"
D19-1226,N18-1103,1,0.913929,"Missing"
D19-1226,D17-1270,1,0.902499,"Missing"
D19-1226,P17-1006,1,0.89768,"Missing"
D19-1226,E17-1042,0,0.0520418,"Missing"
D19-1288,W18-5412,0,0.214596,"d stored it in the form of attribute–value features in publicly accessible databases (Croft, 2002; Dryer and Haspelmath, 2013). The usage of such features to inform neural NLP models is still scarce, partly because the evidence in favor of their effectiveness is mixed (Ponti et al., 2018, 2019). In this work, we propose a way to distantly supervise the model with this side information effectively. We extend our non-conditional language models outlined in §3 (BARE) to a series of variants conditioned on language-specific properties, inspired by Östling and Tiedemann (2017) and Platanios et al. (2018). A fundamental difference from these previous works, however, is that they learn such properties in an end-to-end fashion from the data in a joint multilingual learning setting. Obviously, this is not feasible for the zeroshot setting and unreliable for the few-shot setting. Rather, we represent languages with their typological feature vector, which we assume to be readily available both for both training and held-out languages. Let t` ∈ [0, 1]f be a vector of f typological features for language ` ∈ T t E. We reinterpret the conditional language models within the Bayesian framework by estimat"
D19-1288,N16-1161,0,0.0861779,"shing the problem to its most complex formulation, zero-shot inference, and in taking into account the largest sample of languages for language modeling to date. In addition to those considered in our work, there are also alternative methods to condition language models on features. Kalchbrenner and Blunsom (2013) used encoded features as additional biases in recurrent layers. Kiros et al. (2014) put forth a log-bilinear model that allows for a ‘multiplicative interaction’ between hidden representations and input features (such as images). With a similar device, but a different gating method, Tsvetkov et al. (2016) trained a phoneme-level joint multilingual model of words conditioned on typological features from Moran et al. (2014). The use of the Laplace method for neural transfer learning has been proposed by Kirkpatrick et al. (2017), inspired by synaptic consolidation in neuroscience, with the aim to avoid catastrophic forgetting. Kochurov et al. (2018) tackled the problem of continuous learning by approximating the posterior probabilities through stochastic variational inference. Ritter et al. (2018) substitute diagonal Laplace approximation with a Kronecker factored method, leading to better uncer"
D19-1288,Q19-1040,0,0.125021,"rld’s languages, we hope that these findings will help broaden the scope of applications for language technology. 1 Introduction With the success of recurrent neural networks and other black-box models on core NLP tasks, such as language modeling, researchers have turned their attention to the study of the inductive bias such neural models exhibit (Linzen et al., 2016; Marvin and Linzen, 2018; Ravfogel et al., 2018). A number of natural questions have been asked. For example, do recurrent neural language models learn syntax (Marvin and Linzen, 2018)? Do they map onto grammaticality judgments (Warstadt et al., 2019)? However, as Ravfogel et al. (2019) note, “[m]ost of the work so far has focused on English.” Moreover, these studies have almost always focused on training scenarios where a large number of in-language sentences are available. In this work, we aim to find a prior distribution over network parameters that generalize well to new human languages. The recent vein of research on the inductive biases of neural nets implicitly assumes a uniform (unnormalizable) prior over the space of neural network parameters (Ravfogel et al., 2019, inter alia). In contrast, we take a Bayesian-updating approach: F"
D19-1449,E17-1088,0,0.0317067,"ble 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al., 2019), we then reserve the 5K pairs created from the more frequent L1 words for training, while the next 2K pairs are used for test. Smaller training dictionaries (1K and 500 pairs) are created by again"
D19-1449,W13-3520,0,0.0499261,"LWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical strings), in contrast, yield good solutions for all setups. 5 Fur"
D19-1449,D18-1062,0,0.0195462,"seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their m"
D19-1449,2020.lrec-1.495,0,0.468497,"Missing"
D19-1449,P18-1073,0,0.0976736,"e induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dict"
D19-1449,J82-2005,0,0.673463,"Missing"
D19-1449,D16-1136,0,0.0311352,"tive agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a PanLex-predefined threshold. As in prior work (Conneau et al., 2018a; Glavaš et al.,"
D19-1449,E14-1049,0,0.0557935,"l., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019"
D19-1449,C10-3010,0,0.0486842,"c Uralic Austronesian Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces"
D19-1449,D18-1029,1,0.891637,"Missing"
D19-1449,W16-1614,0,0.118504,"et al. (2018) and further verified by Glavaš et al. (2019) and Doval et al. (2019), the language pair at hand can have a huge impact on CLWE induction: the adversarial method of Conneau et al. (2018a) often gets stuck in poor local optima and yields degenerate solutions for distant language pairs such as English-Finnish. More recent CLWE methods (Artetxe et al., 2018b; Mohiuddin and Joty, 2019) focus on mitigating this robustness issue. However, they still rely on one critical assumption which leads them to degraded performance for distant language pairs: they assume approximate isomorphism (Barone, 2016; Søgaard et al., 2018) between monolingual embedding spaces to learn the initial seed dictionary. In other words, they assume very similar geometric constellations between two monolingual spaces: due to the Zipfian phenomena in language (Zipf, 1949) such near-isomorphism can be satisfied only for similar languages and for similar domains used for training monolingual vectors. This property is reflected in the results reported in Table 3, the number of unsuccessful setups in Table 4, as well as later in Figure 4. For instance, the largest number of unsuccessful BLI setups with the UNSUPERVISED"
D19-1449,P19-1070,1,0.828323,"Missing"
D19-1449,Q17-1010,0,0.186535,"rature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CLWE induction (e.g., when BLI performance is similar to a random baseline): detecting such CLWEs is espe"
D19-1449,D14-1082,0,0.0593674,"uage pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Lit"
D19-1449,N15-1157,0,0.0457001,"lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries t"
D19-1449,D18-1024,0,0.0746406,"typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can rough"
D19-1449,L18-1550,0,0.0283189,"nguage pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families. We run BLI evaluations for all language pairs in both directions, for a total of 15×14=210 BLI setups. Monolingual Embeddings. We use the 300-dim vectors of Grave et al. (2018) for all 15 languages, pretrained on Common Crawl and Wikipedia with fastText (Bojanowski et al., 2017).7 We trim all 5 While BLI is an intrinsic task, as discussed by Glavaš et al. (2019) it is a strong indicator of CLWE quality also for downstream tasks: relative performance in the BLI task correlates well with performance in cross-lingual information retrieval (Litschko et al., 2018) or natural language inference (Conneau et al., 2018b). More importantly, it also provides a means to analyze whether a CLWE method manages to learn anything meaningful at all, and can indicate “unsuccessful” CL"
D19-1449,P15-1119,0,0.0322898,"transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui"
D19-1449,D18-1269,0,0.0727556,"eak cross-lingual supervision. Originally, the seed dictionaries typically spanned several thousand word pairs (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-tra"
D19-1449,N19-1188,1,0.675442,"Missing"
D19-1449,N18-2085,0,0.0180662,"nslations for a test set of source language words. Its lightweight nature allows us to conduct a comprehensive evaluation across a large number of language pairs.5 Since BLI is cast as a ranking task, following Glavaš et al. (2019) we use mean average precision (MAP) as the main evaluation metric: in our BLI setup with only one correct translation for each “query” word, MAP is equal to mean reciprocal rank (MRR).6 (Selection of) Language Pairs. Our selection of test languages is guided by the following goals: a) following recent initiatives in other NLP research (e.g., for language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure the coverage of different genealogical and typological language properties, and b) we aim to analyze a large set of language pairs and offer new evaluation data which extends and surpasses other work in the CLWE literature. These two properties will facilitate analyses between (dis)similar language pairs and offer a comprehensive set of evaluation setups that test the robustness and portability of fully unsupervised CLWEs. The final list of 15 diverse test languages is provided in Table 2, and includes samples from different languages types and families."
D19-1449,E17-1102,1,0.919319,"Missing"
D19-1449,D18-1043,0,0.222539,"languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language families and morphological types, as we argue that fully unsupervised CLWEs have been designed to support exactly these setups. However, we show that even the most robust unsupervised CLWE method (Artetxe et al., 2018b) still fails for a large number of langu"
D19-1449,N19-1386,0,0.228386,"Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupervised extraction of a seed dictionary; C2) a"
D19-1449,kamholz-etal-2014-panlex,0,0.516415,"Kartvelian Koreanic IE: Baltic IE: Germanic Kra-Dai Turkic fusional fusional agglutinative agglutinative agglutinative agglutinative introflexive agglutinative isolating agglutinative agglutinative fusional fusional isolating agglutinative BG CA EO ET EU FI HE HU ID KA KO LT NO TH TR Table 2: The list of 15 languages from our main BLI experiments along with their corresponding language family (IE = Indo-European), broad morphological type, and their ISO 639-1 code. vocabularies to the 200K most frequent words. Training and Test Dictionaries. They are derived from PanLex (Baldwin et al., 2010; Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers some support and supervision also for low-resource language pairs (Adams et al., 2017). For each source language (L1 ), we automatically translate their vocabulary words (if they are present in PanLex) to all 14 target (L2 ) languages. To ensure the reliability of the translation pairs, we retain only unigrams found in the vocabularies of the respective L2 monolingual spaces which scored above a Pa"
D19-1449,D18-1047,0,0.0484142,"solute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demon"
D19-1449,K18-1021,0,0.0602112,"an inherently interesting research topic per se. Nonetheless, the main practical motivation for developing such approaches in the first place is to facilitate the construction of multilingual NLP tools and widen the access to language technology for resource-poor languages and language pairs. However, the first attempts at fully unsupervised CLWE induction failed exactly for these use cases, as shown by Søgaard et al. (2018). Therefore, the follow-up work aimed to improve the robustness of unsupervised CLWE induction by introducing more robust self-learning procedures (Artetxe et al., 2018b; Kementchedjhieva et al., 2018). Besides increased robustness, recent work claims that fully unsupervised projection-based CLWEs can even match or surpass their supervised counterparts (Conneau et al., 2018a; Artetxe et al., 2018b; Alvarez-Melis and Jaakkola, 2018; Hoshen and Wolf, 2018; Heyman et al., 2019). In this paper, we critically examine these claims on robustness and improved performance of unsupervised CLWEs by running a large-scale evaluation in the bilingual lexicon induction (BLI) task on 15 languages (i.e., 210 languages pairs, see Table 2 in §3). The languages were selected to represent different language fam"
D19-1449,P19-1492,0,0.0933756,"Missing"
D19-1449,D18-1101,0,0.0167767,"t al., 2013a; Faruqui and Dyer, 2014; Xing et al., 2015), but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs (Vuli´c and Korhonen, 2016), identical strings (Smith et al., 2017), or even only shared numerals (Artetxe et al., 2017). Taking the idea of reducing cross-lingual supervision to the extreme, the latest CLWE developments almost exclusively focus on fully unsupervised approaches (Conneau et al., 2018a; Artetxe et al., 2018b; Dou et al., 2018; Chen and Cardie, 2018; Alvarez-Melis and Jaakkola, 2018; Kim et al., 2018; Alaux et al., 2019; Mohiuddin and Joty, 2019, inter alia): they fully abandon any source of 4407 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4407–4418, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces. Their modus operandi can roughly be described by three main components: C1) unsupe"
D19-1449,C12-1089,0,0.180369,"n a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and com"
D19-1449,2005.mtsummit-papers.11,0,0.0217419,"more, one could argue that we do not need unsupervised CLWEs for similar languages in the first place: we can harvest cheap supervision here, e.g., cognates. The main motivation behind unsupervised approaches is to support dissimilar and resourcepoor language pairs for which supervision cannot be guaranteed. Domain Differences. Finally, we also verify that UNSUPERVISED CLWEs still cannot account for domain differences when training monolingual vectors. We rely on the probing test of Søgaard et al. (2018): 300-dim fastText vectors are trained on 1.1M sentences on three corpora: 1) EuroParl.v7 (Koehn, 2005) (parliamentary proceedings); 2) Wikipedia (Al-Rfou et al., 2013), and 3) EMEA (Tiedemann, 2009) (medical), and BLI evaluation for three language pairs is conducted on standard MUSE BLI test sets (Conneau et al., 2018a). The results, summarized in Figure 4, reveal that UN SUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups. Weakly supervised methods (|D0 |= 500 or D0 seeded with identical"
D19-1449,P15-1165,0,0.159533,"Missing"
D19-1449,P18-1072,1,0.810814,"Missing"
D19-1449,D18-1549,0,0.0740195,"Missing"
D19-1449,P10-1040,0,0.0777116,"e languages (210 language pairs) show that fully unsupervised CLWE methods still fail for a large number of language pairs (e.g., they yield zero BLI performance for 87/210 pairs). Even when they succeed, they never surpass the performance of weakly supervised methods (seeded with 500-1,000 translation pairs) using the same self-learning procedure in any BLI setup, and the gaps are often substantial. These findings call for revisiting the main motivations behind fully unsupervised CLWE methods. 1 Introduction and Motivation The wide use and success of monolingual word embeddings in NLP tasks (Turian et al., 2010; Chen and Manning, 2014) has inspired further research focus on the induction of cross-lingual word embeddings (CLWEs). CLWE methods learn a shared cross-lingual word vector space where words with similar meanings obtain similar vectors regardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vu"
D19-1449,P16-1024,1,0.918618,"Missing"
D19-1449,N19-1045,0,0.0138207,"the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. Further, the most important contributions of unsupervised CLWE models are, in fact, the improved and more robust self-learning procedures (component C2) and technical enhancements (component C3). In this work we have demonstrated that these components can be equally applied to weakly sup"
D19-1449,D17-1270,1,0.904042,"Missing"
D19-1449,N15-1104,0,0.525331,"Missing"
D19-1449,P19-1307,0,0.0605721,"tance, the underlying assumption of all projection-based methods (both supervised and unsupervised) is the topological similarity between monolingual spaces, which is why standard simple linear projections result in lower absolute BLI scores for distant pairs (see Table 4 and results in the supplemental material). Unsupervised approaches even exploit the assumption twice as their seed extraction is fully based on the topological similarity. Future work should move beyond the restrictive assumption by exploring new methods that can, e.g., 1) increase the isomorphism between monolingual spaces (Zhang et al., 2019) by distinguishing between language-specific and language-pairinvariant subspaces; 2) learn effective non-linear or multiple local projections between monolingual spaces similar to the preliminary work of Nakashole (2018); 3) similar to Vuli´c and Korhonen (2016) and Lubin et al. (2019) “denoisify” seed lexicons during the self-learning procedure. For instance, keeping only mutual/symmetric nearest neighbour as in FULL + SL + SYM can be seen as a form of rudimentary denoisifying: it is indicative to see that the best overall performance in this work is reported with that model configuration. F"
D19-1449,N16-1156,0,0.0208315,"tilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-lingual supervision. Originally, the seed dictionaries typically spanned seve"
D19-1449,D18-1022,1,0.928371,"ardless of their actual language. CLWEs benefit cross-lingual NLP, enabling multilingual modeling of meaning and supporting cross-lingual transfer for downstream tasks and resource-lean languages. CLWEs provide invaluable cross-lingual knowledge for, inter alia, bilingual lexicon induction (Gouws et al., 2015; Heyman et al., 2017), information retrieval (Vuli´c and Moens, 2015; Litschko et al., 2019), machine translation (Artetxe et al., 2018c; Lample et al., 2018b), document classification (Klementiev et al., 2012), cross-lingual plagiarism detection (Glavaš et al., 2018), domain adaptation (Ziser and Reichart, 2018), cross-lingual POS tagging (Gouws and Søgaard, 2015; Zhang et al., 2016), and cross-lingual dependency parsing (Guo et al., 2015; Søgaard et al., 2015). The landscape of CLWE methods has recently been dominated by the so-called projection-based methods (Mikolov et al., 2013a; Ruder et al., 2019; Glavaš et al., 2019). They align two monolingual embedding spaces by learning a projection/mapping based on a training dictionary of translation pairs. Besides their simple conceptual design and competitive performance, their popularity originates from the fact that they rely on rather weak cross-ling"
D19-3031,D18-1547,1,0.855815,"Missing"
D19-3031,W17-5526,0,0.0534769,"Missing"
D19-3031,W17-5506,0,0.0177848,"erson et al., 2014; Mrkˇsi´c et al., 2015) which enumerate the constraints the users can express using a collection of slots (e.g., PRICE RANGE for restaurant search) and their slot values (e.g., CHEAP, EXPENSIVE for the aforementioned slots). Conversations are then modelled as a sequence of actions that constrain slots to particular values. This explicit semantic space is manually engineered by the system designer. It serves as the output of the natural language understanding component as well as the input to the language generation component both in traditional modular systems (Young, 2010; Eric et al., 2017) and in more recent end-to-end task-oriented dialogue systems (Wen et al., 2017; Li et al., 2017; Bordes et al., 2017; Budzianowski et al., 2018, inter alia). Working with such explicit semantics for taskoriented dialogue systems poses several critical challenges on top of the manual time-consuming domain ontology design. First, it is difficult to collect domain-specific data labelled with explicit semantic representations. As a consequence, despite recent data collection efforts to enable training of task-oriented systems across multiple domains (El Asri et al., 2017; Budzianowski et al., 201"
D19-3031,W19-4101,1,0.833944,"Missing"
D19-3031,P19-1536,1,0.859276,"Missing"
D19-3031,L16-1147,0,0.0156308,"to form the final 320-dimensional encoding. That encoding is then passed through three fully-connected non-linear hidden layers of dimensionality 1, 024. The final layer is linear and maps the text into the final l-dimensional (l = 512) representation: hc and hr . Other standard and more sophisticated encoder models can also be used to provide final encodings hc and hr , but the current architecture shows a good trade-off between speed and efficacy with strong and robust performance in our empirical evaluations on the response retrieval task using Reddit (Al-Rfou et al., 2016), OpenSubtitles (Lison and Tiedemann, 2016), and AmazonQA (Wan and McAuley, 2016) conversational test data, see (Henderson et al., 2019a) for further details.3 In training the√constant C is constrained to lie between 0 and l.4 Following Henderson et al. (2017), the scoring function in the training objective aims to maximise the similarity score of context-reply pairs that go together, while minimising the score of random pairings: negative examples. Training proceeds via SGD with batches comprising 500 pairs (1 positive and 499 negatives). Data Source 1: Reddit. For training text representations we use a Reddit dataset similar to AlRfo"
D19-3031,P15-2130,1,0.875498,"Missing"
D19-3031,N18-3006,0,0.0352811,"Missing"
D19-3031,E17-1042,1,0.89642,"Missing"
D19-3031,W12-1812,0,0.0145334,"the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages. 1 Introduction and Background Task-oriented dialogue systems are primarily designed to search and interact with large databases which contain information pertaining to a certain dialogue domain: the main purpose of such systems is to assist the users in accomplishing a welldefined task such as flight booking (El Asri et al., 2017), tourist information (Henderson et al., 2014), restaurant search (Williams, 2012), or booking a taxi (Budzianowski et al., 2018). These systems are typically constructed around rigid task-specific ontologies (Henderson et al., 2014; Mrkˇsi´c et al., 2015) which enumerate the constraints the users can express using a collection of slots (e.g., PRICE RANGE for restaurant search) and their slot values (e.g., CHEAP, EXPENSIVE for the aforementioned slots). Conversations are then modelled as a sequence of actions that constrain slots to particular values. This explicit semantic space is manually engineered by the system designer. It serves as the output of the natural language"
D19-3031,W14-4340,1,0.715465,"arity to the given context, and narrows down the list of relevant entities during the multi-turn conversation. We introduce a restaurant search and booking system powered by the PolyResponse engine, currently available in 8 different languages. 1 Introduction and Background Task-oriented dialogue systems are primarily designed to search and interact with large databases which contain information pertaining to a certain dialogue domain: the main purpose of such systems is to assist the users in accomplishing a welldefined task such as flight booking (El Asri et al., 2017), tourist information (Henderson et al., 2014), restaurant search (Williams, 2012), or booking a taxi (Budzianowski et al., 2018). These systems are typically constructed around rigid task-specific ontologies (Henderson et al., 2014; Mrkˇsi´c et al., 2015) which enumerate the constraints the users can express using a collection of slots (e.g., PRICE RANGE for restaurant search) and their slot values (e.g., CHEAP, EXPENSIVE for the aforementioned slots). Conversations are then modelled as a sequence of actions that constrain slots to particular values. This explicit semantic space is manually engineered by the system designer. It serves as"
D19-5602,P18-1031,0,0.021205,"led corpora to enable task-oriented dialogue modeling. In this work, we rely on the standard language modeling (LM) pretraining, where the task is to predict the next word given the preceding word sequence (Bengio et al., 2003). The objective maximizes the likelihood over the word sequence S = {w1 , ..., w|S |}: |S| ∑ L1 (S) = log P (wi |w0 , w1 , ..., wi−1 ). (1) Figure 1: Dialogue-context-to-text task. i=1 Transfer learning based on such LM pretraining combined with the Transformer decoder model (Vaswani et al., 2017) resulted in significant progress across many downstream tasks (Rei, 2017; Howard and Ruder, 2018; Radford et al., 2018, 2019). corpora can support task-oriented dialogue applications. We first discuss how to combine a set of diverse components such as word tokenization, multi-task learning, and probabilistic sampling to support task-oriented applications. We then show how to adapt the task-oriented dialogue framework to operate entirely on text input, effectively bypassing an explicit dialogue management module and a domain-specific natural language generation module. The proposed model operates entirely in the sequence-to-sequence fashion, consuming only simple text as input. The entire"
D19-5602,D18-1547,1,0.894404,"Missing"
D19-5602,P19-1360,0,0.255538,"and domains, we rely on the multi-domain MultiWOZ dataset (Budzianowski et al., 2018b). MultiWOZ consists of 7 domains and 10, 438 dialogues and it is substantially larger than previous available datasets (Wen et al., 2017; El Asri et al., 2017). The conversations are natural as they were gathered through human-human interactions. However, the dialogues are based on domain-specific vocabulary such as booking IDs or telephone numbers that need to be delexicalized as they are entirely database-dependent. 5 Results and Analysis Following prior work (Budzianowski et al., 2018b; Zhao et al., 2019; Chen et al., 2019), our evaluation task is the dialogue-context-to-text generation task (see Figure 1). Given a dialogue history, the oracle belief state and the database state, the model needs to output the adequate response. By relying on the oracle belief state, prior work has bypassed the possible errors originating from natural language understanding (Budzianowski et al., 2018b). The main evaluation is based on the comparison between the following two models: 1) the baseline is a neural response generation model with an oracle belief state obtained from the wizard annotations as in (Budzianowski et al., 20"
D19-5602,P18-1133,0,0.0801732,"ules. As mentioned in §2.1, the token level layer (Figure 2) informs the transformer decoder what part of the input comes from the system side or from the user side. In our framework, we create two task-oriented specific tokens (System and User tokens) that are learned during fine-tuning. 3.2 Simple Text-Only Input There have been some empirical validations recently which suggest that posing NLP tasks in the form of simple text can yield improvements with unsupervised architectures (Wolf et al., 2019; Radford et al., 2019). For instance, in task-oriented dialogue modeling the Sequicity model (Lei et al., 2018) sees the classification over the belief state as a generation problem. That way, the entire dialogue model pipeline is based on the sequence-tosequence architecture: the output from one model is the input to the subsequent recurrent model. We follow this approach by providing both the belief state and the knowledge base state in a simple text format to the generator. This significantly simplifies the paradigm of building task-oriented models: any new source of information can be simply 3.4 Generation Quality Finally, the long-standing problem of dull and repetitive response generation (Li et"
D19-5602,W17-5526,0,0.06284,"Missing"
D19-5602,D17-1230,0,0.139528,", London, UK pfb30@cam.ac.uk, iv250@cam.ac.uk 1 Abstract (Young et al., 2013). On the other hand, opendomain conversational bots (Li et al., 2017; Serban et al., 2017) can leverage large amounts of freely available unannotated data (Ritter et al., 2010; Henderson et al., 2019a). Large corpora allow for training end-to-end neural models, which typically rely on sequence-to-sequence architectures (Sutskever et al., 2014). Although highly datadriven, such systems are prone to producing unreliable and meaningless responses, which impedes their deployment in the actual conversational applications (Li et al., 2017). Due to the unresolved issues with the end-toend architectures, the focus has been extended to retrieval-based models. Here, the massive datasets can be leveraged to aid task-specific applications (Kannan et al., 2016; Henderson et al., 2017, 2019b). The retrieval systems allow for the full control over system responses, but the behaviour of the system is often highly predictable. It also depends on the pre-existing set of responses, and the coverage is typically insufficient for a multitude of domains and tasks. However, recent progress in training high-capacity language models (e.g., GPT, G"
D19-5602,P19-1608,0,0.196151,"cations (Kannan et al., 2016; Henderson et al., 2017, 2019b). The retrieval systems allow for the full control over system responses, but the behaviour of the system is often highly predictable. It also depends on the pre-existing set of responses, and the coverage is typically insufficient for a multitude of domains and tasks. However, recent progress in training high-capacity language models (e.g., GPT, GPT-2) (Radford et al., 2018, 2019) on large datasets reopens the question of whether such generative models can support task-oriented dialogue applications. Recently, Wolf et al. (2019) and Golovanov et al. (2019) showed that the GPT model, once fine-tuned, can be useful in the domain of personal conversations. In short, their approach led to substantial improvements on the Persona-Chat dataset (Zhang et al., 2018), showcasing the potential of exploiting large pretrained generative models in the conversational domain.1 In this paper, we demonstrate that large generative models pretrained on large general-domain Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to lea"
D19-5602,P02-1040,0,0.103302,"Missing"
D19-5602,P17-1194,0,0.0259809,"rge unlabelled corpora to enable task-oriented dialogue modeling. In this work, we rely on the standard language modeling (LM) pretraining, where the task is to predict the next word given the preceding word sequence (Bengio et al., 2003). The objective maximizes the likelihood over the word sequence S = {w1 , ..., w|S |}: |S| ∑ L1 (S) = log P (wi |w0 , w1 , ..., wi−1 ). (1) Figure 1: Dialogue-context-to-text task. i=1 Transfer learning based on such LM pretraining combined with the Transformer decoder model (Vaswani et al., 2017) resulted in significant progress across many downstream tasks (Rei, 2017; Howard and Ruder, 2018; Radford et al., 2018, 2019). corpora can support task-oriented dialogue applications. We first discuss how to combine a set of diverse components such as word tokenization, multi-task learning, and probabilistic sampling to support task-oriented applications. We then show how to adapt the task-oriented dialogue framework to operate entirely on text input, effectively bypassing an explicit dialogue management module and a domain-specific natural language generation module. The proposed model operates entirely in the sequence-to-sequence fashion, consuming only simple t"
D19-5602,W19-4101,1,0.877087,"Missing"
D19-5602,N10-1020,0,0.09857,"Missing"
D19-5602,P17-1061,0,0.0279218,"esponse generation model with an oracle belief state obtained from the wizard annotations as in (Budzianowski et al., 2018a); 2) the model proposed in §4 and shown in Figure 2 that works entirely with text-only format as input (see §4). We test all three available pretrained GPT models - the original GPT model (Radford et al., 2018). and two GPT-2 models referred to as small (GPT2) and medium (GPT2-M) (Radford et al., 2019). Natural Language as (the Only) Input. GPT operates solely on the text input. This is in opposition to the standard task-oriented dialogue architectures (Wen et al., 2017; Zhao et al., 2017) where the belief state and the database state are encoded in a numerical form. For example, the database state is typically defined as n-bin encodings representing a number of available entities at the current state of the conversation (Wen et al., 2017). Therefore, we transform the belief state and the knowledge base representation to a simple text representation. The belief state takes the following form: Domain1 Slot1 Value1 Slot2 Value2 Domain2 Slot1 ... and the database representation is provided as: Domain1 # of entities Domain2 # of entities ... 2 This is also similar in spirit to the"
D19-5602,D15-1199,0,0.12346,"Missing"
D19-5602,E17-1042,0,0.145032,"Missing"
D19-5602,P18-1205,0,0.183689,"s on the pre-existing set of responses, and the coverage is typically insufficient for a multitude of domains and tasks. However, recent progress in training high-capacity language models (e.g., GPT, GPT-2) (Radford et al., 2018, 2019) on large datasets reopens the question of whether such generative models can support task-oriented dialogue applications. Recently, Wolf et al. (2019) and Golovanov et al. (2019) showed that the GPT model, once fine-tuned, can be useful in the domain of personal conversations. In short, their approach led to substantial improvements on the Persona-Chat dataset (Zhang et al., 2018), showcasing the potential of exploiting large pretrained generative models in the conversational domain.1 In this paper, we demonstrate that large generative models pretrained on large general-domain Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of taskspecific data. In this paper, we demonstrate that recent progress in language modeling pr"
D19-5602,N19-1123,0,0.058184,"sed dialogue tasks and domains, we rely on the multi-domain MultiWOZ dataset (Budzianowski et al., 2018b). MultiWOZ consists of 7 domains and 10, 438 dialogues and it is substantially larger than previous available datasets (Wen et al., 2017; El Asri et al., 2017). The conversations are natural as they were gathered through human-human interactions. However, the dialogues are based on domain-specific vocabulary such as booking IDs or telephone numbers that need to be delexicalized as they are entirely database-dependent. 5 Results and Analysis Following prior work (Budzianowski et al., 2018b; Zhao et al., 2019; Chen et al., 2019), our evaluation task is the dialogue-context-to-text generation task (see Figure 1). Given a dialogue history, the oracle belief state and the database state, the model needs to output the adequate response. By relying on the oracle belief state, prior work has bypassed the possible errors originating from natural language understanding (Budzianowski et al., 2018b). The main evaluation is based on the comparison between the following two models: 1) the baseline is a neural response generation model with an oracle belief state obtained from the wizard annotations as in (Bud"
D19-5602,P16-1162,0,\N,Missing
E12-1034,P08-1090,0,0.133535,"other events that are likely to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the"
E12-1034,P09-1068,0,0.501443,"to belong to the script. Our work aims to answer key questions about how best to (1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these qu"
E12-1034,P11-1098,0,0.0157306,"ecting narrative event statistics, and show that this approach performs better than classic n-gram statistics. Introduction There has been recent interest in automatically acquiring world knowledge in the form of scripts (Schank and Abelson, 1977), that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant. All of the techniques so far proposed for this task share a common sub-task: given an event or partial chain of events, predict other events that belong to the same script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2011; Manshadi et al., 2008; McIntyre and Lapata, 2009; McIntyre and Lapata, 2010; Regneri et al., 2010). Such a model can then serve as input to a system that identifies the order of the events • We propose a new method for ranking events given a partial script, and show that it performs substantially better than ranking methods from prior work. • We propose a new evaluation procedure (using Recall@N) for the cloze test, and advocate its usage instead of average rank used previously in the literature. • We provide a systematic analysis of the interactions between the choices made when constructin"
E12-1034,guthrie-etal-2006-closer,0,0.0137734,"ointwise mutual informations between the event e and each of the events in the script: • 1-skip bigrams. We collect pairs of events that occur with 0 or 1 events intervening between them. For example, given the chain (saw, SUBJ), (kissed, OBJ), (blushed, SUBJ), we would extract three bigrams: the two regular bigrams ((saw, SUBJ), (kissed, OBJ)) and ((kissed, OBJ), (blushed, SUBJ)), plus the 1skip-bigram, ((saw, SUBJ), (blushed, SUBJ)). This approach to collecting n-gram statistics is sometimes called skip-gram modeling, and it can reduce data sparsity by extracting more event pairs per chain (Guthrie et al., 2006). It has not previously been applied in the task of predicting script events, but it may be quite appropriate to this task because in most scripts it is possible to skip some events in the sequence. Chambers and Jurafsky’s description of this score suggests that it is unordered, such that P (a, b) = P (b, a). Thus the probabilities must be defined as: f (e, c) = log i P (e1 , e2 ) = P (ci , e) P (ci )P (e) C(e1 , e2 ) + C(e2 , e1 ) PP C(ei , ej ) ei ej C(e) 0 e0 C(e ) P (e) = P where C(e1 , e2 ) is the number of times that the ordered event pair (e1 , e2 ) was counted in the training data, and"
E12-1034,P03-1054,0,0.0243036,", (3) applying a coreference resolution system and (4) identifying event chains via entities and dependencies. First, articles that had no narrative content were removed from the corpora. In the Reuters Corpus, we removed all files solely listing stock exchange values, interest rates, etc., as well as all articles that were simply summaries of headlines from different countries or cities. After removing these files, the Reuters corpus was reduced to 788, 245 files. Removing files from the Fairy Tale corpus was not necessary – all 437 stories were retained. We then applied the Stanford Parser (Klein and Manning, 2003) to identify the dependency structure of each sentence in each article in the corpus. This parser produces a constitutent-based syntactic parse tree for each sentence, and then converts this tree to a collapsed dependency structure via a set of tree patterns. Next we applied the OpenNLP coreference engine5 to identify the entities in each article, and the noun phrases that were mentions of each entity. Finally, to identify the event chains, we took each of the entities proposed by the coreference system, walked through each of the noun phrases associated with that entity, retrieved any subject"
E12-1034,P09-1025,0,0.247915,"(1) identify representative event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these questions, this article makes several contributions to the field of script and na"
E12-1034,P10-1158,0,0.474702,"event chains from a source text, (2) gather statistics from the event chains, and (3) choose ranking functions for predicting new script events. We make several contributions, introducing skip-grams for collecting event statistics, designing improved methods for ranking event predictions, defining a more reliable evaluation metric for measuring predictiveness, and providing a systematic analysis of the various event prediction models. 1 within that script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) or that generates a story using the selected events (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010). In this article, we analyze and compare techniques for constructing models that, given a partial chain of events, predict other events that belong to the script. In particular, we consider the following questions: • How should representative chains of events be selected from the source text? • Given an event chain, how should statistics be gathered from it? • Given event n-gram statistics, which ranking function best predicts the events for a script? In the process of answering these questions, this article makes several contributions to the field of script and narrative event chain understa"
E12-1034,P10-1100,0,0.522091,"ics. Introduction There has been recent interest in automatically acquiring world knowledge in the form of scripts (Schank and Abelson, 1977), that is, frequently recurring situations that have a stereotypical sequence of events, such as a visit to a restaurant. All of the techniques so far proposed for this task share a common sub-task: given an event or partial chain of events, predict other events that belong to the same script (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2011; Manshadi et al., 2008; McIntyre and Lapata, 2009; McIntyre and Lapata, 2010; Regneri et al., 2010). Such a model can then serve as input to a system that identifies the order of the events • We propose a new method for ranking events given a partial script, and show that it performs substantially better than ranking methods from prior work. • We propose a new evaluation procedure (using Recall@N) for the cloze test, and advocate its usage instead of average rank used previously in the literature. • We provide a systematic analysis of the interactions between the choices made when constructing an event prediction model. 336 Proceedings of the 13th Conference of the European Chapter of the A"
E12-1046,C02-2020,0,0.0586144,"one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between la"
E12-1046,C02-1166,0,0.387985,"Missing"
E12-1046,W04-3208,0,0.0259042,"ild such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled"
E12-1046,P98-1069,0,0.0950968,"In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in comm"
E12-1046,P04-1067,0,0.334945,"Missing"
E12-1046,P08-1088,0,0.156009,"exicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corp"
E12-1046,W02-0902,0,0.544091,"ds have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons. Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. However, the quality of their initial seed lexicon is disputable, 449 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449–459, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics since the construction of their lexicon is languagepair biased and cannot be completely employed on distant languages. It solely relies on unsatisfactory language-pair independent cross-language clue"
E12-1046,C10-1070,0,0.162042,"suming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach"
E12-1046,J00-2011,0,0.733077,"duced for word alignments in SMT (Och and Ney, 2003), where the intersection heuristics is employed for a precision-oriented algorithm. In our setting, it basically means that we keep a translation pair (wiS , wjT ) if and only if, after the symmetrization process, the top translation candidate for the source word wiS is the target word wiT and vice versa. The one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search. The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). The remainder of the paper is structured as follows. Section 2 gives a brief overview of the methods, relying on per-topic word distributions, which serve as the tool for computing crosslanguage similarity between words. In Section 3, we motivate the main assumptions of the algorithm and describe the full algorithm. Section 4 justifies the underlying assumptions of the algorithm by providing comparisons with a current-state-of-the-art system for Italian-English and Dutch-English language pairs. It also contains another set of experiments which investigates the potential of the algorithm in b"
E12-1046,D09-1092,0,0.0376677,"Missing"
E12-1046,P07-1084,0,0.0223644,"nd. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obta"
E12-1046,J05-4003,0,0.0148281,"For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method i"
E12-1046,J03-1002,0,0.021325,"metrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 1 Introduction Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (SMT) (Och and Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995"
E12-1046,P95-1050,0,0.449495,"Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but t"
E12-1046,P99-1067,0,0.318419,"t high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need fo"
E12-1046,P10-1011,0,0.0672013,"en an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (20"
E12-1046,P03-1010,0,0.0188721,"and 9, 116 English nouns. For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for t"
E12-1046,E09-1096,0,0.024643,"and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method is set to λ = 0.1. The parameters of the algorithm, adjusted on"
E12-1046,P11-2084,1,0.463208,"Missing"
E12-1046,C98-1066,0,\N,Missing
E12-1046,J00-2004,0,\N,Missing
E12-1046,P11-2083,0,\N,Missing
E17-1016,W13-3520,0,0.0812908,"Missing"
E17-1016,W16-2519,0,0.0207043,"system and the ground truth ranking. This protocol, however, is not directly applicable to the USF test data. First, the evaluated relation of WA is asymmetric, and the pairs (X, Y ) and (Y, X) may differ dramatically in their WA scores (see the difference in FSG and BSG values from Tab. 1). Second, instead of one global list of pairs, the data comprises a series of ranked lists conditioned on the cue/normed word wc (see Tab. 1 again). Finally, unlike with SimLex999 or MEN scores where it is difficult to interpret “what a similarity/relatedness of 7.69 exactly means” (Batchkarov et al., 2016; Avraham and Goldberg, 2016), the USF FSG scores have a direct meaningful interpretation (i.e., F SG = #P/#G). To fully capture all aspects of the ground truth USF data set, an evaluation protocol should ideally be based not only on response rankings, but also on the actual scores, i.e., the association strength. In this paper, we propose and investigate two different families of evaluation metrics on the USF data: Sect. 3.1 discusses rank correlation evaluation metrics inspired by recent work on the evaluation of vector space models in distributional semantics (Bruni et al., 2014; Hill et al., 2015; Vuli´c et al., 165 2"
E17-1016,P14-1023,0,0.0648873,"t topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word distributions P (·|toi ) (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow ="
E17-1016,W16-2502,0,0.0876675,"ntation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparison"
E17-1016,E14-1049,0,0.0405544,"re (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ rule on the LDA output per-topic word dist"
E17-1016,N15-1184,0,0.085062,"Missing"
E17-1016,P06-1038,0,0.0664548,"Missing"
E17-1016,N13-1092,0,0.0711201,"Missing"
E17-1016,D16-1235,1,0.876283,"Missing"
E17-1016,W09-3207,0,0.142675,"Missing"
E17-1016,C16-1175,0,0.135609,"Missing"
E17-1016,D14-1032,1,0.858335,"fter post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and comparison of current stat"
E17-1016,J15-4004,1,0.929697,"its direct analogy with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces in"
E17-1016,W16-2513,0,0.248291,"imilarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subc"
E17-1016,W14-1503,1,0.866108,"xp. IV. Exp. III: Window Size In the next experiment, we analysed the effect of the window size on models’ ability to capture similarity, relatedness, and association. We train the sgns-pw-bow model (d = 300) with varying window sizes in the interval [1, 30]. The results on similarity (SimLex-999), relatedness (MEN), and WA benchmarks (USF) are presented in Fig. 1(a)-1(b). It is clear that using larger windows deteriorates the performance on SimLex-999 as the focus of the model is shifted from functional to topical similarity. This shift has been detected in prior work on vector space models (Kiela and Clark, 2014). However, we also observe a similar trend with MEN scores, although an opposite effect was expected, which questions the ability of MEN to accurately evaluate relatedness. The opposite effect is, however, visible with the WA evaluation, where it is evident that larger win170 dows (leading to topical similarity) lead to better WA estimates. This also provides the first hint that WA and semantic similarity capture two completely distinct semantic phenomena. Exp. IV: WA vs. Similarity vs. Relatedness We delve deeper into this conjecture by computing correlations between model rankings on the WA"
E17-1016,P14-2135,1,0.843608,"son et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a systematic study and c"
E17-1016,D15-1242,1,0.871942,"Missing"
E17-1016,Y13-1013,0,0.0480397,"d investigate whether the difference originates from inadequate evaluation data and protocols (see Fig. 1(a)-1(b) again), or whether the difference is fundamental. 6 Conclusion and Future Work In future work, we plan to test the portability of the evaluation protocol and apply it to other repositories of word association data in English (De Deyne et al., 2016), as well as in other languages, using existing WA tables in, e.g., German (Schulte im Walde et al., 2008), Dutch (De Deyne and Storms, 2008; Brysbaert et al., 2014), Italian (Guida and Lenci, 2007), Japanese (Joyce, 2005), or Cantonese (Kwong, 2013).15 In another line of future work, we will experiment with other “cognitively plausible” evaluation data such as N400 (Kutas and Federmeier, 2011; Ettinger et al., 2016), and will analyse the similarities and differences between WA and other such “cognitive” evaluation protocols, as the one relying on semantic priming (SPP) (Hutchison et al., 2013; Ettinger and Linzen, 2016). All evaluation scripts and detailed guidelines related to this work are freely available at: github.com/cambridgeltl/wa-eval/ Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL (no 648909). The auth"
E17-1016,P14-2050,0,0.0672836,"e detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and the"
E17-1016,W13-3512,0,0.0696612,"t capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al., 2016) and does not account for the fact that human similarity judgements are asymmetric by nature (Tversky, 1977). What is more, given that humans perform linguistic comparisons between concepts on a subconscious level (Kutas and Federmeier, 2011), it is at least debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation"
E17-1016,W15-1521,0,0.0259935,"ng, rooted in the psychology literature (Steyvers et al., 2004; Griffiths et al., 2007; Steyvers and Griffiths, 2007).9 The following quantitative model of word association has been proposed (Griffiths et al., 2007): 167 9 Griffiths et al. (2007) also experimented with LSA (Landauer and Dumais, 1997) and found that their LDA-based approach consistently outperformed LSA-based approaches. P (wr |wc ) = M X P (wr |toi )P (toi |wc ) paragram-300d, (Wieting et al., 2015)) further refined using linguistic constraints (paragram+cf300d, (Mrkˇsi´c et al., 2016)); (3) Multilingual embedding models from Luong et al. (2015) (biskip256d) and Faruqui and Dyer (2014) (bicca-512d). More detailed descriptions of all VSM models are available in the listed papers and supplementary material attached to this work. (10) i=1 where wc is a cue word, wr ∈ V r any concept from the search space, and toi is the ith latent topic from the set of M topics induced from the corpus data (using LDA). We label this model LDA-assoc. The probability scores P (wr |toi ) select words that are highly descriptive for each particular topic. P (toi |wc ) scores are computed as in prior work, by assuming topic independence and applying Bayes’ r"
E17-1016,P04-1003,0,0.0426851,"ystematic study and comparison of current state-of-the-art representation learning architectures on the WA task. (C3) We present a systematic quantitative analysis of the connections between the models’ performance on the subconscious WA task and their performance on benchmarking similarity and relatedness evaluation sets. 2 Motivation: Association and USF Implicit Cognitive Measures: Means of Semantic Evaluation? Several studies have shown clear correspondence between implicit cognitive measures (most notably semantic priming) and semantic relations encountered in vector space models (VSMs) (McDonald and Brew, 2004; Jones et al., 2006; Pad´o and Lapata, 2007; Herda˘gdelen et al., 2009), suggesting that some of the implicit relation structure in the human brain is already reflected in current statistical models of meaning. These findings encouraged Ettinger and Linzen (2016) to propose a preliminary evaluation framework based on semantic priming experiments (Meyer and Schvaneveldt, 1971).4 They demonstrate the feasibility of such an evaluation using a subconscious language processing task. They use the online database of the Semantic Priming Project (SPP), which compiles priming data for over 6,000 word"
E17-1016,N16-1118,0,0.0350172,"Missing"
E17-1016,N16-1018,0,0.0262075,"Missing"
E17-1016,J07-2002,0,0.137456,"Missing"
E17-1016,P15-2070,0,0.0485682,"Missing"
E17-1016,D14-1162,0,0.0961134,"and Moens, 2013).10 We train LDA with 1,000 topics using suggested parameters (Griffiths et al., 2007). Count-Based Models We evaluate the best performing reduced count-based model from (Baroni et al., 2014). We label this model count-ppmi500d.11 For a more detailed description of the model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The genera"
E17-1016,D15-1036,0,0.0358539,"with information retrieval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variabili"
E17-1016,K15-1026,0,0.0399438,"model’s training data and setup we refer the reader to the original work and supplementary material. Vector Space Models We also compare the performance of prominent representation models on the WA USF task. We include: (1) unsupervised models that learn from distributional information in text, including Glove (Pennington et al., 2014) with d = 50 and d = 300 dimensions (glove-6B-50d and glove-6B-300d), the skip-gram negative-sampling (SGNS) 300dimensional vectors (Mikolov et al., 2013) with various contexts (bow = bag-of-words; deps = dependency contexts) as in (Levy and Goldberg, 2014) and (Schwartz et al., 2015) (sgns-pw-bow-w2, sgns-pw-bow-w5, sgns-pw-deps, sgns-8b-boww2), and the symmetric-pattern based vectors by Schwartz et al. (2015) (sympat-500d); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. Here, we use vectors finetuned to a paraphrase database (paragram-25d, 10 The generative model closely resembles the actual process in the human brain (Griffiths et al., 2007) - when we generate responses, we first tend to associate that word with a related semantic/cognitive concept, i.e., a latent topic (the factor P (toi |wc )), and then, after establishing the co"
E17-1016,D12-1130,0,0.0310874,"s (Nelson et al., 2000; Nelson et al., 2004). After post-processing, the repository contains ~5K queries, and ~70,000 (cue, response) pairs, making it one of the largest semantic evaluation databases available (by contrast, the largest word pair scoring data sets in NLP, SimVerb and MEN, contain 3,500 and 3,000 word pairs respectively). This new resource enables comprehensive quantitative studies of WA and may be used to guide the future development of representation learning architectures. While parts of the USF data set have been used for evaluation in NLP before (Michelbacher et al., 2007; Silberer and Lapata, 2012; Kiela et al., 2014; Hill and Korhonen, 2014, inter alia), we conduct the first full study regarding the evaluation on the quantitative WA task. We compare a wide variety of different semantic representation models, discuss various evaluation metrics and analyse the links between word association and semantic similarity and relatedness. In summary, the main contributions of this paper are as follows:3 (C1) We present an end-to-end evaluation framework for the WA task, and provide new evaluation metrics and detailed guidelines for evaluating semantic models on the WA task. (C2) We conduct a sy"
E17-1016,W16-2521,0,0.0197121,"ast debatable whether current similarity/relatedness evaluation sets fully capture the implicit relational structure underlying human language representation and understanding. As evidenced by recent workshops on evaluation of semantic representations1 , the community appears to recognise that current evaluation methods are inadequate. To fill in this gap, recent work has proposed using subconscious cognitive measures of semantic connection instead, as a proxy for measuring the ability of statistical models to tackle various problems in human language understanding (Ettinger and Linzen, 2016; Søgaard, 2016; Mandera et al., 2017). Motivated by these insights, this work proposes an evaluation framework based on the word association (WA) task, firmly rooted in and described by the psychology literature, e.g., Nelson et al. (2000) and Griffiths et al. (2007)2 . Word associations, provided as simple (cue, response) concept pairs, are naturally asymmetric: they tend to be given as a repository of ranked lists of concepts col1 E.g. RepEval, https://sites.google.com/site/repevalacl16/ The WA task is a free-association task, in which participants are asked to produce the first word that came into their"
E17-1016,D15-1243,0,0.0212972,"eval problems, (3) evaluate various state-of-the-art representation models on this task, and (4) discuss the relationship between WA and prior evaluations of semantic representation with well-known similarity and relatedness evaluation sets. We have made the WA evaluation toolkit publicly available. 1 Introduction The quality of word representations in semantic models is often measured using intrinsic evaluations that capture particular types of relationships (typically semantic similarity and relatedness) between word pairs (Finkelstein et al., 2002; Hill et al., 2015; Schnabel et al., 2015; Tsvetkov et al., 2015, inter alia). Whereas the notions of semantic similarity and relatedness constitute key concepts in such evaluations, they are in fact vaguely defined (Batchkarov et al., 2016; Ettinger and Linzen, 2016). The construction of ground truth evaluation sets that reflect these relations, such as SimLex-999 (Hill et al., 2015), SimVerb-3500 (Gerz et al., 2016), MEN (Bruni et al., 2014) or Rare Words (Luong et al., 2013), relies on manually constructed guidelines that trigger subjective human interpretation of the task at hand. This in turn introduces inter-annotator variability (Batchkarov et al.,"
E17-1016,N13-1011,1,0.811134,"Missing"
E17-1102,D15-1131,0,0.0503498,"d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual space: sim(w, v) = SF (w, ~ ~v ) = cos(w, ~ ~v ). A plethora of variant BWE models were proposed, differing mostly in the strength of bilingual supervision used in training (e.g., word, sentence, document alignments, translation pairs) (Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014; Chandar et al., 2014; Søgaard et al., 2015; Gouws et al., 2015; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia). Although the BLI evaluation of the BWE models was typically performed on Indo-European languages, none of the works attempted to learn character-level representations to enhance the BLI performance. In this work, we experiment with two BWE models that have demonstrated a strong BLI performance using only a small seed set of word translation pairs (Mikolov et al., 2013b), or document alignments (Vuli´c and Moens, 2016) for bilingual supervision. It is also important to note that other word-level 1086 translation evidence was investigated in the literature."
E17-1102,D16-1136,0,0.109307,"ngual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as dense real-valued vectors. The shared space enables words from"
E17-1102,D12-1001,0,0.038715,"including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., wor"
E17-1102,E14-1049,0,0.176486,"oth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as"
E17-1102,P16-1190,0,0.0432286,"Missing"
E17-1102,P08-1088,0,0.427392,"established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to improve on the BLI task. Moreover, while prior work typically relies on simple orthographic distance measures such as edit distance (Navarro, 2001), we show that such character-level representations can be induced from the data. Second, Irvine and Callison-Burch (2013; 2016) demonstrated that bilingual lexicon induction may be framed as a classification task where multiple heterogeneous translation clues/features may be easily combined. Yet, all current BLI models still"
E17-1102,P14-1006,0,0.0532448,"f td ], where f tk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual space: sim(w, v) = SF (w, ~ ~v ) = cos(w, ~ ~v ). A plethora of variant BWE models were proposed, differing mostly in the strength of bilingual supervision used in training (e.g., word, sentence, document alignments, translation pairs) (Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014; Chandar et al., 2014; Søgaard et al., 2015; Gouws et al., 2015; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia). Although the BLI evaluation of the BWE models was typically performed on Indo-European languages, none of the works attempted to learn character-level representations to enhance the BLI performance. In this work, we experiment with two BWE models that have demonstrated a strong BLI performance using only a small seed set of word translation pairs (Mikolov et al., 2013b), or document alignments (Vuli´c and Moens, 2016) for bilingual supervision. It is also important to"
E17-1102,P82-1020,0,0.828624,"Missing"
E17-1102,N13-1056,0,0.11565,"he 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to improve on the BLI task. Moreover, while prior work typically relies on simple orthographic distance measures such as edit distance (Navarro, 2001), we show that such character-level representations can be induced from the data. Second, Irvine and Callison-Burch (2013; 2016) demonstrated that bilingual lexicon induction may be framed as a classification task where multiple heterogeneous translation clues/features may be easily combined. Yet, all current BLI models still rely on straightforward similarity computations in the shared bilingual word-level semantic space (see Sect. 2). Motivated by these insights, we propose a novel bilingual lexicon induction (BLI) model that combines automatically extracted word-level and character-level representations in a classification framework. As the seminal bilingual representation model of Mikolov et al. (2013b), our"
E17-1102,W02-0902,0,0.717637,"a similarity function operating in the space (cosine similarity is typically used). A target language word v with the highest similarity score arg maxv SF (w, ~ ~v ) is then taken as the correct translation of a source language word w. In this work, we detect two major gaps in current representation learning for BLI. First, the standard embedding-based approach to BLI learns representations solely on the basis of word-level information. While early BLI works already established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to impr"
E17-1102,D14-1177,0,0.0128698,"er-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embedding (BWE) models. Given source and target language vo"
E17-1102,C10-1070,0,0.0293514,"is possible to effectively combine word- and character-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embeddi"
E17-1102,P15-1027,0,0.127058,"Missing"
E17-1102,W13-3523,0,0.0132662,"l context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embedding (BWE) models. Given source and target language vocabularies V S and V T , all BWE models learn a representation of each word w ∈ V S tV T as a realvalued vector: w ~ = [f t1 , . . . , f td ], where f tk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual s"
E17-1102,N01-1020,0,0.11925,"atures, and regularities (e.g., ideal:ideaal, apparition:aparici´on). Orthographic translation clues are even more important in certain domains such as medicine, where words with the same roots (from Greek and Latin), and abbreviations are frequently encountered (e.g., D-dimer:D-dimeer, meiosis:meiose). When present, such orthographic clues are typically strong indicators of translation pairs (Haghighi et al., 2008). This observation was exploited in BLI, applying simple string distance metrics such as Longest Common Subsequence Ratio (Melamed, 1995; Koehn and Knight, 2002), or edit distance (Mann and Yarowsky, 2001; Haghighi et al., 2008). Irvine and Callison-Burch (2016) showed that these metrics may be used with languages with different scripts: they transliterate all words to the Latin script before calculating normalized edit distance. BLI as a Classification Task Irvine and Callison-Burch (2016) demonstrate that BLI can be observed as a classification problem. They train a linear classifier to combine similarity scores from different signals (e.g., temporal word variation, normalized edit distance, word burstiness) using a set of training translation pairs. The approach outperforms an unsupervised"
E17-1102,W95-0115,0,0.341095,"and SF denotes a similarity function operating in the space (cosine similarity is typically used). A target language word v with the highest similarity score arg maxv SF (w, ~ ~v ) is then taken as the correct translation of a source language word w. In this work, we detect two major gaps in current representation learning for BLI. First, the standard embedding-based approach to BLI learns representations solely on the basis of word-level information. While early BLI works already established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the dat"
E17-1102,C10-2174,0,0.0380102,"Missing"
E17-1102,P95-1050,0,0.367971,"e orthographic similarity. (C3) We finally show that it is possible to effectively combine word- and character-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of"
E17-1102,P15-1165,0,0.0877573,"Missing"
E17-1102,Q13-1001,0,0.0321998,"Missing"
E17-1102,D12-1003,0,0.0193191,"ownstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as dense real-valued vectors. The shared space enables words from both languages to be represented in a uniform languageindependent manner such that similar words (regardless of the actual"
E17-1102,N16-1072,0,0.0271672,"iting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui an"
E17-1102,P16-1157,0,0.179112,"l information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document al"
E17-1102,P11-2052,0,0.058083,"Missing"
E17-1102,P16-1024,1,0.860214,"Missing"
E17-1102,N13-1011,1,0.861539,"Missing"
E17-1102,D13-1168,1,0.894328,"Missing"
E17-1102,J03-1002,0,0.00622353,"-of-the-art results for BLI, and the best results are obtained by exploiting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are ob"
E17-1102,P11-1133,0,0.0658225,"Missing"
E17-1102,P11-2084,1,0.889928,"Missing"
E17-1102,N01-1026,0,0.0325692,"nduced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ"
E17-1102,N16-1156,0,0.042383,"s in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from paralle"
E17-1102,P09-1007,0,0.0326295,"nguage processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used"
E17-1102,D13-1141,0,0.305567,"for BLI, and the best results are obtained by exploiting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lin"
E17-1102,P98-1069,0,\N,Missing
E17-1102,C98-1066,0,\N,Missing
E17-1102,P04-1067,0,\N,Missing
E17-1102,J17-2001,0,\N,Missing
E17-2065,W13-3520,0,0.167075,"Missing"
E17-2065,Q16-1031,0,0.0301167,"ch targets verb similarity in specific: SimVerb3500 (Gerz et al., 2016) contains similarity scores for 3,500 verb pairs. The results of the CL5 Conclusion and Future Work We have presented a new cross-lingual word embedding model which injects syntactic information into a cross-lingual word vector space, resulting in improved modeling of functional similarity, as evidenced by improvements on word similarity and bilingual lexicon induction tasks for several language pairs. More sophisticated approaches involving the use of more accurate dependency parsers applicable across different languages (Ammar et al., 2016), selection and filtering of reliable dictionary entries (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013b; Vuli´c and Korhonen, 2016b), and more sophisticated approaches to constructing hybrid cross-lingual dependency trees (Fig. 1) may lead to further advances in future work. Other crosslingual semantic tasks such as lexical entailment (Mehdad et al., 2011; Vyas and Carpuat, 2016) or lexical substitution (Mihalcea et al., 2010) may also benefit from syntactically informed cross-lingual representations. We also plan to test the portability of the proposed framework, relying on the abstractiv"
E17-2065,P14-2131,0,0.255734,"2016). Another line of work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinctions compared to vector spaces based on standard bag-ofwords (BOW) contexts. Dependency-based vector spaces steer the induced WEs towards functional similarity (e.g., tiger:cat) rather than topical similarity/relatedness (e.g., tiger:jungle), They support a variety of similarity tasks in monolingual settings, typically outperforming BOW contexts for English (Bansal et al., 2014; Hill et al., 2015; Melamud et al., 2016). However, despite the steadily growing landscape of CL WE models, each requiring a different form of cross-lingual supervision to induce a SCLVS, syntactic information is still typically discarded in the SCLVS learning process. To bridge this gap, in this work we develop a new cross-lingual WE model, termed CL-D EP E MB, which injects syntactic information into a SCLVS. The model is supported by the recent initiatives on language-agnostic annotations for universal lan1 For a comprehensive overview of cross-lingual word embedding models, we refer the r"
E17-2065,P14-1023,0,0.0206872,"ingual lexicon induction, two fundamental semantic tasks emphasising semantic similarity, suggest the usefulness of the proposed syntactically informed crosslingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual “offline mapping” baselines trained using the same setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastog"
E17-2065,C12-1089,0,0.0348455,"ne mapping” baselines trained using the same setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), en"
E17-2065,C10-1011,0,0.169062,"Missing"
E17-2065,D14-1082,0,0.0174273,"ion, two fundamental semantic tasks emphasising semantic similarity, suggest the usefulness of the proposed syntactically informed crosslingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual “offline mapping” baselines trained using the same setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay"
E17-2065,D15-1131,0,0.0349816,"sion. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lea"
E17-2065,D16-1136,0,0.146291,"recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lean languages (Søgaard"
E17-2065,E14-1049,0,0.0418697,"al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lean languages (Søgaard et al., 2015; Guo et al., 2016). Another line of work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 20"
E17-2065,D16-1235,1,0.908359,"Missing"
E17-2065,P81-1022,0,0.134486,"Missing"
E17-2065,P15-1027,0,0.024907,"gual word similarity (WS) and bilingual lexicon induction (BLI), which evaluate the monolingual and cross-lingual quality of the induced SCLVS. We observe consistent improvements over baseline CL WE models which require the same level of bilingual supervision (i.e., a word translation dictionary). For this supervision setting, we show a clear benefit of joint online training compared to standard offline models which construct two separate monolingual BOW-based or DEPS-based WE spaces, and then map them into a SCLVS using dictionary entries as done in (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b, inter alia) 2 Methodology Representation Model In all experiments, we opt for a standard and robust choice in vector space modeling: skip-gram with negative sampling (SGNS) (Mikolov et al., 2013b; Levy et al., 2015). We use word2vecf, a reimplementation of word2vec which is capable of learning from arbitrary (word, context) pairs2 , thus clearly emphasising the role of context in WE learning. (Universal) Dependency-Based Contexts A standard procedure to extract dependency-based contexts (DEPS) (Pad´o and Lapata, 2007; Utt and Pad´o, 2014) from monolingual data is"
E17-2065,P14-2050,0,0.706482,"ole of context in WE learning. (Universal) Dependency-Based Contexts A standard procedure to extract dependency-based contexts (DEPS) (Pad´o and Lapata, 2007; Utt and Pad´o, 2014) from monolingual data is as follows. Given a parsed training corpus, for each target w with modifiers m1 , . . . , mk and a head h, w is paired with context elements m1 r1 , . . . , mk rk , h rh−1 , where r is the type of the dependency relation between the head and the modifier (e.g., amod), and r−1 denotes an inverse relation.3 When extracting DEPS, we adopt the post-parsing prepositional arc collapsing procedure (Levy and Goldberg, 2014a) (see Fig. 1a-1b). Cross-Lingual DEPS: CL-D EP E MB First, a UD-parsed monolingual training corpus is obtained in both languages L1 and L2 . The use of the interlingual UD scheme enables linking dependency trees in both languages (see the structural similarity of the two sentences in English (EN) and Italian (IT), Fig. 1a-1b). For instance, the link between EN words Australian and scientist as well as IT words australiano and scienzato is typed amod in both trees. This link generates the following monolingual EN DEPS: (scientist, Australian amod), (Australian, scientist amod−1 ) (similar for"
E17-2065,W14-1618,0,0.560621,"ole of context in WE learning. (Universal) Dependency-Based Contexts A standard procedure to extract dependency-based contexts (DEPS) (Pad´o and Lapata, 2007; Utt and Pad´o, 2014) from monolingual data is as follows. Given a parsed training corpus, for each target w with modifiers m1 , . . . , mk and a head h, w is paired with context elements m1 r1 , . . . , mk rk , h rh−1 , where r is the type of the dependency relation between the head and the modifier (e.g., amod), and r−1 denotes an inverse relation.3 When extracting DEPS, we adopt the post-parsing prepositional arc collapsing procedure (Levy and Goldberg, 2014a) (see Fig. 1a-1b). Cross-Lingual DEPS: CL-D EP E MB First, a UD-parsed monolingual training corpus is obtained in both languages L1 and L2 . The use of the interlingual UD scheme enables linking dependency trees in both languages (see the structural similarity of the two sentences in English (EN) and Italian (IT), Fig. 1a-1b). For instance, the link between EN words Australian and scientist as well as IT words australiano and scienzato is typed amod in both trees. This link generates the following monolingual EN DEPS: (scientist, Australian amod), (Australian, scientist amod−1 ) (similar for"
E17-2065,Q15-1016,0,0.0392266,"upervision (i.e., a word translation dictionary). For this supervision setting, we show a clear benefit of joint online training compared to standard offline models which construct two separate monolingual BOW-based or DEPS-based WE spaces, and then map them into a SCLVS using dictionary entries as done in (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b, inter alia) 2 Methodology Representation Model In all experiments, we opt for a standard and robust choice in vector space modeling: skip-gram with negative sampling (SGNS) (Mikolov et al., 2013b; Levy et al., 2015). We use word2vecf, a reimplementation of word2vec which is capable of learning from arbitrary (word, context) pairs2 , thus clearly emphasising the role of context in WE learning. (Universal) Dependency-Based Contexts A standard procedure to extract dependency-based contexts (DEPS) (Pad´o and Lapata, 2007; Utt and Pad´o, 2014) from monolingual data is as follows. Given a parsed training corpus, for each target w with modifiers m1 , . . . , mk and a head h, w is paired with context elements m1 r1 , . . . , mk rk , h rh−1 , where r is the type of the dependency relation between the head and the"
E17-2065,P98-2127,0,0.150716,"(typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lean languages (Søgaard et al., 2015; Guo et al., 2016). Another line of work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinctions compared to vector spaces based on standard bag-ofwords (BOW) contexts. Dependency-based vector spaces steer the induced WEs towards functional similarity (e.g., tiger:cat) rather than topical similarity/relatedness (e.g., tiger:jungle), They support a variety of similarity tasks in monolingual settings, typically outperforming BOW contexts for English (Bansal et al., 2014; Hill et al., 2015; Melamud et al., 2016). However, despite the steadily growing landscape of CL WE models, each requiring a diffe"
E17-2065,W15-1521,0,0.0268997,"ensive test data for these pairs (Leviant and Reichart, 2015; Vuli´c and Korhonen, 2016a). Training Setup and Parameters For all languages, we use the Polyglot Wikipedia data (AlRfou et al., 2013).5 as monolingual training data. All corpora were UPOS-tagged and UD-parsed using the procedure of Vuli´c and Korhonen (2016a): UD treebanks v1.4, TurboTagger for tagging (Martins et al., 2013), Mate Parser v3.61 with suggested settings (Bohnet, 2010).6 The SGNS preprocessing scheme is standard (Levy and Goldberg, 2014a): 410 4 A similar idea of extended joint CL training was discussed previously by (Luong et al., 2015; Coulmance et al., 2015). In this work, we show that expensive parallel data and word alignment links are not required to produce a SCLVS. Further, instead of using BOW contexts, we demonstrate how to use DEPS contexts for joint training in the CL settings. 5 https://sites.google.com/site/rmyeid/projects/polyglot 6 LAS scores on the TEST portion of each UD treebank are: 0.852 (EN), 0.884 (IT), 0.802 (DE). all tokens were lowercased, and words and contexts that appeared less than 100 times were filtered out.7 We report results with d = 300-dimensional WEs, as similar trends are observed with o"
E17-2065,P13-2109,0,0.0990553,"Missing"
E17-2065,P11-1134,0,0.0255082,", as evidenced by improvements on word similarity and bilingual lexicon induction tasks for several language pairs. More sophisticated approaches involving the use of more accurate dependency parsers applicable across different languages (Ammar et al., 2016), selection and filtering of reliable dictionary entries (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013b; Vuli´c and Korhonen, 2016b), and more sophisticated approaches to constructing hybrid cross-lingual dependency trees (Fig. 1) may lead to further advances in future work. Other crosslingual semantic tasks such as lexical entailment (Mehdad et al., 2011; Vyas and Carpuat, 2016) or lexical substitution (Mihalcea et al., 2010) may also benefit from syntactically informed cross-lingual representations. We also plan to test the portability of the proposed framework, relying on the abstractive assumption of language-universal dependency structures, to more language pairs, including the ones outside the Indo-European language family. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The author is grateful to the anonymous reviewers for their helpful comments and suggestions."
E17-2065,N16-1118,0,0.464158,"ated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinctions compared to vector spaces based on standard bag-ofwords (BOW) contexts. Dependency-based vector spaces steer the induced WEs towards functional similarity (e.g., tiger:cat) rather than topical similarity/relatedness (e.g., tiger:jungle), They support a variety of similarity tasks in monolingual settings, typically outperforming BOW contexts for English (Bansal et al., 2014; Hill et al., 2015; Melamud et al., 2016). However, despite the steadily growing landscape of CL WE models, each requiring a different form of cross-lingual supervision to induce a SCLVS, syntactic information is still typically discarded in the SCLVS learning process. To bridge this gap, in this work we develop a new cross-lingual WE model, termed CL-D EP E MB, which injects syntactic information into a SCLVS. The model is supported by the recent initiatives on language-agnostic annotations for universal lan1 For a comprehensive overview of cross-lingual word embedding models, we refer the reader to two recent survey papers (Upadhya"
E17-2065,S10-1002,0,0.0806446,"n induction tasks for several language pairs. More sophisticated approaches involving the use of more accurate dependency parsers applicable across different languages (Ammar et al., 2016), selection and filtering of reliable dictionary entries (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013b; Vuli´c and Korhonen, 2016b), and more sophisticated approaches to constructing hybrid cross-lingual dependency trees (Fig. 1) may lead to further advances in future work. Other crosslingual semantic tasks such as lexical entailment (Mehdad et al., 2011; Vyas and Carpuat, 2016) or lexical substitution (Mihalcea et al., 2010) may also benefit from syntactically informed cross-lingual representations. We also plan to test the portability of the proposed framework, relying on the abstractive assumption of language-universal dependency structures, to more language pairs, including the ones outside the Indo-European language family. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The author is grateful to the anonymous reviewers for their helpful comments and suggestions. 10 We also experimented with other language pairs represented in V ULIC"
E17-2065,P14-1006,0,0.0227389,"e setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross"
E17-2065,J15-4004,0,0.0592341,"f work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinctions compared to vector spaces based on standard bag-ofwords (BOW) contexts. Dependency-based vector spaces steer the induced WEs towards functional similarity (e.g., tiger:cat) rather than topical similarity/relatedness (e.g., tiger:jungle), They support a variety of similarity tasks in monolingual settings, typically outperforming BOW contexts for English (Bansal et al., 2014; Hill et al., 2015; Melamud et al., 2016). However, despite the steadily growing landscape of CL WE models, each requiring a different form of cross-lingual supervision to induce a SCLVS, syntactic information is still typically discarded in the SCLVS learning process. To bridge this gap, in this work we develop a new cross-lingual WE model, termed CL-D EP E MB, which injects syntactic information into a SCLVS. The model is supported by the recent initiatives on language-agnostic annotations for universal lan1 For a comprehensive overview of cross-lingual word embedding models, we refer the reader to two recent"
E17-2065,C10-2174,0,0.055991,"Missing"
E17-2065,N13-1011,1,0.886962,"Missing"
E17-2065,D13-1168,1,0.904025,"Missing"
E17-2065,J07-2002,0,0.360685,"Missing"
E17-2065,N10-1135,0,0.0804039,"Missing"
E17-2065,N15-1058,0,0.0320682,"Missing"
E17-2065,P93-1034,0,0.73195,"Missing"
E17-2065,N16-1060,0,0.0566186,"LI metric: Top 1 scores. The same trends are visible with Top 5 and Top 10 scores. All test word pairs were removed from D for training. The results are summarised in Tab. 2, indicating significant improvements with CL-D EP E MB (McNemar’s test, p < 0.05). The gap between the online CL-D EP E MB model and the offline baselines is now even more prominent,10 and there is a huge difference in performance between OFF - DEPS and C L -D EP E MB, two models using exactly the same information for training. EN Experiments on Verbs Following prior work, e.g., (Bansal et al., 2014; Melamud et al., 2016; Schwartz et al., 2016), we further show that WE models which capture functional similarity are especially important for modelling particular “more grammatical” word classes such as verbs and adjectives. Therefore, in Tab. 1 and Tab. 2 we also report results on verb similarity and translation. The results indicate that injecting syntax into crosslingual word vector spaces leads to clear improvements on modelling verbs in both evaluation tasks. We further verify the intuition by running experiments on another word similarity evaluation set, which targets verb similarity in specific: SimVerb3500 (Gerz et al., 2016) co"
E17-2065,P15-1165,0,0.0246679,"Missing"
E17-2065,N16-1072,0,0.0191044,"2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lean languages (Søgaard et al., 2015; Guo et al., 2016). Another line of work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinctions compared to vector spaces based on standard bag-ofwords (BOW) contexts. Dependency-based vector spaces steer the induced WEs towards functional similarity (e.g., tiger:cat) rather than topical similarity/relatedness (e.g., tiger:jungle), They support a v"
E17-2065,D15-1243,0,0.0248193,"he results for the three languages are displayed in Tab. 1. These results suggest that CL-D EP E MB is the best performing and most robust model in our comparison across all three languages, providing the first insight that the online training with the extended set of DEPS pairs is indeed beneficial for modeling true (functional) similarity. We also carry out tests in English using another word similarity metric: QVEC,9 which measures how well the induced word vectors correlate with a matrix of features from manually crafted lexical resources and is better aligned with downstream performance (Tsvetkov et al., 2015). The results are again in favour of CL-D EP E MB with a QVEC score of 0.540 (BNC+GT) and 0.543 (dict.cc), compared to those of OFF - BOW 2 (0.496), OFF - POSIT 2 (0.510), and OFF - DEPS (0.528). Bilingual Lexicon Induction BLI experiments were conducted on several standard test sets: IT411 (decreasing) learning rate 0.025, subsampling rate 1e − 4. 8 9 http://technion.ac.il/∼ira.leviant/MultilingualVSMdata.html https://github.com/ytsvetko/qvec OFF - DEPS BEST- BASELINE 0.259 0.271 CL-D EP E MB (+ IT ) CL-D EP E MB (+ DE ) 0.285 0.310 D EP E MB on SimVerb-3500 with dict.cc are provided in Tab."
E17-2065,P10-1040,0,0.0984291,"ral language pairs on word similarity and bilingual lexicon induction, two fundamental semantic tasks emphasising semantic similarity, suggest the usefulness of the proposed syntactically informed crosslingual word vector spaces. Improvements are observed in both tasks over standard cross-lingual “offline mapping” baselines trained using the same setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically wor"
E17-2065,P16-1157,0,0.141363,"ng, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai and Roth, 2016), and cross-lingual knowledge transfer for resource-lean languages (Søgaard et al., 2015; Guo et al., 2016). Another line of work has demonstrated that syntactically informed dependency-based (DEPS) word vector spaces in monolingual settings (Lin, 1998; Pad´o and Lapata, 2007; Utt and Pad´o, 2014) are able to capture finer-grained distinct"
E17-2065,Q14-1020,0,0.200122,"Missing"
E17-2065,P16-2084,1,0.898056,"Missing"
E17-2065,P16-1024,1,0.793471,"Missing"
E17-2065,N16-1142,0,0.0925157,"rovements on word similarity and bilingual lexicon induction tasks for several language pairs. More sophisticated approaches involving the use of more accurate dependency parsers applicable across different languages (Ammar et al., 2016), selection and filtering of reliable dictionary entries (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013b; Vuli´c and Korhonen, 2016b), and more sophisticated approaches to constructing hybrid cross-lingual dependency trees (Fig. 1) may lead to further advances in future work. Other crosslingual semantic tasks such as lexical entailment (Mehdad et al., 2011; Vyas and Carpuat, 2016) or lexical substitution (Mihalcea et al., 2010) may also benefit from syntactically informed cross-lingual representations. We also plan to test the portability of the proposed framework, relying on the abstractive assumption of language-universal dependency structures, to more language pairs, including the ones outside the Indo-European language family. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The author is grateful to the anonymous reviewers for their helpful comments and suggestions. 10 We also experimented"
E17-2065,D13-1141,0,0.0303201,"ined using the same setup and an equal level of bilingual supervision. 1 Introduction In recent past, NLP as a field has seen tremendous utility of distributed word representations (or word embeddings, termed WEs henceforth) as features in a variety of downstream tasks (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014; Chen and Manning, 2014). The quality of these representations may be further improved by leveraging cross-lingual (CL) distributional information, as evidenced by the recent body of work focused on learning cross-lingual word embeddings (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2014; Gouws et al., 2015; Coulmance et al., 2015; Duong et al., 2016, inter alia).1 The inclusion of cross-lingual information results in a shared cross-lingual word vector space (SCLVS), which leads to improvements on monolingual tasks (typically word similarity) (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016), and also supports cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), entity linking (Tsai"
heylen-etal-2014-termwise,vanallemeersch-2010-belgisch,0,\N,Missing
heylen-etal-2014-termwise,ha-etal-2008-mutual,0,\N,Missing
heylen-etal-2014-termwise,C02-1166,0,\N,Missing
heylen-etal-2014-termwise,2006.jeptalnrecital-invite.2,0,\N,Missing
heylen-etal-2014-termwise,C94-1084,0,\N,Missing
heylen-etal-2014-termwise,P98-1074,0,\N,Missing
heylen-etal-2014-termwise,C98-1071,0,\N,Missing
heylen-etal-2014-termwise,Y09-2038,0,\N,Missing
heylen-etal-2014-termwise,C12-1166,1,\N,Missing
J17-4004,N09-1003,0,0.567913,"alculated the average of all ratings from the accepted raters (≥ 10) for each word pair. The score was finally scaled linearly from the 0–6 to the 0–10 interval as also done by Hill, Reichart, and Korhonen (2015). 799 Computational Linguistics Volume 43, Number 4 Table 4 A comparison of HyperLex IAA with several prominent crowdsourced semantic similarity/ relatedness evaluation benchmarks that also provide scores for word pairs. Numbers in parentheses refer to the total number of word pairs in each evaluation set. Benchmark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A c"
J17-4004,W13-3520,0,0.0197215,"Missing"
J17-4004,P15-1104,0,0.0243363,"ocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate h"
J17-4004,E12-1004,0,0.210541,"derlines the lexical entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by"
J17-4004,P14-1023,0,0.108108,"Missing"
J17-4004,J10-4006,0,0.0102161,"-up regarding training data, their parameter settings, and other modeling choices. DEMs and SLQS. Directional entailment measures DEM1 –DEM4 and both SLQS variants (i.e., SLQS–B ASIC and SLQS–S IM) are based on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work"
J17-4004,W11-2501,0,0.0591396,"owing mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and extensional definitions of a concept (van Benthem and ter Meulen 1996; Baronett 2012). ~ ), difference (Y ~ −X ~ ), or element-wise multiplication ~ ⊕"
J17-4004,W16-2502,0,0.0167661,"ave been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because they typically suffer from"
J17-4004,S13-1002,0,0.0247821,"entailment (LE) relation. Simply put, an instantiation of a member concept such as a cat entails the existence of an animal. This lexical entailment in turns governs many cases of phrasal and sentential entailment: If we know that a cat is in the garden, we can quickly and intuitively conclude that an animal is in the garden, too.1 Because of this fundamental connection to language understanding, the automatic detection and modeling of lexical entailment has been an area of much focus in natural language processing (Bos and Markert 2005; Dagan, Glickman, and Magnini 2006; Baroni et al. 2012; Beltagy et al. 2013, inter alia). The ability to effectively detect and model both lexical and phrasal entailment in a human-like way may be critical for numerous related applications, such as question answering, information retrieval, information extraction, and text summarization and generation (Androutsopoulos and Malakasiotis 2010). For instance, in order to answer a question such as “Which mammal has a strong bite?”, a question-answering system has to know that a jaguar or a grizzly bear are types of mammals, whereas a crocodile or a piranha are not. Although inspired to some extent by theories of human sem"
J17-4004,I13-1095,0,0.26422,"exical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relat"
J17-4004,D15-1075,0,0.076472,"Missing"
J17-4004,P15-2001,0,0.111058,"Missing"
J17-4004,W09-0215,0,0.0685026,"ive than their hyponyms (Murphy 2003), which is also reflected in less specific contexts for hypernyms. Unsupervised (distributional) models of lexical entailment were instigated by the early work of Hearst (1992) on prototypicality patterns (e.g., the pattern “X such as Y” indicates that Y is a hyponym of X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences"
J17-4004,D10-1107,0,0.0153209,"this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, met"
J17-4004,N15-1184,0,0.154452,"Missing"
J17-4004,P15-2076,0,0.0297851,"ons are interlinked. We test the following benchmarking semantic similarity models: (1) Unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) (Mikolov et al. 2013b) with various contexts (BOW = bag of words; DEPS = dependency contexts) as described by Levy and Goldberg (2014); and (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases. We evaluate models that currently hold the peak scores in word similarity tasks: sparse binary vectors built from linguistic resources (N ON -D ISTRIBUTIONAL [Faruqui and Dyer 2015]), vectors fine-tuned to a paraphrase database (PARAGRAM [Wieting et al. 2015]), and further refined using linguistic constraints (PARAGRAM +CF [Mrkši´c et al. 2016]). Because these models are not the main focus of this work, the reader is referred to the relevant literature for detailed descriptions. 6.8 Gaussian Embeddings An alternative approach to learning word embeddings was proposed by Vilnis and McCallum (2015). They represent words as Gaussian densities rather than points in the embedding space. Each concept X is represented as a multivariate K-dimensional Gaussian parameterized as N"
J17-4004,W16-2506,0,0.23782,"nnotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex provides a wide coverage o"
J17-4004,P14-1113,0,0.0669191,"X). The current unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguish"
J17-4004,P05-1014,0,0.141677,"re exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the"
J17-4004,D16-1235,1,0.310735,"Missing"
J17-4004,gheorghita-pierrel-2012-towards,0,0.064616,"Missing"
J17-4004,W16-2507,0,0.00428272,"hat ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investigate attaching graded LE scores to large hierarchical image databases such as ImageNet (Deng et al. 2009; Russakovsky et al. 2015). 9. Conclusions Although the ultimate test of semantic models is their usefulness in downstream applications, the research community is still in need of wide-coverage comprehensive gold standard resources for intrinsic evaluation (Camacho-Collados, Pilehvar, and Navigli 2015; Schnabel et al. 2015; Tsvetkov et al. 2015; Gladkova and Drozd 2016; Hashimoto, Alvarez-Melis, and Jaakkola 2016, inter alia). Such resources can measure the general quality of the representations learned by semantic models, prior to their integration in end-to-end systems. We have presented HyperLex, a large wide-coverage gold standard resource for the evaluation of semantic representations targeting the lexical relation of graded lexical entailment (LE), also known as hypernymy-hyponymy or TYPE - OF relation, a relation which is fundamental in construction and understanding of concept hierarchies, that is, semantic taxonomies. Given that the problem of conc"
J17-4004,W11-2508,0,0.00981994,"d on the cleaned, tokenized, and lowercased Polyglot Wikipedia (Al-Rfou, Perozzi, and Skiena 2013). We have used two set-ups for the induction of word representations, the only difference being that in Set-up 1 context/feature vectors are extracted from the Polyglot Wiki directly based on bigram co-occurrence counts, whereas in Set-up 2, these vectors are extracted from the T YPE DM tensor (Baroni and Lenci 2010) as in the original work of Lenci and Benotto (2012).27 Both set-ups use the positive LMI weighting calculated on syntactic co-occurrence links between each word and its context word (Gulordava and Baroni 2011): LMI(w1 , w2 ) = 1 ,w2 )∗Total C(w1 , w2 ) ∗ log2 C(w C(w1 )C(w2 ) , where C(w) is the unigram count in the Polyglot Wiki for the word w, C(w1 , w2 ) is the dependency based co-occurrence count of the two tokens w1 and w2 , namely (w1 , (dep_rel, w2 )), and Total is the number of all such tuples. The Polyglot Wiki was parsed with Universal Dependencies (Nivre et al. 2015) as in the work of Vuli´c and Korhonen (2016).28 The context vocabulary (i.e., words w2 ) is restricted to the 10K most frequent words in the Polyglot Wiki. The same two set-ups were used for the SLQS model. We also use frequ"
J17-4004,Q16-1020,0,0.0114068,"Missing"
J17-4004,C92-2082,0,0.478737,"ans perceive the concepts of typicality and graded membership within the graded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and mo"
J17-4004,S10-1006,0,0.0353884,"ystem has to predict the relation directionality, that is, which word is the hypernym and which word is the hyponym. More formally, the following mapping is defined by the directionality function fdir : fdir : (X, Y) → {−1, 1} (1) fdir simply maps to 1 when Y is the hypernym, and to −1 otherwise. (ii) Entailment Detection. The system has to predict whether there exists a lexical entailment relation between two words, or the words stand in some other relation (synonymy, meronymy–holonymy, causality, no relation, etc.). A more detailed overview of lexical relations is available in related work (Hendrickx et al. 2010; Jurgens et al. 2012; Vylomova et al. 2016). The following mapping is defined by the detection function fdet : fdet : (X, Y) → {0, 1} (2) fdet simply maps to 1 when (X, Y) stand in a lexical entailment relation, irrespective to the actual directionality of the relation, and to 0 otherwise. (iii) Entailment Detection and Directionality. This recently proposed evaluation protocol (Weeds et al. 2014; Kiela et al. 2015) combines (i) and (ii). The system first has to detect whether there exists a lexical entailment relation between two words (X, Y), and then, if the relation holds, it has to predi"
J17-4004,P13-2078,0,0.08496,"s (X, Y), and then, if the relation holds, it has to predict its directionality (i.e., the correct hypernym). The following mapping is defined by the joint detection and directionality function fdet+dir : fdet+dir : (X, Y) → {−1, 0, 1} (3) fdet+dir maps to 1 when (X, Y) stand in a lexical entailment relation and Y is the hypernym, to −1 if X is the hypernym, and to 0 if X and Y stand in some other lexical relation or no relation. Standard Modeling Approaches. These decisions are typically based on the distributional inclusion hypothesis (Geffet and Dagan 2005) or a lexical generality measure (Herbelot and Ganesalingam 2013). The intuition supporting the former is that the class (i.e., extension) denoted by a hyponym is included in the class denoted by the hypernym, and therefore hyponyms are expected to occur in a subset of the contexts of their hypernyms. The intuition supporting the latter hints that typical characteristics constituting the intension (i.e., concept) expressed by a hypernym (e.g., move or eat for the concept animal) are 787 Computational Linguistics Volume 43, Number 4 semantically more general than the characteristics forming the intension7 of its hyponyms (e.g., bark or has tail for the conce"
J17-4004,J15-4004,1,0.787959,"Missing"
J17-4004,N15-1070,0,0.0250832,"Missing"
J17-4004,S12-1047,0,0.0528398,"Missing"
J17-4004,D14-1005,1,0.757367,"Missing"
J17-4004,D15-1242,1,0.0670611,"k as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shw"
J17-4004,P14-2135,1,0.798609,"ndency, using sets of images associated with each concept word as returned by Google’s image search. The intuition is that the set of images returned for the broader concept animal will consist of pictures of different kinds of animals, that is, exhibiting greater visual variability and lesser concept specificity; on the other hand, the set of images for bird will consist of pictures of different birds, and the set for owl will mostly consist only of images of owls. The generality of a set of n images for each concept X is then computed. The first model relies on the image dispersion measure (Kiela et al. 2014). It is the average −→ −→ pairwise cosine distance between all image representations23 {iX,1 , . . . , iX,n } for X: id(X) = 2 n(n − 1) X j &lt; k ≤n − → −→ 1 − cos(iX,j , iX,k ) (11) Another similar measure instead of calculating the pairwise distance calculates the −→ −→ distance to the centroid − µ→ X of {iX,1 , . . . , iX,n }: 1 cent(X) = n X 1≤j≤n − → → 1 − cos(iX,j , − µX ) (12) Final Model. The following formula summarizes the visual model for ungraded LE directionality and detection that we also test in graded evaluations: ( sθ (X, Y) = 1− 0 f (X)+α f (Y) ~ ≥θ ~ Y) if cos(X, otherwise (13"
J17-4004,P15-2020,1,0.704519,"Missing"
J17-4004,N15-1016,0,0.0344693,"Missing"
J17-4004,S12-1012,0,0.0892939,"in human judgments. For instance, graded LE scores indicate that humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim"
J17-4004,W14-1610,0,0.0265556,"of substitutable LE (see Section 2). Baroni et al. (2012). The N1  N2 evaluation set contains 2,770 nominal concept pairs, with 1,385 pairs labeled as positive examples (i.e., 1 or entails) (Baroni et al. 2012). The remaining 1,385 pairs labeled as negatives were created by inverting the positive pairs and randomly matching concepts from the positive pairs. The pairs and annotations were extracted automatically from WordNet and then validated manually by the authors (e.g., the abstract concepts with a large number of hyponyms such as entity or object were removed from the pool of concepts). Levy et al. (2014). A similar data set for the standard LE evaluation may be extracted from manually annotated entailment graphs of subject–verb–object tuples (i.e., propositions) (Levy, Dagan, and Goldberger 2014): Noun LEs were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the proposition-level entailment to the word level. This data set was built for the medical domain and adopts the looser definition of substitutable LE. Custom Evaluation Sets. A plethora of relevant work on ungraded LE do not rely on established evaluation resources, but simply extrac"
J17-4004,P14-2050,0,0.56899,"overs: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we o"
J17-4004,N15-1098,0,0.247759,"s (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting differences in the models, our findings indicate clearly that none of the currently available models or approaches accurately model the relation of graded LE reflected in human subjects. This study therefore calls for new paradigms and solutions capable of capturing the gradual nature of semantic relations such as hypernymy"
J17-4004,P98-2127,0,0.0625748,"tes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment similarity sim(X, Y) between X and Y, measured by cosine (Weeds, Weir, and McCarthy 2004), or the Lin measure (Lin 1998) as in the balAPinc measure of Kotlerman et al. (2010): DEM2 (X, Y) = DEM1 (X, Y) · sim(X, Y) (6) ClarkeDE (DEM3 ). A close variation of DEM1 was proposed by Clarke (2009): P DEM3 (X, Y) = ft∈FeatX ∩FeatY P min(wX ( ft), wY ( ft)) ft∈FeatX wX ( ft) (7) InvCL (DEM4 ). A variation of DEM3 was introduced by Lenci and Benotto (2012). It takes into account both the inclusion of context features of X in context features of Y and non-inclusion of features of Y in features of X.22 DEM4 (X, Y) = p DEM3 (X, Y) · (1 − DEM3 (Y, X)) (8) 6.2 Generality Measures Another related view towards the TYPE - OF rel"
J17-4004,P15-1145,0,0.0413618,"rch work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity a"
J17-4004,W13-3512,0,0.0649624,"Missing"
J17-4004,W13-0904,0,0.0158973,"HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been"
J17-4004,N16-1018,0,0.00692386,"Missing"
J17-4004,P17-1163,0,0.00734502,"Missing"
J17-4004,D14-1113,0,0.0146062,"that a promising step in that direction are neural net–inspired approaches to LE proposed recently (Vilnis and McCallum 2015; Vendrov et al. 2016), mostly because of their conceptual distinction from other distributional modeling approaches complemented with their modeling adaptability and flexibility. In addition, in order to model hierarchical semantic knowledge more accurately, in future work we may require algorithms that are better suited to fast learning from few examples (Lake et al. 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney 2010b; Neelakantan et al. 2014; Jauhar, Dyer, and Hovy 2015; Šuster, Titov, and van Noord 2016). Despite the abundance of reported experiments and analyses in this work, we have only scratched the surface in terms of the possible analyses with HyperLex and use of such models as components of broader phrase-level and sentence-level textual entailment systems, as well as in other applications, as quickly surveyed in Section 8. Beyond the preliminary conclusions from these initial analyses, we believe that the benefits of HyperLex will become evident as researchers use it to probe the relationship between architectures, algor"
J17-4004,N15-1100,0,0.118597,"Missing"
J17-4004,D07-1042,0,0.061095,"Missing"
J17-4004,P06-1015,0,0.0188982,"raded LE relation. We hope that this will in turn incentivize research into language technology that both reflects human semantic memory more faithfully and interprets and models linguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations suc"
J17-4004,D16-1244,0,0.00552891,"(graded) semantic similarity is appropriate here: It was recently demonstrated that vector space models specializing for similarity and scoring high on SimLex-999 and SimVerb-3500 are able to boost performance of statistical systems in language understanding tasks such as dialogue state tracking (Mrkši´c et al. 2016, 2017; Vuli´c et al. 2017). We assume that the specification of what the degree of LE means for each individual pair may also boost performance of statistical end-to-end systems in another language understanding task in future work: natural language inference (Bowman et al. 2015; Parikh et al. 2016; Agi´c and Schluter 2017). Owing to their adaptability and versatility, representation architectures inspired by neural networks (e.g., Mrkši´c et al. 2016; Vendrov et al. 2016) seem to be a promising avenue for future modeling work on graded lexical entailment in both unsupervised and supervised settings, despite their low performance on the graded LE task at present. 8. Application Areas: A Quick Overview The proposed data set should have an immediate impact in the cognitive science research, providing means to analyze the effects of typicality and gradience in concept representations (Hamp"
J17-4004,N04-3012,0,0.0574719,"Missing"
J17-4004,W14-1608,0,0.0448142,"Missing"
J17-4004,D10-1114,0,0.031074,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N10-1013,0,0.0779609,"ark IAA-1 IAA-2 W ORD S IM (353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over differe"
J17-4004,N13-1008,0,0.0139297,"but simply extract ad hoc LE evaluation data using distant supervision from readily available semantic resources and knowledge bases such as WordNet (Miller 1995), DBPedia (Auer et al. 2007), Freebase (Tanon et al. 2016), Yago (Suchanek, Kasneci, and Weikum 2007), or dictionaries (Gheorghita and Pierrel 2012). Although plenty of the custom evaluation sets are available online, there is a clear tendency to construct a new custom data set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detectio"
J17-4004,E14-1054,0,0.125239,"unsupervised models typically replace the symmetric cosine similarity measure that works well for semantic similarity computations (Bullinaria and Levy 2007; Mikolov et al. 2013a) with an asymmetric similarity measure optimized for entailment (Weeds, Weir, and McCarthy 2004; Clarke 2009; Kotlerman et al. 2010; Lenci and Benotto 2012; Herbelot and Ganesalingam 2013; Santus et al. 2014). Supervised models, on the other hand, learn the asymmetric operator from a training set, differing mostly in the feature selection to represent each candidate pair of words (Baroni et al. 2012; Fu et al. 2014; Rimell 2014; Roller, Erk, and Boleda 2014; Weeds et al. 2014; Fu et al. 2015; Roller and Erk 2016; Shwartz, Goldberg, and Dagan 2016).8 An overview of the supervised techniques also discussing their main shortcomings is provided by Levy et al. (2015); a thorough discussion of differences between unsupervised and supervised entailment models is provided by Turney and Mohammad (2015). Why is HyperLex Different? In short, regardless of the chosen methodology, the evaluation protocols (directionality or detection) may be straightforwardly translated into binary decision problems: (1) distinguishing between h"
J17-4004,D16-1234,0,0.0924128,"Missing"
J17-4004,C14-1097,0,0.050532,"Missing"
J17-4004,P15-2119,0,0.0320914,"ven distributional LE models. In current binary evaluation protocols targeting ungraded LE detection and directionality, even simple methods modeling lexical generality are able to yield very accurate predictions. However, our preliminary analysis in Section 7.2 demonstrates their fundamental limitations for graded lexical entailment. In addition to the use of HyperLex as a new evaluation set, we believe that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and"
J17-4004,C08-3008,0,0.0239186,"eve that the introduction of graded LE will have implications on how the distributional hypothesis (Harris 1954) is exploited in distributional models targeting taxonomic relations in particular (Rubinstein et al. 2015; Shwartz, Goldberg, and Dagan 2016; Roller and Erk 2016, inter alia). Further, a tight connection of LE with the broader phrase-/sentence-level task of recognizing lexical entailment (Dagan, Glickman, and Magnini 2006; Dagan et al. 2013) should lead to further implications for text generation (Biran and McKeown 2013), metaphor detection (Mohler et al. 2013), question answering (Sacaleanu et al. 2008), paraphrasing (Androutsopoulos and Malakasiotis 2010), and so forth. Representation Learning. Prior work in representation learning has mostly focused on the relations of semantic similarity and relatedness, as evidenced by the surge in interest in evaluation of word embeddings on data sets such as SimLex-999, WordSim-353, MEN (Bruni, Tran, and Baroni 2014), Rare Words (Luong, Socher, and Manning 2013), and so on. This strong focus towards similarity and relatedness means that other fundamental semantic relations such as lexical entailment have been largely overlooked in the 824 Vuli´c et al."
J17-4004,E14-4008,0,0.0900705,"at humans rate concepts such as to talk or to speak as more typical instances of the class to communicate than concepts such as to touch, or to pray. In Section 6 we then turn our attention to Q2: We evaluate the performance of a wide range of LE detection or measurement approaches. This review covers: (i) distributional models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test split"
J17-4004,W15-4208,0,0.0243423,"ty evaluations (Santus et al. 2014; Kiela et al. 2015), only the LE subset is used. Note that the original BLESS data are always presented with the hyponym first, so gold annotations are implicitly provided here. Second, for detection evaluations (Roller, Erk, and Boleda 2014; Santus et al. 2014; Levy et al. 2015), the pairs from the LE subset are taken as positive pairs, and all the remaining pairs are considered negative pairs. That way, the evaluation data effectively measure a model’s ability to predict the positive LE relation. Another evaluation data set based on BLESS was introduced by Santus et al. (2015). Following the standard annotation scheme, it comprises 7,429 noun pairs in total, and 1,880 LE pairs in particular, covering a wider range of relations than BLESS (i.e., the data set now includes synonymy and antonymy pairs). Adaptations of the original BLESS evaluation set were proposed recently. First, relying on its LE subset, Weeds et al. (2014) created another data set called WBLESS (Kiela et al. 2015) consisting of 1,976 concept pairs in total. Only (X, Y) pairs where Y is the hypernym are annotated as positive examples. It also contains reversed LE pairs (i.e., X is the hypernym), coh"
J17-4004,D15-1036,0,0.282274,"onverted to ungraded annotations: HyperLex may also be used in the standard format of previous LE evaluation sets (see Table 1) for detection and directionality evaluation protocols (see later in Section 7.2). Second, a typical way to evaluate word representation quality at present is by judging the similarity of representations assigned to similar words. The most popular semantic similarity evaluation sets such as SimLex-999 or SimVerb-3500 consist of word pairs with similarity ratings produced by human annotators. HyperLex is the first resource that can be used for the intrinsic evaluation (Schnabel et al. 2015; Faruqui et al. 2016) of LE-based vector space models (Vendrov et al. 2016); see later in Section 6.6. Encouraged by high inter-annotator agreement scores and evident large gaps between the human and system performance (see Section 7), we believe that HyperLex will guide the development of a new generation of representation-learning architectures that induce hypernymy/LE-specialized word representations, as opposed to the current ubiquitous word representations targeting exclusively semantic similarity and/or relatedness (see later the discussion in Sections 7.4 and 8). Finally, HyperLex prov"
J17-4004,K15-1026,0,0.0344989,"Missing"
J17-4004,P16-1226,0,0.365672,"Missing"
J17-4004,K15-1018,0,0.0116299,"ata set in every subsequent paper that uses the same evaluation protocol for ungraded LE. A standard practice (Snow, Jurafsky, and Ng 2004, 2006; Bordes et al. 2011; Riedel et al. 2013; Socher et al. 2013; Weeds et al. 2014; Shwartz, Goldberg, and Dagan 2016; Vendrov et al. 2016, inter alia) is to extract positive and negative pairs by coupling concepts that are directly related in at least one of the resources. Only pairs standing in an unambiguous hypernymy/LE relation, according to the set of indicators from Table 2, are annotated as positive examples (i.e., again 1 or entailing, Table 1) (Shwartz et al. 2015). All other pairs standing in other relations are taken as negative instances. Using related rather than random concept pairs as negative instances enables detection experiments. We adopt a similar construction principle regarding wide coverage of different lexical relations in HyperLex. This decision will support a variety of interesting analyses related to graded LE and other relations. Table 2 Indicators of LE/hypernymy relation in structured semantic resources. Resource Relation WordNet Wikidata DBPedia Yago instance hypernym, hypernym subclass of, instance of type subclass of 790 Vuli´c e"
J17-4004,E17-1007,0,0.189385,"Missing"
J17-4004,D12-1130,0,0.0201593,"Missing"
J17-4004,P14-1068,0,0.0145115,"353) (Finkelstein et al. 2002) WS-S IM (203) (Agirre et al. 2009) S IM L EX (999) (Hill, Reichart, and Korhonen 2015) 0.611 0.756 0.667 0.651 0.673 0.778 H YPER L EX (2616) 0.854 0.864 H YPER L EX : N OUNS (2163) H YPER L EX : V ERBS (453) 0.854 0.855 0.864 0.862 5. Analysis Inter-Annotator Agreement. We report two different inter-annotator agreement (IAA) measures. IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters. This is a common choice in previous data collection in distributional semantics (Padó, Padó, and Erk 2007; Reisinger and Mooney 2010a; Silberer and Lapata 2014; Hill, Reichart, and Korhonen 2015). A complementary measure would smooth individual annotator effects. For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters. It arguably serves as a better “upper bound” than IAA-1 for the performance of automatic systems. HyperLex obtains ρ = 0.854 (IAA-1) and ρ = 0.864 (IAA-2), a very good agreement compared to other prominent crowdsourced benchmarks for semantic evaluation which also used word pair scoring (see Table 4).19 We also report IAAs over different groups of pairs accordin"
J17-4004,P06-1101,0,0.0237483,"Missing"
J17-4004,Q14-1017,0,0.00703476,"image captioning can be seen as special cases of a partial order over unified visual– semantic hierarchies (Deselaers and Ferrari 2011; Vendrov et al. 2016), see also Figure 6. For instance, image captions may be seen as abstractions of images, and they can be expressed at various levels in the hierarchy. The same image may be abstracted as, for example, A boy and a girl walking their dog, People walking their dog, People walking, A boy, a girl, and a dog, Children with a dog, Children with an animal. LE might prove helpful in research on image captioning (Hodosh, Young, and Hockenmaier 2013; Socher et al. 2014; Bernardi et al. 2016) or cross-modal information retrieval (Pereira et al. 2014) based on such visual–semantic hierarchies, but it is yet to be seen whether the knowledge of gradience and prototypicality may contribute to image captioning systems. Image generality is closely linked to semantic generality, as is evident from recent work (Deselaers and Ferrari 2011; Kiela et al. 2015). The data set could also be very useful in evaluating models that ground language in the physical world (Silberer and Lapata 2012, 2014; Bruni, Tran, and Baroni 2014, inter alia). Future work might also investiga"
J17-4004,D15-1243,0,0.035761,"s lexical entailment have been largely overlooked in the 824 Vuli´c et al. HyperLex: A Large-Scale Evaluation of Graded Lexical Entailment representation learning literature. Notable exceptions building word embeddings for LE have appeared only recently (see the work of Vendrov et al. [2016] and a short overview in Section 7.4), but a comprehensive evaluation resource for intrinsic evaluation of such LE embeddings is still missing. There is a pressing need to improve, broaden, and introduce new evaluation protocols and data sets for representation learning architectures (Schnabel et al. 2015; Tsvetkov et al. 2015; Batchkarov et al. 2016; Faruqui et al. 2016; Yaghoobzadeh and Schütze 2016, inter alia).36 We believe that one immediate application of HyperLex is its use as a comprehensive, wide-coverage large evaluation set for representationlearning architectures focused on the fundamental TYPE - OF taxonomic relation. Data Mining: Extending Knowledge Bases. Ontologies and knowledge bases such as WordNet, Yago, or DBPedia are useful resources in a variety of applications such as text generation, question answering, information retrieval, or for simply providing structured knowledge to users. Because the"
J17-4004,J06-3003,0,0.0147319,"r free variance in the collected data in terms of their quantity and representative concept pairs. In addition, the distinction is often not evident for verb concepts. We leave further developments with respect to the two related phenomena of typicality and vagueness for future work, and refer the interested reader to the aforementioned literature. However, we already provide preliminary qualitative analyses in this article (see Section 5) suggesting that both phenomena are captured in graded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typic"
J17-4004,N16-1160,0,0.0325268,"Missing"
J17-4004,P16-2084,1,0.882326,"Missing"
J17-4004,P17-1006,1,0.87338,"Missing"
J17-4004,P16-1158,0,0.11102,"ded LE ratings. Relation to Relational Similarity. A strand of related research on relational similarity (Turney 2006; Jurgens et al. 2012) also assigns the score s to a pair of concepts (X, Y). Note that there exists a fundamental difference between relational similarity and graded LE. In the latter, s refers to the degree of the LE relation in the (X, Y) pair, that is, to the levels of typicality and graded membership of the instance X for the class Y, whereas the former quantifies the typicality of the pair (X, Y) for some fixed lexical relation class R (Bejar, Chaffin, and Embretson 1991; Vylomova et al. 2016), for example, to what degree the pair (snake, animal) reflects a typical LE relation or a typical synonymy relation.5 Graded LE vs. Semantic Similarity. A plethora of current evaluations in NLP and representation learning almost exclusively focus on semantic similarity and relatedness. Semantic similarity as quantified by, for example, SimLex-999 or SimVerb-3500 (Gerz et al. 2016) may be redefined as graded synonymy relation. The graded scores there, in fact, refer to the strength of the synonymy relation between any pair of concepts (X, Y). One could say that semantic similarity aims to answ"
J17-4004,C14-1212,0,0.55778,"Missing"
J17-4004,W03-1011,0,0.0185238,"s well. We closely follow the work from Lenci and Benotto (2012) in the presentation. Let FeatX denote the set of distributional features ft for a concept word X, and let wX ( ft) refer to the weight of the feature ft for X. The most common choices for the weighting function in traditional count-based distributional models are positive variants of pointwise mutual information (PMI) (Bullinaria and Levy 2007) and local mutual information (LMI) (Evert 2008). WeedsPrec (DEM1 ). This DEM quantifies the weighted inclusion of the features of a concept word X within the features of a concept word Y (Weeds and Weir 2003; Weeds, Weir, and McCarthy 2004; Kotlerman et al. 2010): P DEM1 (X, Y) = ft∈FeatX ∩FeatY P ft∈FeatX wX ( ft) wX ( ft) (5) WeedsSim (DEM2 ). It computes the geometrical average of WeedsPrec (DEM1 ) or any other asymmetric measure (e.g., APinc from Kotlerman et al. [2010]) and the symmetric 21 Note that, unlike with similarity scores, the score now refers to an asymmetric relation stemming from the question “Is X a type of Y” for the word pair (X, Y). Therefore, the scores for two reverse pairs (X, Y) and (Y, X) should be different; see also Table 8. 806 Vuli´c et al. HyperLex: A Large-Scale Ev"
J17-4004,C04-1146,0,0.49462,"Missing"
J17-4004,Q15-1025,0,0.5743,"l models relying on the distributional inclusion hypothesis (Geffet and Dagan 2 HyperLex is available online at: http://people.ds.cam.ac.uk/iv250/hyperlex.html. 783 Computational Linguistics Volume 43, Number 4 2005; Lenci and Benotto 2012) and semantic generality computations (Santus et al. 2014); (ii) multi-modal approaches (Kiela et al. 2015); (iii) WordNet-based approaches (Pedersen, Patwardhan, and Michelizzi 2004); and (iv) a selection of state-of-the-art recent word embeddings, some optimized for similarity on semantic similarity data sets (Mikolov et al. 2013b; Levy and Goldberg 2014; Wieting et al. 2015, inter alia), others developed to better capture the asymmetric LE relation (Vilnis and McCallum 2015; Vendrov et al. 2016). Because of its size, and unlike other word pair scoring data sets such as SimLex-999 or WordSim-353, in HyperLex we provide standard train/dev/test splits (both random and lexical [Levy et al. 2015; Shwartz, Goldberg, and Dagan 2016]) so that HyperLex can be used for supervised learning. We therefore evaluate several prominent supervised LE architectures (Baroni et al. 2012; Roller, Erk, and Boleda 2014; Weeds et al. 2014, inter alia). Although we observe interesting di"
J17-4004,P94-1019,0,0.205773,"in three variant WN-based models: (1) WN–B ASIC: fWN returns a score denoting how similar two concepts are, based on the shortest path that connects the concepts in the WN taxonomy. (2) WN–LC H: Leacock–Chodorow similarity function (Leacock and Chodorow 1998) returns a score denoting how similar two concepts are, based on their shortest connecting path (as above) and the maximum depth of the taxonomy in which the concepts occur. The score is then − log(path/2 · depth), where path is the shortest connecting path length and depth the taxonomy depth. (3) WN–W U P: Wu–Palmer similarity function (Wu and Palmer 1994; Pedersen, Patwardhan, and Michelizzi 2004) returns a score denoting how similar two concepts are, based on their depth in the taxonomy and that of their most specific ancestor node. Note that all three WN-based similarity measures are not well-suited for graded LE experiments by their design: For example, they will rank direct co-hyponyms as more similar than distant hyponymy–hypernymy pairs. 6.6 Order Embeddings Following trends in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a v"
J17-4004,D12-1111,0,0.0202343,"Missing"
J17-4004,Q14-1006,0,0.0151611,"in semantic similarity (or graded synonymy computations, see Section 2 again), Vendrov et al. (2016) have recently demonstrated that it is possible to construct a vector space or a word embedding model that specializes in the lexical entailment relation, rather than in the more popular similarity/synonymy relation. The model is then applied to a variety of tasks including ungraded LE detection and directionality. 809 Computational Linguistics Volume 43, Number 4 Figure 6 A slice of the visual–semantic hierarchy. The toy example is taken from Vendrov et al. (2016), inspired by the resource of Young et al. (2014). The order embedding model exploits the partial order structure of a visual–semantic hierarchy (see Figure 6) by learning a mapping which is not distance-preserving but order-preserving between the visual–semantic hierarchy and a partial order over the embedding space. It learns a mapping from a partially ordered set (U, U ) into a partially ordered embedding space (V, V ): the ordering of a pair in U is then based on the ordering in the embedding space. The chosen embedding space is the reversed product order on RN + , defined by the conjunction of total orders on each coordinate: ~ ~ Y X"
J17-4004,P14-2089,0,0.0202901,"iminary analysis advocates the use of more sophisticated learning algorithms in future work. Another path of research work could investigate how to exploit more training data from resources other than HyperLex to yield improved graded LE models. 7.4 Further Discussion: Specializing Semantic Spaces Following the growing interest in word representation learning, this work also touches upon the ideas of vector/semantic space specialization: A desirable property of representation models is their ability to steer their output vector spaces according to explicit linguistic and dictionary knowledge (Yu and Dredze 2014; Astudillo et al. 2015; Faruqui 823 Computational Linguistics Volume 43, Number 4 et al. 2015; Liu et al. 2015; Wieting et al. 2015; Mrkši´c et al. 2016; Vuli´c et al. 2017, inter alia). Previous work showed that it is possible to build vector spaces specialized for capturing different lexical relations (e.g., antonymy [Yih, Zweig, and Platt 2012; Ono, Miwa, and Sasaki 2015]) or distinguishing between similarity and relatedness (Kiela, Hill, and Clark 2015). Yet, it is to be seen how to build a representation model specialized for the graded LE relation. An analogy with (graded) semantic simi"
J17-4004,N13-1120,0,0.0100078,"ther relations. HyperLex, on the other hand, targets a different type of evaluation. The graded entailment function fgraded defines the following mapping: fgraded : (X, Y) → R+ 0 (4) fgraded outputs the strength of the lexical entailment relation s ∈ R+ 0 . By adopting the graded LE paradigm, HyperLex thus measures the degree of lexical entailment between words X and Y constituting the order-sensitive pair (X, Y). From another perspective, it measures the typicality and graded membership of the instance X for the class/category Y. From the relational similarity viewpoint (Jurgens et al. 2012; Zhila et al. 2013), it also measures the prototypicality of the pair (X, Y) for the LE relation. 3.1.2 Evaluation Sets BLESS. Introduced by Baroni and Lenci (2011), the original BLESS evaluation set includes 200 concrete English nouns as target concepts (i.e., X-s from the pairs (X, Y)), equally divided between animate and inanimate entities. A total of 175 concepts were extracted from the McRae feature norms data set (McRae et al. 2005), and the remaining 25 were selected manually by the authors. These concepts were then paired to 8,625 7 The terms intension and extension assume classical intensional and exten"
J17-4004,J09-3004,0,0.039597,"inguistic entailment more effectively. 2. Graded Lexical Entailment Note on Terminology. Because of dual and inconsistent use in prior work, in this work we use the term lexical entailment (LE) in its stricter definition. It refers precisely to the taxonomical asymmetric hyponymy–hypernymy relation, also known as IS - A, or TYPE - OF relation (Hearst 1992; Snow, Jurafsky, and Ng 2004; Weeds, Weir, and McCarthy 2004; Pantel and Pennacchiotti 2006; Do and Roth 2010, inter alia), e.g., snake is a TYPE - OF animal, computer is a TYPE - OF machine. This is different from an alternative definition (Zhitomirsky-Geffet and Dagan 2009; Kotlerman et al. 2010; Turney and Mohammad 2015) as substitutable lexical entailment: This relation holds for a pair of words (X, Y) if a possible meaning of one word (i.e., X) entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts. This definition is looser and more general than the TYPE - OF definition, as it also encompasses other lexical relations such as synonymy, metonymy, meronymy, and so forth.3 Definitions. The classical definition of ungraded lexical entailment is as follows: Given a concept word pair (X, Y), Y is a hypernym"
J17-4004,H05-1079,0,\N,Missing
J17-4004,W07-1401,0,\N,Missing
J17-4004,C98-2122,0,\N,Missing
J19-3005,P13-2037,0,0.0744461,"Missing"
J19-3005,W17-0401,0,0.0483167,"Missing"
J19-3005,P15-2044,0,0.0358947,"Missing"
J19-3005,W14-4203,0,0.0512933,"Missing"
J19-3005,P15-1040,0,0.0140754,"k is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al."
J19-3005,Q16-1031,0,0.0249656,"Missing"
J19-3005,D18-1549,0,0.0165078,"da et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and consequently their rate of infrequent words. Moreover, the ambiguity of mapping between form and meaning of morphemes determines the usefulness of injecting character-level information (Gerz et al. 2018a, 2018b). This variation has to be taken into account in both language transfer and multilingual joint learning. As a final note, we stress that the addition of new features does not concern just future work, but also the existing typology-savvy methods, which c"
J19-3005,D17-1011,0,0.193671,"ases. In this article, we provide an extensive survey of typologically informed NLP methods to date, including the more recent neural approaches not previously surveyed in this area. We consider the impact of typological (including both structural and semantic) information on system performance and discuss the optimal sources for such information. Traditionally, typological information has been obtained from hand-crafted databases and, therefore, it tends to be coarse-grained and incomplete. Recent research has focused on inferring typological information automatically from multilingual data (Asgari and Schütze 2017, inter alia), with the specific purpose of obtaining a more complete and finer-grained set of feature values. We survey these techniques and discuss ways to integrate their predictions into the current NLP algorithms. To the best of our knowledge, this has not yet been covered in the existing literature. In short, the key questions our paper addresses can be summarized as follows: (i) Which NLP tasks and applications can benefit from typology? (ii) What are the advantages and limitations of currently available typological databases? Can data-driven inference of typological features offer an a"
J19-3005,D08-1014,0,0.0572004,"Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translated documents can also be used to generate multilingual sentence representations, which facilitate language transfer (Zhou, Wan, and Xiao 2016). Some of these methods are hampered by their resource requirements. In fact, annotation projection and translation need parallel texts to align words and train translation systems, respectively (Agi´c, Hovy, and Søgaard 2015). Moreover, comparisons of stateof-the-art algorithms revealed that model"
J19-3005,W09-0106,0,0.0830635,"ction The world’s languages may share universal features at a deep, abstract level, but the structures found in real-world, surface-level texts can vary significantly. This crosslingual variation has challenged the development of robust, multilingually applicable Natural Language Processing (NLP) technology, and as a consequence, existing NLP is still largely limited to a handful of resource-rich languages. The architecture design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das"
J19-3005,P07-1036,0,0.126415,"Missing"
J19-3005,Q18-1039,0,0.0162199,"al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representa"
J19-3005,C16-1298,0,0.0281595,"Missing"
J19-3005,P11-1061,0,0.244408,"009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promising source of informati"
J19-3005,P07-1009,0,0.17624,"Missing"
J19-3005,P16-1038,0,0.174612,"16) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whether a language allows two sounds to differ only in voicing, such as /t/ and /d/. Finally, a small number of experiments adopted the entire feature inventory of typological databases, without any sort of pre-selection. In particular, Agi´c (2017) and Ammar et al. (2016) extracted all the features in WALS, whereas Deri and Knight (2016) extracted all the features in URIEL. Schone and Jurafsky (2001) did not resort to basic typological features, but rather to “several hundred [implicational universals] applicable to syntax” drawn from the Universal Archive (Plank and Filiminova 1996). Typological attributes that are extracted from typological databases are typically represented as feature vectors in which each dimension encodes a feature value. This feature representation is often binarized (Georgi, Xia, and Lewis 2010): For each possible value v of each database attribute a, a new feature is created with value 1 if it corres"
J19-3005,P15-2139,0,0.0321535,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D15-1040,0,0.041461,"ver pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared acros"
J19-3005,D16-1136,0,0.0403964,"Missing"
J19-3005,D12-1001,0,0.018952,"Missing"
J19-3005,P17-2093,0,0.047937,"Missing"
J19-3005,N15-1184,0,0.0387115,"et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mrkši´c et al. 2016; Osborne, Narayan, and Cohen 2016) have proposed means through which external knowledge sourced from linguistic resources (such as WordNet, BabelNet, or lists of morphemes) can be encoded in word embeddings. Among the state-of-the-art specialization methods ATTRACT- REPEL (Mrkši´c et al. 2017; Vuli´c et al. 2017) pushes together or pulls apart 589 Computational Linguistics Volume 45, Number 3 vector pairs according to relational constraints, while preserving the relationship between words in the original space and possibly propagating the specializ"
J19-3005,C10-1044,0,0.0863316,"Missing"
J19-3005,Q18-1032,1,0.878475,"Missing"
J19-3005,D18-1029,1,0.881238,"Missing"
J19-3005,N15-1157,0,0.0210384,"Missing"
J19-3005,C16-1002,0,0.0416532,"ching scenarios (Adel, Vu, and Schultz 2013). In fact, multilingual joint learning improves over pure model transfer also in scenarios with limited amounts of labeled data in target language(s) (Fang and Cohn 2017).4 A key strategy for multilingual joint learning is parameter sharing (Johnson et al. 2017). More specifically, in state-of-the-art neural architectures, input and hidden representations can be either private (language-specific) or shared across languages. Shared representations are the result of tying the parameters of a network component across languages, such as word embeddings (Guo et al. 2016), character embeddings (Yang, Salakhutdinov, and Cohen 2016), hidden layers (Duong et al. 2015b), or the attention mechanism (Pappas and Popescu-Belis 2017). Figure 2 shows an example where all the components of a PoS tagger are shared between two languages (Bambara on the left and Warlpiri on the right). Parameter sharing, however, does not necessarily imply parameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Univ"
J19-3005,P15-1119,0,0.0603047,"Missing"
J19-3005,P16-1228,0,0.0498272,"Missing"
J19-3005,D16-1173,0,0.0523125,"Missing"
J19-3005,P11-1057,0,0.205918,"gure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer"
J19-3005,C12-1089,0,0.0473073,"Missing"
J19-3005,P13-1117,0,0.0959291,"Missing"
J19-3005,P11-2055,0,0.04082,"Missing"
J19-3005,I08-2093,0,0.0406793,"ions. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Glossed Texts (IGT). Such collections of example sentences are collated by linguists and contain grammatical glosses with morphological information. These can guide alignment between the example sentence and its English translation. Lewis and Xia (2008) and Bender et al. (2013) project chunking information from English and train context free grammars on target languages. After collapsing identical rules, they arrange them by frequency and infer word order features. 574 Ponti et al. Modeling Language Variation and Universals Unsupervised Propagation Morphosyntactic Annotation Table 2 An overview of the strategies for prediction of typological features. Author Details Requirements Liu (2010) Lewis and Xia (2008) Treebank count IGT projection Treebank IGT, source chunker 20 97 Bender et al. (2013) IGT projection IGT, source chunker 31 Östling ("
J19-3005,P13-3022,0,0.408676,"k-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word order and case alignment 986 word order 6 word order 325 whole word order and passive whole 14"
J19-3005,W15-1521,0,0.0349482,"Missing"
J19-3005,D17-1268,0,0.142382,"Missing"
J19-3005,P08-1099,0,0.0527456,"hallenge: How can the output of the model be biased to agree with the constraints while the efficiency of the search procedure is kept? In this article we do not answer this question directly but rather survey a number of approaches that succeed in dealing with it. Because linear models have been prominent in NLP research for a much longer time, it is not surprising that frameworks for the integration of soft constraints into these models are much more developed. The approaches proposed for this purpose include posterior regularization (PR) (Ganchev et al. 2010), generalized expectation (GE) (Mann and McCallum 2008), constraint-driven learning (CODL) (Chang, Ratinov, and Roth 2007), dual decomposition (DD) (Globerson and Jaakkola 2007; Komodakis, Paragios, and Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distributio"
J19-3005,P05-1012,0,0.0516887,"Missing"
J19-3005,Q17-1022,1,0.921061,"Missing"
J19-3005,N16-1018,0,0.029262,"Missing"
J19-3005,I17-1046,0,0.464172,"ea, and typology-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme"
J19-3005,P12-1066,0,0.286336,"Missing"
J19-3005,D10-1120,0,0.0346019,"Tziritas 2011), and Bayesian modeling (Cohen 2016). These techniques use different types of knowledge encoding—for example, PR uses expectation constraints on the posterior parameter distribution, GE prefers parameter settings where the model’s distribution on unsupervised data matches a predefined target distribution, CODL enriches existing statistical models with Integer Linear Programming constraints, and in Bayesian modeling a prior distribution is defined on the model parameters. PR has already been used for incorporating universal linguistic knowledge into an unsupervised parsing model (Naseem et al. 2010). In the future, it could be extended to typological knowledge, which is a good fit for soft constraints. As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been"
J19-3005,W11-2124,0,0.0331622,"rameter identity: It can be enforced by minimizing the distance between parameters 4 This approach is also more cost-effective in terms of parameters (Pappas and Popescu-Belis 2017). 566 Ponti et al. Modeling Language Variation and Universals Figure 2 In multilingual joint learning, representations can be private or shared across languages. Tied parameters are shown as neurons with identical color. Image adapted from Fang and Cohn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vector"
J19-3005,C16-1123,1,0.918794,"Missing"
J19-3005,Q16-1030,0,0.164108,"Missing"
J19-3005,P15-2034,0,0.172911,"syntactically annotated texts. For example, word order features can be calculated by counting the average direction of dependency relations or constituency hierarchies (Liu 2010). Consider the tree of a sentence in Welsh from Bender et al. (2013) in Figure 6. The relative order of verb– subject, and verb–object can be deduced from the position of the relevant nodes VBD, NNS , and NNO (highlighted). Morphosyntactic annotation is often unavailable for resource-lean languages. In such cases, it can be projected from a source language to a target language through language transfer. For instance, Östling (2015) projects source morphosyntactic annotation directly to several languages through a multilingual word alignment. After the alignment and projection, word order features are calculated by the average direction of dependency relations. Similarly, Zhang et al. (2016) transfer PoS annotation with a model transfer technique relying on multilingual embeddings, created through monolingual mapping (see § 3.3). After the projection, they predict feature values with a multiclass support vector machine using PoS tag n-gram features. Finally, typological information can be extracted from Interlinear Gloss"
J19-3005,E17-2102,0,0.0715416,"g multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig, and Littell 2017, see § 4.3"
J19-3005,H05-1108,0,0.0802046,"cific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can lead to significant improvements in performance and parameter efficiency over monolingual baselines (Pappas and Popescu-Belis 2017). Another, highly promisin"
J19-3005,I17-1102,0,0.0522275,"Missing"
J19-3005,P18-1142,1,0.820943,"Missing"
J19-3005,D18-1026,1,0.847658,"Missing"
J19-3005,S17-1003,1,0.801184,"Missing"
J19-3005,N12-1008,1,0.844552,". As another option, Bayesian modeling sets prior probability distributions according to the relationships encoded in typological features (Schone and Jurafsky 2001). Finally, DD has been applied to multi-task learning, which paves the way for typological knowledge encoding through a multi-task architecture in which one of the tasks is the actual NLP application and the other is the data-driven prediction of typological features. In fact, a modification of this architecture has already been applied to minimally supervised learning and domain adaptation with soft (non-typological) constraints (Reichart and Barzilay 2012; Rush et al. 2012). The same ideas could be exploited in deep learning algorithms. We have seen in § 3.2 that multilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, simila"
J19-3005,P15-2040,0,0.0202231,"ribution of each language and example. The selection is typically carried out through general language similarity metrics. For instance, Deri and Knight (2016) base their selection on the URIEL language typology database, considering information about genealogical, geographic, syntactic, and phonetic properties. This facilitates language transfer of grapheme-to-phoneme models, by guiding the choice of source languages and aligning phoneme inventories. Metrics for source selection can also be extracted in a data-driven fashion, without explicit reference to structured taxonomies. For instance, Rosa and Zabokrtsky (2015) estimate the Kullback–Leibler divergence between PoS trigram distributions for delexicalized parser transfer. In order to approximate the divergence in syntactic structures between languages, Ponti et al. (2018a) utilize the Jaccard distance between morphological feature sets and the tree edit distance of delexicalized dependency parses of translationally equivalent sentences. A priori and bottom–up approaches can also be combined. For delexicalized parser transfer, Agi´c (2017) relies on a weighted sum of distances based on (1) the PoS divergence defined by Rosa and Zabokrtsky (2015); (2) th"
J19-3005,P15-1173,0,0.0316864,"Missing"
J19-3005,P18-1084,1,0.891535,"Missing"
J19-3005,C14-1098,0,0.0637796,"Missing"
J19-3005,D12-1131,1,0.808979,"Missing"
J19-3005,S10-1027,0,0.0116844,"s-lingual information about frame semantics can be extracted, for example, from the Valency Patterns Leipzig database (ValPaL). Typological information regarding lexical semantics patterns can further assist various NLP tasks by providing information about translationally equivalent words across languages. Such information is provided in databases such as the World Loanword Database (WOLD), the Intercontinental Dictionary Series (IDS), and the Automated Similarity Judgment Program (ASJP). One example task is word sense disambiguation, as senses can be propagated from multilingual word graphs (Silberer and Ponzetto 2010) by bootstrapping from a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, a"
J19-3005,P08-1084,0,0.26666,"ure design, training, and hyper-parameter tuning of most current algorithms are far from being language-agnostic, and often inadvertently incorporate language-specific biases (Bender 2009, 2011). In addition, most state-of-the-art machine learning models rely on supervision from (large amounts of) labeled data—a requirement that cannot be met for the majority of the world’s languages (Snyder 2010). Over time, approaches have been developed to address the data bottleneck in multilingual NLP. These include unsupervised models that do not rely on the availability of manually annotated resources (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011, inter alia) and techniques that transfer data or models from resource-rich to resource-poor languages (Padó and Lapata 2005; Das and Petrov 2011; Täckström, McDonald, and Uszkoreit 2012, inter alia). Some multilingual applications, such as Neural Machine Translation and Information Retrieval, have been facilitated by learning joint models that learn from several languages (Ammar et al. 2016; Johnson et al. 2017, inter alia) or via multilingual distributed representations of words and sentences (Mikolov, Le, and Sutskever 2013, inter alia). Such techniques can"
J19-3005,P11-2120,0,0.0488046,"hand, the accuracy of PoS-based metrics deteriorates easily in scenarios with scarce amounts of data. Source language selection is a special case of source language weighting where weights are one-hot vectors. However, weights can also be gradient and consist of real numbers. Søgaard and Wulff (2012) adapt delexicalized parsers by weighting every 584 Ponti et al. Modeling Language Variation and Universals training instance based on the inverse of the Hamming distance between typological (or genealogical) features in source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing"
J19-3005,C12-2115,0,0.139636,"from the practice of discarding features that are not discriminative, when they are identical for all the languages in the sample. Another group of studies used more comprehensive feature sets. The feature set of Daiber, Stanojevi´c, and Sima’an (2016) included not only WALS word order features but also nominal categories (e.g., “Conjunctions and Universal Quantifiers”) and nominal syntax (e.g., “Possessive Classification”). Berzak, Reichart, and Katz (2015) considered all features from WALS associated with morphosyntax and pruned out the redundant ones, resulting in a total of 119 features. Søgaard and Wulff (2012) utilized all the 571 Computational Linguistics Volume 45, Number 3 Figure 4 Feature sets used in a sample of typologically informed experiments for dependency parsing. The numbers refer to WALS ordering (Dryer and Haspelmath 2013). features in WALS with the exception of phonological features. Tsvetkov et al. (2016) selected 190 binarized phonological features from URIEL (Littel, Mortensen, and Levin 2016). These features encoded the presence of single segments, classes of segments, minimal contrasts in a language inventory, and the number of segments in a class. For instance, they record whet"
J19-3005,N13-1126,0,0.051696,"Missing"
J19-3005,N12-1052,0,0.116447,"Missing"
J19-3005,P15-1150,0,0.0238395,"Missing"
J19-3005,L16-1011,0,0.120366,"Missing"
J19-3005,W15-2137,0,0.099488,"Missing"
J19-3005,P12-1068,0,0.0507951,"d labor. Furthermore, the immense range of possible tasks and languages makes the aim of a complete coverage unrealistic. One solution to this problem explored by the research community abandons the use of annotated resources altogether and instead focuses on unsupervised learning. This class of methods infers probabilistic models of the observations given some latent variables. In other words, it unravels the hidden structures within unlabeled text data. Although these methods have been used extensively for multilingual applications (Snyder and Barzilay 2008; Vuli´c, De Smet, and Moens 2011; Titov and Klementiev 2012, inter alia), their performance tends to lag behind the more linguistically informed supervised learning approaches (Täckström, McDonald, and Nivre 2013). Moreover, they have been rarely combined with typological knowledge. For these reasons, we do not review them in this section. Other promising ways to overcome data scarcity include transferring models or data from resource-rich to resource-poor languages (§ 3.1) or learning joint models from annotated examples in multiple languages (§ 3.2) in order to leverage language interdependencies. Early approaches of this kind have relied on univers"
J19-3005,N16-1161,0,0.0936611,"hn (2017), representing multilingual PoS tagging for Bambara (left) and Warlpiri (right). (Duong et al. 2015a) or between latent representations of parallel sentences (Niehues et al. 2011; Zhou et al. 2015) in separate language-specific models. Another common strategy in multilingual joint modeling is providing information about the properties of the language of the current text in the form of input language vectors (Guo et al. 2016). The intuition is that this helps tailoring the joint model toward specific languages. These vectors can be learned end-to-end in neural language modeling tasks (Tsvetkov et al. 2016; Östling and Tiedemann 2017) or neural machine translation tasks (Ha, Niehues, and Waibel 2016; Johnson et al. 2017). Ammar et al. (2016) instead used language vectors as a prior for language identity or typological features. In § 5.2, we discuss ways in which typological knowledge is used to balance private and shared neural network components and provide informative input language vectors. In § 6.3, we argue that language vectors do not need to be limited to features extracted from typological databases, but should also include automatically induced typological information (Malaviya, Neubig"
J19-3005,P16-1157,0,0.0178939,"§ 4.3). 3.3 Multilingual Representation Learning The multilingual algorithms reviewed in § 3.1 and § 3.2 are facilitated by dense realvalued vector representations of words, known as multilingual word embeddings. These can be learned from corpora and provide pivotal lexical features to several downstream NLP applications. In multilingual word embeddings, similar words (regardless of the actual language) obtain similar representations. Various methods to generate multilingual word embeddings have been developed. We follow the classification proposed by Ruder (2018), and we refer the reader to Upadhyay et al. (2016) for an empirical comparison. 567 Computational Linguistics Volume 45, Number 3 Monolingual mapping generates independent monolingual representations and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual appro"
J19-3005,P11-2084,1,0.843653,"Missing"
J19-3005,P15-2118,1,0.902813,"Missing"
J19-3005,P17-1006,1,0.822412,"Missing"
J19-3005,Q16-1035,0,0.0476954,"source and target languages. An equivalent bottom–up approach is developed by Søgaard (2011), who weighs source language sentences based on the perplexity between their coarse PoS tags and the predictions of a sequential model trained on the target language. Alternatively, the lack of target annotated data can be alleviated by synthesizing new examples, thus boosting the variety and amount of the source data. For instance, the Galactic Dependency Treebanks stem from real trees whose nodes have been permuted probabilistically, according to the word orders of nouns and verbs in other languages (Wang and Eisner 2016). Synthetic trees improve the performance of model transfer for parsing when the source is chosen in a supervised way (performance on target development data) and in an unsupervised way (coverage of target PoS sequences). Rather than generating new synthetic data, Ponti et al. (2018a) leverage typological features to pre-process treebanks in order to reduce their variation in language transfer tasks. In particular, they adapt source trees to the typology of a target language with respect to several constructions in a rule-based fashion. For instance, relative clauses in Arabic (Afro–Asiatic) w"
J19-3005,Q17-1011,0,0.141548,"-based Nearest Neighbors English as a Second Language–based Nearest Neighbors Task-based language vector Task-based language vector WALS 2,150 whole WALS whole whole PoS tag data set 27,824 phonology, morphology, syntax Logistic regression WALS whole whole Bayesian + feature and language interactions Feed-forward Neural Network Genealogy and WALS 2,607 whole Coke, King, and Radev (2016) Littel, Mortensen, and Levin (2016) Berzak, Reichart, and Katz (2014) Supervised Learning Malaviya, Neubig, and Littell (2017) Bjerva and Augenstein (2018) Takamura, Nagata, and Kawasaki (2016) Murawaki (2017) Wang and Eisner (2017) Cotterell and Eisner (2017) Cross-lingual distribution Daumé III and Campbell (2007) Lu (2013) Wälchli and Cysouw (2012) Asgari and Schütze (2017) Roy et al. (2014) Determinant Point Process with neural features Implication universals Automatic discovery Sentence edit distance Pivot alignment Correlations in counts and entropy Genealogy and WALS Genealogy and WALS ESL texts NMT data set WALS, tagger, synthetic treebanks WALS Genealogy and WALS Genealogy and WALS Multi-parallel texts, pivot Multi-parallel texts, pivot None Languages Features word order word and morpheme order, determiners word"
J19-3005,D18-1215,0,0.0199401,"ltilingual joint models combine both shared and language-dependent parameters in order to capture the universal properties and cross-lingual differences, respectively. In order to enforce this division of roles more efficiently, these models could be augmented with the auxiliary task of predicting typological features automatically. This auxiliary objective could update parameters of the language-specific component, or those of the shared component, in an adversarial fashion, similar to what Chen et al. (2018) implemented by predicting language identity. Recently, Hu et al. (2016a, 2016b) and Wang and Poon (2018) proposed frameworks that integrate deep neural models with manually specified or automatically induced constraints. Similar to CODL, the focus in Hu et al. (2016a) and Wang and Poon (2018) is on logical rules, while the ideas in Hu et al. (2016b) are related to PR. These frameworks provide a promising avenue for the integration of typological information and deep models. A particular non-linear deep learning domain where knowledge integration is already prominent is multilingual representation learning (§ 3.3). In this domain, a number of works (Faruqui et al. 2015; Rothe and Schütze 2015; Mr"
J19-3005,Q14-1005,0,0.0236961,"et al. (2005). In its original formulation, as illustrated in Figure 1(a), a source text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompati"
J19-3005,D14-1187,0,0.0145363,"text is parsed and word-aligned with a target parallel raw text. Its annotation (e.g., PoS tags and dependency trees) is then projected directly between corresponding words and used to train a supervised model on the target language. Later refinements to this process are known as soft projection, where constraints can be used to complement alignment, based on distributional similarity (Das and Petrov 2011) or constituent membership (Padó and Lapata 2009). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-indepe"
J19-3005,W14-1613,0,0.0141525,"ns and subsequently learns a linear map between a source language and a target language based on a bilingual lexicon (Mikolov, Le, and Sutskever 2013) or in an unsupervised fashion through adversarial networks (Conneau et al. 2017). Alternatively, both spaces can be cast into a new, lower-dimensional space through canonical correlation analysis based on dictionaries (Ammar et al. 2016) or word alignments (Guo et al. 2015). Pseudo-cross-lingual approaches merge words with contexts of other languages and generate representations based on this mixed corpus. Substitutions are based on Wiktionary (Xiao and Guo 2014) or machine translation (Gouws and Søgaard 2015; Duong et al. 2016). Moreover, the mixed corpus can be produced by randomly shuffling words between aligned documents in two languages (Vuli´c and Moens 2015). Cross-lingual training approaches jointly learn embeddings from parallel corpora and enforce cross-lingual constraints. This involves minimizing the distance of the hidden sentence representations of the two languages (Hermann and Blunsom 2014) or decoding one from the other (Lauly, Boulanger, and Larochelle 2013), possibly adding a correlation term to the loss (Chandar et al. 2014). Joint"
J19-3005,H01-1035,0,0.123357,"Missing"
J19-3005,I08-3008,0,0.0176835,"). Moreover, source model expectations on labels (Wang and Manning 2014; Agi´c et al. 2016) or sets of most likely labels (Khapra et al. 2011; Wisniewski et al. 2014) can be projected instead of single categorical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translat"
J19-3005,C16-1044,0,0.0230991,"Missing"
J19-3005,D15-1213,0,0.0647972,"Missing"
J19-3005,N16-1156,0,0.0228005,"Missing"
J19-3005,D12-1125,1,0.809705,"orical labels. 565 Computational Linguistics Volume 45, Number 3 These can constrain unsupervised models by reducing the divergence between the expectations on target labels and on source labels or supporting “ambiguous learning” on the target language, respectively. Model transfer instead involves training a model (e.g., a parser) on a source language and applying it on a target language (Zeman and Resnik 2008), as shown in Figure 1(b). Due to their incompatible vocabularies, models are typically delexicalized prior to transfer and take language-independent (Nivre et al. 2016) or harmonized (Zhang et al. 2012) features as input. In order to bridge the vocabulary gap, model transfer was later augmented with multilingual Brown word clusters (Täckström, McDonald, and Uszkoreit 2012) or multilingual distributed word representations (see § 3.3). Machine translation offers an alternative to lexicalization in absence of annotated parallel data. As shown in Figure 1(c), a source sentence is machine translated into a target language (Banea et al. 2008), or through a bilingual lexicon (Durrett, Pauls, and Klein 2012). Its annotation is then projected and used to train a target-side supervised model. Translat"
J19-3005,P16-1133,0,0.0606511,"Missing"
J19-3005,D18-1022,1,0.844622,"om a few pivot pairs (Khapra et al. 2011), by imposing constraints in sentence alignments and harvesting bag-of-words features from these (Lefever, Hoste, and De Cock 2011), or by providing seeds for multilingual WordEmbedding-based lexicalized model transfer (Zennaki, Semmar, and Besacier 2016). Another task where lexical semantics is crucial is sentiment analysis, for similar reasons: Bilingual lexicons constrain word alignments for annotation projection (Almeida et al. 2015) and provide pivots for shared multilingual representations in model transfer (Fernández, Esuli, and Sebastiani 2015; Ziser and Reichart 2018). Moreover, sentiment 587 Computational Linguistics Volume 45, Number 3 analysis can leverage morphosyntactic typological information about constructions that alter polarity, such as negation (Ponti, Vuli´c, and Korhonen 2017). Finally, morphological information was shown to aid interpreting the intrinsic difficulty of texts for language modeling and neural machine translation, both in supervised (Johnson et al. 2017) and in unsupervised (Artetxe et al. 2018) set-ups. In fact, the degree of fusion between roots and inflectional/derivative morphemes impacts the type/token ratio of texts, and co"
J19-3005,Q17-1024,0,\N,Missing
J19-3005,D18-1269,0,\N,Missing
K17-1013,W13-3520,0,0.0499698,"eled conjll. If both are used, the label is simply conj=conjlr+conjll.7 Consequently, the individual context bags we use in all experiments are: subj, obj, comp, nummod, appos, nmod, acl, amod, prep, adv, compound, conjlr, conjll. 4.2 Training and Evaluation We run the algorithm for context configuration selection only once, with the SGNS training setup described below. Our main evaluation setup is presented below, but the learned configurations are tested in additional setups, detailed in Sect. 5. Training Data Our training corpus is the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013),8 consisting of approxi7 Given the coordination structure boys and girls, conjlr training pairs are (boys, girls_conj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of t"
K17-1013,P14-2131,0,0.0737811,"v et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still th"
K17-1013,P14-1023,0,0.0288632,"alian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and ve"
K17-1013,C10-1011,0,0.0100926,"nj), (girls, boys_conj −1 ), while conjll pairs are (boys, girls_conj), (girls, boys_conj). 8 https://sites.google.com/site/rmyeid/projects/polyglot 116 mately 75M sentences and 1.7B word tokens. The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the state-of-the art TurboTagger (Martins et al., 2013).9 The parser was trained using default settings (SVM MIRA with 20 iterations, no further parameter tuning) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. The data were then parsed with UD using the graph-based Mate parser v3.61 (Bohnet, 2010)10 with standard settings on TRAIN + DEV of the UD treebank. Evaluation We experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999. We report Spearman’s ρ correlation between the ranks derived from the scores of the evaluated models and the human scores. Our evaluation setup is borrowed from Levy et al. (2015): we perform 2-fold cross-validation, where the context configurations are optimised on a development set, separate from the unseen test data. Unless stated otherwise, the reported scores are always the averages of the 2 runs"
K17-1013,D14-1082,0,0.0395504,"at the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, rea"
K17-1013,W14-1618,0,0.0925589,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,P06-1038,1,0.647241,"sequential position of each context word. Given the example from Fig. 1, POSIT with the window size 2 extracts the following contexts for discovers: Australian_-2, scientist_-1, stars_+2, with_+1. - DEPS-All: All dependency links without any context selection, extracted from dependency-parsed data with prepositional arc collapsing. - COORD: Coordination-based contexts are used as fast lightweight contexts for improved representations of adjectives and verbs (Schwartz et al., 2016). This is in fact the conjlr context bag, a subset of DEPS-All. - SP: Contexts based on symmetric patterns (SPs, (Davidov and Rappoport, 2006; Schwartz et al., 2015)). For example, if the word X and the word 9 10 Context Group Adj Verb Noun conjlr (A+N+V) obj (N+V) prep (N+V) amod (A+N) compound (N) adv (V) nummod (-) 0.415 -0.028 0.188 0.479 -0.124 0.197 -0.142 0.281 0.309 0.344 0.058 -0.019 0.342 -0.065 0.401 0.390 0.387 0.398 0.416 0.104 0.029 Table 1: 2-fold cross-validation results for an illustrative selection of individual context bags. Results are presented for the noun, verb and adjective subsets of SimLex-999. Values in parentheses denote the class-specific initial pools to which each context is selected based on its ρ sc"
K17-1013,de-marneffe-etal-2014-universal,0,0.0293418,"Missing"
K17-1013,W08-1301,0,0.125848,"Missing"
K17-1013,N15-1184,0,0.0343743,"searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scopre stelle con telescopio nmod amod Australian nsubj scientist dobj discovers case stars with telescope prep:with Figure 1: Ext"
K17-1013,C12-1059,0,0.0242772,"ed in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS configuration is outscored by SP, but the margin is negligible. We als"
K17-1013,D15-1242,0,0.0909931,"bs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case do"
K17-1013,Q15-1016,0,0.665137,"trate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, S"
K17-1013,N15-1142,0,0.0798687,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,D15-1161,0,0.115421,"an the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Scienziato with case dobj australiano scop"
K17-1013,P15-1145,0,0.038685,"ore useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope nmod nsubj amod Sci"
K17-1013,P13-2109,0,0.0606688,"Missing"
K17-1013,P13-2017,0,0.0177128,"rch algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based configuration space can be developed for multiple languages based on the same design principles. Indeed, in this work we show that the optimal configurations for English translate to improved representations in two additional languages, German and Italian. And so, given a (UD-)parsed training corpus, for each target word w with modifiers m1 , . . . , mk and a head h, the word w is paired with context elements m1 _r1 , . . . , mk"
K17-1013,N15-1050,0,0.0247132,"its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as contexts for V and A si"
K17-1013,N16-1118,0,0.353877,"ill considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when l"
K17-1013,P14-2050,0,0.0802645,"le features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the ind"
K17-1013,Q17-1022,1,0.874088,"Missing"
K17-1013,J07-2002,0,0.313892,"Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended"
K17-1013,D14-1162,0,0.0756258,"figurations across languages: these configurations improve the SGNS performance when trained with German or Italian corpora and evaluated on class-specific subsets of the multilingual SimLex-999 (Leviant and Reichart, 2015), without any language-specific tuning. 2 Related Work Word representation models typically train on (word, context) pairs. Traditionally, most models use bag-of-words (BOW) contexts, which represent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic"
K17-1013,petrov-etal-2012-universal,0,0.227836,"path of the best-scoring lower-level configuration even if its score is lower than that of its origin. As we do not observe any significant improvement with this variant, we opt for the faster and simpler one. 5 https://bitbucket.org/yoavgo/word2vecf 6 SGNS for all models was trained using stochastic gradient descent and standard settings: 15 negative samples, global learning rate: 0.025, subsampling rate: 1e − 4, 15 epochs. Universal Dependencies as Labels The adopted UD scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the universal POS tagset (Petrov et al., 2012). It is straightforward to “translate” previous annotation schemes to UD (de Marneffe et al., 2014). Providing a consistently annotated inventory of categories for similar syntactic constructions across languages, the UD scheme facilitates representation learning in languages other than English, as shown in (Vuli´c and Korhonen, 2016; Vuli´c, 2017). Individual Context Bags Standard post-parsing steps are performed in order to obtain an initial list of individual context bags for our algorithm: (1) Prepositional arcs are collapsed ((Levy and Goldberg, 2014a; Vuli´c and Korhonen, 2016), see Fig."
K17-1013,P93-1034,0,0.74216,"Missing"
K17-1013,K15-1026,1,0.948135,"for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In thi"
K17-1013,N16-1060,1,0.312769,"). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not all contexts are created equal”. For example, reaching beyond standard BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015, 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Moreover, Schwartz et al. (2016) demonstrated that a subset of dependency-based contexts which covers only coordination structures is particularly effective for SGNS training, both in terms of the quality of the induced representations and in the reduced training time of the model. Interestingly, they also demonstrated that despite the success with adjectives and verbs, BOW contexts are still the optimal choice when learning representations for nouns (N). In this work, we propose a simple yet effective framework for selecting context configurations, which yields improved representations for verbs, adjectives, and nouns. We s"
K17-1013,N03-1033,0,0.0161394,"ining Setup We first test whether the context configurations learned in Sect. 5.1 are useful when SGNS is trained in another English setup (Schwartz et al., 2016), with more training data and other annotation and parser choices, while evaluation is still performed on SimLex-999. In this setup the training corpus is the 8B words corpus generated by the word2vec script.13 A preprocessing step now merges common word pairs and triplets to expression tokens (e.g., Bilbo_Baggins). The corpus is parsed with labelled Stanford dependencies (de Marneffe and Manning, 2008) using the Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012). SGNS preprocessing and parameters are also replicated; we now 13 Table 6: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the setup from Schwartz et al. (2016). d = 500. BEST-* are again the best class-specific configs returned by Alg. 1. train 500-dim embeddings as in prior work.14 Results are presented in Tab. 6. The imported class-specific configurations, computed using a much smaller corpus (Sect. 5.1), again outperform competitive baseline context types for adjectives and nouns. The BEST-VERBS co"
K17-1013,P10-1040,0,0.0566914,"ning time. Our results generalise: we show that the configurations our algorithm learns for one English training setup outperform previously proposed context types in another training setup for English. Moreover, basing the configuration space on universal dependencies, it is possible to transfer the learned configurations to German and Italian. We also demonstrate improved per-class results over other context types in these two languages. 1 Introduction Dense real-valued word representations (embeddings) have become ubiquitous in NLP, serving as invaluable features in a broad range of tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered a robust and effective choice for a word representation model, due to its simplicity, fast training, as well as its solid performance across semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original SGNS implementation learns word representations from local bag-of-words contexts (BOW). However, the underlying model is equally applicable with other context types (Levy and Goldberg, 2014a). Recent work suggests that “not"
K17-1013,Q14-1020,0,0.0199341,"ified by similar adjectives”. In another example, “two verbs are similar if they are used as predicates of similar nominal subjects” (the nsubj and nsubjpass dependency relations). First, we have to define an expressive context configuration space that contains potential training configurations and is effectively decomposed so that useful configurations may be sought algorithmically. We can then continue by designing a search algorithm over the configuration space. Context Configuration Space We focus on the configuration space based on dependency-based contexts (DEPS) (Padó and Lapata, 2007; Utt and Padó, 2014). We choose this space due to multiple reasons. First, dependency structures are known to be very useful in capturing functional relations between words, even if these relations are long distance. Second, they have been proven useful in learning word embeddings (Levy and Goldberg, 2014a; Melamud et al., 2016). Finally, owing to the recent development of the Universal Dependencies (UD) annotation scheme (McDonald et al., 2013; Nivre et al., 2016)1 it is possible to reason over dependency structures in a multilingual manner (e.g., Fig. 1). Consequently, a search algorithm in such DEPS-based conf"
K17-1013,E17-2065,1,0.823839,"Missing"
K17-1013,P16-2084,1,0.761929,"Missing"
K17-1013,Q15-1025,0,0.0546541,"dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers case stars telescope n"
K17-1013,D12-1086,0,0.0288956,"epresent a word using its neighbouring words, irrespective of the syntactic or semantic relations between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, inter alia). Several alternative context types have been proposed, motivated by the limitations of BOW contexts, most notably their focus on topical rather than functional similarity (e.g., coffee:cup vs. coffee:tea). These include dependency contexts (Padó and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al., 2010; Schwartz et al., 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015). Several recent studies examined the effect of context types on word representation learning. Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups: BOW, dependency links, and substitute vectors. They show that the optimal type largely depends on the task at hand, with dependency-based contexts displaying strong performance on semantic similarity tasks. Vuli´c and Korhonen (2016) extended the comparison to more languages, reaching similar conclusions. Schwartz et al. (2016), showed that symmetric patterns are useful as"
K17-1013,P14-2089,0,0.0430435,"ctures, a particular dependency link, are more useful for verbs and adjectives than the entire set of dependencies. In this work, we generalise their approach: our algorithm systematically and efficiently searches the space of dependency-based context configurations, yielding class-specific representations with substantial gains for all three word classes. Previous attempts on specialising word representations for a particular relation (e.g., similarity vs relatedness, antonyms) operate in one of two frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015b); (2) post-processing procedures which use lexical knowledge to refine previously trained word vectors (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017). Our work suggests that the induced representations can be specialised by directly training the word representation model with carefully selected contexts. 3 Context Selection: Methodology The goal of our work is to develop a methodology for the identification of optimal context configura113 3.1 nmod amod Australian nsubj scientist dobj discovers"
K19-1004,N19-1391,0,0.318028,"ly context-aware dataset for evaluating cross-lingual embeddings on the word level is Bilingual Contextual Word Similarity (BCWS) (Chi and Chen, 2018). It challenges a system to predict similarity scores between cross-lingual word pairs with sentential context provided in both languages. However, BCWS does not explicitly test for the retrieval of meaning-equivalent cross-lingual contextualized embeddings, which is explicitly tested in our test. Also, BCWS is only available for one language pair: English-Chinese. Another task used for evaluating contextualized embeddings is Sentence Retrieval (Aldarmaki and Diab, 2019): given a query source sentence, the task is to retrieve the corresponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an or"
K19-1004,D18-1027,0,0.0469943,"e also experimented with directly aligning embeddings obtained from the BERT multilingual model, which is a joint model trained with the same model parameters with shared subword vocabulary (Devlin et al., 2019). This means that identical words in two different languages will obtain the same embeddings. W = arg min kW S − T k2 s.t. W T W = I. (1) W The closed-form solution can be found by solving the orthogonal Procrustes problem (Sch¨onemann, 1966) as follows: T S T = U ΣV T ; W = U V T (2) We also optionally apply a post-processing Meeting-in-the-Middle (MIM) technique, recently proposed by Doval et al. (2018). It first calculates the average of each dictionary item representation in a pair after the orthogonal mapping: we denote the matrix U as the matrix where each column is such an average vector. Then, it finds a linear mapping M from both the source language (denoted as Ms ) and the target language (Mt ) after the previous step of orthogonal mapping to minimize the distance to U via a closed-form solution. Equation (3) formulates how to find Ms , and we do the same from target to source. Ms = arg min kMs W S − U k2 (3) Ms We apply the orthogonal mapping and MIM both on static embeddings (for b"
K19-1004,D16-1250,0,0.220628,"ponding parallel sentence in the target language. Sentences can be represented as averages of contextualized embeddings of their constituent words. As the task does not explicitly evaluate at the word level, even if a system cannot accurately capture polysemy, it can rely on other words in the sentence to retrieve the correct parallel sentence. Therefore, Sentence Retrieval may lead to superficially high scores. Cross-lingual Word Embeddings. We conduct our experiments using a popular projection-based approach that learns an orthogonal mapping between pretrained embeddings (Xing et al., 2015; Artetxe et al., 2016). The orthogonality of the mapping is crucial as it preserves monolingual invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned w"
K19-1004,N13-1073,0,0.110811,"Missing"
K19-1004,Q17-1010,0,0.460088,"n a dictionary with item pairs from source and target languages (si , ti ), and matrices S and T that contain the vector representations corresponding to the item pairs in the columns, we follow the standard practice (Glavaˇs et al., 2019) to find an orthogonal alignment matrix W that minimizes the distance between the transformed matrix W S and T . For improved performance, following Artetxe et al. (2016), we normalize and mean center the embeddings in S and T . The mapping is as follows: Methods Monolingual Contextualized Embeddings Compared to static word embeddings (Mikolov et al., 2013b; Bojanowski et al., 2017), more recent contextualized embeddings provide dynamic representations for a word in context as hidden layers in a deep neural network. They are typically obtained by unsupervised pretraining based on language modeling objectives (Devlin et al., 2019; Yang et al., 2019). The underlying contextualized method in our study is the pretrained BERTbase cased model1 (Devlin et al., 2019). BERT is trained using a transformer architecture (Vaswani et al., 2017) with masked language modelling (MLM) and next sentence prediction (NSP) tasks. MLM predicts the vocabulary id of a randomly masked word in a s"
K19-1004,P19-1070,1,0.84668,"Missing"
K19-1004,P13-1133,0,0.0433986,"e pair are aligned. Therefore, the final contexts for the source and target word in the word pair are indeed non-parallel. The use of non-parallel contexts here is crucial because when we perform the token retrieval task, parallel contexts can be superficially retrieved by simply matching the contexts rather than repreCollecting Translation Pairs. We select a representative set of query words from WordNet (Miller, 1998) (one unique word per WordNet synset). For each source word, we retrieve its WordNet senses and the corresponding translations in the target language from Multilingual WordNet (Bond and Foster, 2013). As WordNet senses are too fine-grained, we collapse senses into clusters if they contain the same translation for the source word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word emb"
K19-1004,P12-1092,0,0.177851,"Missing"
K19-1004,S17-2002,0,0.031663,"uation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings is Bilingual Dictionary Induction (BDI) (Vuli´c and Moens, 2013; Mikolov et al., 2013a; Gouws et al., 2015): given a source query word, the task is to retrieve the translation word in the target language. The test words in BDI are out-of-context and polysemy cannot be addressed properly. The same issue is found in another relevant lexical task, Cross-lingual Semantic Similarity. (Camacho-Collados et al., 2017). (1) Cross-lingual Word Sense Disambigution: source query: the national [coach] of the Irish teams ... answer: allenatore (Italian); Fußbaltrainer; Nationaltrainer; Trainer (German); entrenador(Spanish) ... (2) Cross-lingual Lexical Substitution : source query: She looked as [severely] as she could muster at Draco. answer: rigurosamente, seriamente (3) BTSR: source query: The reflections included in this document are linked to discussions with many colleagues and friends, in the present [tense]. 34 3.2 answer: Scott Peterson meti´o la pata elfondo y us´o el [tiempo] pasado mientras afirmaba q"
K19-1004,W09-2413,0,0.043201,"trieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring"
K19-1004,K18-2005,0,0.0169691,"to serve as anchors for the alignment using static dictionaries, or we use parallel sentences as dictionary items to directly align contextualized word representations on the token level. We discuss this in what follows. 1 To produce the contextualized representation for a word in context, we average the 12 hidden layers of the word’s subword representations in BERT and then average the subword representations as input for the cross-lingual alignment. We leave other ways to extract the representations for future work. 2 We have also experimented with ELMo in lieu of BERT (Peters et al., 2018; Che et al., 2018). However, as we reach similar conclusions in terms of relative performance, while BERT-based cross-lingual embeddings outperform their ELMo-based counterparts in absolute terms, we do not report ELMo’s results for brevity. It should be noted that these pretrained models used different training data. 3.3 Alignment Levels We explore aligning contextualized models on two levels: type-level and token-level. Type-level word representation refers to static word representation that assigns one fixed embedding to a word. All the traditional word embedding models (e.g., skipgram, CBOW, fastText) provi"
K19-1004,D18-1025,0,0.354342,"R task highlights the merits of different alignment methods. In particular, we find that using context average type-level alignment is effective in transferring monolingual contextualized embeddings cross-lingually especially in non-parallel contexts, and at the same time improves the monolingual space. Furthermore, aligning independently trained models yields better performance than aligning multilingual embeddings with shared vocabulary. 1 We evaluate the methods on a variety of contextaware tasks. Besides two previously established evaluation tasks (1) Bilingual Contextual Word Similarity (Chi and Chen, 2018) and (2) Sentence Retrieval (Conneau et al., 2017), we introduce a new task: Bilingual Token-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have b"
K19-1004,N19-1386,0,0.0335393,"al invariance and is empirically proven to be more robust (Smith et al., 2017; Xing et al., 2015). This projection-based method can be applied post-hoc on pretrained monolingual embeddings with an exact analytical solution. Moreover, its performance is often competitive to that of jointly trained crosslingual models using additional bilingual signals in the form of parallel or comparable corpora (Ruder et al., 2019; Glavaˇs et al., 2019). However, projection-based cross-lingual embeddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextu"
K19-1004,N19-1423,0,0.296476,"eval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur mai"
K19-1004,N19-1392,0,0.0656437,"Missing"
K19-1004,D14-1113,0,0.123895,"Missing"
K19-1004,D19-1449,1,0.859792,"Missing"
K19-1004,N13-1011,1,0.878435,"Missing"
K19-1004,N18-1202,0,0.319663,"ken-level Sense Retrieval (BTSR). It is more challenging than the alternatives as it requires the accurate cross-lingual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) alig"
K19-1004,N15-1104,0,0.0932394,"Missing"
K19-1004,P19-1493,0,0.0378202,"gual retrieval of contextualized words on the token level which are disambiguated both in the source and the target language using non-parallel contexts. We provide BTSR task data and run evaluations on two language pairs: English–Chinese (EN–ZH) and English–Spanish (EN–ES). The data and guidelines can be found at: https://github.com/ qianchu/BTSR Introduction Contextualized embeddings have been shown to achieve superior performance compared to static word embeddings in English (Peters et al., 2018; Devlin et al., 2019). Despite recent efforts to better understand their multilingual variants (Pires et al., 2019), leveraging these available pretrained contextualized embeddings to learn cross-lingual contextualized embeddings is still an under-explored area: past cross-lingual embedding alignment methods have mainly focused on static embeddings (Ruder et al., 2019). In this paper, we introduce a first study that investigates and compares different ways of aligning the pretrained contextualized embeddings. In particular, we make the comparisons focused on the following properties: (1) aligning contextualOur main findings are as follows. (1) Using the average of the contextualized word representations as"
K19-1004,N19-1162,0,0.117939,"ddings are still predominantly concerned with static word embeddings (Glavaˇs et al., 2019; Vuli´c et al., 2019; Mohiuddin and Joty, 2019). Learning crosslingual contextualized embeddings is still a large unexplored area with only two concurrent papers at the moment. First, Aldarmaki and Diab (2019) adopt the same projection-based approach as our paper to align contextualized embeddings on the token-level using parallel data. They find that context-aware mapping using parallel data outperforms context-independent mappings from static dictionaries on a parallel Sentence Retrieval task. Second, Schuster et al. (2019) introduce anchor embeddings as the average of contextualized embeddings of a word to perform alignment for contextualized models, and show its effectiveness in cross-lingual dependency parsing. These two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source lan"
K19-1004,W09-2412,1,0.746156,"se two studies are not directly comparable, whereas our paper provides a comprehensive and systematic comparison of various methods for learning cross-lingual contextualized embeddings and introduces a new and more challenging evaluation task. Cross-lingual Word Sense Disambiguation. Our new task is also related to Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2009): given a source language word in context, a system needs to provide the correct sense labels as clustered translation words in a number of target languages. Another related task is Cross-lingual Lexical Substitution (Sinha et al., 2009): the model must provide plausible target language translations for the source language lexical item in the source language context. In contrast, our BTSR task: (1) directly evaluates token-level word representations without the need to predict sense labels from a sense inventory and (2) it contextualizes both the source query and the target candidates ensuring full sense disambiguation. The core differences between the three tasks are illustrated in the following examples below: Evaluation of (Contextualized) Cross-lingual Embeddings. The traditional task to evaluate cross-lingual embeddings"
K19-1004,tian-etal-2014-um,0,0.0293443,"urce word. For example, “uniform” has five WordNet senses which are translated into four distinct Chinese words: 制服(the clothes worn by a particular group), 一致(the translation of two senses: consistent and undifferenti4 Notice the senses are different thus contexts are needed to find the pair corresponding to the same meaning. 37 senting the words in context appropriately. We empirically verified that a simplistic context average baseline outperforms contextualized word embeddings in a variant of our task which relies on parallel contexts. We set aside 1M parallel sentences from the UMCorpus (Tian et al., 2014) (EN–ZH) and the WMT13 news dataset (Bojar et al., 2013) (EN–ES) for extracting the sentence contexts. We end up with 14,604 distinct word pairs with contexts extracted for EN–ZH, and 9,623 pairs for EN–ES. target candidate. We experiment with 20k target candidates and 200k target candidates. 5 Experiments Training Setup. To test the effects of corpora size on the induction of the cross-lingual alignment, we vary the size of the parallel corpus from 100 up to 200k parallel sentences in the UMCorpus and the WMT13 corpus. Word alignment was produced by IBM Model 2 using Fastalign (Dyer et al., 2"
K19-1021,E17-1088,0,0.0541429,"data). Following that, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of e"
K19-1021,Q16-1022,0,0.0419405,"Missing"
K19-1021,P17-2093,0,0.0157947,"at, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of evaluation. Note that"
K19-1021,E17-2040,0,0.0287154,"Missing"
K19-1021,C18-1139,0,0.0675297,"Missing"
K19-1021,E17-2067,0,0.0188003,"word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representa"
K19-1021,Q17-1010,0,0.661817,"tity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of"
K19-1021,D18-1366,0,0.0341798,"translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more"
K19-1021,D17-1078,0,0.0241869,"ecific Data: Task Data. The maximum number of training instances for all languages is again provided in Table 1. As before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keepi"
K19-1021,N18-2085,0,0.0176097,"ithout any available data (Kornai, 2013; Ponti et al., 2018) is a challenge left for future work. (Selection of) Languages. Both sources of data scarcity potentially manifest in degraded task performance for low-resource languages: our goal is to analyze the extent to which these factors affect downstream tasks across morphologically diverse language types that naturally come with varying data sizes to train their respective embeddings and task-based models. Our selection of test languages is therefore guided by the following goals: a) following recent initiatives (e.g. in language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure coverage of different genealogical and typological properties; b) we aim to cover low-resource languages with varying amounts of available WE data and task-specific data. We select 16 languages in total spanning 4 broad In what follows, we further motivate our work by analyzing two different sources of data scarcity: 217 2 For instance, as of April 2019, Wikipedia is available only in 304 out of the estimated 7,000 existing languages. Agglutinative EMB FGET NER MTAG BERT Fusional Introflexive Isolating BM BXR MYV TE TR ZU EN FO GA GOT MT RUE AM HE YO ZH 4"
K19-1021,N15-1140,0,0.04425,"Missing"
K19-1021,Q18-1003,0,0.0380649,"Missing"
K19-1021,D18-1029,1,0.899767,"Missing"
K19-1021,L18-1550,0,0.0288291,"ubword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. In"
K19-1021,L18-1473,1,0.706967,". Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e."
K19-1021,P82-1020,0,0.744642,"Missing"
K19-1021,C18-1216,0,0.0212074,"its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource set"
K19-1021,P18-1007,0,0.0258865,"t al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in"
K19-1021,N16-1030,0,0.0378679,"d-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data spars"
K19-1021,N19-1423,0,0.646923,"at the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the le"
K19-1021,D18-1549,0,0.0214746,"ning the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on variou"
K19-1021,N19-1154,0,0.0223364,"nd taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-in"
K19-1021,P13-1149,0,0.0307702,"leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by p"
K19-1021,E14-2006,0,0.124496,"r (iii) both, we analyse how different data regimes affect the final task performance. 2) We experiment with 16 languages representing 4 diverse morphological types, with a focus on truly low-resource languages such as Zulu, Rusyn, Buryat, or Bambara. 3) We experiment with a variety of subword-informed representation architectures, where the focus is on unsupervised, widely portable language-agnostic methods such as the ones based on character n-grams (Luong and Manning, 2016; Bojanowski et al., 2017), Byte Pair Encodings (BPE) (Sennrich et al., 2016; Heinzerling and Strube, 2018), Morfessor (Smit et al., 2014), or BERT-style pretraining and fine-tuning (Devlin et al., 2019) which relies on WordPieces (Wu et al., 2016). We demonstrate that by tuning subword-informed models in low-resource settings we can obtain substantial gains over subwordagnostic models such as skip-gram with negative sampling (Mikolov et al., 2013) across the board. The main goal of this study is to identify viable and effective subword-informed approaches for truly low-resource languages and offer modeling guidance in relation to the target task, the language at hand, and the (un)availability of general and/or task-specific tra"
K19-1021,W18-1205,0,0.0212108,"e word itself can be appended to the subword sequence and embedded into the subword space in order to incorporate word-level information (Bojanowski et al., 2017). To encode subword order, s can be further enriched by a trainable position embedding p. We use addition to combine subword and position embeddings, namely s := s + p, which has become the de-facto standard method to encode positional information (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al., 2019). Finally, the subword embedding sequence is passed to a composition function, which computes the final word representation. Li et al. (2018) and Zhu et al. (2019) have empirically verified that composition by simple addition, among other more complex composition functions, is a robust choice. Therefore, we use addition in all our experiments. Similar to Bojanowski et al. (2017); Zhu et al. (2019), we adopt skip-gram with negative sampling 218 Component Segmentation Option Morfessor BPE char n-gram Label morf bpeX charn Word token exclusion inclusion ww+ Position embedding exclusion additive pp+ addition add Composition function resentations, and can benefit from the information. Table 2: Components for constructing subwordinformed"
K19-1021,P16-1100,0,0.0620352,"Missing"
K19-1021,P17-1178,0,0.10163,"before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keeping hyper-parameters fixed across different data points will possibly result in underfitting for larger d"
K19-1021,N18-1202,0,0.0352715,"representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the me"
K19-1021,D18-1169,0,0.130939,"a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words"
K19-1021,D17-1010,0,0.0259855,"2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowle"
K19-1021,P16-2067,0,0.096051,"Missing"
K19-1021,P16-1162,0,0.314176,"well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018)"
K19-1021,P17-1184,0,0.0193316,"ed from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused o"
K19-1021,D15-1083,0,0.0434509,"Missing"
K19-1021,C18-1153,0,0.0422342,"Missing"
K19-1021,D18-1059,0,0.0518042,"Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource setups. Our study centers on the following"
K19-1021,N19-1097,1,0.876896,"Missing"
L18-1153,W15-0102,0,0.0258175,"ameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such"
L18-1153,P98-1013,0,0.0943859,"antic clustering, multilingual NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). Howev"
L18-1153,P13-1133,0,0.0212106,"re intentionally restricted, so as to avoid imposing any preconceived semantic categories or classification structure onto the annotators and elicit possibly spontaneous similarity judgments, such discrepancies in detecting ambiguity are inevitable. In order to have more control over which sense of a given verb is taken into consideration in the clustering task, word senses rather than word forms would have to be provided at the start of the task. Such a set-up would also allow comparison of the elicited classes with the existing multilingual sense inventories, like Open Multilingual WordNet (Bond and Foster, 2013) or BabelNet (Navigli and Ponzetto, 2012). Since the aim of the present study was to elicit judgments on basic word forms, without any guidance as to the different word senses available, such comparisons are beyond the scope of this study; however, in future work we intend to extend this analysis and compare our findings against the resources available. 5. Conclusion We have presented the first cross-lingual analysis and evaluation of semantic clustering of verbs by non-expert human annotators. The inter-annotator agreement scores reported for English, Polish, and Croatian are encouraging and"
L18-1153,P12-1090,0,0.0217686,"al verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to th"
L18-1153,D16-1235,1,0.9085,"Missing"
L18-1153,C94-1042,0,0.65147,"c lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and sy"
L18-1153,J15-4004,1,0.623982,"ints of view, for example, lend and borrow, which differ along only one dimension of meaning, that is, the direction of the action (the object of the verb either travels away from the participant (A lends something to B) or towards the participant (B borrows something from A)), and are essentially identical with regard to all other features, which makes them appear semantically close. 4.2. Semantic Similarity versus Relatedness The importance of distinguishing between the concepts of semantic similarity (e.g. cup and mug) and relatedness (e.g. coffee and cup) has been noted in the literature (Hill et al., 2015), and the analysis of our data provides more evidence illustrating the influence of loose association on how humans conceptualize similarity between words, and the difficulty of keeping similarity and relatedness apart. In all three languages we can observe instances of what can be described as a ‘storyline approach’ to judging semantic similarity and verb classification. This is particularly noticeable in Croatian classifications, where several classes formed by the annotators group verbs describing quite different actions, linked via loose thematic ties: (1) marry, conquer, approach, move, w"
L18-1153,S13-2049,0,0.415023,"lusters, and 1 We used the Fuzzy B-Cubed implementation of Jurgens and Klapaftis (2013) but did not associate the clusters with weights, and therefore the metric is equivalent to that of Amig´o et al. (2009). 953 Average B-Cubed English Polish Croatian All 0.262 0.338 0.172 0.205 1c1inst All-instances, One class 0.0 0.069 Table 3: The average B-Cubed F-score (i.e. harmonic mean of B-Cubed precision and recall) calculated for all possible pairings of annotators, for each language individually and across the three languages, and for two SemEval baselines: 1c1inst and All-instances, One class by Jurgens and Klapaftis (2013) to fuzzy clusters, used to evaluate the performance of Word Sense Induction systems in SemEval tasks (Jurgens and Klapaftis, 2013). The B-Cubed metrics (B-Cubed precision and recall) compare two clusterings (say, X and Y) at the item level: for an item i, precision measures how many items sharing a cluster with i in clustering X are placed in its cluster in clustering Y; whereas B-Cubed recall measures how many items sharing a cluster with i in Y are also placed in its cluster in X, with the final B-Cubed score equivalent to the harmonic mean of the two values. In our task, rather than compar"
L18-1153,P14-1097,0,0.0151587,"urrently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge"
L18-1153,korhonen-etal-2006-large,1,0.648455,"for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challeng"
L18-1153,J05-1004,0,0.143765,"NLP 1. Introduction With the recent advances in automatic lexical acquisition, the need for high-quality evaluation resources is ever growing. Due to the pivotal role played by verbs in sentence structure, the problem of creation of verbal classifications has attracted a lot of attention in natural language processing (NLP). Different approaches to creation of verbal classifications have been proposed, varying with regard to the guiding criteria by which the class architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resour"
L18-1153,S16-2012,0,0.0165565,"s NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instructions, which, to the best of our knowledge, is the first evaluation of this approach. D"
L18-1153,W11-2112,0,0.0252591,"ss architecture is organised, prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the"
L18-1153,D12-1048,0,0.0248176,"tic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trai"
L18-1153,C10-1119,1,0.80849,"the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate the need to employ trained lexicographers. However, evaluation of such systems relies on the availability of gold standard classes, and these are still lacking for a great majority of languages. In light of these challenges and the high demand for verbal resources, this paper investigates whether semantic verb classes can be reliably acquired from non-expert native speakers based solely on verb semantics and following simple instruc"
L18-1153,D17-1270,1,0.883378,"Missing"
L18-1153,W11-0110,0,0.0318037,", prioritising semantic (WordNet (Miller, 1995; Fellbaum, 1998), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005)) or syntactic information (COMLEX (Grishman et al., 1994), VALEX (Korhonen et al., 2006)), or combining the two (Levin, 1993; Kipper et al., 2000; Kipper Schuler, 2005). Kipper Schuler’s (2005) VerbNet, grouping English verbs into classes defined by shared meaning components and syntactic behaviour, is one of the richest lexical verb resources currently available, and its utility in various NLP applications has been repeatedly demonstrated (Rios et al., 2011; Windisch Brown et al., 2011; Schmitz et al., 2012; Lippincott et al., 2013; Bailey et al., 2015). However, creation of a similar resource from scratch, drawing simultaneously on semantic and syntactic criteria, is a challenging and time-consuming task when attempted by annotators without theoretical linguistics background (Majewska et al., 2017). A number of approaches to automatic verb classification have been proposed (Joanis et al., 2008; Sun et al., 2010; Falk et al., 2012; Kawahara et al., 2014; Scarton et al., 2014; Peterson et al., 2016; Vuli´c et al., 2017), allowing to minimise the time required and eliminate t"
N13-1011,N09-1003,0,0.00723768,"t translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as"
N13-1011,C10-1003,0,0.0122019,"cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of fre"
N13-1011,J93-2003,0,0.0420768,"Missing"
N13-1011,P11-2071,0,0.0219132,"Missing"
N13-1011,C02-1166,0,0.0175334,"Missing"
N13-1011,C10-2029,0,0.0545471,"s regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual 111 (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (≈ 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and β = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Method"
N13-1011,W04-3208,0,0.0783774,"a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al."
N13-1011,P98-1069,0,0.0359152,"and results on the BLE task for a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce re"
N13-1011,P04-1067,0,0.124411,"Missing"
N13-1011,P08-1088,0,0.198932,". (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V"
N13-1011,D09-1124,0,0.14225,"machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target la"
N13-1011,P10-1026,0,0.0216003,"Missing"
N13-1011,W02-0902,0,0.345399,"d over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additio"
N13-1011,2005.mtsummit-papers.11,0,0.0059277,"ed by the response-based method. That property may be exploited to identify one-to-one translations across languages and build a bilingual lexicon (see Table 1). 4 4.1 Experimental Setup Data Collections We work with the following corpora: • IT-EN-W: A collection of 18, 898 ItalianEnglish Wikipedia article pairs previously used by Vuli´c et al. (2011). • ES-EN-W: A collection of 13, 696 SpanishEnglish Wikipedia article pairs. • NL-EN-W: A collection of 7, 612 DutchEnglish Wikipedia article pairs. • NL-EN-W+EP: The NL-EN-W corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types."
N13-1011,C10-1070,0,0.027199,"is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word r"
N13-1011,P99-1004,0,0.0201023,"action (BLE). Such lexicons and semantically similar words serve as important resources Sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )) (1) cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] denotes a context vector for w1S with N context features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al.,"
N13-1011,D09-1092,0,0.670374,"guage pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similar"
N13-1011,P07-1084,0,0.0104373,"en dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007)"
N13-1011,J03-1002,0,0.0253116,"tic Similarity of Words as the Similarity of Their Semantic Word Responses Ivan Vuli´c and Marie-Francine Moens Department of Computer Science KU Leuven Celestijnenlaan 200A Leuven, Belgium {ivan.vulic,marie-francine.moens}@cs.kuleuven.be Abstract in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are like"
N13-1011,D10-1025,0,0.0173453,"we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual 111 (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (≈ 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and β = 0.01. We are aware that diffe"
N13-1011,P11-1133,0,0.10381,"s-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then: We propose"
N13-1011,P99-1067,0,0.131993,"feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic"
N13-1011,D10-1114,0,0.0178367,"for Response-BC. Additionally, since P (zk |wi ) > 0 and P (wk |wi ) > 0 for each zk ∈ Z and each wk ∈ V S ∪ V T , a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3 4.4 Evaluation Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following"
N13-1011,D12-1003,0,0.535767,"g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word"
N13-1011,P11-2084,1,0.796005,"Missing"
N13-1011,P10-1115,0,0.0615287,"Missing"
N13-1011,P09-1007,0,0.0744512,"Missing"
N13-1011,C98-1066,0,\N,Missing
N18-1048,W13-3520,0,0.199805,"Missing"
N18-1048,D16-1250,0,0.180495,"places word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ... ... ... x&apos;i,h1 (d1=512) Input ... ... ... ... T"
N18-1048,P17-1042,0,0.0514536,"Missing"
N18-1048,P14-2131,0,0.0446942,"ector and the correct target vector to have a maximum cosine similarity. We do not report the results with this variant as, although it outscores the MSE-style objective, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramac"
N18-1048,Q17-1010,0,0.59926,"cialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred"
N18-1048,D14-1082,0,0.0136967,". This approach, applicable to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of inte"
N18-1048,N15-1184,0,0.37829,"Missing"
N18-1048,N13-1092,0,0.108384,"Missing"
N18-1048,D16-1235,1,0.913856,"Missing"
N18-1048,D17-1185,1,0.901185,"Missing"
N18-1048,P15-2011,1,0.915838,"Missing"
N18-1048,P16-1156,0,0.0292948,"these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a lexical resource. This means that they update an"
N18-1048,W14-4337,0,0.149535,"Missing"
N18-1048,J15-4004,1,0.950289,"demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al."
N18-1048,P14-2075,0,0.105851,".10 For a complex word, L IGHT-LS considers the most similar words from the vector space as simplification candidates. Candidates are ranked according to several features, indicating simplicity and fitness for the context (semantic relatedness to the context of the complex word). The substitution is made if the best candidate is simpler than the original word. By providing vector spaces post-specialised for semantic similarity to L IGHT-LS, we expect to more often replace complex words with their true synonyms. We evaluate L IGHT-LS performance in the all setup on the LS benchmark compiled by Horn et al. (2014), who crowdsourced 50 manual simplifications for each complex word. As in prior work, we evaluate performance with the following metrics: 1) Accurracy (Acc.) is the number of correct simplifications made (i.e., the system made the simplification and its substitution is found in the list of crowdsourced substitutions), divided by the total number of indicated complex words; 2) Changed (Ch.) is the percentage of indicated complex words 10 https://github.com/codogogo/lightls Conclusion and Future Work We have presented a novel post-processing model, termed post-specialisation, that specialises wo"
N18-1048,D15-1242,0,0.251526,"Missing"
N18-1048,D15-1032,0,0.0200176,"can thus replace the linear map with a nonlinear function f : Rdim → Rdim . The non-linear mapping, illustrated by Fig. 1b, is implemented as a deep feed-forward fully-connected neural network (DFFN) with H hidden layers and non-linear activations. This variant is called NONLINEAR - MSE. Another variant objective is the contrastive margin-based ranking loss with negative sampling (MM) similar to the original ATTRACT- REPEL objective, used in other applications in prior work (e.g., for cross-modal mapping) (Weston et al., 2011; Frome et al., 2013; Lazaridou et al., 2015; c0 i = f (xi ) denote Kummerfeld et al., 2015). Let x the predicted vector for the word xi ∈ Vs , and let x0 i refer to the “true” vector of xi in the specialised space X0s after the AR specialisation procedure. The MM loss is then defined as follows: JMM = N X k   X  0  0 , x0 0 c c τ δmm − cos x i i + cos x i , x j i=1 j6=i where cos is the cosine similarity measure, δmm is the margin, and k is the number of negative samples. The objective tries to learn the mapping f so c0 i is by the specified that each predicted vector x margin δmm closer to the correct target vector x0 i than to any other of k target vectors x0 j serving as nega"
N18-1048,P15-1027,0,0.481288,"utput of the initial specialisation procedure and replaces word vectors from X0s with their approximations, i.e., f -mapped vectors.2 Objective Functions As mentioned, the N seen words xi ∈ Vs in fact serve as our “pseudotranslation” pairs supporting the learning of a crossspace mapping function. In practice, in its highlevel formulation, our mapping problem is equivalent to those encountered in the literature on crosslingual word embeddings where the goal is to learn a shared cross-lingual space given monolingual vector spaces in two languages and N1 translation pairs (Mikolov et al., 2013a; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Artetxe et al., 2016, 2017; Conneau et al., 2017; Ruder et al., 2017). In our setup, the standard objective based on L2 -penalised 2 We have empirically confirmed the intuition that the first variant is superior to this alternative. We do not report the actual quantitative comparison for brevity. 518 swish swish swish ... swish Xd = Xs ∪ Xu attract-repel X0s ∪ Xu (distributional) (specialised: seen) xi (d=300) mapping c0 Xf = X0s ∪ X u (specialised final: all) (a) High-level illustration Xu x&apos;i,h2 (d2=512) Hidden 1 Hidden 2 ... ... ... ... ... ...... ... ... ... ."
N18-1048,P14-2050,0,0.108038,"nguages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often"
N18-1048,K16-1006,0,0.283738,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,N16-1118,0,0.193378,"ble to any postprocessing model, yields considerable gains over the initial specialisation models both in intrinsic word similarity tasks, and in two downstream tasks: dialogue state tracking and lexical text simplification. The positive effects persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation"
N18-1048,P17-1163,1,0.902962,"Missing"
N18-1048,P15-2130,1,0.814001,"Missing"
N18-1048,Q15-1016,0,0.0670043,"e importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-proc"
N18-1048,Q17-1022,1,0.895295,"Missing"
N18-1048,P15-1145,0,0.186188,"ated Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et"
N18-1048,P16-2074,0,0.109936,"17). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledge only if such knowledge is present in a le"
N18-1048,N15-1100,0,0.578375,"vation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe"
N18-1048,Q16-1030,0,0.371651,"e Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wie"
N18-1048,P15-2070,0,0.0619664,"Missing"
N18-1048,D14-1162,0,0.106855,"s persist across three languages, demonstrating the importance of specialising the full vocabulary of distributional word vector spaces. 1 Introduction Word representation learning is a key research area in current Natural Language Processing (NLP), with its usefulness demonstrated across a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic speci"
N18-1048,P15-1173,0,0.0539443,", 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbitrary vector spaces, yielding state-of-the-art results on language understanding tasks (Faruqui et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Vuli´c et al., 2017b). Existing post-processing models, however, suffer from a major limitation. Their modus operandi is to enrich the distributional information with external knowledg"
N18-1048,K15-1026,0,0.190252,"s a range of tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016b). The standard techniques for inducing distributed word representations are grounded in the distributional hypothesis (Harris, 1954): they rely on co-occurrence information in large textual corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015; Bojanowski et al., 2017). As a result, these models tend to coalesce the notions of semantic similarity and (broader) conceptual relatedness, and cannot accurately distinguish antonyms from synonyms (Hill et al., 2015; Schwartz et al., 2015). Recently, we have witnessed a rise of interest in representation models that move beyond stand-alone unsupervised learning: they leverage external knowledge in human- and automaticallyconstructed lexical resources to enrich the semantic content of distributional word vectors, in a process termed semantic specialisation. This is often done as a post-processing (sometimes referred to as retrofitting) step: input word vectors are fine-tuned to satisfy linguistic constraints extracted from lexical resources such as WordNet or BabelNet (Faruqui et al., 2015; Mrkši´c et al., 2017). The use of exte"
N18-1048,P16-2084,1,0.896635,"Missing"
N18-1048,P16-1024,1,0.911463,"Missing"
N18-1048,N18-1103,1,0.857227,"Missing"
N18-1048,D17-1270,1,0.894634,"Missing"
N18-1048,P17-1006,1,0.878454,"Missing"
N18-1048,E17-1042,1,0.811334,"Missing"
N18-1048,Q15-1025,0,0.28346,"e models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge from available lexical resources (e.g., WordNet, PPDB) into pre-trained word vectors as a post-processing step (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Cotterell et al., 2016; Mrkši´c et al., 2017). Such post-processing models are popular because they offer a portable, flexible, and light-weight approach to incorporating external knowledge into arbit"
N18-1048,P14-2089,0,0.217393,"spaces for English and for three test languages (English, German, Italian), verifying the robustness of our approach. 2 Related Work and Motivation Vector Space Specialisation A standard approach to incorporating external and background knowledge into word vector spaces is to pull the representations of similar words closer together and to push words in undesirable relations (e.g., antonyms) away from each other. Some models integrate such constraints into the training procedure and jointly optimize distributional and nondistributional objectives: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or use a variant of the SGNS-style objective (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). In theory, word embeddings obtained by these joint models could be as good as representations produced by models which finetune input vector space. However, their performance falls behind that of fine-tuning methods (Wieting et al., 2015). Another disadvantage is that their architecture is tied to a specific underlying model (typically word2vec models). In contrast, fine-tuning models inject external knowledge f"
N18-1048,D14-1161,0,0.248362,"tive, it was consistently outperformed by the MM objective. 4 For further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. Additional experiments with other word vectors, e.g., with CONTEXT 2 VEC (Melamud et al., 2016a) (which uses bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for context modeling), and with dependency-word based embeddings (Bansal et al., 2014; Melamud et al., 2016b) lead to similar results and same conclusions. 5 We have experimented with another set of constraints used in prior work (Zhang et al., 2014; Ono et al., 2015), reaching similar conclusions: these were extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009), and comprise 1,023,082 synonymy pairs and 380,873 antonymy pairs. PN  omitted only before the final output layer to enable full-range predictions (see Fig. 1b again). The choices of non-linear activation and initialisation are guided by recent recommendations from the literature. First, we use swish (Ramachandran et al., 2017; Elfwing et al., 2017) as nonlinearity, defined as swish(x) = x · sigmoid(βx). We fix β = 1 as suggested by Ramachandran et al. (2017).6 Second"
N18-1103,W13-3520,0,0.058498,"on combines the symmetric and the asymmetric cost term, in line with the combination of the two used to perform LEAR specialisation. In the evaluation, we show that combining the two cost terms has a synergistic effect, with both terms contributing to stronger performance across all LE tasks used for evaluation. 3 Experimental Setup Starting Distributional Vectors To test the robustness of LEAR specialisation, we experiment with a variety of well-known, publicly available English word vectors: 1) Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) trained on the Polyglot Wikipedia (Al-Rfou et al., 2013) by Levy and Goldberg (2014); 2) G LOVE Common Crawl (Pennington et al., 2014); 3) CONTEXT 2 VEC (Melamud et al., 2016), which replaces CBOW contexts with contexts based on bidirectional LSTMs (Hochreiter and Schmidhuber, 1997); and 4) FASTT EXT (Bojanowski et al., 2017), a SGNS variant which builds word vectors as the sum of their constituent character n-gram vectors.3 Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: directed 1) lexical entailme"
N18-1103,E12-1004,0,0.20297,"Missing"
N18-1103,W11-2501,0,0.347127,"Missing"
N18-1103,P05-1014,0,0.681957,"metric objective performed by the joint LEAR model. Related Work Word Vectors and Lexical Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a significant amount of research in semantics has been invested into its automatic detection and classification. Early work relied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised LE detection models (Tuan et al., 2016; Shwartz et al., 2016; Nguyen et al., 2017; Glavaš and"
N18-1103,D16-1235,1,0.921254,"Missing"
N18-1103,D17-1185,0,0.555597,"Missing"
N18-1103,I13-1095,0,0.237593,"). In this paper, we introduce a novel post-processing model which specialises vector spaces for the lexical entailment (LE) relation. Word-level lexical entailment is an asymmetric semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991). It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies (Fellbaum, 1998). Automatic reasoning about LE supports tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), natural language inference (Dagan et al., 2013; Bowman et al., 2015), text generation (Biran and McKeown, 2013), and metaphor detection (Mohler et al., 2013). Our novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-R EPEL, a state-of-the-art general spe1 Distinguishing between synonymy and antonymy has a positive impact on real-world language understanding tasks such as Dialogue State Tracking (Mrkši´c et al., 2017). 1134 Proceedings of NAACL-HLT 2018, pages 1134–1145 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics The employed asymmetric distance allows one to make graded assertions about hierarchical relation"
N18-1103,Q17-1010,0,0.635989,"ecialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human- or automaticallyconstructed knowledge bases. This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such as WordNet (Faruqui et al., 2015; Mrkši´c et al., 20"
N18-1103,D15-1075,0,0.0548953,"ši´c et al., 2017; Vuli´c et al., 2017b). In this paper, we introduce a novel post-processing model which specialises vector spaces for the lexical entailment (LE) relation. Word-level lexical entailment is an asymmetric semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991). It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies (Fellbaum, 1998). Automatic reasoning about LE supports tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), natural language inference (Dagan et al., 2013; Bowman et al., 2015), text generation (Biran and McKeown, 2013), and metaphor detection (Mohler et al., 2013). Our novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-R EPEL, a state-of-the-art general spe1 Distinguishing between synonymy and antonymy has a positive impact on real-world language understanding tasks such as Dialogue State Tracking (Mrkši´c et al., 2017). 1134 Proceedings of NAACL-HLT 2018, pages 1134–1145 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics The employed asymmetric distance allows one to make gr"
N18-1103,D14-1082,0,0.0240582,"ts. Simultaneously, a joint objective enforces semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular sol"
N18-1103,W09-0215,0,0.17162,"orrelation of ≈ 0.71) and SimVerb (≈ 0.70). This proves that cosine distances remain preserved during the optimization of the asymmetric objective performed by the joint LEAR model. Related Work Word Vectors and Lexical Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a significant amount of research in semantics has been invested into its automatic detection and classification. Early work relied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or"
N18-1103,N15-1184,0,0.380509,"Missing"
N18-1103,P16-1193,0,0.0601195,"elied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised LE detection models (Tuan et al., 2016; Shwartz et al., 2016; Nguyen et al., 2017; Glavaš and Ponzetto, 2017). Several LE models embed useful hierarchical relations from external resources such as WordNet into LE-focused vector spaces, with solutions coming in different flavours. The model of Yu et al. (2015) is a dynamic distance-margin model optimised for the LE detection task using hierarchical WordNet constraints. This model was extended by Tuan et al. (2016) to make use of contextual sententi"
N18-1103,P13-2078,0,0.0360327,"l Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a significant amount of research in semantics has been invested into its automatic detection and classification. Early work relied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised LE detection models (Tuan et al., 2016; Shwartz et al., 2016; Nguyen et al., 2017; Glavaš and Ponzetto, 2017). Several LE models embed useful hierarchical relations from external resources"
N18-1103,J15-4004,0,0.170848,"ilar results are achieved with others) are shown in Table 2. This table shows that, while the stand-alone ASYM - ONLY term seems more beneficial than the SYM - ONLY one, using the two terms jointly yields the strongest performance across all LE tasks. LE and Semantic Similarity We also test whether the asymmetric LE term harms the (normindependent) cosine distances used to represent semantic similarity. The LEAR model is compared to the original ATTRACT- REPEL model making use of the same set of linguistic constraints. Two true semantic similarity datasets are used for evaluation: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). There is no significant difference in performance between the two models, both of which yield similar results on SimLex (Spearman’s rank correlation of ≈ 0.71) and SimVerb (≈ 0.70). This proves that cosine distances remain preserved during the optimization of the asymmetric objective performed by the joint LEAR model. Related Work Word Vectors and Lexical Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a signific"
N18-1103,N15-1070,0,0.073161,"into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017). LEAR falls into the latter category. However, while previous post-processing methods have focused almost exclusively on specialising vector spaces to emphasise semantic similarity (i.e., to distinguish between similarity and relatedness by explicitly pulling synonyms closer and pushing antonyms further apart), this paper proposed a principled methodology for specialising vector spaces for asymmetric hierarchical relations (of which lexical entailment is an instance). Its starting point is the state-of-the"
N18-1103,D15-1242,0,0.390616,"Missing"
N18-1103,P15-2020,1,0.933714,"Missing"
N18-1103,S12-1012,0,0.102157,".70). This proves that cosine distances remain preserved during the optimization of the asymmetric objective performed by the joint LEAR model. Related Work Word Vectors and Lexical Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a significant amount of research in semantics has been invested into its automatic detection and classification. Early work relied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised L"
N18-1103,P14-2050,0,0.510167,"eness and robustness of the proposed asymmetric specialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human- or automaticallyconstructed knowledge bases. This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such a"
N18-1103,P15-1145,0,0.230738,"their expressiveness in the graded LE task. 1140 LEAR variant SYM - ONLY ASYM - ONLY FULL WBLESS BIBLESS HL - A HL - N 5 0.687 0.867 0.912 0.679 0.824 0.875 0.469 0.529 0.686 0.429 0.565 0.705 Vector Space Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017). LEAR falls into the latter category. However, while previous post-processing methods have focused almost exclusively on specialising vector spaces to emphasise semantic similarity (i.e., to distinguish between similarity and rela"
N18-1103,K16-1006,0,0.0355498,"ecialisation. In the evaluation, we show that combining the two cost terms has a synergistic effect, with both terms contributing to stronger performance across all LE tasks used for evaluation. 3 Experimental Setup Starting Distributional Vectors To test the robustness of LEAR specialisation, we experiment with a variety of well-known, publicly available English word vectors: 1) Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) trained on the Polyglot Wikipedia (Al-Rfou et al., 2013) by Levy and Goldberg (2014); 2) G LOVE Common Crawl (Pennington et al., 2014); 3) CONTEXT 2 VEC (Melamud et al., 2016), which replaces CBOW contexts with contexts based on bidirectional LSTMs (Hochreiter and Schmidhuber, 1997); and 4) FASTT EXT (Bojanowski et al., 2017), a SGNS variant which builds word vectors as the sum of their constituent character n-gram vectors.3 Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: directed 1) lexical entailment (LE) pairs; 2) synonymy pairs; and 3) antonymy pairs. Synonyms are included as symmetric ATTRACT pairs (i.e., the BA"
N18-1103,W13-0904,0,0.0985261,"sing model which specialises vector spaces for the lexical entailment (LE) relation. Word-level lexical entailment is an asymmetric semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991). It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies (Fellbaum, 1998). Automatic reasoning about LE supports tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), natural language inference (Dagan et al., 2013; Bowman et al., 2015), text generation (Biran and McKeown, 2013), and metaphor detection (Mohler et al., 2013). Our novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-R EPEL, a state-of-the-art general spe1 Distinguishing between synonymy and antonymy has a positive impact on real-world language understanding tasks such as Dialogue State Tracking (Mrkši´c et al., 2017). 1134 Proceedings of NAACL-HLT 2018, pages 1134–1145 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics The employed asymmetric distance allows one to make graded assertions about hierarchical relationships between concepts in the specialised spac"
N18-1103,P16-2074,0,0.108395,"tations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017). LEAR falls into the latter category. However, while previous post-processing methods have focused almost exclusively on specialising vector spaces to emphasise semantic similarity (i.e., to distinguish between similarity and relatedness by explicitly pulling synonyms closer and pushing antonyms further apart), this paper proposed a principled methodology for specialising vector spaces for asymmetric hierarchical relations (of which lexical entailment is an instance). Its starting point is the state-of-the-art similarity specialisation framework of"
N18-1103,N15-1100,0,0.316227,"- A relation (Rei and Briscoe, 2014; Vuli´c et al., 2017). For a similar reason, 3 All vectors are 300-dimensional except for the 600dimensional CONTEXT 2 VEC vectors; for further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. We also experimented with dependency-based SGNS vectors (Levy and Goldberg, 2014), observing similar patterns in the results. antonyms are clear REPEL constraints as they anticorrelate with the LE relation.4 Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009). In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on average) and 380,873 antonymy pairs (6.5 per word).5 As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (pangasius, animal)). We retained only noun-noun and verb-verb pairs, while the rest were dis"
N18-1103,Q16-1030,0,0.204669,"ss in the graded LE task. 1140 LEAR variant SYM - ONLY ASYM - ONLY FULL WBLESS BIBLESS HL - A HL - N 5 0.687 0.867 0.912 0.679 0.824 0.875 0.469 0.529 0.686 0.429 0.565 0.705 Vector Space Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017). LEAR falls into the latter category. However, while previous post-processing methods have focused almost exclusively on specialising vector spaces to emphasise semantic similarity (i.e., to distinguish between similarity and relatedness by explicitly"
N18-1103,P15-2070,0,0.0827191,"Missing"
N18-1103,N04-3012,0,0.191395,"x indicate that the two generality-based measures are too coarsegrained for graded LE judgements. These models were originally constructed to tackle LE directionality and detection tasks (see Section 4.1), but their performance is surpassed by LEAR on those tasks as well. The VISUAL model outperforms SLQS - SIM. However, its numbers on BLESS (0.88), WBLESS (0.75), and BIBLESS (0.57) are far from the topperforming LEAR vectors (0.96, 0.92, 0.88).11 WN - BEST denotes the best result with asymmetric similarity measures which use the WordNet structure as their starting point (Wu and Palmer, 1994; Pedersen et al., 2004). This model can be observed as a model that directly looks up the full WordNet structure to reason about graded lexical entailment. The reported results from Figure 3(c) suggest it is more effective to quantify the LE re11 We note that SLQS and VISUAL do not leverage any external knowledge from WordNet, but the VISUAL model leverages external visual information about concepts. lation strength by using WordNet as the source of constraints for specialisation models such as H YPERV EC or LEAR . WORD 2 GAUSS (Vilnis and McCallum, 2015) represents words as multivariate K-dimensional Gaussians rath"
N18-1103,P17-1163,1,0.806784,"Missing"
N18-1103,D14-1162,0,0.0985676,"he proposed asymmetric specialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human- or automaticallyconstructed knowledge bases. This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such as WordNet (Faruqui et al."
N18-1103,W14-1608,0,0.0187423,"eiter and Schmidhuber, 1997); and 4) FASTT EXT (Bojanowski et al., 2017), a SGNS variant which builds word vectors as the sum of their constituent character n-gram vectors.3 Linguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: directed 1) lexical entailment (LE) pairs; 2) synonymy pairs; and 3) antonymy pairs. Synonyms are included as symmetric ATTRACT pairs (i.e., the BA pairs) since they can be seen as defining a trivial symmetric IS - A relation (Rei and Briscoe, 2014; Vuli´c et al., 2017). For a similar reason, 3 All vectors are 300-dimensional except for the 600dimensional CONTEXT 2 VEC vectors; for further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. We also experimented with dependency-based SGNS vectors (Levy and Goldberg, 2014), observing similar patterns in the results. antonyms are clear REPEL constraints as they anticorrelate with the LE relation.4 Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are extract"
N18-1103,Q17-1022,1,0.874809,"Missing"
N18-1103,C14-1097,0,0.478754,"Missing"
N18-1103,E14-4008,0,0.740994,"The models are trained for 5 epochs with the AdaGrad algorithm (Duchi et al., 2011), with batch sizes set to k1 = k2 = k3 = 128 for faster convergence. 4 Results and Discussion We test and analyse LEAR-specialised vector spaces in two standard word-level LE tasks used in prior work: hypernymy directionality and detection (Section 4.1) and graded LE (Section 4.2). 4.1 LE Directionality and Detection The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b).7 4 In short, the question “Is X a type of X?” (synonymy) is trivially true, while the question “Is ¬X a type of X?” (antonymy) is trivially false. 5 https://github.com/tticoin/AntonymDetection 6 We also experimented with additional 30,491 LE constraints from the Paraphrase Database (PPDB) 2.0 (Pavlick et al., 2015). Adding them to the WordNet-based LE pairs makes no significant impact on the final performance. We also used synonymy and antonymy pairs from other"
N18-1103,W15-4208,0,0.194641,"Missing"
N18-1103,P16-1226,0,0.359798,"Missing"
N18-1103,C04-1146,0,0.692355,"Missing"
N18-1103,E17-1007,0,0.412422,"the AdaGrad algorithm (Duchi et al., 2011), with batch sizes set to k1 = k2 = k3 = 128 for faster convergence. 4 Results and Discussion We test and analyse LEAR-specialised vector spaces in two standard word-level LE tasks used in prior work: hypernymy directionality and detection (Section 4.1) and graded LE (Section 4.2). 4.1 LE Directionality and Detection The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b).7 4 In short, the question “Is X a type of X?” (synonymy) is trivially true, while the question “Is ¬X a type of X?” (antonymy) is trivially false. 5 https://github.com/tticoin/AntonymDetection 6 We also experimented with additional 30,491 LE constraints from the Paraphrase Database (PPDB) 2.0 (Pavlick et al., 2015). Adding them to the WordNet-based LE pairs makes no significant impact on the final performance. We also used synonymy and antonymy pairs from other sources, such as word pairs from PPDB used"
N18-1103,Q15-1025,0,0.697489,"nal hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human- or automaticallyconstructed knowledge bases. This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such as WordNet (Faruqui et al., 2015; Mrkši´c et al., 2016), the Paraphrase Database (PPDB) (Wieting et al., 2015), or BabelNet (Mrkši´c et al., 2017; Vuli´c et al., 2017a). One advantage of post-processing methods is that they treat the input vector space as a black box, making them applicable to any input space. A key property of these methods is their ability to transform the vector space by specialising it for a particular relationship between words.1 Prior work has predominantly focused on distinguishing between semantic similarity and conceptual relatedness (Faruqui et al., 2015; Mrkši´c et al., 2017; Vuli´c et al., 2017b). In this paper, we introduce a novel post-processing model which specialises"
N18-1103,P06-1101,0,0.373165,"guishing between semantic similarity and conceptual relatedness (Faruqui et al., 2015; Mrkši´c et al., 2017; Vuli´c et al., 2017b). In this paper, we introduce a novel post-processing model which specialises vector spaces for the lexical entailment (LE) relation. Word-level lexical entailment is an asymmetric semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991). It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies (Fellbaum, 1998). Automatic reasoning about LE supports tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), natural language inference (Dagan et al., 2013; Bowman et al., 2015), text generation (Biran and McKeown, 2013), and metaphor detection (Mohler et al., 2013). Our novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-R EPEL, a state-of-the-art general spe1 Distinguishing between synonymy and antonymy has a positive impact on real-world language understanding tasks such as Dialogue State Tracking (Mrkši´c et al., 2017). 1134 Proceedings of NAACL-HLT 2018, pages 1134–1145 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Ass"
N18-1103,D16-1039,0,0.107682,"ased on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al., 1141 2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised LE detection models (Tuan et al., 2016; Shwartz et al., 2016; Nguyen et al., 2017; Glavaš and Ponzetto, 2017). Several LE models embed useful hierarchical relations from external resources such as WordNet into LE-focused vector spaces, with solutions coming in different flavours. The model of Yu et al. (2015) is a dynamic distance-margin model optimised for the LE detection task using hierarchical WordNet constraints. This model was extended by Tuan et al. (2016) to make use of contextual sentential information. A major drawback of both models is their inability to make directionality judgements. Further, their performance has rec"
N18-1103,P10-1040,0,0.00922021,"tric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupervised learning and fine-tune distribu"
N18-1103,N18-1056,0,0.308864,"Missing"
N18-1103,P94-1019,0,0.245017,"he results on HyperLex indicate that the two generality-based measures are too coarsegrained for graded LE judgements. These models were originally constructed to tackle LE directionality and detection tasks (see Section 4.1), but their performance is surpassed by LEAR on those tasks as well. The VISUAL model outperforms SLQS - SIM. However, its numbers on BLESS (0.88), WBLESS (0.75), and BIBLESS (0.57) are far from the topperforming LEAR vectors (0.96, 0.92, 0.88).11 WN - BEST denotes the best result with asymmetric similarity measures which use the WordNet structure as their starting point (Wu and Palmer, 1994; Pedersen et al., 2004). This model can be observed as a model that directly looks up the full WordNet structure to reason about graded lexical entailment. The reported results from Figure 3(c) suggest it is more effective to quantify the LE re11 We note that SLQS and VISUAL do not leverage any external knowledge from WordNet, but the VISUAL model leverages external visual information about concepts. lation strength by using WordNet as the source of constraints for specialisation models such as H YPERV EC or LEAR . WORD 2 GAUSS (Vilnis and McCallum, 2015) represents words as multivariate K-di"
N18-1103,P14-2089,0,0.220179,"i´c et al. (2017), the offthe-shelf ORDER - EMB vectors were trained for the binary ungraded LE detection task: this limits their expressiveness in the graded LE task. 1140 LEAR variant SYM - ONLY ASYM - ONLY FULL WBLESS BIBLESS HL - A HL - N 5 0.687 0.867 0.912 0.679 0.824 0.875 0.469 0.529 0.686 0.429 0.565 0.705 Vector Space Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017). LEAR falls into the latter category. However, while previous post-processing methods have focused almost exc"
N18-1103,D14-1161,0,0.191056,"rivial symmetric IS - A relation (Rei and Briscoe, 2014; Vuli´c et al., 2017). For a similar reason, 3 All vectors are 300-dimensional except for the 600dimensional CONTEXT 2 VEC vectors; for further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. We also experimented with dependency-based SGNS vectors (Levy and Goldberg, 2014), observing similar patterns in the results. antonyms are clear REPEL constraints as they anticorrelate with the LE relation.4 Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009). In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on average) and 380,873 antonymy pairs (6.5 per word).5 As in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (pangasius, animal)). We retained only noun-noun and verb-verb pairs, whil"
N18-1103,D13-1141,0,0.0279315,"semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model. 1 Introduction Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017). A major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution is to go beyond stand-alone unsupe"
N18-1103,J17-4004,1,0.873344,"Missing"
N18-1103,N18-1048,1,0.857855,"Missing"
N18-1103,D17-1270,1,0.898251,"Missing"
N18-1103,P17-1006,1,0.909137,"Missing"
N18-1103,C14-1212,0,0.679788,"d for 5 epochs with the AdaGrad algorithm (Duchi et al., 2011), with batch sizes set to k1 = k2 = k3 = 128 for faster convergence. 4 Results and Discussion We test and analyse LEAR-specialised vector spaces in two standard word-level LE tasks used in prior work: hypernymy directionality and detection (Section 4.1) and graded LE (Section 4.2). 4.1 LE Directionality and Detection The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b).7 4 In short, the question “Is X a type of X?” (synonymy) is trivially true, while the question “Is ¬X a type of X?” (antonymy) is trivially false. 5 https://github.com/tticoin/AntonymDetection 6 We also experimented with additional 30,491 LE constraints from the Paraphrase Database (PPDB) 2.0 (Pavlick et al., 2015). Adding them to the WordNet-based LE pairs makes no significant impact on the final performance. We also used synonymy and antonymy pairs from other sources, such as wor"
N18-2029,P17-1042,0,0.0157153,"r discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (Santus et al., 2016). Further, it exhibits stable performance across languages. Finally, we show that, when coupled with a method for inducing a multilingual distributional space (Artetxe et al., 2017; Smith et al., 2017, inter alia), STM can predict lexico-semantic relations also for languages with no training data available from external linguistic resources. While in this work we use STM to discriminate between four prominent lexico-semantic relations, it can, at least conceptually, be trained to predict over an arbitrary set of lexico-semantic relations, provided the availability of respective training data. 2 Related Work Specializing distributional vectors. Given a pair of words, we cannot reliably determine the nature of the lexico-semantic association between them (if any), purely"
N18-2029,W16-5311,0,0.498255,"ties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors"
N18-2029,E12-1004,0,0.0566818,", 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-cla"
N18-2029,Q17-1010,0,0.600957,"ces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to n"
N18-2029,N15-1184,0,0.038855,"icate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show"
N18-2029,P14-1113,0,0.0318849,"semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the"
N18-2029,P15-2011,1,0.885342,"Missing"
N18-2029,D17-1185,1,0.897839,"Missing"
N18-2029,D15-1242,0,0.13515,"Missing"
N18-2029,J10-3003,0,0.010054,"014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and"
N18-2029,Q17-1022,1,0.863522,"Missing"
N18-2029,D14-1162,0,0.0895772,"STM) simultaneously produces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Mad"
N18-2029,C14-1097,0,0.15897,"Missing"
N18-2029,W16-5309,0,0.234642,"fiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural model for discriminating between (arguably) most prominent lexico-semantic relations – synonymy, antonymy, hypernymy, and meronymy. The STM architecture is based on the hypothesis that different specializations of input distributional vectors are needed for predicting different lexico-semantic relations. Our results show that, despite its simplicity, STM outperforms more complex models on the benchmarking CogALex-V dataset (Santus et al., 2016). Further, it exhibits stable performance across languages. Finally, we show that, when coupled with a method for inducing a multilingual distributional space (Artetxe et al., 2017; Smith et al., 2017, inter alia), STM can predict lexico-semantic relations also for languages with no training data available from external linguistic resources. While in this work we use STM to discriminate between four prominent lexico-semantic relations, it can, at least conceptually, be trained to predict over an arbitrary set of lexico-semantic relations, provided the availability of respective training data."
N18-2029,W16-5310,0,0.32063,"relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni et al., 2012; Roller et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017) or to discriminate between multiple relations (Attia et al., 2016; Shwartz and Dagan, 2016), using labeled word pairs from external resources like WordNet. The LexNet model (Shwartz and Dagan, 2016) combines distributional vectors with recurrent encodings of syntactic paths taken from word co-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors as input. Our specializati"
N18-2029,P16-1226,0,0.084535,"Missing"
N18-2029,H05-1047,0,0.0614312,"ining data. 1 Introduction Distributional vector spaces (i.e., word embeddings) (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017) are ubiquitous in modern natural language processing (NLP). While such vector spaces capture general semantic relatedness, their well-known limitation is the inability to indicate the exact nature of the semantic relation that holds between words. Yet, the ability to recognize the exact semantic relation between words is crucial for many NLP applications: taxonomy induction (Fu et al., 2014; Ristoski et al., 2017), natural language inference (Tatu and Moldovan, 2005; Chen et al., 2017), text simplification (Glavaˇs and ˇ Stajner, 2015), and paraphrase generation (Madnani and Dorr, 2010), to name a few. This is why numerous methods have been proposed that either (1) specialize distributional vectors to better reflect a particular relation (most commonly synonymy) (Faruqui et al., 2015; Kiela et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017) or (2) train supervised relation classifiers using lexico-semantic relations (i.e., labeled word pairs) Contributions. We present the Specialization Tensor Model (STM), a simple and effective feedforward neural"
N18-2029,N18-1103,1,0.896903,"Missing"
N18-2029,P17-1006,1,0.867947,"-occurrences in text corpora. While adding the syntactic information boosts performance, it limits the model’s portability to other languages. Attia et al. (2016) train a convolutional model in a multi-task setting, coupling multi-class relation classification with binary classification of word relatedness. Unlike LexNet, this model requires only distributional vectors as input. Our specialization tensor model also requires only distributional vectors as input, but compared to the model of Attia et al. (2016), it has a simpler and more intuitive feed-forward architecture. Glavaˇs and Ponzetto (2017) recently showed that asymmetric specialization of distributional vectors helps to detect asymmetric relations (hypernymy, meronymy). Following these findings, we hypothesize that detection of different relations requires different specializations of distributional vectors, so we design STM accordingly. 3 Specialization Tensor Model The high-level architecture of the Specialization Tensor Model is depicted in Figure 1. The input to the model is a pair of unspecialized distributional word vectors (x1 , x2 ). Both input vectors are first transformed in K different ways with functions (1) (K) fS"
N18-2029,Q15-1025,0,0.0390057,"na, June 1 - 6, 2018. 2018 Association for Computational Linguistics ... fS(2) fP(2) ... fP(1) fclass ... ... ... ... fS(1) fS(K) fP(K) Figure 1: Architecture of the Specialization Tensor Model (STM). ticular relations use external linguistic constraints (e.g., from WordNet) to either (1) modify the original objective of general embedding algorithms and directly train relation-specific embeddings from corpora (Yu and Dredze, 2014; Kiela et al., 2015) or (2) post-process the pre-trained distributional space by moving closer together (or further apart) words that stand in a particular relation (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space gets post-specialized for one particular relation. Classifying lexico-semantic relations. Supervised relation classifiers learn to either identify one particular relation of interest (Baroni"
N18-2029,P14-2089,0,0.0550538,"ifferent types of semantic associations between words. This is why methods for specializing word embeddings for par181 Proceedings of NAACL-HLT 2018, pages 181–187 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ... fS(2) fP(2) ... fP(1) fclass ... ... ... ... fS(1) fS(K) fP(K) Figure 1: Architecture of the Specialization Tensor Model (STM). ticular relations use external linguistic constraints (e.g., from WordNet) to either (1) modify the original objective of general embedding algorithms and directly train relation-specific embeddings from corpora (Yu and Dredze, 2014; Kiela et al., 2015) or (2) post-process the pre-trained distributional space by moving closer together (or further apart) words that stand in a particular relation (Wieting et al., 2015; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018). While these methods specialize the distributional space to better reflect properties of a particular relation, e.g., synonymy (Wieting et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Vendrov et al., 2016; Vuli´c and Mrkˇsi´c, 2018), they are not able to discriminate between multiple lexico-semantic relations at the same time, i.e., the embedding space get"
N18-6006,W13-4073,0,0.079085,"Missing"
N18-6006,W14-4340,0,0.0268654,"Missing"
N18-6006,D16-1127,0,0.1527,"Missing"
N18-6006,D17-1230,0,0.136501,"Missing"
N18-6006,W16-3603,0,0.0544759,"Missing"
N18-6006,D16-1230,0,0.0909308,"Missing"
N18-6006,P15-2130,1,0.902542,"Missing"
N18-6006,N16-1018,1,0.866526,"Missing"
N18-6006,P17-1163,1,0.852615,"Missing"
N18-6006,Q17-1022,1,0.867831,"Missing"
N18-6006,E17-1029,0,0.0250971,"Missing"
N18-6006,J11-1006,0,0.0500394,"Missing"
N18-6006,W15-4655,1,0.826206,"Missing"
N18-6006,P16-1230,1,0.871219,"Missing"
N18-6006,W17-5518,1,0.838907,"Missing"
N18-6006,P17-4013,1,0.863883,"Missing"
N18-6006,E17-2033,0,0.0339381,"Missing"
N18-6006,P17-1006,1,0.831604,"Missing"
N18-6006,W15-4654,1,0.904678,"Missing"
N18-6006,W15-4639,1,0.895982,"Missing"
N18-6006,D15-1199,1,0.896104,"Missing"
N18-6006,N16-1015,1,0.906839,"Missing"
N18-6006,E17-1042,1,0.871054,"Missing"
N18-6006,W13-4068,0,0.0606163,"Missing"
N19-1097,D17-1010,0,0.108153,"d word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which leads to several fundamental limitations. The standard approaches ignore the internal structure of words, that is, the syntactic or semantic composition from subwords or morphemes to words, and are incapable of parameter sharing at the level of subword units. Assigning only a single vector to each word causes the data sparsity problem, especially in resource-poor settings where huge amounts"
N19-1097,C14-1015,0,0.020782,"ion δ(•) dishonestly Figure 1: Illustration of the general framework for learning subword-informed word representations, with the focus on two crucial components: 1) segmentation of words and 2) subword embedding composition. By varying the two components, and optionally including or excluding position embeddings from the computations, we obtain a wide spectrum of different subwordinformed configurations used in the study (see §2). Our word-level representation model in this work is skipgram (on the top layer of the figure), but it can be replaced by any other distributional word-level model. Qiu et al., 2014; Cotterell and Sch¨utze, 2015; Wieting et al., 2016; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Pinter et al., 2017; Cotterell and Sch¨utze, 2018). First, the models differ in the chosen method for segmenting words into subwords. The methods range from fully supervised approaches (Cotterell and Sch¨utze, 2015) to e.g. unsupervised approaches based on BPE (Heinzerling and Strube, 2018). Second, another crucial aspect is the subword composition function used to obtain word embeddings from the embeddings of each word’s constituent subword units. Despite a steadily increasing interest in"
N19-1097,W13-3512,0,0.493012,"e words (Gerz et al., 2018). Although potentially useful information on word relationships is hidden in their internal subwordlevel structure,1 subword-agnostic word representation models do not take these structure features into account and are effectively unable to represent rare words accurately, or unseen words at all. Therefore, there has been a surge of interest in subword-informed word representation architectures aiming to address these gaps. A large number of architectures has been proposed in related research, and they can be clustered over the two main axes (Lazaridou et al., 2013; Luong et al., 2013; 1 For example, nouns in Finnish have 15 cases and 3 plural forms; Spanish verbs may contain over 40 inflected forms, sharing the lemma and taking up standard suffixes. 912 Proceedings of NAACL-HLT 2019, pages 912–932 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Skip-gram Skip-gram word embedding context embedding They Composition function op f(•) make op op honest ly position embedding subword embedding dis Segmentation δ(•) dishonestly Figure 1: Illustration of the general framework for learning subword-informed word representations, with t"
N19-1097,I17-1007,0,0.0208649,"arsing task. The two best configurations are selected according to LAS. Dependency Parsing Next, we use the syntactic dependency parsing task to analyze the importance of subword information for syntactically-driven downstream applications. For all test languages, we rely on the standard Universal Dependencies treebanks (UD v2.2; Nivre et al. (2016)). We use subword-informed word embeddings from different configurations to initialize the deep biaffine parser of Dozat and Manning (2017) which has shown competitive performance in shared tasks (Dozat et al., 2017) and among other parsing models (Ma and Hovy, 2017; Shi et al., 2017; Ma et al., Fine-Grained Entity Typing The task is to map entities, which could comprise more than one entity token, to predefined entity types (Yaghoobzadeh and Sch¨utze, 2015). It is a suitable semi-semantic task to test our subword models, as the subwords of entities usually carry some semantic information from which the entity types can be inferred. For example, Lincolnshire will belong to /location/county as -shire is a suffix that strongly indicates a location. We rely on an entity typing dataset of Heinzerling and Strube (2018) built for over 250 languages by obtainin"
N19-1097,P18-1130,0,0.0252072,"Missing"
N19-1097,P16-1162,0,0.120272,"ach word, apart from generating Sw , it also outputs the corresponding morphotactic tags Tw .5 In §2.3 we discuss how to incorporate information from Tw into subword representations. Morfessor Morfessor (Smit et al., 2014) denotes a family of generative probabilistic models for unsupervised morphological segmentation used, among other applications, to learn morphologicallyaware word embeddings (Luong et al., 2013). BPE Byte Pair Encoding (BPE; Gage (1994)) is a simple data compression algorithm. It has become a de facto standard for providing subword information in neural machine translation (Sennrich et al., 2016). The input word is initially split into a sequence of characters, with each unique character denoted as a byte. BPE then iteratively replaces the most common pair of consecutive bytes with a new byte that does not occur within the data, and the number of iterations can be set in advance to control the granularity of the byte combinations. An example output for all three methods is shown in Table 1. Note that a standard practice in subword-informed models is to also insert the entire word token into Sw (Bojanowski et al., 2017).6 This is, however, again an optional step and we evaluate configu"
N19-1097,L18-1008,0,0.0229894,"ertion if |Sw |&gt; 1. For CHIPMUNK, a generic tag word is added to the sequence Tw . CHIPMUNK configurations without the use of Tw to analyze its contribution.7 After generating Sw , an optional step is to have a learnable position embedding sequence Pw further operate on Sw to encode the order information. Similar to Ws , the definition of the position embedding matrix Wp also varies: for Morfessor and BPE, we use the absolute positions of subwords in the sequence Sw , whereas for CHIPMUNK morphotactic tags are encoded directly as positions. Finally, following prior work (Gehring et al., 2017; Mikolov et al., 2018), we use addition and element-wise multiplication between each subword vector s from Sw and the corresponding position vector p from Pw to compute each entry r for the final sequence of subword vectors Rw : r=s+p 2.4 or r = s p. (2) Composition Functions A composition function fΘ is then applied to the sequence of subword embeddings Rw to compute the final word embedding w. We investigate three composition functions: 1) addition, 2) single-head and 3) multi-head self-attention (Vaswani et al., 2017; Lin et al., 2017).8 Addition is used in the original fastText model of Bojanowski et al. (2017)"
N19-1097,Q17-1022,1,0.87147,"Missing"
N19-1097,L16-1262,0,0.0896054,"Missing"
N19-1097,D14-1162,0,0.0980802,"ations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which leads to several fundamental limitations. The standard approaches ignore the internal structure of words, that is, the syntactic or semantic composition from subwords or morphemes to words, and are incapable of parameter sharing at the level of subword units"
N19-1097,N18-1202,0,0.0318293,"ndency parsing, fine-grained entity typing) for 5 languages representing 3 language types. Our main results clearly indicate that there is no “one-sizefits-all” configuration, as performance is both language- and task-dependent. We also show that configurations based on unsupervised segmentation (e.g., BPE, Morfessor) are sometimes comparable to or even outperform the ones based on supervised word segmentation. 1 Introduction Word representations are central to a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Jia and Liang, 2016; Ammar et al., 2016; Goldberg, 2017; Peters et al., 2018; Kudo, 2018, inter alia). Standard word representation models are based on the distributional hypothesis (Harris, 1954) and induce representations from large unlabeled corpora using word co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). However, as pointed out by recent work (Bojanowski et al., 2017; Vania and Lopez, 2017; Pinter et al., 2017; Chaudhary et al., 2018; Zhao et al., 2018), mapping a finite set of word types into corresponding word representations limits the capacity of these models to learn beyond distributional information, which"
N19-1097,Q15-1016,0,\N,Missing
N19-1097,J15-4004,1,\N,Missing
N19-1097,P13-1149,0,\N,Missing
N19-1097,E14-2006,0,\N,Missing
N19-1097,D14-1082,0,\N,Missing
N19-1097,K15-1017,0,\N,Missing
N19-1097,Q16-1023,0,\N,Missing
N19-1097,D16-1157,0,\N,Missing
N19-1097,Q17-1010,0,\N,Missing
N19-1097,Q18-1003,0,\N,Missing
N19-1097,E17-2067,0,\N,Missing
N19-1097,K17-3002,0,\N,Missing
N19-1097,W18-1205,0,\N,Missing
N19-1097,C18-1323,0,\N,Missing
N19-1097,D18-1029,1,\N,Missing
N19-1097,D18-1059,0,\N,Missing
N19-1097,D15-1083,0,\N,Missing
N19-1097,N15-1140,0,\N,Missing
N19-1097,W17-0228,0,\N,Missing
N19-1188,P79-1000,0,0.311237,"Missing"
N19-1188,D18-1399,0,0.0678852,"inducing cross-lingual word embeddings for two or more languages in the same vector space. Embeddings of translations and words with similar meaning are geometrically close in the shared cross-lingual vector space. This property makes them effective features for cross-lingual NLP tasks such as cross-lingual document classification (Klementiev et al., 2012), cross-lingual information retrieval (Vuli´c and Moens, 2015), bilingual lexicon induction (Mikolov et al., 2013b; Gouws et al., 2015; Heyman et al., 2017), and (unsupervised) machine translation (Artetxe et al., 2017b; Lample et al., 2018; Artetxe et al., 2018c). Most prior work has focused on methods for constructing bilingual word embeddings (BWEs), yielding word representations for exactly two languages. For problems such as multilingual document classification, however, it is highly-desirable to represent words in a multilingual space. A favourable property is that it enables fitting a single classifier on the union of training datasets in many languages, which results in 1) knowledge transfer across languages that may lead to better classification performance, and 2) a setup that is easier to maintain as it is no longer required to train many"
N19-1188,E17-1084,0,0.442389,"v et al., 2013a) on a union of monolingual corpora where they replace words with their cluster id such that words in the same cluster get the same representation. MultiCCA is the multilingual extension of the method of Faruqui and Dyer (2014): Using canonical correlation analysis (CCA) and dictionaries with English as the target language, monolingual embeddings are projected to the English vector space. MultiSkip is a straightforward extension of the BiSkip method (Luong et al., 2015) which generalizes the monolingual SG objective to account for word alignments in parallel corpora. Similarly, Duong et al. (2017), extend CBOW to multiple languages. All these methods learn multilingual embeddings using bilingual dictionaries of parallel corpora: This limits their applicability for many languages. More recently, Conneau et al. (2018); Artetxe et al. (2018a) showed that BWEs can be effectively induced without any cross-lingual supervision. The approaches are based on the assumption that monolingual embedding spaces are approximately isomorphic.1 Improving on earlier attempts (Cao et al., 2016; Zhang et al., 2017), Conneau et al. (2018) propose a two-step framework to map two monolingual spaces to the sha"
N19-1188,J82-2005,0,0.63485,"Missing"
N19-1188,C16-1171,0,0.0205537,"which generalizes the monolingual SG objective to account for word alignments in parallel corpora. Similarly, Duong et al. (2017), extend CBOW to multiple languages. All these methods learn multilingual embeddings using bilingual dictionaries of parallel corpora: This limits their applicability for many languages. More recently, Conneau et al. (2018); Artetxe et al. (2018a) showed that BWEs can be effectively induced without any cross-lingual supervision. The approaches are based on the assumption that monolingual embedding spaces are approximately isomorphic.1 Improving on earlier attempts (Cao et al., 2016; Zhang et al., 2017), Conneau et al. (2018) propose a two-step framework to map two monolingual spaces to the shared space. First, they use an adversarial objective to get an initial bilingual space in which the discriminator can no longer distinguish to which language a given word embedding belongs. They then fine-tune the initial solution. An important limitation is that the adversarial objective is prone to converge to degenerate solutions. Furthermore, Søgaard et al. (2018) empirically prove that the method typically fails for distant language pairs such as English-Finnish. In parallel, A"
N19-1188,D18-1024,0,0.473412,"ra (Gouws et al., 2015), or subjectaligned document pairs (Vuli´c and Moens, 2016). In such paradigms, modeling dependencies between all languages is impractical as it requires supervision for all language pair combinations. Recent research has shown that BWEs can also be learned without cross-lingual supervision and can even outperform supervised BWE variants 1890 Proceedings of NAACL-HLT 2019, pages 1890–1902 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics on bilingual lexicon induction benchmarks (Conneau et al., 2018; Artetxe et al., 2018a). Chen and Cardie (2018) took a first step towards learning multilingual spaces without supervision while incorporating dependencies between all languages but their approach extends the work of Conneau et al. (2018), which has known limitations concerning optimization stability with distant language pairs (Søgaard et al., 2018). In this work, we investigate robust methods to induce MWEs without any cross-lingual supervision. The robustness of our approach is illustrated in good performance for distant languages such as Finnish and Bulgarian. This paper makes the following contributions. First, based on a reformulatio"
N19-1188,P15-1033,0,0.0215414,"ssifier is the average perceptron used by Klementiev et al. (2012). MLPARSING is a multilingual dependency parsing dataset sampled from the Universal Dependencies 1.1 corpus (Agi´c et al., 2015)12 . It contains 12 languages: English, German, French, Spanish, Italian, Bulgarian, Czech, Danish, Swedish, Greek, Finnish, and Hungarian. The respective training and test set contain 6,748 and 1,200 sentences. The test set contains 100 sentences for each language, while for the training set the number of sentences for a language ranges between 98 and 6,694. The parser used is the stack-LSTM parser by Dyer et al. (2015). The parser is not allowed to use any partof-speech and morphology features, and keeps the input word embeddings fixed to isolate the effect of the evaluated embeddings on the parsing performance (Ammar et al., 2016). The reported scores are UAS scores averaged across languages. For comparison with related work, we train 512dimensional monolingual embeddings on the text collections used by Ammar et al. (2016) and Duong et al. (2017). The monolingual embeddings are again trained using fastText. Training Setup. In all experiments, we set the following hyper-parameters to values that were used i"
N19-1188,E14-1049,0,0.0565925,"o not limit our evaluation to the intrinsic BLI task only. Consequently, we investigate if embedding reweighting, a recently proposed best practice for BWEs, is useful for extrinsic tasks such as document classification and dependency parsing in multilingual settings. 2 Related Work Cross-lingual word embeddings have received a lot of attention in recent years. Most methods construct a space shared between two languages using cross-lingual supervision in the form of bilingual lexicons (Mikolov et al., 2013a; Artetxe et al., 2016; Smith et al., 2017), parallel corpora (Klementiev et al., 2012; Faruqui and Dyer, 2014; Gouws et al., 2015; Luong et al., 2015) or subject-aligned document pairs (Vuli´c and Moens, 2016). See Ruder et al. (2018) for a full overview of BWE model typology in relation to the required supervision. To enable knowledge transfer across an arbitrary number of languages, multilingual methods have been introduced. Huang et al. (2015), propose decomposing a matrix with multilingual cooccurrence counts weighted by probabilistic dictionaries. Ammar et al. (2016) compare this method to three other MWE models: MultiCluster, MultiCCA, and MultiSkip. MultiCluster uses bilingual dictionaries to"
N19-1188,E17-1102,1,0.725248,"Missing"
N19-1417,E17-1088,0,0.0358111,"Missing"
N19-1417,P16-1186,0,0.0524637,"Missing"
N19-1417,N18-2085,0,0.0189208,"annot reach the performance peaks of computationally much more expensive neural models, but a 2× decrease in perplexity comes at the cost of 15, 000× longer training time. Our work reveals that recent ?-gram LMs should be used as strong baselines, especially in resource-lean LM data and for morphologically rich languages. Additionally, ?-gram LMs offer a stringent way of dealing with Out-of-Vocabulary (OOVs) and rare words in a full vocabulary setting without relying on any pruning (Heafield, 2013). However, in neural LMs this remains an open question (Kawakami et al., 2017; Kim et al., 2016; Cotterell et al., 2018), while a common practice is pruning the training corpus and imposing closed vocabulary assumption (Mikolov et al., 2010) where rare words at training and unseen words at test are treated as an UNK token. We provide the mathematical underpinnings of ?-gram models and highlight how this popular treatment works in favor of neural LMs (in comparative studies), and enforces ?-gram LMs to perform much worse than their full potential. 2 ?-gram Language Models: Smoothing We now provide an overview of established smoothing techniques for ?-gram LMs and their recent extensions. Smoothing is typically a"
N19-1417,Q18-1032,1,0.913614,"d, a comparison between neural models and more recent developments in ?-gram models is neglected. In this paper, we examine the recent progress in ?-gram literature, running experiments on 50 languages covering all morphological language families. Experimental results illustrate that a simple extension of Modified KneserNey outperforms an LSTM language model on 42 languages while a word-level Bayesian ?gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018a) on 8 languages. Further experiments on larger Europarl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper ?-gram models: ?-gram training is up to 15, 000× quicker. Our experiments illustrate that standalone ?-gram models lend themselves as natural choices for resource-lean or morphologically rich languages, while the recent progress has significantly improved their accuracy. 1 Introduction Statistical language models (LMs) are the pivot for several natural language processing tasks where a model trained on a text corpus is require"
N19-1417,D18-1029,1,0.915172,"d, a comparison between neural models and more recent developments in ?-gram models is neglected. In this paper, we examine the recent progress in ?-gram literature, running experiments on 50 languages covering all morphological language families. Experimental results illustrate that a simple extension of Modified KneserNey outperforms an LSTM language model on 42 languages while a word-level Bayesian ?gram LM (Shareghi et al., 2017) outperforms the character-aware neural model (Kim et al., 2016) on average across all languages, and its extension which explicitly injects linguistic knowledge (Gerz et al., 2018a) on 8 languages. Further experiments on larger Europarl datasets for 3 languages indicate that neural architectures are able to outperform computationally much cheaper ?-gram models: ?-gram training is up to 15, 000× quicker. Our experiments illustrate that standalone ?-gram models lend themselves as natural choices for resource-lean or morphologically rich languages, while the recent progress has significantly improved their accuracy. 1 Introduction Statistical language models (LMs) are the pivot for several natural language processing tasks where a model trained on a text corpus is require"
N19-1417,P17-1137,0,0.028832,"arl datasets we find that ?-gram models cannot reach the performance peaks of computationally much more expensive neural models, but a 2× decrease in perplexity comes at the cost of 15, 000× longer training time. Our work reveals that recent ?-gram LMs should be used as strong baselines, especially in resource-lean LM data and for morphologically rich languages. Additionally, ?-gram LMs offer a stringent way of dealing with Out-of-Vocabulary (OOVs) and rare words in a full vocabulary setting without relying on any pruning (Heafield, 2013). However, in neural LMs this remains an open question (Kawakami et al., 2017; Kim et al., 2016; Cotterell et al., 2018), while a common practice is pruning the training corpus and imposing closed vocabulary assumption (Mikolov et al., 2010) where rare words at training and unseen words at test are treated as an UNK token. We provide the mathematical underpinnings of ?-gram models and highlight how this popular treatment works in favor of neural LMs (in comparative studies), and enforces ?-gram LMs to perform much worse than their full potential. 2 ?-gram Language Models: Smoothing We now provide an overview of established smoothing techniques for ?-gram LMs and their"
N19-1417,2005.mtsummit-papers.11,0,0.281847,"Missing"
N19-1417,N18-1192,0,0.0432216,"Missing"
N19-1417,J93-2004,0,0.0648147,"Missing"
N19-1417,D16-1094,1,0.945485,"lity to a given sequence ?1 ?2 ...?? (denoted by ?? ). This probability indicates how likely 1 ? is for ?1 to belong to the corpus and is decomposed into conditional probabilities of words given their ∏ ?−1 preceding contexts as ? (?? )= ? ?=1 ? (?? |?1 ). 1 In ?-gram LMs the unbounded conditional probabilities ? (?? |??−1 ) are approximated by imposing 1 a finite-order Markov assumption, ? (?? |??−1 )≈ 1 ?−1 ? (?? |??−?+1 ). Several smoothing techniques address the statistical sparsity issue for computing the conditional probabilities (Kneser and Ney, 1995; Chen and Goodman, 1999; Teh, 2006; Shareghi et al., 2016a), while others avoided the above approximation with unbounded hierarchical nonparametric Bayesian frameworks (Wood et al., 2011; Shareghi et al., 2017). Alternatively, neural LMs compute ? (?? |??−1 ) 1 via recurrent neural units which, in theory, are capable of encoding an unbounded context ??−1 . In 1 recent years, neural LMs have become the prominent class of language modeling and have established state-of-the-art results on almost all sufficiently large benchmarks (Melis et al., 2018; Yang et al., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational short"
N19-1417,Q16-1034,1,0.886349,"lity to a given sequence ?1 ?2 ...?? (denoted by ?? ). This probability indicates how likely 1 ? is for ?1 to belong to the corpus and is decomposed into conditional probabilities of words given their ∏ ?−1 preceding contexts as ? (?? )= ? ?=1 ? (?? |?1 ). 1 In ?-gram LMs the unbounded conditional probabilities ? (?? |??−1 ) are approximated by imposing 1 a finite-order Markov assumption, ? (?? |??−1 )≈ 1 ?−1 ? (?? |??−?+1 ). Several smoothing techniques address the statistical sparsity issue for computing the conditional probabilities (Kneser and Ney, 1995; Chen and Goodman, 1999; Teh, 2006; Shareghi et al., 2016a), while others avoided the above approximation with unbounded hierarchical nonparametric Bayesian frameworks (Wood et al., 2011; Shareghi et al., 2017). Alternatively, neural LMs compute ? (?? |??−1 ) 1 via recurrent neural units which, in theory, are capable of encoding an unbounded context ??−1 . In 1 recent years, neural LMs have become the prominent class of language modeling and have established state-of-the-art results on almost all sufficiently large benchmarks (Melis et al., 2018; Yang et al., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational short"
N19-1417,Q18-1036,0,0.0215156,"l., 2018). While outperforming ?-grams in terms of predictive accuracy, the computational shortcomings of neural LMs are well-documented: Training neural LMs is computationally expensive to the point that running experiments on large data (≥ a few GiBs) is beyond the reach of academic research to this date (Chen et al., 2016; Patwary et al., 2018; Puri et al., 2018). 1 Similarly, querying is slower for neural LMs due to the required matrix-based operations, whereas most of the widely used ?-gram LM toolkits rely on a few hash lookups and much cheaper scalar-based operations (Liu et al., 2018; Tang and Lin, 2018). Nonetheless, it has been shown that the best predictive performance is still achieved by combining the two models via a basic interpolation or a mixture model (Jozefowicz et al., 2016; Neubig and Dyer, 2016): this indicates that the progress in ?-gram LM should eventually be reflected in improving the 1 For instance, ?-gram LMs could be trained on 32GiB of data on a single CPU with ∼32GiB of RAM in half a day (Shareghi et al., 2016b). A ballpark estimate for neural LMs, based on Puri et al. (2018), requires 26 Tesla V100 16GB GPUs to finish within the same amount of time while its financial"
N19-1417,D16-1124,0,0.0148028,"unning experiments on large data (≥ a few GiBs) is beyond the reach of academic research to this date (Chen et al., 2016; Patwary et al., 2018; Puri et al., 2018). 1 Similarly, querying is slower for neural LMs due to the required matrix-based operations, whereas most of the widely used ?-gram LM toolkits rely on a few hash lookups and much cheaper scalar-based operations (Liu et al., 2018; Tang and Lin, 2018). Nonetheless, it has been shown that the best predictive performance is still achieved by combining the two models via a basic interpolation or a mixture model (Jozefowicz et al., 2016; Neubig and Dyer, 2016): this indicates that the progress in ?-gram LM should eventually be reflected in improving the 1 For instance, ?-gram LMs could be trained on 32GiB of data on a single CPU with ∼32GiB of RAM in half a day (Shareghi et al., 2016b). A ballpark estimate for neural LMs, based on Puri et al. (2018), requires 26 Tesla V100 16GB GPUs to finish within the same amount of time while its financial cost is at least 100× higher. 4113 Proceedings of NAACL-HLT 2019, pages 4113–4118 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics state-of-the-art performance. I"
P11-2084,P04-1067,0,0.922188,"Missing"
P11-2084,W02-0902,0,0.411123,"rk and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have design"
P11-2084,D09-1092,0,0.375897,"Missing"
P11-2084,P95-1050,0,0.257456,"BiLDA model used in the experiments, presents all main ideas behind our work and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic mod"
P15-2020,W11-0112,0,0.0323545,"for lexical entailment detection by examining a concept’s generality. We introduce three unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Gan"
P15-2020,P05-1014,0,0.905176,"Missing"
P15-2020,P13-2078,0,0.234332,"te et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypern"
P15-2020,W98-0718,0,0.195269,"Missing"
P15-2020,D14-1005,1,0.765841,"ailment Detection Douwe Kiela Computer Laboratory University of Cambridge douwe.kiela@cl.cam.ac.uk Laura Rimell Computer Laboratory University of Cambridge laura.rimell@cl.cam.ac.uk Ivan Vuli´c Department of Computer Science KU Leuven ivan.vulic@cs.kuleuven.be Stephen Clark Computer Laboratory University of Cambridge stephen.clark@cl.cam.ac.uk Abstract range of tasks, including modelling semantic similarity and conceptual relatedness (Silberer and Lapata, 2014). In fact, under some conditions uni-modal visual representations outperform traditional linguistic representations on semantic tasks (Kiela and Bottou, 2014). We hypothesize that visual representations can be particularly useful for lexical entailment detection. Deselaers and Ferrari (2011) have shown that sets of images corresponding to terms at higher levels in the WordNet hierarchy have greater visual variability than those at lower levels. We exploit this tendency using sets of images returned by Google’s image search. The intuition is that the set of images returned for animal will consist of pictures of different kinds of animals, the set of images for bird will consist of pictures of different birds, while the set for owl will mostly consis"
P15-2020,W11-2501,0,0.154227,"ging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image"
P15-2020,P14-2135,1,0.908604,"in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using Caffe (Jia et al., 201"
P15-2020,R11-1055,0,0.077426,"s known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For the detection experiment, we evaluate on the BLESS -based dataset of Weeds et al. (2014), which consists of 1168 word pairs and which we call WBLESS . In this dataset, the positive examples are hyponym-hypernym pairs. The negative examples 3.1 Image representations Following previous work in multi-modal semantics (Bergsma and Goebel, 2011; Kiela et al., 2014), we obtain images from Google Images1 for the words in the evaluation datasets. It has been shown that images from Google yield higherquality representations than comparable resources such as Flickr and are competitive with “hand prepared datasets” (Bergsma and Goebel, 2011; Fergus et al., 2005). 1 www.google.com/imghp. Images were retrieved on 10 April, 2015 from Cambridge in the United Kingdom. 120 For each image, we extract the pre-softmax layer from a forward pass in a convolutional neural network (CNN) that has been trained on the ImageNet classification task using C"
P15-2020,I13-1095,0,0.207838,"e unsupervised methods for determining a concept’s generality, based on its related images, and obtain state-ofthe-art performance on two standard semantic evaluation datasets. We also introduce a novel task that combines hypernym detection and directionality, significantly outperforming a competitive frequencybased baseline. 1 Introduction Automatic detection of lexical entailment is useful for a number of NLP tasks including search query expansion (Shekarpour et al., 2013), recognising textual entailment (Garrette et al., 2011), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently"
P15-2020,H05-1079,0,0.0594447,"Missing"
P15-2020,S12-1012,0,0.539959,"Missing"
P15-2020,W09-0215,0,0.65594,"Missing"
P15-2020,N15-1098,0,0.0260618,"Missing"
P15-2020,E14-1054,1,0.310309,"Missing"
P15-2020,E14-4008,0,0.567911,"ion (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Given two semantically related words, a key aspect of detecting lexical entailment, or the hyponym-hypernym relation, is the generality of the hypernym compared to the hyponym. For example, bird is more general than eagle, having a broader intension and a larger extension. This property has led to the introduction of lexical entailment measures that compare the entropy of distributional word representations, under the assumption that a more general term has a higher-entropy distribution (Herbelot and Ganesalingam, 2013; Santus et al., 2014). A strand of distributional semantics has recently emerged that exploits the fact that meaning is often grounded in the perceptual system, known as multi-modal distributional semantics (Bruni et al., 2014). Such models enhance purely linguistic models with extra-linguistic perceptual information, and outperform language-only models on a 2 Related Work In the linguistic modality, the most closely related work is by Herbelot and Ganesalingam (2013) and Santus et al. (2014), who use unsupervised distributional generality measures to identify the hypernym in a hyponym-hypernym pair. Herbelot and"
P15-2020,P14-1068,0,0.140668,"Missing"
P15-2020,D11-1063,0,0.0217261,"Missing"
P15-2020,C04-1146,0,0.81603,"Missing"
P15-2020,C14-1212,0,0.405241,"relations, but does not require detection of directionality, since reversed pairs are grouped with the other negatives. For the combined experiment, we assign reversed hyponym-hypernym pairs a value of -1 instead of 0. We call this more challenging dataset BIBLESS . Examples of pairs in the respective datasets can be found in Table 1. Approach We use two standard evaluations for lexical entailment: hypernym directionality, where the task is to predict which of two words is the hypernym; and hypernym detection, where the task is to predict whether two words are in a hypernym-hyponym relation (Weeds et al., 2014; Santus et al., 2014). We also introduce a third, more challenging, evaluation that combines detection and directionality. For the directionality experiment, we evaluate on the hypernym subset of the well-known BLESS dataset (Baroni and Lenci, 2011), which consists of 1337 hyponym-hypernym pairs. In this case, it is known that the words are in an entailment relation and the task is to predict the directionality of the relation. BLESS data is always presented with the hyponym first, so we report how often our measures predict that the second term in the pair is more general than the first. For"
P15-2020,J09-3004,0,0.0525782,"Missing"
P15-2020,W13-0904,0,\N,Missing
P15-2118,W15-1521,0,0.49074,"Missing"
P15-2118,P14-1023,0,0.0408388,"., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has oc719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 719–725, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from document-aligned comparable data. Source language words and documents are drawn as gray boxes, while target langu"
P15-2118,D09-1092,0,0.00826124,"om shuffling procedure and show that the model is fairly robust to different 720 art BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos. Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics (Vuli´c et al., 2011). (2) Assoc-BLI - A BLI model that represents words as vectors of association norms (Roller and Schulte im Walde, 2013) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and"
P15-2118,P04-1067,0,0.0241651,"Missing"
P15-2118,D14-1162,0,0.0944734,"based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al.,"
P15-2118,P11-1133,0,0.0504185,"use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for"
P15-2118,P08-1088,0,0.0675096,"Setup Training Data We use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10"
P15-2118,D13-1115,0,0.0292935,"Missing"
P15-2118,P14-1006,0,0.0376606,"Missing"
P15-2118,W14-1503,0,0.0209037,"opic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflecte"
P15-2118,C12-1089,0,0.155244,"Missing"
P15-2118,D12-1003,0,0.0342663,"larity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wiS from ground truth translation pairs for which the top ranked word cross-l"
P15-2118,Q15-1016,0,0.0146499,"as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has oc719 Proceedings of the 53rd Annual Meeting of the Association for Compu"
P15-2118,N13-1011,1,0.774482,"Missing"
P15-2118,D13-1168,1,0.560609,"Missing"
P15-2118,P11-2084,1,0.607412,"Missing"
P15-2118,D13-1141,0,0.312134,"Missing"
P16-1024,W13-3520,0,0.0115101,"ts). We also compare with a benchmarking Type 1 model from sentence-aligned parallel data called BiCVM (Hermann and Blunsom, 2014b). Finally, a SGNS-based BWE model with the BNC+GT seed lexicon is taken as a baseline Type 4 model (Mikolov et al., 2013a).7 Training Data and Setup We use standard training data and suggested settings to obtain BWEs for all models involved in comparison. We retain the 100K most frequent words in each language for all models. To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Polyglot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025. For BilBOWA, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization is provided by the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We use SGD with a global rate of 0.15.8 The window size is varied from 2 to 16 in steps of 2, and the best scoring model is always reported in all comparisons. BWESG was trained on the cleaned and tokenized document-aligned Wikipedias available online9 , SGD on pseudo-bilingual documents with a global rate 0.025. For BiCVM, we use the tool released by its"
P16-1024,P14-1023,0,0.0253315,"s shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar represen"
P16-1024,D14-1082,0,0.0960485,"BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data. 1 Monolingual vs Bilingual Figure 1: A toy example of a 3-dimensional monolingual vs shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained i"
P16-1024,D15-1131,0,0.323168,"or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main disadvantage of Type 2 mode"
P16-1024,E14-1049,0,0.782736,"word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learning models have been proposed, differing in the essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolin"
P16-1024,P04-1067,0,0.0334336,"., 2015; Ammar et al., 2016): (1) two separate non-aligned monolingual embedding spaces are induced using any monolingual WE learning model (SGNS is the typical choice), (2) given a seed lexicon of word translation pairs as the bilingual signal for training, a mapping function is learned which ties the two monolingual spaces together into a SBWES. All existing work on this class of models assumes that high-quality training seed lexicons are readily available. In reality, little is understood regarding what constitutes a high quality seed lexicon, even with “traditional” distributional models (Gaussier et al., 2004; Holmlund et al., 2005; Vuli´c and Moens, 2013). Therefore, in this work we ask whether BWE learning could be improved by making more intelligent choices when deciding over seed lexicon entries. In order to do this we delve deeper into the cross-lingual mapping problem by analyzing a spectrum of seed lexicons with respect to controllable parameters such as lexicon source, its size, translation method, and translation pair reliability. The contributions of this paper are as follows: (C1) We present a systematic study on the importance of seed lexicons for learning mapping functions between mon"
P16-1024,P15-1119,0,0.0520406,"e essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a po"
P16-1024,P14-1006,0,0.0839575,"propose a simple yet effective hybrid BWE model HYBWE that removes the need for readily available seed lexicons, and satisfies properties P1 and P2. HYBWE relies on an inexpensive seed lexicon of highly reliable word translation pairs obtained by a documentlevel BWE model (Vuli´c and Moens, 2016) from document-aligned comparable data. (C3) Using a careful pair selection process when constructing a seed lexicon, we show that in the BLL task HYBWE outperforms a BWE model of Mikolov et al. (2013a) which relies on readily available seed lexicons. HYBWE also outperforms state-of-the-art models of (Hermann and Blunsom, 2014b; Gouws et al., 2015) which require sentencealigned parallel data. 2 Learning SBWES using Seed Lexicons Given source and target language vocabularies V S and V T , all BWE models learn a representation of each word w ∈ V S t V T in a SBWES as a realvalued vector: w = [f1 , . . . , fd ], where fk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional SBWES. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the SBWES: sim(w, v) = SF (w, v) = cos(w,"
P16-1024,P12-1092,0,0.0176666,"omputed by aggregating over all word embeddings for each cwj ∈ Con(w) using standard addition as the compositional operator (Mitchell and Lapata, 2008) which was proven a robust choice (Milajevs et al., 2014): Con(w) = cw1 + cw2 + . . . + cwr Table 3: Acc1 scores in the SWTC task. All seed lexicons contain 6K translation pairs, except for BNC+HYB+SYM (its sizes provided in parentheses). * denotes a statistically significant improvement over baselines and BNC+GT using McNemar’s statistical significance test with the Bonferroni correction, p &lt; 0.05. cross-lingual variant of the task proposed by Huang et al. (2012) which evaluates monolingual contextsensitive semantic similarity of words in sentential context, and it is also very related to cross-lingual lexical substitution (Mihalcea et al., 2010). To isolate the performance of each BWE induction model from the details of the SWTC setup, we use the same approach with all models: we opt for the SWTC framework proven to yield excellent results with BWEs in the SWTC task (Vuli´c and Moens, 2016). In short, the context bag Con(w) = {cw1 , . . . , cwr } is obtained by harvesting all r words that occur with w in the sentence. (4) where cwj is the embedding o"
P16-1024,D15-1245,0,0.0442718,"ement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models lab"
P16-1024,C12-1089,0,0.086408,"n. (Type 1) Parallel-Only: This group of BWE models relies on sentence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence"
P16-1024,P14-2037,0,0.0693184,"Missing"
P16-1024,P15-1027,0,0.517691,", 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models labeled Post-Hoc Mapping with Seed Lexicons. These models operate as follows (Mikolov et al., 2013a; Dinu et al., 2015; Lazaridou et al., 2015; Ammar et al., 2016): (1) two separate non-aligned monolingual embedding spaces are induced using any monolingual WE learning model (SGNS is the typical choice), (2) given a seed lexicon of word translation pairs as the bilingual signal for training, a mapping function is learned which ties the two monolingual spaces together into a SBWES. All existing work on this class of models assumes that high-quality training seed lexicons are readily available. In reality, little is understood regarding what constitutes a high quality seed lexicon, even with “traditional” distributional models (Gaussie"
P16-1024,P14-2050,0,0.219306,"rd embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A v"
P16-1024,Q15-1016,0,0.0387534,"r SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language processing (NLP), serving as invaluable features in a broad Research interest has recently extended to bilingual word embeddings (BWEs). BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learnin"
P16-1024,W15-1521,0,0.524887,"d Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main disadvantage of Type 2 models is the costly parallel data needed for the bilingual signal (thus colliding"
P16-1024,W15-1501,0,0.0174094,"translation given the sentential context.12 Table 3 summarizes the results (Acc1 scores) in the SWTC task. NO-CONTEXT refers to the contextinsensitive majority baseline obtained by BNC+GT (i.e., it always chooses the most semantically similar translation candidate at the word type level). We also report the results of the best SWTC model from Vuli´c and Moens (2014). The results largely support the claims established with the BLL evaluation. An exter11 The same ranking of different models (with lower absolute scores) is observed when adapting the monolingual lexical substitution framework of Melamud et al. (2015) to the SWTC task as done by Vuli´c and Moens (2016). 12 The SWTC evaluation set is available online at: http://aclweb.org/anthology/attachments/D/D14/D141040.Attachment.zip 254 nal seed lexicon of BNC+GT may be safely replaced by an automatically induced inexpensive seed lexicon (as in HYBWE with BNC+HYB+SYM/ASYM). The best performing models are again BNC+HYB+SYM and HFQ+HYB+SYM. The comparison of ASYM and SYM lexicon variants further suggests that filtering translation pairs using the symmetry constraint again leads to consistent improvements, but stricter selection criteria with higher thre"
P16-1024,N16-1118,0,0.017269,"ent-level embedding space. The results in the tasks of (1) bilingual lexicon learning and (2) suggesting word translations in context demonstrate that – due to its careful selection of reliable translation pairs for seed lexicons – HYBWE outperforms benchmarking BWE induction models, all of which use more expensive bilingual signals for training. In future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (Agi´c et al., 2015; Zhang et al., 2016), other context types inspired by recent work in the monolingual settings (Levy and Goldberg, 2014a; Melamud et al., 2016), as well as model adaptations that can work with multi-word expressions. Encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The authors are grateful to Roi Reichart and the anonymous reviewers for their helpful comments and suggestions. 255 References Željko Agi´c, Dirk Hovy, and Anders Søgaard. 2015. If all you have is a bit of the Bible: Learning POS taggers for truly low-r"
P16-1024,S10-1002,0,0.0251863,"(Milajevs et al., 2014): Con(w) = cw1 + cw2 + . . . + cwr Table 3: Acc1 scores in the SWTC task. All seed lexicons contain 6K translation pairs, except for BNC+HYB+SYM (its sizes provided in parentheses). * denotes a statistically significant improvement over baselines and BNC+GT using McNemar’s statistical significance test with the Bonferroni correction, p &lt; 0.05. cross-lingual variant of the task proposed by Huang et al. (2012) which evaluates monolingual contextsensitive semantic similarity of words in sentential context, and it is also very related to cross-lingual lexical substitution (Mihalcea et al., 2010). To isolate the performance of each BWE induction model from the details of the SWTC setup, we use the same approach with all models: we opt for the SWTC framework proven to yield excellent results with BWEs in the SWTC task (Vuli´c and Moens, 2016). In short, the context bag Con(w) = {cw1 , . . . , cwr } is obtained by harvesting all r words that occur with w in the sentence. (4) where cwj is the embedding of the j-th context word, and Con(w) is the resulting embedding of the context bag Con(w). Finally, for each tj ∈ T C(w), the context-sensitive similarity with w is computed as: sim(w, tj"
P16-1024,D14-1079,0,0.0343167,"Missing"
P16-1024,P08-1028,0,0.0178393,"Missing"
P16-1024,C10-2174,0,0.110497,"Missing"
P16-1024,N10-1135,0,0.169773,"red lexicon pairs available (e.g., 100500)? (2) Can the Type 4 models profit from the inclusion of more seed lexicon pairs (e.g., more than 5K, even up to 40K-50K lexicon pairs)? Translation Pair Reliability When building seed lexicons through SBWES-1 (i.e., BNC+HYB and HFQ+HYB methods), it is possible to control for the reliability of translation pairs to be included in the final lexicon, with the idea that the use of only highly reliable pairs can potentially lead to an improved SBWES-2. A simple yet effective reliability reliability feature for translation pairs is the symmetry constraint (Peirsman and Padó, 2010; Vuli´c and Moens, 2013) : two words xi ∈ V S and yi ∈ V S are used as seed lexicon pairs only if they are mutual nearest neighbours given their representations in SBWES-1. The two variants of seed lexicons with only symmetric pairs are BNC+HYB+SYM and HFREQ+HYB+SYM. We also test the variants without the symmetry constraint (i.e., BNC+HYB+ASYM and HFQ+HYB+ASYM). Even more conservative reliability measures may be applied by exploiting the scores in the lists of translation candidates ranked by their similarity to the cue word xi . We investigate a symmetry constraint with a threshold: two word"
P16-1024,P15-2093,0,0.0579551,"tence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(MonoS +MonoT )+δBi. The monolingual objectives M onoS and M onoT ensure that similar words in each language are assigned similar embeddings and aim to capture the semantic structure of each language, whereas the cross-lingual objective Bi ensures that similar words across languages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual components.1 The main di"
P16-1024,D07-1070,0,0.030323,"uws et al., 2015).5 Baseline Models To induce SBWES-1, we resort to document-level embeddings of Vuli´c and Moens (2016) (Type 3). We also compare to results obtained directly by their model (BWESG) to measure the performance gains with HYBWE. To compare with a representative Type 2 model, we opt for the BilBOWA model of Gouws et al. (2015) due to its solid performance and robustness in the BLL task when trained on general-domain corpora such as Wikipedia (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets, as well as its public availabilliterature (Smith and Eisner, 2007; Tu and Honavar, 2012; Vuli´c and Moens, 2013), but we do not observe any significant gains when resorting to the more complex reliability estimates. 4 http://people.cs.kuleuven.be/~ivan.vulic/ 5 Similar trends are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these results for clarity and the fact that the actual BLL performance is best reflected in Acc1 scores (i.e., best translation only). ity.6 In short, BilBOWA combines the adapted SGNS for monolingual objectives together with a cross-lingual objective that minimizes the L2 -loss between the bag-of-word v"
P16-1024,tiedemann-2012-parallel,0,0.0315584,", 2013a).7 Training Data and Setup We use standard training data and suggested settings to obtain BWEs for all models involved in comparison. We retain the 100K most frequent words in each language for all models. To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Polyglot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025. For BilBOWA, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization is provided by the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We use SGD with a global rate of 0.15.8 The window size is varied from 2 to 16 in steps of 2, and the best scoring model is always reported in all comparisons. BWESG was trained on the cleaned and tokenized document-aligned Wikipedias available online9 , SGD on pseudo-bilingual documents with a global rate 0.025. For BiCVM, we use the tool released by its authors10 and train on the whole Europarl.v7 for each language pair: we train an additive model, with hinge loss margin set to d (i.e., dimensionality) as in the original paper, batch size of 50, and noise parameter of 10. All BiCVM models"
P16-1024,N16-1072,0,0.0994238,"represented in a uniform language-independent manner such that similar words (regardless of the actual language) have similar representations (see Fig. 1). A variety of BWE learning models have been proposed, differing in the essential requirement of a bilingual signal necessary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalab"
P16-1024,D12-1121,0,0.0201055,"line Models To induce SBWES-1, we resort to document-level embeddings of Vuli´c and Moens (2016) (Type 3). We also compare to results obtained directly by their model (BWESG) to measure the performance gains with HYBWE. To compare with a representative Type 2 model, we opt for the BilBOWA model of Gouws et al. (2015) due to its solid performance and robustness in the BLL task when trained on general-domain corpora such as Wikipedia (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets, as well as its public availabilliterature (Smith and Eisner, 2007; Tu and Honavar, 2012; Vuli´c and Moens, 2013), but we do not observe any significant gains when resorting to the more complex reliability estimates. 4 http://people.cs.kuleuven.be/~ivan.vulic/ 5 Similar trends are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these results for clarity and the fact that the actual BLL performance is best reflected in Acc1 scores (i.e., best translation only). ity.6 In short, BilBOWA combines the adapted SGNS for monolingual objectives together with a cross-lingual objective that minimizes the L2 -loss between the bag-of-word vectors of parallel sen"
P16-1024,P10-1040,0,0.0247888,"pace. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data. 1 Monolingual vs Bilingual Figure 1: A toy example of a 3-dimensional monolingual vs shared bilingual word embedding space (further SBWES) from Gouws et al. (2015). range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). Several studies have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010). Yet the widely used skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy and Goldberg, 2014b; Levy et al., 2015). Introduction Dense real-valued vector representations of word"
P16-1024,P16-1157,0,0.30501,"ion of each word w ∈ V S t V T in a SBWES as a realvalued vector: w = [f1 , . . . , fd ], where fk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional SBWES. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the SBWES: sim(w, v) = SF (w, v) = cos(w, v). 2.1 Related Work: BWE Models and Bilingual Signals BWE models may be clustered into four different types according to bilingual signals used in training, and properties P1 and P2 (see Sect. 1). Upadhyay et al. (2016) provide a similar overview of recent bilingual embedding learning architectures regarding different bilingual signals required for the embedding induction. (Type 1) Parallel-Only: This group of BWE models relies on sentence-aligned and/or word-aligned parallel data as the only data source (Zou et al., 2013; Hermann and Blunsom, 2014a; Koˇciský et al., 2014; Hermann and Blunsom, 2014b; Chandar et al., 2014). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). 248 (Type 2) Joint Bilingual Tr"
P16-1024,D13-1168,1,0.647672,"Missing"
P16-1024,D14-1040,1,0.726709,"Missing"
P16-1024,P16-2031,1,0.836232,"Missing"
P16-1024,N16-1156,0,0.165743,"n two monolingual embedding spaces using only highly reliable symmetric translation pairs from an inexpensive seed document-level embedding space. The results in the tasks of (1) bilingual lexicon learning and (2) suggesting word translations in context demonstrate that – due to its careful selection of reliable translation pairs for seed lexicons – HYBWE outperforms benchmarking BWE induction models, all of which use more expensive bilingual signals for training. In future work, we plan to investigate other methods for seed pairs selection, settings with scarce resources (Agi´c et al., 2015; Zhang et al., 2016), other context types inspired by recent work in the monolingual settings (Levy and Goldberg, 2014a; Melamud et al., 2016), as well as model adaptations that can work with multi-word expressions. Encouraged by the excellent results, we also plan to test the portability of the approach to more language pairs, and other tasks and applications. Acknowledgments This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The authors are grateful to Roi Reichart and the anonymous reviewers for their helpful comments and suggestions. 255 References Želj"
P16-1024,D13-1141,0,0.123989,"ruct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity (Faruqui and Dyer, 2014), learning bilingual word lexicons (Mikolov et al., 2013a; Gouws et al., 2015; Vuli´c et al., 2016), cross-lingual entity linking (Tsai and Roth, 2016), 247 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 247–257, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics parsing (Guo et al., 2015; Johannsen et al., 2015), machine translation (Zou et al., 2013), or crosslingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016). BWE models should have two desirable properties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains. While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models labeled Post-Hoc Mapping with Seed Lexicons"
P16-1024,P15-2044,0,\N,Missing
P16-2031,P14-1006,0,0.112167,"that share a common meaning across different languages. It plays an important role in a variety of fundamental tasks in IR and NLP, e.g. cross-lingual information retrieval and statistical machine translation. The majority of current BLL models aim to learn lexicons from comparable data. These approaches work by (1) mapping language pairs to a shared crosslingual vector space (SCLVS) such that words are close when they have similar meanings; and (2) extracting close lexical items from the induced SCLVS. Bilingual word embedding (BWE) induced models currently hold the state-of-the-art on BLL (Hermann and Blunsom, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016). Although methods for learning SCLVSs are predominantly text-based, this space need not be linguistic in nature: Bergsma and van Durme (2011) and Kiela et al. (2015) used labeled images from 2 2.1 Methodology Linguistic Representations We use three representative linguistic BWE models. Given a source and target vocabulary V S and V T , BWE models learn a representation of each word w ∈ V S ∪ V T as a real-valued vec188 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7"
P16-2031,D14-1005,1,0.413644,"eneral performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set for (linguistic) BLL models from comparable data (Vuli´c and Moens, 2016)5 . It comprises 1, 000 nouns in ES, IT, and NL, along with their oneto-one ground-truth word translations in EN compiled semi-automatically. Translation direction is ES/IT /N L → EN . Multi-Modal Representations We experiment with two ways of fusing information stemming from the linguistic and visual modalities. Following recent work in multi-modal semantics (Bruni et al., 2014; Kiela and Bottou, 2014), we construct representations by concatenating the centered and L2 -normalized linguistic and visual feature vectors: wmm = α × wling ||(1 − α) × wvis Experimental Setup Training Data and Setup We used standard training data and suggested settings to learn M/G/V-EMB model representations. M-EMB and G-EMB were trained on the full cleaned and tokenized Wikipedias from the Polyglot website (AlRfou et al., 2013). V-EMB was trained on the full tokenized document-aligned Wikipedias from (1) where ||denotes concatenation and α is a parameter governing the contributions of each unimodal representatio"
P16-2031,P14-2135,1,0.557318,"setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from Kiela et al. (2015). We also propose a weighting technique based on image dispersion (Kiela et al., 2014) that governs the influence of visual information in fused representations, and show that this technique leads to robust multi-modal models which do not require fine tuning of the fusion parameter. Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL). Such image-based BLL methods, however, still fall short of linguistic approaches. In this paper, we propose a simple yet effective multimodal approach that learns bilingual semantic representations that fuse linguistic and visual input. These new bilingual multi-modal embeddings display signi"
P16-2031,D15-1015,1,0.471003,"Missing"
P16-2031,J99-4009,0,0.810588,"image dispersion (ID) (Kiela et al., 2014). ID is defined as the average pairwise cosine distance between all the image representations/vectors {i1 . . . in } in the set of images for a given word w: id(w) = X 2 ij · ik 1− n(n − 1) |ij ||ik | (2) j<k≤n 5 Intuitively, more concrete words display more coherent visual representations and consequently lower ID scores (see Footnote 9 again). The lowest improvements on V ULIC 1000 are reported for the IT-EN language pair, which is incidentally the most abstract test set. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), albeit in a more complex way, since abstract concepts will relate more varied situations (Barsalou and Wiemer-Hastings, 2005). Consequently, uni-modal visual representations are not powerful enough to capture all the semantic intricacies of such abstract concepts, and the linguistic components are more beneficial in such cases. This explains an improved performance with α = 0.7, but also calls for a more intelligent decision mechanism on how much perceptual information to include in the multi-modal models. The decision should be closely related to the degree of a concept’s concreteness, e.g."
P16-2031,P15-1027,0,0.289784,"feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is learned through an L2 -regularized least-squa"
P16-2031,W16-3210,0,0.0149091,"Missing"
P16-2031,E14-1049,0,0.0830889,"l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is le"
P16-2031,W15-1521,0,0.168909,"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tor: wling = [f1ling , . . . , fdling ], where fkling ∈ l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate r"
P16-2031,P04-1067,0,0.0295097,"SCLVS: simvis (w, v) = SF (wvis , vvis ), e.g. cosine. (2) CNN-AVG M AX: An alternative strategy, introduced by Bergsma and van Durme (2011), is to consider the similarities between individual images from the two sets and take the average of the maximum similarity scores as the final similarity simvis (w, v). 2.3 3 Task: Bilingual Lexicon Learning Given a source language word ws , the task is to find a target language word wt closest to ws in the SCLVS, and the resulting pair (ws , wt ) is a bilingual lexicon entry. Performance is measured using the BLL standard Top 1 accuracy (Acc1 ) metric (Gaussier et al., 2004; Gouws et al., 2015). Test Sets We work with three language pairs: English-Spanish/Dutch/Italian (EN-ES/NL/IT), and two benchmarking BLL test sets: (1) B ERGSMA 500: consisting of a set of 500 ground truth noun pairs for the three language pairs, it is considered a benchmarking test set in prior work on BLL using vision (Bergsma and van Durme, 2011)4 . Translation direction in our tests is EN → ES/IT /N L. (2) V ULIC 1000: constructed to measure the general performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set"
P16-2031,N16-1021,0,0.0159524,"le. As future work, we plan to analyse the ability of multi-view representation learning algorithms to yield fused multi-modal representations in bilingual settings (Lazaridou et al., 2015b; Rastogi et al., 2015; Wang et al., 2015), as well as to apply multi-modal bilingual spaces in other tasks such as zero-short learning (Frome et al., 2013) or cross-lingual MM information search and retrieval following paradigms from monolingual settings (Pereira et al., 2014; Vuli´c and Moens, 2015). The inclusion of perceptual data, as this paper reveals, seems especially promising in bilingual settings (Rajendran et al., 2016; Elliott et al., 2016), since the perceptual information demonstrates the ability to transcend linguistic borders. Image Dispersion Weighting The intuition that the inclusion of visual information may lead to negative effects in MM modeling has been exploited by Kiela et al. (2014) in their work on image-dispersion filtering: Although the filtering method displays some clear benefits, its shortcoming lies in the fact that it performs a binary decision which can potentially discard valuable perceptual information for less concrete concepts. Here, we introduce a weighting scheme where the perce"
P16-2031,N15-1058,0,0.0380203,"Missing"
P16-2031,P14-1068,0,0.159578,"ven.be Abstract the Web to learn bilingual lexicons based on visual features, with features derived from deep convolutional neural networks (CNNs) leading to the best results (Kiela et al., 2015). However, vision-based BLL does not yet perform at the same level as state-of-the-art linguistic models. Here, we unify the strengths of both approaches into one single multi-modal vision-language SCLVS. It has been found in multi-modal semantics that linguistic and visual representations are often complementary in terms of the information they encode (Deselaers and Ferrari, 2011; Bruni et al., 2014; Silberer and Lapata, 2014). This is the first work to test the effectiveness of the multi-modal approach in a BLL setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from"
P16-2031,tiedemann-2012-parallel,0,0.0181405,"ting from the BNC word frequency list (Kilgarriff, 1997), the 6, 318 most frequent EN words were translated to the three other languages using Google Translate. The lists were subsequently cleaned, removing all pairs that contain IT/ES/NL words occurring in the test sets and least frequent pairs, to build the final 3×5K training pairs. We trained two monolingual SGNS models, using SGD with a global learning rate of 0.025. For G-EMB, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization was provided in the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We used SGD with a global learning rate 0.15. For V-EMB, monolingual SGNS was trained on pseudo-bilingual documents using SGD with a global learning rate 0.025. All BWEs were trained with d = 300.7 Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e − 4. We report results with two α standard values: 0.5 and 0.7 (more weight assigned to the linguistic part). 4 sions8 . There is a marked difference in performance on B ERGSMA 500 and V ULIC 1000: visual-only BLL models on V ULIC 1000 perform two times worse than linguistic-only BLL models. This is easily explained by the increase"
P16-2031,D13-1168,1,0.367109,"Missing"
P16-2031,W13-3520,0,\N,Missing
P16-2084,W15-0105,0,0.0182753,"ersal NLP” initiative but also owing to the benchmarking evaluation sets for other languages beyond English (i.e., IT, DE) that have very recently become available, e.g., (Leviant and Reichart, 2015). We evaluate SGNS with different context types from sect. 2.1 across the three languages on two benchmarking tasks and datasets: (1) semantic similarity on SimLex-999 (Hill et al., 2015) translated and re-scored by native speakers in EN, DE, and IT (Leviant and Reichart, 2015), and (2) word analogies on the Google dataset (Mikolov et al., 2013a) made available in IT (Berardi et al., 2015) and DE (Köper et al., 2015) only recently. WE Induction: Data All the word representations in comparison are induced from the Polyglot Wikipedia data (Al-Rfou et al., 2013).4 UDEPS-ARC However, UDEPS-NAIVE also produces uninformative context pairs such as (telescope, with_case), and it does not specify the type of e.g. the nmod relation between discovers and telescope which are linked through the preposition with. Our intuition is that a simple post-hoc intervention into the UDEPS context extraction may yield even more focused contexts. UDEPSARC leans on the idea of arc collapsing from prior work (Levy and Goldberg, 201"
P16-2084,W13-3520,0,0.130322,"tly become available, e.g., (Leviant and Reichart, 2015). We evaluate SGNS with different context types from sect. 2.1 across the three languages on two benchmarking tasks and datasets: (1) semantic similarity on SimLex-999 (Hill et al., 2015) translated and re-scored by native speakers in EN, DE, and IT (Leviant and Reichart, 2015), and (2) word analogies on the Google dataset (Mikolov et al., 2013a) made available in IT (Berardi et al., 2015) and DE (Köper et al., 2015) only recently. WE Induction: Data All the word representations in comparison are induced from the Polyglot Wikipedia data (Al-Rfou et al., 2013).4 UDEPS-ARC However, UDEPS-NAIVE also produces uninformative context pairs such as (telescope, with_case), and it does not specify the type of e.g. the nmod relation between discovers and telescope which are linked through the preposition with. Our intuition is that a simple post-hoc intervention into the UDEPS context extraction may yield even more focused contexts. UDEPSARC leans on the idea of arc collapsing from prior work (Levy and Goldberg, 2014a; Melamud et al., 2016) that we now adjust to the UD annotation scheme. The difference to UDEPS-NAIVE is as follows: For each pair of words lin"
P16-2084,P14-2131,0,0.156916,"DTAL, University of Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for"
P16-2084,P14-1023,0,0.055475,"o consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ful UDEPS contexts leads to consistent improvements across languages, especially in detecting functional similarity. This focused contribution is the first crosslinguistic comparison of different context types for learning word representations in three languages, reaching beyond English. It also"
P16-2084,P14-2050,0,0.0843096,"Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline t"
P16-2084,W14-1618,0,0.0984323,"Cambridge {iv250, alk23}@cam.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline t"
P16-2084,Q15-1016,0,0.244474,"ents across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ful UDEPS contexts leads to consistent improvements across languages, especially in detecting functional similarity. This focused contribution is the first crosslinguistic comparison of different context types for learning word representations in three languages, reaching beyond English. It also constitutes a first"
P16-2084,P13-2109,0,0.169567,"Missing"
P16-2084,P10-1131,0,0.0328108,"universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learnin"
P16-2084,D11-1006,0,0.0299181,"e Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word,"
P16-2084,C10-1011,0,0.0884812,"rs_case_with−1 (Fig. 1). In addition, we remove the uninformative case arc and its associated contexts: (with, telescope_case−1 ), (telescope, with_case) from the training pairs. UPOS Tagging and UD Parsing The Wikipedia corpora were UPOS-tagged using a state-of-the art system TurboTagger (Martins et al., 2013).5 TurboTagger was trained using suggested settings without any further parameter fine-tuning (SVM MIRA with 20 iterations) on the TRAIN + DEV portion of the UD treebank annotated with UPOS tags. Following that, the Wikipedia data were UD-parsed6 using the graph-based Mate parser v3.61 (Bohnet, 2010)7 and the same regime: suggested settings on the TRAIN + DEV UD treebank portion.8 The performance of the models measured on the TEST portion of the UD treebanks is reported in Tab. 1. 4 https://sites.google.com/site/rmyeid/projects/polyglot http://www.cs.cmu.edu/ ark/TurboParser/ 6 Besides EN, DE, and IT, we also UPOS-tagged and UDparsed Wikipedias in NL, ES, and HR. We believe that the full UPOS-tagged and UD-parsed Wikipedias in six languages are a valuable asset for future research and we plan to make the resource publicly available at: http://ltl.mml.cam.ac.uk/resources/ 7 https://code.go"
P16-2084,P13-2017,0,0.028495,"ased contexts from UD parses (UDEPS) in English and Italian. Top: the example sentence in English taken from (Levy and Goldberg, 2014a), now UD-parsed. Middle: the same sentence in Italian, UD-parsed. Note the very similar structure of the two parses. Bottom: the intuition behind UDEPS-ARC. The uninformative shortrange case arc between with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typolo"
P16-2084,D14-1082,0,0.130132,"the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages, but their advantage over BOW is not as prominent as previously reported on English. We also show that simple “post-parsing” filtering of useful UDEPS contexts leads to consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-2084,P15-1038,0,0.01382,"ted in Tab. 1. 4 https://sites.google.com/site/rmyeid/projects/polyglot http://www.cs.cmu.edu/ ark/TurboParser/ 6 Besides EN, DE, and IT, we also UPOS-tagged and UDparsed Wikipedias in NL, ES, and HR. We believe that the full UPOS-tagged and UD-parsed Wikipedias in six languages are a valuable asset for future research and we plan to make the resource publicly available at: http://ltl.mml.cam.ac.uk/resources/ 7 https://code.google.com/archive/p/mate-tools/ 8 We opted for the Mate parser due to its speed, simplicity, and state-of-the-art performance according to very recent parser evaluations (Choi et al., 2015). 5 3 Results with another context type relying on substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015) are omitted due to its subpar performance in our experiments as well as across a variety of semantic tasks in a recent Englishfocused study (Melamud et al., 2016). 520 Spearman’s ρ 0.40 0.35 0.40 0.35 0.30 0.35 0.30 0.25 0.30 0.25 0.20 0.25 0.20 UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 500 600 0.15 50 100 300 0.20 0.15 UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 500 600 0.10 50 100 300 d UDEPS-NAIVE UDEPS-ARC BOW-2 POSIT-2 0.14 50 100 d (a) English 300 500 600 d (b) German (c) Italian Figure 2:"
P16-2084,N15-1050,0,0.0876616,"Missing"
P16-2084,D11-1005,0,0.0193508,") complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word, context) pairs.2 Ke"
P16-2084,N16-1118,0,0.471559,"ontexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline that relies on languageuniversal syntactic p"
P16-2084,de-marneffe-etal-2014-universal,0,0.0728399,"Missing"
P16-2084,P12-1066,0,0.0306358,"the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge crosslinguistically; in this case, to yield more informed UDEPS contexts for improved word embeddings. The extraction of UDEPS as the new variant of dependency-based contexts is completely language-agnostic on purpose: exactly the same procedure is followed for each language in comparison in order to make the representation learning framework completely universal. 2.1 nsubj tion of word2vec which is capable of learning from arbitrary (word, context) pairs.2 Keeping the representati"
P16-2084,J15-4004,1,0.900938,"am.ac.uk Abstract contexts (BOW). However, the underlying SGNS model is equally applicable to other context types. Recent comparative studies have demonstrated the usefulness of dependency-based contexts (DEPS) (Padó and Lapata, 2007) for the task. In comparison with BOW, syntactic contexts steer the induced semantic spaces towards functional similarity (e.g., tiger:cat) rather than towards topical similarity/relatedness (e.g., tiger:jungle). DEPS-based embeddings outperform the less informed BOW-based embeddings in a variety of similarity tasks (Bansal et al., 2014; Levy and Goldberg, 2014a; Hill et al., 2015; Melamud et al., 2016). However, these studies have all focused solely on English. A comparison extending to additional languages is required before any cross-lingual generalisations can be drawn. Following recent initiatives on languageagnostic and cross-linguistically consistent universal natural language processing (i.e., universal POS (UPOS) tagging and dependency (UD) parsing) (Nivre et al., 2015), this paper is concerned with two important questions: (Q1) Can one usefully replace the DEPS extraction pipeline optimised for tools developed for English with a pipeline that relies on langua"
P16-2084,J07-2002,0,0.0522346,"Missing"
P16-2084,petrov-etal-2012-universal,0,0.0602513,"om: the intuition behind UDEPS-ARC. The uninformative shortrange case arc between with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of su"
P16-2084,K15-1026,0,0.197455,"e actual language: the claims made for English (i.e., DEPS ≥ BOW) do not extend to other languages (Q2). A comparison of results from Tab. 1 with the task evaluation also shows that excellent tagging and parsing results do not guarantee a strong task performance. The results over the verb subset of SimLex also reveal that claims established with English are not necessarily general and true with other languages. For instance, while it has been noted that modeling verb similarity is indeed a difficult problem in English as evidenced by lower correlation scores on SimLex (see Fig. 2(a) and e.g. (Schwartz et al., 2015)), verbs are apparently easier to model in Italian (Fig. 2(c)), and a real challenge in German, The results are consistent with prior work on the UD treebanks, e.g., (Tiedemann, 2015). Training Setup The SGNS preprocessing scheme for English was replicated from (Levy and Goldberg, 2014a) and extended to the other two languages: all tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. Exactly the same vocabularies were used with all context types (approx. 185K distinct EN words, 163K DE words, and 83K IT words). The word2vecf SGNS was train"
P16-2084,W15-2137,0,0.0267924,"nt tagging and parsing results do not guarantee a strong task performance. The results over the verb subset of SimLex also reveal that claims established with English are not necessarily general and true with other languages. For instance, while it has been noted that modeling verb similarity is indeed a difficult problem in English as evidenced by lower correlation scores on SimLex (see Fig. 2(a) and e.g. (Schwartz et al., 2015)), verbs are apparently easier to model in Italian (Fig. 2(c)), and a real challenge in German, The results are consistent with prior work on the UD treebanks, e.g., (Tiedemann, 2015). Training Setup The SGNS preprocessing scheme for English was replicated from (Levy and Goldberg, 2014a) and extended to the other two languages: all tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. Exactly the same vocabularies were used with all context types (approx. 185K distinct EN words, 163K DE words, and 83K IT words). The word2vecf SGNS was trained using standard settings: 15 epochs, 15 negative samples, global learning rate 0.025, subsampling rate 1e − 4. All WEs were trained with d = 50, 100, 300, 500, 600. BOW-based WEs we"
P16-2084,P10-1040,0,0.143642,"cific optimization. Our results suggest that the universal DEPS (UDEPS) are useful for detecting functional similarity (e.g., verb similarity, solving syntactic analogies) among languages, but their advantage over BOW is not as prominent as previously reported on English. We also show that simple “post-parsing” filtering of useful UDEPS contexts leads to consistent improvements across languages. 1 Introduction Dense real-valued distributed representations of words known as word embeddings (WEs) have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks, e.g., (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014). The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) is still considered the stateof-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). The original implementation of SGNS learns word representations from local bag-of-words 518 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 518–524, c Berlin, Germany, August 7"
P16-2084,D12-1086,0,0.182465,"Missing"
P16-2084,I08-3008,0,0.0595784,"ween with and telescope is removed, and another “pseudo-arc” now specifying the exact link type (i.e., case_with) between discovers and telescope is added. Universal Multilingual Resources The departure point in our experiments is the Universal Dependencies project (McDonald et al., 2013; Nivre et al., 2015) which develops crosslinguistically consistent treebank annotation.1 The annotation scheme leans on the universal Stanford dependencies (de Marneffe et al., 2014) complemented with the Google universal POS tagset (Petrov et al., 2012) and the Interset interlingua for morphological tagsets (Zeman and Resnik, 2008). It provides a universal and consistent inventory of categories for similar syntactic constructions across languages. The main aim of the “universal initiative” is to facilitate cross-lingual and multilingual learning (e.g., multilingual parser development, typologies) by capturing structural similarities across languages and by exploiting connections that exist naturally between them (Berg-Kirkpatrick and Klein, 2010; McDonald et al., 2011; Cohen et al., 2011; Naseem et al., 2012). Here, we test the ability of such a universal annotation scheme to encode potentially useful semantic knowledge"
P17-1006,W16-1603,0,0.0323402,"ll and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016; Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations. The authors are grateful to the anonymous reviewers for their helpful suggestions. 64 References Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research 12:2493–2537. http://dl.acm.org/citation"
P17-1006,ehrmann-etal-2014-representing,0,0.0133747,"r verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca). Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem). Table 3: Vocabulary sizes and counts of ATTRACT (A) and R EPEL (R) constraints. constraints. These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016, i.a.). In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology. This relaxation ensures a wider portability of ATTRACTR EPEL to languages and domains without readily available or adequate resources. Extracting R EPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that d"
P17-1006,D14-1082,0,0.0120453,"roposed method does not require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remu"
P17-1006,W14-4340,1,0.939514,"impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c"
P17-1006,N15-1184,0,0.635015,"aduale lenti lente lenta veloce rapido en_book books memoir novel storybooks blurb booked rebook booking rebooked books de_buch sachbuch buches romandebüt büchlein pamphlet bücher büch büche büches büchen it_libro romanzo racconto volumetto saggio ecclesiaste libri libra librare libre librano Table 1: The nearest neighbours of three example words (expensive, slow and book) in English, German and Italian before (top) and after (bottom) morph-fitting. proliferation of word forms in morphologically rich languages. Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkši´c et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language. The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. 1 and Tab. 2. The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other"
P17-1006,E14-1049,0,0.198571,"contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-"
P17-1006,N16-1077,0,0.0279301,"nally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a"
P17-1006,N15-1070,0,0.0942068,"approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morpholog"
P17-1006,D15-1245,0,0.0180451,"require curated knowledge bases or gold lexicons. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio risch"
P17-1006,E17-1049,0,0.024409,"rect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model whi"
P17-1006,D15-1242,0,0.404513,"Missing"
P17-1006,N13-1092,0,0.182887,"Missing"
P17-1006,D16-1235,1,0.903455,"Missing"
P17-1006,P13-1149,0,0.162129,"ig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al"
P17-1006,W13-4066,0,0.0234804,", DE and IT are trained using four variants of the SGNS - LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3). As shown by Mrkši´c et al. (2017b), semantic specialisation of the employed word vectors benThe Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016). While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural62 0.45 0.45 0.85 0.85 SimLex 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.35 0.40 0.60 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 (a) English 0.80 Distrib MFix MFit-A MFit-AR DST Performance (Joint) 0.80 DST Performance (Joint) 0.40 DST 0.60 (b) German 0.45 0.85 0.45 0.35 0.75 0.30 0.70 0.25 0.65 0.20 0.15 Distrib MFix MFit-A MFit-AR SimLex SimLex (Spearman’s ρ) SimLex (Spearman’s ρ) 0.80 DST Performance (Joint) 0.40 0.40 0.35 0.30 0.25 0.20 0.15 0.60 (c) Italian Distrib MF"
P17-1006,W14-4337,0,0.147086,"Missing"
P17-1006,W13-4065,0,0.0658382,"Missing"
P17-1006,Q17-1022,1,0.883478,"Missing"
P17-1006,E17-1001,0,0.0359627,"We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and conte"
P17-1006,P15-1145,0,0.280709,"Missing"
P17-1006,W15-1521,0,0.150173,"= bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity"
P17-1006,P16-2074,0,0.0671559,"ion into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words. 7 Conclusion and Future Work We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic co"
P17-1006,W13-3512,0,0.0917221,"ts of the post-processing specialisation algorithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters)"
P17-1006,Q16-1030,0,0.239528,"ty and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them"
P17-1006,K16-1006,0,0.171359,"al models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric. First, w"
P17-1006,P15-2070,0,0.0998422,"Missing"
P17-1006,D14-1162,0,0.0854243,"each language are provided in Tab. 3.6 We label these collections of vectors SGNS - LARGE. all of our intrinsic and extrinsic experiments. Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MF IT-A), and (2) using both ATTRACT and R EPEL constraints (MF IT-AR). Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN"
P17-1006,E17-1029,0,0.030739,"sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediat"
P17-1006,P15-2130,1,0.859159,"Missing"
P17-1006,C14-1015,0,0.0339755,"rithm and the constraint selection. Word Vectors and Morphology The use of morphological resources to improve the representations of morphemes and words is an active area of research. The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.). The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes. Contrary to our work, these models typically coalesce all lexical relations. Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al"
P17-1006,N01-1024,0,0.0956175,"constraints such as (rispettosa, irrispettosi) (see Fig. 1). The final A and R constraint counts are given in Tab. 3. The full sets of rules are available as supplemental material. Extracting ATTRACT Pairs The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word. On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017). For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)). This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations. We define two rules for English, widely recognised as morphologically"
P17-1006,P17-1163,1,0.882418,"Missing"
P17-1006,K15-1026,1,0.916649,"Simple Language-Specific Rules Ivan Vuli´c1 , Nikola Mrkši´c1 , Roi Reichart2 Diarmuid Ó Séaghdha3 , Steve Young1 , Anna Korhonen1 1 2 3 University of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word f"
P17-1006,N16-1060,1,0.847607,"analyse the impact of morph-fitting on other collections of well-known EN word vectors. These vectors have varying vocabulary coverage and are trained with different architectures. We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015). We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014). We also experiment with standard well-known distributional spaces in other languages (IT and DE ), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vuli´c and Korhonen, 2016a). 4 Intrinsic Evaluation: Word Similarity"
P17-1006,N15-1186,0,0.023404,"ge-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological v"
P17-1006,E17-1042,1,0.846566,"Missing"
P17-1006,Q15-1025,0,0.601637,"redite) (dressed, undressed) (similar, dissimilar) (formality, informality) (stabil, unstabil) (geformtes, ungeformt) (relevant, irrelevant) (abitata, inabitato) (realtà, irrealtà) (attuato, inattuato) words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch. The second term pushes antonyms away from each other. If (xl , xr ) ∈ BR is the current minibatch of R EPEL constraints, this term can be expressed as follows: The ATTRACT-R EPEL model, proposed by Mrkši´c et al. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al. (2015). It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors. Given the initial vector space and collections of ATTRACT and R EPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart. The method’s cost function consists of three terms. The first term pulls the ATTRACT examples (xl , xr ) ∈ A closer together. If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as: X Germ"
P17-1006,P16-1230,1,0.801978,"Missing"
P17-1006,D16-1157,0,0.0330579,"→ 66.3 (MF IT-AR), setting a new state-of-the-art score for both datasets. The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors. On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms. These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors. 63 spaces for extrinsic tasks such as DST. 6 Wieting et al., 2016; Verwimp et al., 2017, i.a.). In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training. This pipelined approach results in a simpler, more portable model. In spirit, our work is similar to Cotterell et al. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework. Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules. Our"
P17-1006,P15-2111,0,0.0142836,"models from the literature in lieu of ATTRACT-R EPEL using the same set of “morphological” synonymy and antonymy constraints. We compare ATTRACT-R EPEL to the retrofitting model Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress). In future work, we will study how to fur61 ther refine extracted sets of constraints. We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existi"
P17-1006,W10-1401,0,0.0148159,"rsity of Cambridge Technion, Israel Institute of Technology Apple Inc. {iv250,nm480,sjy11,alk23}@cam.ac.uk doseaghdha@apple.com roiri@ie.technion.ac.il Abstract 2011). Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.). Morphologically rich languages, in which “substantial grammatical information. . . is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP. This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology. In the case of distributional vector space models, morphological complexity brings two challenges to the fore: Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures. These effects are detrimental for languag"
P17-1006,P10-1040,0,0.0585254,"ervation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dispendioso dispendiose en_slow fast slow"
P17-1006,P14-2089,0,0.265019,"nd naturally extends to constraints from other sources (e.g., WordNet) in future work. Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures. Related Work Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016). Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016). Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological"
P17-1006,P13-1118,0,0.0513226,"Missing"
P17-1006,E17-1040,0,0.0904069,"Missing"
P17-1006,E17-2033,0,0.0793456,"uct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.). 5 network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkši´c et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkši´c et al., 2017a, i.a.). Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation. The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkši´c et al., 2017a). The NBT learns to compose these vectors into intermediate utterance and context representations. Th"
P17-1006,D13-1141,0,0.035066,"ns. Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the Introduction Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 56 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 56–68 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1006 en_expensive costly costlier cheaper prohibitively pricey expensiveness costly costlier ruinously unaffordable de_teure teuren kostspielige aufwändige kostenintensive aufwendige teures teuren teurem teurer teurerer it_costoso dispendioso remunerativo redditizio rischioso costosa costosa costose costosi dis"
P17-1006,P16-1024,1,0.914352,"Missing"
P17-1006,W13-3520,0,\N,Missing
P17-1006,J15-4004,1,\N,Missing
P17-1006,P08-1087,0,\N,Missing
P17-1006,P14-2050,0,\N,Missing
P17-1006,P14-2131,0,\N,Missing
P17-1006,D16-1047,0,\N,Missing
P17-1006,P16-1156,0,\N,Missing
P17-1006,P16-2084,1,\N,Missing
P17-1006,N15-1140,0,\N,Missing
P18-1004,W13-3520,0,0.0820068,"xi2 , g i )}i=1 be one micro-batch created from one synonymy constraint and let Ma be the analogous micro-batch created from one antonymy constraint. Let us then assume that the first triple (i.e., for i = 1) in every microbatch corresponds to the constraint pair and the remaining 2K triples (i.e., for i ∈ {2, . . . , 2K + 1}) to respective non-constraint word pairs. We then define the contrastive objective as follows: JCNT = X 2K+1 X  i 1 2 1 i 2 (g i − gmin ) − (g 0 − g 0 ) distributional vectors for English: (1) SGNS-W2 – vectors trained on the Wikipedia dump from the Polyglot project (Al-Rfou et al., 2013) using the Skip-Gram algorithm with Negative Sampling (SGNS) (Mikolov et al., 2013b) by Levy and Goldberg (2014b), using the context windows of size 2; (2) G LOV E -CC – vectors trained with the GloVe (Pennington et al., 2014) model on the Common Crawl; and (3) FAST T EXT – vectors trained on Wikipedia with a variant of SGNS that builds word vectors by summing the vectors of their constituent character n-grams (Bojanowski et al., 2017). Linguistic Constraints. We experiment with the sets of linguistic constraints used in prior work (Zhang et al., 2014; Ono et al., 2015). These constraints, ext"
P18-1004,N13-1092,0,0.222432,"Missing"
P18-1004,P98-1013,0,0.152635,"ontrast modeling (Nguyen et al., 2016), and cross-lingual transfer of lexical resources (Vuli´c et al., 2017a). A common goal pertaining to all retrofitting models is to pull the vectors of similar words (e.g., synonyms) closer together, while some models also push the vectors of dissimilar words (e.g., antonyms) further apart. The specialization methods fall into two categories: (1) joint specialization methods, and (2) post-processing (i.e., retrofitting) methods. Methods from both categories make use of similar lexical resources – they typically leverage WordNet (Fellbaum, 1998), FrameNet (Baker et al., 1998), the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013; Pavlick et al., 2015), morphological lexicons (Cotterell et al., 2016), or simple handcrafted linguistic rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014"
P18-1004,D16-1235,1,0.899315,"Missing"
P18-1004,Q17-1010,0,0.16366,"dels trained with different constraints. Italian Croatian .407 .360 .249 .415 .533 .406 .448 .287 .315 Table 3: Spearman’s ρ correlation scores for German, Italian, and Croatian embeddings in the transfer setup: the vectors are specialized using the models trained on English constraints and evaluated on respective language-specific SimLex-999 variants. tor space6 containing word vectors of three other languages – German, Italian, and Croatian – along with the English vectors.7 Concretely, we map the Italian CBOW vectors (Dinu et al., 2015), German FastText vectors trained on German Wikipedia (Bojanowski et al., 2017), and Croatian Skip-Gram vectors trained on HrWaC corpus (Ljubeˇsi´c and Erjavec, 2011) to the G LOVE -CC English space. We create the translation pairs needed to learn the projections by automatically translating 4,000 most frequent English words to all three other languages with Google Translate. We then employ the ER model trained to specialize the G LOVE -CC space using the full set of English constraints, to specialize the distributional spaces of other languages. We evaluate the quality of the specialized spaces on the respective SimLex-999 dataset for each language (Leviant and Reichart"
P18-1004,D17-1185,1,0.874908,"Missing"
P18-1004,P15-2011,1,0.905471,"Missing"
P18-1004,W14-4337,0,0.109219,"Missing"
P18-1004,D14-1082,0,0.0149326,"We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks – lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi"
P18-1004,J15-4004,0,0.343088,"re expensive re-training on large text corpora, but is directly applied on top of any pre-trained vector space. The key idea of ER is to directly learn a specialization function in a supervised setting, using lexical constraints as training instances. In other words, our model, implemented as a deep feedforward neural architecture, learns a (non-linear) function which “translates” word vectors from the distributional space into the specialized space. We show that the proposed ER approach yields considerable gains over distributional spaces in word similarity evaluation on standard benchmarks (Hill et al., 2015; Gerz et al., 2016), as well as in two downstream tasks – lexical simplification and dialog state tracking. Furthermore, we show that, by coupling the ER model with shared multilingual embedding spaces (Mikolov et al., 2013a; Smith et al., 2017), we can also specialize distributional spaces for languages unseen in the training data in a zero-shot language transfer setup. In other words, we show that an explicit retrofitting model trained with external constraints from one language can be successfully used to specialize the distributional space of another language. 2 Mrkˇsi´c, 2017), lexical c"
P18-1004,P14-2075,0,0.134957,"ctor space.8 For each word in the input text L IGHT-LS retrieves most similar replacement candidates from the vector space. The candidates are then ranked according to several measures of simplicity and fitness for the context. Finally, the replacement is made if the top-ranked candidate is estimated to be simpler than the original word. By plugging-in vector spaces specialized by the ER model into L IGHT-LS, we hope to generate true synonymous candidates more frequently than with the unspecialized distributional space. Evaluation Setup. We evaluate L IGHT-LS on the LS dataset crowdsourced by Horn et al. (2014). For each indicated complex word Horn et al. (2014) collected 50 manual simplifications. We use two evaluation metrics from prior work (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015) to quantify the quality and frequency of word replacements: (1) 5.3.2 Dialog State Tracking Finally, we also evaluate the importance of explicit retrofitting in a downstream language understand8 The Light-LS implementation is available at: https://bitbucket.org/gg42554/embesimp 41 Text G LOV E -CC ATTRACT-R EPEL ER-CNT Wrestlers portrayed a villain or a hero as they followed a series of events that built tension"
P18-1004,P16-1156,0,0.0310283,"Missing"
P18-1004,N15-1070,0,0.0625061,"et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016). These models fine-tune the vectors of words present in the linguistic constraints to reflect the ground-truth lexical knowledge. While the large majority of specialization models from both classes operate only with similarity constraints, a line of recent work (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b) demonstrates that knowledge about both similar and dissimilar words leads to Related Work The importance of vector space specialization for downstream tasks has been observed,"
P18-1004,D15-1242,0,0.266231,"Missing"
P18-1004,W16-1607,0,0.228634,"Missing"
P18-1004,N15-1184,0,0.417018,"Missing"
P18-1004,N16-1077,0,0.0280739,"t only vectors of words from external constraints. 3 Our goal is to discern semantic similarity from semantic relatedness by comparing, in the specialized space, the distances between word pairs (wi , wj , r) ∈ C with distances that words wi and wj from those pairs have with other vocabulary words wm . It is intuitive to enforce that the synonyms are as close as possible and antonyms as far as possible. However, we do not know what the distances between non-synonymous and nonantonymous words g(x0 i , xm ) in the specialized space should look like. This is why, for all other words, similar to (Faruqui et al., 2016; Mrkˇsi´c et al., 2017), we assume that the distances in the specialized space for all word pairs not found in C should stay the same as in the distributional space: g(x0 i , x0 m ) = g(xi , xm ). This way we preserve the useful semantic content available in the original distributional space. In downstream tasks most errors stem from vectors of semantically related words (e.g., car – driver) being as similar as vectors of semantically similar words (e.g., car – automobile). To anticipate this, we compare the distances of pairs (wi , wj , r) ∈ C with the distances for pairs (wi , wm ) and (wj"
P18-1004,Q17-1022,1,0.891051,"Missing"
P18-1004,P14-2050,0,0.654274,"ng data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retrofitting models can be applied to arbitrary distributional spaces but they suffer from a major limitation – they locally update only vectors of words present in the external constraints, whereas vectors of all other (unseen) words remain intact. In contrast, joint special"
P18-1004,Q15-1016,0,0.119183,"Missing"
P18-1004,P16-2074,0,0.477053,"al., 2013), or BabelNet (Navigli and Ponzetto, 2012), to specialize distributional spaces for a particular lexical relation, e.g., synonymy (Faruqui et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Glavaˇs and Ponzetto, 2017). External constraints are commonly pairs of words between which a particular relation holds. Existing specialization methods exploit the external linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it (Yu and Dredze, 2014; Kiela et al., 2015; Nguyen et al., 2016, inter alia); (2) post-processing models fine-tune distributional vectors retroactively after training to satisfy the external constraints (Faruqui et al., 2015; Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico"
P18-1004,P15-1145,0,0.173321,"dcrafted linguistic rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al.,"
P18-1004,N15-1100,0,0.354979,"c rules (Vuli´c et al., 2017b). In what follows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al"
P18-1004,Q16-1030,0,0.407434,"llows, we discuss the two model categories. Joint Specialization Models. These models integrate external constraints into the distributional training procedure of general word embedding algorithms such as CBOW, Skip-Gram (Mikolov et al., 2013b), or Canonical Correlation Analysis (Dhillon et al., 2015). They modify the prior or the regularization of the original objective (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015) or integrate the constraints directly into the, e.g., an SGNS- or CBOW-style objective (Liu et al., 2015; Ono et al., 2015; Bollegala et al., 2016; Osborne et al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et"
P18-1004,N16-1118,0,0.0607403,"ver original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks – lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retro"
P18-1004,P15-2070,0,0.0613007,"Missing"
P18-1004,D14-1162,0,0.0919155,".e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces. 1 Introduction Algebraic modeling of word vector spaces is one of the core research areas in modern Natural Language Processing (NLP) and its usefulness has been shown across a wide variety of NLP tasks (Collobert et al., 2011; Chen and Manning, 2014; Melamud et al., 2016). Commonly employed distributional models for word vector induction are based on the distributional hypothesis (Harris, 1954), i.e., they rely on word co-occurrences obtained from large text corpora (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014a; Levy 34 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 34–45 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Mrkˇsi´c et al., 2017, inter alia). The latter, in general, outperform the former (Mrkˇsi´c et al., 2016). Retrofitting models can be applied to arbitrary distributional spaces but they suffer from a major limitation – they locally update only vectors of words present in the external constraints, whereas vectors of all other (unseen) words remain intact. I"
P18-1004,P15-1173,0,0.117133,"Missing"
P18-1004,J13-3004,0,0.0258719,"e Group University of Cambridge University of Mannheim 9 West Road, Cambridge CB3 9DA B6, 29, DE-68161 Mannheim iv250@cam.ac.uk goran@informatik.uni-mannheim.de Abstract et al., 2015; Bojanowski et al., 2017). The dependence on purely distributional knowledge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness (Hill et al., 2015; Schwartz et al., 2015) in the induced vector spaces. Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a precise semantic relation (Yih et al., 2012; Mohammad et al., 2013). For example, it is difficult to discern synonyms from antonyms in distributional spaces. This property has a particularly negative effect on NLP applications like text simplification and statistical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance (Glavaˇs and ˇ Stajner, 2015; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Kim et al., 2016b). A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space specialization or retrofitting."
P18-1004,P17-1163,0,0.0861994,"Missing"
P18-1004,K15-1026,0,0.424347,"Missing"
P18-1004,N18-1103,1,0.847418,"Missing"
P18-1004,D17-1270,1,0.911525,"Missing"
P18-1004,P17-1006,1,0.846512,"Missing"
P18-1004,E17-1042,0,0.0874422,"Missing"
P18-1004,Q15-1025,0,0.184681,"t al., 2016; Nguyen et al., 2016, 2017). Besides generally displaying lower performance compared to retrofitting methods (Mrkˇsi´c et al., 2016), these models are also tied to the distributional objective and any change of the underlying distributional model induces a change of the entire joint model. This makes them less versatile than the retrofitting methods. Post-Processing Models. Models from the popularly termed retrofitting family inject lexical knowledge from external resources into arbitrary pretrained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Sch¨utze, 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2016). These models fine-tune the vectors of words present in the linguistic constraints to reflect the ground-truth lexical knowledge. While the large majority of specialization models from both classes operate only with similarity constraints, a line of recent work (Mrkˇsi´c et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c et al., 2017b) demonstrates that knowledge about both similar and dissimilar words leads to Related Work The importance of vector space specialization for downstream tasks has been observed, inter alia, for dialog state tracking (Mrkˇsi´c"
P18-1004,D12-1111,0,0.0212671,"ata and Web Science Group University of Cambridge University of Mannheim 9 West Road, Cambridge CB3 9DA B6, 29, DE-68161 Mannheim iv250@cam.ac.uk goran@informatik.uni-mannheim.de Abstract et al., 2015; Bojanowski et al., 2017). The dependence on purely distributional knowledge results in a well-known tendency of fusing semantic similarity with other types of semantic relatedness (Hill et al., 2015; Schwartz et al., 2015) in the induced vector spaces. Consequently, the similarity between distributional vectors indicates just an abstract semantic association and not a precise semantic relation (Yih et al., 2012; Mohammad et al., 2013). For example, it is difficult to discern synonyms from antonyms in distributional spaces. This property has a particularly negative effect on NLP applications like text simplification and statistical dialog modeling, in which discerning semantic similarity from other types of semantic relatedness is pivotal to the system performance (Glavaˇs and ˇ Stajner, 2015; Faruqui et al., 2015; Mrkˇsi´c et al., 2016; Kim et al., 2016b). A standard solution is to move beyond purely unsupervised learning of word representations, in a process referred to as word vector space special"
P18-1004,P14-2089,0,0.528449,"the Paraphrase Database (Ganitkevitch et al., 2013), or BabelNet (Navigli and Ponzetto, 2012), to specialize distributional spaces for a particular lexical relation, e.g., synonymy (Faruqui et al., 2015; Mrkˇsi´c et al., 2017) or hypernymy (Glavaˇs and Ponzetto, 2017). External constraints are commonly pairs of words between which a particular relation holds. Existing specialization methods exploit the external linguistic constraints in two prominent ways: (1) joint specialization models modify the learning objective of the original distributional model by integrating the constraints into it (Yu and Dredze, 2014; Kiela et al., 2015; Nguyen et al., 2016, inter alia); (2) post-processing models fine-tune distributional vectors retroactively after training to satisfy the external constraints (Faruqui et al., 2015; Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work,"
P18-1004,D14-1161,0,0.47788,"Missing"
P18-1072,P17-1042,0,0.677938,"form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 Our motivation for this"
P18-1072,D18-1549,0,0.0413667,"ion for this is that dimensionality reduction will alter the geometric shape and remove characteristics of the embedding graphs that are important for alignment; but on the other hand, lower dimensionality introduces regularization, which will make the graphs more similar. Finally, in §4.6, we investigate the impact of different types of query test words on performance, including how performance varies across part-of-speech word classes and on shared vocabulary items. Learning scenarios Unsupervised neural machine translation relies on BDI using cross-lingual embeddings (Lample et al., 2018a; Artetxe et al., 2018), which in turn relies on the assumption that word embedding graphs are approximately isomorphic. The work of Conneau et al. (2018), which we focus on here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We"
P18-1072,W16-1614,0,0.386638,"l Dictionary Induction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary,"
P18-1072,P07-1056,0,0.0348626,"ES when the two monolingual embedding spaces are induced by two different algorithms (see the results of the entire Spanish cbow column).9 In sum, this means that the unsupervised approach is unlikely to work on pre-trained word embeddings unless they are induced on same6 One exception here is French, which they include in their paper, but French arguably has a relatively simple morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP"
P18-1072,Q17-1010,0,0.798117,"here, also makes several implicit assumptions that may or may not be necessary to achieve such isomorphism, and which may or may not scale to low-resource languages. The algorithms are not intended to be limited to learning scenarios where these assumptions hold, but since they do in the reported experiments, it is important to see to what extent these assumptions are necessary for the algorithms to produce useful embeddings or dictionaries. We focus on the work of Conneau et al. (2018), who present a fully unsupervised approach to aligning monolingual word embeddings, induced using fastText (Bojanowski et al., 2017). We describe the learning algorithm in §3.2. Conneau et al. (2018) consider a specific set of learning scenarios: 3.2 Summary of Conneau et al. (2018) We now introduce the method of Conneau et al. (2018).4 The approach builds on existing work on learning a mapping between monolingual word embeddings (Mikolov et al., 2013b; Xing et al., 2015) and consists of the following steps: 1) Monolingual word embeddings: An off-the-shelf word embedding algorithm (Bojanowski et al., 2017) is used to learn source and target language spaces X (a) The authors work with the following languages: English-{Frenc"
P18-1072,C12-1089,0,0.251885,"isomorphic between languages, and that this isomorphism can be learned from the statistics of the realizations of these processes, the monolingual corpora, in principle without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for q"
P18-1072,2005.mtsummit-papers.11,0,0.0562212,"Missing"
P18-1072,S17-2002,0,0.0503011,"morphology. 7 In order to get comparable term distributions, we translate the source language to the target language using the bilingual dictionaries provided by Conneau et al. (2018). 8 We also computed A-distances (Blitzer et al., 2007) and confirmed that trends were similar. 9 We also checked if this result might be due to a lowerquality monolingual ES space. However, monolingual word similarity scores on available datasets in Spanish show performance comparable to that of Spanish skip-gram vectors: e.g., Spearman’s ρ correlation is ≈ 0.7 on the ES evaluation set from SemEval-2017 Task 2 (Camacho-Collados et al., 2017). 783 EP 0.75 Wiki EMEA Wiki EMEA 0.60 0.55 0.65 0.60 0.55 0.50 EN:EP (a) en-es: domain similarity EMEA 0.65 0.60 0.55 0.50 EN:Wiki EN:EMEA Training Corpus (English) Wiki 0.70 Jensen-Shannon Similarity 0.65 EP 0.75 0.70 Jensen-Shannon Similarity 0.70 Jensen-Shannon Similarity EP 0.75 EN:EP 0.50 EN:Wiki EN:EMEA Training Corpus (English) (b) en-fi: domain similarity EN:EP EN:Wiki EN:EMEA Training Corpus (English) (c) en-hu: domain similarity 64.09 50 49.24 46.52 30 25.17 BLI: P@1 BLI: P@1 40 25.48 20 60 60 50 50 40 40 BLI: P@1 60 30 28.63 20 30 26.99 20 15.56 14.79 0 EN:EP 9.63 10 6.63 4.84 0 EN"
P18-1072,J82-2005,0,0.817708,"Missing"
P18-1072,E17-1072,1,0.939289,"nciple without any form of explicit alignment. Our results indicate this assumption is not true in general, and that approaches based on this assumption have important limitations. Introduction Cross-lingual word representations enable us to reason about word meaning in multilingual contexts and facilitate cross-lingual transfer (Ruder et al., 2018). Early cross-lingual word embedding models relied on large amounts of parallel data (Klementiev et al., 2012; Mikolov et al., 2013a), but more recent approaches have tried to minimize the amount of supervision necessary (Vuli´c and Korhonen, 2016; Levy et al., 2017; Artetxe et al., 2017). Some researchers have even presented unsupervised methods that do not rely on any form Contributions We focus on the recent stateof-the-art unsupervised model of Conneau et al. (2018).1 Our contributions are: (a) In §2, we show that the monolingual word embeddings used in Conneau et al. (2018) are not approximately isomorphic, using the VF2 algorithm (Cordella et al., 2001) and we therefore introduce a metric for quantifying the similarity of word embeddings, based on Laplacian eigenvalues. (b) In §3, we identify circumstances under which the unsupervised bilingual 1 O"
P18-1072,D16-1235,1,0.901982,"Missing"
P18-1072,K15-1026,0,0.0281285,"models are evaluated on a held-out set of query words. Here, we analyze the performance of the unsupervised approach across different parts-ofspeech, frequency bins, and with respect to query words that have orthographically identical counterparts in the target language with the same or a different meaning. Part-of-speech We show the impact of the partof-speech of the query words in Table 4; again on a representative subset of our languages. The results indicate that performance on verbs is lowest across the board. This is consistent with research on distributional semantics and verb meaning (Schwartz et al., 2015; Gerz et al., 2016). Frequency We also investigate the impact of the frequency of query words. We calculate the word frequency of English words based on Google’s Trillion Word Corpus: query words are divided in groups based on their rank – i.e., the first group contains the top 100 most frequent words, the second one contains the 101th-1000th most frequent words, etc. – and plot performance (P@1) relative to rank in Figure 3. For EN - FI, P@1 was 0 across all frequency ranks. The plot shows sensitivity to frequency for HU, but less so for ES. Homographs Since we use identical word forms (homo"
P18-1072,P08-1088,0,0.122331,"arity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018) uses additional steps to refine the induced embedding space. 6 Conclusion We investigated when u"
P18-1072,P16-1157,0,0.0607205,"value in the half-open interval [0, ∞). The correlation between BDI performance and graph similarity is strong (ρ ∼ 0.89). 5 Related work Cross-lingual word embeddings Cross-lingual word embedding models typically, unlike Conneau et al. (2018), require aligned words, sentences, or documents (Levy et al., 2017). Most approaches based on word alignments learn an explicit mapping between the two embedding spaces (Mikolov et al., 2013b; Xing et al., 2015). Recent approaches try to minimize the amount of supervision needed (Vuli´c and Korhonen, 2016; Artetxe et al., 2017; Smith et al., 2017). See Upadhyay et al. (2016) and Ruder et al. (2018) for surveys. Unsupervised cross-lingual learning Haghighi et al. (2008) were first to explore unsupervised BDI, using features such as context counts and orthographic substrings, and canonical correlation analysis. Recent approaches use adversarial learning (Goodfellow et al., 2014) and employ a discriminator, trained to distinguish between the translated source and the target language space, and a generator learning a translation matrix (Barone, 2016). Zhang et al. (2017), in addition, use different forms of regularization for convergence, while Conneau et al. (2018)"
P18-1072,P16-1024,1,0.899531,"Missing"
P18-1072,D13-1168,1,0.909528,"Missing"
P18-1072,N15-1104,0,0.328041,"Missing"
P18-1072,P17-1179,0,0.413596,"nduction Anders Søgaard♥ Sebastian Ruder♠♣ Ivan Vuli´c3 ♥ University of Copenhagen, Copenhagen, Denmark ♠ Insight Research Centre, National University of Ireland, Galway, Ireland ♣ Aylien Ltd., Dublin, Ireland 3 Language Technology Lab, University of Cambridge, UK soegaard@di.ku.dk,sebastian@ruder.io,iv250@cam.ac.uk Abstract of cross-lingual supervision at all (Barone, 2016; Conneau et al., 2018; Zhang et al., 2017). Unsupervised cross-lingual word embeddings hold promise to induce bilingual lexicons and machine translation models in the absence of dictionaries and translations (Barone, 2016; Zhang et al., 2017; Lample et al., 2018a), and would therefore be a major step toward machine translation to, from, or even between low-resource languages. Unsupervised approaches to learning crosslingual word embeddings are based on the assumption that monolingual word embedding graphs are approximately isomorphic, that is, after removing a small set of vertices (words) (Mikolov et al., 2013b; Barone, 2016; Zhang et al., 2017; Conneau et al., 2018). In the words of Barone (2016): Unsupervised machine translation—i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or com"
P18-1072,W13-3520,0,\N,Missing
P18-1084,D17-1105,0,0.0222852,"guages by optimizing a contrastive loss function. Furthermore, Rajendran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which suppor"
P18-1084,J75-4040,0,0.644684,"Missing"
P18-1084,W16-3210,0,0.0396034,"Missing"
P18-1084,E14-1049,0,0.0419552,"tions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, va"
P18-1084,N10-1011,0,0.0379506,"ing test time inference.1 1 Introduction Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vi"
P18-1084,D16-1044,0,0.0425216,"ultimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017;"
P18-1084,D15-1070,0,0.428522,"Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, various optimization methods for Deep CCA were proposed. Andrew et al. (2013) use L-BFGS (Byrd et al., 1995) over all tra"
P18-1084,N16-1022,0,0.0272562,"ot surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concept"
P18-1084,D17-1303,0,0.506495,"; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup. The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language. We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao"
P18-1084,W17-6809,1,0.882302,"Missing"
P18-1084,Q14-1023,1,0.856574,"EMB 0.582 0.160 0.306 0.407 0.164 0.285 Table 3: Results on EN and DE SimLex-999 (POS-based evaluation). All scores are Spearman’s rank correlations. INIT EMB refers to initial pre-trained monolingual word embeddings (see §6). EN-DE WIW EN-IT WIW EN-RU WIW EN DE EN IT EN RU DPCCA (A) DPCCA (B) PCCA 0.398 0.405 0.374 0.400 0.400 0.301 0.412 0.413 0.370 0.429 0.427 0.386 0.404 0.413 0.374 0.407 0.402 0.374 DCCA NOI GCCA 0.390 0.395 0.398 0.386 0.413 0.414 0.422 0.407 0.407 0.412 0.398 0.396 INIT EMB 0.321 0.278 0.321 0.361 0.321 0.385 Model more abstract than nouns (Hartmann and Søgaard, 2017; Hill et al., 2014). Considering the fact that SimLex-999 consists of 666 noun pairs, 222 verb pairs and 111 adjective pairs, this is the reason that the gains of DPCCA over the strongest baselines across the entire evaluation set are more modest (Table 4). We note again that the same patterns presented in Table 3 for EN-DE – more prominent verb and adjective gains and a smaller gain on nouns – also hold for EN-IT and EN-RU (see the supplementary material). Table 4: Results (Spearman rank correlation) of our models and the strongest baselines on Multilingual SimLex-999 (all data). a selection of strongest baseli"
P18-1084,P15-2019,0,0.0433432,"Missing"
P18-1084,J15-4004,1,0.89777,"e, we propose an effective optimization algorithm for DPCCA, inspired by the work of Wang et al. (2015b) on Deep CCA (DCCA) optimization. We evaluate our DPCCA architecture on two semantic tasks: 1) multilingual word similarity and 2) cross-lingual image description retrieval. For the former, we construct and provide to the community a new Word-Image-Word (WIW) dataset containing bilingual lexicons for three languages with shared images for 5K+ concepts. WIW is used as training data for word similarity experiments, while evaluation is conducted on the standard multilingual SimLex-999 dataset (Hill et al., 2015; Leviant and Reichart, 2015). The results reveal stable improvements over a large space of non-deep and deep CCA-style baselines in both tasks. Most importantly, 1) PCCA is overall better than other methods which do not use the additional perceptual view; 2) DPCCA outperforms PCCA, indicating the importance of nonlinear transformations modeled through DNNs; 3) DPCCA outscores DCCA, again verifying the importance of conditioning multilingual text embedding induction on the shared visual view; and 4) DPCCA outperforms two recent multi-modal bilingual models which also leverage visual informatio"
P18-1084,P16-1227,0,0.0214066,"ion. Furthermore, Rajendran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2"
P18-1084,Q15-1016,0,0.0416597,"owing values: {2,3,4,5} for number of layers, {tanh, sigmoid, ReLU} as the activation functions (we use the same activation function in all the layers of the same network), {64,128,256} for minibatch size, {0.001,0.0001} for learning rate, and {128,256} for L (the size of the output vectors). The dimensions of all mid-layers are set to the input size. We use the Adam optimizer (Kingma and Ba, 2015), with the number of epochs set to 300. For all participating models, we report test performance of the best hyperparameter on the validation set. For word similarity, following a standard practice (Levy et al., 2015; Vuli´c et al., 2017) we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa. The reported score is the average of the two halves. Similarity scores for all tasks were computed using the cosine similarity measure. 7 Results and Discussion Cross-lingual Image Description Retrieval We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retr"
P18-1084,C04-1072,0,0.016355,"cipating models, we report test performance of the best hyperparameter on the validation set. For word similarity, following a standard practice (Levy et al., 2015; Vuli´c et al., 2017) we tune all models on one half of the SimLex data and evaluate on the other half, and vice versa. The reported score is the average of the two halves. Similarity scores for all tasks were computed using the cosine similarity measure. 7 Results and Discussion Cross-lingual Image Description Retrieval We report two standard evaluation metrics: 1) Recall at 1 (R@1) scores, and 2) the sentence-level BLEU+1 metric (Lin and Och, 2004), a variant of BLEU which smooths terms for higher-order n-grams, making it more suitable for evaluating short sentences. The scores for the retrieval task with all models are summarized in Table 2. R@1 BLEU+1 Model EN→DE DE→EN EN→DE DE→EN DPCCA (Variant A) DPCCA (Variant B) DPCCA(B)+DCCA NOI (concat) DCCA NOI (Wang et al., 2015b) DCCA SDL (Chang et al., 2017) DCCA (Wang et al., 2015a) DCCAE (Wang et al., 2015a) IMG PIVOT (Gella et al., 2017) BCN (Rajendran et al., 2016) PCCA (Rao, 1969) CCA (Hotelling, 1936) GCCA (Funaki and Nakayama, 2015) NCCA (Michaeli et al., 2016) PPCCA (Mukuta and Harad"
P18-1084,W16-2360,0,0.0227027,"ran et al. (2016) extend the work of Chandar et al. (2016) and propose to use a pivot representation in multimodal multilingual setups, with English representations serving as the pivot. While these works learn shared multimodal multilingual vector spaces, we demonstrate improved performance with our models (see §7). Finally, although not directly comparable, recent work in neural machine translation has constructed models that can translate image descriptions by additionally relying on visual features of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al.,"
P18-1084,N15-1028,0,0.0190758,"tures of the image provided (Calixto and Liu, 2017; Elliott et al., 2015; Hitschler et al., 2016; Huang et al., 2016; Nakayama and Nishida, 2017, inter alia). Correlational Models CCA-based techniques support multiple views on related data: e.g., when coupled with a bilingual dictionary, input monolingual word embeddings for two different languages can be seen as two views of the same latent semantic signal. Recently, CCA-based models for bilingual text embedding induction were proposed. These models rely on the basic CCA model (Chandar et al., 2016; Faruqui and Dyer, 2014), its deep variant (Lu et al., 2015), and a CCA extension which supports more than two views (Funaki and Nakayama, 2015; Rastogi et al., 2015). In this work, we propose to use (D)PCCA, which organically supports our setup: it conditions the two (textual) views on a shared (visual) view. CCA-based methods (including PCCA) require the estimation of covariance matrices over all training data (Kessy et al., 2017). This hinders the use of DNNs with these models, as DNNs are typically trained via stochastic optimization over mini911 batches on very large training sets. To address this limitation, various optimization methods for Deep"
P18-1084,P16-4010,0,0.0190778,"ord similarity task. WIW contains three bilingual lexicons (EN-DE, EN-IT, EN-RU) with images shared between words in a lexicon entry. Each WIW entry is a triplet: an English word, its translation in DE/IT/RU, and a set of images relevant to the pair. English words were taken from the January 2017 Wikipedia dump. After removing stop words and punctuation, we extract the 6,000 most frequent words from the cleaned corpus not present in SimLex. DE/IT/RU words were obtained semiautomatically from the EN words using Google Translate. The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only. Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10 Table 1 provides a summary of the WIW dataset. The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2). 6 Experimental Setup Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower9 https://g"
P18-1084,D14-1005,0,0.408971,".1 1 Introduction Research in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu e"
P18-1084,P14-2135,0,0.0603706,"xtract the 6,000 most frequent words from the cleaned corpus not present in SimLex. DE/IT/RU words were obtained semiautomatically from the EN words using Google Translate. The images are crawled from the Bing search engine using MMFeat9 (Kiela, 2016) by querying the EN words only. Following the suggestions from the study of Kiela et al. (2016), we save the top 20 images as relevant images.10 Table 1 provides a summary of the WIW dataset. The dataset contains both concrete and abstract words, and words of different POS tags.11 This property has an influence on the image collection: similar to Kiela et al. (2014), we have noticed that images of more concrete concepts are less dispersed (see also examples from Figure 2). 6 Experimental Setup Data Preprocessing and Embeddings For the sentence-level task, all descriptions were lower9 https://github.com/douwekiela/mmfeat. Offensive words and images are manually cleaned. 11 POS tag information is taken from the NLTK toolkit for the English words. 10 Figure 2: WIW examples from each of the three bilingual lexicons. Note that the designated words can be either abstract (true), express an action (dance) or be more concrete (plant). cased and tokenized. Each s"
P18-1084,D16-1043,0,0.0323394,"Missing"
P18-1084,D15-1015,1,0.933823,"Missing"
P18-1084,J99-4009,0,0.0580764,"CA outperforms two recent multi-modal bilingual models which also leverage visual information (Gella et al., 2017; Rajendran et al., 2016). 2 Related Work This work is related to two research threads: 1) multi-modal models that combine vision and language, with a focus on multilingual settings; 2) correlational multi-view models based on CCA which learn a shared vector space for multiple views. Multi-Modal Modeling in Multilingual Settings Research in cognitive science suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011). Visual context serves as a useful crosslingual grounding signal (Bruni et al., 2014; Glavaˇs et al., 2017) due to its language invariance, even enabling the induction of word-level bilingual semantic spaces solely through tagged images obtained from the Web (Bergsma and Van Durme, 2011; Kiela et al., 2015). Vuli´c et al. (2016) combine text embeddings with visual features via simple techniques of concatenation and averaging to obtain bilingual multi-modal representations, with noted improvements over text-only embeddings on word similarity and bilingual lexicon extraction. H"
P18-1084,N15-1016,0,0.110189,"ch in multi-modal semantics deals with the grounding problem (Harnad, 1990), motivated by evidence that many semantic concepts, irrespective of the actual language, are grounded in the perceptual system (Barsalou and Wiemer-Hastings, 2005). In particular, recent studies have shown that performance on NLP tasks can be improved by joint modeling of text and vision, with multimodal and perceptually enhanced representation learning outperforming purely textual representa1 Our code and data are available at: https://github. com/rotmanguy/DPCCA. tions (Feng and Lapata, 2010; Kiela and Bottou, 2014; Lazaridou et al., 2015). These findings are not surprising, and can be explained by the fact that humans understand language not only by its words, but also by their visual/perceptual context. The ability to connect vision and language has also enabled new tasks which require both visual and language understanding, such as visual question answering (Antol et al., 2015; Fukui et al., 2016; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual"
P18-1084,Q17-1022,1,0.905072,"Missing"
P18-1084,N16-1021,0,0.351509,"; Xu and Saenko, 2016), image-to-text retrieval and text-to-image retrieval (Kiros et al., 2014; Mao et al., 2014), image caption generation (Farhadi et al., 2010; Mao et al., 2015; Vinyals et al., 2015; Xu et al., 2015), and visual sense disambiguation (Gella et al., 2016). While the main focus is still on monolingual settings, the fact that visual data can serve as a natural bridge between languages has sparked additional interest towards multilingual multi-modal modeling. Such models induce bilingual multi-modal spaces based on multi-view learning (Calixto et al., 2017; Gella et al., 2017; Rajendran et al., 2016). In this work, we propose a novel effective approach for learning bilingual text embeddings conditioned on shared visual information. This additional perceptual modality bridges the gap between languages and reveals latent connections between concepts in the multilingual setup. The shared visual information in our work takes the form of images with word-level tags or sentence-level descriptions assigned in more than one language. We propose a deep neural architecture termed Deep Partial Canonical Correlation Analysis (DPCCA) based on the Partial CCA (PCCA) method (Rao, 1969). To the best of o"
P18-1084,N15-1058,0,0.0604349,"Missing"
P18-1084,P16-2031,1,0.909024,"Missing"
P18-1084,K17-1013,1,0.893802,"Missing"
P18-1084,Q14-1006,0,0.042945,"h query. In addition, in our setup, images are not available during inference: retrieval is performed based solely on text queries. This enables a fair comparison between our model and many baseline models that cannot represent images and text in a shared space. Moreover, it allows us to test our model in the realistic setup where images are not available at test time. To avoid the use of images at retrieval time with DPCCA, we perform the retrieval on F (X) and G(Y ), rather than on F (X|Z) and G(Y |Z) (see §3.2). We use the Multi30K dataset (Elliott et al., 2016), originated from Flickr30K (Young et al., 2014) that is comprised of Flicker images described with 1-5 English descriptions per image. Multi30K adds Update parameters: WF ← WF − η∇WF , UH ← UH − η∇UH 1 ] ˆ − 2 F |H, and compute ∇VG , ∇UH Fix F |H = Σ F F |H with respect to: ] min |b1t |kG|H − F |Hk2F VG ,UH Update parameters: VG ← VG − η∇VG , UH ← UH − η∇UH end for Output: (WF , VG , UH ) German descriptions to a total of 30,014 images: most were written independently of the English descriptions, while some are direct translations. Each image is associated with one English and one German description. We rely on the original Multi30K splits"
P18-1142,W17-0401,0,0.422071,"Missing"
P18-1142,W14-4203,0,0.17361,"Missing"
P18-1142,P17-2021,0,0.0122502,"resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is"
P18-1142,P16-1231,0,0.0152094,"port LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate the effect of treebank size on the final transfer results. We conduct experiments with two well-known transition-based parsers (Nivre, 2006): (1) DeSR (Attardi et al., 2007) and (2) SyntaxNet (Andor et al., 2016; Alberti et al., 2017). The two were selected as they represent two different architectures: the former is an SVM-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this e"
P18-1142,P17-1042,0,0.015014,"ovide a different forget gate ftk for each child. Hidden layers Hidden size Input size Batch size Epochs qt = σ (Wq xt + Uq ht−1 + bq ) (5) ct = ft ct−1 + it tanh (Wc xt + Uc ht−1 + bc ) (6) ht = ot tanh(ct ) (7) In our resource-lean cross-lingual scenario the language of the training data (English) differs from that of the target (Arabic). Since TreeLSTM is a lexicalised model, we employ multilingual word embeddings, such that the words of both languages lie in the shared cross-lingual semantic space. In particular, we map English into Arabic through the iterative Procustes method devised by Artetxe et al. (2017). The results are evaluated through the Pearson correlation and the Mean Squared Error (MSE) between predicted and golden labels. Learning rate Optimiser Dropout Results and Discussion Source Selection. The results for cross-lingual parser transfer with the DeSR parser are provided in Figure 3, while the results with SyntaxNet are provided as supplemental material as they follow the same trends. The selection of the source for Nematus (NMT) TreeLSTM (STS) 2 512 160 256 12 (greed); 10 (beam) 0.8 Adam 0.2 / 0.3 2 1000 280 80 Early stopping 1−4 AdaDelta 0.1 / 0.2 1 300 512 25 5 1−2 SGD 0 Table 1:"
P18-1142,D07-1119,0,0.0979575,"Missing"
P18-1142,S17-2001,0,0.0189177,"y annotated by SyntaxNet. The data for cross-lingual STS are chosen to resemble a real-world scenario with a resource-poor target language. The training data (9,709 sentence 8 http://universaldependencies.org/ Language names are substituted in this work by their corresponding ISO 639-1 codes. A table of names and codes is provided in the supplemental material. 10 http://opus.nlpl.eu/OpenSubtitles.php 1535 9 pairs) are in English, taken from the STS benchmark, the ensemble of all the datasets from SemEval 2012-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only lan"
P18-1142,P16-1038,0,0.032079,"turned out to be useful for correcting programming scripts (Tai, 1979), evolution studies, and most notably accounting for transformations in constituency trees (Selkow, 1977). Although previous works were aware of the problem of anisomorphism in the context of syntax-based NLP applications (Ambati, 2008), to our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although th"
P18-1142,N16-1024,0,0.0196463,"limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morp"
P18-1142,P16-1078,0,0.0953751,"can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1"
P18-1142,P03-1011,0,0.115293,"they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims"
P18-1142,W15-2114,0,0.0138647,"ntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic structures) and its opposite, anisomorphism, as a probe to measuring quantitatively the extent to which syntactic tree pairs are cross-lingually compatible. We assess the varia"
P18-1142,2007.mtsummit-papers.29,0,0.07936,"esian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau i"
P18-1142,de-marneffe-etal-2014-universal,0,0.071785,"Missing"
P18-1142,P12-1066,0,0.74434,"upport to cross-lingual transfer, it also supports monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers"
P18-1142,P15-2040,0,0.51718,"e to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguist"
P18-1142,D17-1038,0,0.034432,"our knowledge we are the first to quantify it formally and to leverage it in cross-lingual NLP. For source selection, similarity metrics from prior work mostly relied on information stored in typological databases (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Deri and Knight, 2016). Otherwise, the metrics were derived empirically: they mostly concerned linear-order properties such as part-of-speech ngrams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). In domain adaptation, the selection also hinges upon topic models (Plank and Van Noord, 2011) or Bayesian Optimisation (Ruder and Plank, 2017). The metrics we defined in §2.2 are instead based on configurational properties of languages, and add another piece to the puzzle of source selection. The idea of tree processing dates back to the attempts to steer source towards target syntactic structures in statistical MT, although they were mostly limited to simple reordering steps. Gildea (2003) proposed cloning operations to relocate subtrees. Other works learned rewrite patterns in an automatic fashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner ("
P18-1142,C16-1123,1,0.925498,"Missing"
P18-1142,E17-3017,0,0.0213831,"Missing"
P18-1142,P15-2034,0,0.123715,"Missing"
P18-1142,W16-2209,0,0.0151383,"15) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of the alignment between the predicted word and each of the input words: wi,j = P (a|yi , xj ). This model is enriched with additional linguistic features on input, as proposed by Sennrich and Haddow (2016). In particular, we select the following which are proven as useful in prior work, and also relevant to our experiment: word form, POS tag, and dependency relations. These features are concatenated and fed to the encoder. Tree processing from §2.3 affects these features (and consequently the sentence representation) by changing the initial tree structure. For instance, the original tree in Figure 2a and the processed one in Figure 2c would correspond to these feature sets: Original Preprocessed ladayhim¯a ⊕ N ⊕ ROOT him¯a ⊕ N ⊕ N SUBJ D UMMY ⊕ V ⊕ ROOT ‘aˇsy¯a‘u ⊕ N ⊕ D OBJ muˇstarakatun ⊕ A ⊕"
P18-1142,P02-1040,0,0.102066,"M-based model with a polynomial kernel, whereas the latter is a feed-forward neural network with beam search based on conditional random fields. The results are evaluated in terms of LAS and UAS scores. Neural Machine Translation. For NMT, we examine whether the tree processing procedure from §2.3 can reduce anisomorphism between source and target language syntactic structures. We thus run NMT models in two settings: with and without the anisomorphism reduction procedure. For this experiment we rely on a state-of-the-art syntax-aware NMT architecture. We report its performance by BLEU scores (Papineni et al., 2002). 11 http://alt.qcri.org/semeval2017/ task1/ In particular, we use an attentional encoder-decoder network that jointly learns to translate and align words (Bahdanau et al., 2015) implemented in the Nematus suite12 (Sennrich et al., 2017). The encoder is a bidirectional gated recurrent network. For each step i, the decoder predicts the next word in output by taking as input the current hidden state hi , the previous word wi−1 and a context vector, Pn i.e., a weighted sum of all the hidden states j=1 wj · h1 . The weights are learned by a multilayer perceptron that estimates the likelihood of th"
P18-1142,D09-1086,0,0.21974,"bstantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics In this paper we address these two challenges. We propose the concept of isomorphism (i.e., identity of shapes: syntactic str"
P18-1142,P11-1157,0,0.211875,"Missing"
P18-1142,W17-0903,1,0.791524,"Missing"
P18-1142,S17-1003,1,0.825881,"Missing"
P18-1142,Q17-1020,0,0.160289,"translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variatio"
P18-1142,D09-1085,0,0.0306106,"ashion to minimize differences in the order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from"
P18-1142,J16-4004,0,0.0130895,"he order of chunks (Zhang et al., 2007) or labeled dependencies (Habash, 2007). Instead, Smith and Eisner (2009) proposed to learn jointly a translation and a loose alignment of nodes, in order to avoid enforcing the bias of the source structure. Reviving these approaches within the framework of deep learning seems crucial as far as state-of-art models depend on syntactic information (Eriguchi et al., 2016; Dyer et al., 2016). In general, our approach aims at developing and evaluating models focused on specific constructions rather than languages as a whole (Rimell et al., 2009; Bender, 2011; Rimell et al., 2016). The gist is that current models have reached a plateau in performance because they excel with frequent and simple phenomena, but they still lag behind with respect to rarer or more complex constructions. Conclusions and Future Work We have demonstrated that syntactic structures differ across languages even in well-developed annotation schemes such as Universal Dependencies. This variation stems from morphological and syntactic differences across languages. This phenomenon, which we have labeled as anismorphism, can challenge the transfer of knowledge from one language to another. We have pro"
P18-1142,N13-1126,0,0.507114,"Missing"
P18-1142,P15-1150,0,0.0924878,"Missing"
P18-1142,W15-2137,0,0.089893,"for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP. 1 Introduction Linguistic information can be transferred from resource-rich to resource-poor languages using approaches such as annotation projection, model transfer, and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content word"
P18-1142,P16-2069,0,0.0787408,"Missing"
P18-1142,E17-1034,0,0.0624443,"Missing"
P18-1142,E17-2065,1,0.884029,"Missing"
P18-1142,P16-2084,1,0.906217,"Missing"
P18-1142,Q16-1035,0,0.116694,", and/or translation (Agi´c et al., 2014). Such cross-lingual transfer may rely on syntactic information. Structured and more cross-lingually consistent than linear sequences (Ponti, 2016), syntactic information has proved useful for cross-lingual parsing (Tiedemann, 2015; Rasooli and Collins, 2017), multilingual representation learning (Vuli´c and Korhonen, 2016; Vuli´c, 2017), causal relation identification (Ponti and Korhonen, 2017), and neural machine translation (Eriguchi et al., 2016; Aharoni and Goldberg, 2017). It can also guide the generation of synthetic data for multilingual tasks (Wang and Eisner, 2016). Universal Dependencies (UD) (Nivre et al., 2016) is a collection of treebanks for a variety of languages, annotated with a scheme optimised for knowledge transfer. The tag sets are languageindependent and there are direct links between content words. This reduces the variation of dependency trees, because content words are crosslingually more stable than function words (Croft et al., 2017), and benefits semantically-oriented applications (de Marneffe et al., 2014)1 . Importantly, although UD is tailored to offer support to cross-lingual transfer, it also supports monolingual applications wit"
P18-1142,I08-3008,0,0.202796,"12-2017 STS tasks. The test data (250 sentence pairs) come from Task 1 of SemEval 2017 (Cer et al., 2017); target language is Arabic.11 All the sentence pairs are associated with a label ranging from 0 (dissimilarity) to 5 (equivalence). 4 Methodology Cross-lingual Dependency Parsing. To assess if the anisomorphism metrics devised in §2.2 are reliable in finding compatible languages for knowledge transfer, we use the Jaccard index of the morphological feature sets as a criterion to choose source languages for cross-lingual parser transfer. We adopt the variant of delexicalised model transfer (Zeman and Resnik, 2008) for this task. This technique ignores lexicalised features and leverages only language-independent features instead. For each language from a sample of 7 (typologically diverse) targets, we report LAS scores using three different source languages: (1) the highestranked source according to the Jaccard index; (2) a source sampled from the middle of the list ranked by the Jaccard indices; (3) a very dissimilar language sampled from the bottom of the ranked list. The total number of sentences used for training corresponds to the smallest of the three source language treebanks in order to isolate"
P18-1142,D15-1213,0,0.471468,"rts monolingual applications with a quality comparable to languagespecific annotations (Vincze et al., 2017, inter alia). Despite the careful design of this resource, there are still substantial variations in morphological richness and strategies employed to express the same syntactic constructions across languages. These variations posit challenges for syntax-based knowledge transfer. The first challenge is how to match the source and target languages so that differences are minimised. The common criteria are based on the typology of word order (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) or part-of-speech n-grams (Rosa and Zabokrtsky, 2015; Agi´c, 2017). The second one is how to make knowledge transfer effective by harmonising syntactic trees (Smith and Eisner, 2009; Vilares et al., 2016) as to enable a better correspondence between source and target nodes. 1 It is controversial whether it improves parsing: e.g., Groß and Osborne (2015, inter alia) argue against whereas Attardi et al. (2015, inter alia) argue in favour. 1531 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1531–1542 c Melbourne, Australia, July 15 -"
P18-1142,W07-0401,0,0.107951,"Missing"
P18-2018,W14-4340,0,0.332003,"ialogue management component to choose the next system response (Su et al., 2016, 2017). A large number of DST models (Wang and Lemon, 2013; Sun et al., 2016; Liu and Perez, 2017; Vodol´an et al., 2017, inter alia) treat SLU as a separate problem: the detached SLU modules are a dependency for such systems as they require large amounts of annotated training data. Moreover, recent research has demonstrated that systems which treat SLU and DST as a single problem have proven superior to those which decouple them (Williams et al., 2016). Delexicalisation-based models, such as the one proposed by (Henderson et al., 2014a,b) offer unparalleled generalisation capability. These models use exact matching to replace occurrences of slot names and values with generic tags, allowing them to share parameters across all slot values. This allows them to deal with slot values not seen during training. However, their downside is shifting the problem of dealing with linguistic variation back to the system designers, who have to craft semantic lexicons to specify rephrasings for ontology values. Examples of such rephrasings are [cheaper, affordable, cheaply] for slot-value pair FOOD = CHEAP , or [with internet, has interne"
P18-2018,J15-4004,0,0.0386274,"edgments We thank the anonymous reviewers for their helpful and insightful suggestions. We are also grateful to ´ S´eaghdha and Steve Young for many Diarmuid O fruitful discussions. DST as Downstream Evaluation All of the experiments show that the use of semantically specialised vectors benefits DST performance. The scale of these gains is robust across all experiments, regardless of language or the employed belief state update mechanism. So far, it has been hard to use the DST task as a proxy for measuring the correlation between word vectors’ intrinsic performance (in tasks like SimLex-999 (Hill et al., 2015)) and their usefulness for downstream language understanding tasks. Having eliminated the rule-based update from the NBT model, we make our evaluation framework publicly available in hope that DST performance can serve as a useful tool for measuring the correlation between intrinsic and extrinsic performance of word vector collections. 5 Conclusion This paper proposed an extension to the Neural Belief Tracking (NBT) model for Dialogue State Tracking (DST) (Mrkˇsi´c et al., 2017a). In the previous NBT model, system designers have to tune the belief state update mechanism manually whenever the m"
P18-2018,E17-1001,0,0.0239068,"tract turn-level user goals, which are then incorporated into the belief state, the system’s internal probability distribution over possible dialogue states. The dialogue states are defined by the domainspecific ontology: it enumerates the constraints the users can express using a collection of slots (e.g. price range) and their slot values (e.g. cheap, expensive for the aforementioned slots). The belief state is used by the downstream dialogue management component to choose the next system response (Su et al., 2016, 2017). A large number of DST models (Wang and Lemon, 2013; Sun et al., 2016; Liu and Perez, 2017; Vodol´an et al., 2017, inter alia) treat SLU as a separate problem: the detached SLU modules are a dependency for such systems as they require large amounts of annotated training data. Moreover, recent research has demonstrated that systems which treat SLU and DST as a single problem have proven superior to those which decouple them (Williams et al., 2016). Delexicalisation-based models, such as the one proposed by (Henderson et al., 2014a,b) offer unparalleled generalisation capability. These models use exact matching to replace occurrences of slot names and values with generic tags, allowi"
P18-2018,N16-1018,1,0.878689,"Missing"
P18-2018,P17-1163,1,0.870844,"Missing"
P18-2018,Q17-1022,1,0.918024,"Missing"
P18-2018,P15-2070,0,0.023392,"Missing"
P18-2018,D14-1162,0,0.0811946,"tup We compare three belief state update mechanisms (rule-based vs. two statistical ones) fixing all other NBT components as suggested by Mrkˇsi´c et al. (2017a): the betterperforming NBT-CNN variant is used, trained by Adam (Kingma and Ba, 2015) with dropout (Srivastava et al., 2014) of 0.5, gradient clipping, batch size of 256, and 400 epochs. All model hyperparameters were tuned on the validation sets. Word Vectors To test the model’s robustness, we use a variety of standard word vectors from prior work. For English, following Mrkˇsi´c et al. (2017a) we use 1) distributional GLOVE vectors (Pennington et al., 2014), and 2) specialised PARAGRAM SL 999 vectors (Wieting et al., 2015), obtained by injecting similarity constraints from the Paraphrase Database (Pavlick et al., 2015) into GLOVE. For Italian and German, we compare to the work of Vuli´c et al. (2017), who report state-of-the-art DST scores on the Italian and German WOZ 2.0 datasets. In this experiment, we train the models using distributional skip-gram vectors with a large vocabulary (labelled DIST in Table 2). Subsequently, we compare them to models trained using word vectors specialised using similarity constraints derived from language-specif"
P18-2018,W17-5518,0,0.0231274,"Missing"
P18-2018,P16-1230,1,0.894494,"Missing"
P18-2018,E17-2033,0,0.0371381,"Missing"
P18-2018,P17-1006,1,0.889397,"Missing"
P18-2018,W13-4067,0,0.0561383,"nguage Understanding (SLU) modules to extract turn-level user goals, which are then incorporated into the belief state, the system’s internal probability distribution over possible dialogue states. The dialogue states are defined by the domainspecific ontology: it enumerates the constraints the users can express using a collection of slots (e.g. price range) and their slot values (e.g. cheap, expensive for the aforementioned slots). The belief state is used by the downstream dialogue management component to choose the next system response (Su et al., 2016, 2017). A large number of DST models (Wang and Lemon, 2013; Sun et al., 2016; Liu and Perez, 2017; Vodol´an et al., 2017, inter alia) treat SLU as a separate problem: the detached SLU modules are a dependency for such systems as they require large amounts of annotated training data. Moreover, recent research has demonstrated that systems which treat SLU and DST as a single problem have proven superior to those which decouple them (Williams et al., 2016). Delexicalisation-based models, such as the one proposed by (Henderson et al., 2014a,b) offer unparalleled generalisation capability. These models use exact matching to replace occurrences of slot nam"
P18-2018,E17-1042,1,0.87772,"Missing"
P18-2018,Q15-1025,0,0.0594389,"o statistical ones) fixing all other NBT components as suggested by Mrkˇsi´c et al. (2017a): the betterperforming NBT-CNN variant is used, trained by Adam (Kingma and Ba, 2015) with dropout (Srivastava et al., 2014) of 0.5, gradient clipping, batch size of 256, and 400 epochs. All model hyperparameters were tuned on the validation sets. Word Vectors To test the model’s robustness, we use a variety of standard word vectors from prior work. For English, following Mrkˇsi´c et al. (2017a) we use 1) distributional GLOVE vectors (Pennington et al., 2014), and 2) specialised PARAGRAM SL 999 vectors (Wieting et al., 2015), obtained by injecting similarity constraints from the Paraphrase Database (Pavlick et al., 2015) into GLOVE. For Italian and German, we compare to the work of Vuli´c et al. (2017), who report state-of-the-art DST scores on the Italian and German WOZ 2.0 datasets. In this experiment, we train the models using distributional skip-gram vectors with a large vocabulary (labelled DIST in Table 2). Subsequently, we compare them to models trained using word vectors specialised using similarity constraints derived from language-specific morphological rules (labelled SPEC in Table 2). 4 Results and Di"
P18-2101,andersen-etal-2008-bnc,0,0.0144548,"the features of the broader term. We include this weighted cosine in both directions. • The proportion of shared unique contexts, compared to the number of contexts for one word. This measure is able to capture whether one of the words appears in a subset of the contexts, compared to the other word. This feature is also directional and is therefore included in both directions. We build the sparse distributional word vectors from two versions of the British National Corpus (Leech, 1992). The first counts contexts simply based on a window of size 3. The second uses a parsed version of the BNC (Andersen et al., 2008) and extracts contexts based on dependency relations. In both cases, the features are weighted using pointwise mutual information. Each of the five features is calculated separately for the two vector spaces, resulting in 10 corpus-based features. We integrate them into the network by conditioning the hidden layer h on this vector: h = tanh(Wh d + Wx x + bh ) (11) where x is the feature vector of length 10 and Wx is the corresponding weight matrix. Additional Supervision (AS). Methods such as retrofitting (Faruqui et al., 2015), ATTRACT- REPEL (Mrkˇsi´c et al., 2017) and Poincar´e embeddings ("
P18-2101,Q17-1010,0,0.0805041,"Missing"
P18-2101,N15-1184,0,0.0455295,"Missing"
P18-2101,D17-1185,0,0.658099,"Missing"
P18-2101,P15-2020,1,0.924375,"Missing"
P18-2101,E14-4008,0,0.153031,"al is to make fine-grained assertions regarding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2 w2 Figure 1: Supervis"
P18-2101,P16-1226,0,0.291171,"Missing"
P18-2101,P14-2050,0,0.0887418,"e pairs. However, only binary labels are attached to all word pairs, whereas the task 640 requires predicting a graded score. Initial experiments with optimising the network to predict the minimal and maximal possible score for these cases did not lead to improved performance. Therefore, we instead make use of a hinge loss function that optimises the network to only push these examples to the correct side of the decision boundary: L= X max((y − yˆ)2 − ( i S − R)2 , 0) 2 (12) Evaluation SDSN Training Setup. As input to the SDSN network we use 300-dimensional dependency-based word embeddings by Levy and Goldberg (2014). Layers m1 and m2 also have size 300 and layer h has size 100. For regularisation, we apply dropout to the embeddings with p = 0.5. The margin R is set to 1 for the supervised pre-training stage. The model is optimised using AdaDelta (Zeiler, 2012) with learning rate 1.0. In order to control for random noise, we run each experiment with 10 different random seeds and average the results. Our code and detailed configuration files will be made available online.1 Evaluation Data. We evaluate graded lexical entailment on the HyperLex dataset (Vuli´c et al., 2017) which contains 2,616 word pairs in"
P18-2101,N15-1098,0,0.0609825,"amples that are not yet on the correct side of the boundary, including a margin. This prevents us from penalising the model for predicting a score with slight variations, as the extracted examples are not annotated with sufficient granularity. When optimising the model, we first perform one pretraining pass over these additional word pairs before proceeding with the regular training process. 4 Random DEV TEST Table 1: Graded lexical entailment detection results on the random and lexical splits of the HyperLex dataset. We report Spearman’s ρ on both validation and test sets. split, proposed by Levy et al. (2015), there is no lexical overlap between training and test subsets. This prevents the effect of lexical memorisation, as supervised models tend to learn an independent property of a single concept in the pair instead of learning a relation between the two concepts. In this setup training, validation, and test sets contain 1133, 85, and 269 word pairs, respectively.2 Since plenty of related research on lexical entailment is still focused on the simpler binary detection of asymmetric relations, we also run experiments on the large binary detection HypeNet dataset (Shwartz et al., 2016), where the S"
P18-2101,Q17-1022,1,0.890193,"Missing"
P18-2101,P15-2070,0,0.0457219,"Missing"
P18-2101,D14-1162,0,0.0835971,"Missing"
P18-2101,W14-1608,1,0.878449,"e trouble encoding features such as word frequency, or the number of unique contexts the word has appeared in. This information becomes important when deciding whether one word entails another, as the system needs to determine when a concept is more general and subsumes the other. We construct classical sparse distributional word vectors and use them to extract 5 unique features for every word pair, to complement the features extracted from neural embeddings: • Regular cosine similarity between the sparse distributional vectors of both words. • The sparse weighted cosine measure, described by Rei and Briscoe (2014), comparing the weighted ranks of different distributional contexts. The measure is directional and assigns more importance to the features of the broader term. We include this weighted cosine in both directions. • The proportion of shared unique contexts, compared to the number of contexts for one word. This measure is able to capture whether one of the words appears in a subset of the contexts, compared to the other word. This feature is also directional and is therefore included in both directions. We build the sparse distributional word vectors from two versions of the British National Cor"
P18-2101,D17-1162,1,0.67903,"(Mikolov et al., 2013), making the resulting vector space reflect (a broad relation of) semantic relatedness but unsuitable for lexical entailment (Vuli´c et al., 2017). The mapping stage allows the network to learn a transformation function from the general skip-gram embeddings to a task-specific space for lexical entailment. In addition, the two weight matrices enable asymmetric reasoning, allowing the network to learn separate mappings for hyponyms and hypernyms. We then use a supervised composition function for combining the two representations and returning a confidence score as output. Rei et al. (2017) described a generalised version of cosine similarity for metaphor detection, constructing a supervised operation and learning individual weights for each 639 feature. We apply a similar approach here and modify it to predict a relation score: d = m1 m2 (7) h = tanh(Wh d + bh ) (8) y = S · σ(a(Wy h + by )) (9) where Wh , bh , a, Wy and by are trainable parameters. The annotated labels of lexical relations are generally in a fixed range, therefore we base the output function on logistic regression, which also restricts the range of the predicted scores. by allows for the function to be shifted"
P18-2101,D16-1234,0,0.0941054,"Missing"
P18-2101,E17-1007,0,0.451918,"arding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2 w2 Figure 1: Supervised directional similarity network (SDSN) fo"
P18-2101,J17-4004,1,0.795247,"Missing"
P18-2101,C14-1212,0,0.199844,"l entailment, the goal is to make fine-grained assertions regarding the directional hierarchical semantic relationships between concepts (Vuli´c et al., 2017). The task is grounded in theories of concept (proto)typicality and category vagueness from cognitive science (Rosch, 1975; Kamp and Partee, 1995), and aims at answering the following question: “To what degree is X a type of Y ?”. It quantifies the degree of lexical entailment instead of providing only a binary yes/no decision on the relationship between the concepts X and Y , as done in hypernymy detection tasks (Kotlerman et al., 2010; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2017). Graded lexical entailment provides finer-grained 638 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 638–643 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics y embeddings come from a standard distributional vector space, pre-trained on a large unannotated corpus, and are not fine-tuned during training. An element-wise gating operation is then applied to each word, conditioned on the other word: h x d m1 m2 ~ w1 ~ w g1 g2 w1 2"
P18-2101,Q15-1025,0,0.116782,"Missing"
P19-1070,D18-1214,0,0.335056,"Missing"
P19-1070,P17-1042,0,0.649125,"olingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of"
P19-1070,P18-1073,0,0.0782391,"irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision ("
P19-1070,J82-2005,0,0.676033,"Missing"
P19-1070,D18-1024,0,0.240154,"r alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom"
P19-1070,D18-1269,0,0.132185,"m seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models, not demanding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et"
P19-1070,D18-1062,0,0.0575484,"Missing"
P19-1070,D18-1027,0,0.560132,"ding any bilingual supervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well o"
P19-1070,P18-1128,0,0.0627467,"Missing"
P19-1070,E14-1049,0,0.141575,"S and XT . In the general case, we learn two projection matrices WL1 and WL2 : XCL = XL1 WL1 ∪ XL2 WL2 . Many models, however, learn to directly project XL1 to XL2 , i.e., WL2 = I and XCL = XL1 WL1 ∪ XL2 . 2.2 Projection-Based CLE Models While supervised models employ external dictionaries, unsupervised models automatically induce seed translations using diverse strategies: adversarial learning (Conneau et al., 2018a), similaritybased heuristics (Artetxe et al., 2018b), PCA (Hoshen and Wolf, 2018), and optimal transport (Alvarez-Melis and Jaakkola, 2018). Canonical Correlation Analysis (CCA). Faruqui and Dyer (2014) use CCA to project XL1 and XL2 obtain monolingual vectors) and b) they do not require any multilingual corpora, they lend themselves to a wider spectrum of languages than the alternatives (Ruder et al., 2018b). XL1 , XL2 ← monolingual embeddings of L1 and L2 D ← initial word translation dictionary for each of n iterations do XS , XT ← lookups for D in XL1 , XL2 WL1 ← arg minW kXS W − XT k2 WL2 ← arg minW kXT W − XS k2 X0 L1 ← XL1 WL1 ; X0 L2 ← XL2 WL2 D1,2 ← nn(X0 L1 , XL2 ); D2,1 ← nn(X0 L2 , XL1 ) D ← D ∪ (D1,2 ∩ D2,1 ) return: WL1 (and/or WL2 ) into a shared space XCL . Projection matrices"
P19-1070,S18-2010,0,0.0429158,"of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text classification, a large body of recent work is judged exclusively on the task of bilingual lexicon induction (BLI). This limits our understanding of CLE methodology as: 1) BLI is an intrinsic task, and agreement between BLI and downstream performance has been challenged (Ammar et al., 2016; Bakarov et al., 2018); 2) BLI is not the main motivation for inducing cross-lingual embedding spaces— rather, we seek to exploit CLEs to tackle multilinguality and downstream language transfer (Ruder et al., 2018b). In other words, previous research does not evaluate the true capacity of projectionbased CLE models to support cross-lingual NLP. It is unclear whether and to which extent BLI performance of (projection-based) CLE models correlates with various downstream tasks of different types. At the moment, it is virtually impossible to directly compare all recent projection-based CLE models on BLI due to the lack"
P19-1070,P15-1119,0,0.0573364,"em in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from com"
P19-1070,Q17-1010,0,0.431645,"e.g., (in)appropriate evaluation metrics and lack of significance testing. Language Pairs. Our evaluation comprises eight languages: Croatian (HR), English (EN), Finnish (FI), French (FR), German (DE), Italian (IT), Russian (RU), and Turkish (TR). For diversity, we selected two languages from three different IndoEuropean branches: Germanic (EN, DE), Romance (FR, IT), and Slavic (HR, RU); as well as two nonIndo-European languages (FI, TR). From these, we create a total of 28 language pairs for evaluation. Monolingual Embeddings. Following prior work, we use 300-dimensional fastText embeddings (Bojanowski et al., 2017)6 , pretrained on complete Wikipedias of each language. We trim all vocabularies to the 200K most frequent words. Translation Dictionaries. We automatically created translation dictionaries using Google Translate, similar to prior work (Conneau et al., 2018a). We selected the 20K most frequent English words and automatically translated them to the other seven languages. We retained only tuples for which all translations were unigrams found in vocabularies of respective monolingual embedding spaces, leaving us with ≈7K tuples. We reserved 5K tuples created from the more frequent English words f"
P19-1070,P14-1006,0,0.272128,"ks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that r"
P19-1070,D15-1075,0,0.0199549,"txe and Schwenk, 2018; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then eval"
P19-1070,P17-1152,0,0.0156855,"0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant dif"
P19-1070,D18-1043,0,0.168294,"upervision (Conneau et al., 2018a; Artetxe et al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While"
P19-1070,D15-1127,0,0.190112,"Missing"
P19-1070,D18-1063,0,0.0729896,"ally attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated o"
P19-1070,D18-1330,0,0.452523,"I due to the lack of a common evaluation protocol: different papers consider different language pairs and employ different training and evaluation dictionaries. Furthermore, there is a surprising lack of testing of BLI results for statistical significance. The mismatches in evaluation yield partial conclusions and inconsistencies: on the one hand, some unsupervised models (Artetxe et al., 2018b; Hoshen and Wolf, 2018) reportedly outperform competitive supervised CLE models (Artetxe et al., 2017; Smith et al., 2017). On the other hand, the most recent supervised approaches (Doval et al., 2018; Joulin et al., 2018) report performances surpassing the best unsupervised models. is easily obtainable for most language pairs.2 Therefore, despite the attractive zero-supervision setup, we see unsupervised CLE models practically justified only if such models can, unintuitively, indeed outperform their supervised competition. Contributions. We provide a comprehensive comparative evaluation of a wide range of stateof-the-art—both supervised and unsupervised— projection-based CLE models. Our benchmark encompasses BLI and three cross-lingual (CL) downstream tasks of different nature: document classification (CLDC),"
P19-1070,D18-1047,0,0.174622,"akly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream task"
P19-1070,D18-1101,0,0.0224836,"t al., 2018b, inter alia). Being conceptually attractive, such weakly supervised and unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 20"
P19-1070,C12-1089,0,0.765619,"embeddings (CLEs). CLE models learn vectors of words in two or more languages and represent them in a shared cross-lingual word vector space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 ,"
P19-1070,D18-1549,0,0.0266683,"anguage. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): t"
P19-1070,P15-1027,0,0.0610819,"d bipartite weighted graph G = (E, VL1 ∪ VL2 ) with edges E = VL1 × VL2 . By drawing embeddings from a Gaussian distribution and normalizing them, the weight of each edge (i, j) ∈ E is shown to correspond to the cosine similarity between vectors. In the E-step, a maximal bipartite matching is found on the sparsified graph using the Jonker-Volgenant algorithm (Jonker and Volgenant, 1987). In the M-step, a better projection WL1 is learned by solving the Procrustes problem. Ranking-Based Optimization (RCSLS). Instead of minimizing the Euclidean distance, Joulin et al. (2018) follow earlier work (Lazaridou et al., 2015) and maximize a ranking-based objective, specifically cross-domain similarity local scaling (CSLS; Conneau et al., 2018a), between the XS WL1 and XT . CSLS is an extension of cosine similarity commonly used for BLI inference. Let r(xkL1 W, XL2 ) be the average cosine similarity of the projected source vector with its N nearest neighbors from XL2 . Inversely, let r(xkL2 , XL1 W) be the average cosine similarity of a target vector with its N nearest neighbors from the projected source space XL1 W. By relaxing the orthogonality constraint on WL1 , maximization of relaxed CSLS (dubbed RCSLS) becom"
P19-1070,E17-1072,0,0.0207304,"ual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhan"
P19-1070,W15-1521,0,0.103779,"nguages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections"
P19-1070,D17-1269,0,0.0403092,"Missing"
P19-1070,D18-1042,1,0.877682,"c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it has also been shown that reliable projections can be bootstrapped from small dictionaries of 50–100 pairs (Vuli´c and Korhonen, 2016; Zhang et al., 2016), identical strings and cognates (Smith et al., 2017; Søgaard et al., 2018), and shared numerals (Artetxe et al., 2017). Moreover, recent work has leveraged topological similarities between monolingual vector spaces to introduce fully unsupervised projection-based CLE models,"
P19-1070,P15-1165,0,0.135665,"Missing"
P19-1070,P18-1072,1,0.840679,"Missing"
P19-1070,P16-1157,0,0.0339789,"Procrustes model (P ROC -B, see §2.2) and show it is competitive across the board. We find that overfitting to BLI may severely hurt downstream performance, warranting the coupling of BLI experiments with downstream evaluations in order to paint a more informative picture of CLE models’ properties. 2 Projection-Based CLEs: Methodology In contrast to more recent unsupervised models, CLE models typically require bilingual signal: aligned words, sentences, or documents. CLE models based on sentence and document alignments have been extensively studied in previous work (Vuli´c and Korhonen, 2016; Upadhyay et al., 2016; Ruder et al., 2018b). Current CLE research is almost exclusively focused on projection-based CLE models; they are thus also the focus of our study.3 Supervised projection-based CLEs require merely small-sized translation dictionaries (up to a few thousand word pairs) and such bilingual signal 711 2 We argue that, if acquiring a few thousand word translation pairs is a challenge, one probably deals with a truly underresourced language for which it would be difficult to obtain reliable monolingual embeddings in the first place. Furthermore, there are initiatives in typological linguistics rese"
P19-1070,P16-1024,1,0.936437,"Missing"
P19-1070,N18-1101,0,0.0184084,"; Lample and Conneau, 2019), but rather to provide means to analyze properties and relative performance of diverse CLE models in a downstream language understanding task. Avg 0.561 0.607 0.613 0.615 0.614 0.376 0.390 0.504 0.534 0.543 0.532 0.556 0.357 0.363 0.534 0.568 0.568 0.573 0.536 0.387 0.387 0.544 0.585 0.593 0.599 0.579 0.378 0.399 0.536 0.574 0.579 0.580 0.571 0.374 0.385 0.604 0.611 0.580 0.427* 0.613 0.536 0.510 0.383* 0.534 0.359* 0.400* 0.359* 0.574 0.363* 0.572 0.376* 0.581 0.467 0.516 0.386 Unsupervised Large training corpora for NLI exist only in English (Bowman et al., 2015; Williams et al., 2018). Recently, Conneau et al. (2018b) released a multilingual XNLI corpus created by translating the development and test portions of the MultiNLI corpus (Williams et al., 2018) to 15 other languages. Evaluation Setup. XNLI covers 5 out of 8 languages from our BLI evaluation: EN, DE, FR, RU, and TR. Our setup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2"
P19-1070,N15-1104,0,0.527117,"Missing"
P19-1070,D18-1268,0,0.0814185,"nd unsupervised CLEs have recently taken the field by storm (Grave et al., 2018; Dou Sebastian is now affiliated with DeepMind. 1 In the literature the methods are sometimes referred to as mapping-based CLE approaches or offline approaches. 710 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 710–721 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2018; Doval et al., 2018; Hoshen and Wolf, 2018; Ruder et al., 2018a; Kim et al., 2018; Chen and Cardie, 2018; Mukherjee et al., 2018; Nakashole, 2018; Xu et al., 2018; Alaux et al., 2019). Producing the same end result—a shared crosslingual vector space—all CLE models are directly comparable, regardless of modelling assumptions and supervision requirements. Therefore, they can support exactly the same groups of tasks. Yet, a comprehensive evaluation of recent CLE models is missing. Limited evaluations impede comparative analyses and may lead to inadequate conclusions, as models are commonly trained to perform well on a single task. While early CLE models (Klementiev et al., 2012; Hermann and Blunsom, 2014) were evaluated on downstream tasks like text class"
P19-1070,C00-2137,0,0.327675,"etup is straightforward: we train a well-known robust neural NLI model, Enhanced Sequential Inference Model (ESIM; Chen et al., 2017)12 on the large English MultiNLI corpus, using EN word embeddings from a shared EN–L2 (L2 ∈ {DE, FR, RU, TR }) embedding space. We then evaluate the model on the L2 portion of the XNLI by feeding L2 vectors from the shared space. 13 1K 5K 1K 3K 5K 1K 5K EN – DE EN – FR EN – TR EN – RU V EC M AP M USE ICP GWA Table 3: XNLI performance (test set accuracy). Bold: highest scores, with mutually insignificant differences according to the non-parametric shuffling test (Yeh, 2000). Asterisks denote language pairs for which CLE models could not yield successful runs in the BLI task. are significant differences between BLI and XNLI performance across language pairs—while we observe much better BLI performance for EN–DE and EN– FR compared to EN – RU and especially EN – TR , XNLI performance of most models for EN–RU and EN – TR surpasses that for EN – FR and is close to that for EN–DE. While this can be an artifact of the XNLI dataset creation, we support these observations for invidivual language pairs by measuring an overall Spearman correlation of only 0.13 between BLI"
P19-1070,N16-1156,0,0.132294,"or space where words with similar meanings obtain similar vectors, irrespective of their language. Owing to this property, CLEs hold promise to support cross-lingual NLP by enabling multilingual modeling of meaning and facilitating cross-lingual transfer for downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a con"
P19-1070,D13-1141,0,0.0623751,"downstream NLP tasks and under-resourced ? languages. CLEs are used as (cross-lingual) knowledge sources in range of tasks, such as bilingual lexicon induction (Mikolov et al., 2013), document classification (Klementiev et al., 2012), information retrieval (Vuli´c and Moens, 2015), dependency parsing (Guo et al., 2015), sequence labeling (Zhang et al., 2016; Mayhew et al., 2017), and machine translation (Artetxe et al., 2018c; Lample et al., 2018), among others. Earlier work typically induces CLEs by leveraging bilingual supervision from multilingual corpora aligned at the level of sentences (Zou et al., 2013; Hermann and Blunsom, 2014; Luong et al., 2015, inter alia) and documents (Søgaard et al., 2015; Vuli´c and Moens, 2016; Levy et al., 2017, inter alia). A recent trend are the so-called projectionbased CLE models1 , which post-hoc align pretrained monolingual embeddings. Their popularity stems from competitive performance coupled with a conceptually simple design, requiring only cheap bilingual supervision (Ruder et al., 2018b): they demand word-level supervision from seed translation dictionaries, spanning at most several thousand word pairs (Mikolov et al., 2013; Huang et al., 2015), but it"
P19-1070,P17-1179,0,\N,Missing
P19-1310,D18-1214,0,0.0587474,"Missing"
P19-1310,P18-1073,0,0.124612,"he copyright holder; see https:// www.jw.org/en/terms-of-use/. For all practical purposes their custom terms of use are very closely aligned with the more well-known CC2 We acknowledge the anonymous area chair who contributed this valuable argument as part of their meta-review. 3205 EN ET HR MR MT EN ET HR MR MT – 0.314 0.269 0.094 0.131 0.280 – 0.334 0.144 0.206 0.254 0.302 – 0.112 0.164 0.0 0.001 0.002 – 0.141 0.001 0.0 0.0 0.001 – Table 2: BLI results (MRR scores) on a small subset of JW300 language pairs. The scores with the best-performing unsupervised cross-lingual word embedding model (Artetxe et al., 2018) are in gray cells over the main diagonal; the scores with a simple supervised method (Smith et al., 2017) are below the main diagonal. Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adv"
P19-1310,Q17-1010,0,0.0299646,"lingual lexicon induction task (BLI).4 For the demonstration purposes, we work with all pairs from the following language set: English (EN), Estonian (ET), Croatian (HR), Marathi (MR), and Maltese (MT). Our seed bilingual dictionaries are extracted from the JW300 corpora by taking the most probable target translation for each source word from IBM1-based word translation tables. Following prior work, we use the 5K most frequent translation pairs from training, while the next 2K pairs are used for testing. We use 300-dim monolingual fastText embeddings pretrained on Wikipedia for all languages (Bojanowski et al., 2017),5 but the same trends are observed with other monolingual embeddings. The results in terms of Mean Reciprocal Rank (MRR) are summarized in Table 2. The BLI results are straightforward to interpret: for all experimental runs a simple supervised model with its supervision extracted from the JW300 corpus outperforms its unsupervised competition, further confirming the findings of Glavaˇs et al. (2019). The unsupervised model is even unable to converge for most language pairs, yielding extremely low MRR scores. The scores on another test set (Conneau et al., 2018) for EN - ET and EN - HR also fav"
P19-1310,D18-1024,0,0.0226289,"set of JW300 language pairs. The scores with the best-performing unsupervised cross-lingual word embedding model (Artetxe et al., 2018) are in gray cells over the main diagonal; the scores with a simple supervised method (Smith et al., 2017) are below the main diagonal. Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adversarial training (Conneau et al., 2018), monolingual similarity distributions (Artetxe et al., 2018) or PCA projection similarities (Hoshen and Wolf, 2018), and then learn (gradually refined) projections of two monolingual embedding spaces into a shared cross-lingual space (by also iteratively refining the seed dictionary). Such models hold promise to support crosslingual representation learning for resource-poor language pairs. However, besides their problems with training diverg"
P19-1310,P11-1061,0,0.196274,"-of-speech projection. 1 Introduction In natural language processing (NLP) the rule of thumb is that if we possess some parallel data for a low-resource target language, then we can yield feasible basic tools such as part-of-speech taggers for that language. Without such distant supervision, this task and many others remain unattainable, leaving the majority of languages in the world without basic language technology. Parallel data features a prominent role in building multilingual word representations (Ruder et al., 2017), annotation projection for parts-of-speech and syntactic dependencies (Das and Petrov, 2011; Tiedemann, 2014) and naturally machine translation. The shortage of parallel data in turn creates a bottleneck in cross-lingual processing: without parallel sentences, we cannot yield usable models, nor can we robustly evaluate them, if even just approximately (cf. Agi´c et al. 2017). This absence has over the recent years materialized the proxy fallacy, whereby intended low-resource methods are tested by proxy, exclusively on resource-rich languages, because of the absence of test data or the lack of effort to produce it for approximate evaluation. We seek to alleviate these issues by a sig"
P19-1310,P19-1070,1,0.871609,"Missing"
P19-1310,W19-3621,0,0.0272487,"s, words, and alignments, as well as an illustration of their distributions. Counts are reported for languages with at least one non-empty alignment to another language. Some languages have multiple datasets, e.g. different scripts, sign language. Moreover, the ideological bias of JW300 is fairly well-defined. In that sense, while bias may invalidate the use of our corpus in some application areas, we argue that a wide-coverage collection of parallel data with known bias may in fact be valuable for research on bias in NLP (Bolukbasi et al., 2016; Caliskan et al., 2017; Dev and Phillips, 2019; Gonen and Goldberg, 2019), especially in multilingual settings (Lauscher and Glavaˇs, 2019).2 JW300 excels in low-resource language coverage. For example, OPUS offers over 100 million English-German parallel sentences, and JW300 only 2.1 million. However, in another example, for Afrikaans-Croatian the counts are 300 thousand in OPUS and 990 thousand in JW300, and moreover, the OPUS data for this language pair contains only Linux localizations. Availability. Our dataset is freely available for all non-commercial use. The exact terms of use are provided by the copyright holder; see https:// www.jw.org/en/terms-of-use/."
P19-1310,D18-1043,0,0.0204819,". Better performance for each pair in bold. BY-NC-SA license.3 3 3.1 Experiments Cross-lingual word embedding induction A recent trend in cross-lingual word embedding induction are fully unsupervised projection-based methods that learn on the basis of monolingual data only (Conneau et al., 2018; Alvarez-Melis and Jaakkola, 2018; Chen and Cardie, 2018, inter alia). The main idea is to construct a seed bilingual dictionary in an unsupervised fashion relying on adversarial training (Conneau et al., 2018), monolingual similarity distributions (Artetxe et al., 2018) or PCA projection similarities (Hoshen and Wolf, 2018), and then learn (gradually refined) projections of two monolingual embedding spaces into a shared cross-lingual space (by also iteratively refining the seed dictionary). Such models hold promise to support crosslingual representation learning for resource-poor language pairs. However, besides their problems with training divergence (Søgaard et al., 2018), a recent empirical study (Glavaˇs et al., 2019) has demonstrated that even most robust projectionbased unsupervised models cannot match the performance of projection-based methods which require only 1K-5K seed translation pairs. The largesca"
P19-1310,D18-1330,0,0.0604572,"Missing"
P19-1310,L18-1293,0,0.0562168,"Missing"
P19-1310,2005.mtsummit-papers.11,0,0.387508,"Since these systems are complementary, future work could further explore the benefits of injecting the improved JW300 projections to more complex learners such as D S D S. In particular, D S D S would likely benefit from better projections, since the ones that its current instance uses are inferior to JW300. 4 Related work Our work is a contribution to the pool of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises"
P19-1310,2013.mtsummit-papers.10,0,0.101788,"Missing"
P19-1310,S19-1010,0,0.0620545,"Missing"
P19-1310,L16-1147,0,0.0602004,"ementary, future work could further explore the benefits of injecting the improved JW300 projections to more complex learners such as D S D S. In particular, D S D S would likely benefit from better projections, since the ones that its current instance uses are inferior to JW300. 4 Related work Our work is a contribution to the pool of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises several sentences, or alternatively just p"
P19-1310,mayer-cysouw-2014-creating,0,0.0581759,"l of massively multilingual resources. In that pool we already singled out OPUS (Tiedemann, 2012) as the largest collection of freely available parallel sentences to date. OPUS is a collection that covers large datasets such as Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), along with many others. OPUS also contains a smaller snapshot of Tatoeba, whose original collection hosts 337 languages and 22,427 (±106,815) sentences on average.6 Moving from OPUS and Tatoeba towards greater linguistic breadth, there are several publicly available Bible datasets, most notably those by Mayer and Cysouw (2014) and Christodouloupoulos and Steedman (2015). The Bible datasets are typically aligned by verse and not by sentence, because verse identifiers are assigned by humans, with absolute accuracy. However, a verse sometimes comprises several sentences, or alternatively just parts of one sentence, thus in effect replacing one type of alignment noise with another. Our results strongly favor JW300 for part-of-speech projection. 6 https://tatoeba.org/eng/stats/ sentences_by_language Prior to our work, Agi´c et al. (2016) have also collected a smaller dataset from jw.org to produce cross-lingual dependen"
P19-1310,petrov-etal-2012-universal,0,0.126649,"Missing"
P19-1310,D18-1061,1,0.907582,"Missing"
P19-1310,P16-2067,0,0.185619,"ce word vs packs a label distribution p(l|vs ) of tagger confidences across parts of speech l ∈ L. On top of this parallel dataset, we implement the best practices in annotation projection of sequential labels from multiple sources with low-resource target languages in mind: – Word alignments are obtained from an IBM1 ¨ model Efmaral (Ostling and Tiedemann, 2016) as Agi´c et al. (2016) show that simpler alignment models favor low-resource languages. Thus we acquire all a(vs , vt ) ∈ A. – Source sentences are tagged for parts of speech by a state-of-the-art neural tagger with default settings (Plank et al., 2016). That way all source words attain a tag distribution p(l|vs ). – Source tags are projected through the word alignments and accumulated at the target ends: BALLOT (l|vt ) = X The part-of-speech tag for each target word vt is finally decoded through simple weighted majority voting: = arg max BALLOT(l|vt ). l – The sentences are further filtered so as to remove noisy instances. The model by Plank et al. (2018) is used, whereby for training we select only the top 10 thousand target sentences ranked by mean word alignment coverage ct : ct = n 1X ci,t . n i=1 Mean coverage ct is defined through ind"
P19-1310,P18-1072,1,0.903108,"Missing"
P19-1310,Q13-1001,0,0.0672287,"Missing"
P19-1310,tiedemann-2012-parallel,0,0.857484,"sentences like English, French, and Italian which are all rich in resources. However, the long tail of lowresource languages typically still offers between 50-100 thousand sentences. articles sentences tokens alignments Comparison. With its balance between multilingual breadth and monolingual depth, JW300 fills an important gap in cross-lingual resources: it comprises a multitude of low-resource languages while still offering ample sentences for each individual language, and parallel sentences for language pairs. To illustrate, for JW300 the breadth × depth ratio is 1.2x larger than for OPUS (Tiedemann, 2012), 2x larger than for the full Bible, and even 3x that of New Testament (see Figure 1). JW300 still does come with its own caveats. The crucial one is surely bias: For example, could we indiscriminately use JW300 to train complex machine learning systems that further propagate the attitude of jw.org towards gender differences? From another viewpoint, however, should we rather train part-of-speech taggers through multi-source annotation projection from Watchtower articles on one side, or OPUS Ubuntu menu localizations or Bible psalms on the other side? 343 417 54,376 µ σ 3,202.34 261,573.37 3,54"
P19-1310,C14-1175,0,0.0228576,"1 Introduction In natural language processing (NLP) the rule of thumb is that if we possess some parallel data for a low-resource target language, then we can yield feasible basic tools such as part-of-speech taggers for that language. Without such distant supervision, this task and many others remain unattainable, leaving the majority of languages in the world without basic language technology. Parallel data features a prominent role in building multilingual word representations (Ruder et al., 2017), annotation projection for parts-of-speech and syntactic dependencies (Das and Petrov, 2011; Tiedemann, 2014) and naturally machine translation. The shortage of parallel data in turn creates a bottleneck in cross-lingual processing: without parallel sentences, we cannot yield usable models, nor can we robustly evaluate them, if even just approximately (cf. Agi´c et al. 2017). This absence has over the recent years materialized the proxy fallacy, whereby intended low-resource methods are tested by proxy, exclusively on resource-rich languages, because of the absence of test data or the lack of effort to produce it for approximate evaluation. We seek to alleviate these issues by a significant new addit"
P19-1310,H01-1035,0,0.0789081,"he findings of Glavaˇs et al. (2019). The unsupervised model is even unable to converge for most language pairs, yielding extremely low MRR scores. The scores on another test set (Conneau et al., 2018) for EN - ET and EN - HR also favour the supervised model: 0.342 vs. 0.313 on EN - ET, and 0.289 vs. 0.261 on EN - HR. In sum, these preliminary experiments indicate the potential of JW300 in guiding cross-lingual representation learning. 3.2 Part-of-speech projection Massively parallel data has proven most useful in inducing basic NLP models such as part-of-speech taggers. The formative work by Yarowsky et al. (2001) has inspired many influential works in projecting sequential labels from multiple source languages (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), as well as projecting more complex annotations such as syntactic and semantic dependencies (Hwa et al., 2005; Pad´o and Lapata, 2009; Agi´c et al., 2016). Here we implement an experiment with projecting parts of speech from multiple sources to multiple targets following the line of work by Agi´c et al. (2015) and subsequently Plank et al. (2018), to showcase our corpus. 4 We expect even better performance with recently developed more sophisticate"
P19-1476,P18-1073,0,0.206711,"the entire distributional space. The difference between LE-retrofitting and GLEN is illustrated in Figure 1. Moreover, with GLEN’s ability to LE-specialize unseen words we can seamlessly LE-specialize word vectors of another language (L2), assuming we previously project them to the distributional space of L1 for which we had learned the specialization function. To this end, we can leverage any from the plethora of resource-lean methods for learning the cross-lingual projection (function g in Figure 1) between monolingual distributional vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models"
P19-1476,I13-1095,0,0.0853898,"Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding s"
P19-1476,Q17-1010,0,0.264838,"ck of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in ord"
P19-1476,D15-1075,0,0.106085,"Missing"
P19-1476,N15-1184,0,0.143539,"Missing"
P19-1476,P05-1014,0,0.45601,"Missing"
P19-1476,P18-1004,1,0.899409,"Missing"
P19-1476,P19-1070,1,0.901302,"Missing"
P19-1476,D17-1185,1,0.912081,"Missing"
P19-1476,D18-1330,0,0.0607001,"Missing"
P19-1476,D15-1242,0,0.112892,"Missing"
P19-1476,P14-2050,0,0.0283665,"ollins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to"
P19-1476,N16-1118,0,0.0424006,"and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy"
P19-1476,W13-0904,0,0.0278264,"l LE detection. 1 Background and Motivation Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be d"
P19-1476,Q17-1022,1,0.929649,"Missing"
P19-1476,P16-2074,0,0.0450297,"ng model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied to any pre-trained distributional space. On the downside, retrofitting models specialize only the vectors of words seen in constraints, leaving vectors of unseen words unchanged. In this work, we propose an LE-specialization framework that combines the strengths of both 4824 Proceedings of the 57th Annual Meeting of the A"
P19-1476,N15-1100,0,0.462736,") and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible,"
P19-1476,Q16-1030,0,0.0372214,"contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied"
P19-1476,D14-1162,0,0.0874129,"; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic const"
P19-1476,N18-1202,0,0.0363363,"works and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the l"
P19-1476,D18-1026,1,0.895667,"Missing"
P19-1476,P18-2101,1,0.873555,"Missing"
P19-1476,D18-1042,0,0.111928,"nal vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models for inducing cross-lingual word embedding spaces. Generalized Lexical Entailment Following LEAR (Vuli´c and Mrkˇsi´c, 2018), the state-of-the-art LE-retrofitting model, we use three types of linguistic constraints to learn the general specialization f : synonyms, antonyms, and LE (i.e., hyponym-hypernym) pairs. Similarityfocused specialization models tune only the direction of distributional vectors (Mrkˇsi´c et al., 2017; Glavaˇs and Vuli´c, 2018; Ponti et al., 2018). In LEspecialization we need to emphasize similarities but also"
P19-1476,K15-1026,0,0.0480629,"ference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributional vectors (i.e., embeddings) cannot be directly used to reliably detect LE. Embedding specialization methods remedy for the semantic vagueness of distributional spaces, forcing the vectors to conform to external linguistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the lexico-semantic relation of interest (e.g., semantic similarity of LE) and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and ("
P19-1476,P16-1226,0,0.182048,"Missing"
P19-1476,P06-1101,0,0.230473,"Missing"
P19-1476,D16-1039,0,0.0194116,"al projection (function g in Figure 1) between monolingual distributional vector spaces (Smith et al., 2017; Conneau et al., 2018; Artetxe et al., 2018, inter alia).1 Conceptually, GLEN is similar to the explicit retrofitting model of Glavaˇs and Vuli´c (2018), who focus on the symmetric semantic similarity relation. In contrast, GLEN has to account for the asymmetric nature of the LE relation. Besides joint (Nguyen et al., 2017) and retrofitting (Vuli´c and Mrkˇsi´c, 2018) models for LE, there is a number of supervised LE detection models that employ distributional vectors as input features (Tuan et al., 2016; Shwartz et al., 2016; Glavaˇs and Ponzetto, 1 See (Ruder et al., 2018b; Glavaˇs et al., 2019) for a comprehensive overview of models for inducing cross-lingual word embedding spaces. Generalized Lexical Entailment Following LEAR (Vuli´c and Mrkˇsi´c, 2018), the state-of-the-art LE-retrofitting model, we use three types of linguistic constraints to learn the general specialization f : synonyms, antonyms, and LE (i.e., hyponym-hypernym) pairs. Similarityfocused specialization models tune only the direction of distributional vectors (Mrkˇsi´c et al., 2017; Glavaˇs and Vuli´c, 2018; Ponti et al."
P19-1476,N18-1056,0,0.226603,"Missing"
P19-1476,E17-2065,1,0.902352,"Missing"
P19-1476,J17-4004,1,0.871316,"Missing"
P19-1476,N18-1103,1,0.837789,"Missing"
P19-1476,P19-1490,1,0.770987,"Missing"
P19-1476,N16-1142,0,0.253326,"word pairs) and test portions (900-1000 word pairs): we use the train portions to tune the threshold t that binarizes GLEN’s predictions ILE . We induce the CL embeddings (i.e., learn the projections Wg , see Section §2) by projecting AR, FR, and RU embeddings to the EN space in a supervised fashion, by finding the optimal solution to the Procrustes problem for given 5K word translation pairs (for each language pair). 6 We compare GLEN with more complex models from (Upadhyay et al., 2018): they couple two methods for inducing syntactic CL embeddings – CL-D EP (Vuli´c, 2017) and B I -S PARSE (Vyas and Carpuat, 2016) – with 6 We automatically translated 5K most frequent EN words to AR, FR, and RU with Google Translate. Model EN-FR EN-RU EN-AR Avg HYPO CL-D EP B I -S PARSE GLEN .538 .566 .792 .602 .590 .811 .567 .526 .816 .569 .561 .806 COHYP CL-D EP B I -S PARSE GLEN .610 .667 .779 .562 .636 .849 .631 .668 .821 .601 .657 .816 Table 2: CL LE detection results (accuracy) on CL datasets (HYPO, COHYP) (Upadhyay et al., 2018). an LE scorer based on the distributional inclusion hypothesis (Geffet and Dagan, 2005). Results. GLEN’s cross-lingual LE detection performance is shown in Table 2. GLEN dramatically outp"
P19-1476,Q15-1025,0,0.0429115,"bjective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by fine-tuning word vectors so that they conform to external linguistic constraints. Advantageously, this makes retrofitting models more flexible, as they can be applied to any pre-trained distributional space. On the downside, retrofitting models specialize only the vectors of words seen in constraints, leaving vectors of unseen words unchanged. In this work, we propose an LE-specialization framework that combines the strengths of both 4824 Proceedings of the 57th An"
P19-1476,N18-1101,0,0.0281051,"curacy) over state-ofthe-art in cross-lingual LE detection. 1 Background and Motivation Lexical entailment (LE; hyponymy-hypernymy or is-a relation), is a fundamental asymmetric lexicosemantic relation (Collins and Quillian, 1972; Beckwith et al., 1991) and a key building block of lexico-semantic networks and knowledge bases (Fellbaum, 1998; Navigli and Ponzetto, 2012). Reasoning about word-level entailment supports a multitude of tasks such as taxonomy induction (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017), natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), metaphor detection (Mohler et al., 2013), and text generation (Biran and McKeown, 2013). Due to their distributional nature (Harris, 1954), embedding models (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) conflate paradigmatic relations (e.g., synonymy, antonymy, LE, meronymy) and Ivan Vuli´c PolyAI Ltd. 144A Clerkenwell Road London, United Kingdom ivan@poly-ai.com the broader topical (i.e., syntagmatic) relatedness (Schwartz et al., 2015; Mrkˇsi´c et al., 2017). Consequently, distributio"
P19-1476,C00-2137,0,0.118832,"Missing"
P19-1476,P14-2089,0,0.038428,"uistic constraints (e.g., synonymy or LE word pairs) in order to emphasize the lexico-semantic relation of interest (e.g., semantic similarity of LE) and diminish the contributions of other types of semantic association. Lexical specialization models generally belong to one of the two families: (1) joint optimization models and (2) retrofitting (also known as fine-tuning or post-processing) models. Joint models incorporate linguistic constraints directly into the objective of an embedding model, e.g., Skip-Gram (Mikolov et al., 2013), by modifying the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or by augmenting the objective with additional factors reflecting linguistic constraints (Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Joint models are tightly coupled to a concrete embedding model – any modification to the underlying embedding models warrants a modification of the whole joint model, along with the expensive retraining. Conversely, retrofitting models (Faruqui et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkˇsi´c et al., 2017; Vuli´c and Mrkˇsi´c, 2018, inter alia) change the distributional spaces post-hoc, by"
P19-1476,D14-1161,0,0.257923,"(Smith et al., 2017) based on (closed-form) solution of the Procrustes problem (Sch¨onemann, 1966). Let XS ⊂ XL2 and XT ⊂ XL1 be the subsets of the two monolingual embedding spaces, containing (row-aligned) vectors of word translations. We then obtain the projection matrix as Wg = UV&gt; , where UΣV&gt; is the singular value decomposition of the product matrix XT XS &gt; . 3 Evaluation Experimental Setup. We work with Wikipediatrained FAST T EXT embeddings (Bojanowski et al., 2017). We take English constraints from previous work – synonyms and antonyms were created from WordNet and Roget’s Thesaurus (Zhang et al., 2014; Ono et al., 2015); LE constraints were collected from WordNet by Vuli´c and Mrkˇsi´c (2018) and contain both direct and transitively obtained LE pairs. We retain the constraints for which both words exist in the trimmed (200K) FAST T EXT vocabulary, resulting in a total of 1,493,686 LE, 521,037 synonym, and 141,311 antonym pairs. We reserve 4,000 constraints (E: 2k, S: 1k, A: 1k) for validation and use the rest for training. We identify the following best hyperparameter configuration via grid search: H = 5, dh = 300, ψ = tanh, δa = 1, δs = δd = 0.5, λa = 2, and λr = 1. 3 For a comprehensive"
P19-1490,E17-1088,0,0.0390417,"t al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instructive baseline is the"
P19-1490,W13-3520,0,0.0279406,"alisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual const"
P19-1490,P18-1073,0,0.226842,"pecialisation of the English distributional space, and then 2) translates all test examples in the target language to English relying on the bilingual dictionary D.10 All LE reasoning is then conducted monolingually in English. The TRANS baseline is also used in cross-lingual graded LE evaluation. For cross-lingual datasets without English (e.g., DE-IT), we again translate all words to English and use the English specialised space for graded LE assertions. In addition, for each language pair we also report results of two stateof-the-art cross-lingual word embedding models (Smith et al., 2017; Artetxe et al., 2018), showing the better scoring one in each run (XEMB). For ungraded LE evaluation, in addition to TRANS, we compare CLEAR to two bestperforming baselines from (Upadhyay et al., 2018): they couple two methods for inducing syntactic cross-lingual vectors: 1) B I -S PARSE (Vyas and Carpuat, 2016) and 2) CL-D EP (Vuli´c, 2017) with an LE scorer based on the distributional inclusion hypothesis (Geffet and Dagan, 2005). For more details we refer the reader to (Upadhyay et al., 2018). 9 The translations in PanLex were derived from various sources (e.g., glossaries, dictionaries, automatic inference). T"
P19-1490,Q17-1010,0,0.0292145,"my constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior"
P19-1490,P13-1133,0,0.119829,"Missing"
P19-1490,P05-1014,0,0.443497,"and category vagueness from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Co"
P19-1490,S17-2002,0,0.301956,"peakers.5 Cross-Lingual Datasets. The cross-lingual CL HYPERLEX datasets were then constructed automatically, leveraging word pair translations and scores in three target languages. To this end, we follow the methodology of Camacho-Collados et al. (2015, 2017), used previously for creating cross-lingual semantic similarity datasets. In short, we first intersect aligned concept pairs (obtained through translation) in two languages: e.g., father-ancestor in English and padre-antenato in Italian are used 5 As opposed to (Hill et al., 2015; Gerz et al., 2016; Vuli´c et al., 2017), but similar to (Camacho-Collados et al., 2017; Pilehvar et al., 2018) we did not divide the dataset into smaller tranches; each annotator scored the entire target-language dataset instead. The target languages were selected based on the availability of native speakers; the total number of annotations was restricted by the annotation budget. 4965 Monolingual Datasets 50 picture Person Fahrrad cibo rekreacija 5.90 4.0 0.25 3.25 5.75 Cross-Lingual Datasets (CL - HYPERLEX) EN - DE EN - IT EN - HR DE - IT DE - HR IT- HR dinosaur eye religija Medikation Form aritmetica Kreatur viso belief trattamento prizma matematika EN DE IT HR 40 Percentage"
P19-1490,D16-1235,1,0.922237,"Missing"
P19-1490,P15-2001,0,0.450197,"al lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automatically derive a collection of six cross-lingual GR - LE datasets: CL - HYPERLEX. We analyse in detail the cross-lingual datasets (e.g., by comparing the scores to human-elicited ratings), demonstrating their robustness and reliability. In order to provide a competitive baseline on new monolingual and cross-lingual datasets, we next introduce a cross-lingual specialisation/retrofitting method termed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel): starting from any two monolingual distributional spaces, CLEAR induces a bilingual cross-lingual space that reflects the as"
P19-1490,D18-1269,0,0.159875,"05; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium"
P19-1490,D16-1136,0,0.0207865,"embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR to original non-specialised distributional vectors in each language. Another instruc"
P19-1490,ehrmann-etal-2014-representing,0,0.625,"5; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proc"
P19-1490,P14-1113,0,0.103701,"6, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57t"
P19-1490,P18-1004,1,0.894858,"Missing"
P19-1490,P19-1070,1,0.857159,"Missing"
P19-1490,D17-1185,1,0.786418,"Missing"
P19-1490,P19-1476,1,0.771144,"Missing"
P19-1490,L18-1550,0,0.0176646,"ors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in othe"
P19-1490,J15-4004,0,0.693533,"ation of concepts (i.e., it is not tied to a particular language), a graded LE repository has so far been created only for English: it is the HyperLex dataset of Vuli´c et al. (2017). Starting from the established data creation protocol for HyperLex, in this work we compile similar HyperLex datasets in three other languages and introduce novel multilingual and cross-lingual GR - LE tasks. Graded LE in English. HyperLex (Vuli´c et al., 2017) comprises 2,616 English (EN) word pairs (2,163 noun pairs and 453 verb pairs) annotated for the GR - LE relation. Unlike in symmetric similarity datasets (Hill et al., 2015; Gerz et al., 2016), word order in each pair (X, Y ) is important: this means that pairs (X, Y ) and (Y, X) can obtain drastically different graded LE ratings. The word pairs were first sampled from WordNet to represent a spectrum of different word relations (e.g., hyponymyhypernymy, meronymy, co-hyponymy, synonymy, antonymy, no relation). The ratings in the [0, 6] interval were then collected through crowdsourcing by posing the GR - LE “To what degree...” question to human subjects, with each pair rated by at least 10 raters: the score of 6 indicates strong LE relation between the concepts X"
P19-1490,W19-4310,1,0.844555,"Missing"
P19-1490,kamholz-etal-2014-panlex,0,0.0402875,"glish such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 language varieties with over 12M expressions: it offers support also to low-resource transfer settings.9 Training Setup. CLEAR hyperparameters are adopted from the original Attract-Repel work (Mrkši´c et al., 2017): δatt = 0.6, δrep = 0.0, λreg = λD = 10−9 . All batches are of size 128 (see Eq. (4)), and the model is trained for 5 epochs with Adagrad (Duchi et al., 2011). Baseline Models. In monolingual evaluation, we compare CLEAR t"
P19-1490,P15-2020,1,0.932684,"Missing"
P19-1490,P14-2050,0,0.0537507,"lyse the usefulness of cross-lingual graded LE specialisation performed by CLEAR, and analyse its performance in comparison with distributional word vectors and non-specialised cross-lingual word embeddings. 4.1 Experimental Setup Distributional Vectors. Graded LE is evaluated on EN, DE, IT, and HR (see §2); we also evaluate CLEAR on ungraded cross-lingual LE (Upadhyay et al., 2018) for the following language pairs: ENFR (French); EN - RU (Russian); EN - AR (Arabic). All results are reported with English Skip-Gram with Negative Sampling (SGNS - BOW 2) vectors (Mikolov et al., 2013) trained by Levy and Goldberg (2014) on the Polyglot Wikipedia (Al-Rfou et al., 2013) with bag-of-words context (window size of 2).7 Input vectors for other languages come from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dicti"
P19-1490,W14-0405,0,0.0262033,"Missing"
P19-1490,S13-2005,0,0.197578,"Missing"
P19-1490,N15-1100,0,0.0526911,"rces: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based E"
P19-1490,P09-1034,0,0.0344902,"actual language (the example shows English and Spanish words with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-sem"
P19-1490,D14-1162,0,0.0828019,"n LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex currently spans around 1,300 langua"
P19-1490,N16-1118,0,0.0352914,"Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al., 2017; Vuli´c et al., 2017). PanLex"
P19-1490,N18-1202,0,0.0212284,"rom WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al., 2018), type-based ELMo embeddings (Peters et al., 2018), Context2Vec (Melamud et al., 2016) and Glove (Pennington et al., 2014), but the obtained results follow similar trends. We do not report these results for brevity. 8 Vectors of multi-word expressions in CL - HYPERLEX are obtained by averaging over their constituent words’ vectors. imal), and (beagle, animal) are in the Le set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. Bilingual dictionaries are derived from PanLex (Kamholz et al., 2014), which was used in prior work on cross-lingual word embeddings (Duong et al., 2016; Adams et al."
P19-1490,S10-1002,0,0.0931379,"with the respective prefixes en_ and es_) is reflected by their small co−−−−−−→ sine distances (e.g., the small angle between en_beagle − − − − − − − → −−−−−→ and en_animal), while simultaneously and − es_perro higher-level concepts are assigned larger norms to enforce the LE arrangement in the vector space. An asymmetric distance that takes into account the vector direction as well as the vector magnitude can be used to grade the LE relation strength between any two concepts in the shared cross-lingual vector space. terpretability (Padó et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). In this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR - LE in multilingual settings. First, we extend the research on GR - LE beyond English (Vuli´c et al., 2017; Rei et al., 2018) and provide new human-annotated GR - LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automa"
P19-1490,D18-1169,0,0.0500479,"E), 0.909 (EN - IT), and 0.905 (EN - HR). LE 3 Methodology In order to provide benchmarking graded LE scores on new monolingual and cross-lingual evaluation sets, we now introduce a novel method that can capture GR - LE cross-lingually. CLEAR ( CrossLingual Lexical Entailment Attract-Repel) is a cross-lingual extension of the monolingual LEAR specialisation method (Vuli´c and Mrkši´c, 2018), a state-of-the-art vector space fine-tuning method which specialises any input distributional vector 6 Similarity benchmarks report much lower Pairwise-IAA scores: 0.61 on SimVerb-3500 (Gerz et al., 2016; Pilehvar et al., 2018), and 0.67 on SimLex-999 (Hill et al., 2015) and on WordSim-353 (Finkelstein et al., 2002) CLEAR Specialisation. A high-level overview of the CLEAR specialisation method is provided in Figure 3. The input to the method is as follows: 1) two independently trained monolingual word vector spaces in two languages L1 and L2 ; 2) sets of external lexical constraints in the resource-rich language L1 (e.g., English) extracted from an external lexical resource such as WordNet (Fellbaum, 1998) or BabelNet (Ehrmann et al., 2014); and 3) a bilingual L1 -L2 dictionary D. The goal is to fine-tune input word"
P19-1490,P17-1163,0,0.0216099,"Missing"
P19-1490,Q17-1022,1,0.924323,"Missing"
P19-1490,S12-1053,0,0.0762876,"Missing"
P19-1490,D18-1026,1,0.864529,"Missing"
P19-1490,P18-2101,1,0.869814,"Missing"
P19-1490,P18-2057,0,0.13905,"ion “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song"
P19-1490,E14-4008,0,0.0904314,"ce (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multil"
P19-1490,N16-1142,0,0.265208,"uman judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rat"
P19-1490,P16-1226,0,0.17476,"Missing"
P19-1490,C14-1212,0,0.0844206,"from cognitive science (Rosch, 1973, 1975; Kamp and Partee, 1995). Instead of answering the simpler (discrete) question “Is X a type of Y?”, as in standard LE detection tasks (Kotlerman et al., 2010; Turney and Mohammad, 2015), GR - LE aims at answering the following question: “To what degree is X a type of Y?” The concept of LE gradience is also empirically confirmed by human judgements elicited for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b)"
P19-1490,E17-1007,0,0.171718,"Missing"
P19-1490,D14-1161,0,0.062278,"ome from various sources: AR vectors are fastText vectors trained on the Common Crawl data by Grave et al. (2018). RU vectors are obtained by Kutuzov and Andreev (2015). FR, IT, DE, and HR word vectors are large SGNS vectors trained on the standard frWaC, itWaC, and deWaC corpora (Baroni et al., 2009), and the hrWaC corpus (Ljubeši´c and Klubiˇcka, 2014), also used in prior work (Vuli´c et al., 2017). All word vectors are 300-dim.8 Linguistic Constraints and Dictionaries. We use the same set of monolingual constraints as LEAR (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialisation (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (beagle, dog), (dog, an7 The proposed CLEAR method is by design agnostic of input distributional vectors and its main purpose is to support fine-tuning of a wide spectrum of input vectors. We have experimented with other standard distributional spaces in English such as fastText (Bojanowski et al., 2017; Grave et al.,"
P19-1490,P14-1068,0,0.0674999,"Missing"
P19-1490,L18-1558,0,0.0166563,"2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In contrast, the pair food → pie receives the average rating of only 0.92/6, which confirms the inherent asymmetry of the GR - LE relation. 2 For instance, translating the Italian word calcio to calcium prevents identifying sport as a hypernym of calcio. 4963 Proceedings of the 57th Annual Meeting of the Association for Computational"
P19-1490,N18-1056,0,0.255581,"for HyperLex (Vuli´c et al., 2017), a GR - LE resource in English.1 Furthermore, while simpler binary LE detection has been predominantly studied in monolingual settings only (Geffet and Dagan, 2005; Weeds et al., 2014; Santus et al., 2014; Kiela et al., 2015; Shwartz et al., 2016, 2017; Glavaš and Ponzetto, 2017; Roller et al., 2018, inter alia), more general reasoning over cross-lingual and multilingual LE relationships can improve language understanding in multilingual contexts, e.g., in cases when translations are ambiguous or not equivalent to the source concept (Vyas and Carpuat, 2016; Upadhyay et al., 2018).2 The ability to reason over cross-lingual LE is pivotal for a variety of cross-lingual tasks such as recognising cross-lingual textual entailment (Negri et al., 2012, 2013; Conneau et al., 2018b), constructing multilingual taxonomies (Ehrmann et al., 2014; Fu et al., 2014), cross-lingual event coreference (Song et al., 2018), machine translation in1 For instance, the strength of LE association hamburger → food is on average judged by humans with 5.85/60. In comparison, oregano is seen as a less typical instance of the category/concept food, with the pair’s average rating of 3.58/6.0. In cont"
P19-1490,E17-2065,1,0.908149,"Missing"
P19-1490,J17-4004,1,0.784862,"Missing"
P19-1490,N18-1103,1,0.896402,"Missing"
P19-1490,D17-1270,1,0.902928,"Missing"
P19-1536,S12-1051,0,0.0172507,"al material. To the best of our knowledge, the work of Henderson et al. (2017) and Yang et al. (2018) is closest to our response selection pretraining introduced in §2.1. However, Henderson et al. (2017) optimise their model for one single task: replying to e-mails with short messages (Kannan et al., 2016). They use a simpler feed-forward encoder architecture and do not consider wide portability of a single generaldomain response selection model to diverse target domains through fine-tuning. Yang et al. (2018) use Reddit conversational context to simply probe semantic similarity of sentences (Agirre et al., 2012, 2013; Nakov et al., 2016), but they also do not investigate response selection fine-tuning across diverse target domains. Pretraining and Fine-Tuning. Task-specific fine-tuning of language models (LMs) pretrained on large unsupervised corpora (Peters et al., 2018; Devlin et al., 2019; Howard and Ruder, 2018; Radford et al., 2018; Lample and Conneau, 2019; Liu et al., 2019) has taken NLP by storm. Such LMbased pretrained models support a variety of NLP tasks, ranging from syntactic parsing to natural language inference (Peters et al., 2018; Devlin et al., 2019), as well as machine reading com"
P19-1536,S13-1004,0,0.0406859,"Missing"
P19-1536,N19-1423,0,0.57402,"sk-based dialogue still few and far between, as 5392 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5392–5404 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics well as limited in size.1 Recent work on language modelling (LM) pretraining (Peters et al., 2018; Howard and Ruder, 2018) has shown that task-specific architectures are not necessary in a number of NLP tasks. The best results have been achieved by LM pretraining on large unannotated corpora, followed by supervised fine-tuning on the task at hand (Devlin et al., 2019). Given the compelling benefits of large-scale pretraining, our work poses a revamped question for response selection: can we pretrain a general response selection model and then adapt it to a variety of different dialogue domains? To tackle this problem, we propose a two-step training procedure which: 1) pretrains a response selection model on large conversational corpora (such as Reddit); and then 2) fine-tunes the pretrained model for the target dialogue domain. Throughout the evaluation, we aim to provide answers to the following two questions: 1. (Q1) How to pretrain? Which encoder struct"
P19-1536,P12-3007,0,0.0330006,"e of producing more informative, semantically relevant, controllable, and grammatically correct responses (Ji et al., 2014). Unlike modular and end-to-end task-oriented systems (Young, 2010; Wen et al., 2017b; Mrkši´c and Vuli´c, 2018; Li et al., 2018), they do not require expensive curated domain ontologies, and bypass the modelling of complex domain-specific decision-making policy modules (Gaši´c et al., 2015; Chen et al., 2017). Despite these desirable properties, their potential has not been fully exploited in task-oriented dialogue. Their fundamental building block is response selection (Banchs and Li, 2012; Wang et al., 2013; Al-Rfou et al., 2016; Baudis and Sedivý, 2016). We have witnessed a recent rise of interest in neural architectures for modelling response selection (Wu et al., 2017; Chaudhuri et al., 2018; Zhou et al., 2018; Tao et al., 2019), but the progress is still hindered by insufficient domain-specific training data (El Asri et al., 2017; Budzianowski et al., 2018). While previous work typically focused on a single domain (e.g., Ubuntu technical chats (Lowe et al., 2015, 2017)), in this work we show that much larger general-domain Reddit data can be leveraged to pretrain response"
P19-1536,W18-5708,0,0.0375207,"typical retrieval-based approach to dialogue encodes the input and a large set of responses in a joint semantic space. When framed as an ad-hoc retrieval task (Deerwester et al., 1990; Ji et al., 2014; Kannan et al., 2016; Henderson et al., 2017), the system treats each input utterance as a query and retrieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Respons"
P19-1536,W17-5526,0,0.199584,"Missing"
P19-1536,D18-1547,1,0.911267,"Missing"
P19-1536,W18-6317,0,0.0647565,"Missing"
P19-1536,K18-1048,0,0.232529,"ogue encodes the input and a large set of responses in a joint semantic space. When framed as an ad-hoc retrieval task (Deerwester et al., 1990; Ji et al., 2014; Kannan et al., 2016; Henderson et al., 2017), the system treats each input utterance as a query and retrieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Response selection also differs from chatbot-style sy"
P19-1536,W14-4337,1,0.899507,"Missing"
P19-1536,W14-4340,1,0.920939,"-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Response selection also differs from chatbot-style systems which generate new responses by generalising over training data, their main deficiency being the tendency towards generating universal but irrelevant responses such as “I don’t know” or “Thanks” (Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016; Song et al., 2018). Therefore, response selection removes the need to engineer structured domain ontologies, and to solve the difficult task of general language generation. Furthermore, it is also much easier to constrain or combine the output of response s"
P19-1536,P18-1031,0,0.140647,"gue systems still suffer from data scarcity, as deployment to a new domain requires a sufficiently large in-domain dataset for training the response selection model. Procuring such data is expensive and labour-intensive, with annotated datasets for task-based dialogue still few and far between, as 5392 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5392–5404 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics well as limited in size.1 Recent work on language modelling (LM) pretraining (Peters et al., 2018; Howard and Ruder, 2018) has shown that task-specific architectures are not necessary in a number of NLP tasks. The best results have been achieved by LM pretraining on large unannotated corpora, followed by supervised fine-tuning on the task at hand (Devlin et al., 2019). Given the compelling benefits of large-scale pretraining, our work poses a revamped question for response selection: can we pretrain a general response selection model and then adapt it to a variety of different dialogue domains? To tackle this problem, we propose a two-step training procedure which: 1) pretrains a response selection model on large"
P19-1536,P15-1162,0,0.0285381,"(e.g., compare (Smith et al., 2017) and (Masters and Luschi, 2018)), we obtain better results with larger batches. This is intuitive given the model design: increasing the batch size in fact means learning from a larger number of negative examples. The results also suggest that the model saturates when provided with a sufficient number of parameters, as wider hidden layers and longer training times did not yield any substantial gains. The scores also show the benefits of selfattention and positional embeddings instead of deep feed-forward averaging of the input unigram and bigram embeddings (Iyyer et al., 2015). This is in line with prior work on sentence encoders (Cer et al., 2018; Yang et al., 2018), which reports similar gains on several classification tasks. Finally, we observe a large gap with the unigram-only model variant, confirming the importance of implicitly representing underlying sequences with n-grams (Henderson et al., 2017; Mrkši´c et al., 2017a). Following the results, we fix the pretraining model in all follow-up experiments (top row in Table 2). 4.2 Target-Domain Fine-Tuning Results and Discussion. The main results on all target tasks after fine-tuning are summarised in Table 3. F"
P19-1536,N16-1014,0,0.0237788,"en et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Response selection also differs from chatbot-style systems which generate new responses by generalising over training data, their main deficiency being the tendency towards generating universal but irrelevant responses such as “I don’t know” or “Thanks” (Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016; Song et al., 2018). Therefore, response selection removes the need to engineer structured domain ontologies, and to solve the difficult task of general language generation. Furthermore, it is also much easier to constrain or combine the output of response selection models. This design also bypasses the construction of dedicated decision-making policy modules. Although conceptually attractive, retrieval-based dialogue systems still suffer from data scarcity, as deployment to a new domain requires a sufficiently large in-domain dataset for training the response selection m"
P19-1536,L16-1147,0,0.0378687,"Missing"
P19-1536,E17-1001,0,0.0209326,"rieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Response selection also differs from chatbot-style systems which generate new responses by generalising over training data, their main deficiency being the tendency towards generating universal but irrelevant responses such as “I don’t know” or “Thanks” (Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016;"
P19-1536,P19-1441,0,0.0281426,"er wide portability of a single generaldomain response selection model to diverse target domains through fine-tuning. Yang et al. (2018) use Reddit conversational context to simply probe semantic similarity of sentences (Agirre et al., 2012, 2013; Nakov et al., 2016), but they also do not investigate response selection fine-tuning across diverse target domains. Pretraining and Fine-Tuning. Task-specific fine-tuning of language models (LMs) pretrained on large unsupervised corpora (Peters et al., 2018; Devlin et al., 2019; Howard and Ruder, 2018; Radford et al., 2018; Lample and Conneau, 2019; Liu et al., 2019) has taken NLP by storm. Such LMbased pretrained models support a variety of NLP tasks, ranging from syntactic parsing to natural language inference (Peters et al., 2018; Devlin et al., 2019), as well as machine reading comprehension (Nishida et al., 2018; Xu et al., 2019) and information retrieval tasks (Nogueira and Cho, 2019; Yang et al., 2019). In this work, instead of the LM-based pretraining, we put focus on the response selection pretraining in particular, and show that such models coupled with target task fine-tuning (Howard and Ruder, 2018) lead to improved modelling of conversational"
P19-1536,W15-4640,0,0.0196365,"as not been fully exploited in task-oriented dialogue. Their fundamental building block is response selection (Banchs and Li, 2012; Wang et al., 2013; Al-Rfou et al., 2016; Baudis and Sedivý, 2016). We have witnessed a recent rise of interest in neural architectures for modelling response selection (Wu et al., 2017; Chaudhuri et al., 2018; Zhou et al., 2018; Tao et al., 2019), but the progress is still hindered by insufficient domain-specific training data (El Asri et al., 2017; Budzianowski et al., 2018). While previous work typically focused on a single domain (e.g., Ubuntu technical chats (Lowe et al., 2015, 2017)), in this work we show that much larger general-domain Reddit data can be leveraged to pretrain response selection models that support more specialised target dialogue domains. 14 For clarity, we show the plots with 10 (out of 77) selected categories, while the full plots with all 77 categories are available in the supplemental material. To the best of our knowledge, the work of Henderson et al. (2017) and Yang et al. (2018) is closest to our response selection pretraining introduced in §2.1. However, Henderson et al. (2017) optimise their model for one single task: replying to e-mails"
P19-1536,P18-2018,1,0.898256,"Missing"
P19-1536,P15-2130,1,0.905656,"Missing"
P19-1536,P17-1163,1,0.850185,"Missing"
P19-1536,Q17-1022,1,0.902897,"Missing"
P19-1536,S15-2047,0,0.0651204,"Missing"
P19-1536,N18-1202,0,0.244665,"retrieval-based dialogue systems still suffer from data scarcity, as deployment to a new domain requires a sufficiently large in-domain dataset for training the response selection model. Procuring such data is expensive and labour-intensive, with annotated datasets for task-based dialogue still few and far between, as 5392 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5392–5404 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics well as limited in size.1 Recent work on language modelling (LM) pretraining (Peters et al., 2018; Howard and Ruder, 2018) has shown that task-specific architectures are not necessary in a number of NLP tasks. The best results have been achieved by LM pretraining on large unannotated corpora, followed by supervised fine-tuning on the task at hand (Devlin et al., 2019). Given the compelling benefits of large-scale pretraining, our work poses a revamped question for response selection: can we pretrain a general response selection model and then adapt it to a variety of different dialogue domains? To tackle this problem, we propose a two-step training procedure which: 1) pretrains a response"
P19-1536,N10-1020,0,0.0533829,"architecture configurations later in §4.1. responses, we obtain more than 727M commentresponse pairs which are used for model pretraining. This Reddit dataset is substantially larger than the previous Reddit dataset of Al-Rfou et al. (2016), which spans around 2.1B comments and 133M conversational threads, and is not publicly available. Second, Reddit is extremely diverse topically (Schrading et al., 2015; Al-Rfou et al., 2016): there are more than 300,000 sub-forums (i.e., subreddits) covering diverse topics of discussion. Finally, compared to message-length-restricted Twitter conversations (Ritter et al., 2010), Reddit conversations tend to be more natural. In summary, all these favourable properties hold promise to support a large spectrum of diverse conversational domains. Input and Response Representation. We now turn to describing the architecture of the main pretraining model. The actual description focuses on the best-performing architecture shown in Figure 2, but we also provide a comparative analysis of other architectural choices later in §4.1. First, similar to Henderson et al. (2017), raw text is converted to unigrams and bigrams, that is, we extract n-gram features from each input x and"
P19-1536,D19-1011,0,0.0231431,"8), they do not require expensive curated domain ontologies, and bypass the modelling of complex domain-specific decision-making policy modules (Gaši´c et al., 2015; Chen et al., 2017). Despite these desirable properties, their potential has not been fully exploited in task-oriented dialogue. Their fundamental building block is response selection (Banchs and Li, 2012; Wang et al., 2013; Al-Rfou et al., 2016; Baudis and Sedivý, 2016). We have witnessed a recent rise of interest in neural architectures for modelling response selection (Wu et al., 2017; Chaudhuri et al., 2018; Zhou et al., 2018; Tao et al., 2019), but the progress is still hindered by insufficient domain-specific training data (El Asri et al., 2017; Budzianowski et al., 2018). While previous work typically focused on a single domain (e.g., Ubuntu technical chats (Lowe et al., 2015, 2017)), in this work we show that much larger general-domain Reddit data can be leveraged to pretrain response selection models that support more specialised target dialogue domains. 14 For clarity, we show the plots with 10 (out of 77) selected categories, while the full plots with all 77 categories are available in the supplemental material. To the best o"
P19-1536,D13-1096,0,0.350991,"he input user utterance (i.e., the full dialogue context). A typical retrieval-based approach to dialogue encodes the input and a large set of responses in a joint semantic space. When framed as an ad-hoc retrieval task (Deerwester et al., 1990; Ji et al., 2014; Kannan et al., 2016; Henderson et al., 2017), the system treats each input utterance as a query and retrieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain exper"
P19-1536,D15-1199,1,0.873311,"Missing"
P19-1536,D15-1309,0,0.0190425,"emb positional emb positional emb input: x response: y Figure 2: Schematic input-response encoder model structure. We show the best-performing architecture for brevity, while we evaluate a variety of other encoder architecture configurations later in §4.1. responses, we obtain more than 727M commentresponse pairs which are used for model pretraining. This Reddit dataset is substantially larger than the previous Reddit dataset of Al-Rfou et al. (2016), which spans around 2.1B comments and 133M conversational threads, and is not publicly available. Second, Reddit is extremely diverse topically (Schrading et al., 2015; Al-Rfou et al., 2016): there are more than 300,000 sub-forums (i.e., subreddits) covering diverse topics of discussion. Finally, compared to message-length-restricted Twitter conversations (Ritter et al., 2010), Reddit conversations tend to be more natural. In summary, all these favourable properties hold promise to support a large spectrum of diverse conversational domains. Input and Response Representation. We now turn to describing the architecture of the main pretraining model. The actual description focuses on the best-performing architecture shown in Figure 2, but we also provide a com"
P19-1536,E17-1042,1,0.921281,"Missing"
P19-1536,N18-3006,0,0.0482381,"t require expensive retraining on Reddit. We also show that the proposed two-step response selection training regime is more effective than directly applying offthe-shelf state-of-the-art sentence encoders (Cer et al., 2018; Devlin et al., 2019). 1 For instance, the recently published MultiWOZ dataset (Budzianowski et al., 2018) comprises a total of 115,424 dialogue turns scattered over 7 target domains. It is several times larger than other standard task-based dialogue datasets such as DSTC2 (Henderson et al., 2014b) with 23,354 turns, Frames (El Asri et al., 2017) with 19,986 turns, or M2M (Shah et al., 2018) with 14,796 turns. To illustrate the difference in magnitude, the Reddit corpus used in this work for response selection pretraining comprises 727M dialogue turns. We hope that this paper will inform future development of response-based taskoriented dialogue. Training and test datasets, described in more detail by Henderson et al. (2019), are available at: github.com/ PolyAI-LDN/conversational-datasets. 2 Methodology Why Pretrain and Fine-Tune? By simplifying the conversational learning task to a response selection task, we can relate target domain tasks to general-domain conversational data"
P19-1536,W18-5713,0,0.205957,"ased approach to dialogue encodes the input and a large set of responses in a joint semantic space. When framed as an ad-hoc retrieval task (Deerwester et al., 1990; Ji et al., 2014; Kannan et al., 2016; Henderson et al., 2017), the system treats each input utterance as a query and retrieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et al., 2015). Response selection also diff"
P19-1536,P17-1046,0,0.030113,"we do not see such patterns at all with ELM Oencoded questions without mapping (ELMO - SIM, Figure 4a), such clusters can already be noticed with USE - MAP (Figure 4b) and with the model pretrained on Reddit without fine-tuning (Figure 4c). However, fine-tuning yields the most coherent clusters by far: it attracts encodings of all similar questions related to the same category closer to each other in the semantic space. This is in line with the results reported in Table 3. ING 5 Related Work Retrieval-Based Dialogue Systems. Retrievalbased systems (Yan et al., 2016; Bartl and Spanakis, 2017; Wu et al., 2017; Song et al., 2018; Weston et al., 2018, inter alia) provide less variable output than generative dialogue systems (Wen et al., 2015, 2017a; Vinyals and Le, 2015), but they offer a crucial advantage of producing more informative, semantically relevant, controllable, and grammatically correct responses (Ji et al., 2014). Unlike modular and end-to-end task-oriented systems (Young, 2010; Wen et al., 2017b; Mrkši´c and Vuli´c, 2018; Li et al., 2018), they do not require expensive curated domain ontologies, and bypass the modelling of complex domain-specific decision-making policy modules (Gaši´c"
P19-1536,N19-1242,0,0.0510175,"Missing"
P19-1536,W18-3022,0,0.382762,"ialogue context). A typical retrieval-based approach to dialogue encodes the input and a large set of responses in a joint semantic space. When framed as an ad-hoc retrieval task (Deerwester et al., 1990; Ji et al., 2014; Kannan et al., 2016; Henderson et al., 2017), the system treats each input utterance as a query and retrieves the most relevant response from a large response collection by computing semantic similarity between the query representation and the encoding of each response in the collection. This task is referred to as response selection (Wang et al., 2013; Al-Rfou et al., 2016; Yang et al., 2018; Du and Black, 2018; Weston et al., 2018; Chaudhuri et al., 2018), as illustrated in Figure 1. Formulating dialogue as a response selection task stands in contrast with other data-driven dialogue modeling paradigms such as modular and end-to-end task-based dialogue systems (Young, 2010; Wen et al., 2017b; Liu and Perez, 2017; Li et al., 2017; Bordes et al., 2017). Unlike standard task-based systems, response selection does not rely on explicit task-tailored semantics in the form of domain ontologies, which are hand-crafted for each task by domain experts (Henderson et al., 2014a,b; Mrkši´c et"
P19-1536,P18-1103,0,0.0292075,"018; Li et al., 2018), they do not require expensive curated domain ontologies, and bypass the modelling of complex domain-specific decision-making policy modules (Gaši´c et al., 2015; Chen et al., 2017). Despite these desirable properties, their potential has not been fully exploited in task-oriented dialogue. Their fundamental building block is response selection (Banchs and Li, 2012; Wang et al., 2013; Al-Rfou et al., 2016; Baudis and Sedivý, 2016). We have witnessed a recent rise of interest in neural architectures for modelling response selection (Wu et al., 2017; Chaudhuri et al., 2018; Zhou et al., 2018; Tao et al., 2019), but the progress is still hindered by insufficient domain-specific training data (El Asri et al., 2017; Budzianowski et al., 2018). While previous work typically focused on a single domain (e.g., Ubuntu technical chats (Lowe et al., 2015, 2017)), in this work we show that much larger general-domain Reddit data can be leveraged to pretrain response selection models that support more specialised target dialogue domains. 14 For clarity, we show the plots with 10 (out of 77) selected categories, while the full plots with all 77 categories are available in the supplemental mate"
P19-4007,Q16-1031,0,0.0204922,"offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has alrea"
P19-4007,D18-1024,0,0.0563881,"Missing"
P19-4007,Q17-1010,0,0.178231,"Missing"
P19-4007,D16-1136,0,0.0662203,"Missing"
P19-4007,P17-2037,1,0.876661,"Missing"
P19-4007,D12-1001,0,0.0762697,"Missing"
P19-4007,S17-2002,0,0.078474,"Missing"
P19-4007,N18-2029,1,0.901703,"Missing"
P19-4007,D18-1023,0,0.0295173,"Missing"
P19-4007,P18-1004,1,0.898441,"Missing"
P19-4007,P19-1070,1,0.876625,"Missing"
P19-4007,Q19-1007,0,0.0295517,"Missing"
P19-4007,D18-1330,0,0.069032,"Missing"
P19-4007,K18-1021,1,0.897312,"Missing"
P19-4007,N19-1188,1,0.889169,"Missing"
P19-4007,E17-1102,1,0.891004,"Missing"
P19-4007,D17-1269,0,0.060333,"Missing"
P19-4007,D18-1043,0,0.0398969,"Missing"
P19-4007,D11-1006,0,0.103244,"Missing"
P19-4007,D15-1127,0,0.0637398,"Missing"
P19-4007,N19-1386,0,0.0455551,"Missing"
P19-4007,P18-2035,0,0.0666436,"Missing"
P19-4007,Q17-1022,1,0.905704,"Missing"
P19-4007,D18-1063,0,0.0557082,"Missing"
P19-4007,P15-1165,1,0.881513,"Missing"
P19-4007,D18-1047,0,0.013888,"chosen hyper-parameters, etc. In this part, we will analyze the current problems with robustness and stability of weaklysupervised and unsupervised alignment methods in relation to all these factors, and introduce latest solutions to alleviate these problems. We will provide advice on how to approach weakly-supervised and unsupervised training based on a series of empirical observations available in recent literature (Søgaard et al., 2018; Hartmann et al., 2018). We will also discuss the (im)possibility of learning nonlinear mappings using either non-linear generators or locally linear maps (Nakashole, 2018). We will conclude by providing publicly available software packages and implementations, as well as available training datasets and evaluation protocols and systems. We will also list current state-of-the-art results on standard evaluation datasets, and sketch future research paths. • A general framework for mapping-based approaches. 3 • Importance of seed bilingual lexicons. • Learning alignment with weak supervision: small seed lexicons, shared words, numerals. Part III: Adversarial Seed Induction (30 minutes) • Fully unsupervised models using adversarial training; MUSE and related approach"
P19-4007,P18-1072,1,0.89956,"Missing"
P19-4007,P18-2036,0,0.0408295,"Missing"
P19-4007,D17-1264,0,0.0588289,"Missing"
P19-4007,N16-1072,0,0.071611,"Missing"
P19-4007,N10-1135,0,0.0513539,"Missing"
P19-4007,P16-1157,0,0.0233929,"still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even"
P19-4007,D18-1026,1,0.906688,"Missing"
P19-4007,N18-2084,0,0.0307978,"ource-poor settings where bilingual supervision cannot be guaranteed; 2) critical examinations of different training conditions and requirements under which unsupervised algorithms can and cannot work effectively; 3) more robust methods for distant language pairs that Cross-lingual word representations offer an elegant and language-pair independent way to represent content across different languages. They enable us to reason about word meaning in multilingual contexts and serve as an integral source of knowledge for multilingual applications such as machine translation (Artetxe et al., 2018d; Qi et al., 2018; Lample et al., 2018b) or multilingual search and question answering (Vuli´c and Moens, 2015). In addition, they are a key facilitator of cross-lingual transfer and joint multilingual training, offering support to NLP applications in a large spectrum of languages (Søgaard et al., 2015; Ammar et al., 2016a). While NLP is increasingly more embedded into a variety of products related to, e.g., translation, conversational or search tasks, resources such as annotated training data are still lacking or insufficient to induce satisfying models for many resource-poor languages. There are often no tra"
P19-4007,D18-1270,0,0.0630194,"Missing"
P19-4007,P18-1084,1,0.89476,"Missing"
P19-4007,N18-1056,0,0.0443504,"Missing"
P19-4007,D18-1042,1,0.916875,"ficient to induce satisfying models for many resource-poor languages. There are often no trained linguistic annotators for these languages, and markets may be too small or premature to invest in such training. This is a major challenge, but cross-lingual modelling and transfer can help by exploiting observable correlations between major languages and low-resource languages. Recent work has already verified the usefulness of cross-lingual word representations in a wide variety of downstream tasks, and has provided extensive model classifications in several survey papers (Upadhyay et al., 2016; Ruder et al., 2018b). They cluster supervised cross-lingual word representation models according to the bilingual supervision required to induce such shared cross-lingual semantic spaces, covering models based on word alignments and readily available bilingual dictionaries (Mikolov et al., 2013; Smith et al., 2017), sentence-aligned parallel data (Gouws et al., 2015), document-aligned data (Søgaard et al., 2015; Vuli´c 1 Learning unsupervised cross-lingual models has indeed taken the field by storm: there are 10+ papers on this very topic published in EMNLP 2018 proceedings alone, with even more papers availabl"
P19-4007,P16-1024,1,0.90887,"Missing"
P19-4007,D13-1168,1,0.869883,"Missing"
P19-4007,N19-1162,0,0.0610116,"Missing"
P19-4007,D17-1270,1,0.903119,"Missing"
P19-4007,D18-1268,0,0.0445512,"Missing"
P19-4007,P18-1005,0,0.0613929,"Missing"
P19-4007,C16-1300,0,0.060658,"Missing"
P19-4007,P17-1179,0,0.0632695,"Missing"
P19-4007,D17-1207,0,0.0552572,"Missing"
P19-4007,P19-1307,0,0.0367463,"Missing"
P19-4007,N16-1156,0,0.0701558,"Missing"
P19-4007,D18-1022,0,0.0481647,"Missing"
Q17-1022,S15-1003,0,0.0166028,"scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and"
Q17-1022,Q16-1031,0,0.0184747,"gual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by co"
Q17-1022,P98-1013,0,0.0636087,"gue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating"
Q17-1022,P14-2131,0,0.0313084,"ract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories:"
Q17-1022,D14-1082,0,0.0239881,"lude: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train dis"
Q17-1022,P06-1038,0,0.011388,"enson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vecto"
Q17-1022,P14-1129,0,0.0264494,"Missing"
Q17-1022,D16-1136,0,0.0135803,"differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for reso"
Q17-1022,ehrmann-etal-2014-representing,0,0.0525517,"ural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. Table 1 illustrates the effects of cross-lingual ATTRACT-R EPEL specialization by showing the nearest neighbors for three English words across three cross-lingual spaces. 309 Transactions of the Association for Computational Linguistics, vol. 5, pp. 309–324, 2017. Action Ed"
Q17-1022,E14-1049,0,0.359731,"al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vu"
Q17-1022,P15-2076,0,0.0360459,"borne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. F"
Q17-1022,N15-1184,0,0.532158,"s using Monolingual and Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high-"
Q17-1022,ganitkevitch-callison-burch-2014-multilingual,0,0.0509444,"Missing"
Q17-1022,N13-1092,0,0.0261119,"Missing"
Q17-1022,D16-1235,1,0.905072,"Missing"
Q17-1022,D14-1012,0,0.0360428,"tion datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into"
Q17-1022,P15-1119,0,0.0296068,"Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard et al., 2015; Guo et al., 2015). However, prior work on cross-lingual word embedding has tended not to exploit pre-existing linguistic resources such as BabelNet. In this work, we make use of cross-lingual constraints derived from such repositories to induce high-quality cross-lingual vector spaces by facilitating semantic transfer from highto lower-resource languages. In our experiments, we show that cross-lingual vector spaces produced by ATTRACT-R EPEL consistently outperform a representative selection of five strong cross-lingual word embedding models in both intrinsic and extrinsic evaluation across several languages."
Q17-1022,W14-4337,0,0.0396757,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,W14-4340,1,0.938481,"re expressed by slot-value pairs such as [price: cheap] or [food: Thai]. For modular task-based systems, the Dialogue State Tracking (DST) component is in charge of maintaining the belief state, which is the system’s internal distribution over the possible states of the dialogue. Figure 1 shows the correct dialogue state for each turn of an example dialogue. Unseen Data/Labels As dialogue ontologies can be very large, many of the possible class labels (i.e., the various food types or street names) will not occur in the training set. To overcome this problem, delexicalization-based DST models (Henderson et al., 2014c; Henderson et al., 2014b; Mrkši´c et al., 2015; Wen et al., 2017) replace occurrences of ontology values with generic tags which facilitate transfer learning across different ontology values. This is done through exact matching supplemented with semantic lexicons which encode rephrasings, morphology and other linguistic variation. For instance, such lexicons would be required to deal with the underlined non-exact matches in Figure 1. Exact Matching as a Bottleneck Semantic lexicons can be hand-crafted for small dialogue domains. Mrkši´c et al. (2016) showed that semantically specialized vect"
Q17-1022,P14-1006,0,0.0667397,"ts models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual"
Q17-1022,J15-4004,1,0.925417,"multilingual DST models, which brings further performance improvements. 1 In this paper we advance the semantic specialization paradigm in a number of ways. We introduce a new algorithm, ATTRACT-R EPEL, that uses synonymy and antonymy constraints drawn from lexical resources to tune word vector spaces using linguistic information that is difficult to capture with conventional distributional training. Our evaluation shows that ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We the"
Q17-1022,D15-1127,0,0.0127607,"our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; V"
Q17-1022,D14-1070,0,0.0192932,"chart, 2015). To show that our approach yields semantically informative vectors for lower-resource languages, we collect intrinsic evaluation datasets for Hebrew and Croatian and show that cross-lingual specialization significantly improves word vector quality in these two (comparatively) low-resource languages. In the second part of the paper, we explore the use of ATTRACT-R EPEL-specialized vectors in a downstream application. One important motivation for training word vectors is to improve the lexical coverage of supervised models for language understanding tasks, e.g., question answering (Iyyer et al., 2014) or textual entailment (Rocktäschel et al., 2016). In 1 Some (negative) effects of the distributional hypothesis do persist. For example, nl_krieken (Dutch for cherries), is identified as a synonym for en_morning, presumably because the idiom ‘het krieken van de dag’ translates to ‘the crack of dawn’. 2 Our approach is not suited for languages for which no lexical resources exist. However, many languages have some coverage in cross-lingual lexicons. For instance, BabelNet 3.7 automatically aligns WordNet to Wikipedia, providing accurate cross-lingual mappings between 271 languages. In our eval"
Q17-1022,N15-1070,0,0.0340476,"ons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tuning Pre-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles o"
Q17-1022,D15-1245,0,0.0248451,"EL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations"
Q17-1022,D15-1242,0,0.0418903,"epresentations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use sy"
Q17-1022,W16-1607,0,0.169112,"Missing"
Q17-1022,C12-1089,0,0.0230184,"eover, we show that starting from distributional vectors allows our method to use existing cross-lingual resources to tie distributional vector spaces of different languages into a unified vector space which benefits from positive semantic transfer between its constituent languages. 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al."
Q17-1022,2005.mtsummit-papers.11,0,0.0253673,"into a shared cross-lingual space. Ideally, sharing information across languages should lead to improved semantic content for each language, especially for those with limited monolingual resources. Antonymy BabelNet is also used to extract both monolingual and cross-lingual antonymy constraints. Following Faruqui et al. (2015), who found PPDB constraints more beneficial than the WordNet ones, we do not use BabelNet for monolingual synonymy. Availability of Resources Both PPDB and BabelNet are created automatically. However, PPDB relies on large, high-quality parallel corpora such as Europarl (Koehn, 2005). In total, Multilingual PPDB provides collections of paraphrases for 22 languages. On the other hand, BabelNet uses Wikipedia’s interlanguage links and statistical machine translation (Google Translate) to provide cross-lingual mappings for 271 languages. In our evaluation, we show that PPDB and BabelNet can be used jointly to improve word representations for lower-resource languages by tying them into bilingual spaces with high-resource ones. We validate this claim on Hebrew and Croatian, which act as ‘lower-resource’ languages because of their lack of any PPDB resource and their relatively"
Q17-1022,P15-1027,0,0.0203234,"2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mi"
Q17-1022,P14-2050,0,0.10357,"use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to improve the word representations of lower-resource ones. T"
Q17-1022,P15-1145,0,0.0286255,"Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and d"
Q17-1022,W15-1521,0,0.0244293,". 3 The SimLex-999 web page (www.cl.cam.ac.uk/ ~fh295/simlex.html) lists models with state-of-the-art performance, none of which learn representations jointly. 2.2 Cross-Lingual Word Representations Most existing models which induce cross-lingual word representations rely on cross-lingual distributional information (Klementiev et al., 2012; Zou et al., 2013; Soyer et al., 2015; Huang et al., 2015, inter alia). These models differ in the cross-lingual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shar"
Q17-1022,P15-2130,1,0.34097,"Missing"
Q17-1022,N16-1018,1,0.276948,"Missing"
Q17-1022,P17-1163,1,0.07397,"Missing"
Q17-1022,P16-2074,0,0.108128,"-trained Vectors Rothe and Schütze (2015) fine-tune word vector spaces to improve the representations of synsets/lexemes found in WordNet. Faruqui et al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely"
Q17-1022,N15-1100,0,0.0338437,"ducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than representations produced by fine-tuning distributional vectors. However, their performance has not surpassed that of fine-tuning methods.3 Fine-Tunin"
Q17-1022,Q16-1030,0,0.48294,"de WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2"
Q17-1022,D14-1162,0,0.11428,"t ATTRACT-R EPEL outperforms previous methods which make use of similar lexical resources, achieving state-of-the-art results on two word similarity datasets: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). Introduction Word representation learning has become a research area of central importance in modern natural language processing. The common techniques for inducing distributed word representations are grounded in the distributional hypothesis, relying on co-occurrence information in large textual corpora to learn meaningful word representations (Mikolov et al., 2013b; Pennington et al., 2014; Ó Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014). Recently, methods that go beyond stand-alone unsupervised learning have gained increased popularity. We then deploy the ATTRACT-R EPEL algorithm in a multilingual setting, using semantic relations extracted from BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014), a cross-lingual lexical resource, to inject constraints between words of different languages into the word representations. This allows us to embed vector spaces of multiple languages into a single vector space, exploiting information from high-resource languages to i"
Q17-1022,N15-1058,0,0.0309108,"Missing"
Q17-1022,W16-1622,0,0.0658587,"al. (2015) and Jauhar et al. (2015) use synonymy constraints in a procedure termed retrofitting to bring the vectors of semantically similar words close together, while Wieting et al. (2015) modify the skip-gram objective function to fine-tune word vectors by injecting paraphrasing constraints from PPDB. Mrkši´c et al. (2016) build on the retrofitting approach by jointly injecting synonymy and antonymy constraints; the same idea is reassessed by Nguyen et al. (2016). Kim et al. (2016a) further expand this line of work by incorporating semantic intensity information for the constraints, while Recski et al. (2016) use ensembles of rich concept dictionaries to further improve a combined collection of semantically specialized word vectors. ATTRACT-R EPEL is an instance of the second family of models, providing a portable, light-weight approach for incorporating external knowledge into arbitrary vector spaces. In our experiments, we show that ATTRACT-R EPEL outperforms previously proposed post-processors, setting the new state-of-art performance on the widely used SimLex-999 word similarity dataset. Moreover, we show that starting from distributional vectors allows our method to use existing cross-lingual"
Q17-1022,P15-1173,0,0.0714818,"Missing"
Q17-1022,K15-1026,1,0.312629,"al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches such as the one proposed by Schwartz et al. (2015) use symmetric patterns (Davidov and Rappoport, 2006) to push away antonymous words in 311 their pattern-based vector space. Ono et al. (2015) combines both approaches, using thesauri and distributional data to train embeddings specialized for capturing antonymy. Faruqui and Dyer (2015) use many different lexicons to create interpretable sparse binary vectors which achieve competitive performance across a range of intrinsic evaluation tasks. In theory, word representations produced by models which consider distributional and lexical information jointly could be as good (or better) than represe"
Q17-1022,P13-1045,0,0.0159713,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,D13-1170,0,0.0047346,"ithub.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly)"
Q17-1022,P15-1165,0,0.0197866,"Missing"
Q17-1022,P10-1040,0,0.0185912,"tian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical entailment (Vuli´c et al., 2016). Semantic specialization methods fall (broadly) into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexica"
Q17-1022,P16-1157,0,0.028014,"ual signal/supervision they use to tie languages into unified bilingual vector spaces: some models learn on the basis of parallel word-aligned data (Luong et al., 2015; Coulmance et al., 2015) or sentence-aligned data (Hermann and Blunsom, 2014a; Hermann and Blunsom, 2014b; Chandar et al., 2014; Gouws et al., 2015). Other models require document-aligned data (Søgaard et al., 2015; Vuli´c and Moens, 2016), while some learn on the basis of available bilingual dictionaries (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Lazaridou et al., 2015; Vuli´c and Korhonen, 2016b; Duong et al., 2016). See Upadhyay et al. (2016) and Vuli´c and Korhonen (2016b) for an overview of cross-lingual word embedding work. The inclusion of cross-lingual information results in shared cross-lingual vector spaces which can: a) boost performance on monolingual tasks such as word similarity (Faruqui and Dyer, 2014; Rastogi et al., 2015; Upadhyay et al., 2016); and b) support cross-lingual tasks such as bilingual lexicon induction (Mikolov et al., 2013a; Gouws et al., 2015; Duong et al., 2016), cross-lingual information retrieval (Vuli´c and Moens, 2015; Mitra et al., 2016), and transfer learning for resource-lean languages (Søgaard"
Q17-1022,P16-2084,1,0.84249,"Missing"
Q17-1022,P16-1024,1,0.758571,"Missing"
Q17-1022,E17-1042,1,0.780077,"Missing"
Q17-1022,Q15-1025,0,0.116195,"d Cross-Lingual Constraints Nikola Mrkši´c1,2 , Ivan Vuli´c1 , Diarmuid Ó Séaghdha2 , Ira Leviant3 Roi Reichart3 , Milica Gaši´c1 , Anna Korhonen1 , Steve Young1,2 1 University of Cambridge 2 Apple Inc. 3 Technion, IIT Abstract These models typically build on distributional ones by using human- or automatically-constructed knowledge bases to enrich the semantic content of existing word vector collections. Often this is done as a postprocessing step, where the distributional word vectors are refined to satisfy constraints extracted from a lexical resource such as WordNet (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2016). We term this approach semantic specialization. We present ATTRACT-R EPEL, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. ATTRACT-R EPEL facilitates the use of constraints from mono- and crosslingual resources, yielding semantically specialized cross-lingual vector spaces. Our evaluation shows that the method can make use of existing cross-lingual lexicons to construct highquality vector spaces for a plethora of different languages, facilitating semantic transfer from high- to lower-resource one"
Q17-1022,D16-1157,0,0.0166515,"nsfer (+0.19 / +0.11 over monolingual specialization), with Italian vectors’ performance coming close to the top-performing English ones. 316 Comparison to Baselines Table 3 gives an exhaustive comparison of ATTRACT-R EPEL to counterfitting: ATTRACT-R EPEL achieved substantially stronger performance in all experiments. We believe these results conclusively show that the contextsensitive updates and L2 regularization employed by ATTRACT-R EPEL present a better alternative to the context-insensitive attract/repel terms and pair-wise regularization employed by counter-fitting.11 State-of-the-Art Wieting et al. (2016) note that the hyperparameters of the widely used Paragram-SL999 vectors (Wieting et al., 2015) are tuned on SimLex999, and as such are not comparable to methods which hold out the dataset. This implies that further work which uses these vectors (e.g., (Mrkši´c et al., 11 To understand the relative importance of the contextsensitive updates and the change in regularization, we can compare the two methods to the retrofitting procedure (Faruqui et al., 2015). Retrofitting uses L2 regularization (like ATTRACTR EPEL) and a ‘global’ attract term (like counter-fitting). The performance of retrofitti"
Q17-1022,D12-1111,0,0.0614025,"Missing"
Q17-1022,P14-2089,0,0.0246193,") into two categories: a) those which train distributed representations ‘from scratch’ by combining distributional knowledge and lexical information; and b) those which inject lexical information into pretrained collections of word vectors. Methods from both categories make use of similar lexical resources; common examples include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) or the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). Learning from Scratch Some methods modify the prior or the regularization of the original training procedure using the set of linguistic constraints (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015; Aletras and Stevenson, 2015). Other methods modify the skip-gram (Mikolov et al., 2013b) objective function by introducing semantic constraints (Yih et al., 2012; Liu et al., 2015) to train word vectors which emphasize word similarity over relatedness. Osborne et al. (2016) propose a method for incorporating prior knowledge into the Canonical Correlation Analysis (CCA) method used by Dhillon et al. (2015) to learn spectral word embeddings. While such methods introduce semantic similarity constraints extracted from lexicons, approaches s"
Q17-1022,D13-1141,0,0.0291883,"rce-intensive. All resources related to this paper are available at www.github.com/nmrksic/ attract-repel. These include: 1) the ATTRACTR EPEL source code; 2) bilingual word vector collections combining English with 51 other languages; 3) Hebrew and Croatian intrinsic evaluation datasets; and 4) Italian and German Dialogue State Tracking datasets collected for this work. 2 Related Work 2.1 Semantic Specialization The usefulness of distributional word representations has been demonstrated across many application areas: Part-of-Speech (POS) tagging (Collobert et al., 2011), machine translation (Zou et al., 2013; Devlin et al., 2014), dependency and semantic parsing (Socher et al., 2013a; Bansal et al., 2014; Chen and Manning, 2014; Johannsen et al., 2015; Ammar et al., 2016), sentiment analysis (Socher et al., 2013b), named entity recognition (Turian et al., 2010; Guo et al., 2014), and many others. The importance of semantic specialization for downstream tasks is relatively unexplored, with improvements in performance so far observed for dialogue state tracking (Mrkši´c et al., 2016; Mrkši´c et al., 2017), spoken language understanding (Kim et al., 2016b; Kim et al., 2016a) and judging lexical enta"
Q17-1022,J14-3005,1,\N,Missing
Q17-1022,C98-1013,0,\N,Missing
Q18-1032,E17-1088,0,0.221535,"guarantee a reliable word estimate. Language Modeling (LM) is a key NLP task, serving Since gradual parameter estimation based on conas an important component for applications that retextual information is not feasible for rare phenomena quire some form of text generation, such as machine 451 Transactions of the Association for Computational Linguistics, vol. 6, pp. 451–465, 2018. Action Editor: Brian Roark. Submission batch: 12/2017; Revision batch: 5/2018; Published 7/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in the full vocabulary setup (Adams et al., 2017), it is of crucial importance to construct and enable techniques that can obtain these parameters in alternative ways. One solution is to draw information from additional sources, such as characters and character sequences. As a consequence, such character-aware models should facilitate LM word-level prediction in a real-life LM setup which deals with a large amount of low-frequency or unseen words. Efforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during traini"
Q18-1032,W13-3520,0,0.0421794,"ubsequent fine-tuning. The preserve cost acts as a regularisation pulling the “fine-tuned” vector back to its initial value: X pres(Pw , Nw ) = λreg ||ˆ vw − vw ||2 . (6) xw ∈V w λreg = 10−9 is the L2 -regularisation constant (Mrkši´c et al., 2017); vˆw is the original word vector before the procedure. This term tries to preserve the semantic content present in the original vector space, as long as this information does not contradict the knowledge injected by the constraints. The final cost function adds the two costs: cost = attr + pres. 6 Experiments Datasets We use the Polyglot Wikipedia (Al-Rfou et al., 2013) for all available languages except for Japanese, Chinese, and Thai, and add these and further languages using Wikipedia dumps. The Wiki dumps were cleaned and preprocessed by the Polyglot tokeniser. We construct similarly-sized datasets by extracting 46K sentences for each language from the beginning of each dump, filtered to contain only full sentences, and split into train (40K), validation (3K), and test (3K). The final list of languages along with standard language codes (ISO 639-1 standard, used throughout the paper) and statistics on vocabulary and token counts are provided in Table 4."
Q18-1032,P16-1156,0,0.0377669,"Missing"
Q18-1032,D09-1003,0,0.0188023,"pped to ±2.4 A full summary of all hyper-parameters and their values is provided in Table 3. (Baseline) Language Models The availability of LM evaluation sets in a large number of diverse languages, described in Section 2, now provides an opportunity to perform a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen e"
Q18-1032,N15-1184,0,0.116334,"Missing"
Q18-1032,D15-1042,0,0.057779,"rphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction Daniela Gerz1 , Ivan Vuli´c1 , Edoardo Ponti1 Jason Naradowsky3 , Roi Reichart2 Anna Korhonen1 1 2 Language Technology Lab, DTAL, University of Cambridge Faculty of Industrial Engineering and Management, Technion, IIT 3 Johns Hopkins University 1 {dsg40,iv250,ep490,alk23}@cam.ac.uk 2 roiri@ie.technion.ac.il 3 narad@jhu.edu Abstract translation (Vaswani et al., 2013), speech recognition (Mikolov et al., 2010), dialogue generation (Serban et Neural architectures are prominent in the conal., 2016), or summarisation (Filippova et al., 2015). struction of language models (LMs). HowA traditional recurrent neural network (RNN) LM ever, word-level prediction is typically agnossetup operates on a limited closed vocabulary of tic of subword-level information (characters words (Bengio et al., 2003; Mikolov et al., 2010). and character sequences) and operates over The limitation arises due to the model learning paa closed vocabulary, consisting of a limited rameters exclusive to single words. A standard trainword set. Indeed, while subword-aware models boost performance across a variety of NLP ing procedure for neural LMs gradually modi"
Q18-1032,N13-1092,0,0.0746533,"Missing"
Q18-1032,P13-2121,0,0.0327097,"a full-fledged multilingual analysis of representative LM architectures. At the same time, these different architectures serve as the baselines for our novel model which fine-tunes the output matrix M w . As mentioned, the traditional LM setup is to use words both on the input and on the output side (Goodman, 2001; Bengio et al., 2003; Deschacht and Moens, 2009) relying on n-gram word sequences. We evaluate a strong model from the n-gram family of models from the KenLM package (https://github.com/kpu/kenlm): it is based on 5grams with extended Kneser-Ney smoothing (KN5) (Kneser and Ney, 1995; Heafield et al., 2013) 5 . The rationale behind including this non-neural model is to also probe the limitations of such n-gram-based LM architectures on a diverse set of languages. Recurrent neural networks (RNNs), especially Long-Short-Term Memory networks (LSTMs), have taken over the LM universe recently (Mikolov et al., 2010; Sundermeyer et al., 2015; Chen et al., 2016, i.a.). These LMs map a sequence of input words to embedding vectors using a look-up matrix. The embeddings are passed to the LSTM as input, and 3 This choice has been motivated by the observation that rare words tend to have other rare words as"
Q18-1032,P17-1137,0,0.083385,"Missing"
Q18-1032,2005.mtsummit-papers.11,0,0.0462052,"ised in Table 6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test l"
Q18-1032,P16-1100,0,0.111324,"Missing"
Q18-1032,P11-1015,0,0.0605287,"sed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of"
Q18-1032,J93-2004,0,0.064122,"nd Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent wor"
Q18-1032,D16-1209,0,0.261842,"fforts into this direction have yielded exciting results, primarily on the input side of neural LMs. A standard RNN LM architecture relies on two word representation matrices learned during training for its input and next-word prediction. This effectively means that there are two sets of per-word specific parameters that need to be trained. Recent work shows that it is possible to generate a word representation on-the-fly based on its constituent characters, thereby effectively solving the problem for the parameter set on the input side of the model (Kim et al., 2016; Luong and Manning, 2016; Miyamoto and Cho, 2016; Ling et al., 2015). However, it is not straightforward how to advance these ideas to the output side of the model, as this second set of word-specific parameters is directly responsible for the next-word prediction: it has to encode a much wider range of information, such as topical and semantic knowledge about words, which cannot be easily obtained from its characters alone (Jozefowicz et al., 2016). While one solution is to directly output characters instead of words (Graves, 2013; Miyamoto and Cho, 2016), a recent work from Jozefowicz et al. (2016) suggests that such purely character-base"
Q18-1032,Q17-1022,1,0.901431,"Missing"
Q18-1032,oostdijk-2000-spoken,0,0.0770072,"tion (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 European languages using Wikipedia data, including Finnish. Perhaps the largest and most diverse set of languages used for multilingual LM evaluation so far is the one of Vania and Lopez (2017). Their study includes 10 languages in total representing several morphological types (fusional, e.g., Russian, and agglutinative, e.g., Finnish), as well as languages with particular morphological phenomena (root-and-pattern in Hebrew and reduplication in Malay). In this work, we provide LM evaluation datasets for 50 typologically diverse languages, wi"
Q18-1032,E17-2025,0,0.0199535,"in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutch LM. Kawakami et al. (2017) evaluate on 7 Euro"
Q18-1032,tiedemann-2012-parallel,0,0.0172916,"6. As 461 one important finding, we observe that the gains in perplexity using our fine-tuning AP method extend also to these larger evaluation datasets. In particular, we find improvements of the same magnitude as in the PTB-sized data sets over the strongest baseline model (Char-CNN-LSTM) for all MWC languages. For instance, perplexity is reduced from 1781 to 1578 for Russian, and from 365 to 352 for English. We also observe a gain for French and Spanish with perplexity reduced from 282 to 272 and 255 to 243 respectively. In addition, we test on samples of the Europarl corpus (Koehn, 2005; Tiedemann, 2012) which contains approximately 10 times more tokens than our PTB-sized evaluation data: we use 400K sentences from Europarl for training and testing. However, this data comes from a much narrower domain of parliamentary proceedings: this property yields a very low type/token ratio as visible from Table 6. In fact, we find the type/token ratio in this corpus to be on the same level or even smaller than isolating languages (compare with the scores in Table 4): 0.02 for Dutch and 0.03 for Czech. This leads to similar perplexities with and without +AP for these two selected test languages. The thir"
Q18-1032,P17-1184,0,0.284804,"gical systems. We discuss the implications of typological diversity on the LM task, both theoretically in Section 2, and empirically in Section 7; we find a clear correspondence between performance of state-of-the art LMs and structural linguistic properties. Further, the consistent perplexity gains across the large sample of languages suggest wide applicability of our novel method. Finally, this article can also be read as a comprehensive multilingual analysis of current LM architectures on a set of languages which is much larger than the ones used in recent LM work (Botha and Blunsom, 2014; Vania and Lopez, 2017; Kawakami et al., 2017). We hope that this article with its new datasets, methodology and models, all available online at http://people.ds.cam. ac.uk/dsg40/lmmrl.html, will pave the way for true multilingual research in language modeling. 2 LM Data and Typological Diversity A language model defines a probability distribution over sequences of tokens, and is typically trained to maximise the likelihood of token input sequences. Formally, the LM objective is expressed as follows: P (t1 , ...tn ) = Y i P (ti |t1 , ...ti−1 ). (1) ti is a token with the index i in the sequence. For word-level pred"
Q18-1032,D13-1140,0,0.067895,"Missing"
Q18-1032,E17-1040,0,0.0805667,"Missing"
Q18-1032,P17-1006,1,0.915822,"Missing"
Q18-1032,P16-1125,0,0.0319225,".ti−1 ). (1) ti is a token with the index i in the sequence. For word-level prediction a token corresponds to one word, whereas for character-level (also termed charlevel) prediction it is one character. LMs are most commonly tested on Western European languages. Standard LM benchmarks in English include the Penn Treebank (PTB) (Marcus et al., 1993), the 1 Billion Word Benchmark (BWB) (Chelba et al., 2014), and the Hutter Prize data (Hutter, 2012). English datasets extracted from BBC News (Greene and Cunningham, 2006) and IMDB Movie Reviews (Maas et al., 2011) are also used for LM evaluation (Wang and Cho, 2016; Miyamoto and Cho, 2016; Press and Wolf, 2017). Regarding multilingual LM evaluation, Botha and Blunsom (2014) extract datasets for other languages from the sets provided by the 2013 Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013): they experiment with Czech, French, Spanish, German and Russian. A recent work of Kim et al. (2016) reuses these datasets and adds Arabic. Ling et al. (2015) evaluate on English, Portuguese, Catalan, German and Turkish datasets extracted from Wikipedia. Verwimp et al. (2017) use a subset of the Corpus of Spoken Dutch (Oostdijk, 2000) for Dutc"
Q18-1032,Q15-1025,0,0.161463,"ction cannot fully capture word-level semantics and even hurts LM performance (Jozefowicz et al., 2016). However, shared subword units still provide useful evidence of shared semantics (Cotterell et al., 2016; Vuli´c et al., 2017): injecting this into the space M w to additionally reflect shared subword-level information should lead to improved word vector estimates, especially for MRLs. 5.1 Fine-Tuning and Constraints We inject this information into M w by adapting recent fine-tuning (often termed retrofitting or specialisation) methods for vector space post-processing (Faruqui et al., 2015; Wieting et al., 2015; Mrkši´c et al., 2017; Vuli´c et al., 2017, i.a.). These models enrich initial vector spaces by encoding external knowledge provided in the form of simple linguistic constraints (i.e., word pairs) into the initial vector space. There are two fundamental differences between our work and previous work on specialisation. First, previous models typically use rich hand-crafted lexical resources such as WordNet (Fellbaum, 1998) or the Paraphrase Database (Ganitkevitch et al., 2013), or manually defined rules (Vuli´c et al., 2017) to extract the constraints, while we generate them directly using the"
R13-2021,W02-2024,0,0.00996429,"ofthe-art Stanford NER system when applied to the transcribed speech data. The following sections first review related research, describe the methodology of our approach and the experimental setup, and finally present our evaluation and discuss the results. 2 Related Work Named entity recognition was initially defined in the framework of Message Understanding Conferences (MUC) (Sundheim, 1995a). Since then, many conferences and workshops such as the following MUC editions (Chinchor, 1997; Sundheim, 1995a), the 1999 DARPA broadcast news workshop (Przybocki et al., 1999) and CoNLL shared tasks (Sang, 2002; Sang and Meulder, 2003) focused on extending the state-of-the-art research on NER. One of the first NER systems was designed by Rau (1991). Her system extracts and identifies company names by using hand-crafted heuristic rules. Today, NER in written text still remains a popular task. State-of-the-art NER models typically rely on machine learning algorithms trained on documens with manually annotated named entities. Examples of publicly available NER tools are the Stanford NER, OpenNLP NameFinder1 , Illinois NER system2 , the lingpipe NER system3 . 3 Methodology The task is to label a sequenc"
R13-2021,H05-1062,0,0.0617464,"Missing"
R13-2021,M95-1002,0,0.0153776,"similar written news data. • We present several new methods to recover named entities from speech data by using the external knowledge from high-quality similar written texts. • We improve the performance of the state-ofthe-art Stanford NER system when applied to the transcribed speech data. The following sections first review related research, describe the methodology of our approach and the experimental setup, and finally present our evaluation and discuss the results. 2 Related Work Named entity recognition was initially defined in the framework of Message Understanding Conferences (MUC) (Sundheim, 1995a). Since then, many conferences and workshops such as the following MUC editions (Chinchor, 1997; Sundheim, 1995a), the 1999 DARPA broadcast news workshop (Przybocki et al., 1999) and CoNLL shared tasks (Sang, 2002; Sang and Meulder, 2003) focused on extending the state-of-the-art research on NER. One of the first NER systems was designed by Rau (1991). Her system extracts and identifies company names by using hand-crafted heuristic rules. Today, NER in written text still remains a popular task. State-of-the-art NER models typically rely on machine learning algorithms trained on documens with"
R13-2021,H01-1034,0,0.0271225,"Missing"
R13-2021,W03-0419,0,\N,Missing
S15-2012,S12-1051,0,0.0391829,"r a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011)"
S15-2012,S14-2010,0,0.030874,"Missing"
S15-2012,I13-1041,0,0.0128648,"dditionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in st"
S15-2012,P09-1053,0,0.172779,"ammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluatio"
S15-2012,P12-1091,0,0.0512082,"g the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset prov"
S15-2012,J10-3003,0,0.109662,"es brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 201"
S15-2012,N12-1019,0,0.0843001,"ncy, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages"
S15-2012,D14-1162,0,0.0807917,"tokens in tweets, and the other taking into account only content words. 2 https://github.com/ma-sultan/ monolingual-word-aligner 72 Anchor count (ANC). We re-implemented the M ULTI P model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and γ of our SVM model with the RBF kernel. For the final evaluation, the"
S15-2012,N12-1034,0,0.314377,"Missing"
S15-2012,S12-1060,1,0.880991,"Missing"
S15-2012,S14-2039,0,0.0567314,"nces. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset provided by the organizers, each tweet is associated with a topic, with 10 to 100 tweet pairs per topic. An important preprocessing step is to remove tokens that can be found in the name of a topic. For example, for the topic “Roberto Mancini”, we trim the tweets “Roberto Mancini gets the boot from the Man City” and “City sacked Mancini” to “gets the boot from the Man City” and “City sacked”, respectively, and then compute the features on the trimmed tweets. The rationale is that, given a topic, there is an"
S15-2012,W13-2515,0,0.245099,"Missing"
S15-2012,Q14-1034,0,0.531452,"Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2"
S15-2012,D11-1061,0,0.158719,"2 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number"
S15-2012,S15-2001,0,\N,Missing
S17-1003,P15-1040,0,0.109629,"sequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achiev"
S17-1003,N16-1162,1,0.891573,"Missing"
S17-1003,P15-1162,0,0.0604088,"Missing"
S17-1003,E17-2093,0,0.0424732,"Missing"
S17-1003,D10-1005,0,0.0306589,"ched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function. Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assign"
S17-1003,P11-1033,0,0.0248891,"models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transfer relies on word-aligned parallel texts where the source language text is either manually or automatically annotated. The sentiment information is then projected onto the target text (Almeida et al., 2015), also leveraging non-parallel data (Zhou et al., 2015). Chen et al. (2016) devised a multi-task network where an adversarial branch spurs the shared layers to learn language-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achieve unsatisfactory results in resource-lean 23 (Balahur and Turchi, 2014). Finally, cross-lingual sentiment classification can leverage on shared distributed representations. Zhou et al. (2016) captured shared high-level fea"
S17-1003,D17-1070,0,0.0556519,"Missing"
S17-1003,S14-2001,0,0.0345391,"Missing"
S17-1003,D14-1079,0,0.0159063,"dition) over word representations stemming from a Skip-Gram model (§ 3.1.1). Others are direct methods, including FastSent (§ 3.1.2), a Sequential Denoising AutoEncoder (SDAE, § 3.1.3) and Paragraph Vector (§ 3.1.4). Note that FastSent relies on sentence order, SDAE on word order, and Paragraph Vector on neither. All these algorithms were trained on cleaned-up Wikipedia dumps. The choice of the algorithms was based on following criteria: i) their performance reported in recent surveys (n.b., the surveys were limited to English and evaluated on other tasks), most notably Hill et al. (2016) and Milajevs et al. (2014); ii) the variety of their modelling assumptions and features encoded. The referenced surveys already hinted that the usefulness of a representation is largely dependent on the actual application. Shallower but more interpretable representations can be decoded with spatial distance metrics. Others, more deep and convoluted architectures, outperform the others in supervised tasks. We inquire whether the generalisation is tenable also in the task of Sentiment Analysis targeting sentence polarity. 3.1.1 Additive Skip-Gram As a bottom-up method, we train word embeddings using skip-gram with negati"
S17-1003,D16-1157,0,0.0992835,"t (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. 1 This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016a). 24 Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hill et al. (2016). In FastSent, sentences are represented as bags of words: a context of sentences is used to predict the adjacent sentence. Each word w corresponds to a source vector uw and a target vector vw . A sentence Si is represented P as the sum of the source vectors of its words w∈Si uw . Hence, the cost C of a representation is given by the softmax σ(x) of a sentence representation and the target vectors of the words in its conte"
S17-1003,E17-1096,0,0.0259081,"ource scarcity: they are portable to other tasks and languages. In this section we survey deep learning techniques, adaptive models, and unsupervised distributed representations for sentiment classification in a multilingual scenario. The last approach is the focus of this work. Deep learning algorithms for sentiment classification are designed to deal with compositionality. Hence, they often rely on recurrent networks tracing the sequential history of a sentence, or special compositional devices. Recurrent models include bi-directional LSTMs (Li et al., 2015), possibly enriched with context (Mousa and Schuller, 2017). On the other hand, Socher et al. (2013) put forth a Recursive Neural Tensor Network, which composes representations recursively through a single tensor-based composition function. Subsequent improvements of this line of research include the Structural Attention Neural Networks (Kokkinos and Potamianos, 2017), which adds structural information around each node of a syntactic tree. When supervised monolingual models are not feasible, language transfer can bridge between multiple languages, for instance through supervised latent Dirichlet allocation (Boyd-Graber and Resnik, 2010). Direct transf"
S17-1003,W15-2701,0,0.0198192,"t the word level. In this paper, we focus on sentence representations that are generated in an unsupervised fashion. Furthermore, they are ‘fixed’, that is, they are not fine-tuned for any particular downstream task, since we are interested in their intrinsic content.1 Y (w,c)∈S p(S = 1|w, c, θ) Y p(S 0 = 0|w, c, θ) (w,c)∈S 0 The representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model was proposed by Hill et al. (2016). It hinges on a sentence-level distributional hypothesis (Polajnar et al., 2015; Kiros et al., 2015). In other terms, it assumes that the meaning of a sentence can be inferred by the neighbour sentences in a text. It is a simple additive log-linear model conceived to mitigate the computational expensiveness of algorithms based on a similar assumption. 1 This excludes methods concerned with phrases, like the ECO embeddings (Poliak et al., 2017), or requiring structured knowledge, like CHARAGRAM (Wieting et al., 2016a). 24 Hence, it was preferred over SkipThought (Kiros et al., 2015) because of i) these efficiency issues and ii) its competitive performances reported by Hil"
S17-1003,D11-1016,0,0.0355862,"inates correctly whether it belongs to a set of sentences S or a set of randomly generated incorrect sentences S 0 : Distributed Sentence Representations Word vectors can be combined through various compositional operations to obtain representations of phrases and sentences. Mitchell and Lapata (2010) explored two operations: addition and multiplication. Notwithstanding their simplicity, they are hardly outperformed by more sophisticated operations (Rimell et al., 2016). Some of these compositional representations based on matrix multiplication were also evaluated on sentiment classification (Yessenalina and Cardie, 2011). Alternatively, sentence representations can be induced directly with no intermediate step at the word level. In this paper, we focus on sentence representations that are generated in an unsupervised fashion. Furthermore, they are ‘fixed’, that is, they are not fine-tuned for any particular downstream task, since we are interested in their intrinsic content.1 Y (w,c)∈S p(S = 1|w, c, θ) Y p(S 0 = 0|w, c, θ) (w,c)∈S 0 The representation of a sentence was obtained via element-wise addition of the vectors of the words belonging to it (Mitchell and Lapata, 2010). 3.1.2 FastSent The FastSent model"
S17-1003,E17-2081,0,0.0602349,"Missing"
S17-1003,P16-1133,0,0.0363391,"e-independent features. Finally, Lu et al. (2011) learned from annotated examples in both the source and target language. Alternatively, sentences from other languages are translated into English and assigned a sentiment based on lexical resources (Denecke, 2008) or supervised methods Multilingual Sentiment Analysis The task of sentiment classification is mostly addressed through supervised approaches. However, these achieve unsatisfactory results in resource-lean 23 (Balahur and Turchi, 2014). Finally, cross-lingual sentiment classification can leverage on shared distributed representations. Zhou et al. (2016) captured shared high-level features across aligned sentences through autoencoders. In this latent space, distances were optimised to reflect differences in sentiment. On the other hand, Fern´andez et al. (2015) exploited bilingual word representations, where vector dimensions mirror the distributional overlap with respect to a pivot. Le and Mikolov (2014) concatenated sentence representations obtained through variants of Paragraph Vector and trained a Logistic Regression model on top of them. Previous studies thus demonstrated that sentence representations retain information about polarity, a"
S17-1003,D13-1170,0,0.0639577,"herein. This attitude is measured quantitatively on a scale spanning from negative to positive with arbitrary granularity. As such, polarity consists in a crucial part of the meaning of a sentence, which should not be lost. The polarity of a sentence depends heavily on a complex interaction between lexical items endowed with an intrinsic polarity, and morphosyntactic constructions altering polarity, most notably negation and concession. The interaction is deemed to be recursive, hence some approaches take into account word order and phrase boundaries in order to apply the correct composition (Socher et al., 2013). However, some languages lack continuous constituents: contiguous spans of words do not correspond to syntactic subtrees, making composition unreliable (Ponti, 2016). Moreover, the expression of negation varies across languages, as demonstrated by works in Linguistic Typology (Dahl, 1979, inter alia). In particular, negation can appear as a bounded morpheme or a free morpheme; it can precede or follow the verb; it can ‘agree’ or not in polarity with indefinite pronouns; it can alter the expression of verbal Distributed representations of sentences have been developed recently to represent the"
S17-1003,S15-2027,0,0.0447492,"Missing"
S17-1003,J16-4004,0,\N,Missing
W17-6809,W11-4533,0,0.0171607,"ures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fa"
W17-6809,S12-1059,0,0.0677119,"Missing"
W17-6809,R11-1055,0,0.106629,"gnitive science clearly suggests that human meaning representations are grounded in our perceptual system and sensori-motor experience (Harnad, 1990; Lakoff and Johnson, 1999; Louwerse, 2011, inter alia), previous STS models relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly ext"
W17-6809,S16-1089,0,0.0314869,"Missing"
W17-6809,S13-1005,0,0.014395,"sed STS Measures In the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal"
W17-6809,S16-1116,0,0.0124176,"show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014;"
W17-6809,P16-4010,0,0.0893966,"in all experiments).2 Example images for the four languages we consider in our experiments (cf. Section 5) are shown in Figure 1. We next run a deep convolutional neural network (CNN) pre-trained on the ImageNet classification task (Russakovsky et al., 2015) and extract the 4096dimensional vector from the pre-softmax layer to represent each image. We opt for the VGG network (Simonyan and Zisserman, 2014) which, according to Kiela et al. (2016), has a slight edge on the two other alternatives – AlexNet (Krizhevsky et al., 2012) and GoogLeNet (Szegedy et al., 2015). We used the MMFeat toolkit (Kiela, 2016) to facilitate the process of image retrieval and CNN-based feature extraction. Visual similarity. Because we retrieve more than one image per word, our visual representation of the word is a set of image embedding vectors. This allows for different visual similarity measures taking as input two sets of image embeddings (Kiela et al., 2015), given in Table 1. 3.3 Multi-Modal Representations In order to compute multi-modal STS scores, one can combine linguistic and visual embeddings of words and sentences in a number of ways. Here, we explore three different levels of combining visual and lingu"
W17-6809,D14-1005,0,0.57417,". These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visual information from images alongside linguisti"
W17-6809,P14-2135,0,0.277321,"f similarity scores rather than at the embedding level. Thus, it may be applied both for computing word and sentence similarities. Let sim v be the similarity measure (cf. Section 4) for two words or sentences computed using their visual representations, and let sim t be their similarity computed using their linguistic representations. The late-fusion similarity is then computed as the linear combination of the uni-modal similarities, i.e., as a · sim v + b · sim t . The default late-fusion model uses a = b = 0.5. Selective inclusion of visual information. Previous studies (Hill et al., 2013; Kiela et al., 2014) show that visual signal does not improve the semantic representation equally for all concepts. In fact, the inclusion of visual information deteriorates semantic representations for abstract concepts (e.g., honesty, love, freedom). In order to selectively include the visual information, we need a measure reflecting the quality of the visual signal. To this end, we use the image dispersion score (Kiela et al., 2014). A concept’s image dispersion is the cosine distance between image embeddings, averaged over all pairs of images obtained for the concept w: id (w) = 1 X I(w) 2  1 − cos(ei , ej )"
W17-6809,P15-2020,1,0.923477,"Missing"
W17-6809,D16-1043,0,0.672896,"Missing"
W17-6809,D15-1015,1,0.919942,"Missing"
W17-6809,J99-4009,0,0.936816,") measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). W"
W17-6809,D14-1162,0,0.0807577,"ision, as we were unable to consistently retrieve images for whole sentences as queries. 3.1 Linguistic Representations We use the ubiquitous word embeddings as the linguistic representations of words. Aiming to make our approach language-independent, we opted for embedding models that require nothing but the large corpora as input. Due to the common 1 Using a simple example from Vuli´c et al. (2016), bicycles resemble each other irrespective of whether we call them bicycle, v´elo, fiets, bicicletta, or Fahrrad; see also Fig. 1 3 usage, we chose the Skip-Gram (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) embeddings. For the cross-lingual STS setting, the words of the two languages have to be projected to the same embedding space. To achieve this, we employ the translation matrix model of Mikolov et al. (2013), who have shown that the linear mapping can be established between independently trained embedding spaces. Given a set of translation pairs {si , ti }ni=1 , si ∈ Rds , ti ∈ Rdt (with ds and dt being the sizes of source and target embeddings, respectively), we obtain the translation matrix M ∈ Rds ×dt by minimizing the sum: n X ksi M − ti k2 i=1 Once learned, the matrix M is used to proje"
W17-6809,J03-3002,0,0.0405059,"al Similarity (STS) measures. STS measures exploiting visual signal alone are shown to outperform, in some settings, linguistic-only measures by a wide margin, whereas multi-modal measures yield further performance gains. We also show that selective inclusion of visual information may further boost performance in the multi-modal setup. 1 Introduction Semantic textual similarity (Agirre et al., 2012, 2015, inter alia) measures the degree of semantic equivalence between short texts, usually pairs of sentences. Despite the obvious applicability to sentence alignment for machine translation (MT) (Resnik and Smith, 2003; Aziz and Specia, 2011) or plagiarism detection (Potthast et al., 2011; Franco-Salvador et al., 2013), cross-lingual STS models were proposed only recently (Agirre et al., 2016; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic"
W17-6809,D13-1115,0,0.125683,"Missing"
W17-6809,S12-1060,1,0.897428,"Missing"
W17-6809,N16-1020,0,0.0485671,"s relied exclusively on linguistic processing and textual information. To the best of our knowledge, there has not yet been an STS method that leveraged visual information and combined linguistic and visual input into a visually-informed multi-modal STS system. However, such visually-informed models have been successfully used in other tasks such as selectional preferences (Bergsma and Goebel, 2011), detecting semantic similarity and relatedness (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova et al., 2016), to name only a few. Another important property of visual data is their expected language invariance,1 exploited in recent work on multi-modal modeling in cross-lingual settings (Bergsma and Durme, 2011; Kiela et al., 2015; Vuli´c et al., 2016; Specia et al., 2016). Supported by these findings, in this work we show that our multi-modal STS framework may be straightforwardly extended to cross-lingual settings. 3 Multi-Modal Concept Representations Our multi-modal STS measures combine – at different fusion levels – linguistic and visual concept representations. We obtain linguistic and visual r"
W17-6809,D12-1130,0,0.369012,"16; Brychc´ın and Svoboda, 2016; Jimenez, 2016). These are, however, essentially monolingual STS models coupled with full-blown MT systems that translate sentences to English. Although research in cognitive science (e.g., Lakoff and Johnson (1999); Louwerse (2011)) shows that our meaning representations are grounded in perceptual system, the existing STS models (monolingual and cross-lingual alike) exploit only linguistic signals, despite the fact that models using perceptual information outperform uni-modal linguistic models on tasks like detecting conceptual association and word similarity (Silberer and Lapata, 2012; Bruni et al., 2014; Kiela and Bottou, 2014), predicting phrase compositionality (Roller and Schulte Im Walde, 2013), recognizing lexical entailment (Kiela et al., 2015), and metaphor detection (Shutova 1 et al., 2016). While still predominantly applied in monolingual settings, representations originating from the visual modality are inherently language-invariable (Bergsma and Durme, 2011; Kiela et al., 2015). As such, they could serve as a natural cross-language bridge in cross-lingual STS. In this work, we investigate unsupervised multi-modal and cross-lingual STS models that leverage visua"
W17-6809,W16-2346,0,0.0396104,"Missing"
W17-6809,S14-2039,0,0.0157147,"n the previous section we explained the different levels at which we may combine visual and linguistic representations. However, we still have to define the actual STS measures that compute similarity scores for given pairs of sentences. We propose two simple unsupervised scores for measuring textual similarity. Both scores are agnostic of the actual modality used: this means that we can swap linguistic, visual, and multi-modal vectors as desired without altering the actual STS measure. Optimal aligment similarity. Following the ideas from successful unsupervised STS models (Han et al., 2013; Sultan et al., 2014), we aim to align words between the two sentences at hand. Aiming to devise language-independent STS models (i.e., language-specific tools that could help better align the words are off-limits), we can resort to word similarity measures as the sole information source guiding the alignment process. This STS measure is based on the optimal alignment between the words of the two input sentences. Given the similarity scores for all pairs of words between the sentences S1 and S2 , we are looking for an alignment {(wSi 1 , wSi 2 )}N i=1 (N is the number of aligned pairs, equal to the number of words"
W17-6809,P16-2031,1,0.632732,"Missing"
W17-6809,S15-2045,0,\N,Missing
W17-6809,S12-1051,0,\N,Missing
W17-6809,W13-2609,0,\N,Missing
W17-6809,S16-1081,0,\N,Missing
W18-3018,Q17-1010,0,0.0351138,"del indeed learns useful relationships in the specialised space beyond a simple baseline model that lookups into constraints: large gains over this baseline are reported with a variety of configurations. Distributional SGNS-GN vectors coalesce antonymy and synonymy: as a consequence, they are not a competitive baseline in any of the three evaluation tasks. 7 We have also verified that the specialisation process is robust to the chosen distributional vector space. The best configuration of constraints from Table 2 with two other starting spaces, G LOVE (Pennington et al., 2014) and FAST T EXT (Bojanowski et al., 2017), yields respective correlation scores of 0.787 and 0.774 on SimLex and 0.764 and 0.744 on SimVerb. The model which uses a large set of ANTEXP again cannot match performance of the model which relies on the original ANT. We see this as an interesting finding which suggests that the massive expansion of lexical constraints decreases the strength of originally provided word relationships, which were hand-crafted by linguistic experts. Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL (no 648909). The authors would like to thank the anonymous reviewers for their helpful"
W18-3018,N15-1184,0,0.104398,"Missing"
W18-3018,P15-2076,0,0.042587,"Missing"
W18-3018,D16-1235,1,0.912401,"Missing"
W18-3018,J15-4004,0,0.699622,"ibutions. In this work, we investigate how different constraints affect specialisation. We show that a careful selection of external constraints can guide specialisation models to emphasise lexical contrast in the fine-tuned vector space: e.g., we indicate that direct (i.e., 1-step) WordNet hypernymyhyponymy pairs are useful for boosting lexical contrast. Our specialised word vector spaces yield stateof-the-art results on a range of tasks where modeling lexical contrast is crucial: 1) true semantic similarity; 2) antonymy detection; and 3) distinguishing antonyms from synonyms. Our SimLex999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016) scores are the highest reported results on these datasets to date: the result on SimLex999 is the first result on the dataset surpassing the ceiling of mean inter-annotator agreement. 2 syn (AC) hyp1 (AC) antexp (RC) (outburst, outbreak) (safe, secure) (cordial, warmhearted) (answer, response) (discordance, dissonance) (postmen, deliverymen) (employee, worker) (swap, exchange) (smooth, shake) (clear, obscurity) (relief, pressure) (half, full) Table 1: Examples of linguistic constraints. straints, and are therefore not suited to model both aspects of lexica"
W18-3018,N15-1070,0,0.144392,"al., 2017, i.a.), explicitly modeling the lexical contrast benefits text entailment, dialogue state tracking, spoken language understanding, language generation, etc.2 A popular solution to address the limitation concerning lexical contrast is to move beyond standalone unsupervised learning. Post-processing procedures have been designed that leverage external lexical knowledge available in human- and automatically-constructed lexical resources (e.g., PPDB, WordNet): these methods fine-tune input word vectors to satisfy linguistic constraints from the external resources (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c et al., 2017b, i.a.). This process has been termed retrofitting or vector space specialisation. As one advantage, the post-processing methods are applicable to arbitrary input vector spaces. They are also “light-weight”, that is, they do not require large corpora for (re-)training, as opposed to joint specialisation models (Yu and Dredze, 2014; Kiela et al., 2015; Pham et al., 2015; Nguyen et al., 2016) which integrate lexical knowledge directly into distributional training objectives.3 The main"
W18-3018,D15-1242,0,0.0949452,"Missing"
W18-3018,D14-1162,0,0.0919796,"s also suggest that the specialisation model indeed learns useful relationships in the specialised space beyond a simple baseline model that lookups into constraints: large gains over this baseline are reported with a variety of configurations. Distributional SGNS-GN vectors coalesce antonymy and synonymy: as a consequence, they are not a competitive baseline in any of the three evaluation tasks. 7 We have also verified that the specialisation process is robust to the chosen distributional vector space. The best configuration of constraints from Table 2 with two other starting spaces, G LOVE (Pennington et al., 2014) and FAST T EXT (Bojanowski et al., 2017), yields respective correlation scores of 0.787 and 0.774 on SimLex and 0.764 and 0.744 on SimVerb. The model which uses a large set of ANTEXP again cannot match performance of the model which relies on the original ANT. We see this as an interesting finding which suggests that the massive expansion of lexical constraints decreases the strength of originally provided word relationships, which were hand-crafted by linguistic experts. Acknowledgments This work is supported by the ERC Consolidator Grant LEXICAL (no 648909). The authors would like to thank"
W18-3018,P15-2004,0,0.0577321,"ributional hypothesis (Harris, 1954) generally fail to distinguish highly contrasting words (antonyms) from highly similar ones (synonyms), due to similar word co-occurrence signatures in text corpora (Turney and Pantel, 2010; Mohammad et al., 2013).1 In addition to antonymy and synonymy being fundamental lexical relations that are central to the organisation of the mental lexicon (Miller and Fellbaum, 1991; Murphy, 2010), this undesirable property of distributional word vector spaces has grave implications on their application in NLP reasoning and understanding tasks. As shown in prior work (Pham et al., 2015; Mrkši´c et al., 2016; Kim et al., 2016; Nguyen et al., 2017b; Mrkši´c et al., 2017, i.a.), explicitly modeling the lexical contrast benefits text entailment, dialogue state tracking, spoken language understanding, language generation, etc.2 A popular solution to address the limitation concerning lexical contrast is to move beyond standalone unsupervised learning. Post-processing procedures have been designed that leverage external lexical knowledge available in human- and automatically-constructed lexical resources (e.g., PPDB, WordNet): these methods fine-tune input word vectors to satisfy"
W18-3018,D08-1103,0,0.0333871,"t a representative selection of baselines, currently holding peak scores on the respective benchmarks. Due to a large space of models in our comparison, we refer the interested reader to the original papers for their full descriptions. Task 1: Word Similarity. We evaluate all models on the SimLex-999 dataset (Hill et al., 2015), and SimVerb-3500 (Gerz et al., 2016), a recent verb pair similarity dataset with 3,500 verb pairs.5 The evaluation metric is Spearman’s ρ rank correlation. Task 2: Antonymy Detection. For this task, we rely on the widely used Graduate Record Examination (GRE) dataset (Mohammad et al., 2008, 2013). The task, given an input cue word, is to select the best antonym from five options. Given a word vector space, we take the word with the largest cosine distance to the cue as the best antonym. The GRE dataset contains 950 questions in total. We report balanced F1 scores on the entire dataset. Task 3: Synonymy vs. Antonymy. In this binary classification task, the system must decide whether the relation between two words is synonymy or antonymy. We use the recent dataset of Nguyen et al. (2017b), comprising 1,020 noun (N) test pairs, 908 verb (V) pairs, and 1,986 adjective (A) pairs, wi"
W18-3018,Q17-1022,1,0.912225,"Missing"
W18-3018,D17-1022,0,0.0875502,"Missing"
W18-3018,P16-2074,0,0.203562,"d vectors to satisfy linguistic constraints from the external resources (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c et al., 2017b, i.a.). This process has been termed retrofitting or vector space specialisation. As one advantage, the post-processing methods are applicable to arbitrary input vector spaces. They are also “light-weight”, that is, they do not require large corpora for (re-)training, as opposed to joint specialisation models (Yu and Dredze, 2014; Kiela et al., 2015; Pham et al., 2015; Nguyen et al., 2016) which integrate lexical knowledge directly into distributional training objectives.3 The main driving force of the retrofitting models are the external constraints, which specify which words should be close to each other in the specialised vector space (i.e., the so-called ATTRACT constraints), and which words should be far apart in the space (REPEL). By manipulating the constraints, one can steer the specialisation goal: e.g., Vuli´c et al. (2017a) use verb relations from VerbNet (Kipper, 2005) to accentuate VerbNet-style syntactic-semantic relations in the vector space. 1 As pointed out by"
W18-3018,E17-1008,0,0.126573,"Missing"
W18-3018,N15-1100,0,0.469952,"Earlier post-processors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015) operate only with ATTRACT con138 tional space, as long as this information does not contradict the injected external knowledge. Linguistic Constraints. The constraints are in fact word pairs (xi , xj ), xi , xj ∈ V , where V is the vocabulary represented in the input distributional space. First, the conflation of synonymy and antonymy relations in the input space can be obviously mitigated by assigning synonymy pairs (syn) to the ATTRACT set, and antonymy pairs (ant) to the REPEL set. Further, similar to Ono et al. (2015), it is possible to extend the (typically less exhaustive) list of antonyms by combining the available knowledge from syn and ant word pairs. If (xi , xj ) are a pair of synonyms, and (xi , xk ) are a pair of antonyms, one can add another pair (xj , xk ) to the expanded list of antonyms: this yields a larger set (antexp) to serve as REPEL constraints. Finally, as the analysis of Hill et al. (2015) shows, the taxonomic hypernymy-hyponymy IS - A relation is often mistaken by true synonymy by humans. Therefore, we also experiment with direct (i.e. 1step) IS - A pairs (hyp1) from Wordnet as anothe"
W18-3018,P14-2086,0,0.0344804,"Missing"
W18-3018,P15-1173,0,0.0143178,"plicitly modeling the lexical contrast benefits text entailment, dialogue state tracking, spoken language understanding, language generation, etc.2 A popular solution to address the limitation concerning lexical contrast is to move beyond standalone unsupervised learning. Post-processing procedures have been designed that leverage external lexical knowledge available in human- and automatically-constructed lexical resources (e.g., PPDB, WordNet): these methods fine-tune input word vectors to satisfy linguistic constraints from the external resources (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c et al., 2017b, i.a.). This process has been termed retrofitting or vector space specialisation. As one advantage, the post-processing methods are applicable to arbitrary input vector spaces. They are also “light-weight”, that is, they do not require large corpora for (re-)training, as opposed to joint specialisation models (Yu and Dredze, 2014; Kiela et al., 2015; Pham et al., 2015; Nguyen et al., 2016) which integrate lexical knowledge directly into distributional training objectives.3 The main driving force of the ret"
W18-3018,E14-4008,0,0.0431666,"synonymy and antonymy pairs in each test subset. A classification threshold decides on the relation: all word pairs with their cosine similarity above the threshold are considered synonyms, all the others are antonyms.6 https://github.com/nmrksic/attract-repel 139 5 Unlike WordSim-353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex and SimVerb provide explicit guidelines to discern between true semantic similarity and (more broad) conceptual relatedness, so that related but nonsimilar words (e.g. tiger and jungle) have a low rating. 6 Similar to the work on hypernymy detection (Santus et al., 2014; Nguyen et al., 2017a; Vuli´c and Mrkši´c, 2018), we tune the threshold on a validation set of 206 N pairs, 182 V pairs, and 398 A pairs, also used by Nguyen et al. (2017b). M ODEL SimLex M ODEL SimVerb SGNS-GN (Mikolov et al., 2013) Symmetric Patterns (Schwartz et al., 2015) Non-distributional (Faruqui and Dyer, 2015) Joint Specialisation (Nguyen et al., 2016) Paragram-SL999 (Wieting et al., 2015) Counter-fitting (Mrkši´c et al., 2016) AR: BabelNet (Mrkši´c et al., 2017) 0.414 0.563 0.578 0.590 0.690 0.740 0.751 0.348 0.328 0.596 0.516 0.540 0.628 0.674 RC: ant RC: antexp AC: syn AC: hyp1 AC"
W18-3018,K15-1026,0,0.207661,"Missing"
W18-3018,N18-1048,1,0.872165,"Missing"
W18-3018,N18-1103,1,0.886463,"Missing"
W18-3018,D17-1270,1,0.84938,"Missing"
W18-3018,P17-1006,1,0.891575,"Missing"
W18-3018,Q15-1025,0,0.595807,"ical contrast benefits text entailment, dialogue state tracking, spoken language understanding, language generation, etc.2 A popular solution to address the limitation concerning lexical contrast is to move beyond standalone unsupervised learning. Post-processing procedures have been designed that leverage external lexical knowledge available in human- and automatically-constructed lexical resources (e.g., PPDB, WordNet): these methods fine-tune input word vectors to satisfy linguistic constraints from the external resources (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c et al., 2017b, i.a.). This process has been termed retrofitting or vector space specialisation. As one advantage, the post-processing methods are applicable to arbitrary input vector spaces. They are also “light-weight”, that is, they do not require large corpora for (re-)training, as opposed to joint specialisation models (Yu and Dredze, 2014; Kiela et al., 2015; Pham et al., 2015; Nguyen et al., 2016) which integrate lexical knowledge directly into distributional training objectives.3 The main driving force of the retrofitting models are t"
W18-3018,D12-1111,0,0.107806,"Missing"
W18-3018,P14-2089,0,0.060835,"ces (e.g., PPDB, WordNet): these methods fine-tune input word vectors to satisfy linguistic constraints from the external resources (Faruqui et al., 2015; Jauhar et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c et al., 2017b, i.a.). This process has been termed retrofitting or vector space specialisation. As one advantage, the post-processing methods are applicable to arbitrary input vector spaces. They are also “light-weight”, that is, they do not require large corpora for (re-)training, as opposed to joint specialisation models (Yu and Dredze, 2014; Kiela et al., 2015; Pham et al., 2015; Nguyen et al., 2016) which integrate lexical knowledge directly into distributional training objectives.3 The main driving force of the retrofitting models are the external constraints, which specify which words should be close to each other in the specialised vector space (i.e., the so-called ATTRACT constraints), and which words should be far apart in the space (REPEL). By manipulating the constraints, one can steer the specialisation goal: e.g., Vuli´c et al. (2017a) use verb relations from VerbNet (Kipper, 2005) to accentuate VerbNet-style syntactic"
W18-3018,D14-1161,0,0.448722,") is used for stochastic optimisation, batch size is 50, and we train for 15 epochs. To emphasise lexical contrast in the specialised space we set the respective ATTRACT and REPEL margins δatt and δrpl to the same value: 1.0. We use large 300-dim skip gram vectors with bag-of-words contexts and negative sampling (SGNS-GN) (Mikolov et al., 2013), pre-trained on the 100B Google News corpus. As all other components of the model are kept fixed, the difference in performance can be attributed to the difference in the constraints used. We experiment with external constraints employed in prior work (Zhang et al., 2014; Ono et al., 2015): these were extracted from WordNet (Fellbaum, 1998) and the Roget thesaurus 4 (Kipfer, 2009), and comprise 1,023,082 synonymy (syn) pairs and 380,873 ant pairs. The expanded antexp set of antonyms contains a total of 10,334,811 word pairs. Finally, the hyp1 set extracted from WordNet contains 326,187 word pairs. We evaluate all specialised spaces in three standard tasks with well-defined benchmarks where modeling lexical contrast is beneficial: 1) semantic similarity, 2) antonymy detection, and 3) distinguishing antonyms from synonyms. For each task, we compare against a re"
W19-4101,H90-1021,0,0.798985,"Missing"
W19-4101,W14-4337,1,0.857705,"Missing"
W19-4101,W14-4340,1,0.876953,"t by modular or end-to-end machine learning frameworks (Young, 2010; Vinyals and Le, 2015; Wen et al., 2015, 2017a,b; Mrkši´c and Vuli´c, 2018; Ramadan et al., 2018; Li et al., 2018, inter alia). The research community, as in any machine learning field, benefits from large datasets and standardised evaluation metrics for tracking and comparing different models. However, collecting data to train data-driven dialogue systems has proven notoriously difficult. First, system designers must construct an ontology to define the constrained set of actions and conversations that the system can support (Henderson et al., 2014a,c; Mrkši´c et al., 2015). Furthermore, task-oriented dialogue data must be labeled with highly domain-specific dialogue annotations (El Asri et al., 2017; Budzianowski et al., 2018). Because of this, such annotated dialogue datasets remain scarce, and limited in both their size and in the number of domains they cover. For instance, the recently published MultiWOZ dataset (Budzianowski et al., 2018) contains a total of 115,424 dialogue turns scattered over 7 target domains. Other standard task-based datasets are typically single-domain and smaller by several orders of magnitude: DSTC2 (Hender"
W19-4101,K18-1048,0,0.132835,"Missing"
W19-4101,P18-1031,0,0.0441008,"Missing"
W19-4101,W16-3648,0,0.0195729,", response) pairs can be processed such that the other 99 elements in the batch serve as the negative examples. Response Selection Task The conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Al-Rfou et al., 2016; Henderson et al., 2017; Lowe et al., 2017; Wu et al., 2017; Cer et al., 2018; Chaudhuri et al., Sec"
W19-4101,W18-5708,0,0.0958014,"I would say that the prices are reasonable. This was their second warning. It was so unfortunate to concede the goal. Figure 2: Two examples illustrating the conversational response selection task: given the input context sentence, the goal is to identify the relevant response from a large pool of candidate responses. is filtered such as character names and auditory description text. The English 2018 data consists of 441,450,449 lines, and generates 316,891,717 examples. The data is split into chunks of 100,000 lines, and each chunk is used either for the train set or the test set. 3.3 2018; Du and Black, 2018; Kumar et al., 2018; Liu et al., 2018; Yang et al., 2018; Zhou et al., 2018; Gunasekara et al., 2019; Tao et al., 2019). Models trained to select responses can be used to drive dialogue systems, question-answering systems, and response suggestion systems. The task of response selection provides a powerful signal for learning implicit semantic representations useful for many downstream tasks in natural language understanding (Cer et al., 2018; Yang et al., 2018). AmazonQA This dataset is based on a corpus extracted by Wan and McAuley (2016); McAuley and Yang (2016), who scraped questions and a"
W19-4101,W17-5526,0,0.251859,"Missing"
W19-4101,W19-4107,0,0.3872,"al., 2018; Fadhil and Schiavo, 2019). Conversational systems can also be used to aid in customer service1 or to provide the foundation for intelligent virtual assistants such as Amazon Alexa, Google Assistant, or Apple Siri. Modern approaches to constructing dialogue systems are almost exclusively data-driven, supported 1 For an overview, see poly-ai.com/blog/towards-aiassisted-customer-support-automation 1 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 1–10 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics corpus (Lowe et al., 2015, 2017; Gunasekara et al., 2019), conversations from the three conversational datasets available in the repository are more natural and diverse. What is more, the datasets are large: for instance, after preprocessing around 3.7B comments from Reddit available in 256M conversational threads, we obtain 727M valid contextresponse pairs. Similarly, the number of valid pairs in the OpenSubtitles dataset is 316 million. To put these numbers into perspective, the frequently used Ubuntu corpus v2.0 comprises around 4M dialogue turns. Furthermore, our Reddit corpus includes 2 more years of data and so is substantially larger than the"
W19-4101,L16-1147,0,0.0304019,"than the previous work, 3.7 billion comments rather than 2.1 billion, giving a final dataset with 176 million more examples. Reddit conversations are threaded. Each post may have multiple top-level comments, and every comment may have multiple children comments written in response. In processing, each Reddit thread is used to generate a set of examples. Each response comment generates an example, where 3.2 OpenSubtitles OpenSubtitles is a growing online collection of subtitles for movies and television shows available in multiple languages. As a starting point, we use the corpus collected by Lison and Tiedemann (2016), originally intended for statistical machine translation. This corpus is regenerated every year, in 62 different languages. Consecutive lines in the subtitle data are used to create conversational examples. There is no guarantee that different lines correspond to different speakers, or that consecutive lines belong to the same scene, or even the same show. The data nevertheless contains a lot of interesting examples for modelling the mapping from conversational contexts to responses. Short and long lines are filtered, and some text 3 Input I watched a great movie yesterday. Input Candidate Re"
W19-4101,D15-1309,0,0.0224617,"art. This helps to bound the size of an individual example. The train/test split is deterministic based on the thread ID. As long as all the input to the script is held constant (the input tables, filtering thresholds etc.), the resulting datasets should be identical. The data from 2015 to 2018 inclusive consists of 3,680,746,776 comments, in 256,095,216 threads. In total, 727,013,715 Tensorflow examples are created from this data. Reddit Reddit is an American social news aggregation website, where users can post links, and take part in discussions on these posts. Reddit is extremely diverse (Schrading et al., 2015; Al-Rfou et al., 2016): there are more than 300,000 sub-forums (i.e., subreddits) covering various topics of discussion. These threaded discussions, available in a public BigQuery database, provide a large corpus of conversational contexts paired with appropriate responses. Reddit data has been used to create conversational response selection data by Al-Rfou et al. (2016); Cer et al. (2018); Yang et al. (2018). We share code that allows generating datasets from the Reddit data in a reproducible manner: with consistent filtering, processing, and train/test splitting. We also generate data usin"
W19-4101,N18-3006,0,0.0364799,"pecific dialogue annotations (El Asri et al., 2017; Budzianowski et al., 2018). Because of this, such annotated dialogue datasets remain scarce, and limited in both their size and in the number of domains they cover. For instance, the recently published MultiWOZ dataset (Budzianowski et al., 2018) contains a total of 115,424 dialogue turns scattered over 7 target domains. Other standard task-based datasets are typically single-domain and smaller by several orders of magnitude: DSTC2 (Henderson et al., 2014b) contains 23,354 turns, Frames (El Asri et al., 2017) comprises 19,986 turns, and M2M (Shah et al., 2018) spans 14,796 turns. An alternative solution is to leverage larger conversational datasets available online. Such datasets provide natural conversational structure, that is, the inherent context-to-response relationship which is vital for dialogue modeling. In this work, we present a public repository of three large and diverse conversational datasets containing hundreds of millions of conversation examples. Compared to the most popular conversational datasets used in prior work, such as length-restricted Twitter conversations (Ritter et al., 2010) or very technical domain-restricted technical"
W19-4101,W15-4640,0,0.114118,"nd healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). Conversational systems can also be used to aid in customer service1 or to provide the foundation for intelligent virtual assistants such as Amazon Alexa, Google Assistant, or Apple Siri. Modern approaches to constructing dialogue systems are almost exclusively data-driven, supported 1 For an overview, see poly-ai.com/blog/towards-aiassisted-customer-support-automation 1 Proceedings of the 1st Workshop on NLP for Conversational AI, pages 1–10 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics corpus (Lowe et al., 2015, 2017; Gunasekara et al., 2019), conversations from the three conversational datasets available in the repository are more natural and diverse. What is more, the datasets are large: for instance, after preprocessing around 3.7B comments from Reddit available in 256M conversational threads, we obtain 727M valid contextresponse pairs. Similarly, the number of valid pairs in the OpenSubtitles dataset is 316 million. To put these numbers into perspective, the frequently used Ubuntu corpus v2.0 comprises around 4M dialogue turns. Furthermore, our Reddit corpus includes 2 more years of data and so"
W19-4101,P15-1152,0,0.0272842,"orrelate with user-driven quality metrics (Henderson et al., 2017). For efficient computation of this metric, batches of 100 (context, response) pairs can be processed such that the other 99 elements in the batch serve as the negative examples. Response Selection Task The conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba and Takahashi, 2016"
W19-4101,N15-1020,0,0.0314517,"at has been shown to correlate with user-driven quality metrics (Henderson et al., 2017). For efficient computation of this metric, batches of 100 (context, response) pairs can be processed such that the other 99 elements in the batch serve as the negative examples. Response Selection Task The conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba"
W19-4101,D19-1011,0,0.0222978,"gure 2: Two examples illustrating the conversational response selection task: given the input context sentence, the goal is to identify the relevant response from a large pool of candidate responses. is filtered such as character names and auditory description text. The English 2018 data consists of 441,450,449 lines, and generates 316,891,717 examples. The data is split into chunks of 100,000 lines, and each chunk is used either for the train set or the test set. 3.3 2018; Du and Black, 2018; Kumar et al., 2018; Liu et al., 2018; Yang et al., 2018; Zhou et al., 2018; Gunasekara et al., 2019; Tao et al., 2019). Models trained to select responses can be used to drive dialogue systems, question-answering systems, and response suggestion systems. The task of response selection provides a powerful signal for learning implicit semantic representations useful for many downstream tasks in natural language understanding (Cer et al., 2018; Yang et al., 2018). AmazonQA This dataset is based on a corpus extracted by Wan and McAuley (2016); McAuley and Yang (2016), who scraped questions and answers from Amazon product pages. This provides a corpus of questionanswer pairs in the e-commerce domain. Some question"
W19-4101,P18-2018,1,0.901859,"Missing"
W19-4101,P15-2130,1,0.934744,"Missing"
W19-4101,N18-1202,0,0.0368156,"Missing"
W19-4101,D15-1199,1,0.88015,"Missing"
W19-4101,P18-2069,1,0.911443,"Missing"
W19-4101,N10-1020,0,0.0863883,"et al., 2017) comprises 19,986 turns, and M2M (Shah et al., 2018) spans 14,796 turns. An alternative solution is to leverage larger conversational datasets available online. Such datasets provide natural conversational structure, that is, the inherent context-to-response relationship which is vital for dialogue modeling. In this work, we present a public repository of three large and diverse conversational datasets containing hundreds of millions of conversation examples. Compared to the most popular conversational datasets used in prior work, such as length-restricted Twitter conversations (Ritter et al., 2010) or very technical domain-restricted technical chats from the Ubuntu Progress in Machine Learning is often driven by the availability of large datasets, and consistent evaluation metrics for comparing modeling approaches. To this end, we present a repository of conversational datasets consisting of hundreds of millions of examples, and a standardised evaluation procedure for conversational response selection models using 1-of-100 accuracy. The repository contains scripts that allow researchers to reproduce the standard datasets, or to adapt the pre-processing and data filtering steps to their"
W19-4101,E17-1042,1,0.90923,"Missing"
W19-4101,D11-1054,0,0.0497966,"es a simple summary of model performance that has been shown to correlate with user-driven quality metrics (Henderson et al., 2017). For efficient computation of this metric, batches of 100 (context, response) pairs can be processed such that the other 99 elements in the batch serve as the negative examples. Response Selection Task The conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top"
W19-4101,W12-1812,0,0.0952469,"asets, or to adapt the pre-processing and data filtering steps to their needs. We introduce and evaluate several competitive baselines for conversational response selection, whose implementations are shared in the repository, as well as a neural encoder model that is trained on the entire training set. 1 Introduction Dialogue systems, sometimes referred to as conversational systems or conversational agents, are useful in a wide array of applications. They are used to assist users in accomplishing well-defined tasks such as finding and/or booking flights and restaurants (Hemphill et al., 1990; Williams, 2012; El Asri et al., 2017), or to provide tourist information (Henderson et al., 2014c; Budzianowski et al., 2018). They have found applications in entertainment (Fraser et al., 2018), language learning (Raux et al., 2003; Chen et al., 2017), and healthcare (Laranjo et al., 2018; Fadhil and Schiavo, 2019). Conversational systems can also be used to aid in customer service1 or to provide the foundation for intelligent virtual assistants such as Amazon Alexa, Google Assistant, or Apple Siri. Modern approaches to constructing dialogue systems are almost exclusively data-driven, supported 1 For an ov"
W19-4101,P17-1046,0,0.0303387,"selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Al-Rfou et al., 2016; Henderson et al., 2017; Lowe et al., 2017; Wu et al., 2017; Cer et al., 2018; Chaudhuri et al., Sections 4.1 and 4.2 present baseline methods of conversational response selection that are implemented in the repository. These baselines are intended to run quickly using a subset of the training data, to give some idea of performance and characteristics of each dataset. Section 4.3 describes a more competitive neural encoder model that is trained on the entire training set. 4 4.1 The W and α parameters are learned on a random sample of 10,000 examples from the training set, using the dot product loss from Henderson et al. (2017). A sweep over learning r"
W19-4101,W18-3022,0,0.391998,"d from this data. Reddit Reddit is an American social news aggregation website, where users can post links, and take part in discussions on these posts. Reddit is extremely diverse (Schrading et al., 2015; Al-Rfou et al., 2016): there are more than 300,000 sub-forums (i.e., subreddits) covering various topics of discussion. These threaded discussions, available in a public BigQuery database, provide a large corpus of conversational contexts paired with appropriate responses. Reddit data has been used to create conversational response selection data by Al-Rfou et al. (2016); Cer et al. (2018); Yang et al. (2018). We share code that allows generating datasets from the Reddit data in a reproducible manner: with consistent filtering, processing, and train/test splitting. We also generate data using two more years of data than the previous work, 3.7 billion comments rather than 2.1 billion, giving a final dataset with 176 million more examples. Reddit conversations are threaded. Each post may have multiple top-level comments, and every comment may have multiple children comments written in response. In processing, each Reddit thread is used to generate a set of examples. Each response comment generates a"
W19-4101,W16-3649,0,0.0146907,"ocessed such that the other 99 elements in the batch serve as the negative examples. Response Selection Task The conversational datasets included in this repository facilitate the training and evaluation of a variety of models for natural language tasks. For instance, the datasets are suitable for training generative models of conversational response (Serban et al., 2016; Ritter et al., 2011; Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Kannan et al., 2016), as well as discriminative methods of conversational response selection (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Henderson et al., 2017). The task of conversational response selection is to identify a correct response to a given conversational context from a pool of candidates, as illustrated in figure 2. Such models are typically evaluated using Recall@k, a typical metric in information retrieval literature. This measures how often the correct response is identified as one of the top k ranked responses (Lowe et al., 2015; Inaba and Takahashi, 2016; Yu et al., 2016; Al-Rfou et al., 2016; Henderson et al., 2017; Lowe et al., 2017; Wu et al., 2017; Cer et al., 2018; Chaudhuri et al., Sections 4.1 and 4.2"
W19-4101,P18-1103,0,0.0302522,"t was so unfortunate to concede the goal. Figure 2: Two examples illustrating the conversational response selection task: given the input context sentence, the goal is to identify the relevant response from a large pool of candidate responses. is filtered such as character names and auditory description text. The English 2018 data consists of 441,450,449 lines, and generates 316,891,717 examples. The data is split into chunks of 100,000 lines, and each chunk is used either for the train set or the test set. 3.3 2018; Du and Black, 2018; Kumar et al., 2018; Liu et al., 2018; Yang et al., 2018; Zhou et al., 2018; Gunasekara et al., 2019; Tao et al., 2019). Models trained to select responses can be used to drive dialogue systems, question-answering systems, and response suggestion systems. The task of response selection provides a powerful signal for learning implicit semantic representations useful for many downstream tasks in natural language understanding (Cer et al., 2018; Yang et al., 2018). AmazonQA This dataset is based on a corpus extracted by Wan and McAuley (2016); McAuley and Yang (2016), who scraped questions and answers from Amazon product pages. This provides a corpus of questionanswer p"
W19-4101,D18-1547,1,\N,Missing
W19-4101,N19-1423,0,\N,Missing
W19-4310,D15-1075,0,0.0303085,"´c3 1 Oracle Labs 2 Ubiquitous Knowledge Processing Lab (UKP-TUDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different rel"
W19-4310,N18-1045,0,0.0912161,"rke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vecto"
W19-4310,D18-1024,0,0.0380775,"Lingual LE Specialization Transfer The POSTLE models enable LE specialization of vectors of words unseen in lexical constraints. Conceptually, this also allows for a LE-specialization of a distributional space of another language (possibly without any external constraints), provided a shared bilingual distributional word vector space. To this end, we can resort to any of the methods for inducing shared cross-lingual vector spaces (Ruder et al., 2018). What is more, most recent methods successfully learn the shared space without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018; Chen and Cardie, 2018; Hoshen and Wolf, 2018). Let Xt be the distributional space of some target language for which we have no external lexical constraints and let P (x; θP ) : Rdt 7→ Rds be the (linear) function projecting vectors xt ∈ Xt to the distributional space Xds of the source language with available lexical constraints for which 3 Simply minimizing Euclidean distance also aligns vectors in terms of both direction and size. However, we consistently obtained better results by the objective function from Eq. (6). 75 with m = 2, 048 units and Leaky ReLU (slope 0.2) (Maas et al., 2014) for the generator. The d"
W19-4310,W09-0215,0,0.0509636,"urthermore, transfers with unsupervised (Ar, Co) and supervised bilingual mapping (Sm) yield comparable performance. This implies that a robust LE-specialization of distributional vectors for languages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et"
W19-4310,W13-3520,0,0.227478,"Missing"
W19-4310,P18-1073,0,0.375664,"LE specialization 2a. POSTLE: LE specialization of all words in the source lang observed with different input distributional spaces in several LE-related tasks such as hypernymy detection and directionality, and graded lexical entailment. What is more, the highest gains are reported for resource-lean data scenarios where a high percentage of words in the datasets is unseen. Finally, we show how to LE-specialize distributional spaces for target languages that lack external lexical knowledge. POSTLE can be coupled with any model for inducing cross-lingual embedding spaces (Conneau et al., 2018; Artetxe et al., 2018; Smith et al., 2017). If this model is unsupervised, the procedure effectively yields a zero-shot LE specialization transfer, and holds promise to support the construction of hierarchical semantic networks for resource-lean languages in future work. Source Lang Target Lang Distributional word vectors LE-specialized word vectors 2b. POSTLE: LE specialization of all words in the target lang Figure 1: High-level overview of a) the POSTLE full vocabulary specialization process; and b) zero-shot crosslingual specialization for LE. This relies on an initial shared cross-lingual word embedding space"
W19-4310,E12-1004,0,0.190005,"n, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vectors of words seen in external resources. The same limitation holds for a family of recent models that embed concept hierarchies (i.e., trees or directed acyclic graphs) in hyperbolic spaces (Nickel and Kiela"
W19-4310,N15-1184,0,0.28703,"Missing"
W19-4310,W11-2501,0,0.321138,"Missing"
W19-4310,I13-1095,0,0.173251,"UDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader"
W19-4310,P05-1014,0,0.395253,"anguages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni e"
W19-4310,P18-1004,1,0.83187,"Missing"
W19-4310,S12-1012,0,0.0487335,"d (Ar, Co) and supervised bilingual mapping (Sm) yield comparable performance. This implies that a robust LE-specialization of distributional vectors for languages with no lexico-semantic resources is possible even without any bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or exter"
W19-4310,D17-1185,1,0.917865,"Missing"
W19-4310,P14-2050,0,0.263993,"Missing"
W19-4310,gonzalez-agirre-etal-2012-multilingual,0,0.0233013,"an adversarial unsupervised model fine-tuned with the closed-form Procustes solution (Conneau et al., 2018); 2) an unsupervised self-learning algorithm that iteratively bootstraps new bilingual seeds, initialized according to structural similarities of the monolingual spaces (Artetxe et al., 2018); 3) an orthogonal linear mapping with inverse softmax, supervised by 5K bilingual seeds (Smith et al., 2017). We test POSTLE-specialized Spanish and French word vectors on WN-Hy-ES and WN-Hy-FR, two equally sized datasets (148K word pairs) created by Glavaš and Ponzetto (2017) using the ES WordNet (Gonzalez-Agirre et al., 2012) and the FR WordNet (Sagot and Fišer, 2008). We perform a ranking evaluation: the aim is to rank LE pairs above pairs standing in other relations (meronyms, synonyms, antonyms, and reverse LE). We rank word pairs in the ascending order based on ILE , see Eq. (10). Sm .742 .786 Table 2: Average precision (AP) of POSTLE models in cross-lingual transfer. Results are shown for both POSTLE models ( DFFN and ADV ), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar (Artetxe et al., 2018), Co (Conneau et al., 2018), and Sm (Smith et al., 2017). settin"
W19-4310,P15-1145,0,0.132792,"ace. Related Work Vector Space Specialization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any inpu"
W19-4310,K16-1006,0,0.0214983,"The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as W"
W19-4310,P16-1193,0,0.0609727,"tion. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). Howev"
W19-4310,P13-2078,0,0.0163404,"ny bilingual signal or translation effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 20"
W19-4310,D18-1043,0,0.0379808,"on Transfer The POSTLE models enable LE specialization of vectors of words unseen in lexical constraints. Conceptually, this also allows for a LE-specialization of a distributional space of another language (possibly without any external constraints), provided a shared bilingual distributional word vector space. To this end, we can resort to any of the methods for inducing shared cross-lingual vector spaces (Ruder et al., 2018). What is more, most recent methods successfully learn the shared space without any bilingual signal (Conneau et al., 2018; Artetxe et al., 2018; Chen and Cardie, 2018; Hoshen and Wolf, 2018). Let Xt be the distributional space of some target language for which we have no external lexical constraints and let P (x; θP ) : Rdt 7→ Rds be the (linear) function projecting vectors xt ∈ Xt to the distributional space Xds of the source language with available lexical constraints for which 3 Simply minimizing Euclidean distance also aligns vectors in terms of both direction and size. However, we consistently obtained better results by the objective function from Eq. (6). 75 with m = 2, 048 units and Leaky ReLU (slope 0.2) (Maas et al., 2014) for the generator. The discriminator uses H = 2"
W19-4310,W13-0904,0,0.0234062,"TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015"
W19-4310,N15-1070,0,0.0220684,"is (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic similarity relation."
W19-4310,P15-2020,1,0.934011,"Missing"
W19-4310,Q17-1022,1,0.906083,"Missing"
W19-4310,C14-1097,0,0.0609212,"Missing"
W19-4310,P16-2074,0,0.0372945,"015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic similarity relation. Vuli´c et al. (2018)"
W19-4310,P15-1173,0,0.0284036,"et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to t"
W19-4310,E14-4008,0,0.21465,"n effort. 5 able methodology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018)"
W19-4310,N15-1100,0,0.51525,"of specific concepts. True LE pairs should display both a small cosine distance and a negative norm difference. Therefore, in different LE tasks we can rank the candidate pairs in the ascending order of their asymmetric LE distance ILE . The LE distances are trivially transformed into binary LE predictions, using a binarization threshold t: if ILE (x, y) < t, we predict that LE holds between words x and y with vectors x and y. Linguistic Constraints. We use the same set of constraints as LEAR in prior work (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialization (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (parrot, bird), (bird, animal), and (parrot, animal) are in the LE set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. 4 Evaluation and Results We extensively evaluate the proposed POSTLE models on two fundamental LE tasks: 1) predicting graded LE and 2) LE detection (and directiona"
W19-4310,Q16-1030,0,0.0860588,"alization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify"
W19-4310,K15-1026,0,0.132958,"(Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as WordNet (Fellbaum, 1998) or BabelNet (Navigli and Ponzetto, 2012). This process, termed retrofitting or semantic specialization, is beneficial to language understanding tasks (Faruqui, 2016; Glavaš and Vuli´c, 2018) and is extremely versatile as it can be applied on top of any input distributional space. Retrofitting methods, however, have a major weakness: they only locally"
W19-4310,P16-1226,0,0.289881,"Missing"
W19-4310,D14-1162,0,0.0887635,".uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from extern"
W19-4310,E17-1007,0,0.300758,"odology for specializing all distributional word vectors for the LE relation as well. Modeling Lexical Entailment. Extensive research effort in lexical semantics has been dedicated to automatic detection of the fundamental taxonomic LE relation. Early approaches (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, inter alia) detected LE word pairs by means of asymmetric direction-aware mechanisms such as distributional inclusion hypothesis (Geffet and Dagan, 2005), and concept informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method bel"
W19-4310,N18-1202,0,0.0487797,"rchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency, a standard solution is a post-processing step: distributional vectors are gradually refined to satisfy linguistic constraints extracted from external resources such as WordNet (Fellbaum, 1998) or BabelNet (Navigli a"
W19-4310,D18-1026,1,0.791735,"Missing"
W19-4310,P06-1101,0,0.0982207,"up, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synonymy, antonymy, meronymy and LE and broader topical relatedness (Schwartz et al., 2015; Mrkši´c et al., 2017). To mitigate this deficiency"
W19-4310,D16-1039,0,0.0443433,"informativeness and generality (Herbelot and Ganesalingam, 2013; Santus et al., 2014; Shwartz et al., 2017), but were surpassed by more recent methods that leverage word embeddings. Embedding-based methods either 1) induce LEoriented vector spaces using text (Vilnis and McCallum, 2015; Yu et al., 2015; Vendrov et al., 2016; Henderson and Popa, 2016; Nguyen et al., 2017; Chang et al., 2018; Vuli´c and Mrkši´c, 2018) and/or external hierarchies (Nickel and Kiela, 2017, 2018; Sala et al., 2018) or 2) use distributional vectors as features for supervised LE detection models (Baroni et al., 2012; Tuan et al., 2016; Shwartz et al., 2016; Glavaš and Ponzetto, 2017; Rei et al., 2018). Our POSTLE method belongs to the first group. Vuli´c and Mrkši´c (2018) proposed LEAR, a retrofitting LE model which displays performance gains on a spectrum of graded and ungraded LE evaluations compared to joint specialization models (Nguyen et al., 2017). However, LEAR still specializes only the vectors of words seen in external resources. The same limitation holds for a family of recent models that embed concept hierarchies (i.e., trees or directed acyclic graphs) in hyperbolic spaces (Nickel and Kiela, 2017; Chamberlain"
W19-4310,P18-2101,1,0.874487,"Missing"
W19-4310,P16-1157,0,0.0247947,"erarchical relationships between concepts. Following previous work (Nickel and Kiela, 2017; Vuli´c and Mrkši´c, 2018), we evaluate graded LE on the standard HyperLex dataset (Vuli´c et al., 2017).5 HyperLex contains 2,616 word pairs (2,163 noun pairs, the rest are verb pairs) rated by humans by 4 We experiment with unsupervised and weakly supervised models for inducing cross-lingual embedding spaces. However, we stress that the POSTLE specialization transfer is equally applicable on top of any method for inducing crosslingual word vectors, some of which may require more bilingual supervision (Upadhyay et al., 2016; Ruder et al., 2018). 5 Graded LE is a phenomenon deeply rooted in cognitive science and linguistics: it captures the notions of concept prototypicality (Rosch, 1973; Medin et al., 1984) and category vagueness (Kamp and Partee, 1995; Hampton, 2007). We refer the reader to the original paper for a more detailed discussion. 76 LEAR DFFN ADV LEAR Spearman’s ρ correlation 0.65 Spearman’s ρ correlation DFFN ADV 0.65 0.55 0.45 0.35 0.55 0.45 0.35 0.25 0.25 0 30 50 70 Percentage of seen HyperLex words 90 100 0 (a) SGNS - BOW 2 30 50 70 Percentage of seen HyperLex words 90 100 (b) GLOVE Figure 2: Spe"
W19-4310,J17-4004,1,0.900171,"Missing"
W19-4310,N18-1048,1,0.863589,"Missing"
W19-4310,N18-1103,1,0.430208,"Missing"
W19-4310,C14-1212,0,0.497525,"Missing"
W19-4310,C04-1146,0,0.537145,"Missing"
W19-4310,Q15-1025,0,0.199464,"cal Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018) use external constraints to posthoc fine-tune distributional spaces. Effectively, this makes them applicable to any input distributional space, but they modify only vectors of words seen in the external resource. Nonetheless, retrofitting models tend to outperform joint models in the context of both similarity-based (Mrkši´c et al., 2016) and LE specialization (Vuli´c and Mrkši´c, 2018). The recent post-specialization paradigm has been so far applied only to the symmetric semantic"
W19-4310,N18-1101,0,0.020584,"biquitous Knowledge Processing Lab (UKP-TUDA), TU Darmstadt 3 Language Technology Lab, TAL, University of Cambridge 4 Data and Web Science Group, University of Mannheim 1 aishwarya.kamath@oracle.com 2 pfeiffer@ukp.informatik.tu-darmstadt.de 3 {ep490,iv250}@cam.ac.uk 4 goran@informatik.uni-mannheim.de Abstract The set of these relations constitutes a hierarchical structure that forms the backbone of semantic networks such as WordNet (Fellbaum, 1998). Automatic reasoning about word-level LE benefits a plethora of tasks such as natural language inference (Dagan et al., 2013; Bowman et al., 2015; Williams et al., 2018), text generation (Biran and McKeown, 2013), metaphor detection (Mohler et al., 2013), and automatic taxonomy creation (Snow et al., 2006; Navigli et al., 2011; Gupta et al., 2017). However, standard techniques for inducing word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Melamud et al., 2016; Bojanowski et al., 2017; Peters et al., 2018, inter alia) are unable to effectively capture LE. Due to their crucial dependence on contextual information and the distributional hypothesis (Harris, 1954), they display a clear tendency towards conflating different relationships such as synon"
W19-4310,P14-2089,0,0.16496,"vocabulary words, and unlike joint models, it is computationally inexpensive and applicable to any distributional vector space. Related Work Vector Space Specialization. In general, lexical specialization models fall into two categories: 1) joint optimization models and 2) post-processing or retrofitting models. Joint models integrate external constraints directly into the distributional objective of embedding algorithms such as Skip-Gram and CBOW (Mikolov et al., 2013), or Canonical Correlation Analysis (Dhillon et al., 2015). They either modify the prior or regularization of the objective (Yu and Dredze, 2014; Xu et al., 2014; Kiela et al., 2015) or augment it with factors reflecting external lexical knowledge (Liu et al., 2015; Ono et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Each joint model is tightly coupled to a specific distributional objective: any change to the underlying distributional model requires a modification of the whole joint model and expensive retraining. In contrast, retrofitting models (Faruqui et al., 2015; Rothe and Schütze, 2015; Wieting et al., 2015; Jauhar et al., 2015; Nguyen et al., 2016; Mrkši´c et al., 2016; Mrkši´c et al., 2017; Vuli´c and Mrkši´c, 2018)"
W19-4310,D14-1161,0,0.314347,"norms than vectors of specific concepts. True LE pairs should display both a small cosine distance and a negative norm difference. Therefore, in different LE tasks we can rank the candidate pairs in the ascending order of their asymmetric LE distance ILE . The LE distances are trivially transformed into binary LE predictions, using a binarization threshold t: if ILE (x, y) < t, we predict that LE holds between words x and y with vectors x and y. Linguistic Constraints. We use the same set of constraints as LEAR in prior work (Vuli´c and Mrkši´c, 2018): synonymy and antonymy constraints from (Zhang et al., 2014; Ono et al., 2015) are extracted from WordNet and Roget’s Thesaurus (Kipfer, 2009). As in other work on LE specialization (Nguyen et al., 2017; Nickel and Kiela, 2017), asymmetric LE constraints are extracted from WordNet, and we collect both direct and indirect LE pairs (i.e., (parrot, bird), (bird, animal), and (parrot, animal) are in the LE set) In total, we work with 1,023,082 pairs of synonyms, 380,873 pairs of antonyms, and 1,545,630 LE pairs. 4 Evaluation and Results We extensively evaluate the proposed POSTLE models on two fundamental LE tasks: 1) predicting graded LE and 2) LE detect"
