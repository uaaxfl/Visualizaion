W19-2403,Deep Natural Language Understanding of News Text,2019,-1,-1,4,0,24684,jaya shree,Proceedings of the First Workshop on Narrative Understanding,0,"Early proposals for the deep understanding of natural language text advocated an approach of {``}interpretation as abduction,{''} where the meaning of a text was derived as an explanation that logically entailed the input words, given a knowledge base of lexical and commonsense axioms. While most subsequent NLP research has instead pursued statistical and data-driven methods, the approach of interpretation as abduction has seen steady advancements in both theory and software implementations. In this paper, we summarize advances in deriving the logical form of the text, encoding commonsense knowledge, and technologies for scalable abductive reasoning. We then explore the application of these advancements to the deep understanding of a paragraph of news text, where the subtle meaning of words and phrases are resolved by backward chaining on a knowledge base of 80 hand-authored axioms."
W15-1406,High-Precision Abductive Mapping of Multilingual Metaphors,2015,20,4,2,0,27156,jonathan gordon,Proceedings of the Third Workshop on Metaphor in {NLP},0,"Metaphor is a cognitive phenomenon exhibited in language, where one conceptual domain (the target) is thought of in terms of another (the source). The first level of metaphor interpretation is the mapping of linguistic metaphors to pairs of source and target concepts. Based on the abductive approach to metaphor interpretation proposed by Hobbs (1992) and implemented in the open-source Metaphor-ADP system (Ovchinnikova et al., 2014), we present work to automatically learn knowledge bases to support high-precision conceptual metaphor mapping in English, Spanish, Farsi, and Russian."
W15-1407,A Corpus of Rich Metaphor Annotation,2015,20,2,2,0,27156,jonathan gordon,Proceedings of the Third Workshop on Metaphor in {NLP},0,"Metaphor is a central phenomenon of language, and thus a central problem for natural language understanding. Previous work on the analysis of metaphors has identified which target concepts are being thought of and described in terms of which source concepts, but this is not adequate to explain what motivates the use of particular metaphors. This work proposes the use of conceptual schemas to represent the underspecified scenarios that motivate a metaphoric mapping. To support the creation of systems that can understand metaphors in this way, we have created and are publicly releasing a corpus of manually validated metaphor annotations."
W14-3003,"Case, Constructions, {F}rame{N}et, and the Deep Lexicon",2014,5,0,1,1,24687,jerry hobbs,Proceedings of Frame Semantics in {NLP}: A Workshop in Honor of Chuck {F}illmore (1929-2014),0,"Three major contributions that Charles Fillmore made in linguistics play an important role in the enterprise of deep lexical semantics, which is the effort to link lexical meaning to underlying abstract core theories. I will discuss how case relates to lexical decompositions, how motivated constructions span the borderline between syntax and semantics, and how the frames of FrameNet provide an excellent first step in deep inference."
W14-2305,Abductive Inference for Interpretation of Metaphors,2014,19,10,6,1,38682,ekaterina ovchinnikova,Proceedings of the Second Workshop on Metaphor in {NLP},0,"This paper presents a metaphor interpretation pipeline based on abductive inference. In this framework following (Hobbs, 1992) metaphor interpretation is modelled as a part of the general discourse processing problem, such that the overall discourse coherence is supported. We present an experimental evaluation of the proposed approach using linguistic data in English and Russian."
W13-3817,Abduction for Discourse Interpretation: A Probabilistic Framework,2013,21,9,3,1,38682,ekaterina ovchinnikova,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"Abduction allows us to model interpretation of discourse as the explanation of observables, given additional knowledge about the world. In an abductive framework, many explanations can be constructed for the same observation, requiring an approach to estimate the likelihood of these alternative explanations. We show that, for discourse interpretation, weighted abduction has advantages over alternative approaches to estimating the likelihood of hypotheses. However, weighted abduction has no probabilistic interpretation, which makes the estimation and learning of weights difficult. To address this, we propose a formal probabilistic abductive framework that captures the advantages weighted abduction when applied to discourse interpretation."
J13-4001,{ACL} Lifetime Achievement Award: Influences and Inferences,2013,-1,-1,1,1,24687,jerry hobbs,Computational Linguistics,0,None
C12-1079,Coreference Resolution with {ILP}-based Weighted Abduction,2012,45,13,4,0,7295,naoya inoue,Proceedings of {COLING} 2012,0,"This paper presents the first systematic study of the coreference resolution problem in a general inference-based discourse processing framework. Employing the mode of inference called weighted abduction, we propose a novel solution to the overmerging problem inherent to inference-based frameworks. The overmerging problem consists in erroneously assuming distinct entities to be identical. In discourse processing, overmerging causes establishing wrong coreference links. In order to approach this problem, we extend Hobbs et al. (1993)xe2x80x99s weighted abduction by introducing weighted unification and show how to learn the unification weights by applying machine learning techniques. For making large-scale processing and parameter learning in an abductive logic framework feasible, we employ a new efficient implementation of weighted abduction based on Integer Linear Programming. We then propose several linguistically motivated features for blocking incorrect unifications and employ different large-scale world knowledge resources for establishing unification via inference. We provide a large-scale evaluation on the CoNLL-2011 shared task dataset, showing that all features and almost all knowledge components improve the performance of our system."
W11-0107,Implementing Weighted Abduction in {M}arkov {L}ogic,2011,-1,-1,2,0,44451,james blythe,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,None
W11-0121,Elaborating a Knowledge Base for Deep Lexical Semantics,2011,12,8,2,0,38685,niloofar montazeri,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"We describe the methodology for constructing axioms defining event-related words, anchored in core theories of change of state and causality. We first derive from WordNet senses a smaller set of abstract, general supersenses. We encode axioms for these, and we test them on textual entailment pairs. We look at two specific examples in detail to illustrate both the power of the method and the holes in the knowledge base that it exposes. Then we address the problem of holes more systematically, asking, for example, what kinds of pairwise interactions are possible for core theory predicates like change and cause."
W11-0124,Abductive Reasoning with a Large Knowledge Base for Discourse Processing,2011,28,32,4,1,38682,ekaterina ovchinnikova,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"This paper presents a discourse processing framework based on weighted abduction. We elaborate on ideas described in Hobbs et al. (1993) and implement the abductive inference procedure in a system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowledge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In addition, we provide an evaluation of the semantic role labeling produced by the system taking the Frame-Annotated Corpus for Textual Entailment as a gold standard."
W11-0143,Granularity in Natural Language Discourse,2011,7,16,2,0,44463,rutu mulkarmehta,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"This paper discusses the phenomenon of granularity in natural language1. By 'granularity' we mean the level of detail of description of an event or object. Humans can seamlessly shift their granularity perspective while reading or understanding a text. To emulate this mechanism, we describe a set of features that identify the levels of granularity in text, and empirically verify this feature set using a human annotation study for granularity identification. This theory is the foundation for any system that can learn the (global) behavior of event descriptions from (local) behavior descriptions. This is the first research initiative, to our knowledge, for identifying granularity shifts in natural language descriptions."
J11-4005,Annotating and Learning Event Durations in Text,2011,45,10,3,1,44738,feng pan,Computational Linguistics,0,"This article presents our work on constructing a corpus of news articles in which events are annotated for estimated bounds on their duration, and automatically learning from this corpus. We describe the annotation guidelines, the event classes we categorized to reduce gross discrepancies in inter-annotator judgments, and our use of normal distributions to model vague and implicit temporal information and to measure inter-annotator agreement for these event duration distributions. We then show that machine learning techniques applied to this data can produce coarse-grained event duration information automatically, considerably outperforming a baseline and approaching human performance. The methods described here should be applicable to other kinds of vague but substantive information in texts."
W08-2205,Augmenting {W}ord{N}et for Deep Understanding of Text,2008,16,20,3,1,3543,peter clark,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"One of the big challenges in understanding text, i.e., constructing an overall coherent representation of the text, is that much information needed in that representation is unstated (implicit). Thus, in order to fill in the gaps and create an overall representation, language processing systems need a large amount of world knowledge, and creating those knowledge resources remains a fundamental challenge. In our current work, we are seeking to augment WordNet as a knowledge resource for language understanding in several ways: adding in formal versions of its word sense definitions (glosses); classifying the morphosemantic links between nouns and verbs; encoding a small number of core theories about WordNet's most commonly used terms; and adding in simple representations of scripts. Although this is still work in progress, we describe our experiences so far with what we hope will be a significantly improved resource for the deep understanding of language."
W08-2217,Refining the Meaning of Sense Labels in {PDTB}: {``}Concession{''},2008,14,4,3,0,17179,livio robaldo,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"The most recent release of PDTB 2.0 contains annotations of senses of connectives. The PDTB 2.0 manual describes the hierarchical set of senses used in the annotation and offers rough semantic descriptions of each label. In this paper, we refine the semantics of concession substantially and offer a formal description of concessive relations and the associated inferences drawn by the reader, utilizing basic notions from Hobbs's logic, including the distinction between causes and causal complexes (Hobbs, 2005). This work is part of a larger project on the semantics of connectives which aims at developing formal descriptions of discourse relations, useful for processing real data."
Y07-1003,Deep Lexical Semantics: The Ontological Ascent,2007,11,0,1,1,24687,jerry hobbs,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,"Concepts of greater and greater complexity can be constructed by building systems of entities, by relating other entities to that system with a figure-ground relation, by embedding concepts of figure-ground in the concept of change, by embedding that in causality, and by coarsening the granularity and beginning the process over again. This process can be called the Ontological Ascent. It pervades natural language discourse, and suggests that to do lexical semantics properly, we must carefully axiomatize abstract theories of systems of entities, the figure-ground relation, change, causality, and granularity. In this paper, I outline what these theories should look like."
W07-1409,On the Role of Lexical and World Knowledge in {RTE}3,2007,7,40,5,1,3543,peter clark,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"To score well in RTE3, and even more so to create good justifications for entailments, substantial lexical and world knowledge is needed. With this in mind, we present an analysis of a sample of the RTE3 positive entailment pairs, to identify where and what kinds of world knowledge are needed to fully identify and justify the entailment, and discuss several existing resources and their capacity for supplying that knowledge. We also briefly sketch the path we are following to build an RTE system (Our implementation is very preliminary, scoring 50.9% at the time of RTE). The contribution of this paper is thus a framework for discussing the knowledge requirements posed by RTE and some exploration of how these requirements can be met."
W06-0906,Extending {T}ime{ML} with Typical Durations of Events,2006,21,5,3,1,44738,feng pan,Proceedings of the Workshop on Annotating and Reasoning about Time and Events,0,"In this paper, we demonstrate how to extend TimeML, a rich specification language for event and temporal expressions in text, with the implicit typical durations of events, temporal information in text that has hitherto been largely unexploited. Event duration information can be very important in applications in which the time course of events is to be extracted from text. For example, whether two events overlap or are in sequence often depends very much on their durations."
P06-1050,Learning Event Durations from Event Descriptions,2006,19,26,3,1,44738,feng pan,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration. Here we describe a method for measuring inter-annotator agreement for these event duration distributions. We then show that machine learning techniques applied to this data yield coarse-grained event duration information, considerably outperforming a baseline and approaching human performance."
pan-etal-2006-annotated,An Annotated Corpus of Typical Durations of Events,2006,15,15,3,1,44738,feng pan,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we present our work on generating an annotated corpus for extracting information about the typical durations of events from texts. We include the annotation guidelines, the event classes we categorized, the way we use normal distributions to model vague and implicit temporal information, and how we evaluate inter-annotator agreement. The experimental results show that our guidelines are effective in improving the inter-annotator agreement."
X98-1013,Information Extraction Research and Applications: Current Progress and Future Directions,1998,12,1,2,0,20124,andrew kehler,"TIPSTER TEXT PROGRAM PHASE III: Proceedings of a Workshop held at Baltimore, {M}aryland, October 13-15, 1998",0,"Analysts face a daunting task: they must accurately analyze, categorize, and assimilate a large body of information from a variety of sources and for a variety of domains of interest. The complexity of the task necessitates a variety of information access and extraction tools which technology up to this point has not been able to provide. SRI's TIPSTER Phase III project has focused on two major obstacles to the development of such tools: inadequate degrees of accuracy and portability. We begin by providing an overview of SRI's information extraction (IE) system, FASTUS, and then describe our efforts in these two areas in turn. We then conclude with some thoughts concerning future directions."
P97-1051,A Theory of Parallelism and the Case of {VP} Ellipsis,1997,17,22,1,1,24687,jerry hobbs,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We provide a general account of parallelism in discourse, and apply it to the special case of resolving possible readings for instances of VP ellipsis. We show how several problematic examples are accounted for in a natural and straightforward fashion. The generality of the approach makes it directly applicable to a variety of other types of ellipsis and reference."
X96-1037,{SRI}{'}s Tipster {II} Project,1996,3,3,1,1,24687,jerry hobbs,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,"The principal barrier to the widespread use of information extraction technology is the difficulty in defining the patterns that represent one's information requirements. Much of the work that has been done on SRI's Tipster II project has been directed at overcoming this barrier. In this paper, after some background on the basic structure of the FASTUS system, we present some of these developments. Specifically, we discuss the declarative pattern specification language FastSpec, compile-time transformations, and adapting rules from examples. In addition, we have developed the basic capabilities of FASTUS. We describe our efforts in one are---coreference resolution. We are now experimenting with the use of FASTUS in improving document retrieval and this is also described."
M95-1019,{SRI} {I}nternational {FASTUS} {S}ystem{MUC}-6 Test Results and Analysis,1995,-1,-1,2,0,51819,douglas appelt,"Sixth Message Understanding Conference ({MUC}-6): Proceedings of a Conference Held in {C}olumbia, {M}aryland, November 6-8, 1995",0,None
H94-1032,Principles of Template Design,1994,1,8,1,1,24687,jerry hobbs,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"The functionality of systems that extract information from texts can be specified quite simply: the input is a stream of texts and the output is some representation of the information to be extracted. Hence, the problem of template design is an instance of the problem of knowledge representation. In particular, it is the problem of representing essential facts about situations in a way that can mediate between texts that describe those situations and a variety of applications that involve reasoning about them.The research on which we report here is directed at elucidating principles of template design and at compiling these, with examples, in a manual for template designers."
M93-1009,The Generic Information Extraction System,1993,0,94,1,1,24687,jerry hobbs,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,"An information extraction system is a cascade of transducers or modules that at each step add structure and often lose information, hopefully irrelevant, by applying rules that are acquired manually and/or automatically."
M93-1019,{SRI}: Description of the {JV}-{FASTUS} System Used for {MUC}-5,1993,2,152,2,0.319474,51819,douglas appelt,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,"SRI International developed an information extraction system called FASTUS, a permuted acronym standing for Finite State Automata-based Text Understanding System. The choice of acronym is some-what misleading, however, because FASTUS is a system for information extraction, not text understanding. The former problem is much simpler and more tractable, characterized by a relatively straightforward specification of information to be extracted from the text, only a fraction of which is relevant to the extraction task, and with the author's underlying goals and nuances of meaning of little interest. In contrast, a text understanding task is to recover all of the information in a text, including that which is only implicit in what is actually written. All the richness of natural language becomes fair game, including metaphor, metonymy, discourse structure, and the recognition of the author's underlying intentions, and the full interplay between language and world knowledge becomes central to the task."
H93-1026,{FASTUS}: A System for Extracting Information from Text,1993,3,48,1,1,24687,jerry hobbs,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"FASTUS is a (slightly permuted) acronym for Finite State Automaton Text Understanding System. It is a system for extracting information from free text in English (Japanese is under development), for entry into a database, and potentially for other applications. It works essentially as a set of cascaded, nondeterministic finite state automata."
H93-1030,Session 5: Discourse,1993,-1,-1,1,1,24687,jerry hobbs,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
M92-1019,{SRI} {I}nternational {FASTUS} System {MUC}-4 Test Results and Analysis,1992,1,9,3,0.319474,51819,douglas appelt,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,"The system that SRI used for the MUC-4 evaluation represents a significant departure from system architectures that have been employed in the past. In MUC-2 and MUC-3, SRI used the TACITUS text processing system [1], which was based on the DIALOGIC parser and grammar, and an abudctive reasoner for horn-clause logic. In MUC-4, SRI designed a new system called FASTUS (a permutation of the initial letters in Finite State Automata-based Text Understanding System) which we feel represents a significant advance in the state of the art of text processing. The system shares certain modules with the earlier TACITUS system, namely modules for text preprocessing and standardization, spelling correction, Hispanic name recognition, and the core lexicon. However, the DIALOGIC system and abductive reasoner, which were the heart and soul of the previous system, were replaced by a system whose architecture is based on cascaded finite-state automata. Using this system we were capable of achieving a significant level of performance on the MUC-4 task with less than one month devoted to domain-specific development. In addition, the system is extremely fast, and is capable of processing texts at the rate of approximately 3,200 words per minute, measured in CPU time on a Sun SPARC-2 processor. (Measured according to elapsed real time, the system about 50% slower, but the observed time depends on the particular hardware configuration involved.)"
M92-1036,{SRI} International: Description of the {FASTUS} System Used for {MUC}-4,1992,4,53,1,1,24687,jerry hobbs,"{F}ourth {M}essage {U}understanding {C}onference ({MUC}-4): Proceedings of a Conference Held in {M}c{L}ean, {V}irginia, {J}une 16-18, 1992",0,"FASTUS is a (slightly permuted) acronym for Finite State Automaton Text Understanding System. It is a system for extracting information from free text in English, and potentially other languages as well, for entry into a database, and potentially for other applications. It works essentially as a cascaded, nondeterministic finite state automaton."
H92-1050,Session 8{A}: Machine Translation,1992,-1,-1,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H92-1071,A National Resource Grammar,1992,0,0,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The syntax of English is largely a solved problem. Yet all natural language projects devote a large amount of their effort to developing grammars. The reason for this situation is that there is no very large, generally available grammar of English based on current technology---unification grammar. The solution is to develop a very broad-coverage National Resource Grammar in a unification formalism, perhaps under the auspices of the Linguistic Data Consortium (LDC) and freely available to its members."
H92-1121,{TACITUS}: Research in Text Understanding,1992,0,0,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The aim of the TACITUS project is to develop a general, domain-independent capability for text understanding that allows for variable levels of analysis, depending on the requirements of the task. Four stages of processing are being developed: preprocessing, syntactic analysis, inferential pragmatics processing, and template generation. Template generation is a straightforward programming task, but the other three stages of processing consititute research issues."
A92-1026,Robust Processing of Real-World Natural-Language Texts,1992,18,29,1,1,24687,jerry hobbs,Third Conference on Applied Natural Language Processing,0,"It is often assumed that when natural language processing meets the real world, the ideal of aiming for complete and correct interpretations has to be abandoned. However, our experience with TACITUS; especially in the MUC-3 evaluation, has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with methods for achieving robustness. We describe three techniques for making syntactic analysis more robust-an agenda-based scheduling parser, a recovery technique for failed parses, and a new technique called terminal substring parsing. For pragmatics processing, we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge, performance degrades gracefully. Each of these techniques have been evaluated and the results of the evaluations are presented."
M91-1015,{SRI} {I}nternational{'}s {TACITUS} System: {MUC}-3 Test Results and Analysis,1991,0,5,1,1,24687,jerry hobbs,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"This site report is intended as a companion piece to the System Summary appearing in this volume and is best read in conjunction with it. In particular, it refers to the various modules of the system which are described in that paper."
M91-1030,{SRI} {I}nternational: Description of the {TACITUS} System as Used for {MUC}-3,1991,0,9,1,1,24687,jerry hobbs,"{T}hird {M}essage {U}understanding {C}onference ({MUC}-3): Proceedings of a Conference Held in {S}an {D}iego, {C}alifornia, {M}ay 21-23, 1991",0,"TACITUS is a system for interpreting natural language texts that has been under development since 1985. It has a preprocessor and postprocessor currently tailored to the MUC-3 application. It performs a syntactic analysis of the sentences in the text, using a fairly complete grammar of English, producing a logical form in first-order predicate calculus. Pragmatics problems are solved by abductive inference in a pragmatics, or interpretation, component."
H91-1024,Machine Translation Using Abductive Inference,1991,5,1,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"Machine Translation and World Knowledge. Many existing approaches to machine translation take for granted that the information presented in the output is found somewhere in the input, and, moreover, that such information should be expressed at a single representational level, say, in terms of the parse trees or of semantic assertions. Languages, however, not only express the equivalent information by drastically different linguistic means, but also often disagree in what distinctions should be expressed linguistically at all. For example, in translating from Japanese to English, it is often necessary to supply determiners for noun phrases, and this in general cannot be done without deep understanding of the source text. Similarly, in translating from English to Japanese, politeness considerations, which in English are implicit in the social situation and explicit in very diffuse ways in, for example, the heavy use of hypotheticals, must be realized grammatically in Japanese. Machine translation therefore requires that the appropriate inferences be drawn and that the text be interpreted to some depth. Recently, an elegant approach to inference in discourse interpretation has been developed at a number of sites (e. g., Charniak and Goldman, 1988; Hobbs et al., 1990; Norvig, 1987), all based on the notion of abduction, and we have begun to explore its potential application to machine translation. We argue that this approach provides the possibility of deep reasoning and of mapping between the languages at a variety of levels."
H91-1100,{TACITUS}: The Abductive Commonsense Inference-based Text Understanding System,1991,1,0,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"TACITUS is a natural language processing system that is intended to be general and domain-independent. It performs a syntactic analysis of the sentences in the text, producing a logical form. Next, inferential pragmatics processing is applied to the logical form to solve problems of schema recognition, reference resolution, metonymy resolution, and the interpretation of vague predicates. An analysis component then produces the desired output for the application. TACITUS has been applied to several quite different domains, including naval equipment failure reports, naval operations reports, and terrorist reports."
H90-1012,Making Abduction More Efficient,1990,1,1,2,0.319474,51819,douglas appelt,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"The TACITUS system uses a cost-based abduction scheme for finding and choosing among possible interpretations for natural language texts. Ordinary Prolog-style, backchaining deduction is augmented with the capability of making assumptions and of factoring two goal literals that are unifiable (see Hobbs et al., 1988)."
C90-3028,Translation by Abduction,1990,6,9,1,1,24687,jerry hobbs,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Abstract : Many existing approaches to machine translation take for granted that the information presented in the output is found somewhere in the input, and, moreover, that such information should be expressed at a single representational level, for example, in terms of the parse trees or of semantic assertions. Languages, however, not only express the equivalent information by drastically different linguistic means, but also often disagree in what distinctions should be expressed linguistically at all. For example, in translating from Japanese to English, it is often necessary to supply determiners for noun phrases, and this in general cannot be done without deep understanding of the source text. Similarly, in translating from English to Japanese, politeness considerations, which in English are implicit in the social situation and explicit in very diffuse ways in, for example, the heavy use of hypotheticals, must be realized grammatically in Japanese. Machine translation therefore requires that the appropriate inferences be drawn and that the text be interpreted to some depth (see Oviatt, 1988). Recently, an elegant approach to inference in discourse interpretation has been developed at a number of sites (e.g., Hobbs et al., 1988; Charniak and Goldman, 1988; Norvig, 1987), all based on the notion of abduction, and we have begun to explore its potential application to machine translation. We argue that this approach provides the possibility of deep reasoning and of mapping between the languages at a variety of levels (See also Kaplan et al., 1988, on the latter point.) Abductive inference is inference to the best explanation. The easiest way to understand it is to compare it with two words it rhymes with-deduction and induction. Much of the way we interpret the world in general can be understood as a process of abduction."
C90-3029,Two Principles of Parse Preference,1990,20,52,1,1,24687,jerry hobbs,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"The DIALOGIC system for syntactic analysis and semantic translation has been under development for over ten years, and during that time it has been used in a number of domains in both database interface and message-processing applications. In addition, it has been tested on a number of sentences of linguistic interest. Built into the system are facilities for ranking parses according to syntactic and selectional considerations, and over the years, as various kinds of ambiguity have become apparent, heuristics have been devised for choosing the preferred parses. Our aim in this paper is first to present a compendium of many of these heuristics and second to propose two principles that seem to underlie the heuristics. The first will be useful to researchers engaged in building grammars of similarly broad coverage. The second is of psychological interest and may be a guide for estimating parse preferences for newly discovered ambiguities for which we lack the experience to decide among on a more empirical basis."
H89-2075,{TACITUS}: A Message Understanding System,1989,0,0,1,1,24687,jerry hobbs,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"TACITUS is a general and domain-independent natural language processing system, used so far primarily for message processing. It performs a syntactic analysis of the sentences in the text, producing a logical form. Next, inferential pragmatics processing is applied to the logical form to solve problems of schema recognition, reference resolution, metonymy resolution, and the interpretation of vague predicates. An analysis component then produces the desired output for the application. TACITUS has been applied to several quite different domains, including naval equipment failure reports, naval operations reports, and terrorist reports."
P88-1012,Interpretation as Abduction,1988,71,262,1,1,24687,jerry hobbs,26th Annual Meeting of the Association for Computational Linguistics,1,"An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics."
A88-1032,Localizing Expression of Ambiguity,1988,10,12,2,0.319149,53912,john bear,Second Conference on Applied Natural Language Processing,0,"In this paper we describe an implemented program for localizing the expression of many types of syntactic ambiguity, in the logical forms of sentences, in a manner convenient for subsequent inferential processing. Among the types of ambiguities handled are prepositional phrases, very compound nominals, adverbials, relative clauses, and preposed prepositional phrases. The algorithm we use is presented, and several possible shortcomings and extensions of our method are discussed."
T87-1006,World Knowledge and Word Meaning,1987,5,11,1,1,24687,jerry hobbs,Theoretical Issues in Natural Language Processing 3,0,None
J87-3004,Commonsense Metaphysics and Lexical Semantics,1987,15,55,1,1,24687,jerry hobbs,Computational Linguistics,0,"In the TACITUS project for using commonsense knowledge in the understanding of texts about mechanical devices and their failures, we have been developing various commonsense theories that are needed to mediate between the way we talk about the behavior of such devices and causal models of their operation. Of central importance in this effort is the axiomatization of what might be called commonsense metaphysics. This includes a number of areas that figure in virtually every domain of discourse, such as granularity, scales, time, space, material, physical objects, shape, causality, functionality, and force. Our effort has been to construct core theories of each of these areas, and then to define, or at least characterize, a large number of lexical items in terms provided by the core theories. In this paper we discuss our methodological principles and describe the key ideas in the various domains we are investigating."
J87-1005,An Algorithm for Generating Quantifier Scopings,1987,7,122,1,1,24687,jerry hobbs,Computational Linguistics,0,"The syntactic structure of a sentence often manifests quite clearly the predicate-argument structure and relations of grammatical subordination. But scope dependencies are not so transparent. As a result, many systems for representing the semantics of sentences have ignored scoping or generated scopings with mechanisms that have often been inexplicit as to the range of scopings they choose among or profligate in the scopings they allow.This paper presents, along with proofs of some of its important properties, an algorithm that generates scoped semantic forms from unscoped expressions encoding predicate-argument structure. The algorithm is not profligate as are those based on permutation of quantifiers, and it can provide a solid foundation for computational solutions where completeness is sacrificed for efficiency and heuristic efficacy."
P86-1035,Commonsense Metaphysics and Lexical Semantics,1986,16,17,1,1,24687,jerry hobbs,24th Annual Meeting of the Association for Computational Linguistics,1,"In the TACITUS project for using commonsense knowledge in the understanding of texts about mechanical devices and their failures, we have been developing various commonsense theories that are needed to mediate between the way we talk about the behavior of such devices and causal models of their operation. Of central importance in this effort is the axiomatization of what might be called commonsense metaphysics. This includes a number of areas that figure in virtually every domain of discourse, such as scalar notions, granularity, time, space, material, physical objects, causality, functionality, force, and shape. Our approach to lexical semantics is then to construct core theories of each of these areas, and then to define, or at least characterize, a large number of lexical items in terms provided by the core theories. In the TACITUS system, processes for solving pragmatics problems posed by a text will use the knowledge base consisting of these theories in conjunction with the logical forms of the sentences in the text to produce an interpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers."
J86-3006,"The {F}inite {S}tring Newsletter: Site Report: Another From the {DARPA} Series, Overview of the {TACITUS} Project",1986,-1,-1,1,1,24687,jerry hobbs,Computational Linguistics,0,None
H86-1003,Overview of the {TACITUS} Project,1986,1,30,1,1,24687,jerry hobbs,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"The specific aim of the TACITUS project is to develop interpretation processes for handling casualty reports (casreps), which are messages in free-flowing text about breakdowns of machinery. These interpretation processes will be an essential component, and indeed the principal component, of systems for automatic message routing and systems for the automatic extraction of information from messages for entry into a data base or an expert system. In the latter application, for example, it is desirable to be able to recognize conditions in the message that instantiate conditions in the antecedents of the expert system's rules, so that the expert system can reason on the basis of more up-to-date and more specific information."
H86-1013,Commonsense Metaphysics and Lexical Semantics,1986,16,17,1,1,24687,jerry hobbs,"Strategic Computing - Natural Language Workshop: Proceedings of a Workshop Held at Marina del Rey, California, May 1-2, 1986",0,"In the TACITUS project for using commonsense knowledge in the understanding of texts about mechanical devices and their failures, we have been developing various commonsense theories that are needed to mediate between the way we talk about the behavior of such devices and causal models of their operation. Of central importance in this effort is the axiomatization of what might be called commonsense metaphysics. This includes a number of areas that figure in virtually every domain of discourse, such as scalar notions, granularity, time, space, material, physical objects, causality, functionality, force, and shape. Our approach to lexical semantics is then to construct core theories of each of these areas, and then to define, or at least characterize, a large number of lexical items in terms provided by the core theories. In the TACITUS system, processes for solving pragmatics problems posed by a text will use the knowledge base consisting of these theories in conjunction with the logical forms of the sentences in the text to produce an interpretation. In this paper we do not stress these interpretation processes; this is another, important aspect of the TACITUS project, and it will be described in subsequent papers."
P85-1008,Ontological Promiscuity,1985,10,246,1,1,24687,jerry hobbs,23rd Annual Meeting of the Association for Computational Linguistics,1,"To facilitate work in discourse interpretation, the logical form of English sentences should be both close to English and syntactically simple. In this paper I propose a logical notation which is first-order and nonintensional, and for which semantic translation can be naively compositional. The key move is to expand what kinds of entities one allows in one's ontology, rather than complicating the logical notation, the logical form of sentences, or the semantic translation process. Three classical problems - opaque adverbials, the distinction between de re and de dicto belief reports, and the problem of identity in intensional contexts - are examined for the difficulties they pose for this logical notation, and it is shown that the difficulties can be overcome. The paper closes with a statement about the view of semantics that is presupposed by this approach."
P84-1059,Building a Large Knowledge Base for a Natural Language System,1984,3,2,1,1,24687,jerry hobbs,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"A sophisticated natural language system requires a large knowledge base. A methodology is described for constructing one in a principled way. Facts are selected for the knowledge base by determining what facts are linguistically presupposed by a text in the domain of interest. The facts are sorted into clusters, and within each cluster they are organized according to their logical dependencies. Finally, the facts are encoded as predicate calculus axioms."
P83-1009,An Improper Treatment of Quantification in Ordinary {E}nglish,1983,6,46,1,1,24687,jerry hobbs,21st Annual Meeting of the Association for Computational Linguistics,1,"In the currently standard ways of representing quantification in logical form, this sentence has 120 different readings, or quantifier scopings. Moreover, they are truly distinct, in the sense that for any two readings, there is a model that satisfies one and not the other. With the standard logical forms produced by the syntactic and semantic translation components of current theoretical frameworks and implemented systems, it would seem that an inferencing component must process each of these 120 readings in turn in order to produce a best reading. Yet it is obvious that people do not entertain all 120 possibilities, and people really do understand the sentence. The problem is not Just that inferencing is required for disamblguation. It is that people never do dlsambiguate completely. A single quantifier scoping is never chosen. (Van Lehn [1978] and Bobrow and Webber [1980] have also made this point.) In the currently standard logical notations, it is not clear how this vagueness can be represented. 1"
C82-1015,{DIALOGIC}: A Core Natural-Language Processing System,1982,6,149,4,0,30392,barbara grosz,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The DIALOGIC system translates English sentences into representations of their literal meaning in the context of an utterance. These representations, or logical forms, are intended to be a purely formal language that is as close as possible to the structure of natural language, while providing the semantic compositionality necessary for meaning-dependent computational processing. The design of DIALOGIC (and of its constituent modules) was influenced by the goal of using it as the core language-processing component in a variety of systems, some of which are transportable to new domains of application."
C82-1020,Natural Language Access to Structured Text,1982,10,19,1,1,24687,jerry hobbs,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper discusses the problem of providing natural language access to textual material. We are developing a system that relates a request in English to specific passages in a document on the basis of correspondences between the logical representations of the information in the request and in the passages. In addition, we are developing procedures for automatically generating logical representations of text passages, directly from the text, by means of an analysis of the coherence structure of the passages."
P80-1017,Interactive Discourse: Influence of the Social Context: Panel Chair{'}s Introduction,1980,6,1,1,1,24687,jerry hobbs,18th Annual Meeting of the Association for Computational Linguistics,1,Progress on natural language interfaces can perhaps be stimulated or directed by imagining the ideal natural language system of the future. What features (or even design philosophies) should such a system have in order to become an integral part of our work environments? What scaled-down versions of these features might be possible in the near future in simple service systems [2]? These issues can be broken down into the following four questions:
J75-4010,A General System for Semantic Analysis of {E}nglish and its Use in Drawing Maps from Directions,1975,-1,-1,1,1,24687,jerry hobbs,American Journal of Computational Linguistics,0,None
