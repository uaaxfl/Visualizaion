2020.emnlp-main.408,H90-1021,0,0.347201,"d navigation to events domains. The first internal dataset we use contains over 170k training utterances annotated with flat representations, covering over 140 distinct intents from a variety of domains including weather, communication, music, weather, and device control. The second internal dataset contains over 67k training utterances with fully hierarchical representations, and covers over 60 intents all in the communication domain. The second and third public datasets are SNIPS Natural Language Understanding benchmark1 (SNIPS-NLU) and the Airline Travel Information Systems (ATIS) dataset (Hemphill et al., 1990). We follow the same procedure that was mentioned above for preparing the decoupled data for both of these datasets. As can be seen from Table 1b, our proposed approach outperforms the previous state-ofthe-art results on the ATIS, comparable to state-of-the-art on SNIPS, and TOP semantic parsing task, which had been obtained with the Seq2SeqPtr model by Rongali et al. (2020). Comparing the decoupled model to RNNGs, we note that a single decoupled model, using either biLSTMs or transformers (with RoBERTa or BART pretraining) is able to outperform the RNNG. In fact, the decoupled model even outp"
2020.emnlp-main.408,W19-4111,0,0.0409419,"Missing"
2020.emnlp-main.408,W14-4337,0,0.0952558,"del does indeed benefit from the extra training data, being able to outperform the biLSTMbased model on the two datasets. In both cases, the decoupled model with BART pretraining achieves the top performance. The same procedure was used over our SBTOP dataset, with the only variant being we concatenated SB-TOP and TOP and jointly trained over both datasets. Table 2 shows the test results over 4.3 Slot carryover To evaluate the ability of the decoupled models to work on session-based data, we evaluate them on a task which requires drawing information for multiple utterances. The DSTC2 dataset (Henderson et al., 2014) contains a number of dialogues annotated with dialogue state – slightly over 2k sessions in 5031 Table 1: Frame accuracy of the decoupled models on semantic parsing tasks. † indicates results from Hakkani-T¨ur et al. (2016); ‡, from Goo et al. (2018); ∗, from Zhang et al. (2018); ×, from Chen et al. (2019a). (b) Accuracy on ATIS and SNIPS. (a) Accuracy on TOP. Model Acc. RNNG RNNG + Ensembling RNNG + ELMo 80.86 83.84 83.93 Decoupled biLSTM Decoupled transformer Decoupled ELMo Decoupled RoBERTa Decoupled BART 79.51 64.50 84.85 84.52 87.10 Best Seq2SeqPtr 86.67 Model ATIS SNIPS Joint biRNN Slot"
2020.emnlp-main.408,N16-1024,0,0.191847,"able performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover. 1 (Bapna et al., 2017; Gupta et al., 2018). Although flat representations are trivial to model with standard intent/slot tagging models, the semantic representation is fundamentally limiting. Gupta et al. (2018) explored the limitations of flat representations and proposed a compositional generalization which allowed slots to contain nested intents while allowing easy modeling through neural shift-reduce parsers such as RNNG (Dyer et al., 2016). Our contributions are the following: • We explore the limitations of this compositional form and propose an extension which overcomes these limitations that we call decoupled representation. • To parse this more complicated representation, we propose a family of Seq2Seq models based off the Pointer-Generator architecture that set state of the art in multiple semantic parsing and dialog tasks (See et al., 2017). • To further advance session based task oriented semantic parsing, we release a publicly available set with 60k utterances constituting roughly 20k sessions. Introduction At the core"
2020.emnlp-main.408,N18-2118,0,0.0348741,"Missing"
2020.emnlp-main.408,D18-1300,1,0.864012,"d context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional taskoriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover. 1 (Bapna et al., 2017; Gupta et al., 2018). Although flat representations are trivial to model with standard intent/slot tagging models, the semantic representation is fundamentally limiting. Gupta et al. (2018) explored the limitations of flat representations and proposed a compositional generalization which allowed slots to contain nested intents while allowing easy modeling through neural shift-reduce parsers such as RNNG (Dyer et al., 2016). Our contributions are the following: • We explore the limitations of this compositional form and propose an extension which overcomes these limitations that we call decoupled representation. •"
2020.emnlp-main.408,P16-1002,0,0.111148,"Missing"
2020.emnlp-main.408,D10-1119,0,0.0496252,"ystem actions – in the case of Zhong et al. (2018) – or a fixed length dialogue representation. It is interesting to note that the decoupled models perform better on distant slots: this suggests that the models may be paying more attention to the beginning of the sentences, which may be an artifact of their pretraining. 5 Slot distance 1 2 Related Work Traditional work on semantic parsing, either for the purposes of question answering or taskoriented request understanding, has focused on mapping utterances to logical form representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang, 2016; van Noord et al., 2018). Logical forms, while very expressive, are also complex. Highly trained annotators are required for the creation of training data, and as a result there is a lack of large scale datasets that make use of these formalisms. Intent-slot representations such as those used for the ATIS dataset (Price, 1990) or the datasets released as part of the DSTC challenges (Henderson et al., 2014; Rastogi et al., 2019) have less expressive power, but have the major advantage of being simple enough to enable the creation of large-scale datasets. Gupta et al. (2018) introd"
2020.emnlp-main.408,Q18-1043,0,0.0300283,"Missing"
2020.emnlp-main.408,N18-1202,0,0.0174087,"the decoder. Both consist of two layers of size 512, with randomly initialized embeddings of size 300. The base model is optimized with LAMB while others are optimized with Adam, using parameters β1 = 0.9, β2 = 0.999,  = 10−8 , and L2 penalty 10−5 (Kingma and Ba, 2014). The learning rate is found separately for each experiment via hyperparameter search. We also use stochastic weight averaging (Izmailov et al., 2018), and exponential learning rate decay. For an extended version of this model, we also try incorporating contextualized word vectors, by augmenting the input with ELMo embeddings (Peters et al., 2018). Transformer We also experiment with two further variants of the model, that replace encoder and decoder with transformers. In the first variant, the encoder is initialized with RoBERTa (Liu et al., 2019), a pretrained language model. The decoder is a randomly initialized 3-layer transformer, with hidden size 512 and 4 attention heads. In the second variant, we initialise both encoder and decoder with BART (Lewis et al., 2019), a sequence-tosequence pretained model. Both encoder and decoder consist of 12 layers with hidden size 1024. We train these with stochastic weight averaging (Izmailov e"
2020.emnlp-main.408,H90-1020,0,0.0982957,"Traditional work on semantic parsing, either for the purposes of question answering or taskoriented request understanding, has focused on mapping utterances to logical form representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Liang, 2016; van Noord et al., 2018). Logical forms, while very expressive, are also complex. Highly trained annotators are required for the creation of training data, and as a result there is a lack of large scale datasets that make use of these formalisms. Intent-slot representations such as those used for the ATIS dataset (Price, 1990) or the datasets released as part of the DSTC challenges (Henderson et al., 2014; Rastogi et al., 2019) have less expressive power, but have the major advantage of being simple enough to enable the creation of large-scale datasets. Gupta et al. (2018) introduce a hierarchical intent-slot representation, and show that it is expressive enough to capture the majority of user-generated queries in two domains. Recent approaches to semantic parsing have focused on using techniques such as RNNGs (Gupta et al., 2018), RNNGs augmented with ensembling and re-ranking techniques or contextual embeddings ("
2020.emnlp-main.408,P17-1099,0,0.0899839,"epresentations and proposed a compositional generalization which allowed slots to contain nested intents while allowing easy modeling through neural shift-reduce parsers such as RNNG (Dyer et al., 2016). Our contributions are the following: • We explore the limitations of this compositional form and propose an extension which overcomes these limitations that we call decoupled representation. • To parse this more complicated representation, we propose a family of Seq2Seq models based off the Pointer-Generator architecture that set state of the art in multiple semantic parsing and dialog tasks (See et al., 2017). • To further advance session based task oriented semantic parsing, we release a publicly available set with 60k utterances constituting roughly 20k sessions. Introduction At the core of conversational assistants lies the semantic representation, which provides a structured description of tasks supported by the assistant. Traditional dialog systems operate through a flat representation, usually composed of a single intent and a list of slots with non-overlapping content from the utterance 2 Semantic Representation The compositional extension proposed by Gupta et al. (2018) overcame the limita"
2020.emnlp-main.408,P19-1519,0,0.0131512,"advantage of being simple enough to enable the creation of large-scale datasets. Gupta et al. (2018) introduce a hierarchical intent-slot representation, and show that it is expressive enough to capture the majority of user-generated queries in two domains. Recent approaches to semantic parsing have focused on using techniques such as RNNGs (Gupta et al., 2018), RNNGs augmented with ensembling and re-ranking techniques or contextual embeddings (Einolghozati et al., 2018), sequence-to-sequence recurrent neural networks augmented with pointer mechanisms (Jia and Liang, 2016), capsule networks (Zhang et al., 2019), and Transformer-based architectures (Rongali et al., 2020). 6 Conclusions We started this paper by exploring the limitations of compositional intent-slot representations for semantic parsing. Due to the constraints it imposes, it cannot represent certain utterances with long-term dependencies, and it is unsuitable for semantic parsing at the session (multi-utterance) level. To overcome these limitations we propose an extension of this representation, the decoupled representation. We propose a family of sequenceto-sequence models based on the pointergenerator architecture – using both recurre"
2020.emnlp-main.408,P18-1135,0,0.0342088,"Missing"
2020.emnlp-main.413,2020.emnlp-main.408,1,0.858005,"Missing"
2020.emnlp-main.413,N19-1423,0,0.178039,"ic parsers for low-resource domains (e.g. 25 training samples per intent or slot label), and propose a solution that is competitive against supervised models trained with 10x more data. We identify two key factors for successfully adapting task-oriented semantic parsers to new domains: better representation learning and better training techniques. We first show that pre-trained language representations are critical in the low-resource setting for the model to quickly generalize to new intents and slots. Furthermore, most pre-trained language representations used in previous work such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) are encoder-only models, and are hence not ideal for a compositional parser with an encoder-decoder (seq2seq) architecture. We therefore propose to use BART (Lewis et al., 2020), a pre-trained seq2seq model that can be used to initialize both the encoder and decoder of our semantic parser, which significantly outperforms other pre-trained representations such as RoBERTa. More importantly, these large pre-trained models are sometimes known to pose challenges to fine-tuning with very few training samples. In order to better adapt the semantic parser to lowresource"
2020.emnlp-main.413,W17-2607,0,0.0215538,"tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approaches for handling the more complex compositional queries using neural shift-reduce parsers (Gupta et al., 2018; Einolghozati et al., 2018) or seq2seq models (Jia and Liang, 2016; Rongali et al., 2020). On the other hand, there have been research efforts on scaling task-oriented parsers to new domains with less training data (Jaech et al., 2016; Bapna et al., 2017; Fan et al., 2017; Goyal et al., 2018; Lee and Jha, 2019). These methods, however, only focus on the simpler flat queries. Our proposed method, in contrast, can effectively parse both flat and compositional queries for low5097 resource target domains. Meta-Learning (Lake et al., 2015), or learning to learn, aims to learn a model that can quickly adapt to new tasks with a small amount of training data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute se"
2020.emnlp-main.413,N18-3018,0,0.0590947,"Missing"
2020.emnlp-main.413,D18-1398,0,0.0405991,"quickly adapt to new tasks with a small amount of training data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute second derivatives, which are computation and memory intensive, there have been studies to use either first-order approximation such as firstorder MAML and Reptile (Nichol et al., 2018), or implicit differentiation (Rajeswaran et al., 2019). Furthermore, meta-learning has also been applied to a number of NLP tasks lately (Gu et al., 2018; Dou et al., 2019; Mi et al., 2019; Qian and Yu, 2019; Sun et al., 2019). 7 Conclusion In this work, we study the low-resource domain scaling problem for task-oriented semantic parsing. In particular, we focus on the 25 SPIS setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data. Our approach distinguishes itself from previous methods on two fronts. First of all, we argue the encoder-only pre-trained representations used in existing work are not ideal for the seq2seq model employed in task-oriented semantic parsing, and instead"
2020.emnlp-main.413,D18-1300,1,0.834194,"hen Yashar Mehdad Asish Ghoshal Luke Zettlemoyer Sonal Gupta Facebook Inc. {xilun,aghoshal,mehdad,lsz,sonalgupta}@fb.com Utterance: Driving directions to the Eagles game Abstract Semantic Parse: [IN:GET_DIRECTIONS Driving directions to [SL:DESTINATION [IN:GET_EVENT the [SL:NAME_EVENT Eagles ] [SL:CAT_EVENT game ] ] ] ] Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al., 2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). Tree Representation: Driving directions to the In this paper, we focus on adapting taskoriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2020) to ini"
2020.emnlp-main.413,P16-1002,0,0.0202161,"since 1990s with the advent of the ATIS dataset (Price, 1990). Traditionally, the task is formulated as a joint text classification (intent prediction) and sequence tagging (slot filling) problem, and can be tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approaches for handling the more complex compositional queries using neural shift-reduce parsers (Gupta et al., 2018; Einolghozati et al., 2018) or seq2seq models (Jia and Liang, 2016; Rongali et al., 2020). On the other hand, there have been research efforts on scaling task-oriented parsers to new domains with less training data (Jaech et al., 2016; Bapna et al., 2017; Fan et al., 2017; Goyal et al., 2018; Lee and Jha, 2019). These methods, however, only focus on the simpler flat queries. Our proposed method, in contrast, can effectively parse both flat and compositional queries for low5097 resource target domains. Meta-Learning (Lake et al., 2015), or learning to learn, aims to learn a model that can quickly adapt to new tasks with a small amount of training data. In par"
2020.emnlp-main.413,2020.acl-main.703,1,0.908657,"ueries (Gupta et al., 2018; Rongali et al., 2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). Tree Representation: Driving directions to the In this paper, we focus on adapting taskoriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2020) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to lowresource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain taskoriented semantic parsing dataset (TOPv21 ). 1 Eagles game Figure 1: An compositional query from TOP dataset. emerged to tackle such task-oriented semantic parsing task, for both simple and more complex queries. Introduction Virtual Assistants n"
2020.emnlp-main.413,2021.ccl-1.108,0,0.130635,"Missing"
2020.emnlp-main.413,N19-4009,0,0.0238804,"batch size of 32 for Reptile, with both inner (η) and outer (α) learning rates being 5 × 10−5 . All models are trained for 100 epochs on the source domains with a batch size of 128 (except Reptile), using early stopping if the validation accuracy does not improve in the last 10 epochs. Finetuning is done for 2000 epochs, with a batch size of either 64 (LSTM and RoBERTa) or 32 (BART and meta-learning). Model validation is performed once every 10 epochs during fine-tuning, and stops early after 20 consecutive validations with no improvements. Our model is implemented with the fairseq framework (Ott et al., 2019) and trained on a Nvidia Telsa P100 GPU with 16GB memory. 6 Related Work Task-Oriented Semantic Parsing has attracted attention from the research community since 1990s with the advent of the ATIS dataset (Price, 1990). Traditionally, the task is formulated as a joint text classification (intent prediction) and sequence tagging (slot filling) problem, and can be tackled with sequence labeling models such as RNNs (Mesnil et al., 2013; Liu and Lane, 2016). These models can only parse flat queries with one intent and nonnested slots. More recently, a number of studies propose alternative approache"
2020.emnlp-main.413,H90-1020,0,0.321196,"r, Reptile behaves differently and performs similar updates compared to MAML as shown by Nichol et al. (2018) using Taylor Series analysis. 5 Experiments In this section, we first introduce TOPv2, a multidomain task-oriented semantic parsing dataset we are releasing to the community. It is an extension to the TOP dataset with 6 additional domains and 137k new samples. We then outline the setup of our low-resource domain scaling experiments in §5.2, and present the experimental results in §5.3. 5.1 The TOPv2 Dataset While multiple datasets exist for task-oriented semantic parsing such as ATIS (Price, 1990) or SNIPS (Coucke et al., 2018), the TOP dataset (Gupta et al., 2018) is unique in that it contains compositional queries with complex and hierarchical structures (Figure 1). On the other hand, the queries from the TOP dataset are limited to only two domains, namely navigation and event, making it unsuited for domain scaling experiments. To this end, we extend the TOP dataset with 6 additional domains: alarm, messaging, music, reminder, timer, and weather, with a good mixture of simple (flat) and complex (compositional) domains. Table 1 shows some basic statistics of the TOPv2 dataset. We foll"
2020.emnlp-main.413,P19-1253,0,0.0224571,"raining data. In particular, Finn et al. (2017) propose MAML, an optimization-based meta-learning method, which learns a good parameter initialization suitable for faster adaptation to new tasks. As MAML requires to compute second derivatives, which are computation and memory intensive, there have been studies to use either first-order approximation such as firstorder MAML and Reptile (Nichol et al., 2018), or implicit differentiation (Rajeswaran et al., 2019). Furthermore, meta-learning has also been applied to a number of NLP tasks lately (Gu et al., 2018; Dou et al., 2019; Mi et al., 2019; Qian and Yu, 2019; Sun et al., 2019). 7 Conclusion In this work, we study the low-resource domain scaling problem for task-oriented semantic parsing. In particular, we focus on the 25 SPIS setting to investigate whether a model can effectively adapt to new domains with a very limited amount of training data. Our approach distinguishes itself from previous methods on two fronts. First of all, we argue the encoder-only pre-trained representations used in existing work are not ideal for the seq2seq model employed in task-oriented semantic parsing, and instead propose to use BART, a pre-trained model with an encod"
2020.emnlp-main.413,P17-1099,0,0.0195561,"copy probability and the generation probability. The generation probability gt is produced by the decoder by mapping the decoder state onto the output vocabulary, which only includes ontology tokens but not utterance tokens. gt = sof tmax(OutputEmbed(dt )) Base Model In this section, we present our core model architecture. Our meta-learning technique will be introduced in Section 4. We follow recent state-of-theart approaches (Rongali et al., 2020; Aghajanyan et al., 2020) and adopt a seq2seq model as our base architecture (S EQ 2S EQ -C OPY P TR), derived from the Pointer Generator Network (See et al., 2017). The base architecture is shown in Figure 2. For an input sequence S = [w1 , w2 , . . . , wn ], the encoder first encodes it into a series of hidden vectors (encoder states) [e1 , e2 , . . . , en ]. The encoder states are then passed to an decoder that autoregressively produces target tokens ot for each timestamp t. Specifically, the decoder first outputs a hidden decoder state dt based on the decoder states from previous timestamps as well as all enThe copy probability ct , on the other hand, indicates whether to copy one of the source tokens as the decoder output for timestamp t, and is pre"
2020.emnlp-main.413,P16-1162,0,0.026352,"On the other hand, supervised models are trained with 500 or 1000 SPIS to assess the performance of our low-resource domain scaling model. For the source domains, all available training data is utilized. Validation Set To perform model selection and early stopping, a validation set is adopted, which is also set to 25 SPIS for simplicity. In contrast, the supervised models utilize the entire validation set as shown in Table 2. Data Preprocessing We first perform standard preprocessing such as lower-casing and tokenization. For models initialized with pre-trained language representations, BPE (Sennrich et al., 2016) tokenization is done to match that used by the pretrained model. We do not tokenize ontology tokens (intents and slot labels) into BPE, but instead treat them as atomic tokens which are appended to the BPE vocabulary. We then perform two additional preprocessing (canonicalization) steps, consistent across all models. First of all, note that certain utterance tokens do not contribute to the semantics of the query. For instance, in Figure 1, the phrase Driving directions to under IN:GET_DIRECTIONS and the under IN:GET_EVENT can be omitted as their semantics are already captured by the intent la"
2020.emnlp-main.522,D13-1160,0,0.315709,"Missing"
2020.emnlp-main.522,P17-1171,0,0.0611096,"Missing"
2020.emnlp-main.522,N19-1423,0,0.252704,"acebookresearch/BLINK/tree/master/elq 1 have only been evaluated on long, well-formed documents like news articles (Ji et al., 2010), but not on short, noisy text. Also, most prior works have focused mainly on improving model prediction accuracy, largely overlooking efficiency. In this work, we propose ELQ, a fast and accurate entity linking system that specifically targets questions. Following the Wikification setup (Ratinov et al., 2011), ELQ aims to identify the mention boundaries of entities in a given question and their corresponding Wikipedia entity. We employ a biencoder based on BERT (Devlin et al., 2019) as shown in Figure 1. The entity encoder computes entity embeddings for all entities in Wikipedia, using their short descriptions. Then, the question encoder derives token-level embeddings for the input question. We detect mention boundaries using these embeddings, and disambiguate each entity mention based on an inner product between the mention embeddings (averaged embedding over mention tokens) and the entity embeddings. Our model ex6433 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6433–6441, c November 16–20, 2020. 2020 Association for Comp"
2020.emnlp-main.522,P19-1335,0,0.0527789,"d for building robust question answering (QA) systems. For instance, the question “when did shaq come to the nba?” can be answered by examining Shaquille O’Neal’s Wikipedia article (Min et al., 2019), or its properties in a knowledge graph (Yih et al., 2015; Yu et al., 2017). However, real-world user questions are invariably noisy and ill-formed, lacking cues provided by casing and punctuation, which prove challenging to current end-to-end entity linking systems (Yang and Chang, 2015; Sorokin and Gurevych, 2018). While recent pre-trained models have proven highly effective for entity linking (Logeswaran et al., 2019; Wu et al., 2020), they are only designed for entity disambiguation and require mention boundaries to be given in the input. Additionally, such systems ∗ Work done while at Facebook AI. Code and data available at https://github.com/ facebookresearch/BLINK/tree/master/elq 1 have only been evaluated on long, well-formed documents like news articles (Ji et al., 2010), but not on short, noisy text. Also, most prior works have focused mainly on improving model prediction accuracy, largely overlooking efficiency. In this work, we propose ELQ, a fast and accurate entity linking system that specifica"
2021.acl-long.350,N19-1253,1,0.801193,"th the same color have the same meaning. Although the sentences have a different word order, their syntactic dependency structure is similar. Introduction Cross-lingual transfer reduces the requirement of labeled data to perform natural language processing (NLP) in a target language, and thus has the ability to avail NLP applications in low-resource languages. However, transferring across languages is challenging because of linguistic differences at levels of morphology, syntax, and semantics. For example, word order difference is one of the crucial factors that impact cross-lingual transfer (Ahmad et al., 2019). The two sentences in English and Hindi, as shown in Figure 1 have the same ∗ Work done during internship at Facebook AI. meaning but a different word order (while English has an SVO (Subject-Verb-Object) order, Hindi follows SOV). However, the sentences have a similar dependency structure, and the constituent words have similar part-of-speech tags. Presumably, language syntax can help to bridge the typological differences across languages. In recent years, we have seen a colossal effort to pre-train Transformer encoder (Vaswani et al., 2017) on large-scale unlabeled text data in one or many"
2021.acl-long.350,2020.acl-main.421,0,0.139429,"trees. Named Entity Recognition is a structure prediction task that requires to identify the named entities mentioned in the input sentence. We use the Wikiann dataset (Pan et al., 2017) and a subset of two tasks from CoNLL-2002 (Tjong Kim Sang, 2002) and CoNLL-2003 NER (Tjong Kim Sang and De Meulder, 2003). We collect the CoNLL datasets from XGLUE (Liang et al., 2020). In both datasets, there are 4 types of named entities: Person, Location, Organization, and Miscellaneous.4 Question Answering We evaluate on two crosslingual question answering benchmarks, MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020). We use the SQuAD dataset (Rajpurkar et al., 2016) for training and validation. In the QA task, the inputs are a question and a context passage that consists of many sentences. We formulate QA as a multi-sentence reading comprehension task; jointly train the models to predict the answer sentence and extract the answer span from it. We concatenate the question and each sentence from the context passage and use the [CLS] token representation to score the candidate sentences. We adopt the confidence method from Clark and Gardner (2018) and pick the highest-scored sentence to extract the answer s"
2021.acl-long.350,D15-1075,0,0.0384827,"s. Presumably, language syntax can help to bridge the typological differences across languages. In recent years, we have seen a colossal effort to pre-train Transformer encoder (Vaswani et al., 2017) on large-scale unlabeled text data in one or many languages. Multilingual encoders, such as mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020) map text sequences into a shared multilingual space by jointly pre-training in many languages. This allows us to transfer the multilingual encoders across languages and have found effective for many NLP applications, including text classification (Bowman et al., 2015; Conneau 4538 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4538–4554 August 1–6, 2021. ©2021 Association for Computational Linguistics Q English Spanish English C Spanish mBERT A mBERT + Syn. How many members of the Senate are elected? Cu´antos miembros del Senado son elegidos? The Chamber of Deputies has 630 elected members, while the Senate has 315 elected members. . . . La c´amara de los diputados est´a formada por 630 miembros, mientras que hay 315 senadores m´as lo"
2021.acl-long.350,2020.acl-main.147,0,0.20937,"cussion To foster research in this direction, we discuss additional experiment findings. • A natural question is, instead of using GAT, why we do not modify attention heads in mBERT to embed the dependency structure (as shown in Eq. 3). We observed a consistent performance drop across all the tasks if we intervene in self-attention (blocking pair-wise attention). We anticipate fusing GAT encoded syntax representations helps as it adds bias to the self-attention. For future works, we suggest exploring ways of adding structure bias, e.g., scaling attention weights based on dependency structure (Bugliarello and Okazaki, 2020). • Among the evaluation datasets, Wikiann consists of sentence fragments, and the semantic parsing benchmarks consist of user utterances that are typically short in length. Sorting and analyzing the performance improvements based on sequence lengths suggests that the utilization of dependency structure has limited scope for shorter text sequences. However, part-of-speech tags help to identify span boundaries improving the slot filling tasks. 4.4 Limitations and Challenges In this work, we assume we have access to an offthe-shelf universal parser, e.g., UDPipe (Straka and Strakov´a, 2017) or S"
2021.acl-long.350,P17-1177,0,0.024762,"on networks (GAT) (Veliˇckovi´c et al., 2018), a variant of GNN that employs the multihead attention mechanism. Syntax-aware Multi-head Attention A large body of prior works investigated the advantages of incorporating language syntax to enhance the self-attention mechanism (Vaswani et al., 2017). Existing techniques can be broadly divided into two types. The first type of approach relies on an external parser (or human annotation) to get a sentence’s dependency structure during inference. This type of approaches embed the dependency structure into contextual representations (Wu et al., 2017; Chen et al., 2017; Wang et al., 2019a,b; Zhang et al., 2019, 2020; Bugliarello and Okazaki, 2020; Sachan et al., 2021; Ahmad et al., 2021). Our proposed method falls under this category; however, unlike prior works, our study investigates if fusing the universal dependency structure into the self-attention of existing multilingual encoders help cross-lingual transfer. Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category. The second category of approaches does not require the syntax structure of the input text dur"
2021.acl-long.350,2020.acl-main.493,0,0.0203556,"is followed by “miembros”, while 315 is followed by “senadores” in Spanish. et al., 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recognition (Pires et al., 2019; Wu and Dredze, 2019), and more. Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer. In particular, Wu and Dredze (2019) showed that mBERT captures language syntax that makes it effective for cross-lingual transfer. A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking a tree-like structure that agrees with the Universal Dependencies taxonomy. However, fine-tuning for the downstream task in a source language may not require mBERT to retain structural features or learn to encode syntax. We argue that encouraging mBERT to learn the correlation between syntax structure and target labels can benefit cross-lingual transfer. To support our argument, we show an example of question answering (QA) in Figure 2. In the example, mBERT predicts incorrect answers given the Spanish language context that can be cor"
2021.acl-long.350,P18-1078,0,0.0197933,"ion answering benchmarks, MLQA (Lewis et al., 2020), and XQuAD (Artetxe et al., 2020). We use the SQuAD dataset (Rajpurkar et al., 2016) for training and validation. In the QA task, the inputs are a question and a context passage that consists of many sentences. We formulate QA as a multi-sentence reading comprehension task; jointly train the models to predict the answer sentence and extract the answer span from it. We concatenate the question and each sentence from the context passage and use the [CLS] token representation to score the candidate sentences. We adopt the confidence method from Clark and Gardner (2018) and pick the highest-scored sentence to extract the answer span during inference. We provide more details of the QA models in Appendix. Task-oriented Semantic Parsing The fourth evaluation task is cross-lingual task-oriented semantic parsing. In this task, the input is a user utterance and the goal is to predict the intent of the utterance and fill the corresponding slots. We conduct experiments on two recently proposed benchmarks: (i) mTOP (Li et al., 2021) and (ii) mATIS++ (Xu et al., 2020). We jointly train the BERT models as suggested in Chen et al. (2019). We summarize the evaluation tas"
2021.acl-long.350,2020.acl-main.747,0,0.0836037,"Missing"
2021.acl-long.350,D18-1269,0,0.32364,"cy tree structure helps cross-lingual transfer. We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and taskoriented semantic parsing. The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages. In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA. 1 Figure 1: Two parallel sentences in English and Hindi from XNLI (Conneau et al., 2018) dataset. The words highlighted with the same color have the same meaning. Although the sentences have a different word order, their syntactic dependency structure is similar. Introduction Cross-lingual transfer reduces the requirement of labeled data to perform natural language processing (NLP) in a target language, and thus has the ability to avail NLP applications in low-resource languages. However, transferring across languages is challenging because of linguistic differences at levels of morphology, syntax, and semantics. For example, word order difference is one of the crucial factors th"
2021.acl-long.350,R19-1028,0,0.02055,"ur proposed method falls under this category; however, unlike prior works, our study investigates if fusing the universal dependency structure into the self-attention of existing multilingual encoders help cross-lingual transfer. Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category. The second category of approaches does not require the syntax structure of the input text during inference. These approaches are trained to predict the dependency parse via supervised learning (Strubell et al., 2018; Deguchi et al., 2019). 6 Conclusion In this work, we propose incorporating universal language syntax into multilingual BERT (mBERT) 8 This happen for languages, such as Arabic as parsers normalize the input that lead to inconsistent characters between input text and the output tokenized text. by infusing structured representations into its multihead attention mechanism. We employ a modified graph attention network to encode the syntax structure of the input sequences. The results endorse the effectiveness of our proposed approach in the cross-lingual transfer. We discuss limitations and challenges to drive future"
2021.acl-long.350,D19-1549,0,0.0277643,"ucture during inference. This type of approaches embed the dependency structure into contextual representations (Wu et al., 2017; Chen et al., 2017; Wang et al., 2019a,b; Zhang et al., 2019, 2020; Bugliarello and Okazaki, 2020; Sachan et al., 2021; Ahmad et al., 2021). Our proposed method falls under this category; however, unlike prior works, our study investigates if fusing the universal dependency structure into the self-attention of existing multilingual encoders help cross-lingual transfer. Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category. The second category of approaches does not require the syntax structure of the input text during inference. These approaches are trained to predict the dependency parse via supervised learning (Strubell et al., 2018; Deguchi et al., 2019). 6 Conclusion In this work, we propose incorporating universal language syntax into multilingual BERT (mBERT) 8 This happen for languages, such as Arabic as parsers normalize the input that lead to inconsistent characters between input text and the output tokenized text. by infusing structured representations into its multihead"
2021.acl-long.350,prazak-konopik-2017-cross,0,0.0464265,"Missing"
2021.acl-long.350,2020.acl-demos.14,0,0.0285633,"ng the evaluation datasets, Wikiann consists of sentence fragments, and the semantic parsing benchmarks consist of user utterances that are typically short in length. Sorting and analyzing the performance improvements based on sequence lengths suggests that the utilization of dependency structure has limited scope for shorter text sequences. However, part-of-speech tags help to identify span boundaries improving the slot filling tasks. 4.4 Limitations and Challenges In this work, we assume we have access to an offthe-shelf universal parser, e.g., UDPipe (Straka and Strakov´a, 2017) or Stanza (Qi et al., 2020) to collect part-of-speech tags and the dependency structure of the input sequences. Relying on such a parser has a limitation that it may not support all the languages available in benchmark datasets, e.g., we do not consider Thai and Swahili languages in the benchmark datasets. There are a couple of challenges in utilizing the universal parsers. First, universal parsers tokenize the input sequence into words and provide partof-speech tags and dependencies for them. The tokenized words may not be a part of the input.7 As a result, tasks requiring extracting text spans (e.g., QA) need addition"
2021.acl-long.350,D16-1264,0,0.23039,"0 (7) [Q:English-C:English] 315 (3); [Q:Spanish-C:Spanish] 315 (3) [Q:Spanish-C:English] 315 (3); [Q:English-C:Spanish] 315 (3) Figure 2: A parallel QA example in English (en) and Spanish (es) from MLQA (Lewis et al., 2020) with predictions from mBERT and our proposed syntax-augmented mBERT. In “Q:x-C:y”, x and y indicates question and context languages, respectively. Based on our analysis of the highlighted tokens’ attention weights, we conjecture that mBERT answers 630 as the token is followed by “miembros”, while 315 is followed by “senadores” in Spanish. et al., 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recognition (Pires et al., 2019; Wu and Dredze, 2019), and more. Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer. In particular, Wu and Dredze (2019) showed that mBERT captures language syntax that makes it effective for cross-lingual transfer. A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking a tree-like structure that agrees with the Universal Depe"
2021.acl-long.350,2021.eacl-main.228,0,0.0348442,"on mechanism. Syntax-aware Multi-head Attention A large body of prior works investigated the advantages of incorporating language syntax to enhance the self-attention mechanism (Vaswani et al., 2017). Existing techniques can be broadly divided into two types. The first type of approach relies on an external parser (or human annotation) to get a sentence’s dependency structure during inference. This type of approaches embed the dependency structure into contextual representations (Wu et al., 2017; Chen et al., 2017; Wang et al., 2019a,b; Zhang et al., 2019, 2020; Bugliarello and Okazaki, 2020; Sachan et al., 2021; Ahmad et al., 2021). Our proposed method falls under this category; however, unlike prior works, our study investigates if fusing the universal dependency structure into the self-attention of existing multilingual encoders help cross-lingual transfer. Graph attention networks (GATs) that use multi-head attention has also been adopted for NLP tasks (Huang and Carley, 2019) also fall into this category. The second category of approaches does not require the syntax structure of the input text during inference. These approaches are trained to predict the dependency parse via supervised learning"
2021.acl-long.350,K17-3009,0,0.0728998,"Missing"
2021.acl-long.350,D18-1548,0,0.0533004,"Missing"
2021.acl-long.350,D19-1030,0,0.0531307,"Missing"
2021.acl-long.350,D19-1077,0,0.0181633,"5 (3); [Q:English-C:Spanish] 315 (3) Figure 2: A parallel QA example in English (en) and Spanish (es) from MLQA (Lewis et al., 2020) with predictions from mBERT and our proposed syntax-augmented mBERT. In “Q:x-C:y”, x and y indicates question and context languages, respectively. Based on our analysis of the highlighted tokens’ attention weights, we conjecture that mBERT answers 630 as the token is followed by “miembros”, while 315 is followed by “senadores” in Spanish. et al., 2018), question answering (Rajpurkar et al., 2016; Lewis et al., 2020), named entity recognition (Pires et al., 2019; Wu and Dredze, 2019), and more. Since the introduction of mBERT, several works (Wu and Dredze, 2019; Pires et al., 2019; K et al., 2020) attempted to reason their success in cross-lingual transfer. In particular, Wu and Dredze (2019) showed that mBERT captures language syntax that makes it effective for cross-lingual transfer. A few recent works (Hewitt and Manning, 2019; Jawahar et al., 2019; Chi et al., 2020) suggest that BERT learns compositional features; mimicking a tree-like structure that agrees with the Universal Dependencies taxonomy. However, fine-tuning for the downstream task in a source language may"
2021.acl-long.535,N19-1071,0,0.0128072,"ve text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribut"
2021.acl-long.535,2020.acl-main.461,0,0.024727,"Missing"
2021.acl-long.535,D19-1291,0,0.248492,"onversation datasets: dialogue summarization from SAMSum (Gliwa et al., 2019b), heuristic-generated community question answering from CQASumm (Chowdhury and Chakraborty, 2018), meeting summarization data from AMI and ICSI, and smaller test sets in the news comments, discussion forum, and email domains. We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains. To unify modeling across these conversational domains, we propose to use recent work in end-toend argument mining (Lenz et al., 2020; Stab and Gurevych, 2014; Chakrabarty et al., 2019) to instantiate the theoretical graph framework which motivated our annotation protocol, proposed by Barker and Gaizauskas (2016a) for conversation summarization. This protocol is employed to both identify and use the “issues–viewpoints–assertions” argument structure (discussed in Related Work) for summarizing news comments. We construct this argument graph using entailment relations, linearize the graph, train a graph-to-text model (Ribeiro et al., 2020), and experiment with argument mining as a way to reduce noise in long-text input. Our contributions are the following: (1) we crowdsource da"
2021.acl-long.535,2020.emnlp-main.336,0,0.247035,"chieved stateof-the-art performance across summarization tasks and strong performance in zero and few-shot settings (Fabbri et al., 2020a). However, less work has focused on summarizing online conversations. Unlike documents, articles, and scientific papers, which contain specific linguistic structures and conventions such as topic sentences and abstracts, conversational text scatters main points across multiple utterances and between numerous writers. As a result, the text summarization task in the conversational data domain offers a challenging research field to test newly-developed models (Chen and Yang, 2020). Recently, Gliwa et al. (2019a) introduced a dataset for chat-dialogue conversation summarization consisting of 16k examples, the first largescale dataset of its kind. Previous work in conversation summarization was limited by the data available and focused primarily on meeting summarization, such as the AMI (Kraaij et al., 2005) and ICSI (Janin et al., 2003) datasets. The datasets 6866 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6866–6880 August 1–6, 2021. ©2021 Assoc"
2021.acl-long.535,2020.acl-main.460,0,0.0142711,"modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribution within the current paradigm, which was confirmed by the poor performance of models transferred from other domains c"
2021.acl-long.535,W17-4513,0,0.0129361,"s. Lenz et al. (2020) are the first to propose an end-to-end approach for constructing an argument graph (Stede et al., 2016), a structured representation of claims and premises in an argumentative text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is compara"
2021.acl-long.535,W17-4218,0,0.0218394,"le contribution within the current paradigm, which was confirmed by the poor performance of models transferred from other domains compared to that trained on this validation data. 3 ConvoSumm In this section, we introduce our dataset selection, our annotation protocol, and the characteristics of our crowdsourced dataset. Data Selection For the news comments subdomain, we use the NYT Comments dataset, which consists of 2 million comments made on 9,000 New York Times articles published between 2017 and 2018. It is publicly available and has been used in work for news-comment relevance modeling (Kolhatkar and Taboada, 2017); it also contains metadata that may be of use in summarization modeling. For the discussion forums and debate subdomain, we select Reddit data from CoarseDiscourse (Zhang et al., 2017), which contains annotations about the discourse structure of the threads. For the community question answering subdomain, we use StackExchange (Stack), which provides access to all forums and has been used in modeling for answer relevance and question deduplication (Hoogeveen et al., 2015). We chose StackExchange over the commonly-used Yahoo! Answers data due to licensing reasons. For the email threads subdomai"
2021.acl-long.535,2020.acl-main.703,0,0.0439697,"of comments from a New York Times article discussing people’s favorite parts of the Super Bowl. The summary is an analysis of the comments and quantifies the viewpoints present. Introduction Automatic text summarization is the process of outputting the most salient parts of an input in a concise and readable form. Recent work in summarization has made significant progress due to introducing large-scale datasets such as the CNNDailyMail dataset (Nallapati et al., 2016) and the New York Times dataset (Sandhaus, 2008). Furthermore, the use of large self-supervised pretrained models such as BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2019) has achieved stateof-the-art performance across summarization tasks and strong performance in zero and few-shot settings (Fabbri et al., 2020a). However, less work has focused on summarizing online conversations. Unlike documents, articles, and scientific papers, which contain specific linguistic structures and conventions such as topic sentences and abstracts, conversational text scatters main points across multiple utterances and between numerous writers. As a result, the text summarization task in the conversational data domain offers a challenging research"
2021.acl-long.535,W04-1013,0,0.0255664,"Missing"
2021.acl-long.535,2021.ccl-1.108,0,0.0300145,"Missing"
2021.acl-long.535,N15-1046,0,0.395525,"et has lower inter-document similarity, which presents challenges for models which rely strictly on redundancy in the input, and our datasets generally exhibit less layout bias, when compared to the analysis done in Dey et al. (2020b). Comparison to Existing Datasets Although previous work on conversation summarization, before the introduction of SAMSum (Gliwa et al., 2019b), has largely featured unsupervised or fewshot methods, there exist several datasets with reference summaries. These include SENSEI (Barker et al., 2016b) for news comments, the Argumentative Dialogue Summary Corpus (ADS) (Misra et al., 2015) for discussion forums, and the BC3 (Ulrich et al., 2009) dataset for email data. However, much of the existing datasets are not wide in scope. For example, SENSEI only covers six topics and the ADS Corpus covers one topic and only has 45 dialogues. Furthermore, they each pertain to one subdomain of conversation. Our dataset avoids these issues by covering four diverse subdomains of conversation and having approximately 500 annotated summaries for each subdomain. Additionally, since neural abstractive summarization baselines do not exist for these datasets, we benchmark our models on these dat"
2021.acl-long.535,K16-1028,0,0.0331568,"Missing"
2021.acl-long.535,D18-1206,0,0.0689261,"Missing"
2021.acl-long.535,N19-4009,0,0.0132363,"Summaries Recent work has shown the strength of text-based pretrained models on graph-to-text problems (Ribeiro et al., 2020). Following that work, we linearize the graph by following a depth-first approach starting from the Conversation Node. We found that inserting special tokens to signify edge types did not improve performance, likely due to the size of our data, and simply make use of an arrow → to signify the relationship between sentences. We train a sequence-to-sequence model on our linearized graph input, which we call -arg-graph. 5 Experimental Settings We use the fairseq codebase (Ott et al., 2019) for our experiments. Our base abstractive text summarization model is BART-large (Lewis et al., 2020), a pretrained denoising autoencoder with 336M parameters that builds on the sequence-to-sequence transformer of Vaswani et al. (2017). We finetune BART using a polynomial decay learning rate scheduler with Adam optimizer (Kingma and Ba, 2015). We used a learning rate of 3e-5 and warmup and total updates of 20 and 200, following previous few-shot transfer work (Fabbri et al., 2020a). We could have equally fine-tuned other pretrained models such as Pegasus (Zhang et al., 2019) or T5 (Raffel et"
2021.acl-long.535,W14-4407,1,0.80133,"s of our proposed datasets; (2) we benchmark state-of-theart models on these datasets as well as previous widely-used conversation summarization datasets to provide a clear baseline for future work; and (3) we apply argument mining to model the structure of our conversational data better as well as reduce noise in long-text input, showing comparable or improved results in both automatic and human evaluations.1 2 Related Work Modeling Conversation Summarization Early approaches to conversation summarization consisted of feature engineering (Shasha Xie et al., 2008), template selection methods (Oya et al., 2014), and statistical machine learning approaches (Galley, 2006; Wang and Cardie, 2013). More recent modeling approaches for dialogue summarization have attempted to take advantage of conversation structures found within the data through dialogue act classification (Goo and Chen, 2018b), discourse labeling (Ganesh and Dingliwal, 2019), topic segmentation (Liu et al., 2019c), and keypoint analysis (Liu et al., 2019a). Chen and Yang (2020) utilize multiple conversational structures from different perspectives in its sequence-tosequence model. However, such approaches focus exclusively on dialogue su"
2021.acl-long.535,D14-1006,0,0.369633,"active model on several conversation datasets: dialogue summarization from SAMSum (Gliwa et al., 2019b), heuristic-generated community question answering from CQASumm (Chowdhury and Chakraborty, 2018), meeting summarization data from AMI and ICSI, and smaller test sets in the news comments, discussion forum, and email domains. We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains. To unify modeling across these conversational domains, we propose to use recent work in end-toend argument mining (Lenz et al., 2020; Stab and Gurevych, 2014; Chakrabarty et al., 2019) to instantiate the theoretical graph framework which motivated our annotation protocol, proposed by Barker and Gaizauskas (2016a) for conversation summarization. This protocol is employed to both identify and use the “issues–viewpoints–assertions” argument structure (discussed in Related Work) for summarizing news comments. We construct this argument graph using entailment relations, linearize the graph, train a graph-to-text model (Ribeiro et al., 2020), and experiment with argument mining as a way to reduce noise in long-text input. Our contributions are the follo"
2021.acl-long.535,L16-1167,0,0.0240666,"his framework and advances in argument mining for end-to-end training for summarization. Argument Mining Work in argument mining (Stab and Gurevych, 2014) has aimed to identify these argumentative units and classify them into claims, premises, and major claims, or claims describing the key concept in a text. More recently, Chakrabarty et al. (2019) propose to finetune BERT (Devlin et al., 2019) for identifying argumentative units and relationships between them within a text and across texts. Lenz et al. (2020) are the first to propose an end-to-end approach for constructing an argument graph (Stede et al., 2016), a structured representation of claims and premises in an argumentative text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summari"
2021.acl-long.535,P10-1078,0,0.0476971,"subdomain, we use StackExchange (Stack), which provides access to all forums and has been used in modeling for answer relevance and question deduplication (Hoogeveen et al., 2015). We chose StackExchange over the commonly-used Yahoo! Answers data due to licensing reasons. For the email threads subdomain, we use the publicly-available W3C corpus (Craswell et al., 2005). Previous work also made use of this dataset for email summarization (Ulrich et al., 2008) but provided only a small sample of 40 email threads, for which we provide transfer testing results. We generally follow the guidance of Tomasoni and Huang (2010), from summarizing community question answering forums, for determining which subsets of data to select from the above datasets. We remove an example if (1) there were less than five posts (four in the case of email threads; “post” refers to any answer, comment, or email); (2) the longest post was over 400 words; (3) the sum of all post lengths was outside of [100, 1400] words (although we extended this maximum length for NYT comments); or (4) the average length of the posts was outside of the [50, 300] words interval. For Stack data, we first filtered answers which received a negative communi"
2021.acl-long.535,P13-1137,0,0.0384572,"Missing"
2021.acl-long.535,N18-1101,0,0.0180917,"ction, following Chakrabarty et al. (2019) and making use of data for argument mining from that paper and from Stab and Gurevych (2014). The output of this step can also simply be used without further graph construction as a less noisy version of the input, which we call -arg-filtered. Relationship Type Classification We follow the procedure in Lenz et al. (2020) and use entailment to determine the relationship between argumentative units within a document. However, rather than using the classifier provided, we make use of RoBERTa (Liu et al., 2019b) fine-tuned on the MNLI entailment dataset (Williams et al., 2018). Rather than using both support and contradiction edges between claims and premises, we make the simplification that all relationships can be captured with support edges, as we are dealing with a single document in this step. Within a single text, the 6871 Dataset/Method NYT Reddit Stack Email Lexrank 22.30/3.87/19.14 22.71/4.52/19.38 26.30/5.62/22.27 16.04/3.68/13.38 Textrank 25.11/3.75/20.61 24.38/4.54/19.84 25.43/4.40/20.58 19.50/3.90/16.18 BERT-ext 25.88/3.81/22.00 24.51/4.18/20.95 26.84/4.63/22.85 25.46/6.17/21.73 Table 4: ROUGE-1/2/L results for extractive LexRank (Erkan and Radev, 2004"
2021.acl-long.535,P19-1503,0,0.0134024,"on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribution within the current paradigm, which was confirmed by the poor performance of models transferred"
2021.acl-long.535,2020.findings-emnlp.19,0,0.0196521,"in sequences over the typical 1024 max encoder length with which BART is trained, we copied the encoder positional embeddings to allow sequences up to length 2048. To address the input-length of meeting summaries, which range from 6k to 12k tokens, we use the Longformer (Beltagy et al., 2020), which allows for sequences up to length 16k to6872 Method/Dataset HMNet DDA-GCN Longformer-BART Longformer-BART-arg AMI 53.02/18.57/53.15/22.32/54.20/20.72/51.36 54.47/20.83/51.74 ICSI 46.28/10.60/43.03/12.14/40.26 44.17/11.69/41.33 Table 6: ROUGE-1/2/L results for DDA-GCN (Feng et al., 2020) and HMNet (Zhu et al., 2020) on the AMI and ICSI meeting summarization dataset along with our Longformer and Longformer-arg models. kens. We initialize the Longformer model with BART parameters trained on the CNN-DailyMail dataset, as the meeting summarization datasets contain fewer than 100 data points. We otherwise fine-tune models from vanilla BART, following intuition in few-shot summarization (Fabbri et al., 2020a) and based on initial experiments. In the tables which follow, ”-arg” refers to any model trained with argument-mining-based input, and we specify which -arg-graph or -arg-filtered settings were used for e"
2021.eacl-main.257,2020.acl-main.747,0,0.253518,"Missing"
2021.eacl-main.257,N19-1423,0,0.23268,"other languages. This is mainly due to the painstaking process of manually annotating and creating large datasets for this task in new languages. In addition to the shortage of such datasets, existing datasets (Upadhyay et al., 2018; Schuster et al., 2019a) are not sufficiently diversified in terms of languages and domains, and do not capture complex nested queries. This makes it difficult to perform more systematic and rigorous experimentation and evaluation for this task across multiple languages. Building on these considerations and recent advancements on cross-lingual pre-trained models (Devlin et al., 2019; Lample and Conneau, 2019; Conneau et al., 2020), this paper is making an effort to bridge the above mentioned gaps. The main contributions of this paper can be summarized as follows: Scaling semantic parsing models for taskoriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper"
2021.eacl-main.257,N13-1073,0,0.0481775,"is a common approach to augment target language training data. For semantic parsing tasks, besides translation we need alignment to project slot annotations to target language. This process is similar to how we collect our dataset, but using machine translation and alignment methods. For translation, we use our in-house machine translation system. We also tried other publicly available translation APIs and didn’t find significant difference in final task performance. For alignment, we experimented with both, using attention weights from translation as in Schuster et al. (2019a) and fastalign (Dyer et al., 2013) and found data generated through fastalign leads to better task performance. Thus we only report results that use fastalign. 5.1.2 Multilingual Training With the advancement of multilingual pre-trained models, a single model trained on multiple languages has shown to outperform in-language models (Conneau et al., 2020; Hu et al., 2020). As a result, we also experiment with multilingual training on our benchmark, including training jointly on all in-language data and training on English plus translated and aligned data in all other languages for the zero-shot setting. Instead of concatenating"
2021.eacl-main.257,N16-1024,0,0.0319403,"s been centered around intent detection and slot filling - for example, the representations used on the ATIS dataset (Mesnil et al., 2013; Liu and Lane, 2016; Zhu and Yu, 2017) and in the Dialog State Tracking Challenge (Williams et al., 2016). This essentially boils down to a text classification and a sequence labeling task, which works great for simple non-compositional queries. For more complex queries with recursive slots, state of the art systems use hierarchical representations, such as the TOP representation (Gupta et al., 2018), that is modeled using Recurrent Neural Network Grammars (Dyer et al., 2016) or as a Sequence to Sequence task (Rongali et al., 2020). Pre-trained Cross-lingual Representation Over the past few years, pre-trained cross-lingual representations have demonstrated tremendous success in achieving state of the art in various NLP tasks. The majority of the earlier work focuses on cross-lingual emebedding alignment (Mikolov et al., 2013; Ammar et al., 2016; Lample et al., 2018). Schuster et al. (2019b) further extend upon this by aligning contextual word embeddings from the ELMo model (Peters et al., 2018). Later with the success of Transformer (Vaswani et al., 2017) based ma"
2021.eacl-main.257,D18-1300,1,0.928398,"ce the noise in slot label projection. 1 Introduction With the rising adoption of virtual assistant products, task-oriented dialog systems have been attracting more attention in both academic and industrial communities. One of the first steps in these systems is to extract meaning from the natural language used in conversation to build a semantic representation of the user utterance. Typical systems achieve this by classifying the intent of the utterance and tagging the corresponding slots. With the goal of handling more complex queries, recent approaches propose hierarchical representations (Gupta et al., 2018) that are expressive enough to capture the task-specific semantics of complex nested queries. • MTOP Dataset: We release an almost-parallel multilingual task-oriented semantic parsing dataset covering 6 languages and 11 domains. To the best of our knowledge, this is the first multilingual dataset which contains compositional representations that allow complex nested queries. • We build strong benchmarks on the released MTOP dataset using state-of-the-art multilingual pre-trained models for both flat and compositional representations. We demonstrate the effectiveness of our approaches by achiev"
2021.eacl-main.257,2020.acl-main.703,0,0.073233,"duce mBERT and XLM respectively, and Pires et al. (2019) show the effectiveness of these on sequence labeling tasks. Conneau et al. (2020) present XLM-R, a pre-trained multilingual masked language model trained on data in 100 languages, that provides strong gains over XLM and mBERT on classification and sequence labeling tasks. The models discussed above are encoder-only models. More recently, multilingual seq-to-seq pre-training has become popular. Liu et al. (2020a) introduce mBART, a seq-to-seq denoising auto-encoder pre-trained on monolingual corpora in many languages, which extends BART (Lewis et al., 2020b) to a multilingual setting. More recently, Lewis et al. (2020a) introduced a seq-to-seq model pre-trained on a multilingual multi-document paraphrasing objective, which selfsupervises the reconstruction of target text by retrieving a set of related texts and conditions on them to maximize the likelihood of generating the original. Tran et al. (2020) is another contemporary work that mines parallel data using encoder representations and jointly trains a seq-to-seq model on this parallel data. Cross-Lingual Task-Oriented Semantic Parsing Due to the ubiquity of digital assistants, the task of c"
2021.eacl-main.257,2020.tacl-1.47,0,0.122215,"e success of Transformer (Vaswani et al., 2017) based masked language model pre-training, Devlin et al. (2019) and Lample and Conneau (2019) introduce mBERT and XLM respectively, and Pires et al. (2019) show the effectiveness of these on sequence labeling tasks. Conneau et al. (2020) present XLM-R, a pre-trained multilingual masked language model trained on data in 100 languages, that provides strong gains over XLM and mBERT on classification and sequence labeling tasks. The models discussed above are encoder-only models. More recently, multilingual seq-to-seq pre-training has become popular. Liu et al. (2020a) introduce mBART, a seq-to-seq denoising auto-encoder pre-trained on monolingual corpora in many languages, which extends BART (Lewis et al., 2020b) to a multilingual setting. More recently, Lewis et al. (2020a) introduced a seq-to-seq model pre-trained on a multilingual multi-document paraphrasing objective, which selfsupervises the reconstruction of target text by retrieving a set of related texts and conditions on them to maximize the likelihood of generating the original. Tran et al. (2020) is another contemporary work that mines parallel data using encoder representations and jointly tr"
2021.eacl-main.257,N18-1202,0,0.289025,"et al., 2018), that is modeled using Recurrent Neural Network Grammars (Dyer et al., 2016) or as a Sequence to Sequence task (Rongali et al., 2020). Pre-trained Cross-lingual Representation Over the past few years, pre-trained cross-lingual representations have demonstrated tremendous success in achieving state of the art in various NLP tasks. The majority of the earlier work focuses on cross-lingual emebedding alignment (Mikolov et al., 2013; Ammar et al., 2016; Lample et al., 2018). Schuster et al. (2019b) further extend upon this by aligning contextual word embeddings from the ELMo model (Peters et al., 2018). Later with the success of Transformer (Vaswani et al., 2017) based masked language model pre-training, Devlin et al. (2019) and Lample and Conneau (2019) introduce mBERT and XLM respectively, and Pires et al. (2019) show the effectiveness of these on sequence labeling tasks. Conneau et al. (2020) present XLM-R, a pre-trained multilingual masked language model trained on data in 100 languages, that provides strong gains over XLM and mBERT on classification and sequence labeling tasks. The models discussed above are encoder-only models. More recently, multilingual seq-to-seq pre-training has b"
2021.eacl-main.257,P19-1493,0,0.0199658,"e-trained cross-lingual representations have demonstrated tremendous success in achieving state of the art in various NLP tasks. The majority of the earlier work focuses on cross-lingual emebedding alignment (Mikolov et al., 2013; Ammar et al., 2016; Lample et al., 2018). Schuster et al. (2019b) further extend upon this by aligning contextual word embeddings from the ELMo model (Peters et al., 2018). Later with the success of Transformer (Vaswani et al., 2017) based masked language model pre-training, Devlin et al. (2019) and Lample and Conneau (2019) introduce mBERT and XLM respectively, and Pires et al. (2019) show the effectiveness of these on sequence labeling tasks. Conneau et al. (2020) present XLM-R, a pre-trained multilingual masked language model trained on data in 100 languages, that provides strong gains over XLM and mBERT on classification and sequence labeling tasks. The models discussed above are encoder-only models. More recently, multilingual seq-to-seq pre-training has become popular. Liu et al. (2020a) introduce mBART, a seq-to-seq denoising auto-encoder pre-trained on monolingual corpora in many languages, which extends BART (Lewis et al., 2020b) to a multilingual setting. More rec"
2021.eacl-main.257,H90-1020,0,0.73837,"-to-seq model on this parallel data. Cross-Lingual Task-Oriented Semantic Parsing Due to the ubiquity of digital assistants, the task of cross-lingual and multilingual task-oriented dialog has garnered a lot of attention recenty, and few multilingual benchmark datasets have been released for the same. To the best of our knowledge, all of them only contain simple non-compositional utterances, suitable for the intent and slots detection tasks. Upadhyay et al. (2018) release a benchmark dataset in Turkish and Hindi (600 training examples), obtained by translating utterances from the ATIS corpus (Price, 1990) and using Amazon Mechanical Turk to generate phrase level slot annotation on translations. Schuster et al. (2019a) release a bigger multilingual dataset for task-oriented dialog in English, Spanish and Thai across 3 domains. They also propose various modeling techniques such as using XLU embeddings (see Ruder et al. (2017) for literature review) for cross-lingual transfer, translate-train and ELMo (Peters et al., 2018) for target language training. BERT-style multilingual pre-trained models have also been applied to task-oriented semantic parsing. Castellucci et al. (2019) use multilingual BE"
2021.eacl-main.257,N19-1380,1,0.473127,"a Yashar Mehdad Facebook {aimeeli,abhinavarora,shuohui,anchit}@fb.com {mehdad,sonalgupta}@fb.com Abstract Although, there have been sizable efforts around developing successful semantic parsing models for task-oriented dialog systems in English (Mesnil et al., 2013; Liu and Lane, 2016; Gupta et al., 2018; Rongali et al., 2020), we have only seen limited works for other languages. This is mainly due to the painstaking process of manually annotating and creating large datasets for this task in new languages. In addition to the shortage of such datasets, existing datasets (Upadhyay et al., 2018; Schuster et al., 2019a) are not sufficiently diversified in terms of languages and domains, and do not capture complex nested queries. This makes it difficult to perform more systematic and rigorous experimentation and evaluation for this task across multiple languages. Building on these considerations and recent advancements on cross-lingual pre-trained models (Devlin et al., 2019; Lample and Conneau, 2019; Conneau et al., 2020), this paper is making an effort to bridge the above mentioned gaps. The main contributions of this paper can be summarized as follows: Scaling semantic parsing models for taskoriented dia"
2021.eacl-main.257,N19-1162,0,0.194679,"a Yashar Mehdad Facebook {aimeeli,abhinavarora,shuohui,anchit}@fb.com {mehdad,sonalgupta}@fb.com Abstract Although, there have been sizable efforts around developing successful semantic parsing models for task-oriented dialog systems in English (Mesnil et al., 2013; Liu and Lane, 2016; Gupta et al., 2018; Rongali et al., 2020), we have only seen limited works for other languages. This is mainly due to the painstaking process of manually annotating and creating large datasets for this task in new languages. In addition to the shortage of such datasets, existing datasets (Upadhyay et al., 2018; Schuster et al., 2019a) are not sufficiently diversified in terms of languages and domains, and do not capture complex nested queries. This makes it difficult to perform more systematic and rigorous experimentation and evaluation for this task across multiple languages. Building on these considerations and recent advancements on cross-lingual pre-trained models (Devlin et al., 2019; Lample and Conneau, 2019; Conneau et al., 2020), this paper is making an effort to bridge the above mentioned gaps. The main contributions of this paper can be summarized as follows: Scaling semantic parsing models for taskoriented dia"
2021.emnlp-main.301,2020.acl-main.771,0,0.0515162,"Missing"
2021.emnlp-main.301,2020.findings-emnlp.198,0,0.0334833,"andard benchmarks such as ERASER. However, such models are susceptible to fabricating explanations to justify even their incorrect predictions, as identified by Camburu et al. (2020) and Wiegreffe et al. (2020). We introduce sentence markers into seq2seq models which alleviates this problem and also significantly improves their rationale extraction performance on sentencelevel ERASER benchmark tasks (see Section 4.2). clude that intermediate tasks requiring high levels of reasoning and inference abilities are more likely to help, particularly when task data is scarce. Closest to our method is Kung et al. (2020) who use Squad 2.0 as an intermediate task to fine-tune a shared encoder fitted with task-specific classification heads, for the downstream BeerReview and MovieReview rationalization tasks. Our approach is to strategically restructure large open domain QA datasets (Natural Questions and HotpotQA) to make them amenable to IFT of both the encoder and the decoder of pre-trained seq2seq models. This enables the use of exactly the same model architecture for multiple rationale prediction tasks. Multiple prior works (Paranjape et al., 2020; Jain et al., 2020; Narang et al., 2020) have explored metho"
2021.emnlp-main.301,Q19-1026,0,0.127707,"al., 2020; Pruksachatkun et al., 2016; Swanson et al., 2020; Tenney et al., et al., 2020), more so in the few-shot setting. We 2019), and input token relationships (Lamm et al., find that this method also extends to seq2seq mod- 2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where rationales as Natural Questions (Kwiatkowski et al., 2019) are first selected as the sole inputs to the prediction and HotpotQA (Yang et al., 2018), which then im- stage, either in a supervised (Lehman et al., 2019; proves downstream performance on rationale ex- Pruthi et al., 2020) or an unsupervised (Paranjape traction benchmarks. This approach is motivated et al., 2020; Bastings et al., 2019; Jain et al., 2020) by the similarity of the process of gathering sup- fashion. Alternatively, rationales can also serve as porting facts for QA, to that of rationale extraction post-hoc supporting evidence, produced after the for classification tasks. While e"
2021.emnlp-main.301,P19-1612,0,0.0221775,"luation tasks. We explore this in Section 7.2. 4 Datasets In this section, we discuss the open-domain QA datasets and our pre-processing steps to prepare them for IFT, as well as, the ERASER rationalizing datasets that we use for evaluation. Table 1 presents the sizes of each dataset split, as well as the average input passage lengths, in terms of the number of tokens and sentences, for both types of datasets. 4.1 Intermediate Fine-Tuning Datasets Natural Questions (NQ) (Kwiatkowski et al., 2019) comprises real Google search queries with answer-span annotations from Wikipedia pages. Following Lee et al. (2019) we use a subset containing short answers (&lt; 6 tokens). For every question and answer-span annotation, we use the question as q, the segmented Wikipedia passage as p, the answer tokens as the prediction y, and the single sentence containing the answer span as the rationale e. We remove all tables and lists from the Wikipedia passages, but retain section headers. HotpotQA (Yang et al., 2018) is a multi-hop QA dataset, where each question and answer annotation is accompanied with supporting fact sentence annotations from multiple Wikipedia documents. Similar to NQ, we use the question as q and t"
2021.emnlp-main.301,N19-1371,0,0.073832,"ps (Lamm et al., find that this method also extends to seq2seq mod- 2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where rationales as Natural Questions (Kwiatkowski et al., 2019) are first selected as the sole inputs to the prediction and HotpotQA (Yang et al., 2018), which then im- stage, either in a supervised (Lehman et al., 2019; proves downstream performance on rationale ex- Pruthi et al., 2020) or an unsupervised (Paranjape traction benchmarks. This approach is motivated et al., 2020; Bastings et al., 2019; Jain et al., 2020) by the similarity of the process of gathering sup- fashion. Alternatively, rationales can also serve as porting facts for QA, to that of rationale extraction post-hoc supporting evidence, produced after the for classification tasks. While earlier works on ra- model prediction, as a snippet to help users vertionale generation (Paranjape et al., 2020; Narang ify the prediction (Yang et al., 2018"
2021.emnlp-main.301,D16-1011,0,0.0604492,"Missing"
2021.emnlp-main.301,2020.acl-main.703,0,0.0418172,"s, Underground, Somewhere nearby … Tasha oohed in awe. I said, &quot;Frodo&apos;s been visiting you, eh ?&quot; Malaquez said, &quot;Your pet ?&quot; &quot;Hardly. He lives around here somewhere. I suppose he was attracted to the commotion up the hill.&quot; ... Answer: Somewhere nearby Figure 1: Example questions, answers and corresponding passages from the BoolQ and MultiRC datasets from the ERASER benchmark (DeYoung et al., 2020). Annotated rationales are highlighted. Note that rationales can be multi-sentence and non-contiguous. Introduction While large pre-trained language models (Devlin et al., 2019; Raffel et al., 2019; Lewis et al., 2020) with hundreds of millions of parameters have made super-human performance possible on various NLP datasets, they lack transparency into their decision making process, which can adversely affect user trust in their predictions. Recent works have proposed the use of natural language (NL) rationales (Lei et al., 2016; DeYoung et al., 2020; Latcinnik and Berant, 2020) as a means to either obtain an understanding of the reasoning process of models, or ∗ 1 Equal Contribution. github.com/facebookresearch/fidex as a human-readable snippet for users to verify predictions (Lipton, 2018). Figure 1 prese"
2021.emnlp-main.301,W19-4825,0,0.0252519,"Missing"
2021.emnlp-main.301,2020.blackboxnlp-1.21,0,0.0613138,"Missing"
2021.emnlp-main.301,P04-1035,0,0.0518193,"s rationales. We discuss these datasets in this section. with long Wikipedia passages (&gt; 3,000 tokens), as well as sentence-level rationale annotations (provided by ERASER) that support the answer. MultiRC (Khashabi et al., 2018) comprises input passages and questions, with multiple-choice answers, with sentence level rationale annotations. It is evaluated as a Boolean QA task by concatenating each answer choice to the question, and assigning a True label to correct choices and False to the rest. All choices use the same set of supporting facts. MovieReviews (Movies) (Zaidan and Eisner, 2008; Pang and Lee, 2004) contains movie reviews paired with binary positive/negative labels, without a query q (we set it to “What is the sentiment of this review?” in our models). While ERASER provides span-level rationale annotations, we translate these to sentence level annotations following prior work (Paranjape et al., 2020). FiD-Ex can also potentially be trained to output extracted input phrase markers and we leave this to future work. FEVER (Thorne et al., 2018) The ERASER version of FEVER contains input passages along with claims (q) that must be classified as supported or refuted, based on the passage, toge"
2021.emnlp-main.301,2020.emnlp-main.153,1,0.892967,"Missing"
2021.emnlp-main.301,2020.acl-main.467,0,0.0405225,"Missing"
2021.emnlp-main.301,2020.findings-emnlp.353,0,0.0143462,"2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where rationales as Natural Questions (Kwiatkowski et al., 2019) are first selected as the sole inputs to the prediction and HotpotQA (Yang et al., 2018), which then im- stage, either in a supervised (Lehman et al., 2019; proves downstream performance on rationale ex- Pruthi et al., 2020) or an unsupervised (Paranjape traction benchmarks. This approach is motivated et al., 2020; Bastings et al., 2019; Jain et al., 2020) by the similarity of the process of gathering sup- fashion. Alternatively, rationales can also serve as porting facts for QA, to that of rationale extraction post-hoc supporting evidence, produced after the for classification tasks. While earlier works on ra- model prediction, as a snippet to help users vertionale generation (Paranjape et al., 2020; Narang ify the prediction (Yang et al., 2018; Thorne et al., et al., 2020) are limited by the input passage size"
2021.emnlp-main.301,P19-1487,0,0.0453534,"Missing"
2021.emnlp-main.301,P16-1162,0,0.100217,"Missing"
2021.emnlp-main.301,P19-1282,0,0.0279145,"very little insight into their decision making mechanics. To expose model understanding at various depths, researchers have proposed various structural probing (Tenney et al., 2018; Hewitt and Manning, 2019; Lin et al., 2019) and behavioral probing methods (McCoy et al., 2020; Goldberg, 2019; Warstadt et al., 2019; Ettinger, Fine-tuning pre-trained models on data-rich in- 2020), as well as input saliency maps to highlight termediate tasks before fine-tuning on classification the most important tokens/sentences in the input for end tasks has recently been shown to improve end- each prediction (Serrano and Smith, 2019; Ribeiro task performance (Vu et al., 2020; Pruksachatkun et al., 2016; Swanson et al., 2020; Tenney et al., et al., 2020), more so in the few-shot setting. We 2019), and input token relationships (Lamm et al., find that this method also extends to seq2seq mod- 2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where"
2021.emnlp-main.301,2020.acl-main.496,0,0.0310418,"us depths, researchers have proposed various structural probing (Tenney et al., 2018; Hewitt and Manning, 2019; Lin et al., 2019) and behavioral probing methods (McCoy et al., 2020; Goldberg, 2019; Warstadt et al., 2019; Ettinger, Fine-tuning pre-trained models on data-rich in- 2020), as well as input saliency maps to highlight termediate tasks before fine-tuning on classification the most important tokens/sentences in the input for end tasks has recently been shown to improve end- each prediction (Serrano and Smith, 2019; Ribeiro task performance (Vu et al., 2020; Pruksachatkun et al., 2016; Swanson et al., 2020; Tenney et al., et al., 2020), more so in the few-shot setting. We 2019), and input token relationships (Lamm et al., find that this method also extends to seq2seq mod- 2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where rationales as Natural Questions (Kwiatkowski et al., 2019) are first selected as the sole inp"
2021.emnlp-main.301,P19-1485,0,0.0231865,"jointly produce intermediate QA datasets to improve its regular and few-shot performance for rationale extraction. NL rationales along with model predictions. We Fine-tuning large pre-trained models on intermedi- illustrate our method using the BoolQ dataset from the ERASER explainability benchmark, which ate tasks has been shown to be effective by prior work; Phang et al. (2018) use data-rich intermedi- comprises of questions with passages and boolean ate NLI tasks to improve target classification tasks; answers (see Figure 1), together with human annotated rationales (details in Section 4). Talmor and Berant (2019) fine-tune on multiple QA Formally, given an input query q and an input datasets to improve the generalizability of QA modpassage p comprising sentences p = {sj }N els. Intermediate fine-tuning (IFT) can also hurt j=1 , our performance (Bingel and Søgaard, 2017). Pruk- goal is to produce a prediction y and rationale sensachatkun et al. (2020) recently present a large- tences {ek }K k=1 , ek ∈ p, K  N , that justify y. scale study on fine-tuning a pre-trained RoBERTa Narang et al. (2020) fine-tune the pre-trained T5 model on 100 intermediate-target task combina- (Text-to-Text Transfer Transfor"
2021.emnlp-main.301,D18-1259,0,0.105325,"so in the few-shot setting. We 2019), and input token relationships (Lamm et al., find that this method also extends to seq2seq mod- 2020). Alongside, there is work on producing texels, for explanation generation. We fine-tune pre- tual rationales (Lei et al., 2016), which are snippets trained seq2seq models to extract supporting evi- of NL to help explain model predictions. Models dence for existing open-domain QA datasets such may take a pipelined approach, where rationales as Natural Questions (Kwiatkowski et al., 2019) are first selected as the sole inputs to the prediction and HotpotQA (Yang et al., 2018), which then im- stage, either in a supervised (Lehman et al., 2019; proves downstream performance on rationale ex- Pruthi et al., 2020) or an unsupervised (Paranjape traction benchmarks. This approach is motivated et al., 2020; Bastings et al., 2019; Jain et al., 2020) by the similarity of the process of gathering sup- fashion. Alternatively, rationales can also serve as porting facts for QA, to that of rationale extraction post-hoc supporting evidence, produced after the for classification tasks. While earlier works on ra- model prediction, as a snippet to help users vertionale generation (P"
2021.emnlp-main.301,D08-1004,0,0.06052,"put sentences annotated as rationales. We discuss these datasets in this section. with long Wikipedia passages (&gt; 3,000 tokens), as well as sentence-level rationale annotations (provided by ERASER) that support the answer. MultiRC (Khashabi et al., 2018) comprises input passages and questions, with multiple-choice answers, with sentence level rationale annotations. It is evaluated as a Boolean QA task by concatenating each answer choice to the question, and assigning a True label to correct choices and False to the rest. All choices use the same set of supporting facts. MovieReviews (Movies) (Zaidan and Eisner, 2008; Pang and Lee, 2004) contains movie reviews paired with binary positive/negative labels, without a query q (we set it to “What is the sentiment of this review?” in our models). While ERASER provides span-level rationale annotations, we translate these to sentence level annotations following prior work (Paranjape et al., 2020). FiD-Ex can also potentially be trained to output extracted input phrase markers and we leave this to future work. FEVER (Thorne et al., 2018) The ERASER version of FEVER contains input passages along with claims (q) that must be classified as supported or refuted, based"
2021.emnlp-main.301,P19-1452,0,0.0211651,"Missing"
2021.emnlp-main.301,N18-1074,0,0.0652997,"Missing"
2021.emnlp-main.301,D19-1286,0,0.0609392,"Missing"
2021.findings-acl.95,P17-1171,0,0.0157855,"gnitive science and identify failure cases for ODQA explanations (Section 4.3). 2 Background Open-domain QA. ODQA involves answering questions using a large, broad corpus of unstructured documents (e.g., Wikipedia or the Web). More specifically, the questions are factoid and the target answer is present as a span in one of the documents (Voorhees et al., 1999). Recent models for ODQA use a pipelined approach and contain two components: a document retriever that finds a subset of the most relevant documents from the corpus, and a reader that selects an answer span from the retrieved documents (Chen et al., 2017; Lee et al., 2019a). We use a state-of-the-art ODQA model and a benchmark dataset that contains questions asked by real lay users (Section 3.3). An ODQA model’s prediction can be explained by providing a justification in natural language, e.g., by extracting snippets of text from the retrieved documents (rationales) or more generally by generating new text (abstractive explanation). For example, rationales can explain a text classifier using phrases in the input text that are relevant to the prediction (Lei et al., 2016). However, for some tasks, such as NLI (Camburu et al., 2018) and common-"
2021.findings-acl.95,P19-1612,0,0.0889638,"identify failure cases for ODQA explanations (Section 4.3). 2 Background Open-domain QA. ODQA involves answering questions using a large, broad corpus of unstructured documents (e.g., Wikipedia or the Web). More specifically, the questions are factoid and the target answer is present as a span in one of the documents (Voorhees et al., 1999). Recent models for ODQA use a pipelined approach and contain two components: a document retriever that finds a subset of the most relevant documents from the corpus, and a reader that selects an answer span from the retrieved documents (Chen et al., 2017; Lee et al., 2019a). We use a state-of-the-art ODQA model and a benchmark dataset that contains questions asked by real lay users (Section 3.3). An ODQA model’s prediction can be explained by providing a justification in natural language, e.g., by extracting snippets of text from the retrieved documents (rationales) or more generally by generating new text (abstractive explanation). For example, rationales can explain a text classifier using phrases in the input text that are relevant to the prediction (Lei et al., 2016). However, for some tasks, such as NLI (Camburu et al., 2018) and common-sense reasoning (R"
2021.findings-acl.95,D16-1011,0,0.0773795,"Missing"
2021.naacl-main.100,D13-1160,0,0.0513368,"ring inference. 5 Experiments Datasets We use four benchmark open-domain QA datasets following Lee et al. (2019): Natural Questions (NQ) contains real user questions asked on Google searches; we consider questions with short answers up to 5 tokens. T RIVIAQA (Joshi et al., 2017) consists of questions collected from trivia and quiz-league webTraining We obtain top K predictions (pj , sj ) sites; we take questions in an unfiltered setting and of model M for each question qi in its training discard the provided web snippets. set, which we divide into positives, where sj is WebQuestions (W EB Q) (Berant et al., 2013) is a exactly the groundtruth answer, and remaining neg- collection of questions extracted from the Google atives. We train R using mini-batch gradient de- Suggest API, with answers being Freebase entities. scent, where in each iteration, for question q, we CuratedTREC (Baudiš and Šediv`y, 2015) coninclude 1 randomly chosen positive and M − 1 tains curated questions from TREC QA track. 1282 Model NQ T RIVIAQA W EB Q TREC BM25+BERT (Lee et al., 2019) ORQA (Lee et al., 2019) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever (Asai et al., 2020) REALM (Guu et al., 2020) R"
2021.naacl-main.100,P17-1171,0,0.0191057,"four QA datasets. Open-domain Question Answering (Voorhees et al., Earlier work (Wang et al., 2018c,b) on open1999) (QA) involves answering questions by ex- domain QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail Gorbachev is This training scheme possibly explains empirical correct and the rest are close false positives. Table 1 ∗ presents accura"
2021.naacl-main.100,N19-1423,0,0.180271,"er reranking successful for span-extraction tasks, even over large pretrained models, and improve the state 1 Introduction of the art on four QA datasets. Open-domain Question Answering (Voorhees et al., Earlier work (Wang et al., 2018c,b) on open1999) (QA) involves answering questions by ex- domain QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail"
2021.naacl-main.100,Q19-1026,0,0.0199922,"QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail Gorbachev is This training scheme possibly explains empirical correct and the rest are close false positives. Table 1 ∗ presents accuracies obtained by the same model on Work done while at Facebook AI. 1 github.com/facebookresearch/reconsider four QA datasets, if the answer exactly matches 1280 Proceedin"
2021.naacl-main.100,P19-1612,0,0.0991828,"Missing"
2021.naacl-main.100,P18-1178,0,0.0204178,"020). In this paper, we dence MRC model predictions, and thus, learns to significantly improve MRC model performance by eliminate hard false positives. This can be viewed as a coarse-to-fine approach of training span selec- making re-ranking successful using span-focused tors, with the base MRC model trained on heuris- re-ranking of its highly confident predictions. For Open-domain QA, it is crucial to train MRC tically chosen negatives and the re-ranker trained models to distinguish passage-span pairs containon finer, more subtle negatives. This contrasts with multi-task training approaches (Wang et al., 2018c), ing the answer (positives) from those that do not (negatives). Using negatives that appear as close whose re-scoring gains are limited by training on false positives can produce more robust MRC modthe same data, especially when coupled with large els. However, prior work relies on upstream repre-trained models. Our approach also scales to any number of ranked candidates, unlike previous con- trieval models to supply distantly supervised positives (contain answer string) and negatives (Asai catenation based cross-passage re-ranking methods et al., 2020), that are in-turn trained using heuri"
2021.naacl-main.100,2020.emnlp-main.519,0,0.0755875,"Missing"
2021.naacl-main.100,D19-1284,1,0.833917,"Missing"
2021.naacl-main.100,D16-1264,0,0.208398,"Missing"
2021.naacl-main.57,N19-1071,0,0.0212821,"etworks, small finetune-able layers that aim to reproduce characteristics of the target dataset as seen in a small set of labeled examples. In contrast, we aim to encode the characteristics of our target dataset, such as level of extraction and compression, a priori in the intermediate training phase. In other work, Lebanoff et al. (2018) adapt a single-document summarization model to multi-document settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was"
2021.naacl-main.57,D18-1045,0,0.0306921,"f that given target dataset. Unless otherwise stated, all results reported are ROUGE1/2/L. We run all few-shot transfer experiments on five subsets of supervised data, and the reported numbers, unless zero-shot, are the average of the top three results of the five runs following previous work (Gunel et al., 2020). The 10 data point sets are subsets of the 100 data point sets. Data Augmentation Parameters: For data augmentation via round-trip translation, we use a beam size of 10 and k of 10 on German and Russian translation models; fairseq provides bidirectional pretrained translation models (Edunov et al., 2018) from WMT19 (Ng et al., 2019) for these language pairs. For both 10 and 100 data points, this resulted in 2010 and 20100 total data points. For consistency loss, we use the same augmented data. Model Hyperparameters: We use the fairseq codebase (Ott et al., 2019) for our experiments. Our base abstractive text summarization model is BART-large (Lewis et al., 2020), a pretrained denoising autoencoder with 336M parameters that builds off of the sequence-to-sequence transformer 4 Experimental Settings of Vaswani et al. (2017). We fine-tune BART usDatasets: We experiment with four datasets, CN- ing"
2021.naacl-main.57,2020.acl-main.703,1,0.88821,"ine-tuning an already-pretrained model specifically for summarization on a downstream dataset 2 Related Work by leveraging a generic text corpus (Wikipedia) While advances have been made in neural tech- to create auxiliary fine-tuning data that transfers niques for summarization due in part to large across domains, allowing for more fine-grained datasets, less work has focused on domain adap- control over the transfer process. We show the tation of such methods in the zero and few-shot generalizability of such fine-tuning across domains. settings. Wang et al. (2019) examine domain adap- BART (Lewis et al., 2020) is a pretrained denoising tation, but in extractive summarization. Hua and autoencoder and achieved state-of-the-art perforWang (2017) examine domain adaptation between mance when fine-tuned on summarization tasks at opinion and news summarization, observing that the time. In this work, we use BART as our base models trained on one domain and applied to an- pretrained model but in future work will experiother domain can capture relevant content but differ ment with other pretrained models. 705 3 Methods Data Augmentation via Round-Trip Translation: In addition to fine-tuning on WikiTransfer d"
2021.naacl-main.57,W04-1013,0,0.0207062,"ning pretrained models using unsupervised Wikipedia data. We create dataset-specific unsupervised data for this intermediate fine-tuning, by making use of characteristics of the target dataset such as the average length of input documents, the average summary length, and the general bin of whether the summaries desired are very abstractive or very extractive, as discussed above. Assume that we want a summary of M sentences from source documents of N sentences on average, and that we know approximately how extractive the summaries are in the target dataset, as defined as the upper bound ROUGE (Lin, 2004) performance of an extractive model, the extractive oracle, on that dataset. We bin the level of extraction of the target summaries into extremely abstractive (ROUGE oracle 10-30), more abstractive (ROUGE oracle 20-30), more extractive (ROUGE oracle 30-50), and extremely extractive (ROUGE oracle 40-60). We then iterate the following procedure on all Wikipedia articles available in a Wikipedia dump: We remove the first M sentences from the Wikipedia article for use as a summary and the following N sentences for use as a source document. Then, we want to check whether this pseudo data point matc"
2021.naacl-main.57,W04-3252,0,0.010523,"t settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was shown for abstractive summarization in Zhang et al. (2019) and extractive summarization in Desai et al. (2020). Our work focuses on the zero-shot abstractive summarization setting and the transferability of models fine-tuned on taskspecific data from a generic corpus, rather than just the transferability of a single pretrained model. The closest work to ours for zero-shot transfer is Yang et al. (2020)"
2021.naacl-main.57,K16-1028,0,0.0325103,"of subfunctions of the input, called subaspects, which 1 Introduction determine the output form. Jung et al. (2019) deAutomatic text summarization aims to distill the fine three subaspects for summarization: position, most salient content of a given text in a compact importance, and diversity, and study how these form. Recent advances in summarization have been subaspects manifest themselves in summarization driven by the availability of large-scale datasets corpora and model outputs. For example, a comsuch as the CNN-DailyMail (CNNDM) corpus mon subaspect for the CNNDM dataset is position; (Nallapati et al., 2016) and the New York Times earlier sentences tend to constitute a good sum704 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 704–717 June 6–11, 2021. ©2021 Association for Computational Linguistics mary. Inspired by this view of summarization as subaspects, we aim to encode subaspects of a target dataset into unlabeled data to allow a model finetuned on this data to learn characteristics of the target dataset to improve zero-shot and few-shot transfer of the model. In our work, we focus on the s"
2021.naacl-main.57,P19-1212,0,0.0365573,"Missing"
2021.newsum-1.10,P18-1013,0,0.0218161,"tractiveabstractive baselines. 1 Figure 1: An example of a summary and its evidence (highlighted) as generated by our framework. naturalness, coherence, and conciseness. In this paper, we propose EASE, a novel framework that combines the two systems to produce natural summaries that can be traced back to an interpretable extractive summary. Our general framework can accommodate different pretrained models and suitable for any evidence-based text generation task. The existing extractive-abstractive systems can be divided into three main categories: 1- Relying on attention for interpretability (Hsu et al., 2018). Due to the probabilistic nature of the attention mechanism, it falls short of providing usable evidence; 2- Providing word-level evidence for the generated summaries (Gehrmann et al., 2018). Though more useful than attention, this evidence is too granular to be useful for humans; 3- Training the content selector separately using pseudo labels or other heuristics (Liu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summ"
2021.newsum-1.10,P19-1284,0,0.0625665,"Missing"
2021.newsum-1.10,2020.inlg-1.14,0,0.0136044,"Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the document, reducing their ∗ Equal contribution. 85 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 85–95 November 10, 2021. ©2021 Association for Computational Linguistics cific framew"
2021.newsum-1.10,N19-1071,0,0.023831,"fining it using a pretrained language model. In EASE, we use pretrained models, i.e., BART, to initialize the extractive and abstractive modules but after that, use an end-to-end loss that trains both modules simultaneously. 6.1.1 Self-supervised Summarization Miao and Blunsom (2016) introduced an autoencoder setup for sentence compression to reduce the need for labeled examples. A copy ptr/generator model was used for the compressor which alongside the reconstructor is trained to reconstruct the unlabeled documents. Moreover, REINFORCE (Williams, 1992) was used to train the model end-to-end. Baziotis et al. (2019) introduced a similar autoencoder setup but used the Gumbel Softmax reparametrization for training. (Févry and Phang, 2018) also used a denoising autoencoder to compress sentences and a countdown at the decoder to control summary length. Inspired by the IB principle, West et al. (2019) introduced a recursive algorithm to prune a document to form an unsupervised extractive summary. These summaries are in turn used to train a selfsupervised system using a next-sentence objective is used. In contrast, we use a loss formulation derived directly from the IB and train the model endto-end. (Saito et"
2021.newsum-1.10,2020.emnlp-main.529,0,0.0423038,"t an extractor capable of extracting the most informative parts from which the source can be reconstructed should be better positioned to extract important parts of the source, resulting in higherquality summaries. Therefore, we pretrain EASE on the WikiText-103 (Merity et al., 2017) dataset to reconstruct the original unlabeled documents using the same loss as in (4) by setting Y = X. This can be viewed as a special case of summarization, where the compression rate is one. We only pretrain the token-level model, since pretraining sentence-level models without measures such as topic guidance (Kang and Hovy, 2020) typically leads to hallucination. Results on the CNN/DM dataset by adding pretraining are presented in the second row of Table 5. Even though pretraining improves the token-level model, results for the spanlevel model are mixed. Our hypothesis is that the lasso continuity helps with summarization by picking contiguous spans, as evidenced by the high RL. However, during the reconstruction pretraining, the lasso loss can be problematic by masking long spans, which are then prone to hallucinations. We leave pretraining alongside span extraction using techniques such as guided reconstruction to f"
2021.newsum-1.10,D16-1031,0,0.028326,"hieved the state-of-the-art results on many summarization tasks. Later, Zhang et al. (2019b) extended the MLM denoising objective using sentence masking. Zhang et al. (2019c) introduced a multi-stage encoder for extractive summarization, whereas Zhang et al. (2019a) use a two-stage decoder to generate summaries by creating a draft and refining it using a pretrained language model. In EASE, we use pretrained models, i.e., BART, to initialize the extractive and abstractive modules but after that, use an end-to-end loss that trains both modules simultaneously. 6.1.1 Self-supervised Summarization Miao and Blunsom (2016) introduced an autoencoder setup for sentence compression to reduce the need for labeled examples. A copy ptr/generator model was used for the compressor which alongside the reconstructor is trained to reconstruct the unlabeled documents. Moreover, REINFORCE (Williams, 1992) was used to train the model end-to-end. Baziotis et al. (2019) introduced a similar autoencoder setup but used the Gumbel Softmax reparametrization for training. (Févry and Phang, 2018) also used a denoising autoencoder to compress sentences and a countdown at the decoder to control summary length. Inspired by the IB princ"
2021.newsum-1.10,2020.acl-main.703,1,0.894465,", 2018). Though more useful than attention, this evidence is too granular to be useful for humans; 3- Training the content selector separately using pseudo labels or other heuristics (Liu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the"
2021.newsum-1.10,D18-1206,0,0.0291962,"nd-to-end fashion. Moreover, we observe that the sentence-level model performs slightly better than the tokenlevel model for CNN/DM but slightly worse for XSum. We hypothesize that for the more extractive CNN/DM dataset, keeping continuous spans of text is of paramount importance, while for the more abstractive XSum dataset, the sparsity budget Experimental Settings Datasets: We primarily experiment with the CNN/DailyMail dataset (Hermann et al., 2015) owing to its extractive-like nature; its summaries are typically closely related to the source sentences. We also present results on the XSUM (Narayan et al., 2018) dataset, a highly abstractive dataset in which summaries can be viewed as a title for the source documents. Model Hyperparameters and evaluation metrics: We initialize the seq-to-seq abstractor with the BART-large model and initialize the extractor with the BART-base encoder. We use the fairseq codebase1 for our experiments and use the same hyperparameters as used for fine1 Results 2 https://github.com/pytorch/fairseq 88 https://github.com/pltrdy/files2rouge Model CNN/DailyMail XSum BART-large (Lewis et al., 2019) BERTSUM (Liu and Lapata, 2019) 44.16/21.28/40.90 43.85/20.34/39.90 45.14/22.27/"
2021.newsum-1.10,2020.emnlp-main.153,1,0.851356,"Missing"
2021.newsum-1.10,D19-1387,0,0.295477,"erent pretrained models and suitable for any evidence-based text generation task. The existing extractive-abstractive systems can be divided into three main categories: 1- Relying on attention for interpretability (Hsu et al., 2018). Due to the probabilistic nature of the attention mechanism, it falls short of providing usable evidence; 2- Providing word-level evidence for the generated summaries (Gehrmann et al., 2018). Though more useful than attention, this evidence is too granular to be useful for humans; 3- Training the content selector separately using pseudo labels or other heuristics (Liu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b"
2021.newsum-1.10,2020.emnlp-main.748,0,0.199202,"s and suitable for any evidence-based text generation task. The existing extractive-abstractive systems can be divided into three main categories: 1- Relying on attention for interpretability (Hsu et al., 2018). Due to the probabilistic nature of the attention mechanism, it falls short of providing usable evidence; 2- Providing word-level evidence for the generated summaries (Gehrmann et al., 2018). Though more useful than attention, this evidence is too granular to be useful for humans; 3- Training the content selector separately using pseudo labels or other heuristics (Liu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 202"
2021.newsum-1.10,2021.ccl-1.108,0,0.0648287,"Missing"
2021.newsum-1.10,2020.acl-main.173,0,0.018671,"rhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the document, reducing their ∗ Equal contribution. 85 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 85–95 November 10, 2021. ©2021 Association for Computational Linguistics cific framework, it achieves poor results on benchmarks such as CNN/DM. EASE on the other hand, is based on the Information Bottleneck ("
2021.newsum-1.10,N19-1397,0,0.0218643,"ly grounded IB principle with no pseudo labels. Moreover, we seek consistent models suitable for more extractive datasets and achieve results on par with the abstractive model while only using half of the input. (Gehrmann et al., 2018) trained a content selector separately to tag the words and then use bottom-up attention to only copy words from the tagged set. Similar to our token-level model, this is not useful evidence. Compressive summarization is another way to have a trade-off between extractive and abstractive methods where extractive summaries are compressed to form the final summary (Mendes et al., 2019). Recently, Desai et al. (2020) use syntactic rules to find a high-recall candidate set and then use the notions of plausibility and salience to ensure the grammaticality and importance of the remaining pieces, respectively. Unlike compressive summarization, we explore an extractive-abstractive framework where a concise abstractive summary can be traced back to the evidence; learned jointly with no manual rules or postprocessing. Pretrained Models for Summarization Lewis et al. (2020) introduced BART, a generalpurpose denoising seq2seq transformer, that achieved the state-of-the-art results on"
2021.newsum-1.10,D19-1389,0,0.0175284,"duced an autoencoder setup for sentence compression to reduce the need for labeled examples. A copy ptr/generator model was used for the compressor which alongside the reconstructor is trained to reconstruct the unlabeled documents. Moreover, REINFORCE (Williams, 1992) was used to train the model end-to-end. Baziotis et al. (2019) introduced a similar autoencoder setup but used the Gumbel Softmax reparametrization for training. (Févry and Phang, 2018) also used a denoising autoencoder to compress sentences and a countdown at the decoder to control summary length. Inspired by the IB principle, West et al. (2019) introduced a recursive algorithm to prune a document to form an unsupervised extractive summary. These summaries are in turn used to train a selfsupervised system using a next-sentence objective is used. In contrast, we use a loss formulation derived directly from the IB and train the model endto-end. (Saito et al., 2020) used a saliency model to extract important pieces of a document before feeding them to an abstractive seq2seq model. In contrast with our model, the saliency module is trained separately by using heuristics to provide pseudo labels for the extraction. (Yang et al., 2020b) pr"
2021.newsum-1.10,2020.webnlg-1.11,1,0.816047,"as a coarse version of the final abstractive output. We leverage pretrained language models that first extract the necessary evidence from the source document (extractor) and then, using only the extracted evidence spans, generate the final output (abstractor). Fig. 1 shows an example of the summary and evidence generated by our system. Our main contributions are as follows: many times smaller than the original document, but also reduces the effective input length used during abstraction. This has been shown to directly correlate with the extent of hallucination in pretrained language models (Yang et al., 2020a). In order to formalize the problem, we use the IB principle to learn an optimal model between the original document x and the final summary y through a compressed representation z. The IB objective is to minimize the following: LIB = I(x; z) − βI(z; y), where I() is the mutual information. This objective encourages z to contain only the information about x that is useful in predicting y. Moreover, β controls the trade-off in z between containing information about x (i.e., sparsity) vs about y (i.e., prediction quality). We use a relaxation for (1) similar to Paranjape et al. (2020) to make"
2021.newsum-1.10,2020.findings-emnlp.168,0,0.0486836,"Missing"
2021.newsum-1.10,K19-1074,0,0.339033,"iu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the document, reducing their ∗ Equal contribution. 85 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 85–95 November 10, 2021. ©2021 Association for Computationa"
2021.newsum-1.10,P19-1499,0,0.366404,"iu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the document, reducing their ∗ Equal contribution. 85 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 85–95 November 10, 2021. ©2021 Association for Computationa"
2021.newsum-1.10,P19-1139,0,0.387488,"iu and Lapata, 2019; Pilault et al., 2020). In contrast, we seek a theoreticallygrounded model that can learn the evidence extraction end-to-end. Perhaps the closest work to ours is Zhao et al. (2020) focusing on long-document summarization by training a joint extractive-abstractive model via weak supervision. Though a complicated and speIntroduction Pretrained sequence-to-sequence language models such as BART (Lewis et al., 2020), T5 (Raffel et al., 2019) and their variants have achieved state-of-theart results on various tasks such as summarization, machine translation, and data2text tasks (Zhang et al., 2019b; Kale and Rastogi, 2020). Despite the higher fidelity compared with models without pretraining for tasks such as summarization (Maynez et al., 2020), the lack of interpretability in abstractive generation remains an obstacle to their broader adoption. Extractive summarization systems, on the other hand, have the advantage of being interpretable but are too restrictive by forcing the output to be spans from the document, reducing their ∗ Equal contribution. 85 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 85–95 November 10, 2021. ©2021 Association for Computationa"
D11-1062,W10-0733,0,0.0314394,"Missing"
D11-1062,W10-0701,0,0.0128303,"ios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusin"
D11-1062,N10-1045,1,0.400414,"ection without sacrificing quality. We show that a complex data creation task, for which even experts usually feature low agreement scores, can be effectively decomposed into simple subtasks assigned to non-expert annotators. The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German. 1 Yashar Mehdad FBK-irst and University of Trento Trento, Italy mehdad@fbk.eu Introduction Cross-lingual Textual Entailment (CLTE) has been recently proposed by (Mehdad et al., 2010; Mehdad et al., 2011) as an extension of Textual Entailment (Dagan and Glickman, 2004). The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE, 1 http://www.nist.gov/tac/2011/RTE/ http://nlp.uned.es/clef-qa/ave/ 3 http://www.evalita.it/2009/tasks/te 4 For instance, in the first five RTE Challenges, the average effort needed to create 1,000 pairs featuring full agreement among 3 annotators was around 2.5 person-months. Typical"
D11-1062,P11-1134,1,0.520619,"Missing"
D11-1062,P09-2078,0,0.012216,"bs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer” approach described in the next s"
D11-1062,P08-1051,0,0.0228368,"of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different ways). In such situations the standard quality control mechanisms are not directly applicable, and the detection of errors requires either costly manual verification at the end of the acquisition process, or more complex and creative solutions integrating HITs for quality check. Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010). The few solutions integrating validation HITs address the translation of single sentences, a task that is substantially different from ours (Negri and Mehdad, 2010; Bloodgood and Callison-Burch, 2010). Compared to sentence translation, the task of creating CLTE pairs is both harder to explain without recurring to notions that are difficult to understand to non-experts (e.g. “semantic equivalence”, “unidirectional entailment”), and harder to execute without mastering these notions. To tackle these issues the “divide and conquer”"
D11-1062,W10-0734,1,0.928539,"ating from scratch aligned CLTE corpora for different language combinations. To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools. Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses. In contrast, our approach integrates quality control mechanisms at all stages of the data collection/annotation process, thus minimizing the recourse to experts to check the quality of the collected material. Related research in the CLTE direction is reported in (Negri and Mehdad, 2010), which describes the creation of an English-Spanish corpus obtained from the RTE-3 dataset by translating the English hypotheses into Spanish. Translations have been crowdsourced adopting a methodology based on translation-validation cycles, defined as separate HITs. Although simplifying the CLTE corpus creation problem, which is recast as the task of translating already available annotated data, this solution is relevant to our work for the idea of combining gold standard units and “validation HITS” as a way to control the quality of the collected data at runtime. 3 Quality Control of Crowds"
D11-1062,P10-1122,0,0.0213975,"ince the core monolingual tasks of the process are carried out by manipulating English texts, we are able to address the very large community of English speaking workers, with a considerable reduction of costs and execution time. Finally, as a by-product of our method, the acquired pairs are fully aligned for all language combinations, thus enabling meaningful comparisons between scenarios of different complexity (monolingual TE, and CLTE between close or distant languages). We believe that, in the same spirit of recent works promoting large-scale annotation efforts around entailment corpora (Sammons et al., 2010; Bentivogli et al., 2010), the proposed approach and the resulting dataset5 will contribute to meeting the strong need for resources to develop and evaluate novel solutions for textual entailment. 2 Related Works Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7 , have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence T"
D11-1062,D08-1027,0,0.0603459,"Missing"
D11-1062,W10-0725,0,0.194152,"ounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka “Turkers”) hired through on-line marketplaces. As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers. Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities. Taking a step beyond the task of annotating exist5 The CLTE corpora described in this paper will be made freely available for research purposes through the website of the funding EU Project CoSyne (http://www.cosyne.eu/). 6 https://www.mturk.com/ 7 Although MTurk is directly accessible only to US citizens, the CrowdFlower service (http://crowdflower.com/) provides an interface to MTurk for non-US citizens. ing datasets, and showing the feasibili"
D11-1062,bentivogli-etal-2010-building,1,\N,Missing
D14-1168,C08-2002,0,0.01477,"to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how the opinions on aspects affect each other (e.g."
D14-1168,P99-1071,0,0.0199615,"007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the best of our knowledge, there are only three previous works on abstractive opinion summarization (Ganesan et al., 2010; Carenini et al., 2013; Di Fabbrizio et al., 2014). The first work (Ganesan et al., 2010) proposes a graph-based method for generating ultra concise opinion summaries that are more suitable for viewing on devices with small screens. This method does not provide a well-formed grammatical abstract and the generated summary only contains words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the gen"
D14-1168,W08-1106,1,0.59509,"number of users whose opinions contributed to the evaluation of the aspect. Polarity verbs: for each aspect, a polarity verb is selected based on the average sentiment polarity strength for that aspect. Although the average, in most cases, can be a good metric to evaluate the polarity of an aspect, it fails when the distribution of evaluations is centered on zero, for instance, if there are equal numbers of positive and negative evaluations (i.e., controversial). To partially solve this problem, we first check whether the aspect evaluation is controversial by applying the formula proposed by (Carenini and Cheung, 2008). In the case of controversiality, our microplanner selects a lexical item to express the controversiality of the aspect. In other cases, we use the average and select the polarity verb based on that. Connectives: in order to form more fluent and readable sentences and to increase the language variability, we randomly select our connectives from the list shown in Table 1. Moreover, when a parent aspect (excluding the root in AHT) has two children, they are connected by one of the coordinating conjunction “[and, similarly]” if they agree on polarity, and they will be connected by a choice of “["
D14-1168,W14-4408,0,0.476022,"Missing"
D14-1168,C10-1039,0,0.886339,"mary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the best of our knowledge, there are only three previous works on abstractive opinion summarization (Ganesan et al., 2010; Carenini et al., 2013; Di Fabbrizio et al., 2014). The first work (Ganesan et al., 2010) proposes a graph-based method for generating ultra concise opinion summaries that are more suitable for viewing on devices with small screens. This method does not provide a well-formed grammatical abstract and the generated summary only contains words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the generated summaries do not contain any information about the distribution of opinions. In the second work, (Carenini et al., 2013"
D14-1168,P13-1048,1,0.698154,"cture Theory (RST) (Mann and Thompson, 1988) is one of the most popular. RST divides a text into minimal atomic units, called Elementary Discourse Units (EDUs). It then forms a tree representation of a discourse called a Discourse Tree (DT) using rhetorical relations (e.g., Elaboration, Explanation, etc) as edges, and EDUs as leaves. EDUs linked by a rhetorical relation are also distinguished based on their relative importance in conveying the author’s message: nucleus is the central part, whereas satellite is the peripheral part. We use a publicly available state-of-the-art discourse parser (Joty et al., 2013)1 to generate a DT for each product review. Figure 1 (a) and (b) show DTs for two sample reviews where dotted edges identify the satellite spans. DT1 in Figure 1 (a) shows that review R1 consists of three EDUs with two relations Elaboration and Background between them. It also shows that the first EDU (i.e. I love camera) is the nucleus (shown by solid line) of the relation Elaboration and so the rest of the document (EDUs 2 and 3) is less important and aims at elaborating on what the author meant in the first EDU. Similarly, the structure shows that the third EDU is mentioned as background in"
D14-1168,P13-1160,0,0.0382663,"On the other hand, in contrast with Starlet-H, we do not limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is co"
D14-1168,E09-1059,0,0.0479103,"d the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of sum"
D14-1168,I13-1065,0,0.0238591,"ins words that occur in the original texts. Therefore, this approach is more extractive than abstractive. Another limitation is that the generated summaries do not contain any information about the distribution of opinions. In the second work, (Carenini et al., 2013) addresses some of the aforementioned problems and generates well-formed grammatical abstracts that describe the distribution of opinion over the entity and its features. However, for each product, this approach requires a feature taxonomy handcrafted by humans as an input, which is not scalable. To partially address this problem (Mukherjee and Joshi, 2013) has proposed a method for the automatic generation of a product attribute hierarchy that leverages ConceptNet (Liu and Singh, 2004). However, the resulting ontology tree has been used only for sentiment classification and not for classification. In the third and most recent study, (Di Fabbrizio et al., 2014) proposed Starlet-H as a hybrid abstractive/extractive sentiment summarizer. StarletH uses extractive summarization techniques to select salient quotes from the input reviews and embeds them into the abstractive summary to exemplify, justify or provide evidence for the aggregate positive o"
D14-1168,P04-1035,0,0.140788,"ate a graph. We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al."
D14-1168,W02-1011,0,0.0334496,"rse trees and generate a graph. We then select a subgraph representing the most important aspects and the rhetorical relations between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal"
D14-1168,radev-etal-2004-mead,0,0.0208993,"Missing"
D14-1168,N07-1038,0,0.0283479,"tions between them using a PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Bar"
D14-1168,D09-1018,0,0.00828789,"ot limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how the opinions on aspects af"
D14-1168,P08-1036,0,0.0116134,"PageRank algorithm, and transform the selected subgraph into an aspect tree. Finally, we generate a natural language summary by applying a template-based NLG framework. Quantitative and qualitative analysis of the results, based on two user studies, show that our approach significantly outperforms extractive and abstractive baselines. 1 Introduction Most existing works on sentiment summarization focus on predicting the overall rating on an entity (Pang et al., 2002; Pang and Lee, 2004) or estimating ratings for product features (Lu et al., 2009; Lerman et al., 2009; Snyder and Barzilay, 2007; Titov and McDonald, 2008)). However, the opinion summaries in such systems are extractive, meaning that they generate a summary by concatenating extracts that are representative of opinion on the entity or its aspects. Comparing extractive and abstractive summaries for evaluative texts has shown that an abstractive approach is more appropriate for summarizing evaluative text (Carenini et al., 2013; ∗ The contribution of the first two authors to this paper was equal. Di Fabbrizio et al., 2014). This finding is also supported by a previous study in the context of summarizing news articles (Barzilay et al., 1999). To the"
D14-1168,N13-1100,0,0.0350483,"ntrast with Starlet-H, we do not limit the input reviews to a small number of aspects and our aspect ordering method takes advantage of rhetorical information and does not require any training data. Our method relies on the discourse structure and discourse relations of reviews to infer the importance of aspects as well as the association between them (e.g., which aspects relate to each other). Researchers have recently started using the discourse structure of text in sentiment analysis and have shown its advantage in improving sentiment classification accuracy (e.g., (Lazaridou et al., 2013; Trivedi and Eisenstein, 2013; Somasundaran et al., 2009; Asher et al., 2008)). However, to the best of our knowledge, none of the existing works have looked into exploiting discourse structure in abstractive review summarization. In our work, importance of aspects, derived from the reviews’ discourse structure and relations, is used to rank and select aspects to be included in the summary. More specifically, we start with the most important (highest ranked) aspects to generate a summary and add more aspects to the system until a summary of desired length is obtained. Aspect association is considered to better explain how"
D14-1168,C00-2137,0,0.0194329,"Missing"
kouylekov-etal-2010-mining,D07-1073,0,\N,Missing
kouylekov-etal-2010-mining,C02-1167,0,\N,Missing
kouylekov-etal-2010-mining,E09-1064,0,\N,Missing
kouylekov-etal-2010-mining,W07-1401,0,\N,Missing
kouylekov-etal-2010-mining,P98-2127,0,\N,Missing
kouylekov-etal-2010-mining,C98-2122,0,\N,Missing
kouylekov-etal-2010-mining,kouylekov-magnini-2006-building,1,\N,Missing
kouylekov-etal-2010-mining,W04-3205,0,\N,Missing
L16-1493,E09-1005,0,0.0698683,"Missing"
L16-1493,P05-1018,0,0.0189283,"nted as a set {x1 , . . . , xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ("
L16-1493,N13-1136,0,0.0202871,"xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+"
L16-1493,W02-1001,0,0.295561,"information (Callan, 1994). Although this approach is simple and scalable, document style and structural differences when changing domains or publishers can significantly affect snippet quality. 3. System Our system takes as input an HTML document. We automatically extract the article text from the HTML, and then automatically preprocess the text to obtain sentences, tokens and part of speech tags. Then, we compute various features over the preprocessed document. Each sentence is scored using a combination of feature values and feature weights, which are learned using a structured perceptron (Collins, 2002). Finally, sentences are extracted in a greedy fashion based on their scores while respecting the length constraint. These steps are illustrated in Figure 2. 3.1. Features We implemented various features drawn from the summarization literature that capture aspects of salience, diversity, coverage, content and readability. Table 1 presents features from each category implemented in our framework. 3089 Category Position Length Content Lexical cues Syntactic cues Examples sentence position, paragraph position, in-paragraph position word length, character length, summary length similarity to query"
L16-1493,P06-1039,0,0.0790576,"Missing"
L16-1493,hong-etal-2014-repository,0,0.022483,"cument summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; and (b) for which the reference summaries were abstractive. † Equal contribution. Figure 1: Our summarization system More related to our framework, snippet extraction is a popular approach for search engines and news aggregators to show some content related to the query and the original document (Li et al., 2008). A simple way to identify snippets is to extract a passage from specific areas"
L16-1493,N12-1015,0,0.0159613,"∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+1 P return average weights k1 j wj where S ∗ represents a reference summary for document D. The parameters are estimated using the structured perceptron (Collins, 2002) which minimizes a 0/1 loss over D and incorporates parameter averaging for generalization. The basic learning procedure is described in Algorithm 1. When inference is inexact and carried out via search—as in the case of our framework—convergence and performance can be improved using violation-fixing weight updates (Huang and Feyong, 2012). In addition to greedy search, we also experimented with beams of various sizes to reduce search errors but did not observe performance improvements1 . 4. 4.1. Experiments - Systems Compared Baselines and Other Methods We compare the extractive summaries generated by our framework to three simple baselines (Lead-based and two variations of Greedy) as well as to an array of standard summarization methods from the literature. Lead-based The baseline, lead-based, algorithm takes the first sentences in the document that fit within the budget. Greedy The greedy algorithm is exactly the one describ"
L16-1493,W04-1013,0,0.0124222,"ntence to the document, computed using cosine similarity over term frequency vectors) and a binary feature— computed on the fly—to identify when a candidate sentence is adjacent to a sentence already present in the summary. The scoring function we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the i"
L16-1493,W12-2601,0,0.0322969,"unction we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the input document is a result of errors in article extraction from HTML (e.g. a byline is extracted as part of the article, or a photo caption is included as part of the article) or a repetition of the article title. 5. Experiments - Side by Side Editori"
L16-1493,D07-1047,0,0.0250912,"s without sacrificing meaning or grammaticality. We present evaluation results for the single-document news summarization use case, comparing performance on well written articles as well as on a sampling of news articles of random quality. 2. Related Work Research on single-document, extractive summarization has been conducted since the 1950s (Luhn, 1958). Traditionally, extractive single document summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; a"
L16-1493,E09-1089,0,0.0268594,"ard MEAD, we implemented a variation in which the first sentence in the document is required to be in the summary (S1 + MEAD). Divrank (Mei et al., 2010) is based on a reinforced random walk over a lexical similarity graph. This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way. Lexrank (Erkan and Radev, 2004) is based on eigenvector similarity over a lexical similarity graph. The nodes of the graph correspond to input sentences and the edges to weighted cosine similarity. The most central sentences are selected for the summary. MaxCov (Takamura and Okumura, 2009) applies approximation algorithms for the budgeted maximum coverage problem (Khuller et al., 1999) to document summarization. It assumes the existence of a vocabulary in which each word is associated with some positive profit (word score in the summary). Given a collection of subsets of this vocabulary (sentences), each associated with some cost (number of characters), the budgeted maximum coverage problem identifies a summary whose total cost remains within the budget and whose union maximizes the summary score. Personalized PageRank (PPR) (Agirre and Soroa, 2009) is a variation of the pagera"
L16-1493,J08-1001,0,\N,Missing
N10-1045,P06-1114,0,0.0515693,"ed as a generic framework for modeling language variability. Given two texts T and H, the task consists in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, w"
N10-1045,P07-2045,1,0.00784892,"Missing"
N10-1045,P09-2073,1,0.304023,"tance Textual Entailment Suite). This system is an open source software package based on edit distance algorithms, which computes the T-H distance as the cost of the edit operations (i.e. insertion, deletion and substitution) that are necessary to transform T into H. By defining the edit distance algorithm and a cost scheme (i.e. which defines the costs of each edit operation), this package is able to learn a distance model over a set of training pairs, which is used to decide if an entailment relation holds over each test pair. In order to obtain a monolingual TE model, we trained and tuned (Mehdad, 2009) our model on the RTE3 test set, to reduce the overfitting bias, since 4 http://translate.google.com http://www.statmt.org/moses/ 6 http://www.statmt.org/europarl/ 7 http://www.ldc.upenn.edu 8 http://edits.fbk.eu/ our original data was created over the RTE3 development set. Moreover, we used a set of lexical entailment rules extracted from Wikipedia and WordNet, as described in (Mehdad et al., 2009). To begin with, we used this model to classify the created cross-lingual entailment corpus in three different settings: 1) hypotheses translated by Google, 2) hypotheses translated by Moses (1st be"
N10-1045,W09-0404,0,0.0583924,"Missing"
N10-1045,E06-1052,0,0.0313124,"sts in deciding if the meaning of H can be inferred from the meaning of T. So far, TE has been only applied in a monolingual setting, where both texts are assumed to be written in the same language. In this work, we propose and investigate a cross-lingual extension of TE, where we assume that T and H are written in different languages. The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al., 2006), information extraction (Romano et al., 2006), and document summarization (Lloret et al., 2008). To the best of our knowledge, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. As a matter of fact, despite the great deal of attention that TE has received in recent years (also witnessed by five editions of the Recognizing Textual Entailment Challenge1 ), interest for cross-lingual extensions has not been in the mainstream of TE research, which until now has been mainly focused on the English language. Nevertheless, the strong inter"
N10-1146,N09-1003,0,0.0403911,"Missing"
N10-1146,D09-1110,0,0.0118203,"is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syn"
N10-1146,W05-0601,1,0.800255,"syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the"
N10-1146,H05-1079,0,0.00742673,"ee kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatica"
N10-1146,W07-1402,0,0.0153556,"wing example: T6 ⇒?H6 T6 “In 1956 JFK met Marilyn Monroe” H6 “Marilyn Monroe died in 1956” The problem is that the pairs hT2 , H2 i and hT4 , H4 i share more meaningful features than the rule ρ5 , which should make the difference with respect to the relation between the pairs hT2 , H2 i and hT6 , H6 i. Indeed, the word “kill” is more semantically related to “murdered” than to “meet”. Using this information, it is possible to derive more effective rules from training examples. There are several solutions for taking this information into account, e.g. by using FrameNet semantics (e.g., like in (Burchardt et al., 2007)), it is possible to encode a lexical-syntactic rule using the KILLING and the DEATH frames, i.e.: ρ3 = X killed Y → Y died ρ7 = along with such rules, the temporal information should be taken into consideration. Given the importance of lexical-syntactic rules in RTE, many methods have been proposed for their extraction from large corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)). Unfortunately, these unsupervised methods in general produce rules that can hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Mos"
N10-1146,A00-2018,0,0.0694397,"K, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We"
N10-1146,P02-1034,0,0.0260782,"important fragment from a semantically similar sentence, which cannot be matched by STK but it is matched by SSTK. account in the model definition. Since tree kernels have been shown to be very effective for exploiting syntactic information in natural language tasks, a promising idea is to merge together the two different approaches, i.e. tree kernels and semantic similarities. 4.1 Syntactic Tree Kernel (STK) Tree kernels compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. The standard definition of the STK, given in (Collins and Duffy, 2002), allows for any set of nodes linked by one or more entire production rules to be valid substructures. The formal characterization is given in (Collins and Duffy, 2002) and is reported hereafter: Let F = {f1 , f2 , . . . , f|F |} be the set of tree fragments and χi (n) be an indicator function, equal to 1 if the target fi is rooted at node n and equal to 0 otherwise. A tree kernel function over P T1 and T2 is defined as T K(T1 , T2 ) = P n1 ∈NT1 n2 ∈NT2 ∆(n1 , n2 ), where NT1 and NT2 are the sets of nodes in T1 and T2 , respectively and P|F | ∆(n1 , n2 ) = i=1 χi (n1 )χi (n2 ). ∆ function coun"
N10-1146,E09-1025,0,0.0237506,"describes lexical similarity approaches, which can serve the generalization purpose. Section 4 describes how to integrate lexical similarity in syntactic structures using syntactic/semantic tree kernels (SSTK) whereas Section 5 shows how to use SSTK in a kernel-based RTE system. Section 6 describes the experiments and results. Section 7 discusses the efficiency and accuracy of our system compared with other RTE systems. Finally, we draw the conclusions in Section 8. 2 Related work Lexical-syntactic rules are largely used in textual entailment recognition systems (e.g., (Bar-Haim et al., 2007; Dinu and Wang, 2009)) as they conveniently encode world knowledge into linguistic structures. For example, to decide whether the simple sentences are in the entailment relation: T2 ⇒?H2 T2 “In 1980 Chapman killed Lennon.” H2 “John Lennon died in 1980.” we need a lexical-syntactic rule such as: from examples in terms of complex relational features. This approach can easily miss some useful information and rules. For example, given the pair hT2 , H2 i, to derive the entailment value of the following case: T4 ⇒?H4 T4 “In 1963 Lee Harvey Oswald murdered JFK” H4 “JFK died in 1963” we can only rely on this relatively i"
N10-1146,W07-1401,0,0.0258766,"SSTK) Kernels. The latter according to different similarities distributional vs. Wordnet-based approaches. Second, we derive qualitative and quantitative properties, which justify the selection of one with respect to the other. For this purpose, we tested four different version of SSTK, i.e. using Path, WUP, BNC and WIKI lexical similarities on three different RTE datasets. These correspond to the three different challenges in which the development set was provided. 6.1 Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is als"
N10-1146,H05-1049,0,0.0076299,"w that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more ac"
N10-1146,O97-1002,0,0.0476717,"Experimental Setup We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5, along with the standard split between training and test sets. We did not use RTE1 as it was differently built from the others and RTE4 as it does not contain the development set. We used the following publicly available tools: the Charniak Parser (Charniak, 2000) for parsing sentences and SVM-light-TK (Moschitti, 2006; Joachims, 1999), in which we coded our new kernels for RTE. Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al., 2004) to measure the similarity between T and H. This similarity is also used to define the texthypothesis word overlap kernel (WOK). The distributional semantics is captured by means of LSA: we used the java Latent Semantic Indexing (jLSI) tool (Giuliano, 2007). In particular, we precomputed the word-pair matrices for RTE2, RTE3, and RTE5. We built different LSA matrices from the British National Corpus (BNC) and Wikipedia (Wiki). The British National Corpus (BNC) is a balanced synchronic text corpus containing 100 million words with mor"
N10-1146,E06-1015,1,0.639162,"utomatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
N10-1146,N04-3012,0,0.553662,"n hardly be used: noise and coverage are the most critical issues. Supervised approaches were experimented in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009), where lexical-syntactic rules were derived 1021 died))) DEAT H( KILLIN G(Killer : X , → P rotagonist : Y ) V ictim : Y ) However, to use this model, specific rules and a semantic role labeler on the specific corpora are needed. 3 Lexical similarities Previous research in computational linguistics has produced many effective lexical similarity measures based on many different resources or corpora. For example, WordNet similarities (Pedersen et al., 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions, e.g. (Basili et al., 2006; Basili et al., 2005; Bloehdorn et al., 2006). In this section we present the main component of our new kernel, i.e. a lexical similarity derived from different resources. This is used inside the syntactic/semantic tree kernel defined in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b) to enhance the basic tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan a"
N10-1146,W07-1418,0,0.0285501,"xploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain"
N10-1146,C08-1107,0,0.0297184,"oded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic rules can be automatically extracted from plain corpora (e.g., (Lin and Pantel, 2001; Szpektor and Dagan, 2008)) but the quality (also in terms of little noise) and the coverage is low. In contrast, rules written at the semantic level are more accurate but their automatic design is difficult and so they are typically hand-coded for the specific phenomena. In this paper, we propose models for effectively using syntactic and semantic information in RTE, without requiring either large automatic rule acquisition or hand-coding. These models exploit lexical similarities to generalize lexical-syntactic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form o"
N10-1146,H05-1047,0,0.0318507,"s realized by means of tree kernels, which can exploit lexical relatedness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning. 1 Introduction Recognizing Textual Entailment (RTE) is rather challenging as effectively modeling syntactic and semantic for this task is difficult. Early deep semantic models (e.g., (Norvig, 1987)) as well as more recent ones (e.g., (Tatu and Moldovan, 2005; Bos and Markert, 2005; Roth and Sammons, 2007)) rely on specific world knowledge encoded in rules for drawing decisions. Shallower models exploit matching methods between syntactic/semantic graphs of texts and hypotheses (Haghighi et al., 2005). The matching step is carried out after the application of some lexical-syntactic rules that are used to transform the text T or the hypothesis H (Bar-Haim et al., 2009) at surface form level. For all these methods, the effective use of syntactic and semantic information depends on the coverage and the quality of the specific rules. Lexical-syntactic"
N10-1146,P94-1019,0,0.146531,"tree kernel functions. 3.1 WordNet Similarities WordNet similarities have been heavily used in previous NLP work (Chan and Ng, 2005; Agirre et al., 2009). All WordNet similarities apply to pairs of synonymy sets (synsets) and return a value indicating their semantic relatedness. For example, the following measures, that we use in this study, are based on path lengths between concepts in the Wordnet Hierarchy: Path the measure is equal to the inverse of the shortest path length (path length) between two synsets c1 and c2 in WordNet SimP ath = 1 path length(c1 , c2 ) (1) WUP the Wu and Palmer (Wu and Palmer, 1994) similarity metric is based on the depth of two given synsets c1 and c2 in the WordNet taxonomy, and the depth of their least common subsumer (lcs). These are combined into a similarity score: SimW U P = 2 × depth(lcs) depth(c1 ) + depth(c2 ) (2) Wordnet similarity measures on synsets can be extended to similarity measures between words as follows: κS (w1 , w2 ) = max(c1 ,c2)∈C1 ×C2 SimS (c1 , c2 ) (3) where S is Path or WUP and Ci is the set of the synsets related to the word wi . 3.2 Distributional Semantic Similarity Latent Semantic Analysis (LSA) is one of the corpus-based measure of distr"
N10-1146,C00-2137,0,0.0288195,"m with the standard accuracy and then we determine the statistical significance by using the model RTE2 +WOK RTE3 +WOK RTE5 +WOK STK 61.5 52.62 66.38 53.25 62.0 54.33 SSTK 61.12 52.75 66.5 54.5 62.0 57.33 maxSTK 63.88 61.25 66.5 62.25 64.83 63.33 maxSSTK 64.12 59.38 67.0 64.38 64.83 62.67 STK+maxSTK 63.12 61.25 66.88 63.12 65.5 61.83 SSTK+maxSSTK 63.50 58.75 67.25 63.62 66.5 62.67 ∅ 60.62 66.75 60.67 - Table 2: Comparing different lexico-syntactic kernels with Wiki-based semantic kernels of the j parameters, i.e. j = 1, which was not selected by our limited parameter validation. described in (Yeh, 2000) and implemented in (Pad´o, 2006). 6.2 Distributional vs. WordNet-based Semantics The first experiment compares the basic kernel, i.e. WOK+STK+maxSTK, with the new semantic kernel, i.e. WOK+SSTK+maxSSTK, where SSTK and maxSSTK encode four different kinds of similarities, BNC, WIKI, WUP and Path. The aim is twofold: understanding if semantic similarities can be effectively used to derive generalized lexicosyntactic rules and to determine the best similarity model. Table 1 shows the results according to No Semantics, Wiki, BNC, Path and WUP. The three pairs of rows represent the results over the"
N10-1146,P06-1051,1,0.887883,"actic rules automatically derived by supervised learning methods. In more detail, syntax is encoded in the form of parse trees whereas similarities are defined by means of WordNet simlilarity measures or Latent Semantic Analysis (LSA) applied to Wikipedia or to the British National Corpus (BNC). The joint syntactic/semantic model is realized by means of novel tree kernels, which can match subtrees whose leaves are lexically similar (so not just identical). To assess the benefit of our approach, we carried out comparative experiments with previous work: especially with the method described in (Zanzotto and Moschitti, 2006; Zanzotto et al., 2009). This constitutes our strong baseline as, although it can only exploit lexical-syntactic rules, it has achieved top accuracy in all RTE challenges. The results, across different RTE challenges, show that our approach constantly and significantly improves the 1020 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020–1028, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics baseline model. Moreover, our approach does not require any adaptation or tuning and uses a computation for the"
N13-1018,P12-3014,0,0.101141,"e chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa"
N13-1018,N06-1046,0,0.0173089,"imal”, or “giggle” and “chuckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (c"
N13-1018,P11-1062,0,0.568264,"acted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa (the “unknown” cases i"
N13-1018,P07-1069,0,0.0201103,"extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Moreover, to generate a lab"
N13-1018,W00-1425,0,0.0099463,"ckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (cwi1 ) and share the same P"
N13-1018,W04-3205,0,0.0376809,"for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind u"
N13-1018,W10-1751,0,0.0329095,"t the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is tha"
N13-1018,D10-1038,1,0.842685,"tional datasets. Our interest in dealing with conversational texts derives from two reasons. First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a cor184 pus of 1,024 sentences. There are no publicly available blog corpora annotated with topics. For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with to"
N13-1018,C10-1065,0,0.0588898,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,S10-1004,0,0.127914,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,W07-2324,0,0.0140014,"ng. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Table 1: Topic labeling example. Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Me"
N13-1018,P11-1154,0,0.028743,"007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Mor"
N13-1018,N10-1045,1,0.855593,"keywords. We add those chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in"
N13-1018,P11-1134,1,0.874875,"we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a be"
N13-1018,P12-2024,1,0.873874,"tiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much easier than recognizing entailment between paragraphs or complex long sentences. 2.2.3 Graph edge labeling We set the edge labeling problem as a two-way classification task. Two-way classification casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YESNO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. Moreover, since our training examples are labeled with binary judgments, we are not able to train a three-way classifier. 2.2.4 Identification and selection Assigning all entailment relations between the extracted phrase pairs, we are aiming at identifying"
N13-1018,D11-1062,1,0.854152,"the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3 4 http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ http://www.cs.york.ac.uk/semeval-2013/task8/ dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpose which is closer to our work. We sort the RTE6 and RTE7 dataset pairs based on t"
N13-1018,S12-1053,1,0.898214,"Missing"
N13-1018,C00-2137,0,0.0330731,"ggregation Extraction+Entailment+ Aggregation 12.2 15.1 15.6 18.5 30.8 35.5 33.3 37.6 17.9 20.4 38.7 41.6 ing R-f1 and Sem-R-f1 metrics suggests the need for more flexible automatic evaluation methods for this task. Moreover, although the same trend of improvement is observed in blog and email corpora, the differences between their performance suggest the investigation of specialized methods for various conversational modalities. Table 4: Results for candidate topic labels on blog and email corpora. 8 The statistical significance tests was calculated by approximate randomization described in (Yeh, 2000). 186 0.8 Sem-R-f1 icantly8 in both datasets. On the blog corpus, our key phrase extraction method (Extraction-BL) fails to beat the other baselines (Lead-BL and Freq-BL) in majority of cases (except R-f1 for Lead-BL). However, in the email dataset, it improves the performance over both baselines in both evaluation metrics. This might be due to the shorter topic clusters (in terms of number of sentences) in email corpus which causes a smaller number of phrases to be extracted. We also observe the effectiveness of the aggregation phase. In all cases, there is a significant improvement (p &lt; 0.05"
N13-1018,W04-3252,0,\N,Missing
N13-1018,N10-1146,1,\N,Missing
N13-1018,W07-1401,0,\N,Missing
N16-1008,D07-1069,1,0.692207,"aselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. ("
N16-1008,C08-1040,1,0.865783,"icly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the"
N16-1008,D13-1068,0,0.0170224,"ls for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first"
N16-1008,W04-1013,0,0.0596697,"tent factor models have had huge success in recommender systems. These latent factors, often in the form of low-rank embeddings, capture not only explicit information but also implicit context from the input data. In this work, we propose a novel matrix factorization framework to “recommend” key sentences to a timeline. Figure 2 shows an overview of the framework. More specifically, we formulate this task as a matrix completion problem. Given a news corpus, we assume that there are m total sentences, which are the rows in the matrix. The first column is the metric section, where we use ROUGE (Lin, 2004) as the metric to pre-compute a sentence importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we r"
N16-1008,S15-2138,0,0.0165672,"e first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first to consider multimodal timeline summarization, but they focus on visualization, and do not make use of images"
N16-1008,P14-1087,0,0.0300454,"example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate feature"
N16-1008,nivre-etal-2006-maltparser,0,0.156814,"e sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we keep the 16 convolutional layers and m"
N16-1008,radev-etal-2004-mead,1,0.359708,"date sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches. 1 To distill key insights from news reports, prior work in summarization often relies on feature engineering, and uses clustering techniques (Radev et al., 2004b) to select important events to be included in the final summary. While this approach is unsupervised, the process of feature engineering is always expensive, and the number of clusters is not easy to estimate. To present a complete summary, researchers from the natural language processing (NLP) community often solely rely on the textual information, while studies in the computer vision (CV) community rely solely on the image and video information. However, even though news images are abundantly available together with news stories, approaches that jointly learn textual and visual representat"
N16-1008,N03-1033,0,0.0186037,"e importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we ke"
N16-1008,N15-1112,0,0.147479,"rmance. Our main contributions are three-fold: • We propose a novel matrix factorization approach for extractive summarization, leveraging the success of collaborative filtering; • We are among the first to consider representation learning of a joint embedding for text and images in timeline summarization; • Our model significantly outperforms various competitive baselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popul"
N16-1008,D11-1040,0,0.599874,"Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable mo"
N16-1008,W04-3252,0,\N,Missing
negri-etal-2012-chinese,P02-1040,0,\N,Missing
negri-etal-2012-chinese,D11-1062,1,\N,Missing
negri-etal-2012-chinese,P01-1008,0,\N,Missing
negri-etal-2012-chinese,P08-1077,0,\N,Missing
negri-etal-2012-chinese,P11-1134,1,\N,Missing
negri-etal-2012-chinese,P08-1004,0,\N,Missing
negri-etal-2012-chinese,N06-1058,0,\N,Missing
negri-etal-2012-chinese,P05-1074,0,\N,Missing
negri-etal-2012-chinese,N06-1003,0,\N,Missing
negri-etal-2012-chinese,N10-1045,1,\N,Missing
negri-etal-2012-chinese,P11-1020,0,\N,Missing
negri-etal-2012-chinese,P02-1006,0,\N,Missing
negri-etal-2012-chinese,W10-0734,1,\N,Missing
negri-etal-2012-chinese,W04-3206,0,\N,Missing
P11-1134,P98-1013,0,0.0179973,"g relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the semantic overlap between texts a"
P11-1134,P05-1074,0,0.237919,"een texts and hypotheses. Wikipedia is often used to extract probabilistic entailment rules based word similarity/relatedness scores. Despite the consensus on the usefulness of lexical knowledge for textual inference, determining the actual impact of these resources is not straightforward, as they always represent one component in complex architectures that may use them in different ways. As emerges from the ablation tests reported in (Bentivogli et al., 2010), even the most common resources proved to have a positive impact on some systems and a negative impact on others. Some previous works (Bannard and Callison-Burch, 2005; Zhao et al., 2009; Kouylekov et al., 2009) indicate, as main limitations of the mentioned resources, their limited coverage, their low precision, and the fact that they are mostly suitable to capture relations mainly between single words. Addressing CLTE we have to face additional and more problematic issues related to: i) the stronger need of lexical knowledge, and ii) the limited availability of multilingual lexical resources. As regards the first issue, it’s worth noting that in the monolingual scenario simple “bag of words” (or “bag of ngrams”) approaches are per se sufficient to achieve"
P11-1134,W04-3205,0,0.0289879,"chains can provide entailmentpreserving relations between concepts, indicating that a word in the hypothesis can be replaced by a word from the text. Paths between concepts and glosses can be used to calculate similarity/relatedness scores between single words, that contribute to the computation of the overall similarity between the text and the hypothesis. Besides WordNet, the RTE literature documents the use of a variety of lexical information sources (Bentivogli et al., 2010; Dagan et al., 2009). These include, just to mention the most popular ones, DIRT (Lin and Pantel, 2001), VerbOcean (Chklovski and Pantel, 2004), FrameNet (Baker et al., 1998), and Wikipedia (Mehdad et al., 2010; Kouylekov et al., 2009). DIRT is a collection of statistically learned inference rules, that is often integrated as a source of lexical paraphrases and entailment rules. VerbOcean is a graph of fine-grained semantic relations between verbs, which are frequently used as a source of precise entailment rules between predicates. FrameNet is a knowledge-base of frames describing prototypical situations, and the role of the participants they involve. It can be used as an alternative source of entailment rules, or to determine the s"
P11-1134,N10-1031,0,0.0124482,"hrase table, and extract their equivalents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using t"
P11-1134,E09-1025,0,0.0260215,"alents in l1 ; 3. use the paraphrase table in l1 to find paraphrases of the extracted fragments in l1 ; phrase tables range from 76 to 48 million entries, with an average of 3.9 words per phrase. 4. map such paraphrases to phrases in T. Paraphrase tables (PPHT) contain pairs of corresponding phrases in the same language, possibly associated with probabilities. They proved to be useful in a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined."
P11-1134,N03-1017,0,0.0166869,"es. In particular, “YES” and “NO” judgements are assigned considering the proportion of words in the hypothesis that are found also in the text. This way to approximate entailment reflects the intuition that, as a directional relation between the text and the hypothesis, the full content of H has to be found in T. 3.1 Extracting Phrase and Paraphrase Tables Phrase tables (PHT) contain pairs of corresponding phrases in two languages, together with association probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses"
P11-1134,P07-2045,1,0.00976982,"re several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities of 0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting 1 3.2 In order to maximize the usage of lexical knowledge, our entailment decision criterion is based on similarity s"
P11-1134,kouylekov-etal-2010-mining,1,0.859106,"Missing"
P11-1134,N10-1146,1,0.754403,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,N10-1045,1,0.567905,"ages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge. 1 Introduction Cross-lingual Textual Entailment (CLTE) has been proposed by (Mehdad et al., 2010) as an extension of Textual Entailment (Dagan and Glickman, 2004) that consists in deciding, given two texts T and H in different languages, if the meaning of H can be inferred from the meaning of T. The task is inherently difficult, as it adds issues related to the multilingual dimension to the complexity of semantic inference at the textual level. For instance, the reliance of current monolingual TE systems on lexical resources 1336 Marcello Federico FBK - irst Povo (Trento), Italy federico@fbk.eu (e.g. WordNet, VerbOcean, FrameNet) and deep processing components (e.g. syntactic and semantic"
P11-1134,C02-1167,0,0.0845279,"Missing"
P11-1134,W10-0734,1,0.473862,"s obtained at each n-gram level, and optimize their relative weights, we trained a Support Vector Machine classifier, SVMlight (Joachims, 1999), using each score as a feature. Scoren = 4 Experiments on CLTE To address the first two questions outlined in Section 1, we experimented with the phrase matching method previously described, contrasting the effectiveness of lexical information extracted from parallel corpora with the knowledge provided by other resources used in the same way. 4.1 Dataset the CrowdFlower3 channel to Amazon Mechanical Turk4 (MTurk), adopting the methodology proposed by (Negri and Mehdad, 2010). The method relies on translation-validation cycles, defined as separate jobs routed to MTurk’s workforce. Translation jobs return one Spanish version for each hypothesis. Validation jobs ask multiple workers to check the correctness of each translation using the original English sentence as reference. At each cycle, the translated hypothesis accepted by the majority of trustful validators5 are stored in the CLTE corpus, while wrong translations are sent back to workers in a new translation job. Although the quality of the results is enhanced by the possibility to automatically weed out untru"
P11-1134,J03-1002,0,0.00279614,"ciation probabilities. They are widely used in MT as a way to figure out how to translate input in one language into output in another language (Koehn et al., 2003). There are several methods to build phrase tables. The one adopted in this work consists in learning phrase alignments from a word-aligned bilingual corpus. In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT101 . We run TreeTagger (Schmid, 1994) for tokenization, and used the Giza++ (Och and Ney, 2003) to align the tokenized corpora at the word level. Subsequently, we extracted the bilingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Since the resulting phrase table was very large, we eliminated all the entries with identical content in the two languages, and the ones containing phrases longer than 5 words in one of the two sides. In addition, in order to experiment with different phrase tables providing different degrees of coverage and precision, we extracted 7 phrase tables by pruning the initial one on the direct phrase translation probabilities"
P11-1134,H05-1047,0,0.00878721,"available knowledge sources. Sections 3 and 4 address the first three questions, giving motivations for the use of bilingual parallel corpora in CLTE, and showing the results of our experiments. Section 5 addresses the last question, reporting on our experiments with paraphrase tables extracted from phrase tables on the monolingual RTE datasets. Section 6 concludes the paper, and outlines the directions of our future research. 2 Lexical resources for TE and CLTE All current approaches to monolingual TE, either syntactically oriented (Rus et al., 2005), or applying logical inference (Tatu and Moldovan, 2005), or adopting transformation-based techniques (Kouleykov and Magnini, 2005; Bar-Haim et al., 2008), incorporate different types of lexical knowledge to support textual inference. Such information ranges from i) lexical paraphrases (textual equivalences between terms) to ii) lexical relations preserving entailment between words, and iii) wordlevel similarity/relatedness scores. WordNet, the most widely used resource in TE, provides all the three types of information. Synonymy relations can be used to extract lexical paraphrases indicating that words from the text and the hypothesis entail each"
P11-1134,W09-0441,0,0.00557519,"n a number of NLP applications such as natural language generation (Iordanskaja et al., 1991), multidocument summarization (McKeown et al., 2002), automatic evaluation of MT (Denkowski and Lavie, 2010), and TE (Dinu and Wang, 2009). One of the proposed methods to extract paraphrases relies on a pivot-based approach using phrase alignments in a bilingual parallel corpus (Bannard and Callison-Burch, 2005). With this method, all the different phrases in one language that are aligned with the same phrase in the other language are extracted as paraphrases. After the extraction, pruning techniques (Snover et al., 2009) can be applied to increase the precision of the extracted paraphrases. In our work we used available2 paraphrase databases for English and Spanish which have been extracted using the method previously outlined. Moreover, in order to experiment with different paraphrase sets providing different degrees of coverage and precision, we pruned the main paraphrase table based on the probabilities, associated to its entries, of 0.1, 0.2 and 0.3. The number of phrase pairs extracted varies from 6 million to about 80000, with an average of 3.2 words per phrase. With the second method, phrasal matches b"
P11-1134,D09-1082,0,0.0358948,"Missing"
P11-1134,P01-1067,0,0.0367702,"scenario. Contrasting results with those obtained with the most widely used resources in TE, we demonstrated the effectiveness of paraphrase tables as a mean to overcome the bias towards single words featured by the existing resources. Our future work will address both the extraction of lexical information from bilingual parallel corpora, and its use for TE and CLTE. On one side, we plan to explore alternative ways to build phrase and paraphrase tables. One possible direction is to consider linguistically motivated approaches, such as the extraction of syntactic phrase tables as proposed by (Yamada and Knight, 2001). Another interesting direction is to investigate the potential of paraphrase patterns (i.e. patterns including partof-speech slots), extracted from bilingual parallel corpora with the method proposed by (Zhao et al., 2009). On the other side we will investigate more sophisticated methods to exploit the acquired lexical knowledge. As a first step, the probability scores assigned to phrasal entries will be considered to perform weighted phrase matching as an improved criterion to approximate entailment. Acknowledgments This work has been partially supported by the ECfunded project CoSyne (FP7-I"
P11-1134,C98-1013,0,\N,Missing
P12-2024,carreras-etal-2004-freeling,0,0.169167,"Missing"
P12-2024,de-marneffe-etal-2006-generating,0,0.0595464,"Missing"
P12-2024,P05-1045,0,0.0381264,"Missing"
P12-2024,P07-2045,1,0.00822022,"s, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR features aim to reduce the amount of wrong matches often occurring with bag-of-words methods (both at the lexical level and with recall-oriented SPTs). For instance, the contradiction b"
P12-2024,P10-4008,1,0.231125,"Missing"
P12-2024,W11-2404,1,0.892041,"Missing"
P12-2024,N10-1045,1,0.424311,"pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deci"
P12-2024,P11-1134,1,0.505816,"hine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach, however, still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, we propose a novel approach that improves the state-of-the-art in CLTE. 2 CLTE-based content synchronization CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment which consists of deciding, given a text T and an hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The adoption of entai"
P12-2024,W12-3122,1,0.51127,"et such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the i"
P12-2024,W10-0734,1,0.400732,"e informative with respect to the content of the other page. In this paper we set such problem as an application-oriented, crosslingual variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Along this direction, we make two main contributions: (a) Experiments with multi-directional crosslingual textual entailment. So far, cross-lingual textual entailment (CLTE) has been only applied to: i) available TE datasets (uni-directional relations between monolingual pairs) transformed into their cross-lingual counterpart by translating the hypotheses into other languages (Negri and Mehdad, 2010), and ii) machine translation (MT) evaluation datasets (Mehdad et al., 2012). Instead, we experiment with the only corpus representative of the multilingual content synchronization scenario, and the richer inventory of phenomena arising from it (multi-directional entailment relations). (b) Improvement of current CLTE methods. The CLTE methods proposed so far adopt either a “pivoting approach” based on the translation of the two input texts into the same language (Mehdad et al., 2010), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual"
P12-2024,D11-1062,1,0.47246,"ed words can be either the same, or semantically equivalent terms in the two languages (e.g. according to a bilingual dictionary). Given the dependency tree representations of T and H, for each grammatical relation (r) we calculate a DR matching score as the number of matching occurrences of r in T and H, divided by the number of occurrences of r in H. Separate DR matching scores are calculated for each relation r appearing both in T and H. 4 Experiments and results 4.1 Content synchronization scenario In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, or “Backward” entailment judgements. Although highly relevant for the content synchronization task, “Contradiction” and “Unknown” cases (i.e. “NO” entailment in both directions) are not present in the annotation. However, this is the only available dataset suitable to gather insights about the viability of our approach to multi-directional CLTE recognition.2 We chose the ENG-GER portion of the dataset since for such"
P12-2024,S12-1053,1,0.350796,"taset since for such language pair MT systems performance is often lower, making the adoption of simpler solutions based on pivoting more vulnerable. To build the English-German phrase tables we combined the Europarl, News Commentary and “denews”3 parallel corpora. After tokenization, Giza++ and Moses were respectively used to align the corpora and extract a lexical phrase table (PT). Similarly, the semantic phrase table (SPT) has been ex2 Recently, a new dataset including “Unknown” pairs has been used in the “Cross-Lingual Textual Entailment for Content Synchronization” task at SemEval-2012 (Negri et al., 2012). 3 http://homepages.inf.ed.ac.uk/pkoehn/ 122 tracted from the same corpora annotated with the Stanford NE tagger (Faruqui and Pad´o, 2010; Finkel et al., 2005). Dependency relations (DR) have been extracted running the Stanford parser (Rafferty and Manning, 2008; De Marneffe et al., 2006). The dictionary created during the alignment of the parallel corpora provided the lexical knowledge to perform matches when the connected words are different, but semantically equivalent in the two languages. To combine and weight features at different levels we used SVMlight (Joachims, 1999) with default pa"
P12-2024,P00-1056,0,0.0178874,"place of “out of vocabulary” terms (e.g. unseen person names) is an effective way to improve CLTE performance, even at the cost of some loss in precision. Like lexical phrase tables, SPTs are extracted from parallel corpora. As a first step we annotate the parallel corpora with named-entity taggers for the source and target languages, replacing named entities with general semantic labels chosen from a coarse-grained taxonomy (person, location, organization, date and numeric expression). Then, we combine the sequences of unique labels into one single token of the same label, and we run Giza++ (Och and Ney, 2000) to align the resulting semantically augmented corpora. Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al., 2007). For the matching phase, we first annotate T and H in the same way we labeled our parallel corpora. Then, for each n-gram order (n=1 to 5) we use the SPT to calculate a matching score as the number of n-grams in H that match with phrases in T divided by the number of n-grams in H.1 Dependency Relation (DR) matching targets the increase of CLTE precision. Adding syntactic constraints to the matching process, DR feat"
P12-2024,W08-1006,0,0.0178413,"Missing"
P13-1048,J00-3005,0,0.763712,"and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast"
P13-1048,P12-1007,0,0.355874,"istinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng a"
P13-1048,J91-1002,0,0.302844,"the inference much faster (i.e., complexity of O(M 2 )). Breaking the chain structure also allows us to balance the data for training (equal number instances with S=1 and S=0), which dramatically reduces the learning time of the model. We apply our model to all possible adjacent units at all levels for the multi-sentential case, and 490 Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT"
P13-1048,P07-1062,0,0.741307,", which in turn are also subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not"
P13-1048,N03-1030,0,0.527984,"me-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. frequent relations on a development set containing 20 randomly selected documents from RST-DT. Notice that relations Attribution and Same-Unit are more frequent than Joint in intra-sentential case, whereas Joint is more frequent than the other two in multi-sentential case. On the other hand, different kinds of features are applicable and informative for intra-sentential vs. multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multi-sentential case. Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). First, the intrasentential parser produces one or more discourse sub-trees for each sentence. Then, the multisentential parser generates a full DT for the document from these sub-t"
P13-1048,D12-1083,1,0.239984,"subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequen"
P13-1048,H05-1033,0,0.0311665,"rsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by int"
P13-1048,C04-1007,0,0.820285,"ndow covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential parser Figure 3: Discourse parsing framework. Attribution Same-Unit Contrast Explanation Figure 2: Distributions"
P13-1048,W10-4327,0,0.250002,"rasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaborat"
P13-1048,W04-0213,0,0.0487185,"Missing"
P13-1048,N09-1064,0,0.688998,"en the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. For example, the DT for the second sentence in Figure 1 can be represented as ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. They evaluate their approach on the RST-DT corpus (Carlson et al., 2002) of news articles. On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., t"
P13-1048,P02-1047,0,0.0642381,"h these cases, builds sentence-level sub-trees by applying the intra-sentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential pars"
P13-1048,C04-1048,0,\N,Missing
P14-1115,P12-3014,0,0.0213774,"re scored, the top scored utterances are selected to be sent to the next step. We estimate the percentage of the retrieved utterances based on the development set. 1222 2.2 Redundancy Removal Utterances selected in previous step often include redundant information, which is semantically equivalent but may vary in lexical choices. By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Berant et al., 2011; Adler et al., 2012), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to gener"
P14-1115,J05-3002,0,0.0966105,"tical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract gen"
P14-1115,P11-1062,0,0.0217429,"all the utterances are scored, the top scored utterances are selected to be sent to the next step. We estimate the percentage of the retrieved utterances based on the development set. 1222 2.2 Redundancy Removal Utterances selected in previous step often include redundant information, which is semantically equivalent but may vary in lexical choices. By identifying the semantic relations between the sentences, we can discover what information in one sentence is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Berant et al., 2011; Adler et al., 2012), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). Using entailment in this phase is motivated by taking advantage of semantic relations instead of pure statistical methods (e.g., Maximal Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase,"
P14-1115,C10-1037,0,0.12376,"in generated abstract sentences. This task can be viewed as sentence clustering, where each sentence cluster can provide the content for an abstract sentence. We use the K-mean clustering algorithm by cosine similarity as a distance function between sentence vectors composed of tf.idf scores. Also notice that the lexical similarity between sentences in one cluster facilitates both the construction of the word graph and finding the best path in the word graph, as described next. 2.3.2 Word Graph In order to construct a word graph, we adopt the method recently proposed by (Mehdad et al., 2013a; Filippova, 2010) with some optimizations. Below, we show how the word graph is applied to generate the abstract sentences. Let G = (W, L) be a directed graph with the set of nodes W representing words and a set of directed edges L representing the links between words. Given a cluster of related sentences S = {s1 , s2 , ..., sn }, a word graph is constructed by iteratively adding sentences to it. In the first step, the graph represents one sentence plus the start and end symbols. A node is added to the graph for each word in the sentence, and words adjacent are linked with directed edges. When adding a new sen"
P14-1115,C10-1039,0,0.0651639,") and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract generation in three steps, as follows: 2.3.1"
P14-1115,P07-2049,0,0.022691,"ances should carry the essence of the original text; and ii) utterances should be relevant to the query. To fulfill such requirements we define the concepts of signature terms and query terms. 2.1.1 Signature Terms Signature terms are generally indicative of the content of a document or collection of documents. To identify such terms, we can use frequency, word probability, standard statistic tests, information-theoretic measures or log-likelihood ratio. In this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to better results (Gupta et al., 2007). We use a method described in (Lin and Hovy, 2000) in order to identify such terms and their associated weight. Example 2 demonstrates a chat log and associated signature terms. 2.1.2 Query Terms Query terms are indicative of the content in a phrasal query. In order to identify such terms, we first extract all content terms from the query. Then, following previous studies (e.g., (Gonzalo Utterance Scoring To estimate the utterance score, we view both the query terms and the signature terms as the terms that should appear in a human query-based summary. To achieve this, the most relevant (summ"
P14-1115,C00-1072,0,0.458782,"To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, instead of well-formed questions. As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e., fluency) of the sentence into consideration. 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g., (Wang et al., 2013)), our method can be totally unsupervised and does not depend on human annotation. 4) Although different conversational modalities (e.g., email vs. chat vs. meeting) underline domain-specific characteristics, in this work, we take advantage o"
P14-1115,P09-2066,0,0.0197216,"Marginal Relevance) and shown to be more effective (Mehdad et al., 2013a). We follow the same practice as (Mehdad et al., 2013a) to build an entailment graph for all selected sentences to identify relevant sentences and eliminate the redundant (in terms of meaning) and less informative ones. 2.3 Abstract Generation In this phase, our goal is to generate understandable informative abstract sentences that capture the content of the source sentences and represents the information needs defined by queries. There are several ways of generating abstract sentences (e.g. (Barzilay and McKeown, 2005; Liu and Liu, 2009; Ganesan et al., 2010; Murray et al., 2010)); however, most of them rely heavily on the sentence structure. We believe that such approaches are suboptimal, especially in dealing with conversational data, because multiparty written conversations are often poorly structured. Instead, we apply an approach that does not rely on syntax, nor on a standard NLG architecture. Moreover, since dealing with user queries efficiency is an important aspect, we aim for an approach that is also motivated by the speed with which the abstracts are obtained. We perform the task of abstract generation in three st"
P14-1115,N13-1018,1,0.933465,"s and associated human-written query-based summaries for a chat log. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates s"
P14-1115,W13-2117,1,0.874493,"Missing"
P14-1115,W98-0705,0,0.224458,"Missing"
P14-1115,W10-4211,1,0.939486,"mmaries for a chat log. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, inst"
P14-1115,W11-0501,0,0.0224959,"used for evaluating our query-based summarization system. Most available conversational corpora do not contain any human written summaries, or the gold standard human written summaries are generic (Carletta et al., 2005; Joty et al., 2013). In this work, we use available corpora for emails and chats for written conversations, while for spoken conversation, we employ an available corpus in multiparty meeting conversations. Chat: to the best of our knowledge, the only publicly available chat logs with human written summaries can be downloaded from the GNUe Traffic archive (Zhou and Hovy, 2005; Uthus and Aha, 2011; Uthus and Aha, 2013). Each chat log has a human created summary in the form of a digest. Each digest summarizes IRC logs for a period and consists of few summaries over each chat log with a unique title for the associated human written summary. In this way, the title of each summary 1224 can be counted as a phrasal query and the corresponding summary is considered as the querybased abstract of the associated chat log including only the information most relevant to the title. Therefore, we can use the human-written querybased abstract as gold standards and evaluate our system automatically. O"
P14-1115,P13-1137,0,0.0333719,"Missing"
P14-1115,P13-1136,0,0.028782,"stead of well-formed questions. As a by-product of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). 2) We propose a novel ranking strategy to select the best path in the constructed word graph by taking the query content, overall information content and grammaticality (i.e., fluency) of the sentence into consideration. 3) Although most of the current summarization approaches use supervised algorithms as a part of their system (e.g., (Wang et al., 2013)), our method can be totally unsupervised and does not depend on human annotation. 4) Although different conversational modalities (e.g., email vs. chat vs. meeting) underline domain-specific characteristics, in this work, we take advantage of their underlying similarities to generalize away from specific modalities and determine effective method for query-based summarization of multimodal conversations. We evaluate our system over GNUe Traffic archive2 Internet Relay Chat (IRC) logs, AMI meetings corpus (Carletta et al., 2005) and BC3 emails dataset (Ulrich et al., 2008). Automatic evaluation"
P14-1115,C00-2137,0,0.019505,"Missing"
P14-1115,P05-1037,0,0.0294878,"g. rization focus on news or other well-written documents, while research on summarizing multiparty written conversations (e.g., chats, emails) has been limited. This is because traditional NLP approaches developed for formal texts often are not satisfactory when dealing with multiparty written conversations, which are typically in a casual style and do not display a clear syntactic structure with proper grammar and spelling. Even though some works try to address the problem of summarizing multiparty written conversions (e.g., (Mehdad et al., 2013b; Wang and Cardie, 2013; Murray et al., 2010; Zhou and Hovy, 2005; Gillick et al., 2009)), they do so in a generic way (not querybased) and focus on only one conversational domain (e.g., meetings). Moreover, most of the proposed systems for conversation summarization are extractive. To address such limitations, we propose a fully automatic unsupervised abstract generation framework based on phrasal queries for multimodal conversation summarization. Our key contributions in this work are as follows: 1) To the best of our knowledge, our framework is the first abstractive system that generates summaries based on users phrasal queries, instead of well-formed qu"
P14-1115,W04-3252,0,\N,Missing
S12-1053,W05-0909,0,0.0301332,"Missing"
S12-1053,S12-1065,0,0.0529572,"Missing"
S12-1053,S12-1102,0,0.0711613,"glish pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additional training material. SoftCard [pivoting, multi-class] (Jimenez et al., 2012) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisi"
S12-1053,P10-4008,1,0.237359,"classification using SVMs. HDU [hybrid, compositional] (W¨aschle and Fendrich, 2012) uses a combination of binary classifiers for each entailment direction. The classifiers use both monolingual alignment features based on METEOR (Banerjee and Lavie, 2005) alignments (translations obtained from Google Translate), and cross-lingual alignment features based on GIZA++ (Och and Ney, 2000) (word alignments learned on Europarl). ICT [pivoting, compositional] (Meng et al., 2012) adopts a pivoting method (using Google Translate and an in-house hierarchical MT system), and the open source EDITS system (Kouylekov and Negri, 2010) to calculate similarity scores between monolingual English pairs. Separate unidirectional entailment judgments obtained from binary classifier are combined to return one of the four valid CLTE judgments. 3 http://translate.google.com/ http://extensions.services.openoffice. org/en/taxonomy/term/233 4 404 5 6 http://www.freedict.com/ http://www.wordreference.com/ SP-EN System name BUAP spa-eng run2 celi spa-eng run2 DirRelCond3 spa-eng run4 FBK spa-eng run3 HDU spa-eng run2 ICT spa-eng run1 JU-CSE-NLP spa-eng run1 Sagan spa-eng run3 SoftCard spa-eng run1 UAlacant spa-eng run1 LATE AVG. P 0,337"
S12-1053,S12-1104,0,0.0962891,"ignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus. More in detail: BUAP [pivoting, compositional] (Vilari˜no et al., 2012) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (Google Translate3 and the OpenOffice Thesaurus4 ). Similarity measures (e.g. Jaccard index) and rules are respectively used to annotate the two resulting sentence pairs with entailment judgments and combine them in a single decision. CELI [cross lingual, compositional & multiclass] (Kouylekov, 2012) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. Word overlap and similarity measures are then used in different approaches to the task. In one run (Run 1), they are used to train a classifier that assigns separate entailment judgments for each direction. Such judgments are finally composed into a single one for each pair. In the other runs, the same features are used for multi-class classification. DirRelCond3 [cross lingual, compositional] (Perini, 2012) uses bilingual dictionaries (Freedict5 and WordReference6 ) to translate conten"
S12-1053,N10-1045,1,0.464787,"erence over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad FBK-irst Trento, Italy mehdad@fbk.eu Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated. The great potential for integrating monolingual TE recognition components into NLP architectures has been reported in several areas, including question answering, information retrieval, information extraction, and document summarization. However, mainly due to the absence of cross"
S12-1053,P11-1134,1,0.480581,"Missing"
S12-1053,S12-1105,1,0.885932,"Missing"
S12-1053,P12-2024,1,0.450645,"Missing"
S12-1053,W12-3122,1,0.873098,"Missing"
S12-1053,S12-1108,0,0.0711563,"Missing"
S12-1053,D11-1062,1,0.47997,"is task, both T1 and T2 are assumed to be true statements. Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task. 3 Dataset description Four CLTE corpora have been created for the following language combinations: Spanish/English (SP-EN), Italian/English (IT-EN), French/English (FR-EN), German/English (DE-EN). The datasets are released in the XML format shown in Figure 1. 3.1 Data collection and annotation The dataset was created following the crowdsourcing methodology proposed in (Negri et al., 2011), which consists of the following steps: 1. First, English sentences were manually extracted from copyright-free sources (Wikipedia and Wikinews). The selected sentences represent one of the elements (T1) of each entailment pair; 2. Next, each T1 was modified through crowdsourcing in various ways in order to obtain a corresponding T2 (e.g. introducing meaning-preserving lexical and syntactic changes, adding and removing portions of text); 3. Each T2 was then paired to the original T1, and the resulting pairs were annotated with one of the four entailment judgments. In order to reduce the corre"
S12-1053,S12-1103,0,0.0191844,"1 0,076 0,201 0,568 0,332 F1 0,235 0,397 0,469 0,644 0,521 0,379 0,247 0,625 0,440 No entailment P R F1 0,344 0,688 0,459 0,339 0,312 0,325 0,367 0,320 0,342 0,540 0,488 0,513 0,390 0,512 0,443 0,315 0,560 0,403 0,405 0,600 0,484 0,521 0,488 0,504 0,403 0,496 0,434 Bidirectional P R F1 0,364 0,288 0,321 0,319 0,288 0,303 0,298 0,312 0,305 0,524 0,520 0,522 0,439 0,552 0,489 0,233 0,080 0,119 0,443 0,344 0,387 0,496 0,504 0,500 0,390 0,361 0,368 Table 4: precision, recall and F1 scores, calculated for each team’s best run for all the language combinations. JU-CSE-NLP [pivoting, compositional] (Neogi et al., 2012) uses Microsoft Bing translator7 to produce monolingual English pairs. Separate lexical mapping scores are calculated (from T1 to T2 and vice-versa) considering different types of information and similarity metrics. Binary entailment de7 http://www.microsofttranslator.com/ 405 cisions are then heuristically combined into single decisions. Sagan [pivoting, multi-class] (Castillo and Cardenas, 2012) adopts a pivoting method using Google Translate, and trains a monolingual system based on a SVM multi-class classifier. A CLTE corpus derived from the RTE-3 dataset is also used as a source of additi"
S12-1053,P00-1056,0,0.151501,"Missing"
S12-1053,S12-1107,0,0.0689288,"Missing"
S12-1053,S12-1106,0,0.0276864,"Missing"
S12-1053,S12-1064,0,0.250345,"Missing"
S12-1092,W05-0909,0,0.141398,"ay with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was"
S12-1092,carreras-etal-2004-freeling,0,0.0242724,"Missing"
S12-1092,P05-1022,0,0.0149797,"cessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three"
S12-1092,P04-1014,0,0.0233595,"sed: overlapping according to the part-of-speech; overlapping according to the chunk type; the accumulated NIST metric (Doddington, 2002) scores over different Figure 1: A summary of the class of features explored. sequences (lemmas, parts-of-speech, base phrase chunks and chunk IOB labels). 2.1.3 Semantic Level At the semantic level we aplored three different types of information, namely: discourse representations, named entities and semantic roles. Hereafter they are respectively referred to as dr, ne, and sr features. The discourse relations are automatically annotated using the C&C Tools (Clark and Curran, 2004). The following metrics using semantic tree representations were proposed by (Gim´enez, 2008). A metric similar to the STM in which semantic trees are used instead of constituency trees; the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between"
S12-1092,I05-5003,0,0.311872,"Missing"
S12-1092,gimenez-marquez-2004-svmtool,0,0.0159207,"Missing"
S12-1092,J09-4007,0,0.0312577,"the available thesauri. In order to increase the coverage we extracted concepts from the YAGO2 semantic knowledge base (Hoffart et al., 2011) derived from Wikipedia, Wordnet (Miller, 1995) and Geonames3 . YAGO2 contains knowledge about 10 million entities and more than 120 million facts about these entities. In order to link the entities in the text to the entities in YAGO2 we have used “The Wiki Machine” (TWM) tool4 . The tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is possible to retrieve the Wordnet synsets related to the entity’s entry in YAGO2 and explore different knowledge-based metrics to compute word similarity between entities. In our experiments we selected three different algorithms to calculate word similarity using YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994), the Leacock-Chodorow (Leacock et al., 1998) and 2 http://hlt.fbk.eu/en/technology/jlsi http://www.geonames.org/ 4 http://the"
S12-1092,kouylekov-etal-2010-mining,1,0.843845,": overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the term-by-document matrix using the jLSI tool2 . Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics we"
S12-1092,J98-1006,0,0.05943,"Missing"
S12-1092,P04-1077,0,0.123706,"shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we he"
S12-1092,W05-0904,0,0.0225841,"a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three different perspectives: overlap of words that hang in the same level or in a deeper level of the dependency tree; overlap between words that hang directly from terminal nodes given a specified partof-speech; and overlap between words that are ruled by non-terminal nodes given a specified grammatical relation (subject, object, relative clause, am"
S12-1092,N10-1146,1,0.850476,"e semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the term-by-document matrix using the jLSI tool2 . Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics we calculate the similar"
S12-1092,N03-2021,0,0.0353569,"tching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Synt"
S12-1092,niessen-etal-2000-evaluation,0,0.0728532,"ty, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part"
S12-1092,P02-1040,0,0.106563,"f items inside the linguistic elements of a certain type shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). Th"
S12-1092,2006.amta-papers.25,0,0.0383817,"ollowing n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1 http://nlp.lsi.upc.edu/asiya/ 625 into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency"
S12-1092,W05-0635,0,0.0326889,"tween discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between the named entities in each sentence according to their type and the matching between the named entities in function of their type. Semantic roles were automatically annotated us626 ing the SwiRL package (Surdeanu and Turmo, 2005). The arguments and adjuncts annotated in each sentence are compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As s"
S12-1092,U06-1019,0,0.047547,"Missing"
S12-1092,P94-1019,0,0.286357,"Missing"
S12-1092,S12-1051,0,\N,Missing
S12-1105,carreras-etal-2004-freeling,0,0.0542349,"Missing"
S12-1105,P07-2045,0,0.00682366,"been identified as exact matches are not considered. 702 M atchn |H(n)| (1) In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 Shared Translation Task.2 We run the TreeTagger (Schmid, 1995) and Snowball stemmer (Porter, 2001) for preprocessing, and used the Giza++ (Och and Ney, 2000) toolkit to align the tokenized corpora at the word level. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). 2. Dependency Relation (DR) matching targets the increase of CLTE precision. By adding syntactic constraints to the matching process, DR features aim to reduce wrong matches often occurring at the lexical level. For instance, the contradiction between “Yahoo acquired Overture” and “Overture compr´o Yahoo” is evident when syntax (in this case subject-object inversion) is taken into account, but can not be caught by bag-of-words methods. We define a dependency relation as a triple that connects pairs of words through a grammatical relation. For example, “nsubj (loves, John)” is a dependency re"
S12-1105,N10-1045,1,0.802174,"Missing"
S12-1105,P11-1134,1,0.779175,"nslation evaluation datasets (Mehdad et al., 2012b). The content synchronization task represents a challenging application scenario to test the capabilities of CLTE systems, by proposing a richer inventory of phenomena (i.e. “Bidirectional”/“Forward”/“Backward”/“No entailment” multi-directional entailment relations). The CLTE methods proposed so far adopt either a “pivoting approach” (translation of the two input texts into the same language, as in (Mehdad et al., 2010)), or an “integrated solution” that exploits bilingual phrase tables to capture lexical relations and contextual information (Mehdad et al., 2011). The promising results achieved with the integrated approach still rely on phrasal matching techniques that disregard relevant semantic aspects of the problem. By filling this gap integrating linguistically motivated features, in our participation, we propose an approach that combines lexical, syntactic and semantic features within a machine learning framework (Mehdad et al., 2012a). Our submitted runs have been produced by training and optimizing multiclass and binary SVM classifiers, over the Spanish-English (Spa-Eng) development set. In both cases, our results were positive, showing signif"
S12-1105,P12-2024,1,0.753336,"Missing"
S12-1105,W12-3122,1,0.893173,"Missing"
S12-1105,W10-0734,1,0.902251,"Missing"
S12-1105,D11-1062,1,0.818954,"significant improvements over the median systems and average scores obtained by participants. The overall results confirm the difficulty of the task, and the potential of our approach in combining linguistically motivated features in a “pure” cross-lingual approach that avoids the recourse to external MT components. 701 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701–705, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Experiments In our experiment we used the Spa-Eng portion of the dataset described in (Negri et al., 2012; Negri et al., 2011), consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, “Backward” or “No entailment” judgements. 2.1 Once the matching phase for each n-gram level has been concluded, the number of matches M atchn and the number of phrases in the hypothesis H(n) is used to estimate the portion of phrases in H that are matched at each level n (Equation 1).1 Since languages can express the same meaning with different amounts of words, a phrase with length n in H can mat"
S12-1105,S12-1053,1,0.839505,"e positive, showing significant improvements over the median systems and average scores obtained by participants. The overall results confirm the difficulty of the task, and the potential of our approach in combining linguistically motivated features in a “pure” cross-lingual approach that avoids the recourse to external MT components. 701 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701–705, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2 Experiments In our experiment we used the Spa-Eng portion of the dataset described in (Negri et al., 2012; Negri et al., 2011), consisting of 500 multi-directional entailment pairs which was provided to train the systems and 500 pairs for the submission. Each pair in the dataset is annotated with “Bidirectional”, “Forward”, “Backward” or “No entailment” judgements. 2.1 Once the matching phase for each n-gram level has been concluded, the number of matches M atchn and the number of phrases in the hypothesis H(n) is used to estimate the portion of phrases in H that are matched at each level n (Equation 1).1 Since languages can express the same meaning with different amounts of words, a phrase with"
S12-1105,P00-1056,0,0.664839,"of failure with exact matching, lexical matching is performed at the same three levels. To reduce redundant matches, the lexical matches between pairs of phrases which have already been identified as exact matches are not considered. 702 M atchn |H(n)| (1) In order to build English-Spanish phrase tables for our experiments, we used the freely available Europarl V.4, News Commentary and United Nations Spanish-English parallel corpora released for the WMT10 Shared Translation Task.2 We run the TreeTagger (Schmid, 1995) and Snowball stemmer (Porter, 2001) for preprocessing, and used the Giza++ (Och and Ney, 2000) toolkit to align the tokenized corpora at the word level. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). 2. Dependency Relation (DR) matching targets the increase of CLTE precision. By adding syntactic constraints to the matching process, DR features aim to reduce wrong matches often occurring at the lexical level. For instance, the contradiction between “Yahoo acquired Overture” and “Overture compr´o Yahoo” is evident when syntax (in this case subject-object inversion) is taken into account, but can not be caught"
S13-2005,S13-2024,0,0.0219644,"Missing"
S13-2005,S13-2006,0,0.030069,"ta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measures, and syntactic information. SoftCard [pivoting, multi-class] (Jimenez et al., 2013) after automatic translation with Google Translate, uses SVMs to learn entailment decisions based on information about the cardinality of: T1, T2, their intersection and their union. Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity. Umelb [cross-lingual, pivoting, compositional] (Graham et al., 2013) adopts both pivoting and cross-lingual approaches. For the latter, GIZA++ was used to compute word alignments between the input sentences. Word alignment features are used to train binary SVM classifiers whose decisions are eventually c"
S13-2005,P10-4008,1,0.522467,"using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two sentences (e.g. abbreviation replacement), a number of features is extracted to train a multi-class SVM classifier. Such features consider information about sentence length, text similarity/difference measure"
S13-2005,S13-2099,0,0.0105521,"ned into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers), and majority voting. CELI [cross-lingual, meta-classifier] (Kouylekov, 2013) uses dictionaries for word matching, and a multilingual corpus extracted from Wikipedia for term weighting. A variety of distance measures implemented in the RTE system EDITS (Kouylekov and Negri, 2010; Negri et al., 2009) are used to extract features to train a meta-classifier. Such classifier combines binary decisions (“YES”/“NO”) taken separately for each of the four CLTE judgements. ECNUCS [pivoting, multi-class] (Jiang and Man, 2013) uses Google Translate to obtain the English translation of each T1. After a pre-processing step aimed at maximizing the commonalities between the two senten"
S13-2005,N10-1045,1,0.669554,"nce over texts written in different languages, targeting at the same time a real application scenario. Participants were presented with datasets for different language pairs, where multi-directional entailment relations (“forward”, “backward”, “bidirectional”, “no entailment”) had to be identified. We report on the training and test data used for evaluation, the process of their creation, the participating systems (six teams, 61 runs), the approaches adopted and the results achieved. 1 Yashar Mehdad UBC Vancouver, Canada mehdad@cs.ubc.ca Introduction The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization. Given two texts in different languages, the cross-lingual textual entailment (CLTE) task consists of deciding if the meaning of one text can be inferred from the meaning of the other text. Crosslinguality represents an interesting direction for research on recognizing textual entailment (RTE), especially due to its possible application in a variety of tasks. Among others (e.g. question answering, i"
S13-2005,P11-1134,1,0.635365,"about semantic equivalence and novelty depend on the possibility to fully or partially translate a text fragment into the other. The recent advances on monolingual TE on the one hand, and the methodologies used in Statistical Machine Translation (SMT) on the other, offer promising solutions to approach the CLTE task. In line with a number of systems that model the RTE task as a similarity problem (i.e. handling similarity scores between T and H as features contributing to the entailment decision), the standard sentence and word alignment programs used in SMT offer a strong baseline for CLTE (Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012"
S13-2005,P12-2024,1,0.83509,"Mehdad et al., 2011; 25 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 25–33, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics fragments entail each other (semantic equivalence); • forward (T1→T2 & T16←T2): unidirectional entailment from T1 to T2; • backward (T16→T2 & T1←T2): unidirectional entailment from T2 to T1; • no entailment (T16→T2 & T16←T2): there is no entailment between T1 and T2 in either direction; Figure 1: Example of SP-EN CLTE pairs. Mehdad et al., 2012). However, although representing a solid starting point to approach the problem, similarity-based techniques are just approximations, open to significant improvements coming from semantic inference at the multilingual level (e.g. cross-lingual entailment rules such as “perro”→“animal”). Taken in isolation, similaritybased techniques clearly fall short of providing an effective solution to the problem of assigning directions to the entailment relations (especially in the complex CLTE scenario, where entailment relations are multi-directional). Thanks to the contiguity between CLTE, TE and SMT,"
S13-2005,W10-0734,1,0.383734,"Missing"
S13-2005,D11-1062,1,0.7367,"n/English (DE-EN). Each corpus consists of 1,500 sentence pairs (1,000 for training and 500 for test), balanced across the four entailment judgements. In this year’s evaluation, as training set we used the CLTE-2012 corpus1 that was created for the SemEval-2012 evaluation exercise2 (including both training and test sets). The CLTE-2013 test set was created from scratch, following the methodology described in the next section. 3.1 To collect the entailment pairs for the 2013 test set we adopted a slightly modified version of the crowdsourcing methodology followed to create the CLTE2012 corpus (Negri et al., 2011). The main difference with last year’s procedure is that we did not take advantage of crowdsourcing for the whole data collection process, but only for part of it. As for CLTE-2012, the collection and annotation process consists of the following steps: 1. First, English sentences were manually extracted from Wikipedia and Wikinews. The selected sentences represent one of the elements (T1) of each entailment pair; 1 • bidirectional (T1→T2 & T1←T2): the two 26 Data collection and annotation 2 http://www.celct.it/resources.php?id page=CLTE http://www.cs.york.ac.uk/semeval-2012/task8/ 2. Next, eac"
S13-2005,S12-1053,1,0.323391,"Missing"
S13-2005,J03-1002,0,0.00871819,"a-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or in combination) by different classification strategies including: multi-class, a meta-classifier (i.e. combining the output of 2/3/4-class classifiers)"
S13-2005,S13-2023,1,0.84219,"ls. Regarding the latter dimension, in addition to compositional and multi-class strategies, also alternative solutions that leverage more sophisticated meta-classification strategies have been proposed. 31 Besides the recourse to MT tools (e.g. Google Translate), other tools and resources used by participants include: WordNet, word alignment tools (e.g. Giza++), part-of-speech taggers (e.g. Stanford POS Tagger), stemmers (e.g. Snowball), machine learning libraries (e.g. Weka, SVMlight), parallel corpora (e.g. Europarl), and stopword lists. More in detail: ALTN [cross-lingual, compositional] (Turchi and Negri, 2013) adopts a supervised learning method based on features that consider word alignments between the two sentences obtained with GIZA++ (Och et al., 2003). Binary entailment judgements are taken separately, and combined into final CLTE decisions. BUAP [pivoting, multi-class and metaclassifier] (Vilari˜no et al., 2013) adopts a pivoting method based on translating T1 into the language of T2 and vice versa (using Google Translate5 ). Similarity measures (e.g. Jaccard index) and features based on n-gram overlap, computed at the level of words and part of speech categories, are used (either alone or i"
S13-2005,S13-2022,0,0.0210187,"Missing"
S13-2005,negri-etal-2012-chinese,1,\N,Missing
W10-0734,D08-1027,0,0.0148646,"Missing"
W10-0734,ambati-etal-2010-active,0,0.0205873,"advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics the successive approximations that led to the definition of ou"
W10-0734,D09-1030,0,0.279741,"ems’ development/evaluation cycle. Our first step in this direction takes advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational"
W10-0734,N10-1045,1,0.180136,"s H, the task consists in deciding if the meaning of H can be inferred from the meaning of T. At the monolingual level, the great potential of integrating TE recognition (RTE) components into NLP architectures has been demonstrated in several areas, including question answering, information retrieval, information extraction, and document summarization. In contrast, mainly due to the absence of cross-lingual TE (CLTE) recognition components, similar improvements have not been achieved yet in any cross-lingual application. Along such direction, focusing on feasibility and architectural issues, (Mehdad et al., 2010) recently proposed baseline results demonstrating the potential of a simple approach that integrates Machine Translation and monolingual TE components. As a complementary research problem, this paper addresses the data collection issue, focusing on the definition of a fast, cheap, and reliable methodology to create CLTE corpora. The main motivation is that, as in many other NLP areas, the availability of large quantities of annotated data represents a critical bottleneck in the systems’ development/evaluation cycle. Our first step in this direction takes advantage of an already available monol"
W10-0734,P09-2078,0,0.0254218,"ation cycle. Our first step in this direction takes advantage of an already available monolingual corpus, casting the problem as a translation one. The challenge consists in taking a publicly available RTE dataset of English T-H pairs (i.e. the PASCAL-RTE3 dataset1 ), and create its English-Spanish CLTE equivalent by translating the hypotheses into Spanish. To this aim non-expert workers have been hired through the CrowdFlower2 channel to Amazon Mechanical Turk3 (MTurk), a crowdsourcing marketplace recently used with success for a variety of NLP tasks (Snow et al., 2008; Callison-Burch, 2009; Mihalcea and Strapparava, 2009; Marge et al., 2010; Ambati et al., 2010). The following sections overview our experiments, carried out under strict time (10 days) and cost ($100) limitations. In particular, Section 2 describes our data acquisition process; Section 3 summarizes 1 Available at: http://www.nist.gov/tac/data/RTE/index.html http://crowdflower.com/ 3 https://www.mturk.com/mturk/ 2 212 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 212–216, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics the successive appro"
W11-2404,W07-1428,0,0.0238803,"to more complex and natural ones. However, in contrast with other more stable tasks in terms of evaluation settings and metrics (e.g. machine translation), such changes make it difficult to capitalize on the experience obtained by participants throughout the years. Third, looking at RTE-related literature and the outcomes of the six campaigns organised so far, the conclusions that can be drawn are often controversial. For instance, it is not clear whether the availability of larger amounts of training data correlates with better performance (Hickl et al., 2006) or not (Zanzotto et al., 2007; Hickl and Bensley, 2007), even within the same evaluation setting. In addition, ablation tests carried out in recent editions of the challenge do not allow for definite conclusions about the actual usefulness of tools and resources, even the most popular ones (Bentivogli et al., 2009). Finally, the best performing systems often have different natures from one year to another, showing alternations of deep (Hickl and Bensley, 2007; Tatu and Moldovan, 2007) and shallow approaches (Jia et al., 2010) ranked at the top positions. In light of these considerations, it would be useful for sys30 Proceedings of the TextInfer 20"
W11-2404,P10-4008,1,0.739727,"good performance on all the available RTE Challenge datasets, but also to improve the official results, achieved with the same system, through ad hoc configurations manually defined by the developers team. Our contribution is twofold. On one side, in the spirit of the collaborative nature of open source projects, we extend an existing tool with a useful functionality that was still missing. On the other side, we provide a good “sparring partner” for system developers, to be used as a fast and free term of comparison to position the results of their work. 2 “Coping” with configurability EDITS (Kouylekov and Negri, 2010) is an open source RTE package, which offers a modular, flexible, and adaptable working environment to experiment with the RTE task over different datasets. The package allows to: i) create an entailment engine by defining its basic components (i.e. algorithms, cost schemes, rules, and optimizers); ii) train such entailment engine over an annotated RTE corpus to learn a model; and iii) use the entailment engine and the model to assign an entailment judgement and a confidence score to each pair of an un-annotated test corpus. A key feature of EDITS is represented by its high configurability, al"
W11-2404,P09-2073,1,0.78213,"ine by defining its basic components (i.e. algorithms, cost schemes, rules, and optimizers); ii) train such entailment engine over an annotated RTE corpus to learn a model; and iii) use the entailment engine and the model to assign an entailment judgement and a confidence score to each pair of an un-annotated test corpus. A key feature of EDITS is represented by its high configurability, allowed by the availability of different algorithms, the possibility to integrate different sets of lexical entailment/contradiction rules, and the variety of parameters for performance optimization (see also Mehdad, 2009). Although configurability is per se an important aspect (especially for an open-source and general purpose system), there is another side of the coin. In principle, in order to select the most promising configuration over a given development set, one should exhaustively run a huge number of training/evaluation routines. Such num1 http://edits.fbk.eu/ 31 ber corresponds to the total number of configurations allowed by the system, which result from the possible combinations of parameter settings. When dealing with enlarging dataset sizes, and the tight time constraints usually posed by the eval"
W11-2404,W07-1412,0,\N,Missing
W11-2404,W07-1404,0,\N,Missing
W12-3122,W11-2104,0,0.0497598,"estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features which are not focused on ad"
W12-3122,P11-1022,0,0.0747271,"addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However, their method still includes many features wh"
W12-3122,W05-0909,0,0.029602,"frequency counts with meaning representations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between"
W12-3122,C04-1046,0,0.18869,"from a rich set of variants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translati"
W12-3122,W07-0718,0,0.02155,"on. 4 Experiments and results 4.1 Datasets Datasets with manual evaluation of MT output have been made available through a number of shared evaluation tasks. However, most of these datasets are not specifically annotated for adequacy measurement purposes, and the available adequacy judgements are limited to few hundred sentences for some language pairs. Moreover, most datasets are created by comparing reference translations with MT systems’ output, disregarding the input sentences. Such judgements are hence biased towards the reference. Furthermore, the inter-annotator agreement is often low (Callison-Burch et al., 2007). In light of these limitations, most of the available datasets are per se not fully suitable for adequacy evaluation methods based on supervised learning, nor to provide stable and meaningful results. To partially cope with these problems, our experiments have been carried out over two different datasets: • 16K: 16.000 English-Spanish pairs, with Spanish translations produced by multiple MT systems, annotated by professional translators with quality scores in a 4-point scale (Specia et al., 2010a). • WMT07: 703 English-Spanish pairs derived from MT systems’ output, with explicit adequacy judg"
W12-3122,carreras-etal-2004-freeling,0,0.0213697,"Missing"
W12-3122,W07-0738,0,0.0366771,"Missing"
W12-3122,P07-2045,1,0.0115053,"rsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) matching features target the increase of CLT"
W12-3122,N10-1045,1,0.836751,"im, like (Xiong et al., 2010; Bach et al., 2011; Avramidis et al., 2011; Specia et al., 2010b; Specia et al., 2011) we rely on a large number of features, but focusing on source-target dependent ones, aiming at informed adequacy evaluation of a translation given the source instead of a more generic quality assessment based on surface features. 3 CLTE for adequacy evaluation We address adequacy evaluation by adapting crosslingual textual entailment recognition as a way to measure to what extent a source sentence and its automatic translation are semantically similar. CLTE has been proposed by (Mehdad et al., 2010) as an extension of textual entailment (Dagan and Glickman, 2004) that consists in deciding, given a text T and a hypothesis H in different languages, if the meaning of H can be inferred from the meaning of T. The main motivation in approaching adequacy evaluation using CLTE is that an adequate translation and the source text should convey the same meaning. In terms of entailment, this means that an adequate MT output and the source sentence should entail each other (bi-directional entailment). Losing or altering part of the meaning conveyed by the source sentence (i.e. having more, or differe"
W12-3122,P11-1134,1,0.925957,"will change the entailment direction and, consequently, the adequacy judgement. Framed in this way, CLTE-based adequacy evaluation methods can be designed to distinguish meaning-preserving variations from true divergence, regardless of reference translations. Similarly to many monolingual TE approaches, CLTE solutions proposed so far adopt supervised learning methods, with features that measure to what extent the hypotheses can be mapped into the texts. The underlying assumption is that the probability of entailment is proportional to the number of words in H that can be mapped to words in T (Mehdad et al., 2011). Such mapping can be carried out at different word representation levels (e.g. tokens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore"
W12-3122,P12-2024,1,0.497952,"okens, lemmas, stems), possibly with the support of lexical knowledge in order to cross the language barrier between T and H (e.g. dictionaries, phrase tables). Under the same assumption, since in the adequacy evaluation framework the entailment relation should hold in both directions, the mapping is performed both from the source to the target and vice-versa, building on features extracted from both sentences. Moreover, to improve over previous CLTE methods and boost MT adequacy evaluation performance, we explore the joint contribution of a number of lexical, syntactic and semantic features (Mehdad et al., 2012). Concerning the features used, it’s worth observing that the cost of implementing our approach (in terms of required resources and linguistic processors), and the need of reference translations are intrinsically different bottlenecks for MT. While the limited availability of processing tools for some language pairs is a “temporary” bottleneck, the acquisition of multiple references is a “permanent” one. The former cost is reducing over time due to the progress in NLP research; the latter represents a fixed cost that has to be eliminated. Similar considerations hold regarding the need of annot"
W12-3122,P00-1056,0,0.120809,"provides English and Spanish dependency parsers. Phrase Table (PT) matching features are calculated as in (Mehdad et al., 2011), with a phrasal matching algorithm that takes advantage of a lexical phrase table extracted from a bilingual parallel corpus. The algorithm determines the number of phrases in the source (1 to 5-grams, at the level of tokens, lemmas and stems) that can be mapped into target word sequences, and vice-versa. To build our English-Spanish phrase table, we used the Europarl, News Commentary and United Nations SpanishEnglish parallel corpora. After tokenization, the Giza++ (Och and Ney, 2000) and the Moses toolkit (Koehn et al., 2007) were respectively used to align the corpora and extract the phrase table. Although the phrase table was generated using MT technology, its use to compute our features is still compatible with a system-independent approach since the extraction is carried out without tuning the process towards any particular task. Moreover, our phrase matching algorithm integrates matches from overlapping n-grams of different size and nature (tokens, lemmas and stems) which current MT decoding algorithms cannot explore for complexity reasons. Dependency Relation (DR) m"
W12-3122,W09-0404,0,0.0341673,"Missing"
W12-3122,P02-1040,0,0.124085,"Missing"
W12-3122,quirk-2004-training,0,0.208721,"ariants at five different linguistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequa"
W12-3122,2006.amta-papers.25,0,0.0853718,"presentations. In order to integrate semantics more deeply into MT technology, in this paper we focus on the evaluation dimension. Restricting our investigation to some of the more pressing issues emerging from this area of research, we provide two main contributions. 1. An automatic evaluation method that avoids the use of reference translations. Most current metrics are based on comparisons between automatic translations and human references, and reward lexical similarity at the n-gram level (e.g. BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006)). Due to the variability of natural languages in terms of possible ways to express the same meaning, reliable lexical similarity metrics depend on the availability of multiple hand-crafted (costly) realizations of the same source sentence in the target language. Our approach aims to avoid this bottleneck by adapting cross-lingual semantic inference capabilities and judging a translation only given the source sentence. 2. A method for evaluating translation adequacy. Most current solutions do not consistently reward translation adequacy (semantic equivalence between source sentence and target"
W12-3122,2010.jec-1.5,0,0.0357492,"l, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of"
W12-3122,specia-etal-2010-dataset,0,0.180153,"guistic levels: lexical, shallow-syntactic, syntactic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-orien"
W12-3122,2011.mtsummit-papers.58,0,0.354932,"Missing"
W12-3122,2011.eamt-1.12,0,0.0381818,"tic, shallow-semantic and semantic. More similar to our approach, (Pad´o et al., 2009) proposed semantic adequacy metrics that exploit feature representations motivated by Textual Entailment (TE). Both metrics, however, highly depend on the availability of multiple reference translations. Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation ma"
W12-3122,P10-1062,0,0.0457323,"Early attempts to avoid reference translations addressed quality estimation (QE) by means of large numbers of source, target, and system-dependent features to discriminate between “good” and “bad” translations (Blatz et al., 2004; Quirk, 2004). More recently (Specia et al., 2010b; Specia and Farzindar, 2010; Specia, 2011) conducted a series of experiments using features designed to estimate translation post-editing effort (in terms of volume and time) as an indicator of MT output quality. Good results in QE have been achieved by adding linguistic information such as shallow parsing, POS tags (Xiong et al., 2010), or dependency relations (Bach et al., 2011; Avramidis et al., 2011) as features. However, in general these approaches do not distinguish between fluency (i.e. syntactic correctness of the out172 put translation) and adequacy, and mostly rely on fluency-oriented features (e.g. “number of punctuation marks”). As a result, a simple surface form variation is given the same importance of a content word variation that changes the meaning of the sentence. To the best of our knowledge, only (Specia et al., 2011) proposed an approach to frame MT evaluation as an adequacy estimation problem. However,"
W12-3122,W07-0734,0,\N,Missing
W13-2117,C10-1037,0,0.660917,"ll pipeline to generate an abstractive summary for each meeting transcript. Our system is similar to that of Murray et al. (2010) in terms of generating abstractive summaries for meeting transcripts. However, we take a lighter supervision for the content selection phase and a different approach towards the language generation phase, which does not rely on the conventional Natural Language Generation (NLG) architecture (Reiter and Dale, 2000). 2) We propose a word graph based approach to aggregate and generate the abstractive sentence summary. Our work extends the word graph method proposed by Filippova (2010) with the following novel contributions: i) We take advantage of lexical knowledge to merge similar nodes by finding their relations in WordNet; ii) We generate new sentences through generalization and aggregation of the original ones, which means that our generated sentences are not necessarily composed of the original words; and iii) We adopt a new ranking strategy to select the best path in the graph by taking the information content and the grammaticality (i.e. fluency) of the sentence into consideration. 3) In order to generate an abstract summary for a meeting, we have to be able to capt"
W13-2117,P12-3014,0,0.0439255,"our sentences from the meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such cases, the entailing sentence should repla"
W13-2117,C10-1039,0,0.187708,"Missing"
W13-2117,J05-3002,0,0.0288065,"Missing"
W13-2117,P11-1062,0,0.0964978,"l NLG pipeline (Reiter and Dale, 2000), we exploit 137 C “we should discuss about the remote control and its color” entails “about the remote”, “let’s talk about the remote” and “um remote’s color”, but not “remote’s size is also important”. So we can keep “we should discuss about the remote control and its color” and “remote’s size is also important” and eliminate the others. In this way, TE-based sentence identification can be designed to distinguish meaning-preserving variations from true divergence, regardless of lexical choices and structures. Similar to previous approaches in TE (e.g., (Berant et al., 2011)), we use a supervised method. To train and build the entailment graph, we perform three steps described in the following subsections. F E A x B G x D Figure 2: Building an entailment graph over sentences. Arrows and “x” represent the entailment direction and unknown cases respectively. 1 2.2 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge1 and Crosslingual textual entailment for content synchronization2 (Negri et al., 2012). However,"
W13-2117,W04-3205,0,0.00982008,"pair of sentences. Before aggregating the similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind usi"
W13-2117,W10-1751,0,0.0127616,"similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind using different entailment features is that"
W13-2117,N03-1020,0,0.0385472,"ank system (Garg et al., 2009) and with one abstractive baseline: 6) Original word graph model (Orig. word graph) (Filippova, 2010). In order to measure the effectiveness of different components, we also evaluated our system using human-annotated sentence communities (GC) in comparison with our community detection model (full). Moreover, we measure the performance of our system (GC) ablating the entailment module (-ent). Evaluation Metrics To evaluate performance, we use the ROUGE-1 and ROUGE-2 (unigram and bigram overlap) F1 score, which correlate well with human rankings of summary quality (Lin and Hovy, 2003). We also ignore stopwords to reduce the impact of high overlap when matching them. Furthermore, to evaluate the grammaticality of our generated summaries in comparison with the original word graph method, following common practice (Barzilay and McKeown, 2005), we randomly selected 10 meeting summaries (total 150 sentences). Then, we asked annotators to give one 3 ROUGE-2 3 4.4 5.1 3.8 4.8 4.0 4.2 Table 1: Performance of different summarization algorithms on human transcripts for meeting conversations. 5 For preprocessing our dataset we use OpenNLP3 for tokenization and part-of-speech tagging."
W13-2117,P09-2066,0,0.0317005,"Missing"
W13-2117,D11-1062,1,0.237432,"an entailment graph over sentences. Arrows and “x” represent the entailment direction and unknown cases respectively. 1 2.2 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge1 and Crosslingual textual entailment for content synchronization2 (Negri et al., 2012). However, such datasets cannot directly support our application, since the RTE datasets are often composed of longer wellformed sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset that is more similar to the goal of our entailment framework, we select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpose, which is closer to our work. We sort the RTE6 and RTE7 dataset pairs based on the sentence length and choose the first 2000 samples with an equal number of positive and negative examples."
W13-2117,W01-0100,0,0.509626,"Missing"
W13-2117,S12-1053,1,0.458466,"Missing"
W13-2117,N10-1045,1,0.261584,"differences between our training set and our sentences from the meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such c"
W13-2117,P11-1134,1,0.342034,"we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction from s1 to s2 . In this way, we can estimate the portion of information/facts in s2 which is covered by s1 . The first five scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymy-hyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the similarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 tokens). The rationale behind using different entailment features is that combining various scores will yield a bett"
W13-2117,P12-2024,1,0.82418,"n similarity scores calculated with a sentence-tosentence matching process. Each example pair of sentences (s1 and s2 ) is represented by a feature vector, where each feature is a specific similarity score estimating whether s1 entails s2 . Entailment graph edge labeling Since our training examples are labeled with binary judgments, we are not able to train a threeway classifier. Therefore, we set the edge labeling problem as a two-way classification task that casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YES-NO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. We compute 18 similarity scores for each pair of sentences. Before aggregating the similarity scores to form an entailment score, we normalize the similarity scores by the length of s2 (in terms of lexical items), when checking the entailment direction fr"
W13-2117,C00-2137,0,0.0351027,"Missing"
W13-2117,N13-1018,1,0.755572,"he meeting corpus. However, the collected training samples was the closest available dataset to our needs. Entailment Graph Sentences in a community often include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the sentences in each community, we can discover the information in one sentence that is semantically equivalent, novel, or more/less informative with respect to the content of the other sentences. Similar to earlier work (Lloret et al., 2008; Mehdad et al., 2010; Berant et al., 2011; Adler et al., 2012; Mehdad et al., 2013), we set this problem as a variant of the Textual Entailment (TE) recognition task (Dagan and Glickman, 2004). We build an entailment graph for each community of sentences, where nodes are the linked sentences and edges are the entailment relations between nodes. Given two sentences (s1 and s2 ), we aim at identifying the following cases: i) s1 and s2 express the same meaning (bidirectional entailment). In such cases one of the sentences should be eliminated; ii) s1 is more informative than s2 (unidirectional entailment). In such cases, the entailing sentence should replace or complement the e"
W13-2117,W10-4211,1,0.620092,"ctive task scores; likewise automatic abstractive summaries are preferred in comparison with human extracts (Murray et al., 2010). Moreover, most of the abstractive summarization approaches focus on one component of the system, such as generation (e.g., (Genest and Lapalme, 2010)) or content selection (e.g., (Murray et al., 2012)), instead of developing the full framework for abstractive summarization. To address these limitations, as the main contribution of this paper, we propose a full pipeline to generate an abstractive summary for each meeting transcript. Our system is similar to that of Murray et al. (2010) in terms of generating abstractive summaries for meeting transcripts. However, we take a lighter supervision for the content selection phase and a different approach towards the language generation phase, which does not rely on the conventional Natural Language Generation (NLG) architecture (Reiter and Dale, 2000). 2) We propose a word graph based approach to aggregate and generate the abstractive sentence summary. Our work extends the word graph method proposed by Filippova (2010) with the following novel contributions: i) We take advantage of lexical knowledge to merge similar nodes by find"
W13-2117,W12-2602,1,0.788588,"omponents, which we describe in more detail in the following sections. 2.1 Community Detection While some abstractive summary sentences are very similar to original sentences from the meeting transcript, others can be created by aggregating and merging multiple sentences into an abstract sentence. In order to generate such a sentence, we need to identify which sentences from the original meeting transcript should be combined in generated abstract sentences. This task can be considered as the first step of abstractive meeting summarization and is called “abstractive community detection (ACD)” (Murray et al., 2012). To perform ACD, we follow the same method proposed by Murray et al. (2012), in two steps: First, we classify sentence pairs according to whether or not they should be realized by a common abstractive sentence. For each pair, we extract its structural and linguistic features, and we train a logistic regression classifier over all our training data (described in Section 3.1) exploiting such features. We run the trained classier over sentence pairs, predicting abstractive links between sentences in the document. The result can be represented as an undirected graph where nodes are the sentences,"
W13-2117,W04-3252,0,\N,Missing
W13-2117,W07-1401,0,\N,Missing
W13-4017,W12-1616,0,0.100159,"s and bigrams have been shown to be useful for the task of DA modeling in previous studies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi, 2007; Carvalho, 2005). In addition, unigrams have been shown to be the most effective among the two. So, as the lexical feature, we include the frequency of unigrams in our feature set. Moreover, length of the utterance is another beneficial feature for DA recognition (Ferschke, 2012; Shrestha, 2004; Joty, 2011), which we add to our feature set. The speaker of an utterance has shown its utility for recognizing speech acts (Sun, 2012; Kim, 2010a; Joty, 2011). Sun and Morency (2012) specifically employ a speakeradaptation technique to demonstrate the effectiveness of this feature for DA modeling. We also include the relative position of a sentence in a post for DA modeling since most of previous studies (Ferschke, 2012; Kim, 2010a; Joty, 2011) prove the efficiency of this feature. Dialogue Act Recognition Conversational structure Adjacent utterances in a conversation have a strong correlation in terms of their dialogue acts. As an example, if speaker 1 asks a question to speaker 2, it is a high probability that the next utterance of the conversation would be an answer fr"
W13-4017,W10-4211,1,0.366658,"Missing"
W13-4017,C04-1128,0,0.0163628,"kes it possible to automatically 2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of our knowledge, none of them compare the performance of DA recognition on different synchronous (e.g., meeting and phone) and asynchronous (e.g., email and forum) conversations. Most of the works analyze DA modeling in a specific domain. Carvalho and Cohen (2005) propose classifying emails into their dialogue acts according to two ontologies for nouns and verbs. The ontologies are used for determining the speech acts of each single email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia disc"
W13-4017,D09-1130,0,0.0886966,"Missing"
W13-4017,E12-1079,0,0.094304,"le email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia discussions to analyze the collaborative process of editing Wikipedia pages. Kim et al. (2010a) study the task of supervised classification of dialogue acts in one-to-one online chats in the shopping domain. All these previous studies focus on DA recognition in one or two domains, and do not systematically analyze the performance of different dialog act modeling approaches on a comprehensive set of conversation domains. As far as we know, the present work is the first that proposes domain-independent supervised DA modeling techniques, and analyzes their effectiv"
W13-4017,D09-1035,0,0.0178444,"Missing"
W13-4017,W04-2319,0,\N,Missing
W13-4017,D10-1084,0,\N,Missing
W13-4017,W10-2923,0,\N,Missing
W14-4407,N13-1030,0,0.01564,"three words. Note that these rules, which were defined based on close observation of the results obtained from our development set, greatly reduce the chance of selecting illstructured templates. Second, the remaining paths are reranked by 1) A normalized path weight and 2) A language model learned from hypernym-labeled human-authored summaries in our training data, each of which is described below. Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of"
W14-4407,W10-4211,1,0.386939,"Missing"
W14-4407,W12-2602,1,0.856542,"meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summariza2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailment graph over each of the latter in order to select the salient utterances. It then applies a semantic word graph algorithm to them and creates abstractive summaries. Their results show some improvement in creating informative summaries. 45 Proceedings of the 8th International Natural Language Generation Conference, pages 45–53, c Philadelphia, Pennsylvania, 19-21 June 2014. 2014 Association for Computational Linguistics However, since they create these summaries by merging human utterances, their summaries are still partially extractive. Recently, there have been s"
W14-4407,W11-0501,0,0.045407,"Missing"
W14-4407,C10-1037,0,0.179103,"hese rules, which were defined based on close observation of the results obtained from our development set, greatly reduce the chance of selecting illstructured templates. Second, the remaining paths are reranked by 1) A normalized path weight and 2) A language model learned from hypernym-labeled human-authored summaries in our training data, each of which is described below. Template Fusion We further generalize the clustered templates by applying a word graph algorithm. The algorithm was originally proven to be effective in summarizing a cluster of related sentences (Boudin and Morin, 2013; Filippova, 2010; Mehdad et al., 2013). We extend it so that it can be applied to templates. Word Graph Construction In our system, a word graph is a directed graph with words or blanks serving as nodes and edges representing adjacency relations. Given a set of related templates in a group, the graph is constructed by first creating a start and end node, and then iteratively adding templates to it. When adding a new template, the algorithm first checks each word in the template to see if it can be mapped onto existing nodes in the graph. The word is mapped onto a node if the node consists of the same word and"
W14-4407,P05-1037,0,0.0148029,"Missing"
W14-4407,W09-0613,0,0.00975681,"ome studies on creating abstract summaries of specific aspects of meetings such as decisions, actions and problems (Murray et al. 2010; Wang and Cardie, 2013). These summaries are called the Focused Meeting Summaries (Carenini et al., 2011). The system introduced by Murray et al. first classifies human utterances into specific aspects of meetings, e.g. decisions, problem, and action, and then maps them onto ontologies. It then selects the most informative subsets from these ontologies and finally generates abstractive summaries of them, utilizing a natural language generation tool, simpleNLG (Gatt and Reiter, 2009). Although their approach is essentially focused meeting summarization, after creating summaries of specific aspects, they aggregate them into one single summary covering the whole meeting. Wang and Cardie introduced a template-based focused abstractive meeting summarization system. Their system first clusters human-authored summary sentences and applies a MultipleSequence Alignment algorithm to them to generate templates. Then, given a meeting transcript to be summarized, it identifies a human utterance cluster describing a specific aspect and extracts all summary-worthy relation instances, i"
W14-4407,P13-1138,0,0.0355438,"Missing"
W14-4407,de-marneffe-etal-2006-generating,0,0.0322498,"Missing"
W14-4407,W13-2117,1,0.906723,"utomatic meeting summarization has been attracting peoples’ attention as it can save a great deal of their time and increase their productivity. The most common approaches to automatic meeting summarization have been extractive. Since extractive approaches do not require natural language generation techniques, they are arguably simpler to apply and have been extensively investigated. However, a user study conducted by Murray et al. (2010) indicates that users prefer abstractive summaries to extractive ones. Thereafter, more attention has been paid to abstractive meeting summarization systems (Mehdad et al. 2013; Murray et al. 2010; Wang and Cardie 2013). However, the approaches introduced in previous studies create summaries by either heavily relying on annotated data or by fusing human utterances which may contain grammatical mistakes. In this paper, we address these issues by introducing a novel summariza2. Related Work Several studies have been conducted on creating automatic abstractive meeting summarization systems. One of them includes the system proposed by Mehdad et al., (2013). Their approach first clusters human utterances into communities (Murray et al., 2012) and then builds an entailmen"
W14-4407,W04-3252,0,\N,Missing
W14-4407,P03-1071,0,\N,Missing
W16-3638,P12-2059,0,0.0167208,"pressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform the previous state-of-the-art for this dataset. Although word and character n-grams have been used as features"
W16-3638,N15-1010,0,0.0147501,"ards and guidelines imposed by media companies that users must adhere to, in conjunction with regular expressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform t"
W16-3638,W13-1706,1,0.738086,"rs must adhere to, in conjunction with regular expressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform the previous state-of-the-art for this dataset. Although w"
W16-3638,P12-2018,0,0.0152188,"classes, using our SVM classifier (”Combination”). In terms of overall performance, all methods improved on or tied the Djuric et al. (2015b), C2V and token n-gram baselines. The top performing baseline and current state-of-the-art, Nobata et al. (2016), which consists of a comprehensive combination of a range of different features, is bested by NBSVM using solely character n-grams (77 FSupport Vector Machine with Naive Bayes Features (NBSVM) Naive Bayes (NB) and Support Vector Machines (SVM) have been proven to be effective approaches for NLP applications such as sentiment and text analysis. Wang and Manning (2012) showed the power of combining these two generative and discriminative classifiers where an SVM 301 Method Djuric et al. Nobata et al. Token n-grams C2V d300w10∗ C2V d300w5 C2V d100w10 C2V d100w5 NBSVM (word)∗ NBSVM (char)∗ RNNLM (word)∗ RNNLM (char1 )∗ RNNLM (char2 ) Combination Rec. 79 76 58 57 56 56 60 72 72 78 68 75 Prec. 77 70 77 76 75 76 84 83 59 60 68 84 F-1 78 73 66 66 65 64 70 77 65 68 68 79 AUC 80 91 84 85 84 82 82 89 92 82 85 85 93 2015b). As one would expect, decreasing the dimensionality of the embedding and the context window results in a loss of performance of as much as 18 F-1"
W16-3638,W12-2103,0,0.38382,"stealing like a lil maggot. Hang thm all.” In that example, there are tokenization and normalization issues, as well as a conscious bastardization of words in an effort to evade blacklists or to add color to the post. While previous work for detecting abusive language has been dominated by lexical-based approaches, we claim that morphological features play a more significant role in this task. This 2 Related Work Prior work in abusive language has been rather diffuse as researchers have focused on different aspects ranging from profanity detection (Sood et al., 2012) to hate speech detection (Warner and Hirschberg, 2012) to cyberbullying (Dadvar et al., 2013) and to abusive language in general (Chen et al., 2012; Djuric et al., 2015b). The overwhelming majority of this work has 299 Proceedings of the SIGDIAL 2016 Conference, pages 299–303, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics two, RNNLM and NBSVM, we use as methodologies for which to explore the impact of characterbased vs. token-based features. focused on using supervised classification with canonical NLP features. Token n-grams are one of the most popular features across many works (Yin et al., 2009; Chen"
W17-2614,D07-1018,0,0.0347083,"porate new tags. • The output tag embeddings can be used in other applications. Related Work: Multi-label learning has found several applications in social media and web, like sentiment and topic analysis (Huang et al., 2013a), social text stream analysis (Ren et al., 2014), and online advertising (Agrawal et al., 2013). MLL has also been applied to diverse Natural Language Processing (NLP) tasks. However to the best of our knowledge we are the first to propose embedding based MLL approach to a NLP task. MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives (Boleda et al., 2007). (Huang et al., 2013b) proposed a joint model to predict sentiment and topic for tweets and (Surdeanu et al., 2012) proposed a multiinstance MLL based approach for relation extraction with distant supervision. Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (Passos et al., 2014; Lample et al., 2016; Ma and Hovy, 2016), sentiment classification (Socher et al., 2011; Tang 112 script d of n and M suggests that the number of word and tag is different from document to document. For c"
W17-2614,W14-1504,0,0.0244547,", wi+1 j−1 j spondingly, we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the symbol di interchangeably with the embedding vector di to refer to the i-th document, and use dd to denote the vector representation of document d. Similar conventions apply to word and tag embeddings. Besides we let σ(·) be the sigmoid function, i.e., σ(a) = 1/(1 + exp(−a)). et al., 2014; dos Santos and Gatti, 2014) and summarization (Kaageback et al., 2014; Rush et al., 2015; Li et al., 2015). Also, vector space modeling has been applied to search re-targeting (Grbovic et al., 2015a) and query rewriting (Grbovic et al., 2015b). Given many potential applications, document tagging has been a very active research area. In information retrieval, it is often coined as contentbased tag recommendation problem (Chirita et al., 2007), for which numbers of approaches were proposed, such as (Heymann et al., 2008), (Song et al., 2008b), (Song et al., 2008a) and (Song et al., 2011). Personalized tag recommendation is also studied in the literature (Symeonid"
W17-2614,N16-1030,0,0.0206811,"Missing"
W17-2614,P15-1107,0,0.078233,"Missing"
W17-2614,P16-1101,0,0.0225042,"we are the first to propose embedding based MLL approach to a NLP task. MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives (Boleda et al., 2007). (Huang et al., 2013b) proposed a joint model to predict sentiment and topic for tweets and (Surdeanu et al., 2012) proposed a multiinstance MLL based approach for relation extraction with distant supervision. Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (Passos et al., 2014; Lample et al., 2016; Ma and Hovy, 2016), sentiment classification (Socher et al., 2011; Tang 112 script d of n and M suggests that the number of word and tag is different from document to document. For convenience, we use the shorthand wid : wjd , i ≤ j, to denote the subsequence of words d , . . . , w d , w d in document d. Correwid , wi+1 j−1 j spondingly, we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the symbol di interchangeably with the embedd"
W17-2614,C14-1008,0,0.0382816,". . . , w d , w d in document d. Correwid , wi+1 j−1 j spondingly, we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the symbol di interchangeably with the embedding vector di to refer to the i-th document, and use dd to denote the vector representation of document d. Similar conventions apply to word and tag embeddings. Besides we let σ(·) be the sigmoid function, i.e., σ(a) = 1/(1 + exp(−a)). et al., 2014; dos Santos and Gatti, 2014) and summarization (Kaageback et al., 2014; Rush et al., 2015; Li et al., 2015). Also, vector space modeling has been applied to search re-targeting (Grbovic et al., 2015a) and query rewriting (Grbovic et al., 2015b). Given many potential applications, document tagging has been a very active research area. In information retrieval, it is often coined as contentbased tag recommendation problem (Chirita et al., 2007), for which numbers of approaches were proposed, such as (Heymann et al., 2008), (Song et al., 2008b), (Song et al., 2008a) and (Song et al., 2011). Personalized tag recommendation i"
W17-2614,W14-1609,0,0.0285605,"sks. However to the best of our knowledge we are the first to propose embedding based MLL approach to a NLP task. MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives (Boleda et al., 2007). (Huang et al., 2013b) proposed a joint model to predict sentiment and topic for tweets and (Surdeanu et al., 2012) proposed a multiinstance MLL based approach for relation extraction with distant supervision. Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (Passos et al., 2014; Lample et al., 2016; Ma and Hovy, 2016), sentiment classification (Socher et al., 2011; Tang 112 script d of n and M suggests that the number of word and tag is different from document to document. For convenience, we use the shorthand wid : wjd , i ≤ j, to denote the subsequence of words d , . . . , w d , w d in document d. Correwid , wi+1 j−1 j spondingly, we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the"
W17-2614,D15-1044,0,0.0326093,"we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the symbol di interchangeably with the embedding vector di to refer to the i-th document, and use dd to denote the vector representation of document d. Similar conventions apply to word and tag embeddings. Besides we let σ(·) be the sigmoid function, i.e., σ(a) = 1/(1 + exp(−a)). et al., 2014; dos Santos and Gatti, 2014) and summarization (Kaageback et al., 2014; Rush et al., 2015; Li et al., 2015). Also, vector space modeling has been applied to search re-targeting (Grbovic et al., 2015a) and query rewriting (Grbovic et al., 2015b). Given many potential applications, document tagging has been a very active research area. In information retrieval, it is often coined as contentbased tag recommendation problem (Chirita et al., 2007), for which numbers of approaches were proposed, such as (Heymann et al., 2008), (Song et al., 2008b), (Song et al., 2008a) and (Song et al., 2011). Personalized tag recommendation is also studied in the literature (Symeonidis et al., 2008; Re"
W17-2614,D11-1014,0,0.0606071,"L approach to a NLP task. MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives (Boleda et al., 2007). (Huang et al., 2013b) proposed a joint model to predict sentiment and topic for tweets and (Surdeanu et al., 2012) proposed a multiinstance MLL based approach for relation extraction with distant supervision. Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (Passos et al., 2014; Lample et al., 2016; Ma and Hovy, 2016), sentiment classification (Socher et al., 2011; Tang 112 script d of n and M suggests that the number of word and tag is different from document to document. For convenience, we use the shorthand wid : wjd , i ≤ j, to denote the subsequence of words d , . . . , w d , w d in document d. Correwid , wi+1 j−1 j spondingly, we denote W = [w1 , . . . , wV ] ∈ RK×V as the matrix for word embeddings, D = [d1 , . . . , dN ] ∈ RK×N as the matrix for document embeddings, and T = [t1 , . . . , tM ] ∈ RK×M as the matrix for tag embeddings. Sometimes we may use the symbol di interchangeably with the embedding vector di to refer to the i-th document, an"
W17-2614,D12-1042,0,0.0807343,"has found several applications in social media and web, like sentiment and topic analysis (Huang et al., 2013a), social text stream analysis (Ren et al., 2014), and online advertising (Agrawal et al., 2013). MLL has also been applied to diverse Natural Language Processing (NLP) tasks. However to the best of our knowledge we are the first to propose embedding based MLL approach to a NLP task. MLL has been applied to Word Sense Disambiguation (WSD) problem for polysemic adjectives (Boleda et al., 2007). (Huang et al., 2013b) proposed a joint model to predict sentiment and topic for tweets and (Surdeanu et al., 2012) proposed a multiinstance MLL based approach for relation extraction with distant supervision. Recently learning embeddings of words and sentences from large unannotated corpus has gained immense popularity in many NLP tasks, such as Named Entity Recognition (Passos et al., 2014; Lample et al., 2016; Ma and Hovy, 2016), sentiment classification (Socher et al., 2011; Tang 112 script d of n and M suggests that the number of word and tag is different from document to document. For convenience, we use the shorthand wid : wjd , i ≤ j, to denote the subsequence of words d , . . . , w d , w d in docu"
W17-2614,P14-1146,0,0.0819182,"Missing"
