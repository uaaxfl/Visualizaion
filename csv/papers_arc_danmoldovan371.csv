2020.lrec-1.8,A Study on Entity Resolution for Email Conversations,2020,-1,-1,3,1,16605,parag dakle,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper investigates the problem of entity resolution for email conversations and presents a seed annotated corpus of email threads labeled with entity coreference chains. Characteristics of email threads concerning reference resolution are first discussed, and then the creation of the corpus and annotation steps are explained. Finally, performance of the current state-of-the-art deep learning models on the seed corpus is evaluated and qualitative error analysis on the predictions obtained is presented."
2020.lrec-1.135,Joint Learning of Syntactic Features Helps Discourse Segmentation,2020,-1,-1,3,1,16606,takshak desai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper describes an accurate framework for carrying out multi-lingual discourse segmentation with BERT (Devlin et al., 2019). The model is trained to identify segments by casting the problem as a token classification problem and jointly learning syntactic features like part-of-speech tags and dependency relations. This leads to significant improvements in performance. Experiments are performed in different languages, such as English, Dutch, German, Portuguese Brazilian and Basque to highlight the cross-lingual effectiveness of the segmenter. In particular, the model achieves a state-of-the-art F-score of 96.7 for the RST-DT corpus (Carlson et al., 2003) improving on the previous best model by 7.2{\%}. Additionally, a qualitative explanation is provided for how proposed changes contribute to model performance by analyzing errors made on the test data."
2020.lrec-1.188,Affect in{T}weets: A Transfer Learning Approach,2020,-1,-1,4,1,16997,linrui zhang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"People convey sentiments and emotions through language. To understand these affectual states is an essential step towards understanding natural language. In this paper, we propose a transfer-learning based approach to inferring the affectual state of a person from their tweets. As opposed to the traditional machine learning models which require considerable effort in designing task specific features, our model can be well adapted to the proposed tasks with a very limited amount of fine-tuning, which significantly reduces the manual effort in feature engineering. We aim to show that by leveraging the pre-learned knowledge, transfer learning models can achieve competitive results in the affectual content analysis of tweets, compared to the traditional models. As shown by the experiments on SemEval-2018 Task 1: Affect in Tweets, our model ranking 2nd, 4th and 6th place in four of its subtasks proves the effectiveness of our idea."
2020.coling-main.30,{CEREC}: A Corpus for Entity Resolution in Email Conversations,2020,-1,-1,2,1,16605,parag dakle,Proceedings of the 28th International Conference on Computational Linguistics,0,"We present the first large scale corpus for entity resolution in email conversations (CEREC). The corpus consists of 6001 email threads from the Enron Email Corpus containing 36,448 email messages and 38,996 entity coreference chains. The annotation is carried out as a two-step process with minimal manual effort. Experiments are carried out for evaluating different features and performance of four baselines on the created corpus. For the task of mention identification and coreference resolution, a best performance of 54.1 F1 is reported, highlighting the room for improvement. An in-depth qualitative and quantitative error analysis is presented to understand the limitations of the baselines considered."
W18-3803,Rule-based vs. Neural Net Approaches to Semantic Textual Similarity,2018,-1,-1,2,1,16997,linrui zhang,Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing,0,"This paper presents a neural net approach to determine Semantic Textual Similarity (STS) using attention-based bidirectional Long Short-Term Memory Networks (Bi-LSTM). To this date, most of the traditional STS systems were rule-based that built on top of excessive use of linguistic features and resources. In this paper, we present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against SemEval-2012 benchmark, we make an assessment on the limitations and strengths of neural net systems to rule-based systems on Semantic Textual Similarity."
W18-3701,Generating Questions for Reading Comprehension using Coherence Relations,2018,0,2,3,1,16606,takshak desai,Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,0,"In this paper, we have proposed a technique for generating complex reading comprehension questions from a discourse that are more useful than factual ones derived from assertions. Our system produces a set of general-level questions using coherence relations and a set of well-defined syntactic transformations on the input text. Generated questions evaluate comprehension abilities like a comprehensive analysis of the text and its structure, correct identification of the author{'}s intent, a thorough evaluation of stated arguments; and a deduction of the high-level semantic relations that hold between text spans. Experiments performed on the RST-DT corpus allow us to conclude that our system possesses a strong aptitude for generating intricate questions. These questions are capable of effectively assessing a student{'}s interpretation of the text."
L18-1077,{C}hinese Relation Classification using Long Short Term Memory Networks,2018,0,0,2,1,16997,linrui zhang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
erekhinskaya-etal-2014-multilingual,Multilingual e{X}tended {W}ord{N}et Knowledge Base: Semantic Parsing and Translation of Glosses,2014,8,3,3,0,39911,tatiana erekhinskaya,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a method to create WordNet-like lexical resources for different languages. Instead of directly translating glosses from one language to another, we perform first semantic parsing of WordNet glosses and then translate the resulting semantic representation. The proposed approach simplifies the machine translation of the glosses. The approach provides ready to use semantic representation of glosses in target languages instead of just plain text."
E14-1016,Leveraging Verb-Argument Structures to Infer Semantic Relations,2014,32,6,2,1,7292,eduardo blanco,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper presents a methodology to infer implicit semantic relations from verbargument structures. An annotation effort shows implicit relations boost the amount of meaning explicitly encoded for verbs. Experimental results with automatically obtained parse trees and verb-argument structures demonstrate that inferring implicit relations is a doable task.
D13-1123,A Semantically Enhanced Approach to Determine Textual Similarity,2013,39,2,2,1,7292,eduardo blanco,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods."
N12-1050,Fine-Grained Focus for Pinpointing Positive Implicit Meaning from Negated Statements,2012,29,2,2,1,7292,eduardo blanco,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Negated statements often carry positive implicit meaning. Regardless of the semantic representation one adopts, pinpointing the positive concepts within a negated statement is needed in order to encode the statement's meaning. In this paper, novel ideas to reveal positive implicit meaning using focus of negation are presented. The concept of granularity of focus is introduced and justified. New annotation and features to detect fine-grained focus are discussed and results reported."
tatu-moldovan-2012-tool,A Tool for Extracting Conversational Implicatures,2012,11,1,2,1,42868,marta tatu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Explicitly conveyed knowledge represents only a portion of the information communicated by a text snippet. Automated mechanisms for deriving explicit information exist; however, the implicit assumptions and default inferences that capture our intuitions about a normal interpretation of a communication remain hidden for automated systems, despite the communication participants' ease of grasping the complete meaning of the communication. In this paper, we describe a reasoning framework for the automatic identification of conversational implicatures conveyed by real-world English and Arabic conversations carried via twitter.com. Our system transforms given utterances into deep semantic logical forms. It produces a variety of axioms that identify lexical connections between concepts, define rules of combining semantic relations, capture common-sense world knowledge, and encode Grice's Conversational Maxims. By exploiting this rich body of knowledge and reasoning within the context of the conversation, our system produces entailments and implicatures conveyed by analyzed utterances with an F-measure of 70.42{\%} for English conversations."
moldovan-blanco-2012-polaris,{P}olaris: Lymba{'}s Semantic Parser,2012,18,16,1,1,16607,dan moldovan,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Semantic representation of text is key to text understanding and reasoning. In this paper, we present Polaris, Lymba's semantic parser. Polaris is a supervised semantic parser that given text extracts semantic relations. It extracts relations from a wide variety of lexico-syntactic patterns, including verb-argument structures, noun compounds and others. The output can be provided in several formats: XML, RDF triples, logic forms or plain text, facilitating interoperability with other tools. Polaris is implemented using eight separate modules. Each module is explained and a detailed example of processing using a sample sentence is provided. Overall results using a benchmark are discussed. Per module performance, including errors made and pruned by each module are also analyzed."
W11-0106,A Model for Composing Semantic Relations,2011,18,7,2,1,7292,eduardo blanco,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"This paper presents a model to compose semantic relations. The model is independent of any particular set of relations and uses an extended definition for semantic relations. This extended definition includes restrictions on the domain and range of relations and utilizes semantic primitives to characterize them. Primitives capture elementary properties between the arguments of a relation. An algebra for composing semantic primitives is used to automatically identify the resulting relation of composing a pair of compatible relations. Inference axioms are obtained. Axioms take as input a pair of semantic relations and output a new, previously ignored relation. The usefulness of this proposed model is shown using PropBank relations. Eight inference axioms are obtained and their accuracy and productivity are evaluated. The model offers an unsupervised way of accurately extracting additional semantics from text."
P11-1059,Semantic Representation of Negation Using Focus Detection,2011,23,34,2,1,7292,eduardo blanco,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Negation is present in all human languages and it is used to reverse the polarity of part of statements that are otherwise affirmative by default. A negated statement often carries positive implicit meaning, but to pinpoint the positive part from the negative part is rather difficult. This paper aims at thoroughly representing the semantics of negation by revealing implicit positive meaning. The proposed representation relies on focus of negation detection. For this, new annotation over PropBank and a learning algorithm are proposed."
P11-1146,Unsupervised Learning of Semantic Relation Composition,2011,25,16,2,1,7292,eduardo blanco,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,This paper presents an unsupervised method for deriving inference axioms by composing semantic relations. The method is independent of any particular relation inventory. It relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. The method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over PropBank.
tatu-moldovan-2010-inducing,Inducing Ontologies from Folksonomies using Natural Language Understanding,2010,-1,-1,2,1,42868,marta tatu,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Folksonomies are unsystematic, unsophisticated collections of keywords associated by social bookmarking users to web content and, despite their inconsistency problems (typographical errors, spelling variations, use of space or punctuation as delimiters, same tag applied in different context, synonymy of concepts, etc.), their popularity is increasing among Web 2.0 application developers. In this paper, in addition to eliminating folksonomic irregularities existing at the lexical, syntactic or semantic understanding levels, we propose an algorithm that automatically builds a semantic representation of the folksonomy by exploiting the tags, their social bookmarking associations (co-occuring tags) and, more importantly, the content of labeled documents. We derive the semantics of each tag, discover semantic links between the folksonomic tags and expose the underlying semantic structure of the folksonomy, thus, enabling a number of information discovery and ontology-based reasoning applications."
davis-moldovan-2010-feasibility,Feasibility of Automatically Bootstrapping a {P}ersian {W}ord{N}et,2010,8,0,2,1,25065,chris davis,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper we describe a proof-of-concept for the bootstrapping of a Persian WordNet. This effort was motivated by previous work done at Stanford University on bootstrapping an Arabic WordNet using a parallel corpus and an English WordNet. The principle of that work is based on the premise that paradigmatic relations are by nature deeply semantic, and as such, are likely to remain intact between languages. We performed our task on a Persian-English bilingual corpus of George OrwellÂs Nineteen Eighty-Four. The corpus was neither aligned nor sense tagged, so it was necessary that these were undertaken first. A combination of manual and semiautomated methods were used to tag and sentence align the corpus. Actual mapping of English word senses onto Persian was done using automated techniques. Although Persian is written in Arabic script, it is an Indo-European language, while Arabic is a Central Semitic language. Despite their linguistic differences, we endeavor to test the applicability of the Stanford strategy to our task."
balakrishna-etal-2010-semi,Semi-Automatic Domain Ontology Creation from Text Resources,2010,14,13,2,0,46209,mithun balakrishna,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Analysts in various domains, especially intelligence and financial, have to constantly extract useful knowledge from large amounts of unstructured or semi-structured data. Keyword-based search, faceted search, question-answering, etc. are some of the automated methodologies that have been used to help analysts in their tasks. General-purpose and domain-specific ontologies have been proposed to help these automated methods in organizing data and providing access to useful information. However, problems in ontology creation and maintenance have resulted in expensive procedures for expanding/maintaining the ontology library available to support the growing and evolving needs of analysts. In this paper, we present a generalized and improved procedure to automatically extract deep semantic information from text resources and rapidly create semantically-rich domain ontologies while keeping the manual intervention to a minimum. We also present evaluation results for the intelligence and financial ontology libraries, semi-automatically created by our proposed methodologies using freely-available textual resources from the Web."
D10-1031,Automatic Discovery of Manner Relations and its Applications,2010,23,2,2,1,7292,eduardo blanco,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a method for the automatic discovery of MANNER relations from text. An extended definition of MANNER is proposed, including restrictions on the sorts of concepts that can be part of its domain and range. The connections with other relations and the lexico-syntactic patterns that encode MANNER are analyzed. A new feature set specialized on MANNER detection is depicted and justified. Experimental results show improvement over previous attempts to extract MANNER. Combinations of MANNER with other semantic relations are also discussed."
C10-2009,Composition of Semantic Relations: Model and Applications,2010,24,5,3,1,7292,eduardo blanco,Coling 2010: Posters,0,"This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments defined on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments."
blanco-etal-2008-causal,Causal Relation Extraction,2008,24,58,3,1,7292,eduardo blanco,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents a supervised method for the detection and extraction of Causal Relations from open domain text. First we give a brief outline of the definition of causation and how it relates to other Semantic Relations, as well as a characterization of their encoding. In this work, we only consider marked and explicit causations. Our approach first identifies the syntactic patterns that may encode a causation, then we use Machine Learning techniques to decide whether or not a pattern instance encodes a causation. We focus on the most productive pattern, a verb phrase followed by a relator and a clause, and its reverse version, a relator followed by a clause and a verb phrase. As relators we consider the words as, after, because and since. We present a set of lexical, syntactic and semantic features for the classification task, their rationale and some examples. The results obtained are discussed and the errors analyzed."
W07-1404,{COGEX} at {RTE} 3,2007,5,58,2,1,42868,marta tatu,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper reports on LCC's participation at the Third PASCAL Recognizing Textual Entailment Challenge. First, we summarize our semantic logical-based approach which proved successful in the previous two challenges. Then we highlight this year's innovations which contributed to an overall accuracy of 72.25% for the RTE 3 test data. The novelties include new resources, such as eXtended WordNet KB which provides a large number of world knowledge axioms, event and temporal information provided by the TARSQI toolkit, logic form representations of events, negation, coreference and context, and new improvements of lexical chain axiom generation. Finally, the system's performance and error analysis are discussed."
W06-3121,Phramer - An Open Source Statistical Phrase-Based Translator,2006,7,20,4,1,46210,marian olteanu,Proceedings on the Workshop on Statistical Machine Translation,0,This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder -- Phramer. The paper also presents the UTD (HLTRI) system build for the WMT06 shared task. Our goal was to improve the translation quality by enhancing the translation table and by preprocessing the source language text.
W06-3122,Language Models and Reranking for Machine Translation,2006,7,7,3,1,46210,marian olteanu,Proceedings on the Workshop on Statistical Machine Translation,0,Complex Language Models cannot be easily integrated in the first pass decoding of a Statistical Machine Translation system -- the decoder queries the LM a very large number of times; the search process in the decoding builds the hypotheses incrementally and cannot make use of LMs that analyze the whole sentence. We present in this paper the Language Computer's system for WMT06 that employs LM-powered reranking on hypotheses generated by phrase-based SMT systems.
P06-2038,Speeding Up Full Syntactic Parsing by Leveraging Partial Parsing Decisions,2006,11,11,2,0,49919,elliot glaysher,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Parsing is a computationally intensive task due to the combinatorial explosion seen in chart parsing algorithms that explore possible parse trees. In this paper, we propose a method to limit the combinatorial explosion by restricting the CYK chart parsing algorithm based on the output of a chunk parser. When tested on the three parsers presented in (Collins, 1999), we observed an approximate three-fold speedup with only an average decrease of 0.17% in both precision and recall."
P06-2105,A Logic-Based Semantic Approach to Recognizing Textual Entailment,2006,15,33,2,1,42868,marta tatu,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,This paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments. It also details a lexical inference system which boosts the performance of the deep semantic oriented approach on the RTE data. The linear combination of two slightly different logical systems with the third lexical inference system achieves 73.75% accuracy on the RTE 2006 data.
P06-1113,Question Answering with Lexical Chains Propagating Verb Arguments,2006,16,27,2,1,49110,adrian novischi,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations. The algorithm creates verb argument structures using VerbNet syntactic patterns. In order to increase the coverage, a larger set of verb senses were automatically associated with the existing patterns from VerbNet. The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers. Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4%."
J06-1005,Automatic Discovery of Part-Whole Relations,2006,41,226,3,1,28789,roxana girju,Computational Linguistics,0,"An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent approach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns."
P05-1026,Experiments with Interactive Question-Answering,2005,13,59,4,0.270978,13772,sanda harabagiu,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning. Generated off-line from topic representations of complex scenarios, predictive questions represent requests for information that capture the most salient (and diverse) aspects of a topic. We present experimental results from large user studies (featuring a fully-implemented interactive Q/A system named FERRET) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a Q/A dialogue."
H05-1035,{PP}-attachment Disambiguation using Large Context,2005,19,14,2,1,46210,marian olteanu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,Prepositional Phrase-attachment is a common source of ambiguity in natural language. The previous approaches use limited information to solve the ambiguity -- four lexical heads -- although humans disambiguate much better when the full sentence is available. We propose to solve the PP-attachment ambiguity with a Support Vector Machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the World Wide Web. The system was tested on several datasets obtaining an accuracy of 93.62% on a Penn Treebank-II dataset; 91.79% on a FrameNet dataset when no manually-annotated semantic information is provided and 92.85% when semantic information is provided.
H05-1047,A Semantic Approach to Recognizing Textual Entailment,2005,13,73,2,1,42868,marta tatu,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system."
H05-1112,A Semantic Scattering Model for the Automatic Interpretation of Genitives,2005,17,17,1,1,16607,dan moldovan,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses the automatic classification of the semantic relations expressed by the English genitives. A learning model is introduced based on the statistical analysis of the distribution of genitives' semantic relations on a large corpus. The semantic and contextual features of the genitive's noun phrase constituents play a key role in the identification of the semantic relation. The algorithm was tested on a corpus of approximately 2,000 sentences and achieved an accuracy of 79%, far better than 44% accuracy obtained with C5.0, or 43% obtained with a Naive Bayes algorithm, or 27% accuracy with a Support Vector Machines learner on the same corpus."
W04-2609,Models for the Semantic Classification of Noun Phrases,2004,19,105,1,1,16607,dan moldovan,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,"This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation."
W04-2610,Support Vector Machines Applied to the Classification of Semantic Relations in Nominalized Noun Phrases,2004,18,20,6,1,28789,roxana girju,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,The discovery of semantic relations in text plays an important role in many NLP applications. This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases. Nominalizations represent a subclass of NP constructions in which either the head or the modifier noun is derived from a verb while the other noun is an argument of this verb. Especially designed features are extracted automatically and used in a Support Vector Machine learning model. The paper presents preliminary results for the semantic classification of the most representative NP patterns using four distinct learning models.
W04-0840,Senseval-3 logic forms: A system and possible improvements,2004,4,6,2,0,51643,altaf mohammed,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"Logic Forms, particular powerful logic representations presented in Moldovan and Rus (2001), are simple yet highly effective. In this paper, the structure of Logic Forms and their generation from input text are described. The results of an evaluation comparing the Logic Forms generated by hand with those generated automatically are also reported. Finally, we suggest some improvements to the representation used in the LFI task based on our results."
W04-0841,{SVM} classification of {F}rame{N}et semantic roles,2004,0,4,1,1,16607,dan moldovan,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
W04-0848,{LCC}{'}s {WSD} systems for Senseval-3,2004,0,5,2,1,49110,adrian novischi,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
W03-1207,Discovery of Manner Relations and Their Applicability to Question Answering,2003,5,4,3,1,28789,roxana girju,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Summarization, Text Understanding and others. This paper presents a method for the automatic discovery of manner relations using a Naive Bayes learning algorithm. The method was tested on the UPenn Treebank2 corpus, and the targeted manner relations were detected with a precision of 64.44% and a recall of 68.67%."
N03-1011,Learning Semantic Constraints for the Automatic Discovery of Part-Whole Relations,2003,10,184,3,1,28789,roxana girju,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"The discovery of semantic relations from text becomes increasingly important for applications such as Question Answering, Information Extraction, Text Summarization, Text Understanding, and others. The semantic relations are detected by checking selectional constraints. This paper presents a method and its results for learning semantic constraints to detect part-whole relations. Twenty constraints were found. Their validity was tested on a 10,000 sentence corpus, and the targeted part-whole relations were detected with an accuracy of 83%."
N03-1022,{COGEX}: A Logic Prover for Question Answering,2003,7,180,1,1,16607,dan moldovan,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Recent TREC results have demonstrated the need for deeper text understanding methods. This paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a Question Answering system. The approach is to transform questions and answer passages into logic representations. World knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text. Moreover, the trace of the proofs provide answer justifications. The results show that the prover boosts the performance of the QA system on TREC questions by 30%."
P02-1005,Performance Issues and Error Analysis in an Open-Domain Question Answering System,2002,22,64,1,1,16607,dan moldovan,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding."
C02-1167,Lexical Chains for Question Answering,2002,6,116,1,1,16607,dan moldovan,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"The paper presents a method for finding topically related words on an extended WordNet. By exploiting the information in the WordNet glosses, the connectivity between the synsets is dramatically increased. Topical relations expressed as lexical chains on extended WordNet improve the performance of a question answering system by increasing the document retrieval recall and by providing the much needed axioms that link question keywords with answers."
C02-1169,Open-Domain Voice-Activated Question Answering,2002,14,33,2,0.502245,13772,sanda harabagiu,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,Voice-Activated Question Answering (VAQA) systems represent the next generation capability for universal access by integrating state-of-the-art in question answering Q&A and automatic speech recognition (ASR) in such a way that the performance of the combined system is better than the individual components. This paper presents an implemented VAQA system and describes the techniques that enable the terative refinement of both Q&A and ASR. The results of our experiments show that spoken questions can be processed with surprising accuracy when using our VAQA implementation.
S01-1031,Pattern Learning and Active Feature Selection for Word Sense Disambiguation,2001,6,23,2,1,1124,rada mihalcea,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"We present here the main ideas of the algorithm employed in the SMUls and SMUaw systems. These systems have participated in the Senseval-2 competition attaining the best performance for both English all words and English lexical sample tasks. The algorithm has two main components (1) pattern learning from available sense tagged corpora (SemCor) and dictionary definitions (WordNet), and (2) instance based learning with active feature selection, when training data is available for a particular word."
P01-1037,The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering,2001,14,72,2,0.502245,13772,sanda harabagiu,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents an open-domain textual Question-Answering system that uses several feedback loops to enhance its performance. These feedback loops combine in a new way statistical results with syntactic, semantic or pragmatic information derived from texts and lexical databases. The paper presents the contribution of each feedback loop to the overall performance of 76% human-assessed precise answers."
P01-1052,Logic Form Transformation of {W}ord{N}et and its Applicability to Question Answering,2001,12,127,1,1,16607,dan moldovan,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,WordNet is a rich source of world knowledge from which formal axioms can be derived. In this paper we present a method for transforming the WordNet glosses into logic forms and further into axioms. The transformation of WordNet glosses into logic forms is useful for theorem proving and other applications. The paper demonstrates the utility of the WordNet axioms in a question answering system to rank and extract answers.
W00-1104,Semantic Indexing using {W}ord{N}et Senses,2000,19,128,2,1,1124,rada mihalcea,{ACL}-2000 Workshop on Recent Advances in Natural Language Processing and Information Retrieval,0,"We describe in this paper a boolean Information Retrieval system that adds word semantics to the classic word based indexing. Two of the main tasks of our system, namely the indexing and retrieval components, are using a combined word-based and sense-based approach. The key to our system is a methodology for building semantic representations of open text, at word and collocation level. This new technique, called semantic indexing, shows improved effectiveness over the classic word based indexing techniques."
P00-1071,The Structure and Performance of an Open-Domain Question Answering System,2000,5,152,1,1,16607,dan moldovan,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents the architecture, operation and results obtained with the LASSO Question Answering system developed in the Natural Language Processing Laboratory at SMU. To find answers, the system relies on a combination of syntactic and semantic techniques. The search for the answer is based on a novel form of indexing called paragraph indexing. A score of 55.5% for short answers and 64.5% for long answers was achieved at the TREC-8 competition."
A00-1037,Domain-Specific Knowledge Acquisition from Text,2000,9,24,1,1,16607,dan moldovan,Sixth Applied Natural Language Processing Conference,0,"In many knowledge intensive applications, it is necessary to have extensive domain-specific knowledge in addition to general-purpose knowledge bases. This paper presents a methodology for discovering domain-specific concepts and relationships in an attempt to extend WordNet. The method was tested on five seed concepts selected from the financial domain: interest rate, stock market, inflation, economic growth, and employment."
W99-0501,{W}ord{N}et 2 - A Morphologically and Semantically Enhanced Resource,1999,0,154,3,0.405567,13772,sanda harabagiu,{SIGLEX}99: Standardizing Lexical Resources,0,None
P99-1020,A Method for Word Sense Disambiguation of Unrestricted Text,1999,13,137,2,1,1124,rada mihalcea,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Selecting the most appropriate sense for an ambiguous word in a sentence is a central problem in Natural Language Processing. In this paper, we present a method that attempts to disambiguate all the nouns, verbs, adverbs and adjectives in a text, using the senses provided in WordNet. The senses are ranked using two sources of information: (1) the Internet for gathering statistics for word-word cooccurrences and (2) WordNet for measuring the semantic density for a pair of words. We report an average accuracy of 80% for the first ranked sense, and 91% for the first two ranked senses. Extensions of this method for larger windows of more than two words are considered."
W98-0703,Word Sense Disambiguation based on Semantic Density,1998,13,33,2,1,1124,rada mihalcea,Usage of {W}ord{N}et in Natural Language Processing Systems,0,None
M93-1025,{USC}: Description of the {SNAP} System Used for {MUC}-5,1993,6,4,1,1,16607,dan moldovan,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,"The SNAP information extraction system has been developed as a part of a three-year SNAP project sponsored by the National Science Foundation. The main goal of the SNAP project is to build a massively parallel computer capable of fast and accurate natural language processing [5]. Throughout the project, a parallel computer was built in the Parallel Knowledge Processing Laboratory at USC, and various software was developed to operate the machine [3]. The approach in designing SNAP was to find a knowledge representation and a reasoning paradigm useful for natural language processing which exhibits massive parallelism. We have selected marker-passing on semantic networks as a way to represent and process linguistic knowledge."
C92-2121,Semantic Network Array Processor as a Massively Parallel Computing Platform for High Performance and Large-Scale Natural Language Processing,1992,31,12,2,0,48217,hiroaki kitano,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper demonstrates the utility of the Semantic Network Array Processor (SNAP) as a massively parallel platform for high performance and large-scale natural language processing systems. SNAP is an experimental massively parallel machine which is dedicated to, but not limited to, the natural language processing using semantic networks. In designing the SNAP, we have investigated various natural language processing systems and theories to determine the scope of the hardware support and a set of micro-coded instructions to be provided. As a result, SNAP employs an extended marker-passing model and a dynamically modifiable network model. A set of primitive instructions is micro-coded to directly support a parallel marker-passing, bitoperations, numeric operations, network modifications, and other essential functions for natural language processing. This paper demonstrates the utility of SNAP for various paradigms of natural language processing. We have discovered that the SNAP provides milliseconds or microsectonds performance on several important applications such as the memory-based parsing and translation, classification-based parsing, and VLKB search. Also, we argue that there are numerous opportunities in the NLP community to take advantages of the computational power of the SNAP."
1991.mtsummit-papers.15,Toward High Performance Machine Translation: Preliminary Results from Massively Parallel Memory-Based Translation on {SNAP},1991,28,1,2,0,48217,hiroaki kitano,Proceedings of Machine Translation Summit III: Papers,0,This paper describes a memory-based machine translation system developed for the Semantic Net- work Array Processor (SNAP). The goal of our work is to develop a scalable and high-performance memory-based machine translation system which utilizes the high degree of parallelism provided by the SNAP machine. We have implemented an experimental machine translation system DMSNAP as a central part of a real-time speech-to-speech dia- logue translation system. It is a SNAP version of the Î¦DMDIALOG speech-to-speech translation system. Memory-based natural language processing and syntactic constraint network model has been incorporated using parallel marker-passing which is directly supported from hardware level. Experimental results demonstrate that the parsing of a sentence is done in the order of milliseconds.
